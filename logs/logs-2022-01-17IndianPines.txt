creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd167db3898>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.428, val_acc:0.488]
Epoch [2/120    avg_loss:1.864, val_acc:0.556]
Epoch [3/120    avg_loss:1.508, val_acc:0.666]
Epoch [4/120    avg_loss:1.279, val_acc:0.678]
Epoch [5/120    avg_loss:1.035, val_acc:0.726]
Epoch [6/120    avg_loss:0.981, val_acc:0.704]
Epoch [7/120    avg_loss:0.829, val_acc:0.775]
Epoch [8/120    avg_loss:0.743, val_acc:0.780]
Epoch [9/120    avg_loss:0.684, val_acc:0.806]
Epoch [10/120    avg_loss:0.623, val_acc:0.777]
Epoch [11/120    avg_loss:0.512, val_acc:0.829]
Epoch [12/120    avg_loss:0.461, val_acc:0.811]
Epoch [13/120    avg_loss:0.454, val_acc:0.869]
Epoch [14/120    avg_loss:0.373, val_acc:0.848]
Epoch [15/120    avg_loss:0.473, val_acc:0.842]
Epoch [16/120    avg_loss:0.419, val_acc:0.871]
Epoch [17/120    avg_loss:0.329, val_acc:0.884]
Epoch [18/120    avg_loss:0.240, val_acc:0.910]
Epoch [19/120    avg_loss:0.263, val_acc:0.891]
Epoch [20/120    avg_loss:0.245, val_acc:0.914]
Epoch [21/120    avg_loss:0.178, val_acc:0.912]
Epoch [22/120    avg_loss:0.214, val_acc:0.923]
Epoch [23/120    avg_loss:0.237, val_acc:0.888]
Epoch [24/120    avg_loss:0.201, val_acc:0.916]
Epoch [25/120    avg_loss:0.127, val_acc:0.945]
Epoch [26/120    avg_loss:0.184, val_acc:0.902]
Epoch [27/120    avg_loss:0.287, val_acc:0.890]
Epoch [28/120    avg_loss:0.162, val_acc:0.939]
Epoch [29/120    avg_loss:0.212, val_acc:0.924]
Epoch [30/120    avg_loss:0.160, val_acc:0.932]
Epoch [31/120    avg_loss:0.084, val_acc:0.942]
Epoch [32/120    avg_loss:0.084, val_acc:0.951]
Epoch [33/120    avg_loss:0.102, val_acc:0.916]
Epoch [34/120    avg_loss:0.119, val_acc:0.944]
Epoch [35/120    avg_loss:0.103, val_acc:0.949]
Epoch [36/120    avg_loss:0.104, val_acc:0.962]
Epoch [37/120    avg_loss:0.073, val_acc:0.958]
Epoch [38/120    avg_loss:0.065, val_acc:0.957]
Epoch [39/120    avg_loss:0.084, val_acc:0.956]
Epoch [40/120    avg_loss:0.079, val_acc:0.964]
Epoch [41/120    avg_loss:0.055, val_acc:0.970]
Epoch [42/120    avg_loss:0.089, val_acc:0.946]
Epoch [43/120    avg_loss:0.098, val_acc:0.948]
Epoch [44/120    avg_loss:0.070, val_acc:0.956]
Epoch [45/120    avg_loss:0.044, val_acc:0.967]
Epoch [46/120    avg_loss:0.043, val_acc:0.965]
Epoch [47/120    avg_loss:0.067, val_acc:0.949]
Epoch [48/120    avg_loss:0.035, val_acc:0.966]
Epoch [49/120    avg_loss:0.034, val_acc:0.967]
Epoch [50/120    avg_loss:0.040, val_acc:0.974]
Epoch [51/120    avg_loss:0.053, val_acc:0.966]
Epoch [52/120    avg_loss:0.038, val_acc:0.961]
Epoch [53/120    avg_loss:0.025, val_acc:0.972]
Epoch [54/120    avg_loss:0.062, val_acc:0.834]
Epoch [55/120    avg_loss:0.288, val_acc:0.939]
Epoch [56/120    avg_loss:0.063, val_acc:0.966]
Epoch [57/120    avg_loss:0.099, val_acc:0.939]
Epoch [58/120    avg_loss:0.057, val_acc:0.951]
Epoch [59/120    avg_loss:0.064, val_acc:0.960]
Epoch [60/120    avg_loss:0.055, val_acc:0.963]
Epoch [61/120    avg_loss:0.048, val_acc:0.949]
Epoch [62/120    avg_loss:0.043, val_acc:0.972]
Epoch [63/120    avg_loss:0.065, val_acc:0.969]
Epoch [64/120    avg_loss:0.040, val_acc:0.973]
Epoch [65/120    avg_loss:0.044, val_acc:0.976]
Epoch [66/120    avg_loss:0.029, val_acc:0.975]
Epoch [67/120    avg_loss:0.032, val_acc:0.975]
Epoch [68/120    avg_loss:0.023, val_acc:0.976]
Epoch [69/120    avg_loss:0.019, val_acc:0.976]
Epoch [70/120    avg_loss:0.023, val_acc:0.977]
Epoch [71/120    avg_loss:0.026, val_acc:0.974]
Epoch [72/120    avg_loss:0.022, val_acc:0.973]
Epoch [73/120    avg_loss:0.026, val_acc:0.977]
Epoch [74/120    avg_loss:0.018, val_acc:0.977]
Epoch [75/120    avg_loss:0.020, val_acc:0.977]
Epoch [76/120    avg_loss:0.022, val_acc:0.977]
Epoch [77/120    avg_loss:0.012, val_acc:0.982]
Epoch [78/120    avg_loss:0.020, val_acc:0.980]
Epoch [79/120    avg_loss:0.017, val_acc:0.978]
Epoch [80/120    avg_loss:0.017, val_acc:0.981]
Epoch [81/120    avg_loss:0.016, val_acc:0.981]
Epoch [82/120    avg_loss:0.017, val_acc:0.978]
Epoch [83/120    avg_loss:0.015, val_acc:0.981]
Epoch [84/120    avg_loss:0.018, val_acc:0.977]
Epoch [85/120    avg_loss:0.017, val_acc:0.980]
Epoch [86/120    avg_loss:0.015, val_acc:0.978]
Epoch [87/120    avg_loss:0.017, val_acc:0.980]
Epoch [88/120    avg_loss:0.014, val_acc:0.978]
Epoch [89/120    avg_loss:0.016, val_acc:0.978]
Epoch [90/120    avg_loss:0.013, val_acc:0.981]
Epoch [91/120    avg_loss:0.013, val_acc:0.982]
Epoch [92/120    avg_loss:0.017, val_acc:0.981]
Epoch [93/120    avg_loss:0.018, val_acc:0.982]
Epoch [94/120    avg_loss:0.015, val_acc:0.983]
Epoch [95/120    avg_loss:0.017, val_acc:0.982]
Epoch [96/120    avg_loss:0.014, val_acc:0.982]
Epoch [97/120    avg_loss:0.014, val_acc:0.982]
Epoch [98/120    avg_loss:0.015, val_acc:0.983]
Epoch [99/120    avg_loss:0.015, val_acc:0.983]
Epoch [100/120    avg_loss:0.016, val_acc:0.983]
Epoch [101/120    avg_loss:0.014, val_acc:0.984]
Epoch [102/120    avg_loss:0.017, val_acc:0.983]
Epoch [103/120    avg_loss:0.014, val_acc:0.983]
Epoch [104/120    avg_loss:0.013, val_acc:0.983]
Epoch [105/120    avg_loss:0.015, val_acc:0.982]
Epoch [106/120    avg_loss:0.013, val_acc:0.982]
Epoch [107/120    avg_loss:0.015, val_acc:0.981]
Epoch [108/120    avg_loss:0.012, val_acc:0.980]
Epoch [109/120    avg_loss:0.018, val_acc:0.982]
Epoch [110/120    avg_loss:0.016, val_acc:0.981]
Epoch [111/120    avg_loss:0.011, val_acc:0.981]
Epoch [112/120    avg_loss:0.017, val_acc:0.982]
Epoch [113/120    avg_loss:0.016, val_acc:0.981]
Epoch [114/120    avg_loss:0.011, val_acc:0.980]
Epoch [115/120    avg_loss:0.021, val_acc:0.980]
Epoch [116/120    avg_loss:0.015, val_acc:0.980]
Epoch [117/120    avg_loss:0.015, val_acc:0.980]
Epoch [118/120    avg_loss:0.013, val_acc:0.980]
Epoch [119/120    avg_loss:0.015, val_acc:0.980]
Epoch [120/120    avg_loss:0.013, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1260    4    3    6    0    0    0    0    3    9    0    0
     0    0    0]
 [   0    0    0  733    0    0    0    0    0    4    0    1    6    0
     0    3    0]
 [   0    0    0    4  209    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    1    0    0    1  834   37    0    0
     0    0    0]
 [   0    0    5    0    0    1    0    0    0    5   18 2170    6    1
     0    3    1]
 [   0    0    0    1    0    0    0    0    0    0    0    0  527    0
     0    2    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
  1123   14    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    75  272    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.48509485094851

F1 scores:
[       nan 0.96202532 0.98746082 0.98455339 0.98352941 0.98861048
 0.99618612 1.         0.997669   0.7826087  0.96249279 0.97968397
 0.98046512 0.99730458 0.96023942 0.84867395 0.97109827]

Kappa:
0.9713138536501492
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa2baa2b828>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.429, val_acc:0.551]
Epoch [2/120    avg_loss:1.898, val_acc:0.610]
Epoch [3/120    avg_loss:1.588, val_acc:0.632]
Epoch [4/120    avg_loss:1.401, val_acc:0.659]
Epoch [5/120    avg_loss:1.200, val_acc:0.695]
Epoch [6/120    avg_loss:0.956, val_acc:0.722]
Epoch [7/120    avg_loss:0.876, val_acc:0.755]
Epoch [8/120    avg_loss:0.710, val_acc:0.816]
Epoch [9/120    avg_loss:0.625, val_acc:0.825]
Epoch [10/120    avg_loss:0.603, val_acc:0.778]
Epoch [11/120    avg_loss:0.510, val_acc:0.838]
Epoch [12/120    avg_loss:0.499, val_acc:0.824]
Epoch [13/120    avg_loss:0.459, val_acc:0.842]
Epoch [14/120    avg_loss:0.380, val_acc:0.863]
Epoch [15/120    avg_loss:0.333, val_acc:0.890]
Epoch [16/120    avg_loss:0.344, val_acc:0.892]
Epoch [17/120    avg_loss:0.392, val_acc:0.855]
Epoch [18/120    avg_loss:0.340, val_acc:0.800]
Epoch [19/120    avg_loss:0.298, val_acc:0.904]
Epoch [20/120    avg_loss:0.238, val_acc:0.908]
Epoch [21/120    avg_loss:0.209, val_acc:0.907]
Epoch [22/120    avg_loss:0.245, val_acc:0.888]
Epoch [23/120    avg_loss:0.161, val_acc:0.945]
Epoch [24/120    avg_loss:0.137, val_acc:0.939]
Epoch [25/120    avg_loss:0.182, val_acc:0.928]
Epoch [26/120    avg_loss:0.157, val_acc:0.880]
Epoch [27/120    avg_loss:0.153, val_acc:0.916]
Epoch [28/120    avg_loss:0.119, val_acc:0.926]
Epoch [29/120    avg_loss:0.140, val_acc:0.927]
Epoch [30/120    avg_loss:0.159, val_acc:0.914]
Epoch [31/120    avg_loss:0.126, val_acc:0.919]
Epoch [32/120    avg_loss:0.165, val_acc:0.916]
Epoch [33/120    avg_loss:0.132, val_acc:0.942]
Epoch [34/120    avg_loss:0.103, val_acc:0.948]
Epoch [35/120    avg_loss:0.082, val_acc:0.961]
Epoch [36/120    avg_loss:0.105, val_acc:0.937]
Epoch [37/120    avg_loss:0.162, val_acc:0.913]
Epoch [38/120    avg_loss:0.080, val_acc:0.954]
Epoch [39/120    avg_loss:0.066, val_acc:0.945]
Epoch [40/120    avg_loss:0.126, val_acc:0.945]
Epoch [41/120    avg_loss:0.087, val_acc:0.955]
Epoch [42/120    avg_loss:0.071, val_acc:0.945]
Epoch [43/120    avg_loss:0.118, val_acc:0.950]
Epoch [44/120    avg_loss:0.084, val_acc:0.946]
Epoch [45/120    avg_loss:0.084, val_acc:0.939]
Epoch [46/120    avg_loss:0.072, val_acc:0.955]
Epoch [47/120    avg_loss:0.057, val_acc:0.951]
Epoch [48/120    avg_loss:0.063, val_acc:0.941]
Epoch [49/120    avg_loss:0.058, val_acc:0.971]
Epoch [50/120    avg_loss:0.042, val_acc:0.972]
Epoch [51/120    avg_loss:0.038, val_acc:0.973]
Epoch [52/120    avg_loss:0.031, val_acc:0.975]
Epoch [53/120    avg_loss:0.031, val_acc:0.977]
Epoch [54/120    avg_loss:0.026, val_acc:0.975]
Epoch [55/120    avg_loss:0.031, val_acc:0.976]
Epoch [56/120    avg_loss:0.026, val_acc:0.977]
Epoch [57/120    avg_loss:0.028, val_acc:0.977]
Epoch [58/120    avg_loss:0.026, val_acc:0.976]
Epoch [59/120    avg_loss:0.026, val_acc:0.977]
Epoch [60/120    avg_loss:0.028, val_acc:0.979]
Epoch [61/120    avg_loss:0.029, val_acc:0.979]
Epoch [62/120    avg_loss:0.027, val_acc:0.980]
Epoch [63/120    avg_loss:0.026, val_acc:0.980]
Epoch [64/120    avg_loss:0.029, val_acc:0.979]
Epoch [65/120    avg_loss:0.023, val_acc:0.978]
Epoch [66/120    avg_loss:0.026, val_acc:0.979]
Epoch [67/120    avg_loss:0.027, val_acc:0.978]
Epoch [68/120    avg_loss:0.023, val_acc:0.979]
Epoch [69/120    avg_loss:0.023, val_acc:0.978]
Epoch [70/120    avg_loss:0.023, val_acc:0.979]
Epoch [71/120    avg_loss:0.023, val_acc:0.980]
Epoch [72/120    avg_loss:0.026, val_acc:0.980]
Epoch [73/120    avg_loss:0.019, val_acc:0.979]
Epoch [74/120    avg_loss:0.019, val_acc:0.979]
Epoch [75/120    avg_loss:0.027, val_acc:0.979]
Epoch [76/120    avg_loss:0.023, val_acc:0.979]
Epoch [77/120    avg_loss:0.028, val_acc:0.980]
Epoch [78/120    avg_loss:0.023, val_acc:0.980]
Epoch [79/120    avg_loss:0.026, val_acc:0.980]
Epoch [80/120    avg_loss:0.020, val_acc:0.980]
Epoch [81/120    avg_loss:0.022, val_acc:0.981]
Epoch [82/120    avg_loss:0.023, val_acc:0.980]
Epoch [83/120    avg_loss:0.028, val_acc:0.980]
Epoch [84/120    avg_loss:0.019, val_acc:0.980]
Epoch [85/120    avg_loss:0.022, val_acc:0.980]
Epoch [86/120    avg_loss:0.020, val_acc:0.981]
Epoch [87/120    avg_loss:0.021, val_acc:0.978]
Epoch [88/120    avg_loss:0.020, val_acc:0.980]
Epoch [89/120    avg_loss:0.020, val_acc:0.980]
Epoch [90/120    avg_loss:0.021, val_acc:0.980]
Epoch [91/120    avg_loss:0.022, val_acc:0.981]
Epoch [92/120    avg_loss:0.023, val_acc:0.981]
Epoch [93/120    avg_loss:0.015, val_acc:0.982]
Epoch [94/120    avg_loss:0.019, val_acc:0.982]
Epoch [95/120    avg_loss:0.018, val_acc:0.982]
Epoch [96/120    avg_loss:0.020, val_acc:0.980]
Epoch [97/120    avg_loss:0.020, val_acc:0.982]
Epoch [98/120    avg_loss:0.017, val_acc:0.982]
Epoch [99/120    avg_loss:0.018, val_acc:0.983]
Epoch [100/120    avg_loss:0.021, val_acc:0.982]
Epoch [101/120    avg_loss:0.022, val_acc:0.979]
Epoch [102/120    avg_loss:0.018, val_acc:0.980]
Epoch [103/120    avg_loss:0.016, val_acc:0.980]
Epoch [104/120    avg_loss:0.016, val_acc:0.980]
Epoch [105/120    avg_loss:0.020, val_acc:0.981]
Epoch [106/120    avg_loss:0.020, val_acc:0.981]
Epoch [107/120    avg_loss:0.027, val_acc:0.979]
Epoch [108/120    avg_loss:0.022, val_acc:0.979]
Epoch [109/120    avg_loss:0.020, val_acc:0.980]
Epoch [110/120    avg_loss:0.017, val_acc:0.980]
Epoch [111/120    avg_loss:0.018, val_acc:0.979]
Epoch [112/120    avg_loss:0.021, val_acc:0.980]
Epoch [113/120    avg_loss:0.018, val_acc:0.980]
Epoch [114/120    avg_loss:0.020, val_acc:0.980]
Epoch [115/120    avg_loss:0.016, val_acc:0.980]
Epoch [116/120    avg_loss:0.017, val_acc:0.980]
Epoch [117/120    avg_loss:0.014, val_acc:0.979]
Epoch [118/120    avg_loss:0.017, val_acc:0.980]
Epoch [119/120    avg_loss:0.018, val_acc:0.981]
Epoch [120/120    avg_loss:0.016, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1250    4    8    0    0    0    0    0    6   10    5    0
     0    2    0]
 [   0    0    0  730    0    0    0    0    0    4    0    0    9    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    0    2    0    0    2  835   27    1    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0    6 2196    5    1
     1    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0  529    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1118   19    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    42  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.09214092140921

F1 scores:
[       nan 1.         0.9827044  0.98582039 0.98156682 1.
 0.99771863 1.         0.99883856 0.85714286 0.96867749 0.98829883
 0.97601476 0.98666667 0.97217391 0.9063893  0.97076023]

Kappa:
0.9782435560202838
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa6b6393ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.778, val_acc:0.477]
Epoch [2/120    avg_loss:2.597, val_acc:0.476]
Epoch [3/120    avg_loss:2.374, val_acc:0.477]
Epoch [4/120    avg_loss:2.175, val_acc:0.538]
Epoch [5/120    avg_loss:1.926, val_acc:0.520]
Epoch [6/120    avg_loss:1.691, val_acc:0.621]
Epoch [7/120    avg_loss:1.520, val_acc:0.645]
Epoch [8/120    avg_loss:1.383, val_acc:0.656]
Epoch [9/120    avg_loss:1.135, val_acc:0.741]
Epoch [10/120    avg_loss:1.027, val_acc:0.732]
Epoch [11/120    avg_loss:0.849, val_acc:0.720]
Epoch [12/120    avg_loss:0.775, val_acc:0.754]
Epoch [13/120    avg_loss:0.632, val_acc:0.821]
Epoch [14/120    avg_loss:0.551, val_acc:0.843]
Epoch [15/120    avg_loss:0.469, val_acc:0.865]
Epoch [16/120    avg_loss:0.408, val_acc:0.877]
Epoch [17/120    avg_loss:0.334, val_acc:0.889]
Epoch [18/120    avg_loss:0.361, val_acc:0.879]
Epoch [19/120    avg_loss:0.349, val_acc:0.882]
Epoch [20/120    avg_loss:0.269, val_acc:0.908]
Epoch [21/120    avg_loss:0.223, val_acc:0.923]
Epoch [22/120    avg_loss:0.387, val_acc:0.880]
Epoch [23/120    avg_loss:0.420, val_acc:0.862]
Epoch [24/120    avg_loss:0.262, val_acc:0.908]
Epoch [25/120    avg_loss:0.233, val_acc:0.902]
Epoch [26/120    avg_loss:0.159, val_acc:0.928]
Epoch [27/120    avg_loss:0.142, val_acc:0.926]
Epoch [28/120    avg_loss:0.204, val_acc:0.905]
Epoch [29/120    avg_loss:0.373, val_acc:0.896]
Epoch [30/120    avg_loss:0.206, val_acc:0.911]
Epoch [31/120    avg_loss:0.143, val_acc:0.946]
Epoch [32/120    avg_loss:0.100, val_acc:0.940]
Epoch [33/120    avg_loss:0.136, val_acc:0.926]
Epoch [34/120    avg_loss:0.101, val_acc:0.949]
Epoch [35/120    avg_loss:0.111, val_acc:0.912]
Epoch [36/120    avg_loss:0.085, val_acc:0.937]
Epoch [37/120    avg_loss:0.143, val_acc:0.928]
Epoch [38/120    avg_loss:0.108, val_acc:0.950]
Epoch [39/120    avg_loss:0.089, val_acc:0.948]
Epoch [40/120    avg_loss:0.081, val_acc:0.883]
Epoch [41/120    avg_loss:0.066, val_acc:0.956]
Epoch [42/120    avg_loss:1.534, val_acc:0.586]
Epoch [43/120    avg_loss:1.068, val_acc:0.741]
Epoch [44/120    avg_loss:0.652, val_acc:0.867]
Epoch [45/120    avg_loss:0.404, val_acc:0.866]
Epoch [46/120    avg_loss:0.348, val_acc:0.895]
Epoch [47/120    avg_loss:0.212, val_acc:0.924]
Epoch [48/120    avg_loss:0.137, val_acc:0.943]
Epoch [49/120    avg_loss:0.130, val_acc:0.947]
Epoch [50/120    avg_loss:0.106, val_acc:0.946]
Epoch [51/120    avg_loss:0.095, val_acc:0.959]
Epoch [52/120    avg_loss:0.098, val_acc:0.917]
Epoch [53/120    avg_loss:0.085, val_acc:0.949]
Epoch [54/120    avg_loss:0.065, val_acc:0.962]
Epoch [55/120    avg_loss:0.046, val_acc:0.947]
Epoch [56/120    avg_loss:0.035, val_acc:0.964]
Epoch [57/120    avg_loss:0.051, val_acc:0.956]
Epoch [58/120    avg_loss:0.034, val_acc:0.970]
Epoch [59/120    avg_loss:0.026, val_acc:0.967]
Epoch [60/120    avg_loss:0.023, val_acc:0.967]
Epoch [61/120    avg_loss:0.019, val_acc:0.964]
Epoch [62/120    avg_loss:0.033, val_acc:0.951]
Epoch [63/120    avg_loss:0.052, val_acc:0.972]
Epoch [64/120    avg_loss:0.039, val_acc:0.951]
Epoch [65/120    avg_loss:0.041, val_acc:0.965]
Epoch [66/120    avg_loss:0.035, val_acc:0.962]
Epoch [67/120    avg_loss:0.021, val_acc:0.969]
Epoch [68/120    avg_loss:0.033, val_acc:0.969]
Epoch [69/120    avg_loss:0.023, val_acc:0.966]
Epoch [70/120    avg_loss:0.022, val_acc:0.951]
Epoch [71/120    avg_loss:0.023, val_acc:0.968]
Epoch [72/120    avg_loss:0.019, val_acc:0.957]
Epoch [73/120    avg_loss:0.012, val_acc:0.967]
Epoch [74/120    avg_loss:0.014, val_acc:0.965]
Epoch [75/120    avg_loss:0.025, val_acc:0.970]
Epoch [76/120    avg_loss:0.014, val_acc:0.969]
Epoch [77/120    avg_loss:0.012, val_acc:0.973]
Epoch [78/120    avg_loss:0.010, val_acc:0.973]
Epoch [79/120    avg_loss:0.008, val_acc:0.975]
Epoch [80/120    avg_loss:0.008, val_acc:0.973]
Epoch [81/120    avg_loss:0.009, val_acc:0.975]
Epoch [82/120    avg_loss:0.007, val_acc:0.973]
Epoch [83/120    avg_loss:0.007, val_acc:0.973]
Epoch [84/120    avg_loss:0.007, val_acc:0.974]
Epoch [85/120    avg_loss:0.008, val_acc:0.975]
Epoch [86/120    avg_loss:0.008, val_acc:0.975]
Epoch [87/120    avg_loss:0.009, val_acc:0.974]
Epoch [88/120    avg_loss:0.006, val_acc:0.974]
Epoch [89/120    avg_loss:0.006, val_acc:0.974]
Epoch [90/120    avg_loss:0.007, val_acc:0.975]
Epoch [91/120    avg_loss:0.008, val_acc:0.974]
Epoch [92/120    avg_loss:0.006, val_acc:0.974]
Epoch [93/120    avg_loss:0.008, val_acc:0.974]
Epoch [94/120    avg_loss:0.007, val_acc:0.974]
Epoch [95/120    avg_loss:0.006, val_acc:0.974]
Epoch [96/120    avg_loss:0.005, val_acc:0.975]
Epoch [97/120    avg_loss:0.006, val_acc:0.976]
Epoch [98/120    avg_loss:0.007, val_acc:0.975]
Epoch [99/120    avg_loss:0.006, val_acc:0.976]
Epoch [100/120    avg_loss:0.008, val_acc:0.974]
Epoch [101/120    avg_loss:0.007, val_acc:0.975]
Epoch [102/120    avg_loss:0.006, val_acc:0.976]
Epoch [103/120    avg_loss:0.005, val_acc:0.974]
Epoch [104/120    avg_loss:0.007, val_acc:0.975]
Epoch [105/120    avg_loss:0.005, val_acc:0.976]
Epoch [106/120    avg_loss:0.005, val_acc:0.974]
Epoch [107/120    avg_loss:0.007, val_acc:0.975]
Epoch [108/120    avg_loss:0.006, val_acc:0.974]
Epoch [109/120    avg_loss:0.006, val_acc:0.974]
Epoch [110/120    avg_loss:0.006, val_acc:0.974]
Epoch [111/120    avg_loss:0.008, val_acc:0.974]
Epoch [112/120    avg_loss:0.007, val_acc:0.974]
Epoch [113/120    avg_loss:0.006, val_acc:0.974]
Epoch [114/120    avg_loss:0.007, val_acc:0.974]
Epoch [115/120    avg_loss:0.006, val_acc:0.977]
Epoch [116/120    avg_loss:0.005, val_acc:0.976]
Epoch [117/120    avg_loss:0.006, val_acc:0.975]
Epoch [118/120    avg_loss:0.006, val_acc:0.975]
Epoch [119/120    avg_loss:0.005, val_acc:0.974]
Epoch [120/120    avg_loss:0.005, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    2    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1242   10    9    4    1    0    0    0    1   17    1    0
     0    0    0]
 [   0    0    0  740    3    0    0    0    0    0    0    1    3    0
     0    0    0]
 [   0    0    0    4  207    2    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    2    0    0    0    0    1    0    0
     7    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    2    0    0    0    0  835   27    1    0
     0    0    0]
 [   0    0    1    1    0    0    0    0    0    2   10 2181   11    0
     1    3    0]
 [   0    0    0    2    0    0    0    0    0    0    1    0  529    0
     0    2    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1123   16    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   111  236    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.06233062330624

F1 scores:
[       nan 0.975      0.9787234  0.98404255 0.95833333 0.97701149
 0.99619772 1.         1.         0.94736842 0.96980256 0.98309669
 0.98053753 1.         0.94250944 0.78145695 1.        ]

Kappa:
0.9664739696335346
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f054c2e0e10>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.782, val_acc:0.407]
Epoch [2/120    avg_loss:2.593, val_acc:0.457]
Epoch [3/120    avg_loss:2.369, val_acc:0.515]
Epoch [4/120    avg_loss:2.145, val_acc:0.578]
Epoch [5/120    avg_loss:1.897, val_acc:0.543]
Epoch [6/120    avg_loss:1.674, val_acc:0.615]
Epoch [7/120    avg_loss:1.485, val_acc:0.661]
Epoch [8/120    avg_loss:1.242, val_acc:0.670]
Epoch [9/120    avg_loss:1.069, val_acc:0.661]
Epoch [10/120    avg_loss:0.938, val_acc:0.725]
Epoch [11/120    avg_loss:0.919, val_acc:0.710]
Epoch [12/120    avg_loss:0.759, val_acc:0.754]
Epoch [13/120    avg_loss:0.641, val_acc:0.790]
Epoch [14/120    avg_loss:0.495, val_acc:0.792]
Epoch [15/120    avg_loss:0.558, val_acc:0.738]
Epoch [16/120    avg_loss:0.449, val_acc:0.835]
Epoch [17/120    avg_loss:0.377, val_acc:0.850]
Epoch [18/120    avg_loss:0.307, val_acc:0.889]
Epoch [19/120    avg_loss:0.309, val_acc:0.749]
Epoch [20/120    avg_loss:0.311, val_acc:0.860]
Epoch [21/120    avg_loss:0.310, val_acc:0.910]
Epoch [22/120    avg_loss:0.217, val_acc:0.907]
Epoch [23/120    avg_loss:0.188, val_acc:0.916]
Epoch [24/120    avg_loss:0.167, val_acc:0.918]
Epoch [25/120    avg_loss:0.151, val_acc:0.944]
Epoch [26/120    avg_loss:0.119, val_acc:0.948]
Epoch [27/120    avg_loss:0.128, val_acc:0.940]
Epoch [28/120    avg_loss:0.124, val_acc:0.954]
Epoch [29/120    avg_loss:0.113, val_acc:0.920]
Epoch [30/120    avg_loss:0.247, val_acc:0.859]
Epoch [31/120    avg_loss:0.185, val_acc:0.908]
Epoch [32/120    avg_loss:0.143, val_acc:0.953]
Epoch [33/120    avg_loss:0.089, val_acc:0.941]
Epoch [34/120    avg_loss:0.104, val_acc:0.959]
Epoch [35/120    avg_loss:0.120, val_acc:0.964]
Epoch [36/120    avg_loss:0.095, val_acc:0.943]
Epoch [37/120    avg_loss:0.080, val_acc:0.954]
Epoch [38/120    avg_loss:0.127, val_acc:0.930]
Epoch [39/120    avg_loss:0.109, val_acc:0.941]
Epoch [40/120    avg_loss:0.050, val_acc:0.968]
Epoch [41/120    avg_loss:0.053, val_acc:0.966]
Epoch [42/120    avg_loss:0.045, val_acc:0.969]
Epoch [43/120    avg_loss:0.060, val_acc:0.957]
Epoch [44/120    avg_loss:0.093, val_acc:0.943]
Epoch [45/120    avg_loss:0.068, val_acc:0.956]
Epoch [46/120    avg_loss:0.077, val_acc:0.931]
Epoch [47/120    avg_loss:0.096, val_acc:0.956]
Epoch [48/120    avg_loss:0.052, val_acc:0.960]
Epoch [49/120    avg_loss:0.066, val_acc:0.956]
Epoch [50/120    avg_loss:0.034, val_acc:0.964]
Epoch [51/120    avg_loss:0.026, val_acc:0.969]
Epoch [52/120    avg_loss:0.030, val_acc:0.973]
Epoch [53/120    avg_loss:0.051, val_acc:0.945]
Epoch [54/120    avg_loss:0.110, val_acc:0.942]
Epoch [55/120    avg_loss:0.073, val_acc:0.967]
Epoch [56/120    avg_loss:0.031, val_acc:0.962]
Epoch [57/120    avg_loss:0.025, val_acc:0.963]
Epoch [58/120    avg_loss:0.020, val_acc:0.974]
Epoch [59/120    avg_loss:0.023, val_acc:0.971]
Epoch [60/120    avg_loss:0.034, val_acc:0.976]
Epoch [61/120    avg_loss:0.044, val_acc:0.968]
Epoch [62/120    avg_loss:0.041, val_acc:0.953]
Epoch [63/120    avg_loss:0.032, val_acc:0.977]
Epoch [64/120    avg_loss:0.020, val_acc:0.969]
Epoch [65/120    avg_loss:0.026, val_acc:0.939]
Epoch [66/120    avg_loss:0.028, val_acc:0.967]
Epoch [67/120    avg_loss:0.016, val_acc:0.972]
Epoch [68/120    avg_loss:0.020, val_acc:0.941]
Epoch [69/120    avg_loss:0.021, val_acc:0.968]
Epoch [70/120    avg_loss:0.040, val_acc:0.970]
Epoch [71/120    avg_loss:0.025, val_acc:0.975]
Epoch [72/120    avg_loss:0.015, val_acc:0.976]
Epoch [73/120    avg_loss:0.028, val_acc:0.963]
Epoch [74/120    avg_loss:0.024, val_acc:0.971]
Epoch [75/120    avg_loss:0.020, val_acc:0.960]
Epoch [76/120    avg_loss:0.027, val_acc:0.967]
Epoch [77/120    avg_loss:0.019, val_acc:0.972]
Epoch [78/120    avg_loss:0.013, val_acc:0.974]
Epoch [79/120    avg_loss:0.013, val_acc:0.975]
Epoch [80/120    avg_loss:0.011, val_acc:0.975]
Epoch [81/120    avg_loss:0.007, val_acc:0.976]
Epoch [82/120    avg_loss:0.016, val_acc:0.978]
Epoch [83/120    avg_loss:0.009, val_acc:0.976]
Epoch [84/120    avg_loss:0.009, val_acc:0.975]
Epoch [85/120    avg_loss:0.008, val_acc:0.976]
Epoch [86/120    avg_loss:0.007, val_acc:0.975]
Epoch [87/120    avg_loss:0.006, val_acc:0.975]
Epoch [88/120    avg_loss:0.008, val_acc:0.977]
Epoch [89/120    avg_loss:0.007, val_acc:0.977]
Epoch [90/120    avg_loss:0.010, val_acc:0.976]
Epoch [91/120    avg_loss:0.009, val_acc:0.978]
Epoch [92/120    avg_loss:0.008, val_acc:0.978]
Epoch [93/120    avg_loss:0.009, val_acc:0.977]
Epoch [94/120    avg_loss:0.007, val_acc:0.978]
Epoch [95/120    avg_loss:0.007, val_acc:0.980]
Epoch [96/120    avg_loss:0.010, val_acc:0.975]
Epoch [97/120    avg_loss:0.010, val_acc:0.977]
Epoch [98/120    avg_loss:0.006, val_acc:0.978]
Epoch [99/120    avg_loss:0.007, val_acc:0.978]
Epoch [100/120    avg_loss:0.010, val_acc:0.976]
Epoch [101/120    avg_loss:0.007, val_acc:0.976]
Epoch [102/120    avg_loss:0.007, val_acc:0.977]
Epoch [103/120    avg_loss:0.007, val_acc:0.976]
Epoch [104/120    avg_loss:0.007, val_acc:0.978]
Epoch [105/120    avg_loss:0.007, val_acc:0.976]
Epoch [106/120    avg_loss:0.007, val_acc:0.975]
Epoch [107/120    avg_loss:0.005, val_acc:0.975]
Epoch [108/120    avg_loss:0.012, val_acc:0.975]
Epoch [109/120    avg_loss:0.009, val_acc:0.976]
Epoch [110/120    avg_loss:0.007, val_acc:0.976]
Epoch [111/120    avg_loss:0.006, val_acc:0.976]
Epoch [112/120    avg_loss:0.006, val_acc:0.976]
Epoch [113/120    avg_loss:0.008, val_acc:0.976]
Epoch [114/120    avg_loss:0.006, val_acc:0.976]
Epoch [115/120    avg_loss:0.006, val_acc:0.976]
Epoch [116/120    avg_loss:0.006, val_acc:0.976]
Epoch [117/120    avg_loss:0.006, val_acc:0.976]
Epoch [118/120    avg_loss:0.006, val_acc:0.976]
Epoch [119/120    avg_loss:0.008, val_acc:0.976]
Epoch [120/120    avg_loss:0.006, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1248    6    4    3    1    0    0    0    4   19    0    0
     0    0    0]
 [   0    0    0  706   11    0    0    0    0    0    0   17   13    0
     0    0    0]
 [   0    0    0    3  206    0    0    0    0    0    0    0    3    0
     0    1    0]
 [   0    0    0    0    0  431    0    0    0    0    2    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    2    1    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    1    4    0    1    0    0    0    0  832   36    0    0
     0    1    0]
 [   0    0    3    0    0    1    0    0    0    0    7 2187   12    0
     0    0    0]
 [   0    0    0    1    2    0    0    0    0    0    2    2  523    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1135    3    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    81  263    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.19241192411924

F1 scores:
[       nan 1.         0.98383918 0.96250852 0.94063927 0.98853211
 0.99544073 1.         0.99883856 0.90909091 0.96631823 0.9778672
 0.9640553  1.         0.96308867 0.8538961  0.98245614]

Kappa:
0.9679448173372791
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa92a5cceb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.738, val_acc:0.424]
Epoch [2/120    avg_loss:2.529, val_acc:0.483]
Epoch [3/120    avg_loss:2.309, val_acc:0.508]
Epoch [4/120    avg_loss:2.063, val_acc:0.554]
Epoch [5/120    avg_loss:1.891, val_acc:0.605]
Epoch [6/120    avg_loss:1.703, val_acc:0.674]
Epoch [7/120    avg_loss:1.430, val_acc:0.719]
Epoch [8/120    avg_loss:1.226, val_acc:0.666]
Epoch [9/120    avg_loss:1.097, val_acc:0.748]
Epoch [10/120    avg_loss:0.917, val_acc:0.793]
Epoch [11/120    avg_loss:0.800, val_acc:0.816]
Epoch [12/120    avg_loss:0.648, val_acc:0.813]
Epoch [13/120    avg_loss:0.641, val_acc:0.653]
Epoch [14/120    avg_loss:0.551, val_acc:0.820]
Epoch [15/120    avg_loss:0.504, val_acc:0.804]
Epoch [16/120    avg_loss:0.438, val_acc:0.830]
Epoch [17/120    avg_loss:0.447, val_acc:0.856]
Epoch [18/120    avg_loss:0.313, val_acc:0.850]
Epoch [19/120    avg_loss:0.311, val_acc:0.889]
Epoch [20/120    avg_loss:0.272, val_acc:0.791]
Epoch [21/120    avg_loss:0.757, val_acc:0.427]
Epoch [22/120    avg_loss:0.665, val_acc:0.836]
Epoch [23/120    avg_loss:0.545, val_acc:0.836]
Epoch [24/120    avg_loss:0.297, val_acc:0.886]
Epoch [25/120    avg_loss:0.262, val_acc:0.883]
Epoch [26/120    avg_loss:0.251, val_acc:0.887]
Epoch [27/120    avg_loss:0.198, val_acc:0.903]
Epoch [28/120    avg_loss:0.169, val_acc:0.890]
Epoch [29/120    avg_loss:0.172, val_acc:0.921]
Epoch [30/120    avg_loss:0.148, val_acc:0.902]
Epoch [31/120    avg_loss:0.203, val_acc:0.915]
Epoch [32/120    avg_loss:0.119, val_acc:0.918]
Epoch [33/120    avg_loss:0.107, val_acc:0.930]
Epoch [34/120    avg_loss:0.126, val_acc:0.909]
Epoch [35/120    avg_loss:0.128, val_acc:0.914]
Epoch [36/120    avg_loss:0.142, val_acc:0.920]
Epoch [37/120    avg_loss:0.100, val_acc:0.926]
Epoch [38/120    avg_loss:0.104, val_acc:0.902]
Epoch [39/120    avg_loss:0.073, val_acc:0.938]
Epoch [40/120    avg_loss:0.064, val_acc:0.946]
Epoch [41/120    avg_loss:0.064, val_acc:0.957]
Epoch [42/120    avg_loss:0.056, val_acc:0.935]
Epoch [43/120    avg_loss:0.066, val_acc:0.955]
Epoch [44/120    avg_loss:0.074, val_acc:0.925]
Epoch [45/120    avg_loss:0.063, val_acc:0.959]
Epoch [46/120    avg_loss:0.084, val_acc:0.951]
Epoch [47/120    avg_loss:0.047, val_acc:0.941]
Epoch [48/120    avg_loss:0.041, val_acc:0.951]
Epoch [49/120    avg_loss:0.041, val_acc:0.957]
Epoch [50/120    avg_loss:0.035, val_acc:0.962]
Epoch [51/120    avg_loss:0.029, val_acc:0.957]
Epoch [52/120    avg_loss:0.060, val_acc:0.950]
Epoch [53/120    avg_loss:0.069, val_acc:0.933]
Epoch [54/120    avg_loss:0.095, val_acc:0.937]
Epoch [55/120    avg_loss:0.046, val_acc:0.966]
Epoch [56/120    avg_loss:0.100, val_acc:0.939]
Epoch [57/120    avg_loss:0.065, val_acc:0.939]
Epoch [58/120    avg_loss:0.052, val_acc:0.960]
Epoch [59/120    avg_loss:0.030, val_acc:0.970]
Epoch [60/120    avg_loss:0.034, val_acc:0.942]
Epoch [61/120    avg_loss:0.040, val_acc:0.945]
Epoch [62/120    avg_loss:0.109, val_acc:0.945]
Epoch [63/120    avg_loss:0.051, val_acc:0.969]
Epoch [64/120    avg_loss:0.029, val_acc:0.968]
Epoch [65/120    avg_loss:0.023, val_acc:0.935]
Epoch [66/120    avg_loss:0.031, val_acc:0.965]
Epoch [67/120    avg_loss:0.023, val_acc:0.958]
Epoch [68/120    avg_loss:0.036, val_acc:0.960]
Epoch [69/120    avg_loss:0.022, val_acc:0.968]
Epoch [70/120    avg_loss:0.020, val_acc:0.968]
Epoch [71/120    avg_loss:0.022, val_acc:0.968]
Epoch [72/120    avg_loss:0.026, val_acc:0.954]
Epoch [73/120    avg_loss:0.027, val_acc:0.964]
Epoch [74/120    avg_loss:0.015, val_acc:0.968]
Epoch [75/120    avg_loss:0.016, val_acc:0.969]
Epoch [76/120    avg_loss:0.013, val_acc:0.970]
Epoch [77/120    avg_loss:0.015, val_acc:0.974]
Epoch [78/120    avg_loss:0.013, val_acc:0.973]
Epoch [79/120    avg_loss:0.009, val_acc:0.974]
Epoch [80/120    avg_loss:0.012, val_acc:0.974]
Epoch [81/120    avg_loss:0.012, val_acc:0.971]
Epoch [82/120    avg_loss:0.009, val_acc:0.971]
Epoch [83/120    avg_loss:0.009, val_acc:0.973]
Epoch [84/120    avg_loss:0.010, val_acc:0.974]
Epoch [85/120    avg_loss:0.010, val_acc:0.974]
Epoch [86/120    avg_loss:0.009, val_acc:0.973]
Epoch [87/120    avg_loss:0.008, val_acc:0.973]
Epoch [88/120    avg_loss:0.008, val_acc:0.974]
Epoch [89/120    avg_loss:0.007, val_acc:0.974]
Epoch [90/120    avg_loss:0.008, val_acc:0.976]
Epoch [91/120    avg_loss:0.011, val_acc:0.974]
Epoch [92/120    avg_loss:0.008, val_acc:0.973]
Epoch [93/120    avg_loss:0.010, val_acc:0.974]
Epoch [94/120    avg_loss:0.008, val_acc:0.974]
Epoch [95/120    avg_loss:0.008, val_acc:0.975]
Epoch [96/120    avg_loss:0.008, val_acc:0.975]
Epoch [97/120    avg_loss:0.009, val_acc:0.975]
Epoch [98/120    avg_loss:0.007, val_acc:0.975]
Epoch [99/120    avg_loss:0.011, val_acc:0.974]
Epoch [100/120    avg_loss:0.007, val_acc:0.975]
Epoch [101/120    avg_loss:0.009, val_acc:0.975]
Epoch [102/120    avg_loss:0.006, val_acc:0.976]
Epoch [103/120    avg_loss:0.009, val_acc:0.975]
Epoch [104/120    avg_loss:0.009, val_acc:0.975]
Epoch [105/120    avg_loss:0.008, val_acc:0.975]
Epoch [106/120    avg_loss:0.010, val_acc:0.974]
Epoch [107/120    avg_loss:0.008, val_acc:0.974]
Epoch [108/120    avg_loss:0.008, val_acc:0.974]
Epoch [109/120    avg_loss:0.007, val_acc:0.976]
Epoch [110/120    avg_loss:0.009, val_acc:0.975]
Epoch [111/120    avg_loss:0.008, val_acc:0.975]
Epoch [112/120    avg_loss:0.007, val_acc:0.975]
Epoch [113/120    avg_loss:0.010, val_acc:0.976]
Epoch [114/120    avg_loss:0.008, val_acc:0.975]
Epoch [115/120    avg_loss:0.006, val_acc:0.975]
Epoch [116/120    avg_loss:0.009, val_acc:0.975]
Epoch [117/120    avg_loss:0.010, val_acc:0.975]
Epoch [118/120    avg_loss:0.008, val_acc:0.975]
Epoch [119/120    avg_loss:0.008, val_acc:0.975]
Epoch [120/120    avg_loss:0.009, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1247   10   11    0    0    0    0    0    2   14    1    0
     0    0    0]
 [   0    0    0  732    6    0    0    0    0    4    1    0    4    0
     0    0    0]
 [   0    0    0    7  204    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  430    5    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    1  653    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  845   25    0    0
     0    0    0]
 [   0    0   19    0    0    0    0    0    0    0   18 2171    1    0
     0    1    0]
 [   0    0    0    2    0    0    0    0    0    0    0    0  530    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1133    5    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
   107  233    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.13821138211382

F1 scores:
[       nan 1.         0.97574335 0.97730307 0.94009217 0.99307159
 0.98715042 1.         1.         0.9        0.97070649 0.98213074
 0.98880597 1.         0.95170097 0.79522184 0.98823529]

Kappa:
0.9673435121793856
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb3dd5de48>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.750, val_acc:0.341]
Epoch [2/120    avg_loss:2.558, val_acc:0.479]
Epoch [3/120    avg_loss:2.355, val_acc:0.488]
Epoch [4/120    avg_loss:2.143, val_acc:0.533]
Epoch [5/120    avg_loss:1.887, val_acc:0.599]
Epoch [6/120    avg_loss:1.709, val_acc:0.585]
Epoch [7/120    avg_loss:1.571, val_acc:0.625]
Epoch [8/120    avg_loss:1.390, val_acc:0.652]
Epoch [9/120    avg_loss:1.189, val_acc:0.697]
Epoch [10/120    avg_loss:1.065, val_acc:0.706]
Epoch [11/120    avg_loss:0.931, val_acc:0.693]
Epoch [12/120    avg_loss:0.826, val_acc:0.779]
Epoch [13/120    avg_loss:0.743, val_acc:0.773]
Epoch [14/120    avg_loss:0.654, val_acc:0.784]
Epoch [15/120    avg_loss:0.667, val_acc:0.805]
Epoch [16/120    avg_loss:0.537, val_acc:0.805]
Epoch [17/120    avg_loss:0.519, val_acc:0.807]
Epoch [18/120    avg_loss:0.461, val_acc:0.837]
Epoch [19/120    avg_loss:0.343, val_acc:0.885]
Epoch [20/120    avg_loss:0.297, val_acc:0.886]
Epoch [21/120    avg_loss:0.303, val_acc:0.876]
Epoch [22/120    avg_loss:0.272, val_acc:0.912]
Epoch [23/120    avg_loss:0.202, val_acc:0.905]
Epoch [24/120    avg_loss:0.171, val_acc:0.851]
Epoch [25/120    avg_loss:0.205, val_acc:0.905]
Epoch [26/120    avg_loss:0.169, val_acc:0.861]
Epoch [27/120    avg_loss:0.155, val_acc:0.911]
Epoch [28/120    avg_loss:0.142, val_acc:0.892]
Epoch [29/120    avg_loss:0.130, val_acc:0.923]
Epoch [30/120    avg_loss:0.097, val_acc:0.940]
Epoch [31/120    avg_loss:0.089, val_acc:0.954]
Epoch [32/120    avg_loss:0.099, val_acc:0.933]
Epoch [33/120    avg_loss:0.098, val_acc:0.939]
Epoch [34/120    avg_loss:0.167, val_acc:0.934]
Epoch [35/120    avg_loss:0.099, val_acc:0.949]
Epoch [36/120    avg_loss:0.088, val_acc:0.944]
Epoch [37/120    avg_loss:0.072, val_acc:0.952]
Epoch [38/120    avg_loss:0.066, val_acc:0.942]
Epoch [39/120    avg_loss:0.065, val_acc:0.957]
Epoch [40/120    avg_loss:0.073, val_acc:0.951]
Epoch [41/120    avg_loss:0.072, val_acc:0.966]
Epoch [42/120    avg_loss:0.040, val_acc:0.958]
Epoch [43/120    avg_loss:0.032, val_acc:0.971]
Epoch [44/120    avg_loss:0.026, val_acc:0.962]
Epoch [45/120    avg_loss:0.065, val_acc:0.939]
Epoch [46/120    avg_loss:0.058, val_acc:0.927]
Epoch [47/120    avg_loss:0.067, val_acc:0.950]
Epoch [48/120    avg_loss:0.052, val_acc:0.959]
Epoch [49/120    avg_loss:0.033, val_acc:0.963]
Epoch [50/120    avg_loss:0.047, val_acc:0.949]
Epoch [51/120    avg_loss:0.038, val_acc:0.966]
Epoch [52/120    avg_loss:0.128, val_acc:0.787]
Epoch [53/120    avg_loss:0.073, val_acc:0.957]
Epoch [54/120    avg_loss:0.237, val_acc:0.892]
Epoch [55/120    avg_loss:0.130, val_acc:0.958]
Epoch [56/120    avg_loss:0.041, val_acc:0.957]
Epoch [57/120    avg_loss:0.032, val_acc:0.969]
Epoch [58/120    avg_loss:0.023, val_acc:0.968]
Epoch [59/120    avg_loss:0.022, val_acc:0.968]
Epoch [60/120    avg_loss:0.030, val_acc:0.968]
Epoch [61/120    avg_loss:0.020, val_acc:0.970]
Epoch [62/120    avg_loss:0.019, val_acc:0.971]
Epoch [63/120    avg_loss:0.026, val_acc:0.971]
Epoch [64/120    avg_loss:0.019, val_acc:0.969]
Epoch [65/120    avg_loss:0.020, val_acc:0.972]
Epoch [66/120    avg_loss:0.017, val_acc:0.972]
Epoch [67/120    avg_loss:0.019, val_acc:0.971]
Epoch [68/120    avg_loss:0.024, val_acc:0.972]
Epoch [69/120    avg_loss:0.018, val_acc:0.970]
Epoch [70/120    avg_loss:0.018, val_acc:0.971]
Epoch [71/120    avg_loss:0.019, val_acc:0.971]
Epoch [72/120    avg_loss:0.017, val_acc:0.969]
Epoch [73/120    avg_loss:0.014, val_acc:0.969]
Epoch [74/120    avg_loss:0.017, val_acc:0.969]
Epoch [75/120    avg_loss:0.016, val_acc:0.970]
Epoch [76/120    avg_loss:0.017, val_acc:0.971]
Epoch [77/120    avg_loss:0.013, val_acc:0.971]
Epoch [78/120    avg_loss:0.017, val_acc:0.970]
Epoch [79/120    avg_loss:0.015, val_acc:0.972]
Epoch [80/120    avg_loss:0.016, val_acc:0.971]
Epoch [81/120    avg_loss:0.016, val_acc:0.971]
Epoch [82/120    avg_loss:0.016, val_acc:0.972]
Epoch [83/120    avg_loss:0.014, val_acc:0.971]
Epoch [84/120    avg_loss:0.018, val_acc:0.975]
Epoch [85/120    avg_loss:0.016, val_acc:0.973]
Epoch [86/120    avg_loss:0.024, val_acc:0.970]
Epoch [87/120    avg_loss:0.015, val_acc:0.972]
Epoch [88/120    avg_loss:0.020, val_acc:0.968]
Epoch [89/120    avg_loss:0.017, val_acc:0.974]
Epoch [90/120    avg_loss:0.017, val_acc:0.972]
Epoch [91/120    avg_loss:0.015, val_acc:0.974]
Epoch [92/120    avg_loss:0.015, val_acc:0.975]
Epoch [93/120    avg_loss:0.014, val_acc:0.973]
Epoch [94/120    avg_loss:0.021, val_acc:0.974]
Epoch [95/120    avg_loss:0.016, val_acc:0.970]
Epoch [96/120    avg_loss:0.013, val_acc:0.973]
Epoch [97/120    avg_loss:0.014, val_acc:0.975]
Epoch [98/120    avg_loss:0.013, val_acc:0.974]
Epoch [99/120    avg_loss:0.014, val_acc:0.973]
Epoch [100/120    avg_loss:0.013, val_acc:0.970]
Epoch [101/120    avg_loss:0.014, val_acc:0.972]
Epoch [102/120    avg_loss:0.011, val_acc:0.973]
Epoch [103/120    avg_loss:0.014, val_acc:0.975]
Epoch [104/120    avg_loss:0.013, val_acc:0.975]
Epoch [105/120    avg_loss:0.012, val_acc:0.973]
Epoch [106/120    avg_loss:0.015, val_acc:0.975]
Epoch [107/120    avg_loss:0.011, val_acc:0.975]
Epoch [108/120    avg_loss:0.014, val_acc:0.974]
Epoch [109/120    avg_loss:0.012, val_acc:0.974]
Epoch [110/120    avg_loss:0.014, val_acc:0.975]
Epoch [111/120    avg_loss:0.012, val_acc:0.974]
Epoch [112/120    avg_loss:0.011, val_acc:0.974]
Epoch [113/120    avg_loss:0.011, val_acc:0.973]
Epoch [114/120    avg_loss:0.012, val_acc:0.972]
Epoch [115/120    avg_loss:0.011, val_acc:0.974]
Epoch [116/120    avg_loss:0.012, val_acc:0.976]
Epoch [117/120    avg_loss:0.010, val_acc:0.973]
Epoch [118/120    avg_loss:0.014, val_acc:0.975]
Epoch [119/120    avg_loss:0.010, val_acc:0.975]
Epoch [120/120    avg_loss:0.013, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    1 1249    7    4    8    0    0    0    0    2   14    0    0
     0    0    0]
 [   0    0    6  717    6    4    0    0    0    0    3    7    4    0
     0    0    0]
 [   0    0    0    4  205    3    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    3    0    0    0    0  849   13    1    0
     5    0    0]
 [   0    0    6    0    0    1    2    0    0    0   31 2141   25    0
     3    0    1]
 [   0    0    0    1    0    0    0    0    0    0    1    0  530    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    66  280    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.20325203252033

F1 scores:
[       nan 0.95348837 0.97960784 0.97154472 0.95794393 0.9751693
 0.99696049 1.         0.99649942 1.         0.96422487 0.97606565
 0.96892139 1.         0.96245734 0.87774295 0.98245614]

Kappa:
0.9681252090483666
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f28b2453e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.760, val_acc:0.297]
Epoch [2/120    avg_loss:2.594, val_acc:0.460]
Epoch [3/120    avg_loss:2.380, val_acc:0.482]
Epoch [4/120    avg_loss:2.143, val_acc:0.556]
Epoch [5/120    avg_loss:1.891, val_acc:0.597]
Epoch [6/120    avg_loss:1.675, val_acc:0.606]
Epoch [7/120    avg_loss:1.443, val_acc:0.648]
Epoch [8/120    avg_loss:1.207, val_acc:0.658]
Epoch [9/120    avg_loss:1.064, val_acc:0.736]
Epoch [10/120    avg_loss:0.963, val_acc:0.676]
Epoch [11/120    avg_loss:0.980, val_acc:0.716]
Epoch [12/120    avg_loss:0.821, val_acc:0.774]
Epoch [13/120    avg_loss:0.679, val_acc:0.773]
Epoch [14/120    avg_loss:0.661, val_acc:0.797]
Epoch [15/120    avg_loss:0.524, val_acc:0.818]
Epoch [16/120    avg_loss:0.539, val_acc:0.777]
Epoch [17/120    avg_loss:0.481, val_acc:0.805]
Epoch [18/120    avg_loss:0.426, val_acc:0.816]
Epoch [19/120    avg_loss:0.413, val_acc:0.873]
Epoch [20/120    avg_loss:0.363, val_acc:0.834]
Epoch [21/120    avg_loss:0.344, val_acc:0.878]
Epoch [22/120    avg_loss:0.302, val_acc:0.868]
Epoch [23/120    avg_loss:0.308, val_acc:0.870]
Epoch [24/120    avg_loss:0.230, val_acc:0.889]
Epoch [25/120    avg_loss:0.190, val_acc:0.913]
Epoch [26/120    avg_loss:0.200, val_acc:0.839]
Epoch [27/120    avg_loss:0.295, val_acc:0.844]
Epoch [28/120    avg_loss:0.211, val_acc:0.920]
Epoch [29/120    avg_loss:0.152, val_acc:0.940]
Epoch [30/120    avg_loss:0.135, val_acc:0.909]
Epoch [31/120    avg_loss:0.142, val_acc:0.940]
Epoch [32/120    avg_loss:0.127, val_acc:0.929]
Epoch [33/120    avg_loss:0.128, val_acc:0.932]
Epoch [34/120    avg_loss:0.090, val_acc:0.912]
Epoch [35/120    avg_loss:0.100, val_acc:0.950]
Epoch [36/120    avg_loss:0.141, val_acc:0.933]
Epoch [37/120    avg_loss:0.110, val_acc:0.956]
Epoch [38/120    avg_loss:0.108, val_acc:0.903]
Epoch [39/120    avg_loss:0.141, val_acc:0.948]
Epoch [40/120    avg_loss:0.081, val_acc:0.908]
Epoch [41/120    avg_loss:0.079, val_acc:0.951]
Epoch [42/120    avg_loss:0.050, val_acc:0.954]
Epoch [43/120    avg_loss:0.052, val_acc:0.934]
Epoch [44/120    avg_loss:0.058, val_acc:0.943]
Epoch [45/120    avg_loss:0.047, val_acc:0.946]
Epoch [46/120    avg_loss:0.057, val_acc:0.961]
Epoch [47/120    avg_loss:0.046, val_acc:0.933]
Epoch [48/120    avg_loss:0.037, val_acc:0.955]
Epoch [49/120    avg_loss:0.059, val_acc:0.954]
Epoch [50/120    avg_loss:0.068, val_acc:0.945]
Epoch [51/120    avg_loss:0.046, val_acc:0.960]
Epoch [52/120    avg_loss:0.044, val_acc:0.962]
Epoch [53/120    avg_loss:0.038, val_acc:0.956]
Epoch [54/120    avg_loss:0.052, val_acc:0.946]
Epoch [55/120    avg_loss:0.043, val_acc:0.955]
Epoch [56/120    avg_loss:0.043, val_acc:0.962]
Epoch [57/120    avg_loss:0.049, val_acc:0.893]
Epoch [58/120    avg_loss:0.113, val_acc:0.960]
Epoch [59/120    avg_loss:0.046, val_acc:0.952]
Epoch [60/120    avg_loss:0.034, val_acc:0.966]
Epoch [61/120    avg_loss:0.038, val_acc:0.961]
Epoch [62/120    avg_loss:0.035, val_acc:0.927]
Epoch [63/120    avg_loss:0.042, val_acc:0.947]
Epoch [64/120    avg_loss:0.046, val_acc:0.950]
Epoch [65/120    avg_loss:0.053, val_acc:0.961]
Epoch [66/120    avg_loss:0.103, val_acc:0.804]
Epoch [67/120    avg_loss:0.092, val_acc:0.955]
Epoch [68/120    avg_loss:0.034, val_acc:0.957]
Epoch [69/120    avg_loss:0.024, val_acc:0.965]
Epoch [70/120    avg_loss:0.018, val_acc:0.952]
Epoch [71/120    avg_loss:0.014, val_acc:0.969]
Epoch [72/120    avg_loss:0.018, val_acc:0.968]
Epoch [73/120    avg_loss:0.018, val_acc:0.966]
Epoch [74/120    avg_loss:0.017, val_acc:0.974]
Epoch [75/120    avg_loss:0.010, val_acc:0.967]
Epoch [76/120    avg_loss:0.016, val_acc:0.975]
Epoch [77/120    avg_loss:0.010, val_acc:0.968]
Epoch [78/120    avg_loss:0.009, val_acc:0.971]
Epoch [79/120    avg_loss:0.010, val_acc:0.976]
Epoch [80/120    avg_loss:0.012, val_acc:0.967]
Epoch [81/120    avg_loss:0.010, val_acc:0.973]
Epoch [82/120    avg_loss:0.016, val_acc:0.965]
Epoch [83/120    avg_loss:0.033, val_acc:0.960]
Epoch [84/120    avg_loss:0.015, val_acc:0.964]
Epoch [85/120    avg_loss:0.025, val_acc:0.971]
Epoch [86/120    avg_loss:0.008, val_acc:0.973]
Epoch [87/120    avg_loss:0.009, val_acc:0.965]
Epoch [88/120    avg_loss:0.012, val_acc:0.948]
Epoch [89/120    avg_loss:0.048, val_acc:0.960]
Epoch [90/120    avg_loss:0.030, val_acc:0.940]
Epoch [91/120    avg_loss:0.036, val_acc:0.965]
Epoch [92/120    avg_loss:0.031, val_acc:0.954]
Epoch [93/120    avg_loss:0.027, val_acc:0.960]
Epoch [94/120    avg_loss:0.023, val_acc:0.958]
Epoch [95/120    avg_loss:0.017, val_acc:0.962]
Epoch [96/120    avg_loss:0.014, val_acc:0.966]
Epoch [97/120    avg_loss:0.013, val_acc:0.965]
Epoch [98/120    avg_loss:0.014, val_acc:0.964]
Epoch [99/120    avg_loss:0.014, val_acc:0.964]
Epoch [100/120    avg_loss:0.015, val_acc:0.968]
Epoch [101/120    avg_loss:0.013, val_acc:0.968]
Epoch [102/120    avg_loss:0.013, val_acc:0.968]
Epoch [103/120    avg_loss:0.010, val_acc:0.969]
Epoch [104/120    avg_loss:0.010, val_acc:0.968]
Epoch [105/120    avg_loss:0.010, val_acc:0.969]
Epoch [106/120    avg_loss:0.007, val_acc:0.969]
Epoch [107/120    avg_loss:0.007, val_acc:0.969]
Epoch [108/120    avg_loss:0.009, val_acc:0.969]
Epoch [109/120    avg_loss:0.010, val_acc:0.969]
Epoch [110/120    avg_loss:0.007, val_acc:0.969]
Epoch [111/120    avg_loss:0.009, val_acc:0.969]
Epoch [112/120    avg_loss:0.010, val_acc:0.969]
Epoch [113/120    avg_loss:0.009, val_acc:0.969]
Epoch [114/120    avg_loss:0.007, val_acc:0.969]
Epoch [115/120    avg_loss:0.009, val_acc:0.969]
Epoch [116/120    avg_loss:0.010, val_acc:0.970]
Epoch [117/120    avg_loss:0.008, val_acc:0.970]
Epoch [118/120    avg_loss:0.008, val_acc:0.970]
Epoch [119/120    avg_loss:0.007, val_acc:0.970]
Epoch [120/120    avg_loss:0.007, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    0    2    0    0
     0    0    0]
 [   0    0 1265    0    7    1    0    0    0    0    2   10    0    0
     0    0    0]
 [   0    0    3  736    3    0    0    0    0    3    1    0    1    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   12    0    0    6    1    0    0    0  832   24    0    0
     0    0    0]
 [   0    0   13    0    0    1    1    1    2    0   16 2166   10    0
     0    0    0]
 [   0    0    1    5    0    4    0    0    0    0    2    1  509    0
     5    3    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1116   23    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    62  284    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.38753387533875

F1 scores:
[       nan 0.975      0.98100039 0.98924731 0.97471264 0.97945205
 0.99619772 0.98039216 0.99767981 0.92307692 0.96296296 0.98120045
 0.96401515 1.         0.95876289 0.86453577 0.97076023]

Kappa:
0.9702072205751938
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4cc80c1e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.750, val_acc:0.270]
Epoch [2/120    avg_loss:2.558, val_acc:0.413]
Epoch [3/120    avg_loss:2.332, val_acc:0.469]
Epoch [4/120    avg_loss:2.116, val_acc:0.514]
Epoch [5/120    avg_loss:1.877, val_acc:0.633]
Epoch [6/120    avg_loss:1.639, val_acc:0.634]
Epoch [7/120    avg_loss:1.380, val_acc:0.696]
Epoch [8/120    avg_loss:1.275, val_acc:0.668]
Epoch [9/120    avg_loss:1.068, val_acc:0.714]
Epoch [10/120    avg_loss:0.868, val_acc:0.735]
Epoch [11/120    avg_loss:0.784, val_acc:0.769]
Epoch [12/120    avg_loss:0.856, val_acc:0.648]
Epoch [13/120    avg_loss:0.754, val_acc:0.802]
Epoch [14/120    avg_loss:0.609, val_acc:0.818]
Epoch [15/120    avg_loss:0.570, val_acc:0.799]
Epoch [16/120    avg_loss:0.434, val_acc:0.872]
Epoch [17/120    avg_loss:0.392, val_acc:0.855]
Epoch [18/120    avg_loss:0.459, val_acc:0.847]
Epoch [19/120    avg_loss:0.364, val_acc:0.845]
Epoch [20/120    avg_loss:0.345, val_acc:0.908]
Epoch [21/120    avg_loss:0.295, val_acc:0.911]
Epoch [22/120    avg_loss:0.232, val_acc:0.903]
Epoch [23/120    avg_loss:0.327, val_acc:0.868]
Epoch [24/120    avg_loss:0.480, val_acc:0.895]
Epoch [25/120    avg_loss:0.272, val_acc:0.904]
Epoch [26/120    avg_loss:0.254, val_acc:0.891]
Epoch [27/120    avg_loss:0.211, val_acc:0.935]
Epoch [28/120    avg_loss:0.162, val_acc:0.924]
Epoch [29/120    avg_loss:0.189, val_acc:0.890]
Epoch [30/120    avg_loss:0.208, val_acc:0.862]
Epoch [31/120    avg_loss:0.163, val_acc:0.936]
Epoch [32/120    avg_loss:0.118, val_acc:0.914]
Epoch [33/120    avg_loss:0.099, val_acc:0.961]
Epoch [34/120    avg_loss:0.090, val_acc:0.936]
Epoch [35/120    avg_loss:0.187, val_acc:0.925]
Epoch [36/120    avg_loss:0.135, val_acc:0.963]
Epoch [37/120    avg_loss:0.575, val_acc:0.893]
Epoch [38/120    avg_loss:0.193, val_acc:0.918]
Epoch [39/120    avg_loss:0.169, val_acc:0.941]
Epoch [40/120    avg_loss:0.114, val_acc:0.943]
Epoch [41/120    avg_loss:0.185, val_acc:0.951]
Epoch [42/120    avg_loss:0.152, val_acc:0.954]
Epoch [43/120    avg_loss:0.081, val_acc:0.956]
Epoch [44/120    avg_loss:0.079, val_acc:0.952]
Epoch [45/120    avg_loss:0.054, val_acc:0.967]
Epoch [46/120    avg_loss:0.068, val_acc:0.954]
Epoch [47/120    avg_loss:0.061, val_acc:0.938]
Epoch [48/120    avg_loss:0.064, val_acc:0.968]
Epoch [49/120    avg_loss:0.051, val_acc:0.968]
Epoch [50/120    avg_loss:0.055, val_acc:0.969]
Epoch [51/120    avg_loss:0.047, val_acc:0.975]
Epoch [52/120    avg_loss:0.041, val_acc:0.961]
Epoch [53/120    avg_loss:0.095, val_acc:0.928]
Epoch [54/120    avg_loss:0.112, val_acc:0.967]
Epoch [55/120    avg_loss:0.080, val_acc:0.963]
Epoch [56/120    avg_loss:0.089, val_acc:0.972]
Epoch [57/120    avg_loss:0.092, val_acc:0.960]
Epoch [58/120    avg_loss:0.034, val_acc:0.975]
Epoch [59/120    avg_loss:0.024, val_acc:0.975]
Epoch [60/120    avg_loss:0.073, val_acc:0.910]
Epoch [61/120    avg_loss:0.073, val_acc:0.974]
Epoch [62/120    avg_loss:0.028, val_acc:0.974]
Epoch [63/120    avg_loss:0.032, val_acc:0.974]
Epoch [64/120    avg_loss:0.037, val_acc:0.964]
Epoch [65/120    avg_loss:0.048, val_acc:0.977]
Epoch [66/120    avg_loss:0.030, val_acc:0.966]
Epoch [67/120    avg_loss:0.036, val_acc:0.971]
Epoch [68/120    avg_loss:0.045, val_acc:0.974]
Epoch [69/120    avg_loss:0.018, val_acc:0.978]
Epoch [70/120    avg_loss:0.021, val_acc:0.980]
Epoch [71/120    avg_loss:0.018, val_acc:0.971]
Epoch [72/120    avg_loss:0.036, val_acc:0.971]
Epoch [73/120    avg_loss:0.047, val_acc:0.970]
Epoch [74/120    avg_loss:0.023, val_acc:0.973]
Epoch [75/120    avg_loss:0.045, val_acc:0.975]
Epoch [76/120    avg_loss:0.030, val_acc:0.975]
Epoch [77/120    avg_loss:0.017, val_acc:0.982]
Epoch [78/120    avg_loss:0.010, val_acc:0.970]
Epoch [79/120    avg_loss:0.029, val_acc:0.973]
Epoch [80/120    avg_loss:0.018, val_acc:0.981]
Epoch [81/120    avg_loss:0.011, val_acc:0.981]
Epoch [82/120    avg_loss:0.020, val_acc:0.978]
Epoch [83/120    avg_loss:0.010, val_acc:0.981]
Epoch [84/120    avg_loss:0.007, val_acc:0.982]
Epoch [85/120    avg_loss:0.010, val_acc:0.980]
Epoch [86/120    avg_loss:0.009, val_acc:0.983]
Epoch [87/120    avg_loss:0.013, val_acc:0.977]
Epoch [88/120    avg_loss:0.009, val_acc:0.976]
Epoch [89/120    avg_loss:0.012, val_acc:0.978]
Epoch [90/120    avg_loss:0.015, val_acc:0.984]
Epoch [91/120    avg_loss:0.010, val_acc:0.980]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.009, val_acc:0.973]
Epoch [94/120    avg_loss:0.028, val_acc:0.973]
Epoch [95/120    avg_loss:0.039, val_acc:0.974]
Epoch [96/120    avg_loss:0.010, val_acc:0.978]
Epoch [97/120    avg_loss:0.011, val_acc:0.983]
Epoch [98/120    avg_loss:0.019, val_acc:0.964]
Epoch [99/120    avg_loss:0.016, val_acc:0.973]
Epoch [100/120    avg_loss:0.020, val_acc:0.977]
Epoch [101/120    avg_loss:0.011, val_acc:0.974]
Epoch [102/120    avg_loss:0.008, val_acc:0.970]
Epoch [103/120    avg_loss:0.010, val_acc:0.981]
Epoch [104/120    avg_loss:0.012, val_acc:0.976]
Epoch [105/120    avg_loss:0.013, val_acc:0.981]
Epoch [106/120    avg_loss:0.007, val_acc:0.983]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.007, val_acc:0.983]
Epoch [109/120    avg_loss:0.005, val_acc:0.983]
Epoch [110/120    avg_loss:0.007, val_acc:0.983]
Epoch [111/120    avg_loss:0.005, val_acc:0.983]
Epoch [112/120    avg_loss:0.006, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.005, val_acc:0.983]
Epoch [115/120    avg_loss:0.005, val_acc:0.984]
Epoch [116/120    avg_loss:0.004, val_acc:0.984]
Epoch [117/120    avg_loss:0.005, val_acc:0.984]
Epoch [118/120    avg_loss:0.004, val_acc:0.984]
Epoch [119/120    avg_loss:0.004, val_acc:0.984]
Epoch [120/120    avg_loss:0.005, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1262    3    2    0    0    0    0    0    1   16    1    0
     0    0    0]
 [   0    0    1  729    0    5    0    0    0    0    0    3    8    1
     0    0    0]
 [   0    0    0    0  210    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    1    0    0    0    0  839   25    2    0
     0    0    0]
 [   0    0    4    0    0    2    1    4    0    0    4 2180    8    0
     6    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    3  525    0
     2    4    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    1    5    0    0    0    0    0    0    0
    81  260    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.63685636856368

F1 scores:
[       nan 1.         0.9859375  0.98580122 0.98823529 0.98976109
 0.9939302  0.92592593 1.         1.         0.97614892 0.98242452
 0.96952909 0.99730458 0.95847458 0.83870968 0.98795181]

Kappa:
0.9730324463901784
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:00:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7e14f88eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.749, val_acc:0.352]
Epoch [2/120    avg_loss:2.543, val_acc:0.435]
Epoch [3/120    avg_loss:2.315, val_acc:0.447]
Epoch [4/120    avg_loss:2.109, val_acc:0.494]
Epoch [5/120    avg_loss:1.850, val_acc:0.603]
Epoch [6/120    avg_loss:1.635, val_acc:0.648]
Epoch [7/120    avg_loss:1.387, val_acc:0.724]
Epoch [8/120    avg_loss:1.211, val_acc:0.655]
Epoch [9/120    avg_loss:1.048, val_acc:0.716]
Epoch [10/120    avg_loss:0.902, val_acc:0.733]
Epoch [11/120    avg_loss:0.855, val_acc:0.790]
Epoch [12/120    avg_loss:0.765, val_acc:0.786]
Epoch [13/120    avg_loss:0.625, val_acc:0.747]
Epoch [14/120    avg_loss:0.627, val_acc:0.820]
Epoch [15/120    avg_loss:0.518, val_acc:0.848]
Epoch [16/120    avg_loss:0.519, val_acc:0.816]
Epoch [17/120    avg_loss:0.456, val_acc:0.812]
Epoch [18/120    avg_loss:0.380, val_acc:0.848]
Epoch [19/120    avg_loss:0.356, val_acc:0.842]
Epoch [20/120    avg_loss:0.348, val_acc:0.875]
Epoch [21/120    avg_loss:0.812, val_acc:0.755]
Epoch [22/120    avg_loss:2.081, val_acc:0.235]
Epoch [23/120    avg_loss:2.214, val_acc:0.431]
Epoch [24/120    avg_loss:1.994, val_acc:0.481]
Epoch [25/120    avg_loss:1.911, val_acc:0.531]
Epoch [26/120    avg_loss:1.769, val_acc:0.571]
Epoch [27/120    avg_loss:1.635, val_acc:0.559]
Epoch [28/120    avg_loss:1.575, val_acc:0.555]
Epoch [29/120    avg_loss:1.402, val_acc:0.596]
Epoch [30/120    avg_loss:1.351, val_acc:0.608]
Epoch [31/120    avg_loss:1.274, val_acc:0.599]
Epoch [32/120    avg_loss:1.232, val_acc:0.603]
Epoch [33/120    avg_loss:1.150, val_acc:0.648]
Epoch [34/120    avg_loss:1.065, val_acc:0.651]
Epoch [35/120    avg_loss:1.056, val_acc:0.645]
Epoch [36/120    avg_loss:1.024, val_acc:0.654]
Epoch [37/120    avg_loss:1.030, val_acc:0.657]
Epoch [38/120    avg_loss:1.007, val_acc:0.654]
Epoch [39/120    avg_loss:0.976, val_acc:0.657]
Epoch [40/120    avg_loss:0.983, val_acc:0.657]
Epoch [41/120    avg_loss:1.003, val_acc:0.659]
Epoch [42/120    avg_loss:0.978, val_acc:0.666]
Epoch [43/120    avg_loss:1.003, val_acc:0.665]
Epoch [44/120    avg_loss:0.948, val_acc:0.658]
Epoch [45/120    avg_loss:0.983, val_acc:0.666]
Epoch [46/120    avg_loss:0.970, val_acc:0.675]
Epoch [47/120    avg_loss:0.939, val_acc:0.674]
Epoch [48/120    avg_loss:0.920, val_acc:0.675]
Epoch [49/120    avg_loss:0.950, val_acc:0.674]
Epoch [50/120    avg_loss:0.968, val_acc:0.669]
Epoch [51/120    avg_loss:0.946, val_acc:0.670]
Epoch [52/120    avg_loss:0.939, val_acc:0.668]
Epoch [53/120    avg_loss:0.958, val_acc:0.674]
Epoch [54/120    avg_loss:0.940, val_acc:0.672]
Epoch [55/120    avg_loss:0.934, val_acc:0.674]
Epoch [56/120    avg_loss:0.924, val_acc:0.676]
Epoch [57/120    avg_loss:0.941, val_acc:0.674]
Epoch [58/120    avg_loss:0.908, val_acc:0.675]
Epoch [59/120    avg_loss:0.933, val_acc:0.676]
Epoch [60/120    avg_loss:0.916, val_acc:0.677]
Epoch [61/120    avg_loss:0.947, val_acc:0.677]
Epoch [62/120    avg_loss:0.973, val_acc:0.677]
Epoch [63/120    avg_loss:0.937, val_acc:0.677]
Epoch [64/120    avg_loss:0.971, val_acc:0.677]
Epoch [65/120    avg_loss:0.929, val_acc:0.677]
Epoch [66/120    avg_loss:0.929, val_acc:0.677]
Epoch [67/120    avg_loss:0.908, val_acc:0.677]
Epoch [68/120    avg_loss:0.926, val_acc:0.676]
Epoch [69/120    avg_loss:0.908, val_acc:0.677]
Epoch [70/120    avg_loss:0.930, val_acc:0.676]
Epoch [71/120    avg_loss:0.919, val_acc:0.676]
Epoch [72/120    avg_loss:0.944, val_acc:0.676]
Epoch [73/120    avg_loss:0.981, val_acc:0.676]
Epoch [74/120    avg_loss:0.923, val_acc:0.676]
Epoch [75/120    avg_loss:0.934, val_acc:0.676]
Epoch [76/120    avg_loss:0.949, val_acc:0.676]
Epoch [77/120    avg_loss:0.945, val_acc:0.676]
Epoch [78/120    avg_loss:0.926, val_acc:0.676]
Epoch [79/120    avg_loss:0.931, val_acc:0.676]
Epoch [80/120    avg_loss:0.931, val_acc:0.676]
Epoch [81/120    avg_loss:0.943, val_acc:0.676]
Epoch [82/120    avg_loss:0.946, val_acc:0.676]
Epoch [83/120    avg_loss:0.946, val_acc:0.676]
Epoch [84/120    avg_loss:0.953, val_acc:0.676]
Epoch [85/120    avg_loss:0.925, val_acc:0.676]
Epoch [86/120    avg_loss:0.939, val_acc:0.676]
Epoch [87/120    avg_loss:0.930, val_acc:0.676]
Epoch [88/120    avg_loss:0.937, val_acc:0.676]
Epoch [89/120    avg_loss:0.950, val_acc:0.676]
Epoch [90/120    avg_loss:0.942, val_acc:0.676]
Epoch [91/120    avg_loss:0.951, val_acc:0.676]
Epoch [92/120    avg_loss:0.911, val_acc:0.676]
Epoch [93/120    avg_loss:0.921, val_acc:0.676]
Epoch [94/120    avg_loss:0.936, val_acc:0.676]
Epoch [95/120    avg_loss:0.937, val_acc:0.676]
Epoch [96/120    avg_loss:0.938, val_acc:0.676]
Epoch [97/120    avg_loss:0.953, val_acc:0.676]
Epoch [98/120    avg_loss:0.954, val_acc:0.676]
Epoch [99/120    avg_loss:0.926, val_acc:0.676]
Epoch [100/120    avg_loss:0.964, val_acc:0.676]
Epoch [101/120    avg_loss:0.970, val_acc:0.676]
Epoch [102/120    avg_loss:0.925, val_acc:0.676]
Epoch [103/120    avg_loss:0.924, val_acc:0.676]
Epoch [104/120    avg_loss:0.957, val_acc:0.676]
Epoch [105/120    avg_loss:0.930, val_acc:0.676]
Epoch [106/120    avg_loss:0.943, val_acc:0.676]
Epoch [107/120    avg_loss:0.901, val_acc:0.676]
Epoch [108/120    avg_loss:0.908, val_acc:0.676]
Epoch [109/120    avg_loss:0.951, val_acc:0.676]
Epoch [110/120    avg_loss:0.954, val_acc:0.676]
Epoch [111/120    avg_loss:0.955, val_acc:0.676]
Epoch [112/120    avg_loss:0.924, val_acc:0.676]
Epoch [113/120    avg_loss:0.948, val_acc:0.676]
Epoch [114/120    avg_loss:0.934, val_acc:0.676]
Epoch [115/120    avg_loss:0.934, val_acc:0.676]
Epoch [116/120    avg_loss:0.938, val_acc:0.676]
Epoch [117/120    avg_loss:0.963, val_acc:0.676]
Epoch [118/120    avg_loss:0.942, val_acc:0.676]
Epoch [119/120    avg_loss:0.927, val_acc:0.676]
Epoch [120/120    avg_loss:0.935, val_acc:0.676]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   16    2    0    6    1    0    0   16    0    0    0    0    0
     0    0    0]
 [   0    0  683  112   33    8    2    0    0    0  165  241   27    6
     2    6    0]
 [   0    0   62  305   31   45    5    0    0    0  218   63    3   15
     0    0    0]
 [   0    0   14   23  158    0    0    0    0    0   18    0    0    0
     0    0    0]
 [   0    0    2    6    0  355   36    0    5    0    9    0    0    3
    19    0    0]
 [   0    0    0    1    7    2  603    0    0    0    0    0    0   43
     1    0    0]
 [   0    0    0    0    0   24    1    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    1    0    1    1    0    1  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    3    0    6    0    0    0    0    0    0    9
     0    0    0]
 [   0    0   23   84    2   37    4    0    0    0  442  233    4   32
     2   12    0]
 [   0    0  182   95   12   70   17    0    8    0  144 1392  177   27
    14   31   41]
 [   0    0  133   40   15    5    0    0    0    0   64   63  189    1
     1   16    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    2    0    0
  1094   40    0]
 [   0    0    0    0    0    2   19    0    0    0    0    0    0   33
    59  234    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
66.84010840108401

F1 scores:
[       nan 0.56140351 0.57226644 0.43170559 0.65696466 0.71862348
 0.89333333 0.         0.96271186 0.         0.45684755 0.66222645
 0.40471092 0.6864564  0.93865294 0.68221574 0.77777778]

Kappa:
0.6244342562927809
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efed9d37dd8>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.782, val_acc:0.397]
Epoch [2/120    avg_loss:2.600, val_acc:0.454]
Epoch [3/120    avg_loss:2.392, val_acc:0.468]
Epoch [4/120    avg_loss:2.165, val_acc:0.478]
Epoch [5/120    avg_loss:1.937, val_acc:0.534]
Epoch [6/120    avg_loss:1.682, val_acc:0.632]
Epoch [7/120    avg_loss:1.524, val_acc:0.597]
Epoch [8/120    avg_loss:1.675, val_acc:0.353]
Epoch [9/120    avg_loss:2.006, val_acc:0.504]
Epoch [10/120    avg_loss:1.815, val_acc:0.582]
Epoch [11/120    avg_loss:1.660, val_acc:0.612]
Epoch [12/120    avg_loss:1.419, val_acc:0.669]
Epoch [13/120    avg_loss:1.235, val_acc:0.664]
Epoch [14/120    avg_loss:1.175, val_acc:0.644]
Epoch [15/120    avg_loss:1.004, val_acc:0.733]
Epoch [16/120    avg_loss:0.921, val_acc:0.762]
Epoch [17/120    avg_loss:0.817, val_acc:0.784]
Epoch [18/120    avg_loss:0.754, val_acc:0.812]
Epoch [19/120    avg_loss:0.687, val_acc:0.771]
Epoch [20/120    avg_loss:0.613, val_acc:0.796]
Epoch [21/120    avg_loss:0.606, val_acc:0.805]
Epoch [22/120    avg_loss:0.556, val_acc:0.832]
Epoch [23/120    avg_loss:0.491, val_acc:0.831]
Epoch [24/120    avg_loss:0.454, val_acc:0.868]
Epoch [25/120    avg_loss:0.403, val_acc:0.892]
Epoch [26/120    avg_loss:0.357, val_acc:0.842]
Epoch [27/120    avg_loss:0.324, val_acc:0.878]
Epoch [28/120    avg_loss:0.273, val_acc:0.875]
Epoch [29/120    avg_loss:0.308, val_acc:0.905]
Epoch [30/120    avg_loss:0.239, val_acc:0.928]
Epoch [31/120    avg_loss:0.215, val_acc:0.879]
Epoch [32/120    avg_loss:0.241, val_acc:0.903]
Epoch [33/120    avg_loss:0.196, val_acc:0.927]
Epoch [34/120    avg_loss:0.164, val_acc:0.913]
Epoch [35/120    avg_loss:0.179, val_acc:0.913]
Epoch [36/120    avg_loss:0.144, val_acc:0.913]
Epoch [37/120    avg_loss:0.174, val_acc:0.944]
Epoch [38/120    avg_loss:0.130, val_acc:0.936]
Epoch [39/120    avg_loss:0.173, val_acc:0.920]
Epoch [40/120    avg_loss:0.196, val_acc:0.948]
Epoch [41/120    avg_loss:0.102, val_acc:0.941]
Epoch [42/120    avg_loss:0.108, val_acc:0.946]
Epoch [43/120    avg_loss:0.129, val_acc:0.956]
Epoch [44/120    avg_loss:0.080, val_acc:0.957]
Epoch [45/120    avg_loss:0.126, val_acc:0.869]
Epoch [46/120    avg_loss:0.127, val_acc:0.949]
Epoch [47/120    avg_loss:0.067, val_acc:0.965]
Epoch [48/120    avg_loss:0.056, val_acc:0.967]
Epoch [49/120    avg_loss:0.057, val_acc:0.963]
Epoch [50/120    avg_loss:0.046, val_acc:0.968]
Epoch [51/120    avg_loss:0.059, val_acc:0.963]
Epoch [52/120    avg_loss:0.050, val_acc:0.966]
Epoch [53/120    avg_loss:0.050, val_acc:0.969]
Epoch [54/120    avg_loss:0.052, val_acc:0.965]
Epoch [55/120    avg_loss:0.051, val_acc:0.969]
Epoch [56/120    avg_loss:0.041, val_acc:0.960]
Epoch [57/120    avg_loss:0.043, val_acc:0.960]
Epoch [58/120    avg_loss:0.044, val_acc:0.963]
Epoch [59/120    avg_loss:0.038, val_acc:0.961]
Epoch [60/120    avg_loss:0.032, val_acc:0.969]
Epoch [61/120    avg_loss:0.025, val_acc:0.973]
Epoch [62/120    avg_loss:0.026, val_acc:0.973]
Epoch [63/120    avg_loss:0.025, val_acc:0.970]
Epoch [64/120    avg_loss:0.022, val_acc:0.972]
Epoch [65/120    avg_loss:0.021, val_acc:0.975]
Epoch [66/120    avg_loss:0.021, val_acc:0.970]
Epoch [67/120    avg_loss:0.026, val_acc:0.974]
Epoch [68/120    avg_loss:0.061, val_acc:0.971]
Epoch [69/120    avg_loss:0.038, val_acc:0.972]
Epoch [70/120    avg_loss:0.029, val_acc:0.972]
Epoch [71/120    avg_loss:0.067, val_acc:0.955]
Epoch [72/120    avg_loss:0.048, val_acc:0.956]
Epoch [73/120    avg_loss:0.039, val_acc:0.966]
Epoch [74/120    avg_loss:0.023, val_acc:0.979]
Epoch [75/120    avg_loss:0.038, val_acc:0.970]
Epoch [76/120    avg_loss:0.021, val_acc:0.959]
Epoch [77/120    avg_loss:0.027, val_acc:0.963]
Epoch [78/120    avg_loss:0.021, val_acc:0.973]
Epoch [79/120    avg_loss:0.048, val_acc:0.966]
Epoch [80/120    avg_loss:0.033, val_acc:0.971]
Epoch [81/120    avg_loss:0.023, val_acc:0.973]
Epoch [82/120    avg_loss:0.015, val_acc:0.975]
Epoch [83/120    avg_loss:0.017, val_acc:0.973]
Epoch [84/120    avg_loss:0.014, val_acc:0.975]
Epoch [85/120    avg_loss:0.010, val_acc:0.976]
Epoch [86/120    avg_loss:0.011, val_acc:0.978]
Epoch [87/120    avg_loss:0.019, val_acc:0.967]
Epoch [88/120    avg_loss:0.017, val_acc:0.977]
Epoch [89/120    avg_loss:0.016, val_acc:0.978]
Epoch [90/120    avg_loss:0.010, val_acc:0.979]
Epoch [91/120    avg_loss:0.010, val_acc:0.980]
Epoch [92/120    avg_loss:0.015, val_acc:0.981]
Epoch [93/120    avg_loss:0.013, val_acc:0.981]
Epoch [94/120    avg_loss:0.014, val_acc:0.980]
Epoch [95/120    avg_loss:0.009, val_acc:0.979]
Epoch [96/120    avg_loss:0.009, val_acc:0.980]
Epoch [97/120    avg_loss:0.010, val_acc:0.981]
Epoch [98/120    avg_loss:0.008, val_acc:0.980]
Epoch [99/120    avg_loss:0.010, val_acc:0.979]
Epoch [100/120    avg_loss:0.007, val_acc:0.980]
Epoch [101/120    avg_loss:0.009, val_acc:0.980]
Epoch [102/120    avg_loss:0.011, val_acc:0.980]
Epoch [103/120    avg_loss:0.009, val_acc:0.979]
Epoch [104/120    avg_loss:0.008, val_acc:0.980]
Epoch [105/120    avg_loss:0.007, val_acc:0.981]
Epoch [106/120    avg_loss:0.006, val_acc:0.980]
Epoch [107/120    avg_loss:0.007, val_acc:0.981]
Epoch [108/120    avg_loss:0.007, val_acc:0.981]
Epoch [109/120    avg_loss:0.007, val_acc:0.981]
Epoch [110/120    avg_loss:0.008, val_acc:0.980]
Epoch [111/120    avg_loss:0.012, val_acc:0.980]
Epoch [112/120    avg_loss:0.009, val_acc:0.979]
Epoch [113/120    avg_loss:0.006, val_acc:0.980]
Epoch [114/120    avg_loss:0.009, val_acc:0.980]
Epoch [115/120    avg_loss:0.007, val_acc:0.980]
Epoch [116/120    avg_loss:0.008, val_acc:0.979]
Epoch [117/120    avg_loss:0.007, val_acc:0.978]
Epoch [118/120    avg_loss:0.008, val_acc:0.979]
Epoch [119/120    avg_loss:0.006, val_acc:0.978]
Epoch [120/120    avg_loss:0.010, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    1    0    0    0    0    0    0    0    2    3    0    0
     0    0    0]
 [   0    0 1260    2    3    0    0    0    0    1    6   13    0    0
     0    0    0]
 [   0    0    0  721    5    0    0    0    0    0    5    0   15    0
     0    1    0]
 [   0    0    0    0  209    0    0    0    0    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    0    0    1    0    0
     6    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  845   26    0    0
     0    0    0]
 [   0    0    3    1    0    0    0    0    0    0   10 2185   11    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0  529    0
     0    2    1]
 [   0    0    0    1    0    0    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    0    0    0    0
  1127   10    0]
 [   0    0    0    0    0    1    4    0    0    0    0    0    0    0
    55  287    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.78861788617886

F1 scores:
[       nan 0.92105263 0.98707403 0.97961957 0.97209302 0.98959538
 0.99544073 1.         0.99883856 0.97297297 0.96848138 0.98467778
 0.96709324 0.99728997 0.96779734 0.88717156 0.98809524]

Kappa:
0.9747703903080547
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5053f0ceb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.766, val_acc:0.305]
Epoch [2/120    avg_loss:2.557, val_acc:0.465]
Epoch [3/120    avg_loss:2.298, val_acc:0.473]
Epoch [4/120    avg_loss:2.052, val_acc:0.532]
Epoch [5/120    avg_loss:1.808, val_acc:0.605]
Epoch [6/120    avg_loss:1.580, val_acc:0.650]
Epoch [7/120    avg_loss:1.397, val_acc:0.673]
Epoch [8/120    avg_loss:1.155, val_acc:0.645]
Epoch [9/120    avg_loss:1.048, val_acc:0.663]
Epoch [10/120    avg_loss:0.919, val_acc:0.736]
Epoch [11/120    avg_loss:0.753, val_acc:0.781]
Epoch [12/120    avg_loss:0.734, val_acc:0.783]
Epoch [13/120    avg_loss:0.558, val_acc:0.748]
Epoch [14/120    avg_loss:0.584, val_acc:0.814]
Epoch [15/120    avg_loss:0.455, val_acc:0.857]
Epoch [16/120    avg_loss:0.507, val_acc:0.823]
Epoch [17/120    avg_loss:0.381, val_acc:0.821]
Epoch [18/120    avg_loss:0.351, val_acc:0.902]
Epoch [19/120    avg_loss:0.244, val_acc:0.893]
Epoch [20/120    avg_loss:0.666, val_acc:0.832]
Epoch [21/120    avg_loss:0.311, val_acc:0.860]
Epoch [22/120    avg_loss:0.336, val_acc:0.875]
Epoch [23/120    avg_loss:0.207, val_acc:0.909]
Epoch [24/120    avg_loss:0.221, val_acc:0.896]
Epoch [25/120    avg_loss:0.176, val_acc:0.911]
Epoch [26/120    avg_loss:0.148, val_acc:0.927]
Epoch [27/120    avg_loss:0.119, val_acc:0.929]
Epoch [28/120    avg_loss:0.142, val_acc:0.910]
Epoch [29/120    avg_loss:0.103, val_acc:0.927]
Epoch [30/120    avg_loss:0.124, val_acc:0.927]
Epoch [31/120    avg_loss:0.184, val_acc:0.926]
Epoch [32/120    avg_loss:0.103, val_acc:0.936]
Epoch [33/120    avg_loss:0.128, val_acc:0.928]
Epoch [34/120    avg_loss:0.114, val_acc:0.941]
Epoch [35/120    avg_loss:0.099, val_acc:0.927]
Epoch [36/120    avg_loss:0.102, val_acc:0.937]
Epoch [37/120    avg_loss:0.076, val_acc:0.951]
Epoch [38/120    avg_loss:0.055, val_acc:0.957]
Epoch [39/120    avg_loss:0.050, val_acc:0.951]
Epoch [40/120    avg_loss:0.052, val_acc:0.955]
Epoch [41/120    avg_loss:0.051, val_acc:0.960]
Epoch [42/120    avg_loss:0.045, val_acc:0.956]
Epoch [43/120    avg_loss:0.046, val_acc:0.955]
Epoch [44/120    avg_loss:0.098, val_acc:0.934]
Epoch [45/120    avg_loss:0.082, val_acc:0.948]
Epoch [46/120    avg_loss:0.048, val_acc:0.967]
Epoch [47/120    avg_loss:0.093, val_acc:0.950]
Epoch [48/120    avg_loss:0.049, val_acc:0.948]
Epoch [49/120    avg_loss:0.066, val_acc:0.956]
Epoch [50/120    avg_loss:0.055, val_acc:0.958]
Epoch [51/120    avg_loss:0.054, val_acc:0.957]
Epoch [52/120    avg_loss:0.041, val_acc:0.957]
Epoch [53/120    avg_loss:0.037, val_acc:0.959]
Epoch [54/120    avg_loss:0.028, val_acc:0.965]
Epoch [55/120    avg_loss:0.027, val_acc:0.965]
Epoch [56/120    avg_loss:0.021, val_acc:0.970]
Epoch [57/120    avg_loss:0.017, val_acc:0.958]
Epoch [58/120    avg_loss:0.045, val_acc:0.960]
Epoch [59/120    avg_loss:0.017, val_acc:0.968]
Epoch [60/120    avg_loss:0.016, val_acc:0.967]
Epoch [61/120    avg_loss:0.043, val_acc:0.947]
Epoch [62/120    avg_loss:0.030, val_acc:0.971]
Epoch [63/120    avg_loss:0.037, val_acc:0.958]
Epoch [64/120    avg_loss:0.033, val_acc:0.957]
Epoch [65/120    avg_loss:0.024, val_acc:0.963]
Epoch [66/120    avg_loss:0.021, val_acc:0.961]
Epoch [67/120    avg_loss:0.018, val_acc:0.966]
Epoch [68/120    avg_loss:0.017, val_acc:0.969]
Epoch [69/120    avg_loss:0.027, val_acc:0.961]
Epoch [70/120    avg_loss:0.030, val_acc:0.961]
Epoch [71/120    avg_loss:0.103, val_acc:0.959]
Epoch [72/120    avg_loss:0.033, val_acc:0.960]
Epoch [73/120    avg_loss:0.040, val_acc:0.964]
Epoch [74/120    avg_loss:0.032, val_acc:0.934]
Epoch [75/120    avg_loss:0.028, val_acc:0.965]
Epoch [76/120    avg_loss:0.012, val_acc:0.964]
Epoch [77/120    avg_loss:0.009, val_acc:0.965]
Epoch [78/120    avg_loss:0.010, val_acc:0.968]
Epoch [79/120    avg_loss:0.010, val_acc:0.966]
Epoch [80/120    avg_loss:0.009, val_acc:0.965]
Epoch [81/120    avg_loss:0.009, val_acc:0.968]
Epoch [82/120    avg_loss:0.010, val_acc:0.969]
Epoch [83/120    avg_loss:0.008, val_acc:0.969]
Epoch [84/120    avg_loss:0.008, val_acc:0.968]
Epoch [85/120    avg_loss:0.008, val_acc:0.968]
Epoch [86/120    avg_loss:0.010, val_acc:0.968]
Epoch [87/120    avg_loss:0.008, val_acc:0.968]
Epoch [88/120    avg_loss:0.009, val_acc:0.967]
Epoch [89/120    avg_loss:0.008, val_acc:0.967]
Epoch [90/120    avg_loss:0.008, val_acc:0.967]
Epoch [91/120    avg_loss:0.007, val_acc:0.967]
Epoch [92/120    avg_loss:0.007, val_acc:0.967]
Epoch [93/120    avg_loss:0.008, val_acc:0.967]
Epoch [94/120    avg_loss:0.008, val_acc:0.967]
Epoch [95/120    avg_loss:0.008, val_acc:0.967]
Epoch [96/120    avg_loss:0.007, val_acc:0.967]
Epoch [97/120    avg_loss:0.011, val_acc:0.967]
Epoch [98/120    avg_loss:0.008, val_acc:0.967]
Epoch [99/120    avg_loss:0.008, val_acc:0.968]
Epoch [100/120    avg_loss:0.007, val_acc:0.968]
Epoch [101/120    avg_loss:0.007, val_acc:0.968]
Epoch [102/120    avg_loss:0.009, val_acc:0.968]
Epoch [103/120    avg_loss:0.008, val_acc:0.968]
Epoch [104/120    avg_loss:0.007, val_acc:0.968]
Epoch [105/120    avg_loss:0.011, val_acc:0.968]
Epoch [106/120    avg_loss:0.008, val_acc:0.968]
Epoch [107/120    avg_loss:0.009, val_acc:0.968]
Epoch [108/120    avg_loss:0.008, val_acc:0.968]
Epoch [109/120    avg_loss:0.007, val_acc:0.968]
Epoch [110/120    avg_loss:0.007, val_acc:0.968]
Epoch [111/120    avg_loss:0.007, val_acc:0.968]
Epoch [112/120    avg_loss:0.007, val_acc:0.968]
Epoch [113/120    avg_loss:0.008, val_acc:0.968]
Epoch [114/120    avg_loss:0.008, val_acc:0.968]
Epoch [115/120    avg_loss:0.008, val_acc:0.968]
Epoch [116/120    avg_loss:0.008, val_acc:0.968]
Epoch [117/120    avg_loss:0.006, val_acc:0.968]
Epoch [118/120    avg_loss:0.007, val_acc:0.968]
Epoch [119/120    avg_loss:0.008, val_acc:0.968]
Epoch [120/120    avg_loss:0.008, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1240    0   21    0    0    0    0    0    4   20    0    0
     0    0    0]
 [   0    0    4  734    2    0    0    0    0    0    3    1    2    1
     0    0    0]
 [   0    0    0    1  209    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  427    1    0    0    0    0    2    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  850   19    1    0
     0    0    0]
 [   0    0   14    0    0    0    0    0    0    0   30 2137   27    0
     1    0    1]
 [   0    0    0    6    0    0    0    0    0    0    0    2  519    0
     0    5    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1124   14    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    62  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.06233062330624

F1 scores:
[       nan 0.98765432 0.97293056 0.98655914 0.93932584 0.98957126
 0.99168556 1.         1.         0.94117647 0.96481271 0.97313297
 0.9558011  0.99459459 0.96439296 0.86511628 0.98245614]

Kappa:
0.9665181203668735
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0a72d8eeb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.776, val_acc:0.387]
Epoch [2/120    avg_loss:2.585, val_acc:0.486]
Epoch [3/120    avg_loss:2.335, val_acc:0.495]
Epoch [4/120    avg_loss:2.080, val_acc:0.523]
Epoch [5/120    avg_loss:1.827, val_acc:0.523]
Epoch [6/120    avg_loss:1.635, val_acc:0.586]
Epoch [7/120    avg_loss:1.344, val_acc:0.671]
Epoch [8/120    avg_loss:1.158, val_acc:0.670]
Epoch [9/120    avg_loss:1.047, val_acc:0.726]
Epoch [10/120    avg_loss:0.883, val_acc:0.742]
Epoch [11/120    avg_loss:0.798, val_acc:0.772]
Epoch [12/120    avg_loss:0.743, val_acc:0.780]
Epoch [13/120    avg_loss:0.696, val_acc:0.752]
Epoch [14/120    avg_loss:0.464, val_acc:0.813]
Epoch [15/120    avg_loss:0.460, val_acc:0.814]
Epoch [16/120    avg_loss:0.408, val_acc:0.777]
Epoch [17/120    avg_loss:0.372, val_acc:0.858]
Epoch [18/120    avg_loss:0.321, val_acc:0.865]
Epoch [19/120    avg_loss:0.277, val_acc:0.826]
Epoch [20/120    avg_loss:0.284, val_acc:0.845]
Epoch [21/120    avg_loss:0.269, val_acc:0.864]
Epoch [22/120    avg_loss:0.303, val_acc:0.849]
Epoch [23/120    avg_loss:0.244, val_acc:0.872]
Epoch [24/120    avg_loss:0.331, val_acc:0.880]
Epoch [25/120    avg_loss:0.253, val_acc:0.901]
Epoch [26/120    avg_loss:0.208, val_acc:0.897]
Epoch [27/120    avg_loss:0.199, val_acc:0.911]
Epoch [28/120    avg_loss:0.157, val_acc:0.885]
Epoch [29/120    avg_loss:0.156, val_acc:0.913]
Epoch [30/120    avg_loss:0.166, val_acc:0.902]
Epoch [31/120    avg_loss:0.124, val_acc:0.931]
Epoch [32/120    avg_loss:0.141, val_acc:0.896]
Epoch [33/120    avg_loss:0.165, val_acc:0.921]
Epoch [34/120    avg_loss:0.127, val_acc:0.947]
Epoch [35/120    avg_loss:0.109, val_acc:0.931]
Epoch [36/120    avg_loss:0.108, val_acc:0.946]
Epoch [37/120    avg_loss:0.117, val_acc:0.929]
Epoch [38/120    avg_loss:0.093, val_acc:0.929]
Epoch [39/120    avg_loss:0.083, val_acc:0.936]
Epoch [40/120    avg_loss:0.065, val_acc:0.930]
Epoch [41/120    avg_loss:0.069, val_acc:0.948]
Epoch [42/120    avg_loss:0.084, val_acc:0.924]
Epoch [43/120    avg_loss:0.070, val_acc:0.960]
Epoch [44/120    avg_loss:0.044, val_acc:0.940]
Epoch [45/120    avg_loss:0.071, val_acc:0.949]
Epoch [46/120    avg_loss:0.058, val_acc:0.951]
Epoch [47/120    avg_loss:0.054, val_acc:0.945]
Epoch [48/120    avg_loss:0.065, val_acc:0.939]
Epoch [49/120    avg_loss:0.074, val_acc:0.937]
Epoch [50/120    avg_loss:0.096, val_acc:0.933]
Epoch [51/120    avg_loss:0.094, val_acc:0.901]
Epoch [52/120    avg_loss:0.093, val_acc:0.931]
Epoch [53/120    avg_loss:0.052, val_acc:0.960]
Epoch [54/120    avg_loss:0.036, val_acc:0.963]
Epoch [55/120    avg_loss:0.033, val_acc:0.963]
Epoch [56/120    avg_loss:0.032, val_acc:0.956]
Epoch [57/120    avg_loss:0.047, val_acc:0.937]
Epoch [58/120    avg_loss:0.060, val_acc:0.958]
Epoch [59/120    avg_loss:0.035, val_acc:0.950]
Epoch [60/120    avg_loss:0.039, val_acc:0.949]
Epoch [61/120    avg_loss:0.061, val_acc:0.940]
Epoch [62/120    avg_loss:0.043, val_acc:0.958]
Epoch [63/120    avg_loss:0.078, val_acc:0.923]
Epoch [64/120    avg_loss:0.042, val_acc:0.959]
Epoch [65/120    avg_loss:0.024, val_acc:0.961]
Epoch [66/120    avg_loss:0.048, val_acc:0.950]
Epoch [67/120    avg_loss:0.027, val_acc:0.952]
Epoch [68/120    avg_loss:0.026, val_acc:0.960]
Epoch [69/120    avg_loss:0.018, val_acc:0.964]
Epoch [70/120    avg_loss:0.015, val_acc:0.966]
Epoch [71/120    avg_loss:0.015, val_acc:0.970]
Epoch [72/120    avg_loss:0.010, val_acc:0.967]
Epoch [73/120    avg_loss:0.012, val_acc:0.968]
Epoch [74/120    avg_loss:0.010, val_acc:0.968]
Epoch [75/120    avg_loss:0.013, val_acc:0.967]
Epoch [76/120    avg_loss:0.011, val_acc:0.970]
Epoch [77/120    avg_loss:0.018, val_acc:0.971]
Epoch [78/120    avg_loss:0.016, val_acc:0.970]
Epoch [79/120    avg_loss:0.009, val_acc:0.970]
Epoch [80/120    avg_loss:0.008, val_acc:0.970]
Epoch [81/120    avg_loss:0.009, val_acc:0.970]
Epoch [82/120    avg_loss:0.010, val_acc:0.972]
Epoch [83/120    avg_loss:0.015, val_acc:0.971]
Epoch [84/120    avg_loss:0.010, val_acc:0.971]
Epoch [85/120    avg_loss:0.017, val_acc:0.970]
Epoch [86/120    avg_loss:0.011, val_acc:0.970]
Epoch [87/120    avg_loss:0.008, val_acc:0.970]
Epoch [88/120    avg_loss:0.009, val_acc:0.970]
Epoch [89/120    avg_loss:0.013, val_acc:0.965]
Epoch [90/120    avg_loss:0.008, val_acc:0.967]
Epoch [91/120    avg_loss:0.011, val_acc:0.970]
Epoch [92/120    avg_loss:0.011, val_acc:0.970]
Epoch [93/120    avg_loss:0.010, val_acc:0.971]
Epoch [94/120    avg_loss:0.009, val_acc:0.971]
Epoch [95/120    avg_loss:0.009, val_acc:0.971]
Epoch [96/120    avg_loss:0.008, val_acc:0.971]
Epoch [97/120    avg_loss:0.009, val_acc:0.970]
Epoch [98/120    avg_loss:0.008, val_acc:0.971]
Epoch [99/120    avg_loss:0.009, val_acc:0.971]
Epoch [100/120    avg_loss:0.013, val_acc:0.971]
Epoch [101/120    avg_loss:0.008, val_acc:0.971]
Epoch [102/120    avg_loss:0.008, val_acc:0.971]
Epoch [103/120    avg_loss:0.007, val_acc:0.971]
Epoch [104/120    avg_loss:0.009, val_acc:0.971]
Epoch [105/120    avg_loss:0.009, val_acc:0.971]
Epoch [106/120    avg_loss:0.009, val_acc:0.971]
Epoch [107/120    avg_loss:0.009, val_acc:0.971]
Epoch [108/120    avg_loss:0.010, val_acc:0.971]
Epoch [109/120    avg_loss:0.009, val_acc:0.971]
Epoch [110/120    avg_loss:0.008, val_acc:0.971]
Epoch [111/120    avg_loss:0.009, val_acc:0.971]
Epoch [112/120    avg_loss:0.012, val_acc:0.971]
Epoch [113/120    avg_loss:0.009, val_acc:0.971]
Epoch [114/120    avg_loss:0.009, val_acc:0.971]
Epoch [115/120    avg_loss:0.008, val_acc:0.971]
Epoch [116/120    avg_loss:0.009, val_acc:0.971]
Epoch [117/120    avg_loss:0.008, val_acc:0.971]
Epoch [118/120    avg_loss:0.011, val_acc:0.971]
Epoch [119/120    avg_loss:0.010, val_acc:0.971]
Epoch [120/120    avg_loss:0.008, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    0    1    2    0
     0    0    0]
 [   0    0 1252    1   11    0    0    0    0    0    7   14    0    0
     0    0    0]
 [   0    0    0  731   11    0    0    0    0    2    0    1    2    0
     0    0    0]
 [   0    0    0    2  206    0    0    0    0    0    0    0    5    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    2    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    1    0    1    1    0    0    0  852   13    0    0
     0    0    0]
 [   0    0    8    0    0    1    0    0    0    1   20 2157   23    0
     0    0    0]
 [   0    0    0    8    0    0    0    0    0    0    1    0  523    0
     1    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1120   19    0]
 [   0    0    0    0    0    2   10    0    0    0    0    0    0    0
    84  251    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.0840108401084

F1 scores:
[       nan 0.96202532 0.98119122 0.98120805 0.93424036 0.99196326
 0.98940998 1.         0.99883586 0.92307692 0.96983495 0.98067743
 0.95875344 1.         0.95522388 0.81229773 0.99401198]

Kappa:
0.9667532395637206
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0415bffe80>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.760, val_acc:0.317]
Epoch [2/120    avg_loss:2.572, val_acc:0.324]
Epoch [3/120    avg_loss:2.317, val_acc:0.446]
Epoch [4/120    avg_loss:2.103, val_acc:0.446]
Epoch [5/120    avg_loss:1.918, val_acc:0.541]
Epoch [6/120    avg_loss:1.707, val_acc:0.607]
Epoch [7/120    avg_loss:1.529, val_acc:0.631]
Epoch [8/120    avg_loss:1.322, val_acc:0.652]
Epoch [9/120    avg_loss:1.176, val_acc:0.662]
Epoch [10/120    avg_loss:1.083, val_acc:0.709]
Epoch [11/120    avg_loss:1.154, val_acc:0.709]
Epoch [12/120    avg_loss:0.841, val_acc:0.738]
Epoch [13/120    avg_loss:0.714, val_acc:0.795]
Epoch [14/120    avg_loss:0.639, val_acc:0.812]
Epoch [15/120    avg_loss:0.644, val_acc:0.812]
Epoch [16/120    avg_loss:0.709, val_acc:0.772]
Epoch [17/120    avg_loss:0.544, val_acc:0.839]
Epoch [18/120    avg_loss:0.557, val_acc:0.801]
Epoch [19/120    avg_loss:0.751, val_acc:0.282]
Epoch [20/120    avg_loss:2.064, val_acc:0.436]
Epoch [21/120    avg_loss:1.885, val_acc:0.511]
Epoch [22/120    avg_loss:1.819, val_acc:0.555]
Epoch [23/120    avg_loss:1.745, val_acc:0.580]
Epoch [24/120    avg_loss:1.671, val_acc:0.626]
Epoch [25/120    avg_loss:1.585, val_acc:0.649]
Epoch [26/120    avg_loss:1.495, val_acc:0.647]
Epoch [27/120    avg_loss:1.480, val_acc:0.663]
Epoch [28/120    avg_loss:1.392, val_acc:0.690]
Epoch [29/120    avg_loss:1.361, val_acc:0.704]
Epoch [30/120    avg_loss:1.342, val_acc:0.670]
Epoch [31/120    avg_loss:1.218, val_acc:0.720]
Epoch [32/120    avg_loss:1.196, val_acc:0.734]
Epoch [33/120    avg_loss:1.190, val_acc:0.727]
Epoch [34/120    avg_loss:1.240, val_acc:0.733]
Epoch [35/120    avg_loss:1.178, val_acc:0.732]
Epoch [36/120    avg_loss:1.193, val_acc:0.742]
Epoch [37/120    avg_loss:1.187, val_acc:0.742]
Epoch [38/120    avg_loss:1.201, val_acc:0.744]
Epoch [39/120    avg_loss:1.170, val_acc:0.737]
Epoch [40/120    avg_loss:1.146, val_acc:0.745]
Epoch [41/120    avg_loss:1.166, val_acc:0.741]
Epoch [42/120    avg_loss:1.145, val_acc:0.746]
Epoch [43/120    avg_loss:1.146, val_acc:0.735]
Epoch [44/120    avg_loss:1.141, val_acc:0.741]
Epoch [45/120    avg_loss:1.113, val_acc:0.745]
Epoch [46/120    avg_loss:1.136, val_acc:0.746]
Epoch [47/120    avg_loss:1.141, val_acc:0.745]
Epoch [48/120    avg_loss:1.135, val_acc:0.746]
Epoch [49/120    avg_loss:1.098, val_acc:0.746]
Epoch [50/120    avg_loss:1.164, val_acc:0.747]
Epoch [51/120    avg_loss:1.126, val_acc:0.745]
Epoch [52/120    avg_loss:1.099, val_acc:0.744]
Epoch [53/120    avg_loss:1.127, val_acc:0.744]
Epoch [54/120    avg_loss:1.097, val_acc:0.745]
Epoch [55/120    avg_loss:1.113, val_acc:0.744]
Epoch [56/120    avg_loss:1.133, val_acc:0.744]
Epoch [57/120    avg_loss:1.118, val_acc:0.743]
Epoch [58/120    avg_loss:1.111, val_acc:0.744]
Epoch [59/120    avg_loss:1.114, val_acc:0.743]
Epoch [60/120    avg_loss:1.131, val_acc:0.743]
Epoch [61/120    avg_loss:1.165, val_acc:0.743]
Epoch [62/120    avg_loss:1.117, val_acc:0.743]
Epoch [63/120    avg_loss:1.094, val_acc:0.743]
Epoch [64/120    avg_loss:1.104, val_acc:0.743]
Epoch [65/120    avg_loss:1.108, val_acc:0.743]
Epoch [66/120    avg_loss:1.108, val_acc:0.743]
Epoch [67/120    avg_loss:1.104, val_acc:0.743]
Epoch [68/120    avg_loss:1.128, val_acc:0.743]
Epoch [69/120    avg_loss:1.127, val_acc:0.743]
Epoch [70/120    avg_loss:1.127, val_acc:0.743]
Epoch [71/120    avg_loss:1.115, val_acc:0.743]
Epoch [72/120    avg_loss:1.118, val_acc:0.743]
Epoch [73/120    avg_loss:1.110, val_acc:0.743]
Epoch [74/120    avg_loss:1.119, val_acc:0.743]
Epoch [75/120    avg_loss:1.134, val_acc:0.743]
Epoch [76/120    avg_loss:1.132, val_acc:0.743]
Epoch [77/120    avg_loss:1.118, val_acc:0.743]
Epoch [78/120    avg_loss:1.102, val_acc:0.743]
Epoch [79/120    avg_loss:1.123, val_acc:0.743]
Epoch [80/120    avg_loss:1.128, val_acc:0.743]
Epoch [81/120    avg_loss:1.101, val_acc:0.743]
Epoch [82/120    avg_loss:1.129, val_acc:0.743]
Epoch [83/120    avg_loss:1.128, val_acc:0.743]
Epoch [84/120    avg_loss:1.156, val_acc:0.743]
Epoch [85/120    avg_loss:1.129, val_acc:0.743]
Epoch [86/120    avg_loss:1.080, val_acc:0.743]
Epoch [87/120    avg_loss:1.140, val_acc:0.743]
Epoch [88/120    avg_loss:1.137, val_acc:0.743]
Epoch [89/120    avg_loss:1.102, val_acc:0.743]
Epoch [90/120    avg_loss:1.141, val_acc:0.743]
Epoch [91/120    avg_loss:1.117, val_acc:0.743]
Epoch [92/120    avg_loss:1.140, val_acc:0.743]
Epoch [93/120    avg_loss:1.147, val_acc:0.743]
Epoch [94/120    avg_loss:1.133, val_acc:0.743]
Epoch [95/120    avg_loss:1.116, val_acc:0.743]
Epoch [96/120    avg_loss:1.157, val_acc:0.743]
Epoch [97/120    avg_loss:1.136, val_acc:0.743]
Epoch [98/120    avg_loss:1.139, val_acc:0.743]
Epoch [99/120    avg_loss:1.109, val_acc:0.743]
Epoch [100/120    avg_loss:1.123, val_acc:0.743]
Epoch [101/120    avg_loss:1.116, val_acc:0.743]
Epoch [102/120    avg_loss:1.125, val_acc:0.743]
Epoch [103/120    avg_loss:1.117, val_acc:0.743]
Epoch [104/120    avg_loss:1.127, val_acc:0.743]
Epoch [105/120    avg_loss:1.136, val_acc:0.743]
Epoch [106/120    avg_loss:1.133, val_acc:0.743]
Epoch [107/120    avg_loss:1.102, val_acc:0.743]
Epoch [108/120    avg_loss:1.135, val_acc:0.743]
Epoch [109/120    avg_loss:1.115, val_acc:0.743]
Epoch [110/120    avg_loss:1.118, val_acc:0.743]
Epoch [111/120    avg_loss:1.144, val_acc:0.743]
Epoch [112/120    avg_loss:1.124, val_acc:0.743]
Epoch [113/120    avg_loss:1.125, val_acc:0.743]
Epoch [114/120    avg_loss:1.129, val_acc:0.743]
Epoch [115/120    avg_loss:1.110, val_acc:0.743]
Epoch [116/120    avg_loss:1.120, val_acc:0.743]
Epoch [117/120    avg_loss:1.138, val_acc:0.743]
Epoch [118/120    avg_loss:1.129, val_acc:0.743]
Epoch [119/120    avg_loss:1.119, val_acc:0.743]
Epoch [120/120    avg_loss:1.110, val_acc:0.743]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   22    0    0    1    0    4    0   10    0    0    0    4    0
     0    0    0]
 [   0    0  975   17   36   11   28    0    0    0   86   89    1   42
     0    0    0]
 [   0    0  104  371   50   31   12    0    0    0   23  126    9   21
     0    0    0]
 [   0    0   17   28  165    0    0    0    0    0    0    0    0    3
     0    0    0]
 [   0    0    4    1    0  347   57    0    0    0    0    1    0    4
    21    0    0]
 [   0    0   12    0    1    0  578    0    0    0    0    0    0   64
     2    0    0]
 [   0    0    0    0    0    9   16    0    0    0    0    0    0    0
     0    0    0]
 [   0   17    0    0    6    0    0    0  407    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    5    0   12    0    0    0    0    0    0    1
     0    0    0]
 [   0    0  120    6   32   14   11    0    0    0  542   85    9   56
     0    0    0]
 [   0    3  202   30   54   53   44    0   20    0   79 1604   56   63
     0    1    1]
 [   0    0   38   44   40    3    2    0    0    0   17   16  348    4
     0   11   11]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0  183
     0    0    0]
 [   0    0   24    2    0   16    0    0    3    0    0    8    0   23
  1042   21    0]
 [   0    0   26    1    6    3   81    0    0    0    0   43    7   19
    69   92    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
73.25745257452574

F1 scores:
[       nan 0.53012048 0.69469184 0.59502807 0.54187192 0.7527115
 0.76861702 0.         0.93563218 0.         0.66831073 0.76709708
 0.71752577 0.54790419 0.91684998 0.38983051 0.92134831]

Kappa:
0.6967649572410419
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fac01ab1ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.790, val_acc:0.254]
Epoch [2/120    avg_loss:2.627, val_acc:0.456]
Epoch [3/120    avg_loss:2.414, val_acc:0.502]
Epoch [4/120    avg_loss:2.179, val_acc:0.495]
Epoch [5/120    avg_loss:1.974, val_acc:0.576]
Epoch [6/120    avg_loss:1.801, val_acc:0.585]
Epoch [7/120    avg_loss:1.647, val_acc:0.621]
Epoch [8/120    avg_loss:1.511, val_acc:0.650]
Epoch [9/120    avg_loss:1.354, val_acc:0.678]
Epoch [10/120    avg_loss:1.213, val_acc:0.733]
Epoch [11/120    avg_loss:1.020, val_acc:0.748]
Epoch [12/120    avg_loss:0.940, val_acc:0.697]
Epoch [13/120    avg_loss:0.793, val_acc:0.633]
Epoch [14/120    avg_loss:0.696, val_acc:0.811]
Epoch [15/120    avg_loss:0.662, val_acc:0.766]
Epoch [16/120    avg_loss:0.560, val_acc:0.791]
Epoch [17/120    avg_loss:0.525, val_acc:0.794]
Epoch [18/120    avg_loss:0.430, val_acc:0.851]
Epoch [19/120    avg_loss:0.437, val_acc:0.841]
Epoch [20/120    avg_loss:0.333, val_acc:0.890]
Epoch [21/120    avg_loss:0.339, val_acc:0.854]
Epoch [22/120    avg_loss:0.290, val_acc:0.874]
Epoch [23/120    avg_loss:0.284, val_acc:0.890]
Epoch [24/120    avg_loss:0.224, val_acc:0.911]
Epoch [25/120    avg_loss:0.253, val_acc:0.885]
Epoch [26/120    avg_loss:0.202, val_acc:0.911]
Epoch [27/120    avg_loss:0.193, val_acc:0.891]
Epoch [28/120    avg_loss:0.150, val_acc:0.930]
Epoch [29/120    avg_loss:0.146, val_acc:0.909]
Epoch [30/120    avg_loss:0.179, val_acc:0.908]
Epoch [31/120    avg_loss:0.174, val_acc:0.895]
Epoch [32/120    avg_loss:0.117, val_acc:0.950]
Epoch [33/120    avg_loss:0.105, val_acc:0.939]
Epoch [34/120    avg_loss:0.094, val_acc:0.941]
Epoch [35/120    avg_loss:0.092, val_acc:0.903]
Epoch [36/120    avg_loss:0.111, val_acc:0.949]
Epoch [37/120    avg_loss:0.099, val_acc:0.949]
Epoch [38/120    avg_loss:0.082, val_acc:0.927]
Epoch [39/120    avg_loss:0.097, val_acc:0.925]
Epoch [40/120    avg_loss:0.145, val_acc:0.929]
Epoch [41/120    avg_loss:0.124, val_acc:0.929]
Epoch [42/120    avg_loss:0.071, val_acc:0.919]
Epoch [43/120    avg_loss:0.071, val_acc:0.927]
Epoch [44/120    avg_loss:0.106, val_acc:0.942]
Epoch [45/120    avg_loss:0.074, val_acc:0.944]
Epoch [46/120    avg_loss:0.037, val_acc:0.952]
Epoch [47/120    avg_loss:0.038, val_acc:0.957]
Epoch [48/120    avg_loss:0.039, val_acc:0.955]
Epoch [49/120    avg_loss:0.026, val_acc:0.956]
Epoch [50/120    avg_loss:0.035, val_acc:0.957]
Epoch [51/120    avg_loss:0.030, val_acc:0.959]
Epoch [52/120    avg_loss:0.035, val_acc:0.959]
Epoch [53/120    avg_loss:0.028, val_acc:0.958]
Epoch [54/120    avg_loss:0.027, val_acc:0.959]
Epoch [55/120    avg_loss:0.024, val_acc:0.959]
Epoch [56/120    avg_loss:0.024, val_acc:0.958]
Epoch [57/120    avg_loss:0.028, val_acc:0.958]
Epoch [58/120    avg_loss:0.024, val_acc:0.959]
Epoch [59/120    avg_loss:0.034, val_acc:0.960]
Epoch [60/120    avg_loss:0.025, val_acc:0.961]
Epoch [61/120    avg_loss:0.025, val_acc:0.959]
Epoch [62/120    avg_loss:0.023, val_acc:0.965]
Epoch [63/120    avg_loss:0.021, val_acc:0.961]
Epoch [64/120    avg_loss:0.028, val_acc:0.963]
Epoch [65/120    avg_loss:0.023, val_acc:0.958]
Epoch [66/120    avg_loss:0.026, val_acc:0.961]
Epoch [67/120    avg_loss:0.021, val_acc:0.961]
Epoch [68/120    avg_loss:0.030, val_acc:0.959]
Epoch [69/120    avg_loss:0.022, val_acc:0.961]
Epoch [70/120    avg_loss:0.028, val_acc:0.963]
Epoch [71/120    avg_loss:0.023, val_acc:0.963]
Epoch [72/120    avg_loss:0.022, val_acc:0.964]
Epoch [73/120    avg_loss:0.027, val_acc:0.961]
Epoch [74/120    avg_loss:0.021, val_acc:0.963]
Epoch [75/120    avg_loss:0.024, val_acc:0.964]
Epoch [76/120    avg_loss:0.023, val_acc:0.964]
Epoch [77/120    avg_loss:0.022, val_acc:0.963]
Epoch [78/120    avg_loss:0.021, val_acc:0.961]
Epoch [79/120    avg_loss:0.019, val_acc:0.961]
Epoch [80/120    avg_loss:0.020, val_acc:0.963]
Epoch [81/120    avg_loss:0.020, val_acc:0.961]
Epoch [82/120    avg_loss:0.017, val_acc:0.964]
Epoch [83/120    avg_loss:0.018, val_acc:0.964]
Epoch [84/120    avg_loss:0.019, val_acc:0.964]
Epoch [85/120    avg_loss:0.020, val_acc:0.964]
Epoch [86/120    avg_loss:0.022, val_acc:0.963]
Epoch [87/120    avg_loss:0.020, val_acc:0.964]
Epoch [88/120    avg_loss:0.019, val_acc:0.964]
Epoch [89/120    avg_loss:0.026, val_acc:0.964]
Epoch [90/120    avg_loss:0.021, val_acc:0.964]
Epoch [91/120    avg_loss:0.018, val_acc:0.964]
Epoch [92/120    avg_loss:0.021, val_acc:0.964]
Epoch [93/120    avg_loss:0.021, val_acc:0.964]
Epoch [94/120    avg_loss:0.019, val_acc:0.964]
Epoch [95/120    avg_loss:0.020, val_acc:0.964]
Epoch [96/120    avg_loss:0.018, val_acc:0.964]
Epoch [97/120    avg_loss:0.021, val_acc:0.965]
Epoch [98/120    avg_loss:0.017, val_acc:0.965]
Epoch [99/120    avg_loss:0.019, val_acc:0.965]
Epoch [100/120    avg_loss:0.020, val_acc:0.964]
Epoch [101/120    avg_loss:0.021, val_acc:0.964]
Epoch [102/120    avg_loss:0.020, val_acc:0.964]
Epoch [103/120    avg_loss:0.022, val_acc:0.965]
Epoch [104/120    avg_loss:0.020, val_acc:0.964]
Epoch [105/120    avg_loss:0.020, val_acc:0.964]
Epoch [106/120    avg_loss:0.027, val_acc:0.965]
Epoch [107/120    avg_loss:0.019, val_acc:0.965]
Epoch [108/120    avg_loss:0.020, val_acc:0.965]
Epoch [109/120    avg_loss:0.019, val_acc:0.965]
Epoch [110/120    avg_loss:0.019, val_acc:0.965]
Epoch [111/120    avg_loss:0.020, val_acc:0.965]
Epoch [112/120    avg_loss:0.025, val_acc:0.965]
Epoch [113/120    avg_loss:0.021, val_acc:0.965]
Epoch [114/120    avg_loss:0.022, val_acc:0.965]
Epoch [115/120    avg_loss:0.018, val_acc:0.965]
Epoch [116/120    avg_loss:0.020, val_acc:0.965]
Epoch [117/120    avg_loss:0.020, val_acc:0.965]
Epoch [118/120    avg_loss:0.019, val_acc:0.965]
Epoch [119/120    avg_loss:0.022, val_acc:0.965]
Epoch [120/120    avg_loss:0.021, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    1    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1251    1   13    0    0    0    0    0    8   12    0    0
     0    0    0]
 [   0    0    0  725    3    0    0    0    0    2    2    2   13    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    2    0    0   13    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    1    0    0    0    0  847   21    0    0
     0    1    0]
 [   0    0    9    0    0    1    1    0    0    1   15 2175    7    1
     0    0    0]
 [   0    0    0    5    0    2    0    0    0    0    7    0  515    0
     0    4    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
  1115   22    0]
 [   0    0    0    0    0    0    0    0    0    2    0    0    0    0
    32  313    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.73441734417344

F1 scores:
[       nan 0.98765432 0.98117647 0.97840756 0.96145125 0.98971429
 0.99696049 1.         1.         0.7027027  0.96579247 0.98371777
 0.9608209  0.99730458 0.97550306 0.91120815 0.97590361]

Kappa:
0.974174191985844
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f31a17a2f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.746, val_acc:0.405]
Epoch [2/120    avg_loss:2.544, val_acc:0.449]
Epoch [3/120    avg_loss:2.302, val_acc:0.507]
Epoch [4/120    avg_loss:2.038, val_acc:0.553]
Epoch [5/120    avg_loss:1.888, val_acc:0.600]
Epoch [6/120    avg_loss:1.710, val_acc:0.623]
Epoch [7/120    avg_loss:1.531, val_acc:0.708]
Epoch [8/120    avg_loss:1.298, val_acc:0.745]
Epoch [9/120    avg_loss:1.141, val_acc:0.744]
Epoch [10/120    avg_loss:0.945, val_acc:0.741]
Epoch [11/120    avg_loss:0.803, val_acc:0.783]
Epoch [12/120    avg_loss:0.695, val_acc:0.833]
Epoch [13/120    avg_loss:0.631, val_acc:0.766]
Epoch [14/120    avg_loss:0.591, val_acc:0.836]
Epoch [15/120    avg_loss:0.517, val_acc:0.872]
Epoch [16/120    avg_loss:0.445, val_acc:0.867]
Epoch [17/120    avg_loss:0.423, val_acc:0.890]
Epoch [18/120    avg_loss:0.376, val_acc:0.848]
Epoch [19/120    avg_loss:0.295, val_acc:0.892]
Epoch [20/120    avg_loss:0.274, val_acc:0.905]
Epoch [21/120    avg_loss:0.227, val_acc:0.920]
Epoch [22/120    avg_loss:0.239, val_acc:0.918]
Epoch [23/120    avg_loss:0.294, val_acc:0.465]
Epoch [24/120    avg_loss:2.128, val_acc:0.502]
Epoch [25/120    avg_loss:1.794, val_acc:0.515]
Epoch [26/120    avg_loss:1.690, val_acc:0.555]
Epoch [27/120    avg_loss:1.525, val_acc:0.583]
Epoch [28/120    avg_loss:1.479, val_acc:0.603]
Epoch [29/120    avg_loss:1.375, val_acc:0.635]
Epoch [30/120    avg_loss:1.329, val_acc:0.639]
Epoch [31/120    avg_loss:1.287, val_acc:0.660]
Epoch [32/120    avg_loss:1.222, val_acc:0.677]
Epoch [33/120    avg_loss:1.171, val_acc:0.702]
Epoch [34/120    avg_loss:1.135, val_acc:0.680]
Epoch [35/120    avg_loss:1.069, val_acc:0.708]
Epoch [36/120    avg_loss:1.030, val_acc:0.714]
Epoch [37/120    avg_loss:1.059, val_acc:0.715]
Epoch [38/120    avg_loss:1.016, val_acc:0.716]
Epoch [39/120    avg_loss:1.017, val_acc:0.717]
Epoch [40/120    avg_loss:1.031, val_acc:0.716]
Epoch [41/120    avg_loss:1.018, val_acc:0.717]
Epoch [42/120    avg_loss:1.040, val_acc:0.723]
Epoch [43/120    avg_loss:1.033, val_acc:0.723]
Epoch [44/120    avg_loss:1.046, val_acc:0.726]
Epoch [45/120    avg_loss:0.994, val_acc:0.720]
Epoch [46/120    avg_loss:0.992, val_acc:0.716]
Epoch [47/120    avg_loss:0.996, val_acc:0.724]
Epoch [48/120    avg_loss:1.016, val_acc:0.725]
Epoch [49/120    avg_loss:0.994, val_acc:0.727]
Epoch [50/120    avg_loss:1.017, val_acc:0.729]
Epoch [51/120    avg_loss:0.975, val_acc:0.725]
Epoch [52/120    avg_loss:0.977, val_acc:0.725]
Epoch [53/120    avg_loss:0.994, val_acc:0.729]
Epoch [54/120    avg_loss:0.994, val_acc:0.726]
Epoch [55/120    avg_loss:0.983, val_acc:0.730]
Epoch [56/120    avg_loss:0.976, val_acc:0.728]
Epoch [57/120    avg_loss:0.947, val_acc:0.728]
Epoch [58/120    avg_loss:0.975, val_acc:0.727]
Epoch [59/120    avg_loss:0.995, val_acc:0.729]
Epoch [60/120    avg_loss:0.958, val_acc:0.729]
Epoch [61/120    avg_loss:0.981, val_acc:0.727]
Epoch [62/120    avg_loss:0.967, val_acc:0.727]
Epoch [63/120    avg_loss:0.997, val_acc:0.727]
Epoch [64/120    avg_loss:0.980, val_acc:0.726]
Epoch [65/120    avg_loss:0.968, val_acc:0.726]
Epoch [66/120    avg_loss:1.019, val_acc:0.726]
Epoch [67/120    avg_loss:1.013, val_acc:0.726]
Epoch [68/120    avg_loss:0.997, val_acc:0.727]
Epoch [69/120    avg_loss:0.965, val_acc:0.727]
Epoch [70/120    avg_loss:0.967, val_acc:0.726]
Epoch [71/120    avg_loss:0.967, val_acc:0.727]
Epoch [72/120    avg_loss:0.988, val_acc:0.727]
Epoch [73/120    avg_loss:0.969, val_acc:0.727]
Epoch [74/120    avg_loss:0.976, val_acc:0.727]
Epoch [75/120    avg_loss:1.001, val_acc:0.727]
Epoch [76/120    avg_loss:0.951, val_acc:0.727]
Epoch [77/120    avg_loss:1.001, val_acc:0.727]
Epoch [78/120    avg_loss:0.984, val_acc:0.727]
Epoch [79/120    avg_loss:0.962, val_acc:0.727]
Epoch [80/120    avg_loss:0.941, val_acc:0.727]
Epoch [81/120    avg_loss:0.960, val_acc:0.727]
Epoch [82/120    avg_loss:0.969, val_acc:0.727]
Epoch [83/120    avg_loss:0.978, val_acc:0.727]
Epoch [84/120    avg_loss:0.948, val_acc:0.727]
Epoch [85/120    avg_loss:0.958, val_acc:0.727]
Epoch [86/120    avg_loss:0.982, val_acc:0.727]
Epoch [87/120    avg_loss:0.982, val_acc:0.727]
Epoch [88/120    avg_loss:1.007, val_acc:0.727]
Epoch [89/120    avg_loss:0.975, val_acc:0.727]
Epoch [90/120    avg_loss:1.008, val_acc:0.727]
Epoch [91/120    avg_loss:0.962, val_acc:0.727]
Epoch [92/120    avg_loss:0.994, val_acc:0.727]
Epoch [93/120    avg_loss:1.001, val_acc:0.727]
Epoch [94/120    avg_loss:0.987, val_acc:0.727]
Epoch [95/120    avg_loss:0.986, val_acc:0.727]
Epoch [96/120    avg_loss:0.977, val_acc:0.727]
Epoch [97/120    avg_loss:1.004, val_acc:0.727]
Epoch [98/120    avg_loss:1.000, val_acc:0.727]
Epoch [99/120    avg_loss:0.970, val_acc:0.727]
Epoch [100/120    avg_loss:0.979, val_acc:0.727]
Epoch [101/120    avg_loss:0.970, val_acc:0.727]
Epoch [102/120    avg_loss:1.011, val_acc:0.727]
Epoch [103/120    avg_loss:0.985, val_acc:0.727]
Epoch [104/120    avg_loss:0.985, val_acc:0.727]
Epoch [105/120    avg_loss:0.971, val_acc:0.727]
Epoch [106/120    avg_loss:1.019, val_acc:0.727]
Epoch [107/120    avg_loss:0.991, val_acc:0.727]
Epoch [108/120    avg_loss:1.002, val_acc:0.727]
Epoch [109/120    avg_loss:0.973, val_acc:0.727]
Epoch [110/120    avg_loss:0.984, val_acc:0.727]
Epoch [111/120    avg_loss:1.004, val_acc:0.727]
Epoch [112/120    avg_loss:0.966, val_acc:0.727]
Epoch [113/120    avg_loss:0.997, val_acc:0.727]
Epoch [114/120    avg_loss:0.962, val_acc:0.727]
Epoch [115/120    avg_loss:0.995, val_acc:0.727]
Epoch [116/120    avg_loss:0.998, val_acc:0.727]
Epoch [117/120    avg_loss:1.005, val_acc:0.727]
Epoch [118/120    avg_loss:0.986, val_acc:0.727]
Epoch [119/120    avg_loss:0.993, val_acc:0.727]
Epoch [120/120    avg_loss:0.986, val_acc:0.727]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   31    0    0    0    5    0    1    0    0    4    0    0    0
     0    0    0]
 [   0    0  888   42   38   28   53    0    0    0   82  150    1    0
     1    2    0]
 [   0    0   33  345   40   27   84    0    0    0    2  131   71   10
     0    4    0]
 [   0    0    3    5  172    0   22    0    0    0    0    0   11    0
     0    0    0]
 [   0    0    4    0    0  351   43    4    0    0    0    0    0    0
    33    0    0]
 [   0    0    0    0    0    0  642    0    0    0    0    4    0    0
    11    0    0]
 [   0    0    0    1    0   22    0    2    0    0    0    0    0    0
     0    0    0]
 [   0   10    0    0    0    0    3    0  408    0    0    0    6    0
     0    3    0]
 [   0    0    0    0    0    0   18    0    0    0    0    0    0    0
     0    0    0]
 [   0    0   91   20   25   43   70    0    0    0  481  133    3    5
     0    4    0]
 [   0    2  221   49   89   30  110    0    0    0   45 1526   79   14
     0   45    0]
 [   0    0    4    9   20   15   29    0    0    0    0   13  402    0
     0   31   11]
 [   0    0    3    0    0    2   13    0    0    0    0    0    0  167
     0    0    0]
 [   0    0    6    2    6    6    4    0    0    0    0    2    4    0
  1063   46    0]
 [   0    0    2    0    0    4  104    0    0    0    0  106    6    0
    10  115    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
72.35772357723577

F1 scores:
[       nan 0.73809524 0.6992126  0.56557377 0.57048093 0.72520661
 0.69330454 0.125      0.97374702 0.         0.64607119 0.71391813
 0.71849866 0.87664042 0.94195835 0.38525963 0.92655367]

Kappa:
0.6865158832552167
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3f87b31eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.722, val_acc:0.232]
Epoch [2/120    avg_loss:2.523, val_acc:0.393]
Epoch [3/120    avg_loss:2.309, val_acc:0.432]
Epoch [4/120    avg_loss:2.094, val_acc:0.499]
Epoch [5/120    avg_loss:1.881, val_acc:0.546]
Epoch [6/120    avg_loss:1.733, val_acc:0.601]
Epoch [7/120    avg_loss:1.520, val_acc:0.603]
Epoch [8/120    avg_loss:1.370, val_acc:0.695]
Epoch [9/120    avg_loss:1.166, val_acc:0.701]
Epoch [10/120    avg_loss:1.024, val_acc:0.751]
Epoch [11/120    avg_loss:0.875, val_acc:0.683]
Epoch [12/120    avg_loss:0.833, val_acc:0.787]
Epoch [13/120    avg_loss:0.736, val_acc:0.800]
Epoch [14/120    avg_loss:0.623, val_acc:0.812]
Epoch [15/120    avg_loss:0.704, val_acc:0.805]
Epoch [16/120    avg_loss:0.471, val_acc:0.795]
Epoch [17/120    avg_loss:0.410, val_acc:0.864]
Epoch [18/120    avg_loss:0.412, val_acc:0.881]
Epoch [19/120    avg_loss:0.334, val_acc:0.863]
Epoch [20/120    avg_loss:0.266, val_acc:0.860]
Epoch [21/120    avg_loss:0.294, val_acc:0.901]
Epoch [22/120    avg_loss:0.256, val_acc:0.890]
Epoch [23/120    avg_loss:0.256, val_acc:0.900]
Epoch [24/120    avg_loss:0.198, val_acc:0.872]
Epoch [25/120    avg_loss:0.193, val_acc:0.920]
Epoch [26/120    avg_loss:0.180, val_acc:0.902]
Epoch [27/120    avg_loss:0.164, val_acc:0.905]
Epoch [28/120    avg_loss:0.179, val_acc:0.899]
Epoch [29/120    avg_loss:0.148, val_acc:0.937]
Epoch [30/120    avg_loss:0.120, val_acc:0.931]
Epoch [31/120    avg_loss:0.103, val_acc:0.909]
Epoch [32/120    avg_loss:0.096, val_acc:0.911]
Epoch [33/120    avg_loss:0.117, val_acc:0.918]
Epoch [34/120    avg_loss:0.107, val_acc:0.942]
Epoch [35/120    avg_loss:0.070, val_acc:0.938]
Epoch [36/120    avg_loss:0.066, val_acc:0.939]
Epoch [37/120    avg_loss:0.092, val_acc:0.939]
Epoch [38/120    avg_loss:0.101, val_acc:0.945]
Epoch [39/120    avg_loss:0.088, val_acc:0.926]
Epoch [40/120    avg_loss:0.078, val_acc:0.932]
Epoch [41/120    avg_loss:0.130, val_acc:0.860]
Epoch [42/120    avg_loss:0.131, val_acc:0.928]
Epoch [43/120    avg_loss:0.089, val_acc:0.949]
Epoch [44/120    avg_loss:0.127, val_acc:0.957]
Epoch [45/120    avg_loss:0.067, val_acc:0.946]
Epoch [46/120    avg_loss:0.040, val_acc:0.950]
Epoch [47/120    avg_loss:0.033, val_acc:0.938]
Epoch [48/120    avg_loss:0.055, val_acc:0.950]
Epoch [49/120    avg_loss:0.050, val_acc:0.938]
Epoch [50/120    avg_loss:0.044, val_acc:0.953]
Epoch [51/120    avg_loss:0.051, val_acc:0.947]
Epoch [52/120    avg_loss:0.330, val_acc:0.757]
Epoch [53/120    avg_loss:0.730, val_acc:0.896]
Epoch [54/120    avg_loss:0.152, val_acc:0.910]
Epoch [55/120    avg_loss:0.101, val_acc:0.942]
Epoch [56/120    avg_loss:0.067, val_acc:0.945]
Epoch [57/120    avg_loss:0.064, val_acc:0.940]
Epoch [58/120    avg_loss:0.051, val_acc:0.953]
Epoch [59/120    avg_loss:0.038, val_acc:0.962]
Epoch [60/120    avg_loss:0.031, val_acc:0.964]
Epoch [61/120    avg_loss:0.024, val_acc:0.962]
Epoch [62/120    avg_loss:0.027, val_acc:0.959]
Epoch [63/120    avg_loss:0.020, val_acc:0.962]
Epoch [64/120    avg_loss:0.021, val_acc:0.961]
Epoch [65/120    avg_loss:0.023, val_acc:0.964]
Epoch [66/120    avg_loss:0.020, val_acc:0.963]
Epoch [67/120    avg_loss:0.025, val_acc:0.965]
Epoch [68/120    avg_loss:0.017, val_acc:0.965]
Epoch [69/120    avg_loss:0.020, val_acc:0.964]
Epoch [70/120    avg_loss:0.018, val_acc:0.965]
Epoch [71/120    avg_loss:0.019, val_acc:0.966]
Epoch [72/120    avg_loss:0.020, val_acc:0.965]
Epoch [73/120    avg_loss:0.017, val_acc:0.967]
Epoch [74/120    avg_loss:0.018, val_acc:0.968]
Epoch [75/120    avg_loss:0.019, val_acc:0.965]
Epoch [76/120    avg_loss:0.017, val_acc:0.968]
Epoch [77/120    avg_loss:0.021, val_acc:0.970]
Epoch [78/120    avg_loss:0.019, val_acc:0.967]
Epoch [79/120    avg_loss:0.016, val_acc:0.966]
Epoch [80/120    avg_loss:0.019, val_acc:0.970]
Epoch [81/120    avg_loss:0.016, val_acc:0.970]
Epoch [82/120    avg_loss:0.015, val_acc:0.970]
Epoch [83/120    avg_loss:0.016, val_acc:0.970]
Epoch [84/120    avg_loss:0.018, val_acc:0.971]
Epoch [85/120    avg_loss:0.013, val_acc:0.971]
Epoch [86/120    avg_loss:0.014, val_acc:0.972]
Epoch [87/120    avg_loss:0.014, val_acc:0.971]
Epoch [88/120    avg_loss:0.013, val_acc:0.971]
Epoch [89/120    avg_loss:0.015, val_acc:0.972]
Epoch [90/120    avg_loss:0.014, val_acc:0.972]
Epoch [91/120    avg_loss:0.017, val_acc:0.971]
Epoch [92/120    avg_loss:0.014, val_acc:0.971]
Epoch [93/120    avg_loss:0.013, val_acc:0.971]
Epoch [94/120    avg_loss:0.013, val_acc:0.970]
Epoch [95/120    avg_loss:0.013, val_acc:0.971]
Epoch [96/120    avg_loss:0.016, val_acc:0.972]
Epoch [97/120    avg_loss:0.013, val_acc:0.971]
Epoch [98/120    avg_loss:0.013, val_acc:0.972]
Epoch [99/120    avg_loss:0.013, val_acc:0.971]
Epoch [100/120    avg_loss:0.014, val_acc:0.975]
Epoch [101/120    avg_loss:0.012, val_acc:0.972]
Epoch [102/120    avg_loss:0.017, val_acc:0.968]
Epoch [103/120    avg_loss:0.014, val_acc:0.971]
Epoch [104/120    avg_loss:0.013, val_acc:0.971]
Epoch [105/120    avg_loss:0.013, val_acc:0.973]
Epoch [106/120    avg_loss:0.013, val_acc:0.973]
Epoch [107/120    avg_loss:0.013, val_acc:0.973]
Epoch [108/120    avg_loss:0.014, val_acc:0.974]
Epoch [109/120    avg_loss:0.013, val_acc:0.970]
Epoch [110/120    avg_loss:0.016, val_acc:0.972]
Epoch [111/120    avg_loss:0.015, val_acc:0.972]
Epoch [112/120    avg_loss:0.011, val_acc:0.972]
Epoch [113/120    avg_loss:0.012, val_acc:0.974]
Epoch [114/120    avg_loss:0.011, val_acc:0.974]
Epoch [115/120    avg_loss:0.011, val_acc:0.974]
Epoch [116/120    avg_loss:0.011, val_acc:0.974]
Epoch [117/120    avg_loss:0.012, val_acc:0.973]
Epoch [118/120    avg_loss:0.011, val_acc:0.974]
Epoch [119/120    avg_loss:0.013, val_acc:0.973]
Epoch [120/120    avg_loss:0.010, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0 1256    2    6    0    0    0    0    0    2   19    0    0
     0    0    0]
 [   0    0    0  735    0    0    0    0    0    0    0    2   10    0
     0    0    0]
 [   0    0    0    2  208    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  837   31    0    0
     0    1    0]
 [   0    0    6    0    0   15    0    0    0    0    9 2159   19    0
     2    0    0]
 [   0    0    0    3    0    0    0    0    0    0    2    0  525    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1137    2    0]
 [   0    0    0    0    0    1    9    0    0    0    0    0    0    0
    60  277    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.57181571815718

F1 scores:
[       nan 0.98765432 0.98394046 0.98723976 0.97423888 0.97963801
 0.99090909 1.         1.         1.         0.97043478 0.97581921
 0.96065874 1.         0.97179487 0.88076312 0.97619048]

Kappa:
0.9723034290697709
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f560388ce48>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.760, val_acc:0.400]
Epoch [2/120    avg_loss:2.563, val_acc:0.497]
Epoch [3/120    avg_loss:2.312, val_acc:0.510]
Epoch [4/120    avg_loss:2.087, val_acc:0.522]
Epoch [5/120    avg_loss:1.876, val_acc:0.537]
Epoch [6/120    avg_loss:1.641, val_acc:0.613]
Epoch [7/120    avg_loss:1.471, val_acc:0.539]
Epoch [8/120    avg_loss:1.300, val_acc:0.580]
Epoch [9/120    avg_loss:1.140, val_acc:0.669]
Epoch [10/120    avg_loss:1.080, val_acc:0.722]
Epoch [11/120    avg_loss:0.897, val_acc:0.749]
Epoch [12/120    avg_loss:0.763, val_acc:0.790]
Epoch [13/120    avg_loss:0.729, val_acc:0.736]
Epoch [14/120    avg_loss:0.687, val_acc:0.778]
Epoch [15/120    avg_loss:0.549, val_acc:0.832]
Epoch [16/120    avg_loss:0.497, val_acc:0.817]
Epoch [17/120    avg_loss:0.435, val_acc:0.810]
Epoch [18/120    avg_loss:0.435, val_acc:0.845]
Epoch [19/120    avg_loss:0.327, val_acc:0.900]
Epoch [20/120    avg_loss:0.288, val_acc:0.893]
Epoch [21/120    avg_loss:0.285, val_acc:0.914]
Epoch [22/120    avg_loss:0.246, val_acc:0.894]
Epoch [23/120    avg_loss:0.257, val_acc:0.887]
Epoch [24/120    avg_loss:0.198, val_acc:0.912]
Epoch [25/120    avg_loss:0.167, val_acc:0.922]
Epoch [26/120    avg_loss:0.167, val_acc:0.886]
Epoch [27/120    avg_loss:0.149, val_acc:0.918]
Epoch [28/120    avg_loss:0.198, val_acc:0.943]
Epoch [29/120    avg_loss:0.145, val_acc:0.934]
Epoch [30/120    avg_loss:0.132, val_acc:0.951]
Epoch [31/120    avg_loss:0.099, val_acc:0.939]
Epoch [32/120    avg_loss:0.127, val_acc:0.884]
Epoch [33/120    avg_loss:0.137, val_acc:0.952]
Epoch [34/120    avg_loss:0.141, val_acc:0.936]
Epoch [35/120    avg_loss:0.077, val_acc:0.946]
Epoch [36/120    avg_loss:0.096, val_acc:0.963]
Epoch [37/120    avg_loss:0.106, val_acc:0.947]
Epoch [38/120    avg_loss:0.093, val_acc:0.910]
Epoch [39/120    avg_loss:0.090, val_acc:0.953]
Epoch [40/120    avg_loss:0.107, val_acc:0.942]
Epoch [41/120    avg_loss:0.085, val_acc:0.941]
Epoch [42/120    avg_loss:0.050, val_acc:0.959]
Epoch [43/120    avg_loss:0.107, val_acc:0.946]
Epoch [44/120    avg_loss:0.077, val_acc:0.917]
Epoch [45/120    avg_loss:0.113, val_acc:0.932]
Epoch [46/120    avg_loss:0.086, val_acc:0.952]
Epoch [47/120    avg_loss:0.052, val_acc:0.955]
Epoch [48/120    avg_loss:0.082, val_acc:0.957]
Epoch [49/120    avg_loss:0.049, val_acc:0.965]
Epoch [50/120    avg_loss:0.040, val_acc:0.963]
Epoch [51/120    avg_loss:0.047, val_acc:0.972]
Epoch [52/120    avg_loss:0.033, val_acc:0.965]
Epoch [53/120    avg_loss:0.038, val_acc:0.957]
Epoch [54/120    avg_loss:0.029, val_acc:0.972]
Epoch [55/120    avg_loss:0.027, val_acc:0.975]
Epoch [56/120    avg_loss:0.028, val_acc:0.973]
Epoch [57/120    avg_loss:0.022, val_acc:0.970]
Epoch [58/120    avg_loss:0.049, val_acc:0.952]
Epoch [59/120    avg_loss:0.182, val_acc:0.911]
Epoch [60/120    avg_loss:0.090, val_acc:0.926]
Epoch [61/120    avg_loss:0.099, val_acc:0.944]
Epoch [62/120    avg_loss:0.061, val_acc:0.946]
Epoch [63/120    avg_loss:0.039, val_acc:0.972]
Epoch [64/120    avg_loss:0.036, val_acc:0.965]
Epoch [65/120    avg_loss:0.032, val_acc:0.976]
Epoch [66/120    avg_loss:0.027, val_acc:0.959]
Epoch [67/120    avg_loss:0.018, val_acc:0.975]
Epoch [68/120    avg_loss:0.027, val_acc:0.967]
Epoch [69/120    avg_loss:0.015, val_acc:0.977]
Epoch [70/120    avg_loss:0.013, val_acc:0.976]
Epoch [71/120    avg_loss:0.013, val_acc:0.974]
Epoch [72/120    avg_loss:0.032, val_acc:0.974]
Epoch [73/120    avg_loss:0.020, val_acc:0.972]
Epoch [74/120    avg_loss:0.020, val_acc:0.975]
Epoch [75/120    avg_loss:0.031, val_acc:0.970]
Epoch [76/120    avg_loss:0.016, val_acc:0.973]
Epoch [77/120    avg_loss:0.012, val_acc:0.974]
Epoch [78/120    avg_loss:0.027, val_acc:0.973]
Epoch [79/120    avg_loss:0.019, val_acc:0.975]
Epoch [80/120    avg_loss:0.018, val_acc:0.978]
Epoch [81/120    avg_loss:0.011, val_acc:0.975]
Epoch [82/120    avg_loss:0.016, val_acc:0.974]
Epoch [83/120    avg_loss:0.008, val_acc:0.976]
Epoch [84/120    avg_loss:0.008, val_acc:0.980]
Epoch [85/120    avg_loss:0.006, val_acc:0.979]
Epoch [86/120    avg_loss:0.007, val_acc:0.978]
Epoch [87/120    avg_loss:0.016, val_acc:0.959]
Epoch [88/120    avg_loss:0.014, val_acc:0.970]
Epoch [89/120    avg_loss:0.019, val_acc:0.956]
Epoch [90/120    avg_loss:0.012, val_acc:0.974]
Epoch [91/120    avg_loss:0.017, val_acc:0.970]
Epoch [92/120    avg_loss:0.014, val_acc:0.976]
Epoch [93/120    avg_loss:0.008, val_acc:0.980]
Epoch [94/120    avg_loss:0.008, val_acc:0.975]
Epoch [95/120    avg_loss:0.007, val_acc:0.973]
Epoch [96/120    avg_loss:0.011, val_acc:0.966]
Epoch [97/120    avg_loss:0.007, val_acc:0.974]
Epoch [98/120    avg_loss:0.006, val_acc:0.976]
Epoch [99/120    avg_loss:0.009, val_acc:0.971]
Epoch [100/120    avg_loss:0.044, val_acc:0.971]
Epoch [101/120    avg_loss:0.020, val_acc:0.971]
Epoch [102/120    avg_loss:0.021, val_acc:0.964]
Epoch [103/120    avg_loss:0.035, val_acc:0.976]
Epoch [104/120    avg_loss:0.012, val_acc:0.980]
Epoch [105/120    avg_loss:0.673, val_acc:0.940]
Epoch [106/120    avg_loss:0.072, val_acc:0.947]
Epoch [107/120    avg_loss:0.033, val_acc:0.967]
Epoch [108/120    avg_loss:0.033, val_acc:0.961]
Epoch [109/120    avg_loss:0.039, val_acc:0.966]
Epoch [110/120    avg_loss:0.034, val_acc:0.974]
Epoch [111/120    avg_loss:0.059, val_acc:0.924]
Epoch [112/120    avg_loss:0.037, val_acc:0.953]
Epoch [113/120    avg_loss:0.020, val_acc:0.938]
Epoch [114/120    avg_loss:0.034, val_acc:0.961]
Epoch [115/120    avg_loss:0.028, val_acc:0.972]
Epoch [116/120    avg_loss:0.026, val_acc:0.972]
Epoch [117/120    avg_loss:0.010, val_acc:0.974]
Epoch [118/120    avg_loss:0.008, val_acc:0.973]
Epoch [119/120    avg_loss:0.006, val_acc:0.974]
Epoch [120/120    avg_loss:0.007, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1233   10   12    0    0    0    0    0    3   26    1    0
     0    0    0]
 [   0    0    3  723    4    0    0    0    0    7    2    3    4    0
     0    1    0]
 [   0    0    1    0  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    0    0    0    0    0
     0    1    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    1  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    0    0
     0    1    0]
 [   0    0    2    0    0    0    0    0    0    1  834   36    1    0
     0    1    0]
 [   0    0   11    0    0    1    0    0    0    0   14 2175    9    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    4    1    0  524    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1131    7    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    52  282    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.38753387533875

F1 scores:
[       nan 0.98765432 0.97278107 0.9757085  0.96145125 0.99654776
 0.98945783 0.96153846 0.99883586 0.72340426 0.96416185 0.97752809
 0.97670084 1.         0.97374085 0.88125    0.98823529]

Kappa:
0.9701987653779015
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4d6bb49ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.739, val_acc:0.215]
Epoch [2/120    avg_loss:2.538, val_acc:0.328]
Epoch [3/120    avg_loss:2.316, val_acc:0.440]
Epoch [4/120    avg_loss:2.090, val_acc:0.507]
Epoch [5/120    avg_loss:1.906, val_acc:0.575]
Epoch [6/120    avg_loss:1.701, val_acc:0.591]
Epoch [7/120    avg_loss:1.528, val_acc:0.693]
Epoch [8/120    avg_loss:1.347, val_acc:0.534]
Epoch [9/120    avg_loss:1.289, val_acc:0.717]
Epoch [10/120    avg_loss:1.077, val_acc:0.731]
Epoch [11/120    avg_loss:0.864, val_acc:0.780]
Epoch [12/120    avg_loss:0.818, val_acc:0.803]
Epoch [13/120    avg_loss:0.607, val_acc:0.842]
Epoch [14/120    avg_loss:0.521, val_acc:0.819]
Epoch [15/120    avg_loss:0.527, val_acc:0.862]
Epoch [16/120    avg_loss:0.428, val_acc:0.871]
Epoch [17/120    avg_loss:0.331, val_acc:0.900]
Epoch [18/120    avg_loss:0.290, val_acc:0.896]
Epoch [19/120    avg_loss:0.296, val_acc:0.922]
Epoch [20/120    avg_loss:0.287, val_acc:0.843]
Epoch [21/120    avg_loss:0.299, val_acc:0.895]
Epoch [22/120    avg_loss:0.202, val_acc:0.917]
Epoch [23/120    avg_loss:0.181, val_acc:0.899]
Epoch [24/120    avg_loss:0.146, val_acc:0.929]
Epoch [25/120    avg_loss:0.135, val_acc:0.943]
Epoch [26/120    avg_loss:0.133, val_acc:0.934]
Epoch [27/120    avg_loss:0.134, val_acc:0.835]
Epoch [28/120    avg_loss:0.190, val_acc:0.945]
Epoch [29/120    avg_loss:0.162, val_acc:0.941]
Epoch [30/120    avg_loss:0.234, val_acc:0.925]
Epoch [31/120    avg_loss:0.173, val_acc:0.950]
Epoch [32/120    avg_loss:0.114, val_acc:0.948]
Epoch [33/120    avg_loss:0.093, val_acc:0.934]
Epoch [34/120    avg_loss:0.066, val_acc:0.957]
Epoch [35/120    avg_loss:0.060, val_acc:0.959]
Epoch [36/120    avg_loss:0.067, val_acc:0.949]
Epoch [37/120    avg_loss:0.061, val_acc:0.968]
Epoch [38/120    avg_loss:0.079, val_acc:0.957]
Epoch [39/120    avg_loss:0.044, val_acc:0.970]
Epoch [40/120    avg_loss:0.033, val_acc:0.964]
Epoch [41/120    avg_loss:0.038, val_acc:0.969]
Epoch [42/120    avg_loss:0.039, val_acc:0.960]
Epoch [43/120    avg_loss:0.042, val_acc:0.969]
Epoch [44/120    avg_loss:0.064, val_acc:0.949]
Epoch [45/120    avg_loss:0.109, val_acc:0.962]
Epoch [46/120    avg_loss:0.067, val_acc:0.970]
Epoch [47/120    avg_loss:0.050, val_acc:0.961]
Epoch [48/120    avg_loss:0.034, val_acc:0.964]
Epoch [49/120    avg_loss:0.041, val_acc:0.970]
Epoch [50/120    avg_loss:0.040, val_acc:0.961]
Epoch [51/120    avg_loss:0.041, val_acc:0.978]
Epoch [52/120    avg_loss:0.056, val_acc:0.946]
Epoch [53/120    avg_loss:0.045, val_acc:0.967]
Epoch [54/120    avg_loss:0.047, val_acc:0.964]
Epoch [55/120    avg_loss:0.025, val_acc:0.960]
Epoch [56/120    avg_loss:0.033, val_acc:0.969]
Epoch [57/120    avg_loss:0.038, val_acc:0.946]
Epoch [58/120    avg_loss:0.035, val_acc:0.974]
Epoch [59/120    avg_loss:0.019, val_acc:0.975]
Epoch [60/120    avg_loss:0.024, val_acc:0.970]
Epoch [61/120    avg_loss:0.019, val_acc:0.968]
Epoch [62/120    avg_loss:0.037, val_acc:0.970]
Epoch [63/120    avg_loss:0.020, val_acc:0.959]
Epoch [64/120    avg_loss:0.073, val_acc:0.958]
Epoch [65/120    avg_loss:0.040, val_acc:0.968]
Epoch [66/120    avg_loss:0.023, val_acc:0.974]
Epoch [67/120    avg_loss:0.021, val_acc:0.977]
Epoch [68/120    avg_loss:0.016, val_acc:0.978]
Epoch [69/120    avg_loss:0.015, val_acc:0.980]
Epoch [70/120    avg_loss:0.014, val_acc:0.980]
Epoch [71/120    avg_loss:0.022, val_acc:0.979]
Epoch [72/120    avg_loss:0.015, val_acc:0.977]
Epoch [73/120    avg_loss:0.012, val_acc:0.981]
Epoch [74/120    avg_loss:0.012, val_acc:0.982]
Epoch [75/120    avg_loss:0.013, val_acc:0.980]
Epoch [76/120    avg_loss:0.012, val_acc:0.980]
Epoch [77/120    avg_loss:0.011, val_acc:0.982]
Epoch [78/120    avg_loss:0.010, val_acc:0.984]
Epoch [79/120    avg_loss:0.009, val_acc:0.982]
Epoch [80/120    avg_loss:0.009, val_acc:0.983]
Epoch [81/120    avg_loss:0.009, val_acc:0.981]
Epoch [82/120    avg_loss:0.011, val_acc:0.981]
Epoch [83/120    avg_loss:0.010, val_acc:0.984]
Epoch [84/120    avg_loss:0.012, val_acc:0.982]
Epoch [85/120    avg_loss:0.009, val_acc:0.981]
Epoch [86/120    avg_loss:0.009, val_acc:0.981]
Epoch [87/120    avg_loss:0.016, val_acc:0.982]
Epoch [88/120    avg_loss:0.008, val_acc:0.983]
Epoch [89/120    avg_loss:0.009, val_acc:0.982]
Epoch [90/120    avg_loss:0.008, val_acc:0.982]
Epoch [91/120    avg_loss:0.010, val_acc:0.983]
Epoch [92/120    avg_loss:0.008, val_acc:0.983]
Epoch [93/120    avg_loss:0.009, val_acc:0.984]
Epoch [94/120    avg_loss:0.009, val_acc:0.983]
Epoch [95/120    avg_loss:0.009, val_acc:0.983]
Epoch [96/120    avg_loss:0.009, val_acc:0.984]
Epoch [97/120    avg_loss:0.009, val_acc:0.982]
Epoch [98/120    avg_loss:0.007, val_acc:0.982]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.008, val_acc:0.984]
Epoch [101/120    avg_loss:0.009, val_acc:0.983]
Epoch [102/120    avg_loss:0.011, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.984]
Epoch [104/120    avg_loss:0.007, val_acc:0.983]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.983]
Epoch [107/120    avg_loss:0.006, val_acc:0.983]
Epoch [108/120    avg_loss:0.007, val_acc:0.983]
Epoch [109/120    avg_loss:0.007, val_acc:0.983]
Epoch [110/120    avg_loss:0.007, val_acc:0.984]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.010, val_acc:0.982]
Epoch [113/120    avg_loss:0.007, val_acc:0.982]
Epoch [114/120    avg_loss:0.008, val_acc:0.981]
Epoch [115/120    avg_loss:0.007, val_acc:0.983]
Epoch [116/120    avg_loss:0.008, val_acc:0.982]
Epoch [117/120    avg_loss:0.006, val_acc:0.983]
Epoch [118/120    avg_loss:0.007, val_acc:0.983]
Epoch [119/120    avg_loss:0.009, val_acc:0.980]
Epoch [120/120    avg_loss:0.006, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1260    0    4    0    1    0    0    0    1   19    0    0
     0    0    0]
 [   0    0    0  726    9    5    0    0    0    0    1    2    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    1    1    0    0    0  853   14    0    0
     0    0    0]
 [   0    0   15    0    0    0    1    0    0    0   17 2176    0    0
     1    0    0]
 [   0    0    2    2    0    2    0    0    0    0    1    1  521    0
     1    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1122   17    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    91  247    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.37669376693766

F1 scores:
[       nan 0.98765432 0.98130841 0.98373984 0.97038724 0.98742857
 0.98942598 1.         0.99649942 0.97142857 0.97541452 0.98372514
 0.98116761 1.         0.9520577  0.80587276 0.98823529]

Kappa:
0.9700674850200665
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:01:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb27de02ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.734, val_acc:0.421]
Epoch [2/120    avg_loss:2.502, val_acc:0.456]
Epoch [3/120    avg_loss:2.283, val_acc:0.486]
Epoch [4/120    avg_loss:2.074, val_acc:0.526]
Epoch [5/120    avg_loss:1.843, val_acc:0.607]
Epoch [6/120    avg_loss:1.592, val_acc:0.664]
Epoch [7/120    avg_loss:1.389, val_acc:0.645]
Epoch [8/120    avg_loss:1.176, val_acc:0.654]
Epoch [9/120    avg_loss:0.993, val_acc:0.696]
Epoch [10/120    avg_loss:0.860, val_acc:0.740]
Epoch [11/120    avg_loss:0.795, val_acc:0.772]
Epoch [12/120    avg_loss:0.632, val_acc:0.796]
Epoch [13/120    avg_loss:0.544, val_acc:0.812]
Epoch [14/120    avg_loss:0.504, val_acc:0.838]
Epoch [15/120    avg_loss:0.437, val_acc:0.826]
Epoch [16/120    avg_loss:0.416, val_acc:0.842]
Epoch [17/120    avg_loss:0.323, val_acc:0.828]
Epoch [18/120    avg_loss:0.263, val_acc:0.902]
Epoch [19/120    avg_loss:0.293, val_acc:0.887]
Epoch [20/120    avg_loss:0.287, val_acc:0.890]
Epoch [21/120    avg_loss:0.223, val_acc:0.835]
Epoch [22/120    avg_loss:0.350, val_acc:0.814]
Epoch [23/120    avg_loss:0.267, val_acc:0.878]
Epoch [24/120    avg_loss:0.221, val_acc:0.901]
Epoch [25/120    avg_loss:0.186, val_acc:0.885]
Epoch [26/120    avg_loss:0.177, val_acc:0.916]
Epoch [27/120    avg_loss:0.154, val_acc:0.843]
Epoch [28/120    avg_loss:0.168, val_acc:0.932]
Epoch [29/120    avg_loss:0.097, val_acc:0.932]
Epoch [30/120    avg_loss:0.090, val_acc:0.929]
Epoch [31/120    avg_loss:0.063, val_acc:0.942]
Epoch [32/120    avg_loss:0.072, val_acc:0.935]
Epoch [33/120    avg_loss:0.087, val_acc:0.897]
Epoch [34/120    avg_loss:0.076, val_acc:0.933]
Epoch [35/120    avg_loss:0.076, val_acc:0.959]
Epoch [36/120    avg_loss:0.074, val_acc:0.950]
Epoch [37/120    avg_loss:0.050, val_acc:0.935]
Epoch [38/120    avg_loss:0.037, val_acc:0.966]
Epoch [39/120    avg_loss:0.029, val_acc:0.974]
Epoch [40/120    avg_loss:0.042, val_acc:0.958]
Epoch [41/120    avg_loss:0.039, val_acc:0.965]
Epoch [42/120    avg_loss:0.044, val_acc:0.954]
Epoch [43/120    avg_loss:0.106, val_acc:0.917]
Epoch [44/120    avg_loss:0.072, val_acc:0.944]
Epoch [45/120    avg_loss:0.079, val_acc:0.953]
Epoch [46/120    avg_loss:0.053, val_acc:0.963]
Epoch [47/120    avg_loss:0.042, val_acc:0.943]
Epoch [48/120    avg_loss:0.026, val_acc:0.965]
Epoch [49/120    avg_loss:0.022, val_acc:0.950]
Epoch [50/120    avg_loss:0.028, val_acc:0.969]
Epoch [51/120    avg_loss:0.023, val_acc:0.938]
Epoch [52/120    avg_loss:0.036, val_acc:0.967]
Epoch [53/120    avg_loss:0.020, val_acc:0.971]
Epoch [54/120    avg_loss:0.016, val_acc:0.973]
Epoch [55/120    avg_loss:0.020, val_acc:0.976]
Epoch [56/120    avg_loss:0.013, val_acc:0.976]
Epoch [57/120    avg_loss:0.018, val_acc:0.974]
Epoch [58/120    avg_loss:0.012, val_acc:0.976]
Epoch [59/120    avg_loss:0.017, val_acc:0.974]
Epoch [60/120    avg_loss:0.012, val_acc:0.974]
Epoch [61/120    avg_loss:0.012, val_acc:0.974]
Epoch [62/120    avg_loss:0.011, val_acc:0.973]
Epoch [63/120    avg_loss:0.017, val_acc:0.975]
Epoch [64/120    avg_loss:0.011, val_acc:0.975]
Epoch [65/120    avg_loss:0.011, val_acc:0.975]
Epoch [66/120    avg_loss:0.011, val_acc:0.977]
Epoch [67/120    avg_loss:0.010, val_acc:0.977]
Epoch [68/120    avg_loss:0.012, val_acc:0.977]
Epoch [69/120    avg_loss:0.013, val_acc:0.975]
Epoch [70/120    avg_loss:0.015, val_acc:0.977]
Epoch [71/120    avg_loss:0.009, val_acc:0.977]
Epoch [72/120    avg_loss:0.010, val_acc:0.976]
Epoch [73/120    avg_loss:0.009, val_acc:0.976]
Epoch [74/120    avg_loss:0.011, val_acc:0.976]
Epoch [75/120    avg_loss:0.011, val_acc:0.975]
Epoch [76/120    avg_loss:0.008, val_acc:0.975]
Epoch [77/120    avg_loss:0.010, val_acc:0.976]
Epoch [78/120    avg_loss:0.011, val_acc:0.976]
Epoch [79/120    avg_loss:0.008, val_acc:0.977]
Epoch [80/120    avg_loss:0.010, val_acc:0.975]
Epoch [81/120    avg_loss:0.009, val_acc:0.976]
Epoch [82/120    avg_loss:0.007, val_acc:0.976]
Epoch [83/120    avg_loss:0.008, val_acc:0.976]
Epoch [84/120    avg_loss:0.009, val_acc:0.977]
Epoch [85/120    avg_loss:0.009, val_acc:0.976]
Epoch [86/120    avg_loss:0.009, val_acc:0.974]
Epoch [87/120    avg_loss:0.010, val_acc:0.976]
Epoch [88/120    avg_loss:0.007, val_acc:0.975]
Epoch [89/120    avg_loss:0.008, val_acc:0.976]
Epoch [90/120    avg_loss:0.009, val_acc:0.976]
Epoch [91/120    avg_loss:0.009, val_acc:0.976]
Epoch [92/120    avg_loss:0.007, val_acc:0.976]
Epoch [93/120    avg_loss:0.014, val_acc:0.977]
Epoch [94/120    avg_loss:0.008, val_acc:0.976]
Epoch [95/120    avg_loss:0.009, val_acc:0.977]
Epoch [96/120    avg_loss:0.007, val_acc:0.977]
Epoch [97/120    avg_loss:0.007, val_acc:0.978]
Epoch [98/120    avg_loss:0.008, val_acc:0.977]
Epoch [99/120    avg_loss:0.007, val_acc:0.977]
Epoch [100/120    avg_loss:0.008, val_acc:0.977]
Epoch [101/120    avg_loss:0.009, val_acc:0.978]
Epoch [102/120    avg_loss:0.007, val_acc:0.978]
Epoch [103/120    avg_loss:0.008, val_acc:0.978]
Epoch [104/120    avg_loss:0.008, val_acc:0.977]
Epoch [105/120    avg_loss:0.010, val_acc:0.978]
Epoch [106/120    avg_loss:0.008, val_acc:0.977]
Epoch [107/120    avg_loss:0.006, val_acc:0.978]
Epoch [108/120    avg_loss:0.008, val_acc:0.978]
Epoch [109/120    avg_loss:0.008, val_acc:0.978]
Epoch [110/120    avg_loss:0.008, val_acc:0.978]
Epoch [111/120    avg_loss:0.007, val_acc:0.976]
Epoch [112/120    avg_loss:0.010, val_acc:0.979]
Epoch [113/120    avg_loss:0.007, val_acc:0.979]
Epoch [114/120    avg_loss:0.007, val_acc:0.979]
Epoch [115/120    avg_loss:0.008, val_acc:0.979]
Epoch [116/120    avg_loss:0.008, val_acc:0.978]
Epoch [117/120    avg_loss:0.007, val_acc:0.979]
Epoch [118/120    avg_loss:0.007, val_acc:0.978]
Epoch [119/120    avg_loss:0.008, val_acc:0.978]
Epoch [120/120    avg_loss:0.006, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0 1236    3    5    4    0    0    0    0    7   29    1    0
     0    0    0]
 [   0    0    2  723    6    0    2    0    0    5    4    2    3    0
     0    0    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    1  655    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  423    0    0    0    7    0
     0    0    0]
 [   0    0    0    0    0    0   10    0    0    7    0    0    1    0
     0    0    0]
 [   0    0    6    0    0    2    1    0    0    0  844   21    0    0
     0    1    0]
 [   0    0    1    0    0    0    3    0    0    0    7 2192    7    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  533    0
     1    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1127   11    0]
 [   0    0    0    0    0    2   16    0    0    0    0    0    0    0
    52  277    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.48509485094851

F1 scores:
[       nan 0.975      0.9770751  0.98100407 0.97011494 0.98861048
 0.97470238 1.         0.99063232 0.4516129  0.97179044 0.98428379
 0.97708524 1.         0.97155172 0.87106918 0.98795181]

Kappa:
0.9713048836669254
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd1436d6ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.774, val_acc:0.335]
Epoch [2/120    avg_loss:2.626, val_acc:0.475]
Epoch [3/120    avg_loss:2.438, val_acc:0.499]
Epoch [4/120    avg_loss:2.232, val_acc:0.532]
Epoch [5/120    avg_loss:2.002, val_acc:0.584]
Epoch [6/120    avg_loss:1.764, val_acc:0.655]
Epoch [7/120    avg_loss:1.543, val_acc:0.654]
Epoch [8/120    avg_loss:1.340, val_acc:0.669]
Epoch [9/120    avg_loss:1.180, val_acc:0.685]
Epoch [10/120    avg_loss:1.048, val_acc:0.738]
Epoch [11/120    avg_loss:0.914, val_acc:0.767]
Epoch [12/120    avg_loss:0.813, val_acc:0.768]
Epoch [13/120    avg_loss:0.719, val_acc:0.792]
Epoch [14/120    avg_loss:0.589, val_acc:0.810]
Epoch [15/120    avg_loss:0.542, val_acc:0.858]
Epoch [16/120    avg_loss:0.421, val_acc:0.864]
Epoch [17/120    avg_loss:0.505, val_acc:0.857]
Epoch [18/120    avg_loss:0.414, val_acc:0.862]
Epoch [19/120    avg_loss:0.362, val_acc:0.882]
Epoch [20/120    avg_loss:0.277, val_acc:0.922]
Epoch [21/120    avg_loss:0.294, val_acc:0.886]
Epoch [22/120    avg_loss:0.229, val_acc:0.883]
Epoch [23/120    avg_loss:0.284, val_acc:0.924]
Epoch [24/120    avg_loss:0.218, val_acc:0.922]
Epoch [25/120    avg_loss:0.226, val_acc:0.919]
Epoch [26/120    avg_loss:0.206, val_acc:0.933]
Epoch [27/120    avg_loss:0.168, val_acc:0.916]
Epoch [28/120    avg_loss:0.154, val_acc:0.928]
Epoch [29/120    avg_loss:0.177, val_acc:0.935]
Epoch [30/120    avg_loss:0.096, val_acc:0.941]
Epoch [31/120    avg_loss:0.095, val_acc:0.949]
Epoch [32/120    avg_loss:0.102, val_acc:0.956]
Epoch [33/120    avg_loss:0.087, val_acc:0.956]
Epoch [34/120    avg_loss:0.089, val_acc:0.946]
Epoch [35/120    avg_loss:0.070, val_acc:0.933]
Epoch [36/120    avg_loss:0.069, val_acc:0.959]
Epoch [37/120    avg_loss:0.064, val_acc:0.963]
Epoch [38/120    avg_loss:0.063, val_acc:0.953]
Epoch [39/120    avg_loss:0.094, val_acc:0.963]
Epoch [40/120    avg_loss:0.077, val_acc:0.971]
Epoch [41/120    avg_loss:0.070, val_acc:0.965]
Epoch [42/120    avg_loss:0.055, val_acc:0.971]
Epoch [43/120    avg_loss:0.047, val_acc:0.949]
Epoch [44/120    avg_loss:0.051, val_acc:0.963]
Epoch [45/120    avg_loss:0.100, val_acc:0.962]
Epoch [46/120    avg_loss:0.064, val_acc:0.959]
Epoch [47/120    avg_loss:0.034, val_acc:0.964]
Epoch [48/120    avg_loss:0.028, val_acc:0.978]
Epoch [49/120    avg_loss:0.036, val_acc:0.967]
Epoch [50/120    avg_loss:0.031, val_acc:0.970]
Epoch [51/120    avg_loss:0.071, val_acc:0.968]
Epoch [52/120    avg_loss:0.027, val_acc:0.976]
Epoch [53/120    avg_loss:0.022, val_acc:0.981]
Epoch [54/120    avg_loss:0.022, val_acc:0.964]
Epoch [55/120    avg_loss:0.045, val_acc:0.967]
Epoch [56/120    avg_loss:0.047, val_acc:0.941]
Epoch [57/120    avg_loss:0.048, val_acc:0.970]
Epoch [58/120    avg_loss:0.030, val_acc:0.978]
Epoch [59/120    avg_loss:0.026, val_acc:0.980]
Epoch [60/120    avg_loss:0.025, val_acc:0.957]
Epoch [61/120    avg_loss:0.037, val_acc:0.981]
Epoch [62/120    avg_loss:0.020, val_acc:0.972]
Epoch [63/120    avg_loss:0.013, val_acc:0.977]
Epoch [64/120    avg_loss:0.075, val_acc:0.951]
Epoch [65/120    avg_loss:0.050, val_acc:0.958]
Epoch [66/120    avg_loss:0.031, val_acc:0.968]
Epoch [67/120    avg_loss:0.036, val_acc:0.970]
Epoch [68/120    avg_loss:0.028, val_acc:0.978]
Epoch [69/120    avg_loss:0.037, val_acc:0.969]
Epoch [70/120    avg_loss:0.014, val_acc:0.979]
Epoch [71/120    avg_loss:0.012, val_acc:0.979]
Epoch [72/120    avg_loss:0.013, val_acc:0.979]
Epoch [73/120    avg_loss:0.014, val_acc:0.979]
Epoch [74/120    avg_loss:0.017, val_acc:0.977]
Epoch [75/120    avg_loss:0.012, val_acc:0.984]
Epoch [76/120    avg_loss:0.010, val_acc:0.984]
Epoch [77/120    avg_loss:0.011, val_acc:0.982]
Epoch [78/120    avg_loss:0.007, val_acc:0.982]
Epoch [79/120    avg_loss:0.011, val_acc:0.982]
Epoch [80/120    avg_loss:0.010, val_acc:0.982]
Epoch [81/120    avg_loss:0.006, val_acc:0.982]
Epoch [82/120    avg_loss:0.006, val_acc:0.982]
Epoch [83/120    avg_loss:0.014, val_acc:0.983]
Epoch [84/120    avg_loss:0.009, val_acc:0.983]
Epoch [85/120    avg_loss:0.008, val_acc:0.984]
Epoch [86/120    avg_loss:0.008, val_acc:0.984]
Epoch [87/120    avg_loss:0.007, val_acc:0.984]
Epoch [88/120    avg_loss:0.008, val_acc:0.983]
Epoch [89/120    avg_loss:0.006, val_acc:0.983]
Epoch [90/120    avg_loss:0.006, val_acc:0.984]
Epoch [91/120    avg_loss:0.007, val_acc:0.981]
Epoch [92/120    avg_loss:0.007, val_acc:0.983]
Epoch [93/120    avg_loss:0.006, val_acc:0.981]
Epoch [94/120    avg_loss:0.007, val_acc:0.982]
Epoch [95/120    avg_loss:0.006, val_acc:0.983]
Epoch [96/120    avg_loss:0.011, val_acc:0.984]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.009, val_acc:0.984]
Epoch [99/120    avg_loss:0.006, val_acc:0.985]
Epoch [100/120    avg_loss:0.006, val_acc:0.985]
Epoch [101/120    avg_loss:0.005, val_acc:0.985]
Epoch [102/120    avg_loss:0.006, val_acc:0.983]
Epoch [103/120    avg_loss:0.005, val_acc:0.984]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.006, val_acc:0.983]
Epoch [106/120    avg_loss:0.006, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.986]
Epoch [108/120    avg_loss:0.006, val_acc:0.985]
Epoch [109/120    avg_loss:0.008, val_acc:0.985]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.008, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.006, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1242    4    9    2    0    0    0    0    5   23    0    0
     0    0    0]
 [   0    0    0  725   10    0    0    0    0    6    3    1    1    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    1    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    1    3    0    0    0  832   37    0    0
     0    0    0]
 [   0    0    5    0    0    5    0    0    0    4   10 2171   14    0
     1    0    0]
 [   0    0    0    0    0    0    0    0    0    0    5    0  525    0
     1    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    0    0    0    3    0    0    0    0
    50  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.4308943089431

F1 scores:
[       nan 1.         0.98026835 0.98238482 0.95730337 0.98394495
 0.99469295 1.         0.9953271  0.63829787 0.96184971 0.97726761
 0.97312326 0.99730458 0.96985357 0.89634146 0.98809524]

Kappa:
0.9707007194664765
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb13e991ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.781, val_acc:0.392]
Epoch [2/120    avg_loss:2.620, val_acc:0.465]
Epoch [3/120    avg_loss:2.400, val_acc:0.493]
Epoch [4/120    avg_loss:2.162, val_acc:0.550]
Epoch [5/120    avg_loss:1.962, val_acc:0.579]
Epoch [6/120    avg_loss:1.729, val_acc:0.618]
Epoch [7/120    avg_loss:1.570, val_acc:0.602]
Epoch [8/120    avg_loss:1.441, val_acc:0.681]
Epoch [9/120    avg_loss:1.192, val_acc:0.690]
Epoch [10/120    avg_loss:1.090, val_acc:0.746]
Epoch [11/120    avg_loss:0.918, val_acc:0.767]
Epoch [12/120    avg_loss:0.770, val_acc:0.755]
Epoch [13/120    avg_loss:0.709, val_acc:0.832]
Epoch [14/120    avg_loss:0.557, val_acc:0.838]
Epoch [15/120    avg_loss:0.543, val_acc:0.783]
Epoch [16/120    avg_loss:0.486, val_acc:0.855]
Epoch [17/120    avg_loss:0.390, val_acc:0.849]
Epoch [18/120    avg_loss:0.336, val_acc:0.834]
Epoch [19/120    avg_loss:0.343, val_acc:0.871]
Epoch [20/120    avg_loss:0.337, val_acc:0.874]
Epoch [21/120    avg_loss:0.643, val_acc:0.845]
Epoch [22/120    avg_loss:0.366, val_acc:0.870]
Epoch [23/120    avg_loss:0.248, val_acc:0.807]
Epoch [24/120    avg_loss:0.289, val_acc:0.883]
Epoch [25/120    avg_loss:0.201, val_acc:0.930]
Epoch [26/120    avg_loss:0.261, val_acc:0.886]
Epoch [27/120    avg_loss:0.233, val_acc:0.917]
Epoch [28/120    avg_loss:0.165, val_acc:0.928]
Epoch [29/120    avg_loss:0.189, val_acc:0.915]
Epoch [30/120    avg_loss:0.155, val_acc:0.937]
Epoch [31/120    avg_loss:0.201, val_acc:0.905]
Epoch [32/120    avg_loss:0.178, val_acc:0.926]
Epoch [33/120    avg_loss:0.099, val_acc:0.957]
Epoch [34/120    avg_loss:0.085, val_acc:0.908]
Epoch [35/120    avg_loss:0.280, val_acc:0.887]
Epoch [36/120    avg_loss:0.270, val_acc:0.918]
Epoch [37/120    avg_loss:0.168, val_acc:0.894]
Epoch [38/120    avg_loss:0.146, val_acc:0.942]
Epoch [39/120    avg_loss:0.119, val_acc:0.945]
Epoch [40/120    avg_loss:0.104, val_acc:0.940]
Epoch [41/120    avg_loss:0.090, val_acc:0.946]
Epoch [42/120    avg_loss:0.107, val_acc:0.960]
Epoch [43/120    avg_loss:0.071, val_acc:0.929]
Epoch [44/120    avg_loss:0.108, val_acc:0.938]
Epoch [45/120    avg_loss:0.144, val_acc:0.905]
Epoch [46/120    avg_loss:0.096, val_acc:0.947]
Epoch [47/120    avg_loss:0.057, val_acc:0.958]
Epoch [48/120    avg_loss:0.067, val_acc:0.953]
Epoch [49/120    avg_loss:0.077, val_acc:0.943]
Epoch [50/120    avg_loss:0.052, val_acc:0.960]
Epoch [51/120    avg_loss:0.035, val_acc:0.959]
Epoch [52/120    avg_loss:0.060, val_acc:0.958]
Epoch [53/120    avg_loss:0.053, val_acc:0.958]
Epoch [54/120    avg_loss:0.033, val_acc:0.960]
Epoch [55/120    avg_loss:0.048, val_acc:0.959]
Epoch [56/120    avg_loss:0.032, val_acc:0.963]
Epoch [57/120    avg_loss:0.043, val_acc:0.942]
Epoch [58/120    avg_loss:0.053, val_acc:0.953]
Epoch [59/120    avg_loss:0.051, val_acc:0.936]
Epoch [60/120    avg_loss:0.043, val_acc:0.956]
Epoch [61/120    avg_loss:0.028, val_acc:0.968]
Epoch [62/120    avg_loss:0.030, val_acc:0.956]
Epoch [63/120    avg_loss:0.022, val_acc:0.935]
Epoch [64/120    avg_loss:0.040, val_acc:0.961]
Epoch [65/120    avg_loss:0.029, val_acc:0.967]
Epoch [66/120    avg_loss:0.027, val_acc:0.962]
Epoch [67/120    avg_loss:0.032, val_acc:0.960]
Epoch [68/120    avg_loss:0.023, val_acc:0.953]
Epoch [69/120    avg_loss:0.026, val_acc:0.960]
Epoch [70/120    avg_loss:0.022, val_acc:0.966]
Epoch [71/120    avg_loss:0.017, val_acc:0.961]
Epoch [72/120    avg_loss:0.017, val_acc:0.971]
Epoch [73/120    avg_loss:0.022, val_acc:0.970]
Epoch [74/120    avg_loss:0.037, val_acc:0.951]
Epoch [75/120    avg_loss:0.039, val_acc:0.958]
Epoch [76/120    avg_loss:0.054, val_acc:0.954]
Epoch [77/120    avg_loss:0.054, val_acc:0.952]
Epoch [78/120    avg_loss:0.028, val_acc:0.955]
Epoch [79/120    avg_loss:0.058, val_acc:0.960]
Epoch [80/120    avg_loss:0.019, val_acc:0.971]
Epoch [81/120    avg_loss:0.015, val_acc:0.971]
Epoch [82/120    avg_loss:0.041, val_acc:0.953]
Epoch [83/120    avg_loss:0.031, val_acc:0.966]
Epoch [84/120    avg_loss:0.055, val_acc:0.951]
Epoch [85/120    avg_loss:0.038, val_acc:0.963]
Epoch [86/120    avg_loss:0.024, val_acc:0.954]
Epoch [87/120    avg_loss:0.034, val_acc:0.966]
Epoch [88/120    avg_loss:0.048, val_acc:0.959]
Epoch [89/120    avg_loss:0.014, val_acc:0.962]
Epoch [90/120    avg_loss:0.019, val_acc:0.965]
Epoch [91/120    avg_loss:0.014, val_acc:0.968]
Epoch [92/120    avg_loss:0.019, val_acc:0.970]
Epoch [93/120    avg_loss:0.046, val_acc:0.955]
Epoch [94/120    avg_loss:0.023, val_acc:0.973]
Epoch [95/120    avg_loss:0.018, val_acc:0.967]
Epoch [96/120    avg_loss:0.022, val_acc:0.971]
Epoch [97/120    avg_loss:0.008, val_acc:0.974]
Epoch [98/120    avg_loss:0.011, val_acc:0.978]
Epoch [99/120    avg_loss:0.031, val_acc:0.960]
Epoch [100/120    avg_loss:0.056, val_acc:0.953]
Epoch [101/120    avg_loss:0.030, val_acc:0.963]
Epoch [102/120    avg_loss:0.021, val_acc:0.968]
Epoch [103/120    avg_loss:0.010, val_acc:0.973]
Epoch [104/120    avg_loss:0.009, val_acc:0.975]
Epoch [105/120    avg_loss:0.016, val_acc:0.970]
Epoch [106/120    avg_loss:0.012, val_acc:0.971]
Epoch [107/120    avg_loss:0.012, val_acc:0.972]
Epoch [108/120    avg_loss:0.012, val_acc:0.961]
Epoch [109/120    avg_loss:0.016, val_acc:0.972]
Epoch [110/120    avg_loss:0.046, val_acc:0.957]
Epoch [111/120    avg_loss:0.019, val_acc:0.971]
Epoch [112/120    avg_loss:0.014, val_acc:0.973]
Epoch [113/120    avg_loss:0.013, val_acc:0.974]
Epoch [114/120    avg_loss:0.012, val_acc:0.974]
Epoch [115/120    avg_loss:0.008, val_acc:0.974]
Epoch [116/120    avg_loss:0.008, val_acc:0.974]
Epoch [117/120    avg_loss:0.006, val_acc:0.974]
Epoch [118/120    avg_loss:0.007, val_acc:0.975]
Epoch [119/120    avg_loss:0.008, val_acc:0.975]
Epoch [120/120    avg_loss:0.008, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0 1258    4    9    0    0    0    0    0    2   12    0    0
     0    0    0]
 [   0    0    2  730    2    0    0    0    0    1    0   11    0    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    1    0    0
     1    4    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    1    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    6    1    0    3    1    0    0    0  847   16    0    0
     1    0    0]
 [   0    0    7    0    0    0    0    0    0    0    6 2186    9    0
     0    2    0]
 [   0    0    1    1    0    1    0    0    0    0   11    3  511    0
     0    4    2]
 [   0    0    0    1    0    0    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1118   21    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
    96  244    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.21409214092141

F1 scores:
[       nan 0.98765432 0.98319656 0.98250336 0.97482838 0.98847926
 0.99167298 1.         1.         0.88235294 0.97300402 0.98468468
 0.96780303 0.99459459 0.94906621 0.78456592 0.98224852]

Kappa:
0.9682041529353387
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6b66404ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.736, val_acc:0.378]
Epoch [2/120    avg_loss:2.528, val_acc:0.492]
Epoch [3/120    avg_loss:2.299, val_acc:0.531]
Epoch [4/120    avg_loss:2.073, val_acc:0.556]
Epoch [5/120    avg_loss:1.886, val_acc:0.582]
Epoch [6/120    avg_loss:1.693, val_acc:0.633]
Epoch [7/120    avg_loss:1.519, val_acc:0.681]
Epoch [8/120    avg_loss:1.359, val_acc:0.693]
Epoch [9/120    avg_loss:1.175, val_acc:0.731]
Epoch [10/120    avg_loss:1.019, val_acc:0.741]
Epoch [11/120    avg_loss:0.865, val_acc:0.751]
Epoch [12/120    avg_loss:0.844, val_acc:0.765]
Epoch [13/120    avg_loss:0.702, val_acc:0.838]
Epoch [14/120    avg_loss:0.564, val_acc:0.786]
Epoch [15/120    avg_loss:0.454, val_acc:0.882]
Epoch [16/120    avg_loss:0.425, val_acc:0.847]
Epoch [17/120    avg_loss:0.377, val_acc:0.876]
Epoch [18/120    avg_loss:0.316, val_acc:0.855]
Epoch [19/120    avg_loss:0.317, val_acc:0.857]
Epoch [20/120    avg_loss:0.278, val_acc:0.911]
Epoch [21/120    avg_loss:0.263, val_acc:0.859]
Epoch [22/120    avg_loss:0.231, val_acc:0.914]
Epoch [23/120    avg_loss:0.216, val_acc:0.882]
Epoch [24/120    avg_loss:0.177, val_acc:0.916]
Epoch [25/120    avg_loss:0.181, val_acc:0.912]
Epoch [26/120    avg_loss:0.174, val_acc:0.927]
Epoch [27/120    avg_loss:0.122, val_acc:0.931]
Epoch [28/120    avg_loss:0.143, val_acc:0.940]
Epoch [29/120    avg_loss:0.135, val_acc:0.934]
Epoch [30/120    avg_loss:0.106, val_acc:0.947]
Epoch [31/120    avg_loss:0.083, val_acc:0.952]
Epoch [32/120    avg_loss:0.115, val_acc:0.950]
Epoch [33/120    avg_loss:0.132, val_acc:0.931]
Epoch [34/120    avg_loss:0.065, val_acc:0.953]
Epoch [35/120    avg_loss:0.082, val_acc:0.935]
Epoch [36/120    avg_loss:0.102, val_acc:0.936]
Epoch [37/120    avg_loss:0.049, val_acc:0.960]
Epoch [38/120    avg_loss:0.088, val_acc:0.957]
Epoch [39/120    avg_loss:0.037, val_acc:0.952]
Epoch [40/120    avg_loss:0.058, val_acc:0.957]
Epoch [41/120    avg_loss:0.056, val_acc:0.944]
Epoch [42/120    avg_loss:0.098, val_acc:0.948]
Epoch [43/120    avg_loss:0.141, val_acc:0.956]
Epoch [44/120    avg_loss:0.046, val_acc:0.967]
Epoch [45/120    avg_loss:0.100, val_acc:0.928]
Epoch [46/120    avg_loss:0.061, val_acc:0.958]
Epoch [47/120    avg_loss:0.056, val_acc:0.943]
Epoch [48/120    avg_loss:0.064, val_acc:0.943]
Epoch [49/120    avg_loss:0.073, val_acc:0.958]
Epoch [50/120    avg_loss:0.032, val_acc:0.963]
Epoch [51/120    avg_loss:0.039, val_acc:0.964]
Epoch [52/120    avg_loss:0.035, val_acc:0.970]
Epoch [53/120    avg_loss:0.025, val_acc:0.965]
Epoch [54/120    avg_loss:0.025, val_acc:0.950]
Epoch [55/120    avg_loss:0.030, val_acc:0.973]
Epoch [56/120    avg_loss:0.027, val_acc:0.967]
Epoch [57/120    avg_loss:0.024, val_acc:0.969]
Epoch [58/120    avg_loss:0.045, val_acc:0.954]
Epoch [59/120    avg_loss:0.025, val_acc:0.968]
Epoch [60/120    avg_loss:0.018, val_acc:0.970]
Epoch [61/120    avg_loss:0.018, val_acc:0.968]
Epoch [62/120    avg_loss:0.032, val_acc:0.972]
Epoch [63/120    avg_loss:0.021, val_acc:0.977]
Epoch [64/120    avg_loss:0.017, val_acc:0.973]
Epoch [65/120    avg_loss:0.013, val_acc:0.976]
Epoch [66/120    avg_loss:0.015, val_acc:0.976]
Epoch [67/120    avg_loss:0.011, val_acc:0.982]
Epoch [68/120    avg_loss:0.021, val_acc:0.975]
Epoch [69/120    avg_loss:0.025, val_acc:0.964]
Epoch [70/120    avg_loss:0.019, val_acc:0.980]
Epoch [71/120    avg_loss:0.024, val_acc:0.966]
Epoch [72/120    avg_loss:0.020, val_acc:0.977]
Epoch [73/120    avg_loss:0.025, val_acc:0.943]
Epoch [74/120    avg_loss:0.027, val_acc:0.968]
Epoch [75/120    avg_loss:0.012, val_acc:0.980]
Epoch [76/120    avg_loss:0.009, val_acc:0.981]
Epoch [77/120    avg_loss:0.013, val_acc:0.956]
Epoch [78/120    avg_loss:0.015, val_acc:0.977]
Epoch [79/120    avg_loss:0.010, val_acc:0.981]
Epoch [80/120    avg_loss:0.008, val_acc:0.981]
Epoch [81/120    avg_loss:0.006, val_acc:0.981]
Epoch [82/120    avg_loss:0.007, val_acc:0.980]
Epoch [83/120    avg_loss:0.006, val_acc:0.979]
Epoch [84/120    avg_loss:0.005, val_acc:0.981]
Epoch [85/120    avg_loss:0.007, val_acc:0.981]
Epoch [86/120    avg_loss:0.005, val_acc:0.981]
Epoch [87/120    avg_loss:0.006, val_acc:0.980]
Epoch [88/120    avg_loss:0.006, val_acc:0.982]
Epoch [89/120    avg_loss:0.005, val_acc:0.981]
Epoch [90/120    avg_loss:0.006, val_acc:0.983]
Epoch [91/120    avg_loss:0.006, val_acc:0.979]
Epoch [92/120    avg_loss:0.004, val_acc:0.981]
Epoch [93/120    avg_loss:0.005, val_acc:0.981]
Epoch [94/120    avg_loss:0.005, val_acc:0.982]
Epoch [95/120    avg_loss:0.005, val_acc:0.983]
Epoch [96/120    avg_loss:0.006, val_acc:0.981]
Epoch [97/120    avg_loss:0.005, val_acc:0.981]
Epoch [98/120    avg_loss:0.005, val_acc:0.984]
Epoch [99/120    avg_loss:0.005, val_acc:0.984]
Epoch [100/120    avg_loss:0.004, val_acc:0.983]
Epoch [101/120    avg_loss:0.005, val_acc:0.981]
Epoch [102/120    avg_loss:0.005, val_acc:0.983]
Epoch [103/120    avg_loss:0.005, val_acc:0.983]
Epoch [104/120    avg_loss:0.008, val_acc:0.983]
Epoch [105/120    avg_loss:0.004, val_acc:0.984]
Epoch [106/120    avg_loss:0.005, val_acc:0.983]
Epoch [107/120    avg_loss:0.005, val_acc:0.983]
Epoch [108/120    avg_loss:0.005, val_acc:0.983]
Epoch [109/120    avg_loss:0.006, val_acc:0.985]
Epoch [110/120    avg_loss:0.007, val_acc:0.983]
Epoch [111/120    avg_loss:0.006, val_acc:0.982]
Epoch [112/120    avg_loss:0.005, val_acc:0.982]
Epoch [113/120    avg_loss:0.004, val_acc:0.982]
Epoch [114/120    avg_loss:0.005, val_acc:0.983]
Epoch [115/120    avg_loss:0.005, val_acc:0.982]
Epoch [116/120    avg_loss:0.005, val_acc:0.983]
Epoch [117/120    avg_loss:0.006, val_acc:0.982]
Epoch [118/120    avg_loss:0.004, val_acc:0.983]
Epoch [119/120    avg_loss:0.004, val_acc:0.983]
Epoch [120/120    avg_loss:0.005, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1262    8    1    0    1    0    0    1    4    8    0    0
     0    0    0]
 [   0    0    1  734    1    0    1    0    0    5    1    1    3    0
     0    0    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  433    1    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  427    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    2    0    0    0  866    4    0    0
     0    0    0]
 [   0    0   16    1    0    7    5    0    0    0   13 2165    3    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    4    0  525    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   105  242    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.60433604336043

F1 scores:
[       nan 0.97619048 0.98324893 0.98391421 0.99061033 0.98971429
 0.99244713 1.         0.99649942 0.8372093  0.98241634 0.98678213
 0.98314607 1.         0.95117845 0.80936455 0.98224852]

Kappa:
0.9726794365919502
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f889f911e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.767, val_acc:0.400]
Epoch [2/120    avg_loss:2.584, val_acc:0.422]
Epoch [3/120    avg_loss:2.367, val_acc:0.470]
Epoch [4/120    avg_loss:2.142, val_acc:0.518]
Epoch [5/120    avg_loss:1.928, val_acc:0.590]
Epoch [6/120    avg_loss:1.770, val_acc:0.626]
Epoch [7/120    avg_loss:1.605, val_acc:0.672]
Epoch [8/120    avg_loss:1.386, val_acc:0.682]
Epoch [9/120    avg_loss:1.218, val_acc:0.702]
Epoch [10/120    avg_loss:1.046, val_acc:0.737]
Epoch [11/120    avg_loss:0.872, val_acc:0.756]
Epoch [12/120    avg_loss:0.792, val_acc:0.792]
Epoch [13/120    avg_loss:0.733, val_acc:0.746]
Epoch [14/120    avg_loss:0.608, val_acc:0.817]
Epoch [15/120    avg_loss:0.640, val_acc:0.776]
Epoch [16/120    avg_loss:0.625, val_acc:0.816]
Epoch [17/120    avg_loss:0.442, val_acc:0.856]
Epoch [18/120    avg_loss:0.434, val_acc:0.886]
Epoch [19/120    avg_loss:0.377, val_acc:0.879]
Epoch [20/120    avg_loss:0.367, val_acc:0.881]
Epoch [21/120    avg_loss:0.316, val_acc:0.910]
Epoch [22/120    avg_loss:0.238, val_acc:0.911]
Epoch [23/120    avg_loss:0.262, val_acc:0.919]
Epoch [24/120    avg_loss:0.226, val_acc:0.893]
Epoch [25/120    avg_loss:0.232, val_acc:0.915]
Epoch [26/120    avg_loss:0.204, val_acc:0.935]
Epoch [27/120    avg_loss:0.179, val_acc:0.915]
Epoch [28/120    avg_loss:0.166, val_acc:0.932]
Epoch [29/120    avg_loss:0.153, val_acc:0.915]
Epoch [30/120    avg_loss:0.113, val_acc:0.917]
Epoch [31/120    avg_loss:0.129, val_acc:0.942]
Epoch [32/120    avg_loss:0.120, val_acc:0.926]
Epoch [33/120    avg_loss:0.109, val_acc:0.948]
Epoch [34/120    avg_loss:0.116, val_acc:0.924]
Epoch [35/120    avg_loss:0.115, val_acc:0.942]
Epoch [36/120    avg_loss:0.070, val_acc:0.954]
Epoch [37/120    avg_loss:0.062, val_acc:0.946]
Epoch [38/120    avg_loss:0.050, val_acc:0.966]
Epoch [39/120    avg_loss:0.106, val_acc:0.903]
Epoch [40/120    avg_loss:0.102, val_acc:0.962]
Epoch [41/120    avg_loss:0.072, val_acc:0.951]
Epoch [42/120    avg_loss:0.109, val_acc:0.948]
Epoch [43/120    avg_loss:0.051, val_acc:0.954]
Epoch [44/120    avg_loss:0.331, val_acc:0.904]
Epoch [45/120    avg_loss:0.117, val_acc:0.966]
Epoch [46/120    avg_loss:0.088, val_acc:0.964]
Epoch [47/120    avg_loss:0.051, val_acc:0.956]
Epoch [48/120    avg_loss:0.041, val_acc:0.951]
Epoch [49/120    avg_loss:0.066, val_acc:0.968]
Epoch [50/120    avg_loss:0.037, val_acc:0.971]
Epoch [51/120    avg_loss:0.062, val_acc:0.952]
Epoch [52/120    avg_loss:0.043, val_acc:0.953]
Epoch [53/120    avg_loss:0.040, val_acc:0.964]
Epoch [54/120    avg_loss:0.039, val_acc:0.965]
Epoch [55/120    avg_loss:0.030, val_acc:0.975]
Epoch [56/120    avg_loss:0.033, val_acc:0.911]
Epoch [57/120    avg_loss:0.054, val_acc:0.973]
Epoch [58/120    avg_loss:0.027, val_acc:0.972]
Epoch [59/120    avg_loss:0.018, val_acc:0.980]
Epoch [60/120    avg_loss:0.024, val_acc:0.978]
Epoch [61/120    avg_loss:0.017, val_acc:0.972]
Epoch [62/120    avg_loss:0.018, val_acc:0.975]
Epoch [63/120    avg_loss:0.013, val_acc:0.979]
Epoch [64/120    avg_loss:0.017, val_acc:0.968]
Epoch [65/120    avg_loss:0.031, val_acc:0.978]
Epoch [66/120    avg_loss:0.020, val_acc:0.978]
Epoch [67/120    avg_loss:0.021, val_acc:0.973]
Epoch [68/120    avg_loss:0.033, val_acc:0.974]
Epoch [69/120    avg_loss:0.020, val_acc:0.973]
Epoch [70/120    avg_loss:0.021, val_acc:0.986]
Epoch [71/120    avg_loss:0.012, val_acc:0.981]
Epoch [72/120    avg_loss:0.013, val_acc:0.975]
Epoch [73/120    avg_loss:0.012, val_acc:0.980]
Epoch [74/120    avg_loss:0.019, val_acc:0.981]
Epoch [75/120    avg_loss:0.015, val_acc:0.979]
Epoch [76/120    avg_loss:0.014, val_acc:0.987]
Epoch [77/120    avg_loss:0.010, val_acc:0.982]
Epoch [78/120    avg_loss:0.011, val_acc:0.981]
Epoch [79/120    avg_loss:0.024, val_acc:0.973]
Epoch [80/120    avg_loss:0.012, val_acc:0.984]
Epoch [81/120    avg_loss:0.007, val_acc:0.983]
Epoch [82/120    avg_loss:0.030, val_acc:0.980]
Epoch [83/120    avg_loss:0.011, val_acc:0.984]
Epoch [84/120    avg_loss:0.008, val_acc:0.980]
Epoch [85/120    avg_loss:0.007, val_acc:0.983]
Epoch [86/120    avg_loss:0.011, val_acc:0.975]
Epoch [87/120    avg_loss:0.007, val_acc:0.982]
Epoch [88/120    avg_loss:0.008, val_acc:0.985]
Epoch [89/120    avg_loss:0.007, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.986]
Epoch [91/120    avg_loss:0.007, val_acc:0.986]
Epoch [92/120    avg_loss:0.004, val_acc:0.986]
Epoch [93/120    avg_loss:0.006, val_acc:0.987]
Epoch [94/120    avg_loss:0.005, val_acc:0.987]
Epoch [95/120    avg_loss:0.005, val_acc:0.987]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.006, val_acc:0.987]
Epoch [98/120    avg_loss:0.005, val_acc:0.986]
Epoch [99/120    avg_loss:0.004, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.985]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.005, val_acc:0.986]
Epoch [103/120    avg_loss:0.004, val_acc:0.986]
Epoch [104/120    avg_loss:0.006, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.987]
Epoch [106/120    avg_loss:0.004, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.985]
Epoch [108/120    avg_loss:0.004, val_acc:0.987]
Epoch [109/120    avg_loss:0.006, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.986]
Epoch [111/120    avg_loss:0.004, val_acc:0.986]
Epoch [112/120    avg_loss:0.004, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.004, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.008, val_acc:0.986]
Epoch [117/120    avg_loss:0.004, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.986]
Epoch [120/120    avg_loss:0.004, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1253    0   10    0    0    0    0    0    6   16    0    0
     0    0    0]
 [   0    0    3  699    5    0    0    0    0   17    2    0   20    1
     0    0    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    0    0    2    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   15    0    0    1    0
     0    0    0]
 [   0    0   14    0    0    5    0    0    0    0  839   17    0    0
     0    0    0]
 [   0    0   13    0    0    0    2    0    1    0   13 2174    6    1
     0    0    0]
 [   0    0    0    0    2    2    0    0    0    1    1    2  523    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1136    2    0]
 [   0    0    0    0    0    0    7    0    0    2    0    0    0    0
    73  265    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.12737127371274

F1 scores:
[       nan 0.975      0.9758567  0.96613683 0.9569161  0.98390805
 0.99093656 1.         0.99767442 0.56603774 0.96492237 0.98326549
 0.96228151 0.99462366 0.96557586 0.86178862 0.98224852]

Kappa:
0.9672337456425087
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb19a59ff60>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.744, val_acc:0.363]
Epoch [2/120    avg_loss:2.511, val_acc:0.505]
Epoch [3/120    avg_loss:2.295, val_acc:0.570]
Epoch [4/120    avg_loss:2.053, val_acc:0.579]
Epoch [5/120    avg_loss:1.869, val_acc:0.587]
Epoch [6/120    avg_loss:1.696, val_acc:0.639]
Epoch [7/120    avg_loss:1.541, val_acc:0.660]
Epoch [8/120    avg_loss:1.372, val_acc:0.686]
Epoch [9/120    avg_loss:1.248, val_acc:0.688]
Epoch [10/120    avg_loss:1.074, val_acc:0.734]
Epoch [11/120    avg_loss:0.963, val_acc:0.762]
Epoch [12/120    avg_loss:0.815, val_acc:0.778]
Epoch [13/120    avg_loss:0.724, val_acc:0.767]
Epoch [14/120    avg_loss:0.655, val_acc:0.852]
Epoch [15/120    avg_loss:0.660, val_acc:0.835]
Epoch [16/120    avg_loss:0.579, val_acc:0.819]
Epoch [17/120    avg_loss:0.465, val_acc:0.829]
Epoch [18/120    avg_loss:0.406, val_acc:0.885]
Epoch [19/120    avg_loss:0.342, val_acc:0.862]
Epoch [20/120    avg_loss:0.311, val_acc:0.877]
Epoch [21/120    avg_loss:0.293, val_acc:0.892]
Epoch [22/120    avg_loss:0.247, val_acc:0.909]
Epoch [23/120    avg_loss:0.287, val_acc:0.916]
Epoch [24/120    avg_loss:0.202, val_acc:0.934]
Epoch [25/120    avg_loss:0.189, val_acc:0.943]
Epoch [26/120    avg_loss:0.145, val_acc:0.935]
Epoch [27/120    avg_loss:0.145, val_acc:0.901]
Epoch [28/120    avg_loss:0.148, val_acc:0.922]
Epoch [29/120    avg_loss:0.179, val_acc:0.930]
Epoch [30/120    avg_loss:0.129, val_acc:0.952]
Epoch [31/120    avg_loss:0.096, val_acc:0.940]
Epoch [32/120    avg_loss:0.112, val_acc:0.922]
Epoch [33/120    avg_loss:0.136, val_acc:0.935]
Epoch [34/120    avg_loss:0.179, val_acc:0.921]
Epoch [35/120    avg_loss:0.149, val_acc:0.941]
Epoch [36/120    avg_loss:0.128, val_acc:0.943]
Epoch [37/120    avg_loss:0.074, val_acc:0.957]
Epoch [38/120    avg_loss:0.065, val_acc:0.966]
Epoch [39/120    avg_loss:0.055, val_acc:0.958]
Epoch [40/120    avg_loss:0.105, val_acc:0.905]
Epoch [41/120    avg_loss:0.075, val_acc:0.968]
Epoch [42/120    avg_loss:0.067, val_acc:0.968]
Epoch [43/120    avg_loss:0.043, val_acc:0.966]
Epoch [44/120    avg_loss:0.045, val_acc:0.959]
Epoch [45/120    avg_loss:0.061, val_acc:0.958]
Epoch [46/120    avg_loss:0.122, val_acc:0.949]
Epoch [47/120    avg_loss:0.099, val_acc:0.957]
Epoch [48/120    avg_loss:0.049, val_acc:0.968]
Epoch [49/120    avg_loss:0.030, val_acc:0.972]
Epoch [50/120    avg_loss:0.071, val_acc:0.951]
Epoch [51/120    avg_loss:0.059, val_acc:0.969]
Epoch [52/120    avg_loss:0.050, val_acc:0.970]
Epoch [53/120    avg_loss:0.028, val_acc:0.975]
Epoch [54/120    avg_loss:0.022, val_acc:0.981]
Epoch [55/120    avg_loss:0.013, val_acc:0.982]
Epoch [56/120    avg_loss:0.019, val_acc:0.980]
Epoch [57/120    avg_loss:0.034, val_acc:0.962]
Epoch [58/120    avg_loss:0.023, val_acc:0.974]
Epoch [59/120    avg_loss:0.027, val_acc:0.967]
Epoch [60/120    avg_loss:0.051, val_acc:0.969]
Epoch [61/120    avg_loss:0.033, val_acc:0.966]
Epoch [62/120    avg_loss:0.018, val_acc:0.975]
Epoch [63/120    avg_loss:0.016, val_acc:0.981]
Epoch [64/120    avg_loss:0.018, val_acc:0.981]
Epoch [65/120    avg_loss:0.018, val_acc:0.979]
Epoch [66/120    avg_loss:0.014, val_acc:0.977]
Epoch [67/120    avg_loss:0.015, val_acc:0.980]
Epoch [68/120    avg_loss:0.017, val_acc:0.980]
Epoch [69/120    avg_loss:0.018, val_acc:0.984]
Epoch [70/120    avg_loss:0.012, val_acc:0.986]
Epoch [71/120    avg_loss:0.009, val_acc:0.984]
Epoch [72/120    avg_loss:0.011, val_acc:0.987]
Epoch [73/120    avg_loss:0.008, val_acc:0.987]
Epoch [74/120    avg_loss:0.010, val_acc:0.989]
Epoch [75/120    avg_loss:0.008, val_acc:0.986]
Epoch [76/120    avg_loss:0.009, val_acc:0.988]
Epoch [77/120    avg_loss:0.008, val_acc:0.987]
Epoch [78/120    avg_loss:0.009, val_acc:0.988]
Epoch [79/120    avg_loss:0.012, val_acc:0.987]
Epoch [80/120    avg_loss:0.011, val_acc:0.989]
Epoch [81/120    avg_loss:0.007, val_acc:0.988]
Epoch [82/120    avg_loss:0.009, val_acc:0.988]
Epoch [83/120    avg_loss:0.009, val_acc:0.988]
Epoch [84/120    avg_loss:0.008, val_acc:0.987]
Epoch [85/120    avg_loss:0.008, val_acc:0.988]
Epoch [86/120    avg_loss:0.007, val_acc:0.988]
Epoch [87/120    avg_loss:0.007, val_acc:0.986]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.009, val_acc:0.987]
Epoch [90/120    avg_loss:0.009, val_acc:0.987]
Epoch [91/120    avg_loss:0.006, val_acc:0.987]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.008, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.006, val_acc:0.988]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.008, val_acc:0.988]
Epoch [98/120    avg_loss:0.008, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.009, val_acc:0.987]
Epoch [101/120    avg_loss:0.011, val_acc:0.988]
Epoch [102/120    avg_loss:0.008, val_acc:0.987]
Epoch [103/120    avg_loss:0.007, val_acc:0.987]
Epoch [104/120    avg_loss:0.007, val_acc:0.987]
Epoch [105/120    avg_loss:0.008, val_acc:0.987]
Epoch [106/120    avg_loss:0.007, val_acc:0.987]
Epoch [107/120    avg_loss:0.006, val_acc:0.987]
Epoch [108/120    avg_loss:0.006, val_acc:0.987]
Epoch [109/120    avg_loss:0.010, val_acc:0.987]
Epoch [110/120    avg_loss:0.007, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.009, val_acc:0.987]
Epoch [113/120    avg_loss:0.008, val_acc:0.987]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.008, val_acc:0.987]
Epoch [116/120    avg_loss:0.007, val_acc:0.987]
Epoch [117/120    avg_loss:0.007, val_acc:0.987]
Epoch [118/120    avg_loss:0.007, val_acc:0.987]
Epoch [119/120    avg_loss:0.007, val_acc:0.987]
Epoch [120/120    avg_loss:0.009, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1241    4   10    0    3    0    0    0    4   16    7    0
     0    0    0]
 [   0    0    6  714    2    0    0    0    0    7    0    0   18    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     3    1    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    2    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    1    0    0    0    0  846   19    1    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    4    0   11 2192    0    0
     0    1    0]
 [   0    0    0    0    0    0    0    0    0    0   11    0  520    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1127    9    0]
 [   0    0    0    0    0    0   31    0    0    0    0    0    0    0
    60  256    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.27913279132791

F1 scores:
[       nan 0.975      0.97639654 0.97407913 0.96583144 0.9908046
 0.97401633 1.         0.99537037 0.7804878  0.96740995 0.98805499
 0.96296296 1.         0.96738197 0.83252033 0.98823529]

Kappa:
0.9689611940092032
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb9c2f11f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.762, val_acc:0.397]
Epoch [2/120    avg_loss:2.579, val_acc:0.408]
Epoch [3/120    avg_loss:2.363, val_acc:0.483]
Epoch [4/120    avg_loss:2.150, val_acc:0.547]
Epoch [5/120    avg_loss:1.966, val_acc:0.543]
Epoch [6/120    avg_loss:1.783, val_acc:0.536]
Epoch [7/120    avg_loss:1.618, val_acc:0.633]
Epoch [8/120    avg_loss:1.394, val_acc:0.692]
Epoch [9/120    avg_loss:1.241, val_acc:0.708]
Epoch [10/120    avg_loss:1.051, val_acc:0.738]
Epoch [11/120    avg_loss:0.908, val_acc:0.790]
Epoch [12/120    avg_loss:0.776, val_acc:0.796]
Epoch [13/120    avg_loss:0.680, val_acc:0.804]
Epoch [14/120    avg_loss:0.578, val_acc:0.840]
Epoch [15/120    avg_loss:0.460, val_acc:0.856]
Epoch [16/120    avg_loss:0.411, val_acc:0.878]
Epoch [17/120    avg_loss:0.380, val_acc:0.890]
Epoch [18/120    avg_loss:0.378, val_acc:0.836]
Epoch [19/120    avg_loss:0.345, val_acc:0.877]
Epoch [20/120    avg_loss:0.341, val_acc:0.903]
Epoch [21/120    avg_loss:0.279, val_acc:0.806]
Epoch [22/120    avg_loss:0.247, val_acc:0.898]
Epoch [23/120    avg_loss:0.257, val_acc:0.864]
Epoch [24/120    avg_loss:0.195, val_acc:0.918]
Epoch [25/120    avg_loss:0.182, val_acc:0.918]
Epoch [26/120    avg_loss:0.153, val_acc:0.917]
Epoch [27/120    avg_loss:0.147, val_acc:0.914]
Epoch [28/120    avg_loss:0.129, val_acc:0.921]
Epoch [29/120    avg_loss:0.147, val_acc:0.926]
Epoch [30/120    avg_loss:0.129, val_acc:0.928]
Epoch [31/120    avg_loss:0.167, val_acc:0.922]
Epoch [32/120    avg_loss:0.196, val_acc:0.928]
Epoch [33/120    avg_loss:0.110, val_acc:0.934]
Epoch [34/120    avg_loss:0.096, val_acc:0.912]
Epoch [35/120    avg_loss:0.091, val_acc:0.925]
Epoch [36/120    avg_loss:0.090, val_acc:0.935]
Epoch [37/120    avg_loss:0.087, val_acc:0.943]
Epoch [38/120    avg_loss:0.066, val_acc:0.965]
Epoch [39/120    avg_loss:0.070, val_acc:0.959]
Epoch [40/120    avg_loss:0.088, val_acc:0.933]
Epoch [41/120    avg_loss:0.102, val_acc:0.963]
Epoch [42/120    avg_loss:0.051, val_acc:0.954]
Epoch [43/120    avg_loss:0.054, val_acc:0.949]
Epoch [44/120    avg_loss:0.061, val_acc:0.957]
Epoch [45/120    avg_loss:0.045, val_acc:0.958]
Epoch [46/120    avg_loss:0.047, val_acc:0.954]
Epoch [47/120    avg_loss:0.072, val_acc:0.923]
Epoch [48/120    avg_loss:0.058, val_acc:0.977]
Epoch [49/120    avg_loss:0.042, val_acc:0.932]
Epoch [50/120    avg_loss:0.039, val_acc:0.957]
Epoch [51/120    avg_loss:0.058, val_acc:0.959]
Epoch [52/120    avg_loss:0.065, val_acc:0.970]
Epoch [53/120    avg_loss:0.044, val_acc:0.960]
Epoch [54/120    avg_loss:0.058, val_acc:0.956]
Epoch [55/120    avg_loss:0.051, val_acc:0.967]
Epoch [56/120    avg_loss:0.027, val_acc:0.964]
Epoch [57/120    avg_loss:0.024, val_acc:0.966]
Epoch [58/120    avg_loss:0.019, val_acc:0.963]
Epoch [59/120    avg_loss:0.019, val_acc:0.965]
Epoch [60/120    avg_loss:0.026, val_acc:0.951]
Epoch [61/120    avg_loss:0.028, val_acc:0.962]
Epoch [62/120    avg_loss:0.020, val_acc:0.970]
Epoch [63/120    avg_loss:0.016, val_acc:0.969]
Epoch [64/120    avg_loss:0.015, val_acc:0.970]
Epoch [65/120    avg_loss:0.015, val_acc:0.968]
Epoch [66/120    avg_loss:0.009, val_acc:0.971]
Epoch [67/120    avg_loss:0.012, val_acc:0.972]
Epoch [68/120    avg_loss:0.014, val_acc:0.971]
Epoch [69/120    avg_loss:0.012, val_acc:0.970]
Epoch [70/120    avg_loss:0.017, val_acc:0.972]
Epoch [71/120    avg_loss:0.014, val_acc:0.972]
Epoch [72/120    avg_loss:0.015, val_acc:0.974]
Epoch [73/120    avg_loss:0.011, val_acc:0.975]
Epoch [74/120    avg_loss:0.011, val_acc:0.974]
Epoch [75/120    avg_loss:0.012, val_acc:0.974]
Epoch [76/120    avg_loss:0.009, val_acc:0.975]
Epoch [77/120    avg_loss:0.009, val_acc:0.975]
Epoch [78/120    avg_loss:0.010, val_acc:0.975]
Epoch [79/120    avg_loss:0.008, val_acc:0.975]
Epoch [80/120    avg_loss:0.012, val_acc:0.975]
Epoch [81/120    avg_loss:0.012, val_acc:0.975]
Epoch [82/120    avg_loss:0.017, val_acc:0.974]
Epoch [83/120    avg_loss:0.011, val_acc:0.974]
Epoch [84/120    avg_loss:0.010, val_acc:0.974]
Epoch [85/120    avg_loss:0.013, val_acc:0.975]
Epoch [86/120    avg_loss:0.014, val_acc:0.975]
Epoch [87/120    avg_loss:0.008, val_acc:0.975]
Epoch [88/120    avg_loss:0.011, val_acc:0.974]
Epoch [89/120    avg_loss:0.011, val_acc:0.974]
Epoch [90/120    avg_loss:0.010, val_acc:0.974]
Epoch [91/120    avg_loss:0.009, val_acc:0.974]
Epoch [92/120    avg_loss:0.011, val_acc:0.974]
Epoch [93/120    avg_loss:0.009, val_acc:0.974]
Epoch [94/120    avg_loss:0.012, val_acc:0.974]
Epoch [95/120    avg_loss:0.013, val_acc:0.974]
Epoch [96/120    avg_loss:0.011, val_acc:0.974]
Epoch [97/120    avg_loss:0.013, val_acc:0.974]
Epoch [98/120    avg_loss:0.012, val_acc:0.974]
Epoch [99/120    avg_loss:0.009, val_acc:0.974]
Epoch [100/120    avg_loss:0.011, val_acc:0.974]
Epoch [101/120    avg_loss:0.010, val_acc:0.974]
Epoch [102/120    avg_loss:0.009, val_acc:0.974]
Epoch [103/120    avg_loss:0.010, val_acc:0.974]
Epoch [104/120    avg_loss:0.011, val_acc:0.974]
Epoch [105/120    avg_loss:0.009, val_acc:0.974]
Epoch [106/120    avg_loss:0.012, val_acc:0.974]
Epoch [107/120    avg_loss:0.011, val_acc:0.974]
Epoch [108/120    avg_loss:0.009, val_acc:0.974]
Epoch [109/120    avg_loss:0.010, val_acc:0.974]
Epoch [110/120    avg_loss:0.014, val_acc:0.974]
Epoch [111/120    avg_loss:0.009, val_acc:0.974]
Epoch [112/120    avg_loss:0.010, val_acc:0.974]
Epoch [113/120    avg_loss:0.012, val_acc:0.974]
Epoch [114/120    avg_loss:0.011, val_acc:0.974]
Epoch [115/120    avg_loss:0.010, val_acc:0.974]
Epoch [116/120    avg_loss:0.013, val_acc:0.974]
Epoch [117/120    avg_loss:0.009, val_acc:0.974]
Epoch [118/120    avg_loss:0.010, val_acc:0.974]
Epoch [119/120    avg_loss:0.010, val_acc:0.974]
Epoch [120/120    avg_loss:0.009, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1254    0   10    0    0    0    0    0    6   14    1    0
     0    0    0]
 [   0    0    2  728    5    0    0    0    0    3    3    4    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   14    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  872    2    0    0
     0    1    0]
 [   0    0   13    0    0    1    0    0    0    0   22 2166    8    0
     0    0    0]
 [   0    0    3    4    0    1    0    0    0    0    5    1  516    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1122   17    0]
 [   0    0    0    0    0    0    0    0    0    3    0    0    0    0
    28  316    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.12466124661246

F1 scores:
[       nan 1.         0.98083692 0.98444895 0.96598639 0.99308756
 0.9977221  1.         0.997669   0.73684211 0.97812675 0.98521719
 0.96992481 1.         0.97863061 0.92532943 0.98823529]

Kappa:
0.9786294826113948
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f53fb930f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.772, val_acc:0.290]
Epoch [2/120    avg_loss:2.565, val_acc:0.377]
Epoch [3/120    avg_loss:2.363, val_acc:0.472]
Epoch [4/120    avg_loss:2.143, val_acc:0.562]
Epoch [5/120    avg_loss:1.958, val_acc:0.596]
Epoch [6/120    avg_loss:1.760, val_acc:0.655]
Epoch [7/120    avg_loss:1.513, val_acc:0.648]
Epoch [8/120    avg_loss:1.345, val_acc:0.650]
Epoch [9/120    avg_loss:1.142, val_acc:0.665]
Epoch [10/120    avg_loss:0.957, val_acc:0.753]
Epoch [11/120    avg_loss:0.798, val_acc:0.792]
Epoch [12/120    avg_loss:0.699, val_acc:0.819]
Epoch [13/120    avg_loss:0.631, val_acc:0.831]
Epoch [14/120    avg_loss:0.477, val_acc:0.846]
Epoch [15/120    avg_loss:0.456, val_acc:0.866]
Epoch [16/120    avg_loss:0.415, val_acc:0.875]
Epoch [17/120    avg_loss:0.339, val_acc:0.878]
Epoch [18/120    avg_loss:0.313, val_acc:0.864]
Epoch [19/120    avg_loss:0.346, val_acc:0.900]
Epoch [20/120    avg_loss:0.271, val_acc:0.873]
Epoch [21/120    avg_loss:0.256, val_acc:0.886]
Epoch [22/120    avg_loss:0.228, val_acc:0.887]
Epoch [23/120    avg_loss:0.229, val_acc:0.915]
Epoch [24/120    avg_loss:0.254, val_acc:0.886]
Epoch [25/120    avg_loss:0.186, val_acc:0.920]
Epoch [26/120    avg_loss:0.166, val_acc:0.926]
Epoch [27/120    avg_loss:0.174, val_acc:0.916]
Epoch [28/120    avg_loss:0.125, val_acc:0.941]
Epoch [29/120    avg_loss:0.124, val_acc:0.930]
Epoch [30/120    avg_loss:0.185, val_acc:0.928]
Epoch [31/120    avg_loss:0.173, val_acc:0.943]
Epoch [32/120    avg_loss:0.105, val_acc:0.895]
Epoch [33/120    avg_loss:0.111, val_acc:0.942]
Epoch [34/120    avg_loss:0.113, val_acc:0.939]
Epoch [35/120    avg_loss:0.100, val_acc:0.945]
Epoch [36/120    avg_loss:0.067, val_acc:0.958]
Epoch [37/120    avg_loss:0.070, val_acc:0.966]
Epoch [38/120    avg_loss:0.069, val_acc:0.962]
Epoch [39/120    avg_loss:0.062, val_acc:0.942]
Epoch [40/120    avg_loss:0.065, val_acc:0.966]
Epoch [41/120    avg_loss:0.064, val_acc:0.941]
Epoch [42/120    avg_loss:0.094, val_acc:0.967]
Epoch [43/120    avg_loss:0.071, val_acc:0.958]
Epoch [44/120    avg_loss:0.066, val_acc:0.943]
Epoch [45/120    avg_loss:0.044, val_acc:0.970]
Epoch [46/120    avg_loss:0.045, val_acc:0.958]
Epoch [47/120    avg_loss:0.177, val_acc:0.940]
Epoch [48/120    avg_loss:0.087, val_acc:0.960]
Epoch [49/120    avg_loss:0.047, val_acc:0.967]
Epoch [50/120    avg_loss:0.048, val_acc:0.954]
Epoch [51/120    avg_loss:0.051, val_acc:0.972]
Epoch [52/120    avg_loss:0.031, val_acc:0.964]
Epoch [53/120    avg_loss:0.027, val_acc:0.970]
Epoch [54/120    avg_loss:0.091, val_acc:0.961]
Epoch [55/120    avg_loss:0.097, val_acc:0.953]
Epoch [56/120    avg_loss:0.053, val_acc:0.974]
Epoch [57/120    avg_loss:0.028, val_acc:0.973]
Epoch [58/120    avg_loss:0.037, val_acc:0.972]
Epoch [59/120    avg_loss:0.028, val_acc:0.978]
Epoch [60/120    avg_loss:0.019, val_acc:0.977]
Epoch [61/120    avg_loss:0.029, val_acc:0.975]
Epoch [62/120    avg_loss:0.015, val_acc:0.973]
Epoch [63/120    avg_loss:0.024, val_acc:0.984]
Epoch [64/120    avg_loss:0.014, val_acc:0.981]
Epoch [65/120    avg_loss:0.018, val_acc:0.978]
Epoch [66/120    avg_loss:0.023, val_acc:0.976]
Epoch [67/120    avg_loss:0.023, val_acc:0.986]
Epoch [68/120    avg_loss:0.027, val_acc:0.944]
Epoch [69/120    avg_loss:0.019, val_acc:0.985]
Epoch [70/120    avg_loss:0.038, val_acc:0.936]
Epoch [71/120    avg_loss:0.097, val_acc:0.944]
Epoch [72/120    avg_loss:0.064, val_acc:0.948]
Epoch [73/120    avg_loss:0.071, val_acc:0.968]
Epoch [74/120    avg_loss:0.018, val_acc:0.978]
Epoch [75/120    avg_loss:0.019, val_acc:0.978]
Epoch [76/120    avg_loss:0.022, val_acc:0.980]
Epoch [77/120    avg_loss:0.015, val_acc:0.980]
Epoch [78/120    avg_loss:0.035, val_acc:0.933]
Epoch [79/120    avg_loss:0.062, val_acc:0.956]
Epoch [80/120    avg_loss:0.041, val_acc:0.978]
Epoch [81/120    avg_loss:0.017, val_acc:0.982]
Epoch [82/120    avg_loss:0.017, val_acc:0.982]
Epoch [83/120    avg_loss:0.014, val_acc:0.982]
Epoch [84/120    avg_loss:0.013, val_acc:0.983]
Epoch [85/120    avg_loss:0.016, val_acc:0.983]
Epoch [86/120    avg_loss:0.013, val_acc:0.983]
Epoch [87/120    avg_loss:0.011, val_acc:0.983]
Epoch [88/120    avg_loss:0.014, val_acc:0.984]
Epoch [89/120    avg_loss:0.012, val_acc:0.984]
Epoch [90/120    avg_loss:0.011, val_acc:0.984]
Epoch [91/120    avg_loss:0.014, val_acc:0.983]
Epoch [92/120    avg_loss:0.013, val_acc:0.983]
Epoch [93/120    avg_loss:0.010, val_acc:0.984]
Epoch [94/120    avg_loss:0.013, val_acc:0.984]
Epoch [95/120    avg_loss:0.011, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.985]
Epoch [97/120    avg_loss:0.009, val_acc:0.984]
Epoch [98/120    avg_loss:0.009, val_acc:0.985]
Epoch [99/120    avg_loss:0.011, val_acc:0.985]
Epoch [100/120    avg_loss:0.010, val_acc:0.985]
Epoch [101/120    avg_loss:0.010, val_acc:0.985]
Epoch [102/120    avg_loss:0.010, val_acc:0.984]
Epoch [103/120    avg_loss:0.013, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.984]
Epoch [105/120    avg_loss:0.009, val_acc:0.985]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.009, val_acc:0.984]
Epoch [108/120    avg_loss:0.009, val_acc:0.984]
Epoch [109/120    avg_loss:0.012, val_acc:0.984]
Epoch [110/120    avg_loss:0.010, val_acc:0.984]
Epoch [111/120    avg_loss:0.011, val_acc:0.984]
Epoch [112/120    avg_loss:0.009, val_acc:0.984]
Epoch [113/120    avg_loss:0.009, val_acc:0.984]
Epoch [114/120    avg_loss:0.010, val_acc:0.984]
Epoch [115/120    avg_loss:0.010, val_acc:0.984]
Epoch [116/120    avg_loss:0.009, val_acc:0.984]
Epoch [117/120    avg_loss:0.010, val_acc:0.984]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.984]
Epoch [120/120    avg_loss:0.010, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    0   12    0    0    0    0    0    1   11    0    0
     0    0    0]
 [   0    0    1  725    4    0    0    0    0    2    0    7    8    0
     0    0    0]
 [   0    0    0    0  210    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  845   24    0    0
     1    0    0]
 [   0    0    0    0    0    0    0    0    0    0   10 2197    3    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    4    4  520    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    28  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.38482384823848

F1 scores:
[       nan 0.97619048 0.98824451 0.98438561 0.95671982 0.99770115
 0.99169811 1.         0.997669   0.88888889 0.9740634  0.98652896
 0.97378277 1.         0.98699046 0.93939394 0.98823529]

Kappa:
0.9815725206059579
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:02:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8a7ddaae80>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.788, val_acc:0.165]
Epoch [2/120    avg_loss:2.618, val_acc:0.254]
Epoch [3/120    avg_loss:2.385, val_acc:0.391]
Epoch [4/120    avg_loss:2.194, val_acc:0.502]
Epoch [5/120    avg_loss:2.026, val_acc:0.530]
Epoch [6/120    avg_loss:1.826, val_acc:0.608]
Epoch [7/120    avg_loss:1.656, val_acc:0.627]
Epoch [8/120    avg_loss:1.384, val_acc:0.669]
Epoch [9/120    avg_loss:1.205, val_acc:0.731]
Epoch [10/120    avg_loss:1.021, val_acc:0.679]
Epoch [11/120    avg_loss:0.886, val_acc:0.781]
Epoch [12/120    avg_loss:0.829, val_acc:0.828]
Epoch [13/120    avg_loss:0.634, val_acc:0.813]
Epoch [14/120    avg_loss:0.568, val_acc:0.840]
Epoch [15/120    avg_loss:0.525, val_acc:0.828]
Epoch [16/120    avg_loss:0.444, val_acc:0.869]
Epoch [17/120    avg_loss:0.443, val_acc:0.905]
Epoch [18/120    avg_loss:0.361, val_acc:0.890]
Epoch [19/120    avg_loss:0.320, val_acc:0.893]
Epoch [20/120    avg_loss:0.310, val_acc:0.911]
Epoch [21/120    avg_loss:0.280, val_acc:0.927]
Epoch [22/120    avg_loss:0.222, val_acc:0.889]
Epoch [23/120    avg_loss:0.308, val_acc:0.892]
Epoch [24/120    avg_loss:0.283, val_acc:0.902]
Epoch [25/120    avg_loss:0.287, val_acc:0.929]
Epoch [26/120    avg_loss:0.205, val_acc:0.925]
Epoch [27/120    avg_loss:0.201, val_acc:0.922]
Epoch [28/120    avg_loss:0.128, val_acc:0.949]
Epoch [29/120    avg_loss:0.213, val_acc:0.916]
Epoch [30/120    avg_loss:0.191, val_acc:0.931]
Epoch [31/120    avg_loss:0.137, val_acc:0.937]
Epoch [32/120    avg_loss:0.141, val_acc:0.934]
Epoch [33/120    avg_loss:0.106, val_acc:0.951]
Epoch [34/120    avg_loss:0.118, val_acc:0.946]
Epoch [35/120    avg_loss:0.095, val_acc:0.953]
Epoch [36/120    avg_loss:0.089, val_acc:0.960]
Epoch [37/120    avg_loss:0.111, val_acc:0.922]
Epoch [38/120    avg_loss:0.100, val_acc:0.953]
Epoch [39/120    avg_loss:0.060, val_acc:0.963]
Epoch [40/120    avg_loss:0.077, val_acc:0.952]
Epoch [41/120    avg_loss:0.118, val_acc:0.949]
Epoch [42/120    avg_loss:0.080, val_acc:0.948]
Epoch [43/120    avg_loss:0.080, val_acc:0.957]
Epoch [44/120    avg_loss:0.057, val_acc:0.947]
Epoch [45/120    avg_loss:0.064, val_acc:0.956]
Epoch [46/120    avg_loss:0.051, val_acc:0.957]
Epoch [47/120    avg_loss:0.061, val_acc:0.954]
Epoch [48/120    avg_loss:0.047, val_acc:0.958]
Epoch [49/120    avg_loss:0.063, val_acc:0.964]
Epoch [50/120    avg_loss:0.051, val_acc:0.967]
Epoch [51/120    avg_loss:0.053, val_acc:0.972]
Epoch [52/120    avg_loss:0.034, val_acc:0.975]
Epoch [53/120    avg_loss:0.028, val_acc:0.973]
Epoch [54/120    avg_loss:0.022, val_acc:0.972]
Epoch [55/120    avg_loss:0.047, val_acc:0.936]
Epoch [56/120    avg_loss:0.198, val_acc:0.910]
Epoch [57/120    avg_loss:0.111, val_acc:0.949]
Epoch [58/120    avg_loss:0.057, val_acc:0.970]
Epoch [59/120    avg_loss:0.062, val_acc:0.920]
Epoch [60/120    avg_loss:0.047, val_acc:0.972]
Epoch [61/120    avg_loss:0.030, val_acc:0.981]
Epoch [62/120    avg_loss:0.035, val_acc:0.951]
Epoch [63/120    avg_loss:0.090, val_acc:0.962]
Epoch [64/120    avg_loss:0.056, val_acc:0.964]
Epoch [65/120    avg_loss:0.046, val_acc:0.966]
Epoch [66/120    avg_loss:0.030, val_acc:0.971]
Epoch [67/120    avg_loss:0.027, val_acc:0.982]
Epoch [68/120    avg_loss:0.026, val_acc:0.966]
Epoch [69/120    avg_loss:0.017, val_acc:0.969]
Epoch [70/120    avg_loss:0.020, val_acc:0.973]
Epoch [71/120    avg_loss:0.019, val_acc:0.980]
Epoch [72/120    avg_loss:0.020, val_acc:0.980]
Epoch [73/120    avg_loss:0.029, val_acc:0.972]
Epoch [74/120    avg_loss:0.038, val_acc:0.958]
Epoch [75/120    avg_loss:0.057, val_acc:0.974]
Epoch [76/120    avg_loss:0.021, val_acc:0.980]
Epoch [77/120    avg_loss:0.038, val_acc:0.964]
Epoch [78/120    avg_loss:0.018, val_acc:0.973]
Epoch [79/120    avg_loss:0.014, val_acc:0.983]
Epoch [80/120    avg_loss:0.026, val_acc:0.978]
Epoch [81/120    avg_loss:0.015, val_acc:0.981]
Epoch [82/120    avg_loss:0.017, val_acc:0.976]
Epoch [83/120    avg_loss:0.011, val_acc:0.980]
Epoch [84/120    avg_loss:0.064, val_acc:0.978]
Epoch [85/120    avg_loss:0.010, val_acc:0.983]
Epoch [86/120    avg_loss:0.036, val_acc:0.973]
Epoch [87/120    avg_loss:0.023, val_acc:0.980]
Epoch [88/120    avg_loss:0.015, val_acc:0.984]
Epoch [89/120    avg_loss:0.012, val_acc:0.976]
Epoch [90/120    avg_loss:0.012, val_acc:0.974]
Epoch [91/120    avg_loss:0.017, val_acc:0.974]
Epoch [92/120    avg_loss:0.011, val_acc:0.983]
Epoch [93/120    avg_loss:0.007, val_acc:0.980]
Epoch [94/120    avg_loss:0.017, val_acc:0.985]
Epoch [95/120    avg_loss:0.008, val_acc:0.984]
Epoch [96/120    avg_loss:0.004, val_acc:0.984]
Epoch [97/120    avg_loss:0.005, val_acc:0.983]
Epoch [98/120    avg_loss:0.004, val_acc:0.984]
Epoch [99/120    avg_loss:0.011, val_acc:0.974]
Epoch [100/120    avg_loss:0.007, val_acc:0.983]
Epoch [101/120    avg_loss:0.005, val_acc:0.983]
Epoch [102/120    avg_loss:0.005, val_acc:0.984]
Epoch [103/120    avg_loss:0.009, val_acc:0.976]
Epoch [104/120    avg_loss:0.045, val_acc:0.974]
Epoch [105/120    avg_loss:0.036, val_acc:0.974]
Epoch [106/120    avg_loss:0.032, val_acc:0.974]
Epoch [107/120    avg_loss:0.018, val_acc:0.976]
Epoch [108/120    avg_loss:0.011, val_acc:0.979]
Epoch [109/120    avg_loss:0.008, val_acc:0.981]
Epoch [110/120    avg_loss:0.008, val_acc:0.982]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.008, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.985]
Epoch [114/120    avg_loss:0.005, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.006, val_acc:0.984]
Epoch [117/120    avg_loss:0.008, val_acc:0.981]
Epoch [118/120    avg_loss:0.006, val_acc:0.983]
Epoch [119/120    avg_loss:0.005, val_acc:0.982]
Epoch [120/120    avg_loss:0.007, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1249    3    7    0    0    0    0    0    4   17    4    0
     0    1    0]
 [   0    0    6  701    7   15    0    0    0    6    2    9    0    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    1    0    0    0    2    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    4    0    0   13    0    1    0    0
     0    0    0]
 [   0    0    1    0    0    3    0    0    0    0  856   14    0    0
     0    1    0]
 [   0    2    1    0    0    0    0    0    0    0    2 2200    5    0
     0    0    0]
 [   0    0    0    1    0    2    0    0    0    0   18    5  505    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1128   10    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
    34  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.53929539295393

F1 scores:
[       nan 0.97619048 0.98269079 0.96556474 0.96818182 0.96715742
 0.97764531 0.98039216 1.         0.7027027  0.97438816 0.98676833
 0.96098953 0.99730458 0.97831743 0.88751926 0.97590361]

Kappa:
0.9719234149139865
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3525f07eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.763, val_acc:0.278]
Epoch [2/120    avg_loss:2.614, val_acc:0.434]
Epoch [3/120    avg_loss:2.413, val_acc:0.354]
Epoch [4/120    avg_loss:2.200, val_acc:0.507]
Epoch [5/120    avg_loss:1.970, val_acc:0.596]
Epoch [6/120    avg_loss:1.751, val_acc:0.640]
Epoch [7/120    avg_loss:1.494, val_acc:0.670]
Epoch [8/120    avg_loss:1.349, val_acc:0.681]
Epoch [9/120    avg_loss:1.212, val_acc:0.702]
Epoch [10/120    avg_loss:1.036, val_acc:0.771]
Epoch [11/120    avg_loss:0.878, val_acc:0.778]
Epoch [12/120    avg_loss:0.765, val_acc:0.829]
Epoch [13/120    avg_loss:0.672, val_acc:0.795]
Epoch [14/120    avg_loss:0.643, val_acc:0.740]
Epoch [15/120    avg_loss:0.541, val_acc:0.853]
Epoch [16/120    avg_loss:0.452, val_acc:0.857]
Epoch [17/120    avg_loss:0.444, val_acc:0.896]
Epoch [18/120    avg_loss:0.400, val_acc:0.877]
Epoch [19/120    avg_loss:0.284, val_acc:0.847]
Epoch [20/120    avg_loss:0.331, val_acc:0.900]
Epoch [21/120    avg_loss:0.270, val_acc:0.905]
Epoch [22/120    avg_loss:0.335, val_acc:0.906]
Epoch [23/120    avg_loss:0.369, val_acc:0.894]
Epoch [24/120    avg_loss:0.234, val_acc:0.915]
Epoch [25/120    avg_loss:0.163, val_acc:0.942]
Epoch [26/120    avg_loss:0.120, val_acc:0.917]
Epoch [27/120    avg_loss:0.169, val_acc:0.944]
Epoch [28/120    avg_loss:0.146, val_acc:0.941]
Epoch [29/120    avg_loss:0.122, val_acc:0.949]
Epoch [30/120    avg_loss:0.121, val_acc:0.959]
Epoch [31/120    avg_loss:0.110, val_acc:0.928]
Epoch [32/120    avg_loss:0.097, val_acc:0.943]
Epoch [33/120    avg_loss:0.075, val_acc:0.958]
Epoch [34/120    avg_loss:0.072, val_acc:0.928]
Epoch [35/120    avg_loss:0.095, val_acc:0.969]
Epoch [36/120    avg_loss:0.065, val_acc:0.959]
Epoch [37/120    avg_loss:0.070, val_acc:0.918]
Epoch [38/120    avg_loss:0.053, val_acc:0.958]
Epoch [39/120    avg_loss:0.060, val_acc:0.969]
Epoch [40/120    avg_loss:0.050, val_acc:0.955]
Epoch [41/120    avg_loss:0.039, val_acc:0.955]
Epoch [42/120    avg_loss:0.117, val_acc:0.959]
Epoch [43/120    avg_loss:0.103, val_acc:0.940]
Epoch [44/120    avg_loss:0.086, val_acc:0.978]
Epoch [45/120    avg_loss:0.060, val_acc:0.969]
Epoch [46/120    avg_loss:0.054, val_acc:0.963]
Epoch [47/120    avg_loss:0.032, val_acc:0.967]
Epoch [48/120    avg_loss:0.025, val_acc:0.978]
Epoch [49/120    avg_loss:0.025, val_acc:0.969]
Epoch [50/120    avg_loss:0.049, val_acc:0.937]
Epoch [51/120    avg_loss:0.034, val_acc:0.967]
Epoch [52/120    avg_loss:0.034, val_acc:0.966]
Epoch [53/120    avg_loss:0.027, val_acc:0.955]
Epoch [54/120    avg_loss:0.053, val_acc:0.963]
Epoch [55/120    avg_loss:0.023, val_acc:0.974]
Epoch [56/120    avg_loss:0.030, val_acc:0.959]
Epoch [57/120    avg_loss:0.035, val_acc:0.974]
Epoch [58/120    avg_loss:0.017, val_acc:0.973]
Epoch [59/120    avg_loss:0.020, val_acc:0.985]
Epoch [60/120    avg_loss:0.020, val_acc:0.985]
Epoch [61/120    avg_loss:0.019, val_acc:0.985]
Epoch [62/120    avg_loss:0.011, val_acc:0.982]
Epoch [63/120    avg_loss:0.053, val_acc:0.908]
Epoch [64/120    avg_loss:0.109, val_acc:0.967]
Epoch [65/120    avg_loss:0.061, val_acc:0.969]
Epoch [66/120    avg_loss:0.031, val_acc:0.967]
Epoch [67/120    avg_loss:0.029, val_acc:0.979]
Epoch [68/120    avg_loss:0.023, val_acc:0.979]
Epoch [69/120    avg_loss:0.027, val_acc:0.976]
Epoch [70/120    avg_loss:0.018, val_acc:0.987]
Epoch [71/120    avg_loss:0.036, val_acc:0.959]
Epoch [72/120    avg_loss:0.021, val_acc:0.966]
Epoch [73/120    avg_loss:0.026, val_acc:0.978]
Epoch [74/120    avg_loss:0.013, val_acc:0.979]
Epoch [75/120    avg_loss:0.008, val_acc:0.980]
Epoch [76/120    avg_loss:0.009, val_acc:0.982]
Epoch [77/120    avg_loss:0.009, val_acc:0.978]
Epoch [78/120    avg_loss:0.020, val_acc:0.982]
Epoch [79/120    avg_loss:0.012, val_acc:0.973]
Epoch [80/120    avg_loss:0.025, val_acc:0.959]
Epoch [81/120    avg_loss:0.019, val_acc:0.984]
Epoch [82/120    avg_loss:0.018, val_acc:0.978]
Epoch [83/120    avg_loss:0.008, val_acc:0.982]
Epoch [84/120    avg_loss:0.007, val_acc:0.985]
Epoch [85/120    avg_loss:0.006, val_acc:0.986]
Epoch [86/120    avg_loss:0.006, val_acc:0.986]
Epoch [87/120    avg_loss:0.006, val_acc:0.987]
Epoch [88/120    avg_loss:0.006, val_acc:0.986]
Epoch [89/120    avg_loss:0.009, val_acc:0.987]
Epoch [90/120    avg_loss:0.005, val_acc:0.986]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.006, val_acc:0.988]
Epoch [93/120    avg_loss:0.005, val_acc:0.987]
Epoch [94/120    avg_loss:0.006, val_acc:0.988]
Epoch [95/120    avg_loss:0.005, val_acc:0.987]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.007, val_acc:0.985]
Epoch [98/120    avg_loss:0.006, val_acc:0.985]
Epoch [99/120    avg_loss:0.007, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.006, val_acc:0.986]
Epoch [102/120    avg_loss:0.006, val_acc:0.987]
Epoch [103/120    avg_loss:0.006, val_acc:0.987]
Epoch [104/120    avg_loss:0.005, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.987]
Epoch [106/120    avg_loss:0.005, val_acc:0.987]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.004, val_acc:0.987]
Epoch [109/120    avg_loss:0.004, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.988]
Epoch [111/120    avg_loss:0.006, val_acc:0.985]
Epoch [112/120    avg_loss:0.004, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.008, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.006, val_acc:0.987]
Epoch [118/120    avg_loss:0.005, val_acc:0.988]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1257    1    7    0    0    0    0    0    3   13    4    0
     0    0    0]
 [   0    0    1  730    2    3    0    0    0    8    0    2    1    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    4    0    0    0    0  842   18    1    0
     4    0    0]
 [   0    0   13    0    0    0    0    0    0    0    9 2187    0    0
     0    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  527    0
     1    3    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
  1130    7    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
    66  274    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.86449864498645

F1 scores:
[       nan 0.98765432 0.98126464 0.98715348 0.97695853 0.98401826
 0.99470098 1.         1.         0.81818182 0.9734104  0.98713609
 0.98781631 0.99728997 0.96457533 0.86708861 0.98245614]

Kappa:
0.9756382034633384
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1e622f9eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.772, val_acc:0.331]
Epoch [2/120    avg_loss:2.601, val_acc:0.429]
Epoch [3/120    avg_loss:2.420, val_acc:0.493]
Epoch [4/120    avg_loss:2.197, val_acc:0.554]
Epoch [5/120    avg_loss:1.992, val_acc:0.584]
Epoch [6/120    avg_loss:1.830, val_acc:0.629]
Epoch [7/120    avg_loss:1.585, val_acc:0.648]
Epoch [8/120    avg_loss:1.374, val_acc:0.660]
Epoch [9/120    avg_loss:1.224, val_acc:0.689]
Epoch [10/120    avg_loss:1.088, val_acc:0.702]
Epoch [11/120    avg_loss:0.919, val_acc:0.771]
Epoch [12/120    avg_loss:0.775, val_acc:0.783]
Epoch [13/120    avg_loss:0.664, val_acc:0.792]
Epoch [14/120    avg_loss:0.662, val_acc:0.838]
Epoch [15/120    avg_loss:0.558, val_acc:0.834]
Epoch [16/120    avg_loss:0.518, val_acc:0.827]
Epoch [17/120    avg_loss:0.453, val_acc:0.852]
Epoch [18/120    avg_loss:0.471, val_acc:0.890]
Epoch [19/120    avg_loss:0.362, val_acc:0.903]
Epoch [20/120    avg_loss:0.311, val_acc:0.861]
Epoch [21/120    avg_loss:0.214, val_acc:0.897]
Epoch [22/120    avg_loss:0.335, val_acc:0.910]
Epoch [23/120    avg_loss:0.281, val_acc:0.914]
Epoch [24/120    avg_loss:0.197, val_acc:0.914]
Epoch [25/120    avg_loss:0.293, val_acc:0.944]
Epoch [26/120    avg_loss:0.176, val_acc:0.941]
Epoch [27/120    avg_loss:0.187, val_acc:0.908]
Epoch [28/120    avg_loss:0.145, val_acc:0.949]
Epoch [29/120    avg_loss:0.101, val_acc:0.950]
Epoch [30/120    avg_loss:0.120, val_acc:0.945]
Epoch [31/120    avg_loss:0.127, val_acc:0.904]
Epoch [32/120    avg_loss:0.100, val_acc:0.952]
Epoch [33/120    avg_loss:0.106, val_acc:0.954]
Epoch [34/120    avg_loss:0.080, val_acc:0.948]
Epoch [35/120    avg_loss:0.091, val_acc:0.954]
Epoch [36/120    avg_loss:0.105, val_acc:0.909]
Epoch [37/120    avg_loss:0.090, val_acc:0.959]
Epoch [38/120    avg_loss:0.064, val_acc:0.951]
Epoch [39/120    avg_loss:0.052, val_acc:0.961]
Epoch [40/120    avg_loss:0.076, val_acc:0.964]
Epoch [41/120    avg_loss:0.065, val_acc:0.968]
Epoch [42/120    avg_loss:0.074, val_acc:0.962]
Epoch [43/120    avg_loss:0.071, val_acc:0.971]
Epoch [44/120    avg_loss:0.080, val_acc:0.969]
Epoch [45/120    avg_loss:0.068, val_acc:0.943]
Epoch [46/120    avg_loss:0.074, val_acc:0.958]
Epoch [47/120    avg_loss:0.112, val_acc:0.941]
Epoch [48/120    avg_loss:0.093, val_acc:0.948]
Epoch [49/120    avg_loss:0.094, val_acc:0.964]
Epoch [50/120    avg_loss:0.066, val_acc:0.975]
Epoch [51/120    avg_loss:0.047, val_acc:0.970]
Epoch [52/120    avg_loss:0.039, val_acc:0.972]
Epoch [53/120    avg_loss:0.035, val_acc:0.972]
Epoch [54/120    avg_loss:0.040, val_acc:0.963]
Epoch [55/120    avg_loss:0.026, val_acc:0.977]
Epoch [56/120    avg_loss:0.026, val_acc:0.979]
Epoch [57/120    avg_loss:0.020, val_acc:0.975]
Epoch [58/120    avg_loss:0.051, val_acc:0.959]
Epoch [59/120    avg_loss:0.038, val_acc:0.972]
Epoch [60/120    avg_loss:0.017, val_acc:0.979]
Epoch [61/120    avg_loss:0.026, val_acc:0.977]
Epoch [62/120    avg_loss:0.031, val_acc:0.979]
Epoch [63/120    avg_loss:0.016, val_acc:0.979]
Epoch [64/120    avg_loss:0.018, val_acc:0.975]
Epoch [65/120    avg_loss:0.021, val_acc:0.981]
Epoch [66/120    avg_loss:0.012, val_acc:0.980]
Epoch [67/120    avg_loss:0.017, val_acc:0.971]
Epoch [68/120    avg_loss:0.018, val_acc:0.976]
Epoch [69/120    avg_loss:0.015, val_acc:0.977]
Epoch [70/120    avg_loss:0.015, val_acc:0.977]
Epoch [71/120    avg_loss:0.014, val_acc:0.984]
Epoch [72/120    avg_loss:0.016, val_acc:0.987]
Epoch [73/120    avg_loss:0.014, val_acc:0.981]
Epoch [74/120    avg_loss:0.012, val_acc:0.985]
Epoch [75/120    avg_loss:0.025, val_acc:0.965]
Epoch [76/120    avg_loss:0.023, val_acc:0.979]
Epoch [77/120    avg_loss:0.043, val_acc:0.959]
Epoch [78/120    avg_loss:0.046, val_acc:0.971]
Epoch [79/120    avg_loss:0.020, val_acc:0.986]
Epoch [80/120    avg_loss:0.009, val_acc:0.983]
Epoch [81/120    avg_loss:0.010, val_acc:0.984]
Epoch [82/120    avg_loss:0.023, val_acc:0.975]
Epoch [83/120    avg_loss:0.017, val_acc:0.979]
Epoch [84/120    avg_loss:0.014, val_acc:0.982]
Epoch [85/120    avg_loss:0.031, val_acc:0.972]
Epoch [86/120    avg_loss:0.015, val_acc:0.980]
Epoch [87/120    avg_loss:0.013, val_acc:0.981]
Epoch [88/120    avg_loss:0.012, val_acc:0.982]
Epoch [89/120    avg_loss:0.011, val_acc:0.982]
Epoch [90/120    avg_loss:0.010, val_acc:0.982]
Epoch [91/120    avg_loss:0.007, val_acc:0.983]
Epoch [92/120    avg_loss:0.009, val_acc:0.983]
Epoch [93/120    avg_loss:0.009, val_acc:0.984]
Epoch [94/120    avg_loss:0.006, val_acc:0.985]
Epoch [95/120    avg_loss:0.010, val_acc:0.984]
Epoch [96/120    avg_loss:0.007, val_acc:0.984]
Epoch [97/120    avg_loss:0.008, val_acc:0.984]
Epoch [98/120    avg_loss:0.008, val_acc:0.984]
Epoch [99/120    avg_loss:0.008, val_acc:0.984]
Epoch [100/120    avg_loss:0.007, val_acc:0.984]
Epoch [101/120    avg_loss:0.007, val_acc:0.984]
Epoch [102/120    avg_loss:0.007, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.984]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.006, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.008, val_acc:0.984]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.006, val_acc:0.984]
Epoch [113/120    avg_loss:0.008, val_acc:0.984]
Epoch [114/120    avg_loss:0.006, val_acc:0.984]
Epoch [115/120    avg_loss:0.006, val_acc:0.984]
Epoch [116/120    avg_loss:0.005, val_acc:0.984]
Epoch [117/120    avg_loss:0.006, val_acc:0.984]
Epoch [118/120    avg_loss:0.006, val_acc:0.984]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1257    0    7    0    0    0    0    0    2   14    1    0
     0    4    0]
 [   0    0    0  714    4    0    0    0    0    2    2    0   25    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  423    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0  869    3    0    0
     0    2    0]
 [   0    0   21    0    0    0    0    0    0    0   28 2158    2    0
     0    1    0]
 [   0    0    0    0    0    0    0    0    0    0   15    0  515    0
     0    3    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1136    3    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    24  311    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.91869918699187

F1 scores:
[       nan 0.94117647 0.98049922 0.97741273 0.97482838 0.99421965
 0.99020347 1.         0.99179367 0.91891892 0.96986607 0.98426454
 0.95282146 1.         0.98611111 0.92697466 0.98809524]

Kappa:
0.9762835281711837
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fef41b0cef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.766, val_acc:0.339]
Epoch [2/120    avg_loss:2.608, val_acc:0.476]
Epoch [3/120    avg_loss:2.400, val_acc:0.536]
Epoch [4/120    avg_loss:2.164, val_acc:0.583]
Epoch [5/120    avg_loss:1.915, val_acc:0.609]
Epoch [6/120    avg_loss:1.766, val_acc:0.618]
Epoch [7/120    avg_loss:1.561, val_acc:0.665]
Epoch [8/120    avg_loss:1.364, val_acc:0.650]
Epoch [9/120    avg_loss:1.220, val_acc:0.654]
Epoch [10/120    avg_loss:1.095, val_acc:0.709]
Epoch [11/120    avg_loss:0.995, val_acc:0.744]
Epoch [12/120    avg_loss:0.951, val_acc:0.764]
Epoch [13/120    avg_loss:0.780, val_acc:0.782]
Epoch [14/120    avg_loss:0.641, val_acc:0.839]
Epoch [15/120    avg_loss:0.601, val_acc:0.825]
Epoch [16/120    avg_loss:0.514, val_acc:0.858]
Epoch [17/120    avg_loss:0.528, val_acc:0.811]
Epoch [18/120    avg_loss:0.511, val_acc:0.847]
Epoch [19/120    avg_loss:0.362, val_acc:0.850]
Epoch [20/120    avg_loss:0.372, val_acc:0.877]
Epoch [21/120    avg_loss:0.315, val_acc:0.874]
Epoch [22/120    avg_loss:0.231, val_acc:0.861]
Epoch [23/120    avg_loss:0.286, val_acc:0.888]
Epoch [24/120    avg_loss:0.265, val_acc:0.888]
Epoch [25/120    avg_loss:0.257, val_acc:0.885]
Epoch [26/120    avg_loss:0.279, val_acc:0.891]
Epoch [27/120    avg_loss:0.251, val_acc:0.890]
Epoch [28/120    avg_loss:0.177, val_acc:0.948]
Epoch [29/120    avg_loss:0.160, val_acc:0.925]
Epoch [30/120    avg_loss:0.179, val_acc:0.936]
Epoch [31/120    avg_loss:0.124, val_acc:0.934]
Epoch [32/120    avg_loss:0.123, val_acc:0.939]
Epoch [33/120    avg_loss:0.106, val_acc:0.940]
Epoch [34/120    avg_loss:0.110, val_acc:0.950]
Epoch [35/120    avg_loss:0.075, val_acc:0.954]
Epoch [36/120    avg_loss:0.077, val_acc:0.870]
Epoch [37/120    avg_loss:0.108, val_acc:0.960]
Epoch [38/120    avg_loss:0.103, val_acc:0.889]
Epoch [39/120    avg_loss:0.081, val_acc:0.954]
Epoch [40/120    avg_loss:0.136, val_acc:0.963]
Epoch [41/120    avg_loss:0.063, val_acc:0.968]
Epoch [42/120    avg_loss:0.078, val_acc:0.942]
Epoch [43/120    avg_loss:0.073, val_acc:0.944]
Epoch [44/120    avg_loss:0.111, val_acc:0.960]
Epoch [45/120    avg_loss:0.106, val_acc:0.969]
Epoch [46/120    avg_loss:0.063, val_acc:0.944]
Epoch [47/120    avg_loss:0.041, val_acc:0.962]
Epoch [48/120    avg_loss:0.053, val_acc:0.963]
Epoch [49/120    avg_loss:0.052, val_acc:0.943]
Epoch [50/120    avg_loss:0.047, val_acc:0.976]
Epoch [51/120    avg_loss:0.041, val_acc:0.964]
Epoch [52/120    avg_loss:0.049, val_acc:0.972]
Epoch [53/120    avg_loss:0.036, val_acc:0.969]
Epoch [54/120    avg_loss:0.034, val_acc:0.982]
Epoch [55/120    avg_loss:0.041, val_acc:0.972]
Epoch [56/120    avg_loss:0.027, val_acc:0.976]
Epoch [57/120    avg_loss:0.035, val_acc:0.974]
Epoch [58/120    avg_loss:0.037, val_acc:0.951]
Epoch [59/120    avg_loss:0.031, val_acc:0.974]
Epoch [60/120    avg_loss:0.023, val_acc:0.976]
Epoch [61/120    avg_loss:0.027, val_acc:0.982]
Epoch [62/120    avg_loss:0.020, val_acc:0.980]
Epoch [63/120    avg_loss:0.024, val_acc:0.983]
Epoch [64/120    avg_loss:0.022, val_acc:0.976]
Epoch [65/120    avg_loss:0.027, val_acc:0.977]
Epoch [66/120    avg_loss:0.018, val_acc:0.984]
Epoch [67/120    avg_loss:0.015, val_acc:0.980]
Epoch [68/120    avg_loss:0.023, val_acc:0.967]
Epoch [69/120    avg_loss:0.042, val_acc:0.975]
Epoch [70/120    avg_loss:0.036, val_acc:0.964]
Epoch [71/120    avg_loss:0.033, val_acc:0.982]
Epoch [72/120    avg_loss:0.028, val_acc:0.974]
Epoch [73/120    avg_loss:0.018, val_acc:0.978]
Epoch [74/120    avg_loss:0.017, val_acc:0.979]
Epoch [75/120    avg_loss:0.022, val_acc:0.983]
Epoch [76/120    avg_loss:0.022, val_acc:0.980]
Epoch [77/120    avg_loss:0.017, val_acc:0.965]
Epoch [78/120    avg_loss:0.028, val_acc:0.971]
Epoch [79/120    avg_loss:0.049, val_acc:0.977]
Epoch [80/120    avg_loss:0.020, val_acc:0.982]
Epoch [81/120    avg_loss:0.016, val_acc:0.981]
Epoch [82/120    avg_loss:0.018, val_acc:0.979]
Epoch [83/120    avg_loss:0.012, val_acc:0.982]
Epoch [84/120    avg_loss:0.017, val_acc:0.980]
Epoch [85/120    avg_loss:0.011, val_acc:0.980]
Epoch [86/120    avg_loss:0.013, val_acc:0.980]
Epoch [87/120    avg_loss:0.009, val_acc:0.980]
Epoch [88/120    avg_loss:0.013, val_acc:0.980]
Epoch [89/120    avg_loss:0.011, val_acc:0.979]
Epoch [90/120    avg_loss:0.011, val_acc:0.981]
Epoch [91/120    avg_loss:0.010, val_acc:0.982]
Epoch [92/120    avg_loss:0.010, val_acc:0.983]
Epoch [93/120    avg_loss:0.011, val_acc:0.983]
Epoch [94/120    avg_loss:0.010, val_acc:0.983]
Epoch [95/120    avg_loss:0.010, val_acc:0.983]
Epoch [96/120    avg_loss:0.010, val_acc:0.983]
Epoch [97/120    avg_loss:0.010, val_acc:0.983]
Epoch [98/120    avg_loss:0.010, val_acc:0.983]
Epoch [99/120    avg_loss:0.009, val_acc:0.981]
Epoch [100/120    avg_loss:0.008, val_acc:0.980]
Epoch [101/120    avg_loss:0.009, val_acc:0.981]
Epoch [102/120    avg_loss:0.010, val_acc:0.983]
Epoch [103/120    avg_loss:0.010, val_acc:0.983]
Epoch [104/120    avg_loss:0.010, val_acc:0.983]
Epoch [105/120    avg_loss:0.009, val_acc:0.983]
Epoch [106/120    avg_loss:0.008, val_acc:0.983]
Epoch [107/120    avg_loss:0.010, val_acc:0.983]
Epoch [108/120    avg_loss:0.008, val_acc:0.982]
Epoch [109/120    avg_loss:0.010, val_acc:0.982]
Epoch [110/120    avg_loss:0.009, val_acc:0.982]
Epoch [111/120    avg_loss:0.011, val_acc:0.982]
Epoch [112/120    avg_loss:0.012, val_acc:0.982]
Epoch [113/120    avg_loss:0.011, val_acc:0.983]
Epoch [114/120    avg_loss:0.008, val_acc:0.983]
Epoch [115/120    avg_loss:0.007, val_acc:0.983]
Epoch [116/120    avg_loss:0.010, val_acc:0.981]
Epoch [117/120    avg_loss:0.011, val_acc:0.982]
Epoch [118/120    avg_loss:0.010, val_acc:0.980]
Epoch [119/120    avg_loss:0.010, val_acc:0.981]
Epoch [120/120    avg_loss:0.010, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1259    5    1    0    0    0    0    0    5   14    1    0
     0    0    0]
 [   0    0    3  725    3    0    0    0    0    0    0    0   15    1
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   16    2    0    4    0    0    0    0  826   26    1    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0   23 2178    7    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0  531    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1132    6    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
   104  237    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.15989159891599

F1 scores:
[       nan 0.975      0.98167641 0.97972973 0.98834499 0.99311927
 0.99240122 1.         0.995338   0.97297297 0.95436164 0.98351772
 0.97163769 0.99730458 0.9512605  0.80338983 0.98224852]

Kappa:
0.9675845798440101
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f62567d2e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.800, val_acc:0.280]
Epoch [2/120    avg_loss:2.645, val_acc:0.432]
Epoch [3/120    avg_loss:2.403, val_acc:0.454]
Epoch [4/120    avg_loss:2.158, val_acc:0.507]
Epoch [5/120    avg_loss:1.919, val_acc:0.538]
Epoch [6/120    avg_loss:1.717, val_acc:0.619]
Epoch [7/120    avg_loss:1.477, val_acc:0.659]
Epoch [8/120    avg_loss:1.222, val_acc:0.672]
Epoch [9/120    avg_loss:1.076, val_acc:0.741]
Epoch [10/120    avg_loss:0.923, val_acc:0.732]
Epoch [11/120    avg_loss:0.747, val_acc:0.802]
Epoch [12/120    avg_loss:0.701, val_acc:0.784]
Epoch [13/120    avg_loss:0.607, val_acc:0.821]
Epoch [14/120    avg_loss:0.520, val_acc:0.856]
Epoch [15/120    avg_loss:0.405, val_acc:0.850]
Epoch [16/120    avg_loss:0.403, val_acc:0.831]
Epoch [17/120    avg_loss:0.371, val_acc:0.886]
Epoch [18/120    avg_loss:0.320, val_acc:0.878]
Epoch [19/120    avg_loss:0.307, val_acc:0.902]
Epoch [20/120    avg_loss:0.247, val_acc:0.863]
Epoch [21/120    avg_loss:0.243, val_acc:0.913]
Epoch [22/120    avg_loss:0.254, val_acc:0.916]
Epoch [23/120    avg_loss:0.195, val_acc:0.905]
Epoch [24/120    avg_loss:0.177, val_acc:0.921]
Epoch [25/120    avg_loss:0.183, val_acc:0.915]
Epoch [26/120    avg_loss:0.149, val_acc:0.911]
Epoch [27/120    avg_loss:0.191, val_acc:0.896]
Epoch [28/120    avg_loss:0.153, val_acc:0.938]
Epoch [29/120    avg_loss:0.157, val_acc:0.877]
Epoch [30/120    avg_loss:0.139, val_acc:0.932]
Epoch [31/120    avg_loss:0.113, val_acc:0.928]
Epoch [32/120    avg_loss:0.135, val_acc:0.926]
Epoch [33/120    avg_loss:0.102, val_acc:0.939]
Epoch [34/120    avg_loss:0.094, val_acc:0.957]
Epoch [35/120    avg_loss:0.066, val_acc:0.954]
Epoch [36/120    avg_loss:0.116, val_acc:0.931]
Epoch [37/120    avg_loss:0.110, val_acc:0.913]
Epoch [38/120    avg_loss:0.079, val_acc:0.951]
Epoch [39/120    avg_loss:0.090, val_acc:0.937]
Epoch [40/120    avg_loss:0.070, val_acc:0.952]
Epoch [41/120    avg_loss:0.127, val_acc:0.846]
Epoch [42/120    avg_loss:0.139, val_acc:0.931]
Epoch [43/120    avg_loss:0.073, val_acc:0.924]
Epoch [44/120    avg_loss:0.066, val_acc:0.946]
Epoch [45/120    avg_loss:0.054, val_acc:0.953]
Epoch [46/120    avg_loss:0.038, val_acc:0.956]
Epoch [47/120    avg_loss:0.048, val_acc:0.964]
Epoch [48/120    avg_loss:0.036, val_acc:0.962]
Epoch [49/120    avg_loss:0.043, val_acc:0.964]
Epoch [50/120    avg_loss:0.073, val_acc:0.884]
Epoch [51/120    avg_loss:0.183, val_acc:0.944]
Epoch [52/120    avg_loss:0.062, val_acc:0.951]
Epoch [53/120    avg_loss:0.109, val_acc:0.908]
Epoch [54/120    avg_loss:0.069, val_acc:0.946]
Epoch [55/120    avg_loss:0.070, val_acc:0.952]
Epoch [56/120    avg_loss:0.039, val_acc:0.959]
Epoch [57/120    avg_loss:0.033, val_acc:0.964]
Epoch [58/120    avg_loss:0.044, val_acc:0.949]
Epoch [59/120    avg_loss:0.039, val_acc:0.955]
Epoch [60/120    avg_loss:0.030, val_acc:0.962]
Epoch [61/120    avg_loss:0.037, val_acc:0.963]
Epoch [62/120    avg_loss:0.022, val_acc:0.966]
Epoch [63/120    avg_loss:0.024, val_acc:0.970]
Epoch [64/120    avg_loss:0.016, val_acc:0.967]
Epoch [65/120    avg_loss:0.027, val_acc:0.949]
Epoch [66/120    avg_loss:0.024, val_acc:0.972]
Epoch [67/120    avg_loss:0.044, val_acc:0.949]
Epoch [68/120    avg_loss:0.037, val_acc:0.967]
Epoch [69/120    avg_loss:0.019, val_acc:0.973]
Epoch [70/120    avg_loss:0.012, val_acc:0.974]
Epoch [71/120    avg_loss:0.013, val_acc:0.964]
Epoch [72/120    avg_loss:0.013, val_acc:0.979]
Epoch [73/120    avg_loss:0.009, val_acc:0.971]
Epoch [74/120    avg_loss:0.012, val_acc:0.972]
Epoch [75/120    avg_loss:0.011, val_acc:0.966]
Epoch [76/120    avg_loss:0.016, val_acc:0.971]
Epoch [77/120    avg_loss:0.020, val_acc:0.966]
Epoch [78/120    avg_loss:0.021, val_acc:0.960]
Epoch [79/120    avg_loss:0.036, val_acc:0.965]
Epoch [80/120    avg_loss:0.040, val_acc:0.944]
Epoch [81/120    avg_loss:0.043, val_acc:0.960]
Epoch [82/120    avg_loss:0.013, val_acc:0.968]
Epoch [83/120    avg_loss:0.013, val_acc:0.970]
Epoch [84/120    avg_loss:0.012, val_acc:0.972]
Epoch [85/120    avg_loss:0.009, val_acc:0.970]
Epoch [86/120    avg_loss:0.010, val_acc:0.973]
Epoch [87/120    avg_loss:0.009, val_acc:0.974]
Epoch [88/120    avg_loss:0.009, val_acc:0.973]
Epoch [89/120    avg_loss:0.006, val_acc:0.974]
Epoch [90/120    avg_loss:0.006, val_acc:0.974]
Epoch [91/120    avg_loss:0.008, val_acc:0.974]
Epoch [92/120    avg_loss:0.008, val_acc:0.975]
Epoch [93/120    avg_loss:0.009, val_acc:0.976]
Epoch [94/120    avg_loss:0.005, val_acc:0.976]
Epoch [95/120    avg_loss:0.006, val_acc:0.976]
Epoch [96/120    avg_loss:0.011, val_acc:0.975]
Epoch [97/120    avg_loss:0.007, val_acc:0.975]
Epoch [98/120    avg_loss:0.006, val_acc:0.975]
Epoch [99/120    avg_loss:0.007, val_acc:0.975]
Epoch [100/120    avg_loss:0.007, val_acc:0.975]
Epoch [101/120    avg_loss:0.006, val_acc:0.975]
Epoch [102/120    avg_loss:0.007, val_acc:0.975]
Epoch [103/120    avg_loss:0.007, val_acc:0.975]
Epoch [104/120    avg_loss:0.006, val_acc:0.975]
Epoch [105/120    avg_loss:0.007, val_acc:0.975]
Epoch [106/120    avg_loss:0.005, val_acc:0.974]
Epoch [107/120    avg_loss:0.005, val_acc:0.974]
Epoch [108/120    avg_loss:0.006, val_acc:0.975]
Epoch [109/120    avg_loss:0.007, val_acc:0.975]
Epoch [110/120    avg_loss:0.005, val_acc:0.975]
Epoch [111/120    avg_loss:0.005, val_acc:0.975]
Epoch [112/120    avg_loss:0.007, val_acc:0.975]
Epoch [113/120    avg_loss:0.005, val_acc:0.975]
Epoch [114/120    avg_loss:0.005, val_acc:0.975]
Epoch [115/120    avg_loss:0.008, val_acc:0.975]
Epoch [116/120    avg_loss:0.009, val_acc:0.975]
Epoch [117/120    avg_loss:0.010, val_acc:0.974]
Epoch [118/120    avg_loss:0.012, val_acc:0.975]
Epoch [119/120    avg_loss:0.006, val_acc:0.975]
Epoch [120/120    avg_loss:0.005, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    0    3    0    0    0    0    0    2   19    0    0
     0    0    0]
 [   0    0    1  712    3    0    0    0    0    1    0    2   27    1
     0    0    0]
 [   0    0    0    0  207    0    0    0    0    0    0    5    1    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    1    0    0    1    0    0    0  847   19    2    0
     0    0    0]
 [   0    0   14    0    0    1    0    0    0    0   17 2178    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    2    0  529    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    1    0    0    0    0    0    0    0
  1132    4    0]
 [   0    0    0    0    0    0   31    0    0    2    0    0    0    0
    33  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.76693766937669

F1 scores:
[       nan 1.         0.98285269 0.97467488 0.97183099 0.99541284
 0.97473997 1.         1.         0.92307692 0.97188755 0.98240866
 0.96709324 0.99730458 0.98178664 0.88924051 0.98809524]

Kappa:
0.9745269271632578
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:03:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbc3f23b7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.757, val_acc:0.407]
Epoch [2/120    avg_loss:2.555, val_acc:0.468]
Epoch [3/120    avg_loss:2.331, val_acc:0.530]
Epoch [4/120    avg_loss:2.122, val_acc:0.571]
Epoch [5/120    avg_loss:1.967, val_acc:0.601]
Epoch [6/120    avg_loss:1.692, val_acc:0.623]
Epoch [7/120    avg_loss:1.593, val_acc:0.662]
Epoch [8/120    avg_loss:1.415, val_acc:0.682]
Epoch [9/120    avg_loss:1.251, val_acc:0.680]
Epoch [10/120    avg_loss:1.087, val_acc:0.739]
Epoch [11/120    avg_loss:1.010, val_acc:0.707]
Epoch [12/120    avg_loss:0.888, val_acc:0.748]
Epoch [13/120    avg_loss:0.824, val_acc:0.791]
Epoch [14/120    avg_loss:0.759, val_acc:0.812]
Epoch [15/120    avg_loss:0.635, val_acc:0.831]
Epoch [16/120    avg_loss:0.542, val_acc:0.811]
Epoch [17/120    avg_loss:0.475, val_acc:0.797]
Epoch [18/120    avg_loss:0.471, val_acc:0.794]
Epoch [19/120    avg_loss:0.401, val_acc:0.890]
Epoch [20/120    avg_loss:0.330, val_acc:0.889]
Epoch [21/120    avg_loss:0.277, val_acc:0.913]
Epoch [22/120    avg_loss:0.256, val_acc:0.879]
Epoch [23/120    avg_loss:0.280, val_acc:0.906]
Epoch [24/120    avg_loss:0.266, val_acc:0.834]
Epoch [25/120    avg_loss:0.211, val_acc:0.929]
Epoch [26/120    avg_loss:0.179, val_acc:0.885]
Epoch [27/120    avg_loss:0.213, val_acc:0.911]
Epoch [28/120    avg_loss:0.369, val_acc:0.894]
Epoch [29/120    avg_loss:0.225, val_acc:0.865]
Epoch [30/120    avg_loss:0.190, val_acc:0.919]
Epoch [31/120    avg_loss:0.132, val_acc:0.920]
Epoch [32/120    avg_loss:0.179, val_acc:0.921]
Epoch [33/120    avg_loss:0.112, val_acc:0.932]
Epoch [34/120    avg_loss:0.104, val_acc:0.957]
Epoch [35/120    avg_loss:0.106, val_acc:0.949]
Epoch [36/120    avg_loss:0.099, val_acc:0.958]
Epoch [37/120    avg_loss:0.062, val_acc:0.957]
Epoch [38/120    avg_loss:0.112, val_acc:0.947]
Epoch [39/120    avg_loss:0.117, val_acc:0.953]
Epoch [40/120    avg_loss:0.102, val_acc:0.969]
Epoch [41/120    avg_loss:0.121, val_acc:0.930]
Epoch [42/120    avg_loss:0.104, val_acc:0.955]
Epoch [43/120    avg_loss:0.102, val_acc:0.931]
Epoch [44/120    avg_loss:0.129, val_acc:0.962]
Epoch [45/120    avg_loss:0.071, val_acc:0.957]
Epoch [46/120    avg_loss:0.043, val_acc:0.938]
Epoch [47/120    avg_loss:0.060, val_acc:0.921]
Epoch [48/120    avg_loss:0.037, val_acc:0.974]
Epoch [49/120    avg_loss:0.035, val_acc:0.971]
Epoch [50/120    avg_loss:0.024, val_acc:0.978]
Epoch [51/120    avg_loss:0.030, val_acc:0.957]
Epoch [52/120    avg_loss:0.023, val_acc:0.980]
Epoch [53/120    avg_loss:0.020, val_acc:0.977]
Epoch [54/120    avg_loss:0.032, val_acc:0.966]
Epoch [55/120    avg_loss:0.018, val_acc:0.977]
Epoch [56/120    avg_loss:0.034, val_acc:0.983]
Epoch [57/120    avg_loss:0.023, val_acc:0.974]
Epoch [58/120    avg_loss:0.033, val_acc:0.963]
Epoch [59/120    avg_loss:0.031, val_acc:0.971]
Epoch [60/120    avg_loss:0.023, val_acc:0.981]
Epoch [61/120    avg_loss:0.028, val_acc:0.970]
Epoch [62/120    avg_loss:0.024, val_acc:0.968]
Epoch [63/120    avg_loss:0.031, val_acc:0.977]
Epoch [64/120    avg_loss:0.015, val_acc:0.984]
Epoch [65/120    avg_loss:0.013, val_acc:0.978]
Epoch [66/120    avg_loss:0.012, val_acc:0.981]
Epoch [67/120    avg_loss:0.012, val_acc:0.974]
Epoch [68/120    avg_loss:0.012, val_acc:0.980]
Epoch [69/120    avg_loss:0.088, val_acc:0.957]
Epoch [70/120    avg_loss:0.043, val_acc:0.920]
Epoch [71/120    avg_loss:0.045, val_acc:0.954]
Epoch [72/120    avg_loss:0.031, val_acc:0.973]
Epoch [73/120    avg_loss:0.034, val_acc:0.954]
Epoch [74/120    avg_loss:0.039, val_acc:0.969]
Epoch [75/120    avg_loss:0.082, val_acc:0.898]
Epoch [76/120    avg_loss:0.041, val_acc:0.972]
Epoch [77/120    avg_loss:0.029, val_acc:0.970]
Epoch [78/120    avg_loss:0.014, val_acc:0.974]
Epoch [79/120    avg_loss:0.012, val_acc:0.980]
Epoch [80/120    avg_loss:0.014, val_acc:0.980]
Epoch [81/120    avg_loss:0.011, val_acc:0.980]
Epoch [82/120    avg_loss:0.011, val_acc:0.980]
Epoch [83/120    avg_loss:0.015, val_acc:0.981]
Epoch [84/120    avg_loss:0.008, val_acc:0.981]
Epoch [85/120    avg_loss:0.008, val_acc:0.976]
Epoch [86/120    avg_loss:0.013, val_acc:0.980]
Epoch [87/120    avg_loss:0.012, val_acc:0.982]
Epoch [88/120    avg_loss:0.011, val_acc:0.981]
Epoch [89/120    avg_loss:0.012, val_acc:0.981]
Epoch [90/120    avg_loss:0.009, val_acc:0.980]
Epoch [91/120    avg_loss:0.007, val_acc:0.980]
Epoch [92/120    avg_loss:0.008, val_acc:0.980]
Epoch [93/120    avg_loss:0.008, val_acc:0.980]
Epoch [94/120    avg_loss:0.007, val_acc:0.980]
Epoch [95/120    avg_loss:0.008, val_acc:0.980]
Epoch [96/120    avg_loss:0.010, val_acc:0.980]
Epoch [97/120    avg_loss:0.008, val_acc:0.980]
Epoch [98/120    avg_loss:0.013, val_acc:0.980]
Epoch [99/120    avg_loss:0.007, val_acc:0.980]
Epoch [100/120    avg_loss:0.009, val_acc:0.980]
Epoch [101/120    avg_loss:0.007, val_acc:0.980]
Epoch [102/120    avg_loss:0.008, val_acc:0.980]
Epoch [103/120    avg_loss:0.007, val_acc:0.978]
Epoch [104/120    avg_loss:0.008, val_acc:0.978]
Epoch [105/120    avg_loss:0.008, val_acc:0.978]
Epoch [106/120    avg_loss:0.010, val_acc:0.978]
Epoch [107/120    avg_loss:0.007, val_acc:0.978]
Epoch [108/120    avg_loss:0.008, val_acc:0.978]
Epoch [109/120    avg_loss:0.010, val_acc:0.978]
Epoch [110/120    avg_loss:0.008, val_acc:0.978]
Epoch [111/120    avg_loss:0.007, val_acc:0.980]
Epoch [112/120    avg_loss:0.013, val_acc:0.978]
Epoch [113/120    avg_loss:0.008, val_acc:0.978]
Epoch [114/120    avg_loss:0.008, val_acc:0.978]
Epoch [115/120    avg_loss:0.010, val_acc:0.978]
Epoch [116/120    avg_loss:0.008, val_acc:0.978]
Epoch [117/120    avg_loss:0.008, val_acc:0.978]
Epoch [118/120    avg_loss:0.008, val_acc:0.978]
Epoch [119/120    avg_loss:0.008, val_acc:0.978]
Epoch [120/120    avg_loss:0.009, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1252    5    9    0    0    0    0    0    7   12    0    0
     0    0    0]
 [   0    0    1  735    3    0    0    0    0    2    2    0    0    3
     0    1    0]
 [   0    0    0   10  203    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    0    0    0    0    0
     9    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    5    0    0   12    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  861    8    0    0
     0    1    0]
 [   0    5    2    0    0    0    0    0    0    0    7 2194    0    0
     1    1    0]
 [   0    0    1    4    0    0    0    0    0    0    0    0  525    0
     0    3    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1130    8    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
   108  237    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.47425474254743

F1 scores:
[       nan 0.94252874 0.98350353 0.97869507 0.94859813 0.98954704
 0.9908953  1.         1.         0.75       0.98287671 0.99163842
 0.99056604 0.9919571  0.94560669 0.79264214 0.98809524]

Kappa:
0.9711741865549703
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f77f0f26ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.763, val_acc:0.371]
Epoch [2/120    avg_loss:2.596, val_acc:0.385]
Epoch [3/120    avg_loss:2.393, val_acc:0.463]
Epoch [4/120    avg_loss:2.217, val_acc:0.529]
Epoch [5/120    avg_loss:2.042, val_acc:0.589]
Epoch [6/120    avg_loss:1.850, val_acc:0.646]
Epoch [7/120    avg_loss:1.687, val_acc:0.664]
Epoch [8/120    avg_loss:1.497, val_acc:0.686]
Epoch [9/120    avg_loss:1.349, val_acc:0.725]
Epoch [10/120    avg_loss:1.236, val_acc:0.737]
Epoch [11/120    avg_loss:1.034, val_acc:0.752]
Epoch [12/120    avg_loss:0.958, val_acc:0.799]
Epoch [13/120    avg_loss:0.787, val_acc:0.808]
Epoch [14/120    avg_loss:0.684, val_acc:0.834]
Epoch [15/120    avg_loss:0.589, val_acc:0.877]
Epoch [16/120    avg_loss:0.544, val_acc:0.835]
Epoch [17/120    avg_loss:0.619, val_acc:0.845]
Epoch [18/120    avg_loss:0.537, val_acc:0.853]
Epoch [19/120    avg_loss:0.445, val_acc:0.846]
Epoch [20/120    avg_loss:0.438, val_acc:0.864]
Epoch [21/120    avg_loss:0.453, val_acc:0.860]
Epoch [22/120    avg_loss:0.333, val_acc:0.884]
Epoch [23/120    avg_loss:0.303, val_acc:0.905]
Epoch [24/120    avg_loss:0.271, val_acc:0.909]
Epoch [25/120    avg_loss:0.260, val_acc:0.903]
Epoch [26/120    avg_loss:0.239, val_acc:0.921]
Epoch [27/120    avg_loss:0.171, val_acc:0.936]
Epoch [28/120    avg_loss:0.197, val_acc:0.902]
Epoch [29/120    avg_loss:0.203, val_acc:0.940]
Epoch [30/120    avg_loss:0.200, val_acc:0.896]
Epoch [31/120    avg_loss:0.190, val_acc:0.951]
Epoch [32/120    avg_loss:0.145, val_acc:0.926]
Epoch [33/120    avg_loss:0.141, val_acc:0.954]
Epoch [34/120    avg_loss:0.139, val_acc:0.945]
Epoch [35/120    avg_loss:0.118, val_acc:0.948]
Epoch [36/120    avg_loss:0.095, val_acc:0.953]
Epoch [37/120    avg_loss:0.117, val_acc:0.957]
Epoch [38/120    avg_loss:0.088, val_acc:0.953]
Epoch [39/120    avg_loss:0.102, val_acc:0.943]
Epoch [40/120    avg_loss:0.076, val_acc:0.964]
Epoch [41/120    avg_loss:0.136, val_acc:0.960]
Epoch [42/120    avg_loss:0.057, val_acc:0.967]
Epoch [43/120    avg_loss:0.056, val_acc:0.959]
Epoch [44/120    avg_loss:0.077, val_acc:0.952]
Epoch [45/120    avg_loss:0.077, val_acc:0.957]
Epoch [46/120    avg_loss:0.083, val_acc:0.961]
Epoch [47/120    avg_loss:0.057, val_acc:0.974]
Epoch [48/120    avg_loss:0.045, val_acc:0.977]
Epoch [49/120    avg_loss:0.061, val_acc:0.976]
Epoch [50/120    avg_loss:0.044, val_acc:0.967]
Epoch [51/120    avg_loss:0.041, val_acc:0.974]
Epoch [52/120    avg_loss:0.045, val_acc:0.967]
Epoch [53/120    avg_loss:0.045, val_acc:0.970]
Epoch [54/120    avg_loss:0.029, val_acc:0.974]
Epoch [55/120    avg_loss:0.036, val_acc:0.978]
Epoch [56/120    avg_loss:0.064, val_acc:0.970]
Epoch [57/120    avg_loss:0.023, val_acc:0.980]
Epoch [58/120    avg_loss:0.020, val_acc:0.978]
Epoch [59/120    avg_loss:0.023, val_acc:0.984]
Epoch [60/120    avg_loss:0.016, val_acc:0.985]
Epoch [61/120    avg_loss:0.031, val_acc:0.965]
Epoch [62/120    avg_loss:0.024, val_acc:0.983]
Epoch [63/120    avg_loss:0.030, val_acc:0.897]
Epoch [64/120    avg_loss:0.201, val_acc:0.904]
Epoch [65/120    avg_loss:0.073, val_acc:0.952]
Epoch [66/120    avg_loss:0.042, val_acc:0.977]
Epoch [67/120    avg_loss:0.038, val_acc:0.976]
Epoch [68/120    avg_loss:0.032, val_acc:0.967]
Epoch [69/120    avg_loss:0.034, val_acc:0.972]
Epoch [70/120    avg_loss:0.021, val_acc:0.973]
Epoch [71/120    avg_loss:0.021, val_acc:0.977]
Epoch [72/120    avg_loss:0.030, val_acc:0.978]
Epoch [73/120    avg_loss:0.021, val_acc:0.976]
Epoch [74/120    avg_loss:0.015, val_acc:0.982]
Epoch [75/120    avg_loss:0.014, val_acc:0.984]
Epoch [76/120    avg_loss:0.013, val_acc:0.984]
Epoch [77/120    avg_loss:0.011, val_acc:0.985]
Epoch [78/120    avg_loss:0.012, val_acc:0.984]
Epoch [79/120    avg_loss:0.012, val_acc:0.985]
Epoch [80/120    avg_loss:0.012, val_acc:0.984]
Epoch [81/120    avg_loss:0.011, val_acc:0.985]
Epoch [82/120    avg_loss:0.010, val_acc:0.985]
Epoch [83/120    avg_loss:0.010, val_acc:0.987]
Epoch [84/120    avg_loss:0.011, val_acc:0.986]
Epoch [85/120    avg_loss:0.011, val_acc:0.986]
Epoch [86/120    avg_loss:0.009, val_acc:0.986]
Epoch [87/120    avg_loss:0.010, val_acc:0.986]
Epoch [88/120    avg_loss:0.012, val_acc:0.985]
Epoch [89/120    avg_loss:0.012, val_acc:0.985]
Epoch [90/120    avg_loss:0.009, val_acc:0.985]
Epoch [91/120    avg_loss:0.009, val_acc:0.985]
Epoch [92/120    avg_loss:0.009, val_acc:0.985]
Epoch [93/120    avg_loss:0.008, val_acc:0.985]
Epoch [94/120    avg_loss:0.009, val_acc:0.985]
Epoch [95/120    avg_loss:0.011, val_acc:0.985]
Epoch [96/120    avg_loss:0.010, val_acc:0.985]
Epoch [97/120    avg_loss:0.009, val_acc:0.985]
Epoch [98/120    avg_loss:0.008, val_acc:0.985]
Epoch [99/120    avg_loss:0.008, val_acc:0.985]
Epoch [100/120    avg_loss:0.009, val_acc:0.986]
Epoch [101/120    avg_loss:0.008, val_acc:0.986]
Epoch [102/120    avg_loss:0.009, val_acc:0.986]
Epoch [103/120    avg_loss:0.010, val_acc:0.986]
Epoch [104/120    avg_loss:0.008, val_acc:0.986]
Epoch [105/120    avg_loss:0.009, val_acc:0.985]
Epoch [106/120    avg_loss:0.011, val_acc:0.986]
Epoch [107/120    avg_loss:0.010, val_acc:0.986]
Epoch [108/120    avg_loss:0.009, val_acc:0.985]
Epoch [109/120    avg_loss:0.010, val_acc:0.985]
Epoch [110/120    avg_loss:0.010, val_acc:0.985]
Epoch [111/120    avg_loss:0.009, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.985]
Epoch [113/120    avg_loss:0.008, val_acc:0.985]
Epoch [114/120    avg_loss:0.009, val_acc:0.985]
Epoch [115/120    avg_loss:0.009, val_acc:0.985]
Epoch [116/120    avg_loss:0.009, val_acc:0.985]
Epoch [117/120    avg_loss:0.014, val_acc:0.985]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.009, val_acc:0.985]
Epoch [120/120    avg_loss:0.010, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1241    0   11    0    0    0    0    1    4   22    6    0
     0    0    0]
 [   0    0    3  736    1    0    0    0    0    0    0    0    7    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5   53    0    0    0    0    0    0  797   19    0    0
     0    1    0]
 [   0    0    5    0    0    0    0    0    0    0   12 2191    2    0
     0    0    0]
 [   0    0    0   19    0    0    0    0    0    0   13   11  491    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0    0    0    0    0
  1137    1    0]
 [   0    0    0    0    0    0   10    0    0    3    0    0    0    0
   119  215    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.39024390243902

F1 scores:
[       nan 0.98765432 0.97678079 0.94662379 0.97260274 0.99769585
 0.99244713 0.98039216 1.         0.9        0.93709583 0.98383476
 0.94332373 1.         0.94947808 0.76241135 0.99401198]

Kappa:
0.9587778355033378
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5c2d19bef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.743, val_acc:0.389]
Epoch [2/120    avg_loss:2.572, val_acc:0.444]
Epoch [3/120    avg_loss:2.384, val_acc:0.475]
Epoch [4/120    avg_loss:2.211, val_acc:0.503]
Epoch [5/120    avg_loss:2.033, val_acc:0.535]
Epoch [6/120    avg_loss:1.904, val_acc:0.575]
Epoch [7/120    avg_loss:1.787, val_acc:0.609]
Epoch [8/120    avg_loss:1.655, val_acc:0.605]
Epoch [9/120    avg_loss:1.476, val_acc:0.683]
Epoch [10/120    avg_loss:1.325, val_acc:0.723]
Epoch [11/120    avg_loss:1.198, val_acc:0.754]
Epoch [12/120    avg_loss:1.034, val_acc:0.747]
Epoch [13/120    avg_loss:0.920, val_acc:0.781]
Epoch [14/120    avg_loss:0.824, val_acc:0.771]
Epoch [15/120    avg_loss:0.779, val_acc:0.799]
Epoch [16/120    avg_loss:0.735, val_acc:0.784]
Epoch [17/120    avg_loss:0.602, val_acc:0.853]
Epoch [18/120    avg_loss:0.551, val_acc:0.848]
Epoch [19/120    avg_loss:0.480, val_acc:0.837]
Epoch [20/120    avg_loss:0.420, val_acc:0.877]
Epoch [21/120    avg_loss:0.477, val_acc:0.885]
Epoch [22/120    avg_loss:0.468, val_acc:0.846]
Epoch [23/120    avg_loss:0.372, val_acc:0.876]
Epoch [24/120    avg_loss:0.313, val_acc:0.853]
Epoch [25/120    avg_loss:0.313, val_acc:0.908]
Epoch [26/120    avg_loss:0.249, val_acc:0.911]
Epoch [27/120    avg_loss:0.244, val_acc:0.929]
Epoch [28/120    avg_loss:0.173, val_acc:0.922]
Epoch [29/120    avg_loss:0.193, val_acc:0.904]
Epoch [30/120    avg_loss:0.226, val_acc:0.897]
Epoch [31/120    avg_loss:0.177, val_acc:0.950]
Epoch [32/120    avg_loss:0.195, val_acc:0.935]
Epoch [33/120    avg_loss:0.162, val_acc:0.954]
Epoch [34/120    avg_loss:0.220, val_acc:0.910]
Epoch [35/120    avg_loss:0.157, val_acc:0.931]
Epoch [36/120    avg_loss:0.112, val_acc:0.957]
Epoch [37/120    avg_loss:0.102, val_acc:0.886]
Epoch [38/120    avg_loss:0.134, val_acc:0.962]
Epoch [39/120    avg_loss:0.141, val_acc:0.955]
Epoch [40/120    avg_loss:0.136, val_acc:0.941]
Epoch [41/120    avg_loss:0.252, val_acc:0.951]
Epoch [42/120    avg_loss:0.125, val_acc:0.957]
Epoch [43/120    avg_loss:0.083, val_acc:0.976]
Epoch [44/120    avg_loss:0.067, val_acc:0.973]
Epoch [45/120    avg_loss:0.075, val_acc:0.966]
Epoch [46/120    avg_loss:0.084, val_acc:0.974]
Epoch [47/120    avg_loss:0.066, val_acc:0.966]
Epoch [48/120    avg_loss:0.058, val_acc:0.962]
Epoch [49/120    avg_loss:0.050, val_acc:0.973]
Epoch [50/120    avg_loss:0.055, val_acc:0.975]
Epoch [51/120    avg_loss:0.070, val_acc:0.954]
Epoch [52/120    avg_loss:0.079, val_acc:0.976]
Epoch [53/120    avg_loss:0.045, val_acc:0.985]
Epoch [54/120    avg_loss:0.054, val_acc:0.945]
Epoch [55/120    avg_loss:0.078, val_acc:0.969]
Epoch [56/120    avg_loss:0.059, val_acc:0.967]
Epoch [57/120    avg_loss:0.034, val_acc:0.985]
Epoch [58/120    avg_loss:0.031, val_acc:0.968]
Epoch [59/120    avg_loss:0.048, val_acc:0.978]
Epoch [60/120    avg_loss:0.026, val_acc:0.982]
Epoch [61/120    avg_loss:0.034, val_acc:0.988]
Epoch [62/120    avg_loss:0.020, val_acc:0.987]
Epoch [63/120    avg_loss:0.048, val_acc:0.929]
Epoch [64/120    avg_loss:0.061, val_acc:0.987]
Epoch [65/120    avg_loss:0.028, val_acc:0.981]
Epoch [66/120    avg_loss:0.027, val_acc:0.987]
Epoch [67/120    avg_loss:0.024, val_acc:0.985]
Epoch [68/120    avg_loss:0.032, val_acc:0.982]
Epoch [69/120    avg_loss:0.026, val_acc:0.979]
Epoch [70/120    avg_loss:0.023, val_acc:0.986]
Epoch [71/120    avg_loss:0.032, val_acc:0.987]
Epoch [72/120    avg_loss:0.021, val_acc:0.984]
Epoch [73/120    avg_loss:0.024, val_acc:0.984]
Epoch [74/120    avg_loss:0.030, val_acc:0.952]
Epoch [75/120    avg_loss:0.019, val_acc:0.988]
Epoch [76/120    avg_loss:0.018, val_acc:0.989]
Epoch [77/120    avg_loss:0.011, val_acc:0.990]
Epoch [78/120    avg_loss:0.014, val_acc:0.989]
Epoch [79/120    avg_loss:0.012, val_acc:0.989]
Epoch [80/120    avg_loss:0.012, val_acc:0.990]
Epoch [81/120    avg_loss:0.010, val_acc:0.990]
Epoch [82/120    avg_loss:0.014, val_acc:0.990]
Epoch [83/120    avg_loss:0.012, val_acc:0.990]
Epoch [84/120    avg_loss:0.014, val_acc:0.991]
Epoch [85/120    avg_loss:0.010, val_acc:0.991]
Epoch [86/120    avg_loss:0.011, val_acc:0.990]
Epoch [87/120    avg_loss:0.010, val_acc:0.990]
Epoch [88/120    avg_loss:0.023, val_acc:0.989]
Epoch [89/120    avg_loss:0.011, val_acc:0.990]
Epoch [90/120    avg_loss:0.013, val_acc:0.990]
Epoch [91/120    avg_loss:0.012, val_acc:0.989]
Epoch [92/120    avg_loss:0.010, val_acc:0.989]
Epoch [93/120    avg_loss:0.008, val_acc:0.989]
Epoch [94/120    avg_loss:0.016, val_acc:0.990]
Epoch [95/120    avg_loss:0.009, val_acc:0.989]
Epoch [96/120    avg_loss:0.009, val_acc:0.989]
Epoch [97/120    avg_loss:0.008, val_acc:0.989]
Epoch [98/120    avg_loss:0.015, val_acc:0.990]
Epoch [99/120    avg_loss:0.008, val_acc:0.990]
Epoch [100/120    avg_loss:0.012, val_acc:0.990]
Epoch [101/120    avg_loss:0.010, val_acc:0.990]
Epoch [102/120    avg_loss:0.008, val_acc:0.989]
Epoch [103/120    avg_loss:0.009, val_acc:0.989]
Epoch [104/120    avg_loss:0.013, val_acc:0.989]
Epoch [105/120    avg_loss:0.009, val_acc:0.989]
Epoch [106/120    avg_loss:0.008, val_acc:0.989]
Epoch [107/120    avg_loss:0.011, val_acc:0.990]
Epoch [108/120    avg_loss:0.009, val_acc:0.990]
Epoch [109/120    avg_loss:0.008, val_acc:0.989]
Epoch [110/120    avg_loss:0.008, val_acc:0.990]
Epoch [111/120    avg_loss:0.008, val_acc:0.989]
Epoch [112/120    avg_loss:0.009, val_acc:0.989]
Epoch [113/120    avg_loss:0.009, val_acc:0.989]
Epoch [114/120    avg_loss:0.007, val_acc:0.989]
Epoch [115/120    avg_loss:0.008, val_acc:0.989]
Epoch [116/120    avg_loss:0.009, val_acc:0.989]
Epoch [117/120    avg_loss:0.008, val_acc:0.989]
Epoch [118/120    avg_loss:0.009, val_acc:0.989]
Epoch [119/120    avg_loss:0.010, val_acc:0.989]
Epoch [120/120    avg_loss:0.008, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1262    0    8    0    2    0    0    0    0   11    1    0
     0    1    0]
 [   0    0    1  721    4    8    0    0    0    8    1    0    3    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    4    0    0   11    0    1    0    0
     0    0    0]
 [   0    0    5   12    0    3    0    0    0    0  842   12    0    0
     0    1    0]
 [   0    0    1    0    0    0    0    0    0    0   22 2184    3    0
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    0    3  522    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    1    0    1    0    1    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    72  270    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.6910569105691

F1 scores:
[       nan 1.         0.98825372 0.97300945 0.97260274 0.97737557
 0.98942598 1.         0.99883856 0.57894737 0.9672602  0.98801176
 0.98212606 1.         0.96472588 0.8723748  0.98823529]

Kappa:
0.9736627593353835
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f52d92e9f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.792, val_acc:0.214]
Epoch [2/120    avg_loss:2.637, val_acc:0.453]
Epoch [3/120    avg_loss:2.463, val_acc:0.462]
Epoch [4/120    avg_loss:2.299, val_acc:0.496]
Epoch [5/120    avg_loss:2.117, val_acc:0.488]
Epoch [6/120    avg_loss:1.921, val_acc:0.578]
Epoch [7/120    avg_loss:1.789, val_acc:0.554]
Epoch [8/120    avg_loss:1.626, val_acc:0.617]
Epoch [9/120    avg_loss:1.490, val_acc:0.653]
Epoch [10/120    avg_loss:1.337, val_acc:0.703]
Epoch [11/120    avg_loss:1.176, val_acc:0.699]
Epoch [12/120    avg_loss:1.116, val_acc:0.735]
Epoch [13/120    avg_loss:0.888, val_acc:0.736]
Epoch [14/120    avg_loss:0.779, val_acc:0.788]
Epoch [15/120    avg_loss:0.782, val_acc:0.825]
Epoch [16/120    avg_loss:0.685, val_acc:0.845]
Epoch [17/120    avg_loss:0.522, val_acc:0.849]
Epoch [18/120    avg_loss:0.523, val_acc:0.876]
Epoch [19/120    avg_loss:0.494, val_acc:0.873]
Epoch [20/120    avg_loss:0.398, val_acc:0.851]
Epoch [21/120    avg_loss:0.391, val_acc:0.904]
Epoch [22/120    avg_loss:0.357, val_acc:0.884]
Epoch [23/120    avg_loss:0.305, val_acc:0.863]
Epoch [24/120    avg_loss:0.359, val_acc:0.884]
Epoch [25/120    avg_loss:0.301, val_acc:0.877]
Epoch [26/120    avg_loss:0.229, val_acc:0.917]
Epoch [27/120    avg_loss:0.234, val_acc:0.926]
Epoch [28/120    avg_loss:0.216, val_acc:0.913]
Epoch [29/120    avg_loss:0.287, val_acc:0.923]
Epoch [30/120    avg_loss:0.210, val_acc:0.888]
Epoch [31/120    avg_loss:0.166, val_acc:0.930]
Epoch [32/120    avg_loss:0.169, val_acc:0.929]
Epoch [33/120    avg_loss:0.154, val_acc:0.957]
Epoch [34/120    avg_loss:0.180, val_acc:0.903]
Epoch [35/120    avg_loss:0.129, val_acc:0.951]
Epoch [36/120    avg_loss:0.122, val_acc:0.948]
Epoch [37/120    avg_loss:0.120, val_acc:0.933]
Epoch [38/120    avg_loss:0.085, val_acc:0.935]
Epoch [39/120    avg_loss:0.202, val_acc:0.916]
Epoch [40/120    avg_loss:0.125, val_acc:0.951]
Epoch [41/120    avg_loss:0.105, val_acc:0.935]
Epoch [42/120    avg_loss:0.073, val_acc:0.959]
Epoch [43/120    avg_loss:0.064, val_acc:0.960]
Epoch [44/120    avg_loss:0.044, val_acc:0.958]
Epoch [45/120    avg_loss:0.063, val_acc:0.928]
Epoch [46/120    avg_loss:0.067, val_acc:0.959]
Epoch [47/120    avg_loss:0.086, val_acc:0.950]
Epoch [48/120    avg_loss:0.106, val_acc:0.949]
Epoch [49/120    avg_loss:0.052, val_acc:0.961]
Epoch [50/120    avg_loss:0.048, val_acc:0.974]
Epoch [51/120    avg_loss:0.032, val_acc:0.943]
Epoch [52/120    avg_loss:0.046, val_acc:0.959]
Epoch [53/120    avg_loss:0.038, val_acc:0.967]
Epoch [54/120    avg_loss:0.047, val_acc:0.942]
Epoch [55/120    avg_loss:0.031, val_acc:0.955]
Epoch [56/120    avg_loss:0.035, val_acc:0.966]
Epoch [57/120    avg_loss:0.029, val_acc:0.967]
Epoch [58/120    avg_loss:0.020, val_acc:0.973]
Epoch [59/120    avg_loss:0.090, val_acc:0.943]
Epoch [60/120    avg_loss:1.858, val_acc:0.300]
Epoch [61/120    avg_loss:2.103, val_acc:0.379]
Epoch [62/120    avg_loss:1.801, val_acc:0.441]
Epoch [63/120    avg_loss:1.584, val_acc:0.572]
Epoch [64/120    avg_loss:1.457, val_acc:0.582]
Epoch [65/120    avg_loss:1.452, val_acc:0.576]
Epoch [66/120    avg_loss:1.391, val_acc:0.602]
Epoch [67/120    avg_loss:1.375, val_acc:0.597]
Epoch [68/120    avg_loss:1.343, val_acc:0.611]
Epoch [69/120    avg_loss:1.311, val_acc:0.617]
Epoch [70/120    avg_loss:1.293, val_acc:0.615]
Epoch [71/120    avg_loss:1.284, val_acc:0.625]
Epoch [72/120    avg_loss:1.271, val_acc:0.616]
Epoch [73/120    avg_loss:1.239, val_acc:0.626]
Epoch [74/120    avg_loss:1.208, val_acc:0.634]
Epoch [75/120    avg_loss:1.230, val_acc:0.634]
Epoch [76/120    avg_loss:1.175, val_acc:0.641]
Epoch [77/120    avg_loss:1.159, val_acc:0.639]
Epoch [78/120    avg_loss:1.157, val_acc:0.642]
Epoch [79/120    avg_loss:1.119, val_acc:0.641]
Epoch [80/120    avg_loss:1.124, val_acc:0.640]
Epoch [81/120    avg_loss:1.140, val_acc:0.639]
Epoch [82/120    avg_loss:1.149, val_acc:0.643]
Epoch [83/120    avg_loss:1.184, val_acc:0.645]
Epoch [84/120    avg_loss:1.139, val_acc:0.648]
Epoch [85/120    avg_loss:1.130, val_acc:0.643]
Epoch [86/120    avg_loss:1.150, val_acc:0.647]
Epoch [87/120    avg_loss:1.112, val_acc:0.642]
Epoch [88/120    avg_loss:1.127, val_acc:0.643]
Epoch [89/120    avg_loss:1.125, val_acc:0.643]
Epoch [90/120    avg_loss:1.108, val_acc:0.643]
Epoch [91/120    avg_loss:1.129, val_acc:0.647]
Epoch [92/120    avg_loss:1.155, val_acc:0.645]
Epoch [93/120    avg_loss:1.141, val_acc:0.642]
Epoch [94/120    avg_loss:1.124, val_acc:0.645]
Epoch [95/120    avg_loss:1.138, val_acc:0.642]
Epoch [96/120    avg_loss:1.124, val_acc:0.643]
Epoch [97/120    avg_loss:1.119, val_acc:0.643]
Epoch [98/120    avg_loss:1.125, val_acc:0.642]
Epoch [99/120    avg_loss:1.083, val_acc:0.646]
Epoch [100/120    avg_loss:1.134, val_acc:0.645]
Epoch [101/120    avg_loss:1.117, val_acc:0.645]
Epoch [102/120    avg_loss:1.106, val_acc:0.645]
Epoch [103/120    avg_loss:1.096, val_acc:0.645]
Epoch [104/120    avg_loss:1.134, val_acc:0.645]
Epoch [105/120    avg_loss:1.096, val_acc:0.645]
Epoch [106/120    avg_loss:1.111, val_acc:0.645]
Epoch [107/120    avg_loss:1.091, val_acc:0.645]
Epoch [108/120    avg_loss:1.098, val_acc:0.645]
Epoch [109/120    avg_loss:1.129, val_acc:0.645]
Epoch [110/120    avg_loss:1.131, val_acc:0.645]
Epoch [111/120    avg_loss:1.128, val_acc:0.645]
Epoch [112/120    avg_loss:1.085, val_acc:0.645]
Epoch [113/120    avg_loss:1.097, val_acc:0.645]
Epoch [114/120    avg_loss:1.116, val_acc:0.645]
Epoch [115/120    avg_loss:1.137, val_acc:0.645]
Epoch [116/120    avg_loss:1.106, val_acc:0.645]
Epoch [117/120    avg_loss:1.141, val_acc:0.645]
Epoch [118/120    avg_loss:1.122, val_acc:0.645]
Epoch [119/120    avg_loss:1.144, val_acc:0.645]
Epoch [120/120    avg_loss:1.120, val_acc:0.645]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   21    6    0    0    0    3    0   11    0    0    0    0    0
     0    0    0]
 [   0    0  758   26   31    4    6    0    1    0  165  241   45    3
     5    0    0]
 [   0    0   42  107    0   31   17    0    0    0  118  406   12   12
     2    0    0]
 [   0    0   53   14   83    0    0    0    0    0   39   16    4    4
     0    0    0]
 [   0    0    0    0    7  251   78    0    0    0   12    0    0   41
    46    0    0]
 [   0    0    0    0    0    0  624    0    0    0    0    5    0    0
    28    0    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    3    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    3   15    0    0    0    0    0    0    0
     0    0    0]
 [   0    0  105   28    5   16   54    0    0    0  521  129    7    3
     6    1    0]
 [   0    1  221  158   57    0   41    0   22    0   63 1397  207   35
     6    2    0]
 [   0    0   64    0   11    0    0    0    0    0   17  117  299    4
     0    1   21]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    1    0    0   12
  1097   25    0]
 [   0    0    0    0    1   10   96    0    0    0    0    3    6   41
   159   31    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
63.77235772357724

F1 scores:
[       nan 0.66666667 0.59826361 0.19814815 0.40686275 0.66578249
 0.7708462  0.         0.95847363 0.         0.57537272 0.61759505
 0.53584229 0.7047619  0.8818328  0.15233415 0.87700535]

Kappa:
0.5854118358483174
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbbe42bee80>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.785, val_acc:0.383]
Epoch [2/120    avg_loss:2.658, val_acc:0.497]
Epoch [3/120    avg_loss:2.511, val_acc:0.560]
Epoch [4/120    avg_loss:2.296, val_acc:0.568]
Epoch [5/120    avg_loss:2.090, val_acc:0.574]
Epoch [6/120    avg_loss:1.911, val_acc:0.573]
Epoch [7/120    avg_loss:1.719, val_acc:0.654]
Epoch [8/120    avg_loss:1.545, val_acc:0.651]
Epoch [9/120    avg_loss:1.336, val_acc:0.686]
Epoch [10/120    avg_loss:1.205, val_acc:0.739]
Epoch [11/120    avg_loss:1.012, val_acc:0.752]
Epoch [12/120    avg_loss:0.902, val_acc:0.773]
Epoch [13/120    avg_loss:0.803, val_acc:0.791]
Epoch [14/120    avg_loss:0.715, val_acc:0.815]
Epoch [15/120    avg_loss:0.677, val_acc:0.235]
Epoch [16/120    avg_loss:0.722, val_acc:0.821]
Epoch [17/120    avg_loss:0.830, val_acc:0.814]
Epoch [18/120    avg_loss:0.548, val_acc:0.856]
Epoch [19/120    avg_loss:0.456, val_acc:0.883]
Epoch [20/120    avg_loss:0.352, val_acc:0.940]
Epoch [21/120    avg_loss:0.376, val_acc:0.892]
Epoch [22/120    avg_loss:0.320, val_acc:0.921]
Epoch [23/120    avg_loss:0.301, val_acc:0.886]
Epoch [24/120    avg_loss:0.241, val_acc:0.915]
Epoch [25/120    avg_loss:0.217, val_acc:0.913]
Epoch [26/120    avg_loss:0.244, val_acc:0.909]
Epoch [27/120    avg_loss:0.241, val_acc:0.934]
Epoch [28/120    avg_loss:0.329, val_acc:0.188]
Epoch [29/120    avg_loss:2.405, val_acc:0.357]
Epoch [30/120    avg_loss:1.979, val_acc:0.470]
Epoch [31/120    avg_loss:1.765, val_acc:0.496]
Epoch [32/120    avg_loss:1.671, val_acc:0.506]
Epoch [33/120    avg_loss:1.600, val_acc:0.511]
Epoch [34/120    avg_loss:1.402, val_acc:0.539]
Epoch [35/120    avg_loss:1.373, val_acc:0.555]
Epoch [36/120    avg_loss:1.294, val_acc:0.565]
Epoch [37/120    avg_loss:1.318, val_acc:0.572]
Epoch [38/120    avg_loss:1.322, val_acc:0.586]
Epoch [39/120    avg_loss:1.277, val_acc:0.585]
Epoch [40/120    avg_loss:1.275, val_acc:0.577]
Epoch [41/120    avg_loss:1.239, val_acc:0.589]
Epoch [42/120    avg_loss:1.232, val_acc:0.594]
Epoch [43/120    avg_loss:1.215, val_acc:0.601]
Epoch [44/120    avg_loss:1.248, val_acc:0.595]
Epoch [45/120    avg_loss:1.191, val_acc:0.594]
Epoch [46/120    avg_loss:1.161, val_acc:0.608]
Epoch [47/120    avg_loss:1.177, val_acc:0.608]
Epoch [48/120    avg_loss:1.143, val_acc:0.611]
Epoch [49/120    avg_loss:1.164, val_acc:0.610]
Epoch [50/120    avg_loss:1.136, val_acc:0.610]
Epoch [51/120    avg_loss:1.119, val_acc:0.605]
Epoch [52/120    avg_loss:1.165, val_acc:0.610]
Epoch [53/120    avg_loss:1.148, val_acc:0.607]
Epoch [54/120    avg_loss:1.185, val_acc:0.612]
Epoch [55/120    avg_loss:1.128, val_acc:0.608]
Epoch [56/120    avg_loss:1.162, val_acc:0.616]
Epoch [57/120    avg_loss:1.156, val_acc:0.612]
Epoch [58/120    avg_loss:1.176, val_acc:0.612]
Epoch [59/120    avg_loss:1.137, val_acc:0.608]
Epoch [60/120    avg_loss:1.186, val_acc:0.609]
Epoch [61/120    avg_loss:1.159, val_acc:0.608]
Epoch [62/120    avg_loss:1.130, val_acc:0.608]
Epoch [63/120    avg_loss:1.181, val_acc:0.608]
Epoch [64/120    avg_loss:1.149, val_acc:0.608]
Epoch [65/120    avg_loss:1.122, val_acc:0.608]
Epoch [66/120    avg_loss:1.152, val_acc:0.608]
Epoch [67/120    avg_loss:1.180, val_acc:0.609]
Epoch [68/120    avg_loss:1.162, val_acc:0.610]
Epoch [69/120    avg_loss:1.127, val_acc:0.610]
Epoch [70/120    avg_loss:1.150, val_acc:0.610]
Epoch [71/120    avg_loss:1.132, val_acc:0.611]
Epoch [72/120    avg_loss:1.152, val_acc:0.610]
Epoch [73/120    avg_loss:1.108, val_acc:0.610]
Epoch [74/120    avg_loss:1.163, val_acc:0.610]
Epoch [75/120    avg_loss:1.140, val_acc:0.610]
Epoch [76/120    avg_loss:1.138, val_acc:0.610]
Epoch [77/120    avg_loss:1.144, val_acc:0.610]
Epoch [78/120    avg_loss:1.159, val_acc:0.610]
Epoch [79/120    avg_loss:1.172, val_acc:0.610]
Epoch [80/120    avg_loss:1.164, val_acc:0.610]
Epoch [81/120    avg_loss:1.133, val_acc:0.610]
Epoch [82/120    avg_loss:1.137, val_acc:0.610]
Epoch [83/120    avg_loss:1.131, val_acc:0.610]
Epoch [84/120    avg_loss:1.134, val_acc:0.610]
Epoch [85/120    avg_loss:1.140, val_acc:0.610]
Epoch [86/120    avg_loss:1.127, val_acc:0.610]
Epoch [87/120    avg_loss:1.157, val_acc:0.610]
Epoch [88/120    avg_loss:1.099, val_acc:0.610]
Epoch [89/120    avg_loss:1.126, val_acc:0.610]
Epoch [90/120    avg_loss:1.141, val_acc:0.610]
Epoch [91/120    avg_loss:1.138, val_acc:0.610]
Epoch [92/120    avg_loss:1.123, val_acc:0.610]
Epoch [93/120    avg_loss:1.166, val_acc:0.610]
Epoch [94/120    avg_loss:1.164, val_acc:0.610]
Epoch [95/120    avg_loss:1.174, val_acc:0.610]
Epoch [96/120    avg_loss:1.158, val_acc:0.610]
Epoch [97/120    avg_loss:1.143, val_acc:0.610]
Epoch [98/120    avg_loss:1.127, val_acc:0.610]
Epoch [99/120    avg_loss:1.141, val_acc:0.610]
Epoch [100/120    avg_loss:1.153, val_acc:0.610]
Epoch [101/120    avg_loss:1.093, val_acc:0.610]
Epoch [102/120    avg_loss:1.124, val_acc:0.610]
Epoch [103/120    avg_loss:1.156, val_acc:0.610]
Epoch [104/120    avg_loss:1.140, val_acc:0.610]
Epoch [105/120    avg_loss:1.144, val_acc:0.610]
Epoch [106/120    avg_loss:1.118, val_acc:0.610]
Epoch [107/120    avg_loss:1.138, val_acc:0.610]
Epoch [108/120    avg_loss:1.150, val_acc:0.610]
Epoch [109/120    avg_loss:1.147, val_acc:0.610]
Epoch [110/120    avg_loss:1.203, val_acc:0.610]
Epoch [111/120    avg_loss:1.154, val_acc:0.610]
Epoch [112/120    avg_loss:1.160, val_acc:0.610]
Epoch [113/120    avg_loss:1.164, val_acc:0.610]
Epoch [114/120    avg_loss:1.153, val_acc:0.610]
Epoch [115/120    avg_loss:1.113, val_acc:0.610]
Epoch [116/120    avg_loss:1.139, val_acc:0.610]
Epoch [117/120    avg_loss:1.130, val_acc:0.610]
Epoch [118/120    avg_loss:1.146, val_acc:0.610]
Epoch [119/120    avg_loss:1.097, val_acc:0.610]
Epoch [120/120    avg_loss:1.124, val_acc:0.610]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   15    3    0   10    0    7    0    0    0    4    2    0    0
     0    0    0]
 [   0    0  631   45   65    4    7    0    0    0  200  292   26    0
     7    1    7]
 [   0    0  172  119   97   28    0    0    0    0  131  104   42   32
    22    0    0]
 [   0    0   16   36  159    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    2    0  363   33    0    0    0    9    1    0    0
    26    1    0]
 [   0    0    0   25    3    0  599    0    0    0    0    1    0   17
    12    0    0]
 [   0    0    0    0    0    8   13    0    0    0    0    4    0    0
     0    0    0]
 [   0    1   10    0    8    0    0    0  406    0    0    0    0    5
     0    0    0]
 [   0    0    0    0    1    0   17    0    0    0    0    0    0    0
     0    0    0]
 [   0    0   71   72    0   12    0    0    0    0  445  190   21   32
    11   21    0]
 [   0    0  118  185   22   44    4    0   32    0  168 1384  124   38
    12    0   79]
 [   0    0  156    4   71    1    0    0    6    0   16    0  194    0
     3    0   83]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  184
     1    0    0]
 [   0    0    0    0    0   27    0    0    8    0    0    0    3    1
  1100    0    0]
 [   0    0    0   39    0   22    1    0    1    0    0  106    2    1
   144   31    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
61.94037940379404

F1 scores:
[       nan 0.52631579 0.51259139 0.18681319 0.48998459 0.7690678
 0.89536622 0.         0.9195923  0.         0.48160173 0.6446204
 0.4092827  0.74343434 0.88817117 0.15461347 0.49851632]

Kappa:
0.5677894293134672
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7865399e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.807, val_acc:0.253]
Epoch [2/120    avg_loss:2.647, val_acc:0.421]
Epoch [3/120    avg_loss:2.466, val_acc:0.502]
Epoch [4/120    avg_loss:2.270, val_acc:0.521]
Epoch [5/120    avg_loss:2.099, val_acc:0.570]
Epoch [6/120    avg_loss:1.949, val_acc:0.614]
Epoch [7/120    avg_loss:1.806, val_acc:0.618]
Epoch [8/120    avg_loss:1.631, val_acc:0.623]
Epoch [9/120    avg_loss:1.498, val_acc:0.636]
Epoch [10/120    avg_loss:1.350, val_acc:0.679]
Epoch [11/120    avg_loss:1.168, val_acc:0.718]
Epoch [12/120    avg_loss:0.967, val_acc:0.754]
Epoch [13/120    avg_loss:0.912, val_acc:0.742]
Epoch [14/120    avg_loss:1.018, val_acc:0.707]
Epoch [15/120    avg_loss:0.800, val_acc:0.793]
Epoch [16/120    avg_loss:0.685, val_acc:0.785]
Epoch [17/120    avg_loss:0.596, val_acc:0.816]
Epoch [18/120    avg_loss:0.526, val_acc:0.848]
Epoch [19/120    avg_loss:0.443, val_acc:0.860]
Epoch [20/120    avg_loss:0.453, val_acc:0.861]
Epoch [21/120    avg_loss:0.378, val_acc:0.873]
Epoch [22/120    avg_loss:0.382, val_acc:0.883]
Epoch [23/120    avg_loss:0.296, val_acc:0.883]
Epoch [24/120    avg_loss:0.264, val_acc:0.891]
Epoch [25/120    avg_loss:0.250, val_acc:0.863]
Epoch [26/120    avg_loss:0.327, val_acc:0.907]
Epoch [27/120    avg_loss:0.215, val_acc:0.898]
Epoch [28/120    avg_loss:0.163, val_acc:0.905]
Epoch [29/120    avg_loss:0.149, val_acc:0.942]
Epoch [30/120    avg_loss:0.234, val_acc:0.932]
Epoch [31/120    avg_loss:0.148, val_acc:0.923]
Epoch [32/120    avg_loss:0.182, val_acc:0.941]
Epoch [33/120    avg_loss:0.156, val_acc:0.914]
Epoch [34/120    avg_loss:0.117, val_acc:0.936]
Epoch [35/120    avg_loss:0.122, val_acc:0.934]
Epoch [36/120    avg_loss:0.124, val_acc:0.946]
Epoch [37/120    avg_loss:0.116, val_acc:0.912]
Epoch [38/120    avg_loss:0.102, val_acc:0.933]
Epoch [39/120    avg_loss:0.086, val_acc:0.955]
Epoch [40/120    avg_loss:0.097, val_acc:0.932]
Epoch [41/120    avg_loss:0.154, val_acc:0.942]
Epoch [42/120    avg_loss:0.159, val_acc:0.949]
Epoch [43/120    avg_loss:0.078, val_acc:0.961]
Epoch [44/120    avg_loss:0.071, val_acc:0.943]
Epoch [45/120    avg_loss:0.065, val_acc:0.953]
Epoch [46/120    avg_loss:0.059, val_acc:0.964]
Epoch [47/120    avg_loss:0.052, val_acc:0.933]
Epoch [48/120    avg_loss:0.079, val_acc:0.953]
Epoch [49/120    avg_loss:0.045, val_acc:0.961]
Epoch [50/120    avg_loss:0.047, val_acc:0.966]
Epoch [51/120    avg_loss:0.042, val_acc:0.964]
Epoch [52/120    avg_loss:0.041, val_acc:0.957]
Epoch [53/120    avg_loss:0.041, val_acc:0.950]
Epoch [54/120    avg_loss:0.060, val_acc:0.920]
Epoch [55/120    avg_loss:0.061, val_acc:0.962]
Epoch [56/120    avg_loss:0.050, val_acc:0.948]
Epoch [57/120    avg_loss:0.034, val_acc:0.974]
Epoch [58/120    avg_loss:0.030, val_acc:0.961]
Epoch [59/120    avg_loss:0.021, val_acc:0.970]
Epoch [60/120    avg_loss:0.027, val_acc:0.962]
Epoch [61/120    avg_loss:0.028, val_acc:0.957]
Epoch [62/120    avg_loss:0.023, val_acc:0.967]
Epoch [63/120    avg_loss:0.040, val_acc:0.941]
Epoch [64/120    avg_loss:0.038, val_acc:0.968]
Epoch [65/120    avg_loss:0.024, val_acc:0.968]
Epoch [66/120    avg_loss:0.036, val_acc:0.966]
Epoch [67/120    avg_loss:0.022, val_acc:0.968]
Epoch [68/120    avg_loss:0.020, val_acc:0.975]
Epoch [69/120    avg_loss:0.017, val_acc:0.963]
Epoch [70/120    avg_loss:0.019, val_acc:0.965]
Epoch [71/120    avg_loss:0.028, val_acc:0.967]
Epoch [72/120    avg_loss:0.022, val_acc:0.968]
Epoch [73/120    avg_loss:0.022, val_acc:0.966]
Epoch [74/120    avg_loss:0.021, val_acc:0.973]
Epoch [75/120    avg_loss:0.018, val_acc:0.966]
Epoch [76/120    avg_loss:0.017, val_acc:0.967]
Epoch [77/120    avg_loss:0.049, val_acc:0.964]
Epoch [78/120    avg_loss:0.055, val_acc:0.960]
Epoch [79/120    avg_loss:0.039, val_acc:0.950]
Epoch [80/120    avg_loss:0.051, val_acc:0.964]
Epoch [81/120    avg_loss:0.027, val_acc:0.966]
Epoch [82/120    avg_loss:0.016, val_acc:0.970]
Epoch [83/120    avg_loss:0.014, val_acc:0.971]
Epoch [84/120    avg_loss:0.015, val_acc:0.972]
Epoch [85/120    avg_loss:0.013, val_acc:0.976]
Epoch [86/120    avg_loss:0.013, val_acc:0.972]
Epoch [87/120    avg_loss:0.012, val_acc:0.973]
Epoch [88/120    avg_loss:0.012, val_acc:0.974]
Epoch [89/120    avg_loss:0.010, val_acc:0.968]
Epoch [90/120    avg_loss:0.012, val_acc:0.973]
Epoch [91/120    avg_loss:0.012, val_acc:0.972]
Epoch [92/120    avg_loss:0.014, val_acc:0.974]
Epoch [93/120    avg_loss:0.012, val_acc:0.975]
Epoch [94/120    avg_loss:0.011, val_acc:0.971]
Epoch [95/120    avg_loss:0.009, val_acc:0.971]
Epoch [96/120    avg_loss:0.010, val_acc:0.968]
Epoch [97/120    avg_loss:0.010, val_acc:0.971]
Epoch [98/120    avg_loss:0.009, val_acc:0.971]
Epoch [99/120    avg_loss:0.010, val_acc:0.971]
Epoch [100/120    avg_loss:0.010, val_acc:0.971]
Epoch [101/120    avg_loss:0.008, val_acc:0.971]
Epoch [102/120    avg_loss:0.009, val_acc:0.970]
Epoch [103/120    avg_loss:0.012, val_acc:0.970]
Epoch [104/120    avg_loss:0.009, val_acc:0.970]
Epoch [105/120    avg_loss:0.010, val_acc:0.970]
Epoch [106/120    avg_loss:0.008, val_acc:0.968]
Epoch [107/120    avg_loss:0.009, val_acc:0.968]
Epoch [108/120    avg_loss:0.011, val_acc:0.968]
Epoch [109/120    avg_loss:0.009, val_acc:0.971]
Epoch [110/120    avg_loss:0.011, val_acc:0.970]
Epoch [111/120    avg_loss:0.008, val_acc:0.968]
Epoch [112/120    avg_loss:0.010, val_acc:0.970]
Epoch [113/120    avg_loss:0.010, val_acc:0.970]
Epoch [114/120    avg_loss:0.010, val_acc:0.970]
Epoch [115/120    avg_loss:0.009, val_acc:0.968]
Epoch [116/120    avg_loss:0.010, val_acc:0.968]
Epoch [117/120    avg_loss:0.008, val_acc:0.968]
Epoch [118/120    avg_loss:0.008, val_acc:0.968]
Epoch [119/120    avg_loss:0.010, val_acc:0.968]
Epoch [120/120    avg_loss:0.011, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1253    5    5    0    0    0    0    0    5    9    0    0
     1    7    0]
 [   0    0    1  732    1    9    0    0    0    0    0    2    1    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3   12    0    2    0    0    0    0  853    3    0    0
     1    1    0]
 [   0    0   18    1    0    0    1    0    0    0   14 2175    1    0
     0    0    0]
 [   0    0    0    7    0    2    0    0    0    0    1    4  517    0
     0    3    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1136    3    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    71  272    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.81029810298104

F1 scores:
[       nan 0.98765432 0.97852401 0.97340426 0.98611111 0.98412698
 0.99620925 1.         1.         1.         0.97597254 0.98773842
 0.98102467 0.99730458 0.96763203 0.85939968 0.99401198]

Kappa:
0.9750288114409733
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:04:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f29f4de8f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.760, val_acc:0.368]
Epoch [2/120    avg_loss:2.563, val_acc:0.457]
Epoch [3/120    avg_loss:2.329, val_acc:0.513]
Epoch [4/120    avg_loss:2.129, val_acc:0.578]
Epoch [5/120    avg_loss:1.903, val_acc:0.608]
Epoch [6/120    avg_loss:1.697, val_acc:0.625]
Epoch [7/120    avg_loss:1.544, val_acc:0.608]
Epoch [8/120    avg_loss:1.373, val_acc:0.707]
Epoch [9/120    avg_loss:1.221, val_acc:0.730]
Epoch [10/120    avg_loss:1.055, val_acc:0.736]
Epoch [11/120    avg_loss:0.989, val_acc:0.710]
Epoch [12/120    avg_loss:0.889, val_acc:0.796]
Epoch [13/120    avg_loss:0.752, val_acc:0.760]
Epoch [14/120    avg_loss:0.754, val_acc:0.772]
Epoch [15/120    avg_loss:0.627, val_acc:0.800]
Epoch [16/120    avg_loss:0.586, val_acc:0.819]
Epoch [17/120    avg_loss:0.517, val_acc:0.813]
Epoch [18/120    avg_loss:0.400, val_acc:0.864]
Epoch [19/120    avg_loss:0.385, val_acc:0.876]
Epoch [20/120    avg_loss:0.383, val_acc:0.861]
Epoch [21/120    avg_loss:0.344, val_acc:0.877]
Epoch [22/120    avg_loss:0.320, val_acc:0.864]
Epoch [23/120    avg_loss:0.281, val_acc:0.893]
Epoch [24/120    avg_loss:0.244, val_acc:0.901]
Epoch [25/120    avg_loss:0.213, val_acc:0.938]
Epoch [26/120    avg_loss:0.179, val_acc:0.925]
Epoch [27/120    avg_loss:0.191, val_acc:0.916]
Epoch [28/120    avg_loss:0.182, val_acc:0.902]
Epoch [29/120    avg_loss:0.170, val_acc:0.879]
Epoch [30/120    avg_loss:0.117, val_acc:0.948]
Epoch [31/120    avg_loss:0.103, val_acc:0.943]
Epoch [32/120    avg_loss:0.114, val_acc:0.933]
Epoch [33/120    avg_loss:0.098, val_acc:0.956]
Epoch [34/120    avg_loss:0.096, val_acc:0.942]
Epoch [35/120    avg_loss:0.072, val_acc:0.960]
Epoch [36/120    avg_loss:0.077, val_acc:0.957]
Epoch [37/120    avg_loss:0.078, val_acc:0.958]
Epoch [38/120    avg_loss:0.071, val_acc:0.970]
Epoch [39/120    avg_loss:0.082, val_acc:0.953]
Epoch [40/120    avg_loss:0.127, val_acc:0.951]
Epoch [41/120    avg_loss:0.096, val_acc:0.948]
Epoch [42/120    avg_loss:0.076, val_acc:0.950]
Epoch [43/120    avg_loss:0.079, val_acc:0.958]
Epoch [44/120    avg_loss:0.078, val_acc:0.935]
Epoch [45/120    avg_loss:0.069, val_acc:0.963]
Epoch [46/120    avg_loss:0.049, val_acc:0.953]
Epoch [47/120    avg_loss:0.040, val_acc:0.959]
Epoch [48/120    avg_loss:0.046, val_acc:0.939]
Epoch [49/120    avg_loss:0.038, val_acc:0.973]
Epoch [50/120    avg_loss:0.056, val_acc:0.932]
Epoch [51/120    avg_loss:0.060, val_acc:0.969]
Epoch [52/120    avg_loss:0.023, val_acc:0.969]
Epoch [53/120    avg_loss:0.072, val_acc:0.960]
Epoch [54/120    avg_loss:0.049, val_acc:0.964]
Epoch [55/120    avg_loss:0.045, val_acc:0.971]
Epoch [56/120    avg_loss:0.042, val_acc:0.973]
Epoch [57/120    avg_loss:0.067, val_acc:0.941]
Epoch [58/120    avg_loss:0.046, val_acc:0.962]
Epoch [59/120    avg_loss:0.027, val_acc:0.970]
Epoch [60/120    avg_loss:0.050, val_acc:0.925]
Epoch [61/120    avg_loss:0.091, val_acc:0.960]
Epoch [62/120    avg_loss:0.042, val_acc:0.968]
Epoch [63/120    avg_loss:0.036, val_acc:0.964]
Epoch [64/120    avg_loss:0.026, val_acc:0.970]
Epoch [65/120    avg_loss:0.019, val_acc:0.978]
Epoch [66/120    avg_loss:0.030, val_acc:0.968]
Epoch [67/120    avg_loss:0.016, val_acc:0.962]
Epoch [68/120    avg_loss:0.028, val_acc:0.980]
Epoch [69/120    avg_loss:0.027, val_acc:0.966]
Epoch [70/120    avg_loss:0.016, val_acc:0.958]
Epoch [71/120    avg_loss:0.022, val_acc:0.938]
Epoch [72/120    avg_loss:0.040, val_acc:0.969]
Epoch [73/120    avg_loss:0.020, val_acc:0.976]
Epoch [74/120    avg_loss:0.012, val_acc:0.972]
Epoch [75/120    avg_loss:0.016, val_acc:0.979]
Epoch [76/120    avg_loss:0.018, val_acc:0.974]
Epoch [77/120    avg_loss:0.014, val_acc:0.975]
Epoch [78/120    avg_loss:0.015, val_acc:0.978]
Epoch [79/120    avg_loss:0.011, val_acc:0.979]
Epoch [80/120    avg_loss:0.008, val_acc:0.982]
Epoch [81/120    avg_loss:0.015, val_acc:0.980]
Epoch [82/120    avg_loss:0.012, val_acc:0.968]
Epoch [83/120    avg_loss:0.009, val_acc:0.986]
Epoch [84/120    avg_loss:0.007, val_acc:0.974]
Epoch [85/120    avg_loss:0.008, val_acc:0.985]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.007, val_acc:0.986]
Epoch [88/120    avg_loss:0.024, val_acc:0.965]
Epoch [89/120    avg_loss:0.010, val_acc:0.978]
Epoch [90/120    avg_loss:0.010, val_acc:0.969]
Epoch [91/120    avg_loss:0.010, val_acc:0.978]
Epoch [92/120    avg_loss:0.008, val_acc:0.978]
Epoch [93/120    avg_loss:0.008, val_acc:0.978]
Epoch [94/120    avg_loss:0.023, val_acc:0.940]
Epoch [95/120    avg_loss:0.033, val_acc:0.980]
Epoch [96/120    avg_loss:0.010, val_acc:0.975]
Epoch [97/120    avg_loss:0.014, val_acc:0.983]
Epoch [98/120    avg_loss:0.020, val_acc:0.973]
Epoch [99/120    avg_loss:0.014, val_acc:0.978]
Epoch [100/120    avg_loss:0.009, val_acc:0.980]
Epoch [101/120    avg_loss:0.007, val_acc:0.982]
Epoch [102/120    avg_loss:0.007, val_acc:0.980]
Epoch [103/120    avg_loss:0.007, val_acc:0.983]
Epoch [104/120    avg_loss:0.006, val_acc:0.983]
Epoch [105/120    avg_loss:0.007, val_acc:0.983]
Epoch [106/120    avg_loss:0.006, val_acc:0.984]
Epoch [107/120    avg_loss:0.005, val_acc:0.983]
Epoch [108/120    avg_loss:0.006, val_acc:0.983]
Epoch [109/120    avg_loss:0.005, val_acc:0.984]
Epoch [110/120    avg_loss:0.008, val_acc:0.984]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.006, val_acc:0.985]
Epoch [113/120    avg_loss:0.005, val_acc:0.984]
Epoch [114/120    avg_loss:0.005, val_acc:0.984]
Epoch [115/120    avg_loss:0.005, val_acc:0.984]
Epoch [116/120    avg_loss:0.005, val_acc:0.984]
Epoch [117/120    avg_loss:0.006, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.005, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1272    0    1    0    0    0    0    1    2    8    1    0
     0    0    0]
 [   0    0    0  733    0    1    0    0    0   11    0    2    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3   37    0    1    1    0    0    0  823   10    0    0
     0    0    0]
 [   0    0    0    0    0    1    0    0    4    0   11 2193    0    1
     0    0    0]
 [   0    0    0    7    0    3    0    0    0    0   13   27  480    0
     1    2    1]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    84  262    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.38753387533875

F1 scores:
[       nan 0.98765432 0.99336197 0.96194226 0.99765808 0.98971429
 0.99771863 1.         0.99537037 0.73469388 0.95475638 0.9851752
 0.94488189 0.99459459 0.96402878 0.85761047 0.98809524]

Kappa:
0.9701739503752006
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd467e25eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.761, val_acc:0.339]
Epoch [2/120    avg_loss:2.568, val_acc:0.433]
Epoch [3/120    avg_loss:2.397, val_acc:0.498]
Epoch [4/120    avg_loss:2.188, val_acc:0.536]
Epoch [5/120    avg_loss:2.000, val_acc:0.560]
Epoch [6/120    avg_loss:1.803, val_acc:0.603]
Epoch [7/120    avg_loss:1.678, val_acc:0.628]
Epoch [8/120    avg_loss:1.452, val_acc:0.661]
Epoch [9/120    avg_loss:1.355, val_acc:0.664]
Epoch [10/120    avg_loss:1.215, val_acc:0.689]
Epoch [11/120    avg_loss:1.046, val_acc:0.730]
Epoch [12/120    avg_loss:0.939, val_acc:0.764]
Epoch [13/120    avg_loss:0.821, val_acc:0.769]
Epoch [14/120    avg_loss:0.706, val_acc:0.817]
Epoch [15/120    avg_loss:0.659, val_acc:0.756]
Epoch [16/120    avg_loss:0.545, val_acc:0.801]
Epoch [17/120    avg_loss:0.510, val_acc:0.850]
Epoch [18/120    avg_loss:0.466, val_acc:0.822]
Epoch [19/120    avg_loss:0.424, val_acc:0.869]
Epoch [20/120    avg_loss:0.371, val_acc:0.835]
Epoch [21/120    avg_loss:0.368, val_acc:0.853]
Epoch [22/120    avg_loss:1.923, val_acc:0.441]
Epoch [23/120    avg_loss:1.893, val_acc:0.471]
Epoch [24/120    avg_loss:1.808, val_acc:0.506]
Epoch [25/120    avg_loss:1.694, val_acc:0.536]
Epoch [26/120    avg_loss:1.643, val_acc:0.539]
Epoch [27/120    avg_loss:1.619, val_acc:0.539]
Epoch [28/120    avg_loss:1.534, val_acc:0.560]
Epoch [29/120    avg_loss:1.479, val_acc:0.592]
Epoch [30/120    avg_loss:1.429, val_acc:0.594]
Epoch [31/120    avg_loss:1.401, val_acc:0.615]
Epoch [32/120    avg_loss:1.379, val_acc:0.613]
Epoch [33/120    avg_loss:1.324, val_acc:0.622]
Epoch [34/120    avg_loss:1.297, val_acc:0.627]
Epoch [35/120    avg_loss:1.288, val_acc:0.627]
Epoch [36/120    avg_loss:1.282, val_acc:0.630]
Epoch [37/120    avg_loss:1.287, val_acc:0.631]
Epoch [38/120    avg_loss:1.247, val_acc:0.629]
Epoch [39/120    avg_loss:1.257, val_acc:0.629]
Epoch [40/120    avg_loss:1.279, val_acc:0.630]
Epoch [41/120    avg_loss:1.253, val_acc:0.627]
Epoch [42/120    avg_loss:1.235, val_acc:0.627]
Epoch [43/120    avg_loss:1.257, val_acc:0.625]
Epoch [44/120    avg_loss:1.284, val_acc:0.628]
Epoch [45/120    avg_loss:1.252, val_acc:0.636]
Epoch [46/120    avg_loss:1.260, val_acc:0.635]
Epoch [47/120    avg_loss:1.234, val_acc:0.634]
Epoch [48/120    avg_loss:1.244, val_acc:0.633]
Epoch [49/120    avg_loss:1.259, val_acc:0.631]
Epoch [50/120    avg_loss:1.215, val_acc:0.633]
Epoch [51/120    avg_loss:1.269, val_acc:0.633]
Epoch [52/120    avg_loss:1.247, val_acc:0.631]
Epoch [53/120    avg_loss:1.260, val_acc:0.631]
Epoch [54/120    avg_loss:1.200, val_acc:0.633]
Epoch [55/120    avg_loss:1.217, val_acc:0.633]
Epoch [56/120    avg_loss:1.255, val_acc:0.630]
Epoch [57/120    avg_loss:1.218, val_acc:0.630]
Epoch [58/120    avg_loss:1.228, val_acc:0.630]
Epoch [59/120    avg_loss:1.230, val_acc:0.630]
Epoch [60/120    avg_loss:1.230, val_acc:0.630]
Epoch [61/120    avg_loss:1.273, val_acc:0.630]
Epoch [62/120    avg_loss:1.243, val_acc:0.630]
Epoch [63/120    avg_loss:1.248, val_acc:0.629]
Epoch [64/120    avg_loss:1.237, val_acc:0.630]
Epoch [65/120    avg_loss:1.185, val_acc:0.630]
Epoch [66/120    avg_loss:1.254, val_acc:0.630]
Epoch [67/120    avg_loss:1.231, val_acc:0.630]
Epoch [68/120    avg_loss:1.212, val_acc:0.630]
Epoch [69/120    avg_loss:1.209, val_acc:0.630]
Epoch [70/120    avg_loss:1.253, val_acc:0.630]
Epoch [71/120    avg_loss:1.218, val_acc:0.630]
Epoch [72/120    avg_loss:1.245, val_acc:0.630]
Epoch [73/120    avg_loss:1.249, val_acc:0.630]
Epoch [74/120    avg_loss:1.242, val_acc:0.630]
Epoch [75/120    avg_loss:1.232, val_acc:0.630]
Epoch [76/120    avg_loss:1.228, val_acc:0.630]
Epoch [77/120    avg_loss:1.241, val_acc:0.630]
Epoch [78/120    avg_loss:1.244, val_acc:0.630]
Epoch [79/120    avg_loss:1.236, val_acc:0.630]
Epoch [80/120    avg_loss:1.260, val_acc:0.630]
Epoch [81/120    avg_loss:1.261, val_acc:0.630]
Epoch [82/120    avg_loss:1.237, val_acc:0.630]
Epoch [83/120    avg_loss:1.224, val_acc:0.630]
Epoch [84/120    avg_loss:1.239, val_acc:0.630]
Epoch [85/120    avg_loss:1.183, val_acc:0.630]
Epoch [86/120    avg_loss:1.205, val_acc:0.630]
Epoch [87/120    avg_loss:1.239, val_acc:0.630]
Epoch [88/120    avg_loss:1.238, val_acc:0.630]
Epoch [89/120    avg_loss:1.238, val_acc:0.630]
Epoch [90/120    avg_loss:1.272, val_acc:0.630]
Epoch [91/120    avg_loss:1.229, val_acc:0.630]
Epoch [92/120    avg_loss:1.210, val_acc:0.630]
Epoch [93/120    avg_loss:1.232, val_acc:0.630]
Epoch [94/120    avg_loss:1.302, val_acc:0.630]
Epoch [95/120    avg_loss:1.248, val_acc:0.630]
Epoch [96/120    avg_loss:1.241, val_acc:0.630]
Epoch [97/120    avg_loss:1.219, val_acc:0.630]
Epoch [98/120    avg_loss:1.217, val_acc:0.630]
Epoch [99/120    avg_loss:1.280, val_acc:0.630]
Epoch [100/120    avg_loss:1.259, val_acc:0.630]
Epoch [101/120    avg_loss:1.229, val_acc:0.630]
Epoch [102/120    avg_loss:1.214, val_acc:0.630]
Epoch [103/120    avg_loss:1.216, val_acc:0.630]
Epoch [104/120    avg_loss:1.270, val_acc:0.630]
Epoch [105/120    avg_loss:1.230, val_acc:0.630]
Epoch [106/120    avg_loss:1.207, val_acc:0.630]
Epoch [107/120    avg_loss:1.197, val_acc:0.630]
Epoch [108/120    avg_loss:1.205, val_acc:0.630]
Epoch [109/120    avg_loss:1.223, val_acc:0.630]
Epoch [110/120    avg_loss:1.245, val_acc:0.630]
Epoch [111/120    avg_loss:1.247, val_acc:0.630]
Epoch [112/120    avg_loss:1.243, val_acc:0.630]
Epoch [113/120    avg_loss:1.278, val_acc:0.630]
Epoch [114/120    avg_loss:1.244, val_acc:0.630]
Epoch [115/120    avg_loss:1.197, val_acc:0.630]
Epoch [116/120    avg_loss:1.242, val_acc:0.630]
Epoch [117/120    avg_loss:1.213, val_acc:0.630]
Epoch [118/120    avg_loss:1.239, val_acc:0.630]
Epoch [119/120    avg_loss:1.239, val_acc:0.630]
Epoch [120/120    avg_loss:1.230, val_acc:0.630]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   25    0    0   11    0    1    0    0    0    4    0    0    0
     0    0    0]
 [   0    5  901   36   50    0   68    0    0    0  119   71   30    5
     0    0    0]
 [   0    0   58  235   65   44   49    0    0    0  102   61   48   85
     0    0    0]
 [   0    0    1    3  190    0    7    0    0    0    0    0   12    0
     0    0    0]
 [   0    0   11    0   11  264  105    0    0    0    0    0    0    2
    42    0    0]
 [   0    0    0   12    0    0  561    0    0    0    0    1    0   62
    21    0    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
     0    0    0]
 [   0   18    0    3    2    0    0    0  396    0    0   11    0    0
     0    0    0]
 [   0    0    1    0    0    1   16    0    0    0    0    0    0    0
     0    0    0]
 [   0    0   94   33   33   20   10    0    0    0  489   88   51   57
     0    0    0]
 [   0    4  245  100  115   12   83    0   11    0   19 1390   84   57
     0    0   90]
 [   0    0    0   40   60   11    2    0    1    0   23    0  344    0
     0   18   35]
 [   0    0    0    0    0    0   22    0    0    0    0    0    0  163
     0    0    0]
 [   0    0   58    0   10    1    0    0   19    0    0    0   19    0
  1029    3    0]
 [   0    0   58    0   21    0   44    0   15    0    0  142   25    0
    34    8    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
65.8970189701897

F1 scores:
[       nan 0.53763441 0.66445428 0.38875103 0.4865557  0.67005076
 0.68       0.         0.90825688 0.         0.59963213 0.69954706
 0.59982563 0.52922078 0.90860927 0.04255319 0.57337884]

Kappa:
0.6157345938130011
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fab50f51eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.767, val_acc:0.298]
Epoch [2/120    avg_loss:2.601, val_acc:0.372]
Epoch [3/120    avg_loss:2.376, val_acc:0.493]
Epoch [4/120    avg_loss:2.166, val_acc:0.582]
Epoch [5/120    avg_loss:1.891, val_acc:0.598]
Epoch [6/120    avg_loss:1.664, val_acc:0.654]
Epoch [7/120    avg_loss:1.453, val_acc:0.698]
Epoch [8/120    avg_loss:1.268, val_acc:0.739]
Epoch [9/120    avg_loss:1.112, val_acc:0.762]
Epoch [10/120    avg_loss:0.970, val_acc:0.772]
Epoch [11/120    avg_loss:0.802, val_acc:0.799]
Epoch [12/120    avg_loss:0.711, val_acc:0.842]
Epoch [13/120    avg_loss:0.590, val_acc:0.822]
Epoch [14/120    avg_loss:0.587, val_acc:0.842]
Epoch [15/120    avg_loss:0.510, val_acc:0.841]
Epoch [16/120    avg_loss:0.451, val_acc:0.887]
Epoch [17/120    avg_loss:0.371, val_acc:0.885]
Epoch [18/120    avg_loss:0.319, val_acc:0.891]
Epoch [19/120    avg_loss:0.320, val_acc:0.915]
Epoch [20/120    avg_loss:0.314, val_acc:0.878]
Epoch [21/120    avg_loss:0.255, val_acc:0.914]
Epoch [22/120    avg_loss:0.246, val_acc:0.937]
Epoch [23/120    avg_loss:0.213, val_acc:0.934]
Epoch [24/120    avg_loss:0.190, val_acc:0.937]
Epoch [25/120    avg_loss:0.191, val_acc:0.926]
Epoch [26/120    avg_loss:0.168, val_acc:0.927]
Epoch [27/120    avg_loss:0.169, val_acc:0.947]
Epoch [28/120    avg_loss:0.120, val_acc:0.957]
Epoch [29/120    avg_loss:0.109, val_acc:0.955]
Epoch [30/120    avg_loss:0.145, val_acc:0.939]
Epoch [31/120    avg_loss:0.138, val_acc:0.911]
Epoch [32/120    avg_loss:0.117, val_acc:0.952]
Epoch [33/120    avg_loss:0.064, val_acc:0.961]
Epoch [34/120    avg_loss:0.078, val_acc:0.962]
Epoch [35/120    avg_loss:0.094, val_acc:0.950]
Epoch [36/120    avg_loss:0.083, val_acc:0.967]
Epoch [37/120    avg_loss:0.051, val_acc:0.970]
Epoch [38/120    avg_loss:0.055, val_acc:0.954]
Epoch [39/120    avg_loss:0.063, val_acc:0.972]
Epoch [40/120    avg_loss:0.061, val_acc:0.959]
Epoch [41/120    avg_loss:0.066, val_acc:0.961]
Epoch [42/120    avg_loss:0.043, val_acc:0.962]
Epoch [43/120    avg_loss:0.065, val_acc:0.957]
Epoch [44/120    avg_loss:0.060, val_acc:0.971]
Epoch [45/120    avg_loss:0.068, val_acc:0.960]
Epoch [46/120    avg_loss:0.045, val_acc:0.965]
Epoch [47/120    avg_loss:0.063, val_acc:0.965]
Epoch [48/120    avg_loss:0.043, val_acc:0.973]
Epoch [49/120    avg_loss:0.045, val_acc:0.958]
Epoch [50/120    avg_loss:0.053, val_acc:0.965]
Epoch [51/120    avg_loss:0.052, val_acc:0.924]
Epoch [52/120    avg_loss:0.100, val_acc:0.968]
Epoch [53/120    avg_loss:0.035, val_acc:0.977]
Epoch [54/120    avg_loss:0.032, val_acc:0.980]
Epoch [55/120    avg_loss:0.036, val_acc:0.977]
Epoch [56/120    avg_loss:0.029, val_acc:0.975]
Epoch [57/120    avg_loss:0.024, val_acc:0.976]
Epoch [58/120    avg_loss:0.021, val_acc:0.978]
Epoch [59/120    avg_loss:0.024, val_acc:0.973]
Epoch [60/120    avg_loss:0.056, val_acc:0.967]
Epoch [61/120    avg_loss:0.040, val_acc:0.968]
Epoch [62/120    avg_loss:0.018, val_acc:0.976]
Epoch [63/120    avg_loss:0.030, val_acc:0.964]
Epoch [64/120    avg_loss:0.028, val_acc:0.976]
Epoch [65/120    avg_loss:0.036, val_acc:0.972]
Epoch [66/120    avg_loss:0.026, val_acc:0.965]
Epoch [67/120    avg_loss:0.026, val_acc:0.974]
Epoch [68/120    avg_loss:0.017, val_acc:0.977]
Epoch [69/120    avg_loss:0.017, val_acc:0.976]
Epoch [70/120    avg_loss:0.010, val_acc:0.977]
Epoch [71/120    avg_loss:0.011, val_acc:0.976]
Epoch [72/120    avg_loss:0.009, val_acc:0.977]
Epoch [73/120    avg_loss:0.011, val_acc:0.977]
Epoch [74/120    avg_loss:0.010, val_acc:0.978]
Epoch [75/120    avg_loss:0.010, val_acc:0.976]
Epoch [76/120    avg_loss:0.012, val_acc:0.976]
Epoch [77/120    avg_loss:0.009, val_acc:0.977]
Epoch [78/120    avg_loss:0.012, val_acc:0.977]
Epoch [79/120    avg_loss:0.011, val_acc:0.976]
Epoch [80/120    avg_loss:0.011, val_acc:0.976]
Epoch [81/120    avg_loss:0.007, val_acc:0.976]
Epoch [82/120    avg_loss:0.009, val_acc:0.976]
Epoch [83/120    avg_loss:0.012, val_acc:0.976]
Epoch [84/120    avg_loss:0.009, val_acc:0.976]
Epoch [85/120    avg_loss:0.009, val_acc:0.976]
Epoch [86/120    avg_loss:0.008, val_acc:0.976]
Epoch [87/120    avg_loss:0.009, val_acc:0.976]
Epoch [88/120    avg_loss:0.009, val_acc:0.976]
Epoch [89/120    avg_loss:0.010, val_acc:0.976]
Epoch [90/120    avg_loss:0.012, val_acc:0.976]
Epoch [91/120    avg_loss:0.008, val_acc:0.976]
Epoch [92/120    avg_loss:0.009, val_acc:0.976]
Epoch [93/120    avg_loss:0.009, val_acc:0.976]
Epoch [94/120    avg_loss:0.011, val_acc:0.976]
Epoch [95/120    avg_loss:0.009, val_acc:0.976]
Epoch [96/120    avg_loss:0.008, val_acc:0.976]
Epoch [97/120    avg_loss:0.011, val_acc:0.976]
Epoch [98/120    avg_loss:0.010, val_acc:0.976]
Epoch [99/120    avg_loss:0.010, val_acc:0.976]
Epoch [100/120    avg_loss:0.012, val_acc:0.976]
Epoch [101/120    avg_loss:0.009, val_acc:0.976]
Epoch [102/120    avg_loss:0.008, val_acc:0.976]
Epoch [103/120    avg_loss:0.013, val_acc:0.976]
Epoch [104/120    avg_loss:0.009, val_acc:0.976]
Epoch [105/120    avg_loss:0.009, val_acc:0.976]
Epoch [106/120    avg_loss:0.011, val_acc:0.976]
Epoch [107/120    avg_loss:0.009, val_acc:0.976]
Epoch [108/120    avg_loss:0.010, val_acc:0.976]
Epoch [109/120    avg_loss:0.012, val_acc:0.976]
Epoch [110/120    avg_loss:0.010, val_acc:0.976]
Epoch [111/120    avg_loss:0.008, val_acc:0.976]
Epoch [112/120    avg_loss:0.010, val_acc:0.976]
Epoch [113/120    avg_loss:0.008, val_acc:0.976]
Epoch [114/120    avg_loss:0.011, val_acc:0.976]
Epoch [115/120    avg_loss:0.010, val_acc:0.976]
Epoch [116/120    avg_loss:0.009, val_acc:0.976]
Epoch [117/120    avg_loss:0.010, val_acc:0.976]
Epoch [118/120    avg_loss:0.009, val_acc:0.976]
Epoch [119/120    avg_loss:0.010, val_acc:0.976]
Epoch [120/120    avg_loss:0.011, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1257    2    9    0    0    0    0    0    3   14    0    0
     0    0    0]
 [   0    0    2  707    9   19    0    0    0    6    0    0    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    1    0    0    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    1    9    0    6    1    0    0    0  845   10    3    0
     0    0    0]
 [   0    0   11    0    0    0    0    0    3    0   10 2182    3    0
     0    0    1]
 [   0    0    2   11    0    3    0    0    0    0    0   24  493    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1138    1    0]
 [   0    0    0    0    0    0    6    0    0   10    0    0    0    0
    94  237    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.84552845528455

F1 scores:
[       nan 1.         0.98279906 0.95799458 0.95945946 0.95955056
 0.99317665 0.98039216 0.99652375 0.66666667 0.97518754 0.98288288
 0.94807692 1.         0.95630252 0.81025641 0.97619048]

Kappa:
0.9639969630830314
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f32b89fceb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.780, val_acc:0.169]
Epoch [2/120    avg_loss:2.616, val_acc:0.361]
Epoch [3/120    avg_loss:2.405, val_acc:0.552]
Epoch [4/120    avg_loss:2.227, val_acc:0.580]
Epoch [5/120    avg_loss:2.016, val_acc:0.588]
Epoch [6/120    avg_loss:1.824, val_acc:0.604]
Epoch [7/120    avg_loss:1.689, val_acc:0.670]
Epoch [8/120    avg_loss:1.506, val_acc:0.696]
Epoch [9/120    avg_loss:1.333, val_acc:0.705]
Epoch [10/120    avg_loss:1.208, val_acc:0.773]
Epoch [11/120    avg_loss:0.992, val_acc:0.783]
Epoch [12/120    avg_loss:0.859, val_acc:0.811]
Epoch [13/120    avg_loss:0.727, val_acc:0.827]
Epoch [14/120    avg_loss:0.686, val_acc:0.837]
Epoch [15/120    avg_loss:0.582, val_acc:0.887]
Epoch [16/120    avg_loss:0.472, val_acc:0.890]
Epoch [17/120    avg_loss:0.506, val_acc:0.868]
Epoch [18/120    avg_loss:0.407, val_acc:0.906]
Epoch [19/120    avg_loss:0.347, val_acc:0.878]
Epoch [20/120    avg_loss:0.363, val_acc:0.907]
Epoch [21/120    avg_loss:0.352, val_acc:0.868]
Epoch [22/120    avg_loss:0.241, val_acc:0.926]
Epoch [23/120    avg_loss:0.211, val_acc:0.944]
Epoch [24/120    avg_loss:0.172, val_acc:0.965]
Epoch [25/120    avg_loss:0.127, val_acc:0.957]
Epoch [26/120    avg_loss:0.134, val_acc:0.929]
Epoch [27/120    avg_loss:0.223, val_acc:0.933]
Epoch [28/120    avg_loss:0.186, val_acc:0.944]
Epoch [29/120    avg_loss:0.148, val_acc:0.950]
Epoch [30/120    avg_loss:0.090, val_acc:0.926]
Epoch [31/120    avg_loss:0.123, val_acc:0.951]
Epoch [32/120    avg_loss:0.087, val_acc:0.957]
Epoch [33/120    avg_loss:0.082, val_acc:0.959]
Epoch [34/120    avg_loss:0.065, val_acc:0.957]
Epoch [35/120    avg_loss:0.077, val_acc:0.955]
Epoch [36/120    avg_loss:0.073, val_acc:0.944]
Epoch [37/120    avg_loss:0.233, val_acc:0.955]
Epoch [38/120    avg_loss:0.118, val_acc:0.957]
Epoch [39/120    avg_loss:0.081, val_acc:0.960]
Epoch [40/120    avg_loss:0.083, val_acc:0.960]
Epoch [41/120    avg_loss:0.073, val_acc:0.962]
Epoch [42/120    avg_loss:0.067, val_acc:0.963]
Epoch [43/120    avg_loss:0.068, val_acc:0.968]
Epoch [44/120    avg_loss:0.057, val_acc:0.968]
Epoch [45/120    avg_loss:0.066, val_acc:0.965]
Epoch [46/120    avg_loss:0.049, val_acc:0.968]
Epoch [47/120    avg_loss:0.052, val_acc:0.968]
Epoch [48/120    avg_loss:0.039, val_acc:0.969]
Epoch [49/120    avg_loss:0.046, val_acc:0.976]
Epoch [50/120    avg_loss:0.053, val_acc:0.970]
Epoch [51/120    avg_loss:0.048, val_acc:0.973]
Epoch [52/120    avg_loss:0.044, val_acc:0.975]
Epoch [53/120    avg_loss:0.041, val_acc:0.971]
Epoch [54/120    avg_loss:0.041, val_acc:0.969]
Epoch [55/120    avg_loss:0.045, val_acc:0.973]
Epoch [56/120    avg_loss:0.048, val_acc:0.973]
Epoch [57/120    avg_loss:0.035, val_acc:0.971]
Epoch [58/120    avg_loss:0.041, val_acc:0.973]
Epoch [59/120    avg_loss:0.035, val_acc:0.975]
Epoch [60/120    avg_loss:0.035, val_acc:0.973]
Epoch [61/120    avg_loss:0.035, val_acc:0.968]
Epoch [62/120    avg_loss:0.038, val_acc:0.972]
Epoch [63/120    avg_loss:0.036, val_acc:0.973]
Epoch [64/120    avg_loss:0.039, val_acc:0.972]
Epoch [65/120    avg_loss:0.042, val_acc:0.972]
Epoch [66/120    avg_loss:0.033, val_acc:0.972]
Epoch [67/120    avg_loss:0.039, val_acc:0.972]
Epoch [68/120    avg_loss:0.038, val_acc:0.973]
Epoch [69/120    avg_loss:0.036, val_acc:0.975]
Epoch [70/120    avg_loss:0.036, val_acc:0.975]
Epoch [71/120    avg_loss:0.047, val_acc:0.975]
Epoch [72/120    avg_loss:0.035, val_acc:0.973]
Epoch [73/120    avg_loss:0.038, val_acc:0.973]
Epoch [74/120    avg_loss:0.038, val_acc:0.975]
Epoch [75/120    avg_loss:0.038, val_acc:0.975]
Epoch [76/120    avg_loss:0.030, val_acc:0.975]
Epoch [77/120    avg_loss:0.033, val_acc:0.975]
Epoch [78/120    avg_loss:0.032, val_acc:0.975]
Epoch [79/120    avg_loss:0.035, val_acc:0.975]
Epoch [80/120    avg_loss:0.032, val_acc:0.975]
Epoch [81/120    avg_loss:0.032, val_acc:0.975]
Epoch [82/120    avg_loss:0.029, val_acc:0.975]
Epoch [83/120    avg_loss:0.030, val_acc:0.975]
Epoch [84/120    avg_loss:0.035, val_acc:0.975]
Epoch [85/120    avg_loss:0.046, val_acc:0.975]
Epoch [86/120    avg_loss:0.042, val_acc:0.975]
Epoch [87/120    avg_loss:0.038, val_acc:0.975]
Epoch [88/120    avg_loss:0.031, val_acc:0.973]
Epoch [89/120    avg_loss:0.033, val_acc:0.973]
Epoch [90/120    avg_loss:0.038, val_acc:0.973]
Epoch [91/120    avg_loss:0.037, val_acc:0.975]
Epoch [92/120    avg_loss:0.028, val_acc:0.975]
Epoch [93/120    avg_loss:0.051, val_acc:0.975]
Epoch [94/120    avg_loss:0.031, val_acc:0.975]
Epoch [95/120    avg_loss:0.033, val_acc:0.975]
Epoch [96/120    avg_loss:0.035, val_acc:0.973]
Epoch [97/120    avg_loss:0.032, val_acc:0.973]
Epoch [98/120    avg_loss:0.038, val_acc:0.973]
Epoch [99/120    avg_loss:0.029, val_acc:0.973]
Epoch [100/120    avg_loss:0.038, val_acc:0.973]
Epoch [101/120    avg_loss:0.035, val_acc:0.975]
Epoch [102/120    avg_loss:0.031, val_acc:0.975]
Epoch [103/120    avg_loss:0.036, val_acc:0.975]
Epoch [104/120    avg_loss:0.038, val_acc:0.975]
Epoch [105/120    avg_loss:0.038, val_acc:0.975]
Epoch [106/120    avg_loss:0.036, val_acc:0.975]
Epoch [107/120    avg_loss:0.030, val_acc:0.975]
Epoch [108/120    avg_loss:0.033, val_acc:0.975]
Epoch [109/120    avg_loss:0.037, val_acc:0.975]
Epoch [110/120    avg_loss:0.033, val_acc:0.975]
Epoch [111/120    avg_loss:0.032, val_acc:0.975]
Epoch [112/120    avg_loss:0.039, val_acc:0.975]
Epoch [113/120    avg_loss:0.042, val_acc:0.975]
Epoch [114/120    avg_loss:0.033, val_acc:0.975]
Epoch [115/120    avg_loss:0.046, val_acc:0.975]
Epoch [116/120    avg_loss:0.034, val_acc:0.975]
Epoch [117/120    avg_loss:0.038, val_acc:0.975]
Epoch [118/120    avg_loss:0.033, val_acc:0.975]
Epoch [119/120    avg_loss:0.039, val_acc:0.975]
Epoch [120/120    avg_loss:0.030, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1251    0    8    0    0    0    0    0    1   24    1    0
     0    0    0]
 [   0    0    6  732    3    0    0    0    0    0    1    2    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    0    0    1    0    0
     6    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   19   12    0    4    0    0    0    0  812   22    1    0
     2    3    0]
 [   0    0   11    0    0    0    0    0    2    0   15 2181    0    0
     0    1    0]
 [   0    0    0    7    0    4    0    0    0    0    5    6  507    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    0    0    2    0    0    0    0    0
  1128    0    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    47  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.36585365853658

F1 scores:
[       nan 0.98765432 0.97240575 0.97730307 0.97482838 0.97272727
 0.99316629 1.         0.99537037 1.         0.95026331 0.98088599
 0.96940727 1.         0.9707401  0.90740741 0.98823529]

Kappa:
0.969941829812757
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2f63a27ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.784, val_acc:0.363]
Epoch [2/120    avg_loss:2.598, val_acc:0.426]
Epoch [3/120    avg_loss:2.385, val_acc:0.450]
Epoch [4/120    avg_loss:2.155, val_acc:0.551]
Epoch [5/120    avg_loss:1.974, val_acc:0.575]
Epoch [6/120    avg_loss:1.815, val_acc:0.588]
Epoch [7/120    avg_loss:1.646, val_acc:0.618]
Epoch [8/120    avg_loss:1.489, val_acc:0.609]
Epoch [9/120    avg_loss:1.688, val_acc:0.655]
Epoch [10/120    avg_loss:1.872, val_acc:0.432]
Epoch [11/120    avg_loss:2.114, val_acc:0.490]
Epoch [12/120    avg_loss:2.029, val_acc:0.512]
Epoch [13/120    avg_loss:1.913, val_acc:0.524]
Epoch [14/120    avg_loss:1.817, val_acc:0.543]
Epoch [15/120    avg_loss:1.743, val_acc:0.535]
Epoch [16/120    avg_loss:1.660, val_acc:0.575]
Epoch [17/120    avg_loss:1.577, val_acc:0.592]
Epoch [18/120    avg_loss:1.495, val_acc:0.595]
Epoch [19/120    avg_loss:1.389, val_acc:0.611]
Epoch [20/120    avg_loss:1.358, val_acc:0.612]
Epoch [21/120    avg_loss:1.322, val_acc:0.649]
Epoch [22/120    avg_loss:1.200, val_acc:0.659]
Epoch [23/120    avg_loss:1.188, val_acc:0.659]
Epoch [24/120    avg_loss:1.110, val_acc:0.672]
Epoch [25/120    avg_loss:1.053, val_acc:0.698]
Epoch [26/120    avg_loss:1.007, val_acc:0.716]
Epoch [27/120    avg_loss:0.907, val_acc:0.730]
Epoch [28/120    avg_loss:0.914, val_acc:0.752]
Epoch [29/120    avg_loss:0.837, val_acc:0.750]
Epoch [30/120    avg_loss:0.830, val_acc:0.747]
Epoch [31/120    avg_loss:0.775, val_acc:0.766]
Epoch [32/120    avg_loss:0.709, val_acc:0.779]
Epoch [33/120    avg_loss:0.680, val_acc:0.764]
Epoch [34/120    avg_loss:0.622, val_acc:0.780]
Epoch [35/120    avg_loss:0.606, val_acc:0.799]
Epoch [36/120    avg_loss:0.577, val_acc:0.801]
Epoch [37/120    avg_loss:0.527, val_acc:0.825]
Epoch [38/120    avg_loss:0.543, val_acc:0.820]
Epoch [39/120    avg_loss:0.477, val_acc:0.785]
Epoch [40/120    avg_loss:0.469, val_acc:0.829]
Epoch [41/120    avg_loss:0.422, val_acc:0.850]
Epoch [42/120    avg_loss:0.414, val_acc:0.842]
Epoch [43/120    avg_loss:0.402, val_acc:0.849]
Epoch [44/120    avg_loss:0.348, val_acc:0.865]
Epoch [45/120    avg_loss:0.353, val_acc:0.827]
Epoch [46/120    avg_loss:0.343, val_acc:0.868]
Epoch [47/120    avg_loss:0.293, val_acc:0.888]
Epoch [48/120    avg_loss:0.247, val_acc:0.872]
Epoch [49/120    avg_loss:0.290, val_acc:0.864]
Epoch [50/120    avg_loss:0.257, val_acc:0.861]
Epoch [51/120    avg_loss:0.237, val_acc:0.913]
Epoch [52/120    avg_loss:0.207, val_acc:0.902]
Epoch [53/120    avg_loss:0.275, val_acc:0.909]
Epoch [54/120    avg_loss:0.213, val_acc:0.922]
Epoch [55/120    avg_loss:0.193, val_acc:0.916]
Epoch [56/120    avg_loss:0.181, val_acc:0.935]
Epoch [57/120    avg_loss:0.183, val_acc:0.930]
Epoch [58/120    avg_loss:0.153, val_acc:0.929]
Epoch [59/120    avg_loss:0.162, val_acc:0.937]
Epoch [60/120    avg_loss:0.123, val_acc:0.928]
Epoch [61/120    avg_loss:0.152, val_acc:0.926]
Epoch [62/120    avg_loss:0.128, val_acc:0.945]
Epoch [63/120    avg_loss:0.132, val_acc:0.947]
Epoch [64/120    avg_loss:0.167, val_acc:0.930]
Epoch [65/120    avg_loss:0.137, val_acc:0.915]
Epoch [66/120    avg_loss:0.150, val_acc:0.929]
Epoch [67/120    avg_loss:0.132, val_acc:0.952]
Epoch [68/120    avg_loss:0.109, val_acc:0.942]
Epoch [69/120    avg_loss:0.088, val_acc:0.927]
Epoch [70/120    avg_loss:0.101, val_acc:0.930]
Epoch [71/120    avg_loss:0.118, val_acc:0.949]
Epoch [72/120    avg_loss:0.089, val_acc:0.963]
Epoch [73/120    avg_loss:0.098, val_acc:0.951]
Epoch [74/120    avg_loss:0.076, val_acc:0.910]
Epoch [75/120    avg_loss:0.087, val_acc:0.960]
Epoch [76/120    avg_loss:0.074, val_acc:0.960]
Epoch [77/120    avg_loss:0.070, val_acc:0.955]
Epoch [78/120    avg_loss:0.057, val_acc:0.967]
Epoch [79/120    avg_loss:0.151, val_acc:0.922]
Epoch [80/120    avg_loss:0.095, val_acc:0.952]
Epoch [81/120    avg_loss:0.060, val_acc:0.971]
Epoch [82/120    avg_loss:0.062, val_acc:0.962]
Epoch [83/120    avg_loss:0.065, val_acc:0.962]
Epoch [84/120    avg_loss:0.071, val_acc:0.955]
Epoch [85/120    avg_loss:0.045, val_acc:0.970]
Epoch [86/120    avg_loss:0.064, val_acc:0.979]
Epoch [87/120    avg_loss:0.047, val_acc:0.977]
Epoch [88/120    avg_loss:0.041, val_acc:0.983]
Epoch [89/120    avg_loss:0.037, val_acc:0.966]
Epoch [90/120    avg_loss:0.042, val_acc:0.976]
Epoch [91/120    avg_loss:0.037, val_acc:0.974]
Epoch [92/120    avg_loss:0.034, val_acc:0.982]
Epoch [93/120    avg_loss:0.035, val_acc:0.963]
Epoch [94/120    avg_loss:0.025, val_acc:0.972]
Epoch [95/120    avg_loss:0.029, val_acc:0.977]
Epoch [96/120    avg_loss:0.035, val_acc:0.978]
Epoch [97/120    avg_loss:0.031, val_acc:0.982]
Epoch [98/120    avg_loss:0.030, val_acc:0.978]
Epoch [99/120    avg_loss:0.023, val_acc:0.983]
Epoch [100/120    avg_loss:0.042, val_acc:0.966]
Epoch [101/120    avg_loss:0.035, val_acc:0.983]
Epoch [102/120    avg_loss:0.043, val_acc:0.978]
Epoch [103/120    avg_loss:0.025, val_acc:0.974]
Epoch [104/120    avg_loss:0.023, val_acc:0.977]
Epoch [105/120    avg_loss:0.048, val_acc:0.962]
Epoch [106/120    avg_loss:0.056, val_acc:0.958]
Epoch [107/120    avg_loss:0.024, val_acc:0.984]
Epoch [108/120    avg_loss:0.016, val_acc:0.979]
Epoch [109/120    avg_loss:0.020, val_acc:0.974]
Epoch [110/120    avg_loss:0.020, val_acc:0.983]
Epoch [111/120    avg_loss:0.034, val_acc:0.980]
Epoch [112/120    avg_loss:0.046, val_acc:0.970]
Epoch [113/120    avg_loss:0.047, val_acc:0.970]
Epoch [114/120    avg_loss:0.029, val_acc:0.978]
Epoch [115/120    avg_loss:0.027, val_acc:0.957]
Epoch [116/120    avg_loss:0.041, val_acc:0.982]
Epoch [117/120    avg_loss:0.035, val_acc:0.980]
Epoch [118/120    avg_loss:0.046, val_acc:0.978]
Epoch [119/120    avg_loss:0.030, val_acc:0.977]
Epoch [120/120    avg_loss:0.013, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    4 1257    0    0    0    0    0    0    0   11    7    4    0
     0    2    0]
 [   0    0    1  726    5    0    0    0    0    3    0    2    7    0
     0    3    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    0    0    1    0    0
     6    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  425    0    0    0    5    0
     0    0    0]
 [   0    0    0    3    0    0    3    0    0   11    0    0    1    0
     0    0    0]
 [   0    0    0   62    0    2    0    0    0    0  795    5    1    0
     0   10    0]
 [   0    0   24    0    0    0    1    0    4    0   21 2155    2    0
     0    3    0]
 [   0    0    4   20    1    0    0    0    0    0    0    2  500    0
     0    2    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    0    0    0    0    0    0    0    0
  1129    1    0]
 [   0    0    0    0    0    0   23    0    0    2    0    0    0    0
    58  264    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.34688346883469

F1 scores:
[       nan 0.91566265 0.97782964 0.93136626 0.9837587  0.97940503
 0.97834205 1.         0.9895227  0.64705882 0.93255132 0.98334474
 0.9478673  1.         0.96785255 0.83544304 0.96511628]

Kappa:
0.9583611817255543
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f208e9c6e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.765, val_acc:0.365]
Epoch [2/120    avg_loss:2.592, val_acc:0.470]
Epoch [3/120    avg_loss:2.376, val_acc:0.465]
Epoch [4/120    avg_loss:2.195, val_acc:0.519]
Epoch [5/120    avg_loss:2.043, val_acc:0.550]
Epoch [6/120    avg_loss:1.888, val_acc:0.643]
Epoch [7/120    avg_loss:1.672, val_acc:0.665]
Epoch [8/120    avg_loss:1.481, val_acc:0.698]
Epoch [9/120    avg_loss:1.299, val_acc:0.726]
Epoch [10/120    avg_loss:1.208, val_acc:0.759]
Epoch [11/120    avg_loss:0.993, val_acc:0.789]
Epoch [12/120    avg_loss:0.850, val_acc:0.806]
Epoch [13/120    avg_loss:0.793, val_acc:0.768]
Epoch [14/120    avg_loss:0.723, val_acc:0.817]
Epoch [15/120    avg_loss:0.666, val_acc:0.827]
Epoch [16/120    avg_loss:0.677, val_acc:0.842]
Epoch [17/120    avg_loss:0.512, val_acc:0.890]
Epoch [18/120    avg_loss:0.450, val_acc:0.865]
Epoch [19/120    avg_loss:0.426, val_acc:0.894]
Epoch [20/120    avg_loss:0.395, val_acc:0.907]
Epoch [21/120    avg_loss:0.350, val_acc:0.848]
Epoch [22/120    avg_loss:0.384, val_acc:0.919]
Epoch [23/120    avg_loss:0.330, val_acc:0.923]
Epoch [24/120    avg_loss:0.232, val_acc:0.923]
Epoch [25/120    avg_loss:0.239, val_acc:0.945]
Epoch [26/120    avg_loss:0.281, val_acc:0.933]
Epoch [27/120    avg_loss:0.268, val_acc:0.931]
Epoch [28/120    avg_loss:0.200, val_acc:0.955]
Epoch [29/120    avg_loss:0.166, val_acc:0.919]
Epoch [30/120    avg_loss:0.166, val_acc:0.938]
Epoch [31/120    avg_loss:0.119, val_acc:0.959]
Epoch [32/120    avg_loss:0.125, val_acc:0.942]
Epoch [33/120    avg_loss:0.119, val_acc:0.945]
Epoch [34/120    avg_loss:0.198, val_acc:0.951]
Epoch [35/120    avg_loss:0.136, val_acc:0.933]
Epoch [36/120    avg_loss:0.099, val_acc:0.955]
Epoch [37/120    avg_loss:0.075, val_acc:0.958]
Epoch [38/120    avg_loss:0.073, val_acc:0.951]
Epoch [39/120    avg_loss:0.103, val_acc:0.938]
Epoch [40/120    avg_loss:0.082, val_acc:0.968]
Epoch [41/120    avg_loss:0.063, val_acc:0.961]
Epoch [42/120    avg_loss:0.121, val_acc:0.945]
Epoch [43/120    avg_loss:0.129, val_acc:0.953]
Epoch [44/120    avg_loss:0.084, val_acc:0.966]
Epoch [45/120    avg_loss:0.050, val_acc:0.965]
Epoch [46/120    avg_loss:0.049, val_acc:0.966]
Epoch [47/120    avg_loss:0.081, val_acc:0.938]
Epoch [48/120    avg_loss:0.132, val_acc:0.936]
Epoch [49/120    avg_loss:0.163, val_acc:0.923]
Epoch [50/120    avg_loss:0.094, val_acc:0.939]
Epoch [51/120    avg_loss:0.111, val_acc:0.953]
Epoch [52/120    avg_loss:0.103, val_acc:0.959]
Epoch [53/120    avg_loss:0.044, val_acc:0.964]
Epoch [54/120    avg_loss:0.031, val_acc:0.969]
Epoch [55/120    avg_loss:0.031, val_acc:0.972]
Epoch [56/120    avg_loss:0.027, val_acc:0.970]
Epoch [57/120    avg_loss:0.026, val_acc:0.969]
Epoch [58/120    avg_loss:0.025, val_acc:0.970]
Epoch [59/120    avg_loss:0.027, val_acc:0.972]
Epoch [60/120    avg_loss:0.027, val_acc:0.973]
Epoch [61/120    avg_loss:0.025, val_acc:0.972]
Epoch [62/120    avg_loss:0.023, val_acc:0.972]
Epoch [63/120    avg_loss:0.021, val_acc:0.976]
Epoch [64/120    avg_loss:0.021, val_acc:0.975]
Epoch [65/120    avg_loss:0.035, val_acc:0.977]
Epoch [66/120    avg_loss:0.026, val_acc:0.973]
Epoch [67/120    avg_loss:0.020, val_acc:0.977]
Epoch [68/120    avg_loss:0.029, val_acc:0.974]
Epoch [69/120    avg_loss:0.024, val_acc:0.976]
Epoch [70/120    avg_loss:0.021, val_acc:0.976]
Epoch [71/120    avg_loss:0.023, val_acc:0.982]
Epoch [72/120    avg_loss:0.020, val_acc:0.980]
Epoch [73/120    avg_loss:0.022, val_acc:0.981]
Epoch [74/120    avg_loss:0.021, val_acc:0.981]
Epoch [75/120    avg_loss:0.019, val_acc:0.984]
Epoch [76/120    avg_loss:0.026, val_acc:0.978]
Epoch [77/120    avg_loss:0.019, val_acc:0.978]
Epoch [78/120    avg_loss:0.021, val_acc:0.981]
Epoch [79/120    avg_loss:0.020, val_acc:0.982]
Epoch [80/120    avg_loss:0.018, val_acc:0.980]
Epoch [81/120    avg_loss:0.020, val_acc:0.980]
Epoch [82/120    avg_loss:0.016, val_acc:0.982]
Epoch [83/120    avg_loss:0.017, val_acc:0.982]
Epoch [84/120    avg_loss:0.016, val_acc:0.981]
Epoch [85/120    avg_loss:0.017, val_acc:0.978]
Epoch [86/120    avg_loss:0.015, val_acc:0.980]
Epoch [87/120    avg_loss:0.022, val_acc:0.981]
Epoch [88/120    avg_loss:0.017, val_acc:0.985]
Epoch [89/120    avg_loss:0.017, val_acc:0.984]
Epoch [90/120    avg_loss:0.017, val_acc:0.981]
Epoch [91/120    avg_loss:0.016, val_acc:0.982]
Epoch [92/120    avg_loss:0.018, val_acc:0.984]
Epoch [93/120    avg_loss:0.017, val_acc:0.982]
Epoch [94/120    avg_loss:0.020, val_acc:0.983]
Epoch [95/120    avg_loss:0.017, val_acc:0.982]
Epoch [96/120    avg_loss:0.019, val_acc:0.978]
Epoch [97/120    avg_loss:0.022, val_acc:0.982]
Epoch [98/120    avg_loss:0.015, val_acc:0.981]
Epoch [99/120    avg_loss:0.014, val_acc:0.981]
Epoch [100/120    avg_loss:0.018, val_acc:0.978]
Epoch [101/120    avg_loss:0.014, val_acc:0.982]
Epoch [102/120    avg_loss:0.014, val_acc:0.982]
Epoch [103/120    avg_loss:0.017, val_acc:0.981]
Epoch [104/120    avg_loss:0.013, val_acc:0.981]
Epoch [105/120    avg_loss:0.016, val_acc:0.981]
Epoch [106/120    avg_loss:0.017, val_acc:0.981]
Epoch [107/120    avg_loss:0.016, val_acc:0.981]
Epoch [108/120    avg_loss:0.016, val_acc:0.981]
Epoch [109/120    avg_loss:0.018, val_acc:0.981]
Epoch [110/120    avg_loss:0.016, val_acc:0.982]
Epoch [111/120    avg_loss:0.017, val_acc:0.980]
Epoch [112/120    avg_loss:0.014, val_acc:0.980]
Epoch [113/120    avg_loss:0.015, val_acc:0.981]
Epoch [114/120    avg_loss:0.014, val_acc:0.981]
Epoch [115/120    avg_loss:0.016, val_acc:0.981]
Epoch [116/120    avg_loss:0.015, val_acc:0.980]
Epoch [117/120    avg_loss:0.016, val_acc:0.980]
Epoch [118/120    avg_loss:0.016, val_acc:0.980]
Epoch [119/120    avg_loss:0.016, val_acc:0.981]
Epoch [120/120    avg_loss:0.015, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1240    1    1    0    0    0    0    0   17   26    0    0
     0    0    0]
 [   0    0    0  743    1    0    0    0    1    0    0    0    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    0    0    0    0    0    0    0
    11    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   40   67    0    0    2    0    0    0  762    1    1    0
     2    0    0]
 [   0    0    3    0    0    0    2    0    4    0   10 2184    5    0
     2    0    0]
 [   0    0   13    8    0    0    0    0    1    0    7    4  497    0
     1    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1137    1    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
   138  205    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.83739837398375

F1 scores:
[       nan 0.98765432 0.96049574 0.94770408 0.9953271  0.98719441
 0.99318698 1.         0.99192618 0.94117647 0.91202873 0.98711864
 0.95668912 1.         0.93541752 0.7400722  0.98823529]

Kappa:
0.95246221222941
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:05:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f241b848f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.773, val_acc:0.242]
Epoch [2/120    avg_loss:2.582, val_acc:0.288]
Epoch [3/120    avg_loss:2.396, val_acc:0.473]
Epoch [4/120    avg_loss:2.191, val_acc:0.428]
Epoch [5/120    avg_loss:2.020, val_acc:0.573]
Epoch [6/120    avg_loss:1.858, val_acc:0.614]
Epoch [7/120    avg_loss:1.637, val_acc:0.648]
Epoch [8/120    avg_loss:1.444, val_acc:0.704]
Epoch [9/120    avg_loss:1.256, val_acc:0.704]
Epoch [10/120    avg_loss:1.099, val_acc:0.706]
Epoch [11/120    avg_loss:0.978, val_acc:0.735]
Epoch [12/120    avg_loss:0.875, val_acc:0.775]
Epoch [13/120    avg_loss:0.799, val_acc:0.785]
Epoch [14/120    avg_loss:0.786, val_acc:0.730]
Epoch [15/120    avg_loss:0.763, val_acc:0.793]
Epoch [16/120    avg_loss:0.612, val_acc:0.825]
Epoch [17/120    avg_loss:0.540, val_acc:0.845]
Epoch [18/120    avg_loss:0.504, val_acc:0.840]
Epoch [19/120    avg_loss:0.477, val_acc:0.849]
Epoch [20/120    avg_loss:0.431, val_acc:0.855]
Epoch [21/120    avg_loss:0.420, val_acc:0.882]
Epoch [22/120    avg_loss:0.334, val_acc:0.892]
Epoch [23/120    avg_loss:0.310, val_acc:0.902]
Epoch [24/120    avg_loss:0.344, val_acc:0.905]
Epoch [25/120    avg_loss:0.326, val_acc:0.919]
Epoch [26/120    avg_loss:0.249, val_acc:0.926]
Epoch [27/120    avg_loss:0.216, val_acc:0.913]
Epoch [28/120    avg_loss:0.218, val_acc:0.917]
Epoch [29/120    avg_loss:0.228, val_acc:0.910]
Epoch [30/120    avg_loss:0.193, val_acc:0.892]
Epoch [31/120    avg_loss:0.216, val_acc:0.938]
Epoch [32/120    avg_loss:0.158, val_acc:0.954]
Epoch [33/120    avg_loss:0.136, val_acc:0.959]
Epoch [34/120    avg_loss:0.130, val_acc:0.935]
Epoch [35/120    avg_loss:0.153, val_acc:0.920]
Epoch [36/120    avg_loss:0.138, val_acc:0.936]
Epoch [37/120    avg_loss:0.130, val_acc:0.944]
Epoch [38/120    avg_loss:0.094, val_acc:0.963]
Epoch [39/120    avg_loss:0.100, val_acc:0.946]
Epoch [40/120    avg_loss:0.092, val_acc:0.961]
Epoch [41/120    avg_loss:0.120, val_acc:0.945]
Epoch [42/120    avg_loss:0.089, val_acc:0.938]
Epoch [43/120    avg_loss:0.086, val_acc:0.950]
Epoch [44/120    avg_loss:0.072, val_acc:0.952]
Epoch [45/120    avg_loss:0.082, val_acc:0.895]
Epoch [46/120    avg_loss:0.139, val_acc:0.947]
Epoch [47/120    avg_loss:0.142, val_acc:0.948]
Epoch [48/120    avg_loss:0.079, val_acc:0.957]
Epoch [49/120    avg_loss:0.092, val_acc:0.946]
Epoch [50/120    avg_loss:0.079, val_acc:0.964]
Epoch [51/120    avg_loss:0.060, val_acc:0.952]
Epoch [52/120    avg_loss:0.074, val_acc:0.967]
Epoch [53/120    avg_loss:0.105, val_acc:0.920]
Epoch [54/120    avg_loss:0.120, val_acc:0.954]
Epoch [55/120    avg_loss:0.077, val_acc:0.952]
Epoch [56/120    avg_loss:0.077, val_acc:0.949]
Epoch [57/120    avg_loss:0.053, val_acc:0.967]
Epoch [58/120    avg_loss:0.043, val_acc:0.975]
Epoch [59/120    avg_loss:0.029, val_acc:0.964]
Epoch [60/120    avg_loss:0.035, val_acc:0.967]
Epoch [61/120    avg_loss:0.032, val_acc:0.976]
Epoch [62/120    avg_loss:0.036, val_acc:0.973]
Epoch [63/120    avg_loss:0.030, val_acc:0.975]
Epoch [64/120    avg_loss:0.036, val_acc:0.973]
Epoch [65/120    avg_loss:0.042, val_acc:0.972]
Epoch [66/120    avg_loss:0.041, val_acc:0.967]
Epoch [67/120    avg_loss:0.033, val_acc:0.961]
Epoch [68/120    avg_loss:0.042, val_acc:0.970]
Epoch [69/120    avg_loss:0.040, val_acc:0.968]
Epoch [70/120    avg_loss:0.024, val_acc:0.980]
Epoch [71/120    avg_loss:0.056, val_acc:0.973]
Epoch [72/120    avg_loss:0.018, val_acc:0.980]
Epoch [73/120    avg_loss:0.062, val_acc:0.962]
Epoch [74/120    avg_loss:0.029, val_acc:0.979]
Epoch [75/120    avg_loss:0.032, val_acc:0.979]
Epoch [76/120    avg_loss:0.078, val_acc:0.950]
Epoch [77/120    avg_loss:0.087, val_acc:0.953]
Epoch [78/120    avg_loss:0.050, val_acc:0.973]
Epoch [79/120    avg_loss:0.033, val_acc:0.971]
Epoch [80/120    avg_loss:0.021, val_acc:0.975]
Epoch [81/120    avg_loss:0.033, val_acc:0.975]
Epoch [82/120    avg_loss:0.033, val_acc:0.974]
Epoch [83/120    avg_loss:0.019, val_acc:0.976]
Epoch [84/120    avg_loss:0.019, val_acc:0.983]
Epoch [85/120    avg_loss:0.014, val_acc:0.979]
Epoch [86/120    avg_loss:0.017, val_acc:0.963]
Epoch [87/120    avg_loss:0.023, val_acc:0.981]
Epoch [88/120    avg_loss:0.012, val_acc:0.981]
Epoch [89/120    avg_loss:0.017, val_acc:0.980]
Epoch [90/120    avg_loss:0.024, val_acc:0.976]
Epoch [91/120    avg_loss:0.070, val_acc:0.947]
Epoch [92/120    avg_loss:0.050, val_acc:0.968]
Epoch [93/120    avg_loss:0.046, val_acc:0.970]
Epoch [94/120    avg_loss:0.033, val_acc:0.977]
Epoch [95/120    avg_loss:0.015, val_acc:0.979]
Epoch [96/120    avg_loss:0.016, val_acc:0.980]
Epoch [97/120    avg_loss:0.018, val_acc:0.979]
Epoch [98/120    avg_loss:0.014, val_acc:0.977]
Epoch [99/120    avg_loss:0.012, val_acc:0.980]
Epoch [100/120    avg_loss:0.010, val_acc:0.980]
Epoch [101/120    avg_loss:0.013, val_acc:0.980]
Epoch [102/120    avg_loss:0.010, val_acc:0.981]
Epoch [103/120    avg_loss:0.009, val_acc:0.981]
Epoch [104/120    avg_loss:0.015, val_acc:0.981]
Epoch [105/120    avg_loss:0.011, val_acc:0.982]
Epoch [106/120    avg_loss:0.010, val_acc:0.980]
Epoch [107/120    avg_loss:0.009, val_acc:0.983]
Epoch [108/120    avg_loss:0.010, val_acc:0.984]
Epoch [109/120    avg_loss:0.008, val_acc:0.985]
Epoch [110/120    avg_loss:0.010, val_acc:0.983]
Epoch [111/120    avg_loss:0.009, val_acc:0.983]
Epoch [112/120    avg_loss:0.009, val_acc:0.985]
Epoch [113/120    avg_loss:0.008, val_acc:0.985]
Epoch [114/120    avg_loss:0.008, val_acc:0.985]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.009, val_acc:0.984]
Epoch [117/120    avg_loss:0.010, val_acc:0.984]
Epoch [118/120    avg_loss:0.010, val_acc:0.985]
Epoch [119/120    avg_loss:0.009, val_acc:0.985]
Epoch [120/120    avg_loss:0.010, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1259    1    7    0    0    0    0    0    6   12    0    0
     0    0    0]
 [   0    0    2  703   11    0    0    0    0    9    0    0   22    0
     0    0    0]
 [   0    0    0    2  210    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    4   87    0    1    0    0    0    1  765   16    1    0
     0    0    0]
 [   0    0   11    0    0    0    0    0    5    0    2 2192    0    0
     0    0    0]
 [   0    0    0   25    0    3    0    0    0    0    0    0  502    0
     0    3    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    2    0    1    0    1    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
    74  248    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.28184281842819

F1 scores:
[       nan 0.975      0.98320968 0.89782886 0.95238095 0.99082569
 0.97986577 0.97959184 0.99307159 0.75555556 0.92671108 0.98939291
 0.94716981 1.         0.96595745 0.82943144 0.99408284]

Kappa:
0.9575818714087263
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcf5b641e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.722, val_acc:0.475]
Epoch [2/120    avg_loss:2.547, val_acc:0.480]
Epoch [3/120    avg_loss:2.327, val_acc:0.565]
Epoch [4/120    avg_loss:2.138, val_acc:0.596]
Epoch [5/120    avg_loss:1.947, val_acc:0.647]
Epoch [6/120    avg_loss:1.783, val_acc:0.648]
Epoch [7/120    avg_loss:1.577, val_acc:0.702]
Epoch [8/120    avg_loss:1.445, val_acc:0.719]
Epoch [9/120    avg_loss:1.303, val_acc:0.756]
Epoch [10/120    avg_loss:1.112, val_acc:0.777]
Epoch [11/120    avg_loss:0.974, val_acc:0.802]
Epoch [12/120    avg_loss:0.827, val_acc:0.820]
Epoch [13/120    avg_loss:0.743, val_acc:0.852]
Epoch [14/120    avg_loss:0.663, val_acc:0.864]
Epoch [15/120    avg_loss:0.593, val_acc:0.878]
Epoch [16/120    avg_loss:0.545, val_acc:0.868]
Epoch [17/120    avg_loss:0.472, val_acc:0.896]
Epoch [18/120    avg_loss:0.414, val_acc:0.902]
Epoch [19/120    avg_loss:0.400, val_acc:0.903]
Epoch [20/120    avg_loss:0.348, val_acc:0.942]
Epoch [21/120    avg_loss:1.738, val_acc:0.513]
Epoch [22/120    avg_loss:1.676, val_acc:0.603]
Epoch [23/120    avg_loss:1.462, val_acc:0.652]
Epoch [24/120    avg_loss:1.284, val_acc:0.653]
Epoch [25/120    avg_loss:1.017, val_acc:0.766]
Epoch [26/120    avg_loss:0.865, val_acc:0.808]
Epoch [27/120    avg_loss:0.678, val_acc:0.831]
Epoch [28/120    avg_loss:0.561, val_acc:0.830]
Epoch [29/120    avg_loss:0.510, val_acc:0.859]
Epoch [30/120    avg_loss:0.401, val_acc:0.888]
Epoch [31/120    avg_loss:0.409, val_acc:0.887]
Epoch [32/120    avg_loss:0.351, val_acc:0.892]
Epoch [33/120    avg_loss:0.280, val_acc:0.912]
Epoch [34/120    avg_loss:0.195, val_acc:0.936]
Epoch [35/120    avg_loss:0.166, val_acc:0.942]
Epoch [36/120    avg_loss:0.163, val_acc:0.950]
Epoch [37/120    avg_loss:0.152, val_acc:0.946]
Epoch [38/120    avg_loss:0.157, val_acc:0.948]
Epoch [39/120    avg_loss:0.144, val_acc:0.952]
Epoch [40/120    avg_loss:0.132, val_acc:0.954]
Epoch [41/120    avg_loss:0.132, val_acc:0.954]
Epoch [42/120    avg_loss:0.135, val_acc:0.956]
Epoch [43/120    avg_loss:0.119, val_acc:0.956]
Epoch [44/120    avg_loss:0.114, val_acc:0.951]
Epoch [45/120    avg_loss:0.111, val_acc:0.960]
Epoch [46/120    avg_loss:0.115, val_acc:0.961]
Epoch [47/120    avg_loss:0.119, val_acc:0.955]
Epoch [48/120    avg_loss:0.109, val_acc:0.962]
Epoch [49/120    avg_loss:0.098, val_acc:0.953]
Epoch [50/120    avg_loss:0.101, val_acc:0.959]
Epoch [51/120    avg_loss:0.098, val_acc:0.961]
Epoch [52/120    avg_loss:0.096, val_acc:0.962]
Epoch [53/120    avg_loss:0.097, val_acc:0.964]
Epoch [54/120    avg_loss:0.106, val_acc:0.965]
Epoch [55/120    avg_loss:0.092, val_acc:0.967]
Epoch [56/120    avg_loss:0.086, val_acc:0.967]
Epoch [57/120    avg_loss:0.088, val_acc:0.974]
Epoch [58/120    avg_loss:0.083, val_acc:0.971]
Epoch [59/120    avg_loss:0.084, val_acc:0.974]
Epoch [60/120    avg_loss:0.085, val_acc:0.973]
Epoch [61/120    avg_loss:0.080, val_acc:0.973]
Epoch [62/120    avg_loss:0.078, val_acc:0.971]
Epoch [63/120    avg_loss:0.083, val_acc:0.975]
Epoch [64/120    avg_loss:0.076, val_acc:0.973]
Epoch [65/120    avg_loss:0.075, val_acc:0.978]
Epoch [66/120    avg_loss:0.070, val_acc:0.972]
Epoch [67/120    avg_loss:0.064, val_acc:0.977]
Epoch [68/120    avg_loss:0.061, val_acc:0.977]
Epoch [69/120    avg_loss:0.074, val_acc:0.974]
Epoch [70/120    avg_loss:0.070, val_acc:0.972]
Epoch [71/120    avg_loss:0.069, val_acc:0.973]
Epoch [72/120    avg_loss:0.065, val_acc:0.977]
Epoch [73/120    avg_loss:0.076, val_acc:0.973]
Epoch [74/120    avg_loss:0.067, val_acc:0.978]
Epoch [75/120    avg_loss:0.064, val_acc:0.974]
Epoch [76/120    avg_loss:0.060, val_acc:0.972]
Epoch [77/120    avg_loss:0.060, val_acc:0.972]
Epoch [78/120    avg_loss:0.057, val_acc:0.979]
Epoch [79/120    avg_loss:0.070, val_acc:0.970]
Epoch [80/120    avg_loss:0.063, val_acc:0.973]
Epoch [81/120    avg_loss:0.057, val_acc:0.975]
Epoch [82/120    avg_loss:0.063, val_acc:0.979]
Epoch [83/120    avg_loss:0.063, val_acc:0.981]
Epoch [84/120    avg_loss:0.056, val_acc:0.979]
Epoch [85/120    avg_loss:0.062, val_acc:0.979]
Epoch [86/120    avg_loss:0.053, val_acc:0.974]
Epoch [87/120    avg_loss:0.049, val_acc:0.980]
Epoch [88/120    avg_loss:0.053, val_acc:0.979]
Epoch [89/120    avg_loss:0.051, val_acc:0.980]
Epoch [90/120    avg_loss:0.045, val_acc:0.980]
Epoch [91/120    avg_loss:0.050, val_acc:0.980]
Epoch [92/120    avg_loss:0.056, val_acc:0.981]
Epoch [93/120    avg_loss:0.049, val_acc:0.981]
Epoch [94/120    avg_loss:0.045, val_acc:0.978]
Epoch [95/120    avg_loss:0.047, val_acc:0.981]
Epoch [96/120    avg_loss:0.052, val_acc:0.965]
Epoch [97/120    avg_loss:0.051, val_acc:0.983]
Epoch [98/120    avg_loss:0.049, val_acc:0.982]
Epoch [99/120    avg_loss:0.044, val_acc:0.988]
Epoch [100/120    avg_loss:0.047, val_acc:0.985]
Epoch [101/120    avg_loss:0.048, val_acc:0.982]
Epoch [102/120    avg_loss:0.043, val_acc:0.979]
Epoch [103/120    avg_loss:0.047, val_acc:0.979]
Epoch [104/120    avg_loss:0.046, val_acc:0.982]
Epoch [105/120    avg_loss:0.051, val_acc:0.978]
Epoch [106/120    avg_loss:0.044, val_acc:0.981]
Epoch [107/120    avg_loss:0.042, val_acc:0.981]
Epoch [108/120    avg_loss:0.048, val_acc:0.981]
Epoch [109/120    avg_loss:0.048, val_acc:0.985]
Epoch [110/120    avg_loss:0.042, val_acc:0.985]
Epoch [111/120    avg_loss:0.042, val_acc:0.978]
Epoch [112/120    avg_loss:0.047, val_acc:0.983]
Epoch [113/120    avg_loss:0.043, val_acc:0.983]
Epoch [114/120    avg_loss:0.037, val_acc:0.984]
Epoch [115/120    avg_loss:0.037, val_acc:0.984]
Epoch [116/120    avg_loss:0.043, val_acc:0.983]
Epoch [117/120    avg_loss:0.037, val_acc:0.982]
Epoch [118/120    avg_loss:0.040, val_acc:0.983]
Epoch [119/120    avg_loss:0.039, val_acc:0.982]
Epoch [120/120    avg_loss:0.036, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1232   10    6    0    0    0    0    0    7   19   10    0
     0    1    0]
 [   0    0    0  734    0    4    0    0    0    4    0    0    1    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    1    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   10   84    0    2    5    0    0    0  753   12    2    1
     2    4    0]
 [   0    0    2    0    0    0    2    0    0    0    8 2192    3    3
     0    0    0]
 [   0    0    0    8    0    1    0    0    0    0    0    0  519    0
     0    4    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    1    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
   132  210    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
95.91327913279133

F1 scores:
[       nan 0.98765432 0.97429814 0.92735313 0.98611111 0.9793578
 0.99018868 1.         1.         0.85       0.91550152 0.9887235
 0.96648045 0.97883598 0.93995859 0.74204947 0.96385542]

Kappa:
0.9533586755901008
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f35ef18ef98>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.767, val_acc:0.237]
Epoch [2/120    avg_loss:2.663, val_acc:0.314]
Epoch [3/120    avg_loss:2.536, val_acc:0.450]
Epoch [4/120    avg_loss:2.357, val_acc:0.540]
Epoch [5/120    avg_loss:2.189, val_acc:0.575]
Epoch [6/120    avg_loss:2.000, val_acc:0.585]
Epoch [7/120    avg_loss:1.829, val_acc:0.640]
Epoch [8/120    avg_loss:1.668, val_acc:0.631]
Epoch [9/120    avg_loss:1.577, val_acc:0.683]
Epoch [10/120    avg_loss:1.417, val_acc:0.701]
Epoch [11/120    avg_loss:1.222, val_acc:0.717]
Epoch [12/120    avg_loss:1.143, val_acc:0.738]
Epoch [13/120    avg_loss:1.143, val_acc:0.754]
Epoch [14/120    avg_loss:1.631, val_acc:0.654]
Epoch [15/120    avg_loss:1.206, val_acc:0.725]
Epoch [16/120    avg_loss:0.938, val_acc:0.771]
Epoch [17/120    avg_loss:0.849, val_acc:0.717]
Epoch [18/120    avg_loss:0.951, val_acc:0.741]
Epoch [19/120    avg_loss:0.752, val_acc:0.813]
Epoch [20/120    avg_loss:1.409, val_acc:0.334]
Epoch [21/120    avg_loss:2.134, val_acc:0.491]
Epoch [22/120    avg_loss:1.957, val_acc:0.506]
Epoch [23/120    avg_loss:1.860, val_acc:0.529]
Epoch [24/120    avg_loss:1.795, val_acc:0.558]
Epoch [25/120    avg_loss:1.710, val_acc:0.555]
Epoch [26/120    avg_loss:1.645, val_acc:0.567]
Epoch [27/120    avg_loss:1.595, val_acc:0.612]
Epoch [28/120    avg_loss:1.538, val_acc:0.588]
Epoch [29/120    avg_loss:1.514, val_acc:0.619]
Epoch [30/120    avg_loss:1.445, val_acc:0.613]
Epoch [31/120    avg_loss:1.422, val_acc:0.623]
Epoch [32/120    avg_loss:1.392, val_acc:0.649]
Epoch [33/120    avg_loss:1.323, val_acc:0.647]
Epoch [34/120    avg_loss:1.299, val_acc:0.649]
Epoch [35/120    avg_loss:1.291, val_acc:0.645]
Epoch [36/120    avg_loss:1.290, val_acc:0.653]
Epoch [37/120    avg_loss:1.274, val_acc:0.657]
Epoch [38/120    avg_loss:1.266, val_acc:0.655]
Epoch [39/120    avg_loss:1.284, val_acc:0.656]
Epoch [40/120    avg_loss:1.276, val_acc:0.656]
Epoch [41/120    avg_loss:1.297, val_acc:0.656]
Epoch [42/120    avg_loss:1.297, val_acc:0.660]
Epoch [43/120    avg_loss:1.274, val_acc:0.657]
Epoch [44/120    avg_loss:1.249, val_acc:0.657]
Epoch [45/120    avg_loss:1.272, val_acc:0.662]
Epoch [46/120    avg_loss:1.264, val_acc:0.659]
Epoch [47/120    avg_loss:1.249, val_acc:0.659]
Epoch [48/120    avg_loss:1.263, val_acc:0.660]
Epoch [49/120    avg_loss:1.248, val_acc:0.659]
Epoch [50/120    avg_loss:1.246, val_acc:0.659]
Epoch [51/120    avg_loss:1.260, val_acc:0.660]
Epoch [52/120    avg_loss:1.269, val_acc:0.660]
Epoch [53/120    avg_loss:1.237, val_acc:0.660]
Epoch [54/120    avg_loss:1.280, val_acc:0.662]
Epoch [55/120    avg_loss:1.278, val_acc:0.660]
Epoch [56/120    avg_loss:1.276, val_acc:0.664]
Epoch [57/120    avg_loss:1.257, val_acc:0.664]
Epoch [58/120    avg_loss:1.257, val_acc:0.664]
Epoch [59/120    avg_loss:1.246, val_acc:0.664]
Epoch [60/120    avg_loss:1.246, val_acc:0.664]
Epoch [61/120    avg_loss:1.249, val_acc:0.664]
Epoch [62/120    avg_loss:1.298, val_acc:0.664]
Epoch [63/120    avg_loss:1.265, val_acc:0.664]
Epoch [64/120    avg_loss:1.273, val_acc:0.663]
Epoch [65/120    avg_loss:1.246, val_acc:0.664]
Epoch [66/120    avg_loss:1.276, val_acc:0.664]
Epoch [67/120    avg_loss:1.255, val_acc:0.664]
Epoch [68/120    avg_loss:1.256, val_acc:0.664]
Epoch [69/120    avg_loss:1.239, val_acc:0.664]
Epoch [70/120    avg_loss:1.243, val_acc:0.664]
Epoch [71/120    avg_loss:1.256, val_acc:0.664]
Epoch [72/120    avg_loss:1.274, val_acc:0.664]
Epoch [73/120    avg_loss:1.235, val_acc:0.664]
Epoch [74/120    avg_loss:1.250, val_acc:0.664]
Epoch [75/120    avg_loss:1.254, val_acc:0.664]
Epoch [76/120    avg_loss:1.257, val_acc:0.664]
Epoch [77/120    avg_loss:1.276, val_acc:0.664]
Epoch [78/120    avg_loss:1.236, val_acc:0.664]
Epoch [79/120    avg_loss:1.283, val_acc:0.664]
Epoch [80/120    avg_loss:1.235, val_acc:0.664]
Epoch [81/120    avg_loss:1.261, val_acc:0.664]
Epoch [82/120    avg_loss:1.254, val_acc:0.664]
Epoch [83/120    avg_loss:1.239, val_acc:0.664]
Epoch [84/120    avg_loss:1.235, val_acc:0.664]
Epoch [85/120    avg_loss:1.217, val_acc:0.664]
Epoch [86/120    avg_loss:1.262, val_acc:0.664]
Epoch [87/120    avg_loss:1.218, val_acc:0.664]
Epoch [88/120    avg_loss:1.291, val_acc:0.664]
Epoch [89/120    avg_loss:1.316, val_acc:0.664]
Epoch [90/120    avg_loss:1.241, val_acc:0.664]
Epoch [91/120    avg_loss:1.226, val_acc:0.664]
Epoch [92/120    avg_loss:1.260, val_acc:0.664]
Epoch [93/120    avg_loss:1.260, val_acc:0.664]
Epoch [94/120    avg_loss:1.238, val_acc:0.664]
Epoch [95/120    avg_loss:1.271, val_acc:0.664]
Epoch [96/120    avg_loss:1.243, val_acc:0.664]
Epoch [97/120    avg_loss:1.267, val_acc:0.664]
Epoch [98/120    avg_loss:1.269, val_acc:0.664]
Epoch [99/120    avg_loss:1.273, val_acc:0.664]
Epoch [100/120    avg_loss:1.217, val_acc:0.664]
Epoch [101/120    avg_loss:1.250, val_acc:0.664]
Epoch [102/120    avg_loss:1.263, val_acc:0.664]
Epoch [103/120    avg_loss:1.253, val_acc:0.664]
Epoch [104/120    avg_loss:1.231, val_acc:0.664]
Epoch [105/120    avg_loss:1.299, val_acc:0.664]
Epoch [106/120    avg_loss:1.212, val_acc:0.664]
Epoch [107/120    avg_loss:1.240, val_acc:0.664]
Epoch [108/120    avg_loss:1.268, val_acc:0.664]
Epoch [109/120    avg_loss:1.248, val_acc:0.664]
Epoch [110/120    avg_loss:1.238, val_acc:0.664]
Epoch [111/120    avg_loss:1.284, val_acc:0.664]
Epoch [112/120    avg_loss:1.299, val_acc:0.664]
Epoch [113/120    avg_loss:1.222, val_acc:0.664]
Epoch [114/120    avg_loss:1.232, val_acc:0.664]
Epoch [115/120    avg_loss:1.246, val_acc:0.664]
Epoch [116/120    avg_loss:1.290, val_acc:0.664]
Epoch [117/120    avg_loss:1.270, val_acc:0.664]
Epoch [118/120    avg_loss:1.250, val_acc:0.664]
Epoch [119/120    avg_loss:1.264, val_acc:0.664]
Epoch [120/120    avg_loss:1.248, val_acc:0.664]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   25    4    0    1    0    9    0    0    0    2    0    0    0
     0    0    0]
 [   0    4  752   49   33    3   42    0    0    0  155  181   29   35
     0    0    2]
 [   0    0   95   70   85   27   57    0    0    0  103  108   47  155
     0    0    0]
 [   0    0   28    1  183    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0  233  118    0    0    0    4    0    2    5
    73    0    0]
 [   0    0    0    2    1    0  472    0    0    0    0    2    0  165
    15    0    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    7    0    0    6    0  406    0    0   10    0    0
     1    0    0]
 [   0    0    0    1    0    0   15    0    0    0    0    0    0    2
     0    0    0]
 [   0    0   94   11    5   18   22    0    0    0  527   45   70   81
     0    2    0]
 [   0    9  204   39   22    5  100    0   32    0   44 1456  128  171
     0    0    0]
 [   0    0   56   21   79    0    7    0    0    0   15   33  256    2
     0    0   65]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0  166
     0    0    0]
 [   0    0    4    0    0   60    0    0   10    0    0    5   28    0
  1032    0    0]
 [   0    0    0    1    0    0   59    0    0    0    0  143   64    1
    77    2    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
61.39837398373984

F1 scores:
[       nan 0.63291139 0.5963521  0.14752371 0.58842444 0.59667093
 0.58706468 0.         0.92482916 0.         0.61101449 0.69432523
 0.44214162 0.34297521 0.88318357 0.01139601 0.71489362]

Kappa:
0.5638491813279497
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f91bf08df28>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.755, val_acc:0.353]
Epoch [2/120    avg_loss:2.572, val_acc:0.435]
Epoch [3/120    avg_loss:2.352, val_acc:0.549]
Epoch [4/120    avg_loss:2.112, val_acc:0.581]
Epoch [5/120    avg_loss:1.944, val_acc:0.596]
Epoch [6/120    avg_loss:1.755, val_acc:0.609]
Epoch [7/120    avg_loss:1.597, val_acc:0.644]
Epoch [8/120    avg_loss:1.430, val_acc:0.683]
Epoch [9/120    avg_loss:1.324, val_acc:0.702]
Epoch [10/120    avg_loss:1.089, val_acc:0.729]
Epoch [11/120    avg_loss:1.000, val_acc:0.710]
Epoch [12/120    avg_loss:0.904, val_acc:0.739]
Epoch [13/120    avg_loss:0.794, val_acc:0.791]
Epoch [14/120    avg_loss:0.655, val_acc:0.799]
Epoch [15/120    avg_loss:0.690, val_acc:0.815]
Epoch [16/120    avg_loss:0.544, val_acc:0.820]
Epoch [17/120    avg_loss:0.553, val_acc:0.854]
Epoch [18/120    avg_loss:0.429, val_acc:0.863]
Epoch [19/120    avg_loss:0.404, val_acc:0.858]
Epoch [20/120    avg_loss:0.378, val_acc:0.882]
Epoch [21/120    avg_loss:0.388, val_acc:0.875]
Epoch [22/120    avg_loss:0.337, val_acc:0.895]
Epoch [23/120    avg_loss:0.276, val_acc:0.894]
Epoch [24/120    avg_loss:0.301, val_acc:0.882]
Epoch [25/120    avg_loss:0.255, val_acc:0.894]
Epoch [26/120    avg_loss:0.255, val_acc:0.919]
Epoch [27/120    avg_loss:0.571, val_acc:0.342]
Epoch [28/120    avg_loss:1.911, val_acc:0.458]
Epoch [29/120    avg_loss:1.751, val_acc:0.547]
Epoch [30/120    avg_loss:1.661, val_acc:0.528]
Epoch [31/120    avg_loss:1.571, val_acc:0.575]
Epoch [32/120    avg_loss:1.522, val_acc:0.587]
Epoch [33/120    avg_loss:1.493, val_acc:0.604]
Epoch [34/120    avg_loss:1.419, val_acc:0.590]
Epoch [35/120    avg_loss:1.429, val_acc:0.626]
Epoch [36/120    avg_loss:1.403, val_acc:0.614]
Epoch [37/120    avg_loss:1.317, val_acc:0.628]
Epoch [38/120    avg_loss:1.260, val_acc:0.624]
Epoch [39/120    avg_loss:1.227, val_acc:0.655]
Epoch [40/120    avg_loss:1.195, val_acc:0.648]
Epoch [41/120    avg_loss:1.141, val_acc:0.653]
Epoch [42/120    avg_loss:1.155, val_acc:0.655]
Epoch [43/120    avg_loss:1.124, val_acc:0.657]
Epoch [44/120    avg_loss:1.151, val_acc:0.655]
Epoch [45/120    avg_loss:1.107, val_acc:0.658]
Epoch [46/120    avg_loss:1.122, val_acc:0.654]
Epoch [47/120    avg_loss:1.110, val_acc:0.655]
Epoch [48/120    avg_loss:1.139, val_acc:0.656]
Epoch [49/120    avg_loss:1.141, val_acc:0.663]
Epoch [50/120    avg_loss:1.076, val_acc:0.662]
Epoch [51/120    avg_loss:1.118, val_acc:0.663]
Epoch [52/120    avg_loss:1.089, val_acc:0.658]
Epoch [53/120    avg_loss:1.091, val_acc:0.661]
Epoch [54/120    avg_loss:1.106, val_acc:0.661]
Epoch [55/120    avg_loss:1.113, val_acc:0.658]
Epoch [56/120    avg_loss:1.118, val_acc:0.660]
Epoch [57/120    avg_loss:1.082, val_acc:0.661]
Epoch [58/120    avg_loss:1.074, val_acc:0.664]
Epoch [59/120    avg_loss:1.055, val_acc:0.660]
Epoch [60/120    avg_loss:1.109, val_acc:0.660]
Epoch [61/120    avg_loss:1.114, val_acc:0.658]
Epoch [62/120    avg_loss:1.091, val_acc:0.660]
Epoch [63/120    avg_loss:1.038, val_acc:0.658]
Epoch [64/120    avg_loss:1.107, val_acc:0.658]
Epoch [65/120    avg_loss:1.093, val_acc:0.660]
Epoch [66/120    avg_loss:1.073, val_acc:0.660]
Epoch [67/120    avg_loss:1.065, val_acc:0.661]
Epoch [68/120    avg_loss:1.087, val_acc:0.662]
Epoch [69/120    avg_loss:1.108, val_acc:0.663]
Epoch [70/120    avg_loss:1.081, val_acc:0.663]
Epoch [71/120    avg_loss:1.088, val_acc:0.662]
Epoch [72/120    avg_loss:1.106, val_acc:0.662]
Epoch [73/120    avg_loss:1.064, val_acc:0.662]
Epoch [74/120    avg_loss:1.089, val_acc:0.661]
Epoch [75/120    avg_loss:1.100, val_acc:0.662]
Epoch [76/120    avg_loss:1.087, val_acc:0.662]
Epoch [77/120    avg_loss:1.080, val_acc:0.661]
Epoch [78/120    avg_loss:1.091, val_acc:0.662]
Epoch [79/120    avg_loss:1.111, val_acc:0.662]
Epoch [80/120    avg_loss:1.107, val_acc:0.662]
Epoch [81/120    avg_loss:1.082, val_acc:0.662]
Epoch [82/120    avg_loss:1.093, val_acc:0.662]
Epoch [83/120    avg_loss:1.097, val_acc:0.662]
Epoch [84/120    avg_loss:1.088, val_acc:0.662]
Epoch [85/120    avg_loss:1.084, val_acc:0.662]
Epoch [86/120    avg_loss:1.110, val_acc:0.662]
Epoch [87/120    avg_loss:1.065, val_acc:0.662]
Epoch [88/120    avg_loss:1.099, val_acc:0.662]
Epoch [89/120    avg_loss:1.082, val_acc:0.662]
Epoch [90/120    avg_loss:1.122, val_acc:0.662]
Epoch [91/120    avg_loss:1.058, val_acc:0.662]
Epoch [92/120    avg_loss:1.074, val_acc:0.662]
Epoch [93/120    avg_loss:1.052, val_acc:0.662]
Epoch [94/120    avg_loss:1.083, val_acc:0.662]
Epoch [95/120    avg_loss:1.086, val_acc:0.662]
Epoch [96/120    avg_loss:1.090, val_acc:0.662]
Epoch [97/120    avg_loss:1.067, val_acc:0.662]
Epoch [98/120    avg_loss:1.118, val_acc:0.662]
Epoch [99/120    avg_loss:1.070, val_acc:0.662]
Epoch [100/120    avg_loss:1.107, val_acc:0.662]
Epoch [101/120    avg_loss:1.077, val_acc:0.662]
Epoch [102/120    avg_loss:1.058, val_acc:0.662]
Epoch [103/120    avg_loss:1.128, val_acc:0.662]
Epoch [104/120    avg_loss:1.073, val_acc:0.662]
Epoch [105/120    avg_loss:1.153, val_acc:0.662]
Epoch [106/120    avg_loss:1.087, val_acc:0.662]
Epoch [107/120    avg_loss:1.070, val_acc:0.662]
Epoch [108/120    avg_loss:1.100, val_acc:0.662]
Epoch [109/120    avg_loss:1.090, val_acc:0.662]
Epoch [110/120    avg_loss:1.075, val_acc:0.662]
Epoch [111/120    avg_loss:1.039, val_acc:0.662]
Epoch [112/120    avg_loss:1.080, val_acc:0.662]
Epoch [113/120    avg_loss:1.072, val_acc:0.662]
Epoch [114/120    avg_loss:1.073, val_acc:0.662]
Epoch [115/120    avg_loss:1.122, val_acc:0.662]
Epoch [116/120    avg_loss:1.087, val_acc:0.662]
Epoch [117/120    avg_loss:1.064, val_acc:0.662]
Epoch [118/120    avg_loss:1.088, val_acc:0.662]
Epoch [119/120    avg_loss:1.059, val_acc:0.662]
Epoch [120/120    avg_loss:1.095, val_acc:0.662]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   32    0    4    0    0    2    0    0    0    3    0    0    0
     0    0    0]
 [   0   29  887  100   12    0    6    0    0    0  103  133    3    6
     0    0    6]
 [   0    0  117  293   18   37    0    0    0    0   41  124   10  107
     0    0    0]
 [   0    0   18   18  177    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    8   64    0  312    0    1    0    0    0    0    0    8
    42    0    0]
 [   0    0    0   24    0    0  531    0    0    0    0    6    0   81
    15    0    0]
 [   0    0    0    0    0   18    1    6    0    0    0    0    0    0
     0    0    0]
 [   0    0    0   20    5    0    0    0  405    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0   10    0    0    0    0    0    0    5
     0    0    0]
 [   0    5  117   49    0   13    0    0    0    0  472  107   58   54
     0    0    0]
 [   0    0  215  147    0   19   20    0   16    0   90 1404  136   77
    86    0    0]
 [   0    0   98   59   29    0    0    0    0    0    6   38  251    0
     0    0   53]
 [   0    0    1    0    0    0    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0  110    6    0   10    0    0    2    0    0    0   21    5
   985    0    0]
 [   0    0   62   46    0    4    0    0    0    0    0  165   35    0
    17   18    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
65.47425474254743

F1 scores:
[       nan 0.59813084 0.60795065 0.37088608 0.77973568 0.73584906
 0.86552567 0.375      0.94958968 0.         0.59371069 0.67064724
 0.478551   0.51685393 0.86252189 0.09863014 0.73451327]

Kappa:
0.607722439215728
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efdecf09e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.780, val_acc:0.356]
Epoch [2/120    avg_loss:2.616, val_acc:0.468]
Epoch [3/120    avg_loss:2.414, val_acc:0.548]
Epoch [4/120    avg_loss:2.185, val_acc:0.581]
Epoch [5/120    avg_loss:1.966, val_acc:0.618]
Epoch [6/120    avg_loss:1.795, val_acc:0.639]
Epoch [7/120    avg_loss:1.629, val_acc:0.664]
Epoch [8/120    avg_loss:1.441, val_acc:0.683]
Epoch [9/120    avg_loss:1.260, val_acc:0.730]
Epoch [10/120    avg_loss:1.101, val_acc:0.765]
Epoch [11/120    avg_loss:1.002, val_acc:0.785]
Epoch [12/120    avg_loss:0.841, val_acc:0.770]
Epoch [13/120    avg_loss:0.727, val_acc:0.797]
Epoch [14/120    avg_loss:0.669, val_acc:0.816]
Epoch [15/120    avg_loss:0.598, val_acc:0.826]
Epoch [16/120    avg_loss:0.466, val_acc:0.836]
Epoch [17/120    avg_loss:0.454, val_acc:0.879]
Epoch [18/120    avg_loss:0.427, val_acc:0.896]
Epoch [19/120    avg_loss:0.347, val_acc:0.885]
Epoch [20/120    avg_loss:0.345, val_acc:0.919]
Epoch [21/120    avg_loss:0.298, val_acc:0.920]
Epoch [22/120    avg_loss:0.275, val_acc:0.894]
Epoch [23/120    avg_loss:0.242, val_acc:0.917]
Epoch [24/120    avg_loss:0.217, val_acc:0.908]
Epoch [25/120    avg_loss:0.227, val_acc:0.943]
Epoch [26/120    avg_loss:0.180, val_acc:0.950]
Epoch [27/120    avg_loss:0.176, val_acc:0.932]
Epoch [28/120    avg_loss:0.140, val_acc:0.953]
Epoch [29/120    avg_loss:0.173, val_acc:0.942]
Epoch [30/120    avg_loss:0.120, val_acc:0.946]
Epoch [31/120    avg_loss:0.095, val_acc:0.961]
Epoch [32/120    avg_loss:0.116, val_acc:0.950]
Epoch [33/120    avg_loss:0.116, val_acc:0.959]
Epoch [34/120    avg_loss:0.095, val_acc:0.953]
Epoch [35/120    avg_loss:0.080, val_acc:0.967]
Epoch [36/120    avg_loss:0.096, val_acc:0.950]
Epoch [37/120    avg_loss:0.073, val_acc:0.954]
Epoch [38/120    avg_loss:0.064, val_acc:0.959]
Epoch [39/120    avg_loss:0.085, val_acc:0.954]
Epoch [40/120    avg_loss:0.099, val_acc:0.956]
Epoch [41/120    avg_loss:0.054, val_acc:0.961]
Epoch [42/120    avg_loss:0.084, val_acc:0.974]
Epoch [43/120    avg_loss:0.054, val_acc:0.964]
Epoch [44/120    avg_loss:0.046, val_acc:0.981]
Epoch [45/120    avg_loss:0.057, val_acc:0.944]
Epoch [46/120    avg_loss:0.156, val_acc:0.962]
Epoch [47/120    avg_loss:0.072, val_acc:0.970]
Epoch [48/120    avg_loss:0.037, val_acc:0.966]
Epoch [49/120    avg_loss:0.041, val_acc:0.964]
Epoch [50/120    avg_loss:0.052, val_acc:0.971]
Epoch [51/120    avg_loss:0.034, val_acc:0.977]
Epoch [52/120    avg_loss:0.121, val_acc:0.964]
Epoch [53/120    avg_loss:0.040, val_acc:0.962]
Epoch [54/120    avg_loss:0.037, val_acc:0.976]
Epoch [55/120    avg_loss:0.023, val_acc:0.976]
Epoch [56/120    avg_loss:0.025, val_acc:0.973]
Epoch [57/120    avg_loss:0.023, val_acc:0.980]
Epoch [58/120    avg_loss:0.017, val_acc:0.984]
Epoch [59/120    avg_loss:0.025, val_acc:0.984]
Epoch [60/120    avg_loss:0.022, val_acc:0.982]
Epoch [61/120    avg_loss:0.019, val_acc:0.985]
Epoch [62/120    avg_loss:0.018, val_acc:0.985]
Epoch [63/120    avg_loss:0.017, val_acc:0.985]
Epoch [64/120    avg_loss:0.014, val_acc:0.984]
Epoch [65/120    avg_loss:0.014, val_acc:0.984]
Epoch [66/120    avg_loss:0.014, val_acc:0.985]
Epoch [67/120    avg_loss:0.015, val_acc:0.982]
Epoch [68/120    avg_loss:0.017, val_acc:0.987]
Epoch [69/120    avg_loss:0.013, val_acc:0.984]
Epoch [70/120    avg_loss:0.012, val_acc:0.986]
Epoch [71/120    avg_loss:0.014, val_acc:0.986]
Epoch [72/120    avg_loss:0.012, val_acc:0.986]
Epoch [73/120    avg_loss:0.013, val_acc:0.984]
Epoch [74/120    avg_loss:0.013, val_acc:0.985]
Epoch [75/120    avg_loss:0.012, val_acc:0.984]
Epoch [76/120    avg_loss:0.013, val_acc:0.986]
Epoch [77/120    avg_loss:0.014, val_acc:0.985]
Epoch [78/120    avg_loss:0.012, val_acc:0.985]
Epoch [79/120    avg_loss:0.013, val_acc:0.985]
Epoch [80/120    avg_loss:0.022, val_acc:0.985]
Epoch [81/120    avg_loss:0.013, val_acc:0.986]
Epoch [82/120    avg_loss:0.012, val_acc:0.986]
Epoch [83/120    avg_loss:0.016, val_acc:0.986]
Epoch [84/120    avg_loss:0.011, val_acc:0.986]
Epoch [85/120    avg_loss:0.014, val_acc:0.986]
Epoch [86/120    avg_loss:0.014, val_acc:0.987]
Epoch [87/120    avg_loss:0.014, val_acc:0.986]
Epoch [88/120    avg_loss:0.012, val_acc:0.986]
Epoch [89/120    avg_loss:0.013, val_acc:0.986]
Epoch [90/120    avg_loss:0.012, val_acc:0.987]
Epoch [91/120    avg_loss:0.012, val_acc:0.987]
Epoch [92/120    avg_loss:0.015, val_acc:0.987]
Epoch [93/120    avg_loss:0.013, val_acc:0.987]
Epoch [94/120    avg_loss:0.011, val_acc:0.986]
Epoch [95/120    avg_loss:0.013, val_acc:0.987]
Epoch [96/120    avg_loss:0.011, val_acc:0.986]
Epoch [97/120    avg_loss:0.010, val_acc:0.986]
Epoch [98/120    avg_loss:0.011, val_acc:0.987]
Epoch [99/120    avg_loss:0.012, val_acc:0.987]
Epoch [100/120    avg_loss:0.010, val_acc:0.987]
Epoch [101/120    avg_loss:0.013, val_acc:0.985]
Epoch [102/120    avg_loss:0.012, val_acc:0.987]
Epoch [103/120    avg_loss:0.011, val_acc:0.986]
Epoch [104/120    avg_loss:0.013, val_acc:0.986]
Epoch [105/120    avg_loss:0.012, val_acc:0.987]
Epoch [106/120    avg_loss:0.012, val_acc:0.987]
Epoch [107/120    avg_loss:0.011, val_acc:0.986]
Epoch [108/120    avg_loss:0.011, val_acc:0.986]
Epoch [109/120    avg_loss:0.014, val_acc:0.986]
Epoch [110/120    avg_loss:0.010, val_acc:0.987]
Epoch [111/120    avg_loss:0.012, val_acc:0.986]
Epoch [112/120    avg_loss:0.013, val_acc:0.986]
Epoch [113/120    avg_loss:0.010, val_acc:0.986]
Epoch [114/120    avg_loss:0.010, val_acc:0.986]
Epoch [115/120    avg_loss:0.011, val_acc:0.986]
Epoch [116/120    avg_loss:0.011, val_acc:0.986]
Epoch [117/120    avg_loss:0.012, val_acc:0.986]
Epoch [118/120    avg_loss:0.012, val_acc:0.986]
Epoch [119/120    avg_loss:0.009, val_acc:0.986]
Epoch [120/120    avg_loss:0.012, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1252    0    0    0    0    0    0    0    3   21    7    0
     0    2    0]
 [   0    0    3  724    1    6    0    0    0    8    0    0    5    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  418    0    0    0    0    0    0    0    0
    17    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   15    0    0    1    0
     0    0    0]
 [   0    0   15   90    0    2    0    0    0    0  745   22    0    0
     1    0    0]
 [   0    0    1    0    0    1    0    0    0    0    7 2200    1    0
     0    0    0]
 [   0    0    0    2    0   12    0    0    0    0    2    5  511    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    0    0
    69  263    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.44444444444444

F1 scores:
[       nan 0.98765432 0.97927259 0.92642354 0.99530516 0.95542857
 0.99771863 0.97959184 1.         0.52631579 0.9129902  0.98676833
 0.96415094 1.         0.96321353 0.85807504 0.98809524]

Kappa:
0.9594157901789109
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f94af863e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.757, val_acc:0.359]
Epoch [2/120    avg_loss:2.594, val_acc:0.489]
Epoch [3/120    avg_loss:2.406, val_acc:0.493]
Epoch [4/120    avg_loss:2.215, val_acc:0.538]
Epoch [5/120    avg_loss:2.040, val_acc:0.537]
Epoch [6/120    avg_loss:1.881, val_acc:0.602]
Epoch [7/120    avg_loss:1.713, val_acc:0.559]
Epoch [8/120    avg_loss:1.540, val_acc:0.556]
Epoch [9/120    avg_loss:1.516, val_acc:0.690]
Epoch [10/120    avg_loss:1.313, val_acc:0.657]
Epoch [11/120    avg_loss:1.220, val_acc:0.685]
Epoch [12/120    avg_loss:1.109, val_acc:0.698]
Epoch [13/120    avg_loss:0.997, val_acc:0.748]
Epoch [14/120    avg_loss:0.961, val_acc:0.742]
Epoch [15/120    avg_loss:0.812, val_acc:0.710]
Epoch [16/120    avg_loss:0.780, val_acc:0.740]
Epoch [17/120    avg_loss:0.700, val_acc:0.779]
Epoch [18/120    avg_loss:0.634, val_acc:0.785]
Epoch [19/120    avg_loss:0.575, val_acc:0.826]
Epoch [20/120    avg_loss:0.549, val_acc:0.818]
Epoch [21/120    avg_loss:0.560, val_acc:0.830]
Epoch [22/120    avg_loss:0.491, val_acc:0.862]
Epoch [23/120    avg_loss:0.413, val_acc:0.864]
Epoch [24/120    avg_loss:0.430, val_acc:0.873]
Epoch [25/120    avg_loss:0.352, val_acc:0.840]
Epoch [26/120    avg_loss:0.375, val_acc:0.856]
Epoch [27/120    avg_loss:0.341, val_acc:0.849]
Epoch [28/120    avg_loss:0.325, val_acc:0.853]
Epoch [29/120    avg_loss:0.381, val_acc:0.916]
Epoch [30/120    avg_loss:0.217, val_acc:0.920]
Epoch [31/120    avg_loss:0.225, val_acc:0.885]
Epoch [32/120    avg_loss:0.249, val_acc:0.900]
Epoch [33/120    avg_loss:0.337, val_acc:0.902]
Epoch [34/120    avg_loss:0.217, val_acc:0.883]
Epoch [35/120    avg_loss:2.223, val_acc:0.392]
Epoch [36/120    avg_loss:2.024, val_acc:0.454]
Epoch [37/120    avg_loss:1.890, val_acc:0.468]
Epoch [38/120    avg_loss:1.768, val_acc:0.523]
Epoch [39/120    avg_loss:1.690, val_acc:0.539]
Epoch [40/120    avg_loss:1.637, val_acc:0.580]
Epoch [41/120    avg_loss:1.587, val_acc:0.577]
Epoch [42/120    avg_loss:1.480, val_acc:0.603]
Epoch [43/120    avg_loss:1.389, val_acc:0.647]
Epoch [44/120    avg_loss:1.315, val_acc:0.653]
Epoch [45/120    avg_loss:1.297, val_acc:0.661]
Epoch [46/120    avg_loss:1.253, val_acc:0.654]
Epoch [47/120    avg_loss:1.233, val_acc:0.656]
Epoch [48/120    avg_loss:1.238, val_acc:0.661]
Epoch [49/120    avg_loss:1.261, val_acc:0.665]
Epoch [50/120    avg_loss:1.212, val_acc:0.664]
Epoch [51/120    avg_loss:1.226, val_acc:0.666]
Epoch [52/120    avg_loss:1.207, val_acc:0.665]
Epoch [53/120    avg_loss:1.187, val_acc:0.670]
Epoch [54/120    avg_loss:1.197, val_acc:0.670]
Epoch [55/120    avg_loss:1.203, val_acc:0.674]
Epoch [56/120    avg_loss:1.147, val_acc:0.680]
Epoch [57/120    avg_loss:1.136, val_acc:0.677]
Epoch [58/120    avg_loss:1.146, val_acc:0.679]
Epoch [59/120    avg_loss:1.162, val_acc:0.680]
Epoch [60/120    avg_loss:1.150, val_acc:0.680]
Epoch [61/120    avg_loss:1.158, val_acc:0.682]
Epoch [62/120    avg_loss:1.121, val_acc:0.682]
Epoch [63/120    avg_loss:1.128, val_acc:0.681]
Epoch [64/120    avg_loss:1.158, val_acc:0.682]
Epoch [65/120    avg_loss:1.143, val_acc:0.682]
Epoch [66/120    avg_loss:1.127, val_acc:0.683]
Epoch [67/120    avg_loss:1.131, val_acc:0.680]
Epoch [68/120    avg_loss:1.141, val_acc:0.681]
Epoch [69/120    avg_loss:1.120, val_acc:0.681]
Epoch [70/120    avg_loss:1.126, val_acc:0.681]
Epoch [71/120    avg_loss:1.104, val_acc:0.682]
Epoch [72/120    avg_loss:1.126, val_acc:0.682]
Epoch [73/120    avg_loss:1.142, val_acc:0.682]
Epoch [74/120    avg_loss:1.120, val_acc:0.681]
Epoch [75/120    avg_loss:1.124, val_acc:0.682]
Epoch [76/120    avg_loss:1.116, val_acc:0.682]
Epoch [77/120    avg_loss:1.151, val_acc:0.682]
Epoch [78/120    avg_loss:1.151, val_acc:0.681]
Epoch [79/120    avg_loss:1.154, val_acc:0.681]
Epoch [80/120    avg_loss:1.140, val_acc:0.682]
Epoch [81/120    avg_loss:1.125, val_acc:0.681]
Epoch [82/120    avg_loss:1.139, val_acc:0.681]
Epoch [83/120    avg_loss:1.112, val_acc:0.681]
Epoch [84/120    avg_loss:1.144, val_acc:0.681]
Epoch [85/120    avg_loss:1.131, val_acc:0.681]
Epoch [86/120    avg_loss:1.118, val_acc:0.681]
Epoch [87/120    avg_loss:1.145, val_acc:0.681]
Epoch [88/120    avg_loss:1.140, val_acc:0.681]
Epoch [89/120    avg_loss:1.156, val_acc:0.681]
Epoch [90/120    avg_loss:1.146, val_acc:0.681]
Epoch [91/120    avg_loss:1.115, val_acc:0.681]
Epoch [92/120    avg_loss:1.130, val_acc:0.681]
Epoch [93/120    avg_loss:1.137, val_acc:0.681]
Epoch [94/120    avg_loss:1.120, val_acc:0.681]
Epoch [95/120    avg_loss:1.134, val_acc:0.681]
Epoch [96/120    avg_loss:1.161, val_acc:0.681]
Epoch [97/120    avg_loss:1.125, val_acc:0.681]
Epoch [98/120    avg_loss:1.132, val_acc:0.681]
Epoch [99/120    avg_loss:1.114, val_acc:0.681]
Epoch [100/120    avg_loss:1.142, val_acc:0.681]
Epoch [101/120    avg_loss:1.131, val_acc:0.681]
Epoch [102/120    avg_loss:1.134, val_acc:0.681]
Epoch [103/120    avg_loss:1.140, val_acc:0.681]
Epoch [104/120    avg_loss:1.145, val_acc:0.681]
Epoch [105/120    avg_loss:1.133, val_acc:0.681]
Epoch [106/120    avg_loss:1.132, val_acc:0.681]
Epoch [107/120    avg_loss:1.111, val_acc:0.681]
Epoch [108/120    avg_loss:1.108, val_acc:0.681]
Epoch [109/120    avg_loss:1.169, val_acc:0.681]
Epoch [110/120    avg_loss:1.139, val_acc:0.681]
Epoch [111/120    avg_loss:1.117, val_acc:0.681]
Epoch [112/120    avg_loss:1.143, val_acc:0.681]
Epoch [113/120    avg_loss:1.155, val_acc:0.681]
Epoch [114/120    avg_loss:1.124, val_acc:0.681]
Epoch [115/120    avg_loss:1.161, val_acc:0.681]
Epoch [116/120    avg_loss:1.107, val_acc:0.681]
Epoch [117/120    avg_loss:1.092, val_acc:0.681]
Epoch [118/120    avg_loss:1.172, val_acc:0.681]
Epoch [119/120    avg_loss:1.162, val_acc:0.681]
Epoch [120/120    avg_loss:1.198, val_acc:0.681]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   20   10    0    4    0    5    0    0    0    2    0    0    0
     0    0    0]
 [   0    0  651   42   77    4   26    0    0    0  288  125   44    9
    10    0    9]
 [   0    0   45  243   71    6   29    0    0    0  127   34   66  120
     6    0    0]
 [   0    0    1    0  186    0   16    0    0    0    0    0    0   10
     0    0    0]
 [   0    0    0    3    0  231  126    0    0    0   13    0    0    0
    62    0    0]
 [   0    0    2    9   23    0  611    0    0    0    6    0    0    0
     6    0    0]
 [   0    0    0    0    0    2   23    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    3    1    0    0    0    0  423    0    0    0    2    1
     0    0    0]
 [   0    0    0    2    0    0   16    0    0    0    0    0    0    0
     0    0    0]
 [   0    0   52   50   23    5   47    0    0    0  573   48   47    5
    24    0    1]
 [   0    0  155  168  122   11   62    0   54    0   28 1473   44   82
     6    0    5]
 [   0    0   19   16   74    0   41    0    0    0   32   20  296   15
     1    0   20]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   33    0    0    1    0    0    0    1    1
  1103    0    0]
 [   0    0    0   25   11    0  134    0    0    0    0    3   41    0
   132    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
65.88617886178862

F1 scores:
[       nan 0.6557377  0.58569501 0.37212864 0.46268657 0.63548831
 0.68153932 0.         0.93171806 0.         0.58950617 0.75287503
 0.54967502 0.60358891 0.88629972 0.00574713 0.8159204 ]

Kappa:
0.6162759966524854
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f01501c9f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.781, val_acc:0.272]
Epoch [2/120    avg_loss:2.659, val_acc:0.412]
Epoch [3/120    avg_loss:2.506, val_acc:0.492]
Epoch [4/120    avg_loss:2.311, val_acc:0.510]
Epoch [5/120    avg_loss:2.162, val_acc:0.561]
Epoch [6/120    avg_loss:1.936, val_acc:0.580]
Epoch [7/120    avg_loss:1.803, val_acc:0.619]
Epoch [8/120    avg_loss:1.586, val_acc:0.665]
Epoch [9/120    avg_loss:1.450, val_acc:0.671]
Epoch [10/120    avg_loss:1.303, val_acc:0.689]
Epoch [11/120    avg_loss:1.228, val_acc:0.725]
Epoch [12/120    avg_loss:1.101, val_acc:0.726]
Epoch [13/120    avg_loss:0.914, val_acc:0.779]
Epoch [14/120    avg_loss:0.794, val_acc:0.784]
Epoch [15/120    avg_loss:0.749, val_acc:0.813]
Epoch [16/120    avg_loss:0.659, val_acc:0.800]
Epoch [17/120    avg_loss:0.584, val_acc:0.834]
Epoch [18/120    avg_loss:0.535, val_acc:0.842]
Epoch [19/120    avg_loss:0.497, val_acc:0.876]
Epoch [20/120    avg_loss:0.507, val_acc:0.787]
Epoch [21/120    avg_loss:0.398, val_acc:0.897]
Epoch [22/120    avg_loss:0.329, val_acc:0.884]
Epoch [23/120    avg_loss:0.405, val_acc:0.872]
Epoch [24/120    avg_loss:0.304, val_acc:0.898]
Epoch [25/120    avg_loss:0.283, val_acc:0.909]
Epoch [26/120    avg_loss:0.266, val_acc:0.917]
Epoch [27/120    avg_loss:0.209, val_acc:0.857]
Epoch [28/120    avg_loss:0.253, val_acc:0.927]
Epoch [29/120    avg_loss:0.168, val_acc:0.942]
Epoch [30/120    avg_loss:0.167, val_acc:0.944]
Epoch [31/120    avg_loss:0.176, val_acc:0.909]
Epoch [32/120    avg_loss:0.114, val_acc:0.904]
Epoch [33/120    avg_loss:0.119, val_acc:0.955]
Epoch [34/120    avg_loss:0.085, val_acc:0.945]
Epoch [35/120    avg_loss:0.144, val_acc:0.954]
Epoch [36/120    avg_loss:0.094, val_acc:0.939]
Epoch [37/120    avg_loss:0.098, val_acc:0.952]
Epoch [38/120    avg_loss:0.100, val_acc:0.961]
Epoch [39/120    avg_loss:0.092, val_acc:0.957]
Epoch [40/120    avg_loss:0.072, val_acc:0.960]
Epoch [41/120    avg_loss:0.102, val_acc:0.959]
Epoch [42/120    avg_loss:0.063, val_acc:0.965]
Epoch [43/120    avg_loss:0.068, val_acc:0.958]
Epoch [44/120    avg_loss:0.051, val_acc:0.969]
Epoch [45/120    avg_loss:0.046, val_acc:0.975]
Epoch [46/120    avg_loss:0.049, val_acc:0.967]
Epoch [47/120    avg_loss:0.038, val_acc:0.969]
Epoch [48/120    avg_loss:0.039, val_acc:0.976]
Epoch [49/120    avg_loss:0.035, val_acc:0.967]
Epoch [50/120    avg_loss:0.031, val_acc:0.966]
Epoch [51/120    avg_loss:0.052, val_acc:0.970]
Epoch [52/120    avg_loss:0.039, val_acc:0.975]
Epoch [53/120    avg_loss:0.036, val_acc:0.982]
Epoch [54/120    avg_loss:0.024, val_acc:0.972]
Epoch [55/120    avg_loss:0.032, val_acc:0.977]
Epoch [56/120    avg_loss:0.021, val_acc:0.978]
Epoch [57/120    avg_loss:0.019, val_acc:0.980]
Epoch [58/120    avg_loss:0.020, val_acc:0.982]
Epoch [59/120    avg_loss:0.027, val_acc:0.976]
Epoch [60/120    avg_loss:0.044, val_acc:0.951]
Epoch [61/120    avg_loss:0.044, val_acc:0.977]
Epoch [62/120    avg_loss:0.020, val_acc:0.979]
Epoch [63/120    avg_loss:0.016, val_acc:0.980]
Epoch [64/120    avg_loss:0.011, val_acc:0.980]
Epoch [65/120    avg_loss:0.017, val_acc:0.979]
Epoch [66/120    avg_loss:0.025, val_acc:0.965]
Epoch [67/120    avg_loss:0.046, val_acc:0.965]
Epoch [68/120    avg_loss:0.037, val_acc:0.972]
Epoch [69/120    avg_loss:0.093, val_acc:0.951]
Epoch [70/120    avg_loss:0.046, val_acc:0.975]
Epoch [71/120    avg_loss:0.074, val_acc:0.954]
Epoch [72/120    avg_loss:0.033, val_acc:0.973]
Epoch [73/120    avg_loss:0.024, val_acc:0.978]
Epoch [74/120    avg_loss:0.023, val_acc:0.980]
Epoch [75/120    avg_loss:0.023, val_acc:0.982]
Epoch [76/120    avg_loss:0.018, val_acc:0.983]
Epoch [77/120    avg_loss:0.016, val_acc:0.986]
Epoch [78/120    avg_loss:0.016, val_acc:0.986]
Epoch [79/120    avg_loss:0.015, val_acc:0.986]
Epoch [80/120    avg_loss:0.015, val_acc:0.983]
Epoch [81/120    avg_loss:0.015, val_acc:0.986]
Epoch [82/120    avg_loss:0.015, val_acc:0.983]
Epoch [83/120    avg_loss:0.015, val_acc:0.985]
Epoch [84/120    avg_loss:0.020, val_acc:0.986]
Epoch [85/120    avg_loss:0.013, val_acc:0.985]
Epoch [86/120    avg_loss:0.022, val_acc:0.983]
Epoch [87/120    avg_loss:0.015, val_acc:0.985]
Epoch [88/120    avg_loss:0.016, val_acc:0.985]
Epoch [89/120    avg_loss:0.015, val_acc:0.985]
Epoch [90/120    avg_loss:0.016, val_acc:0.985]
Epoch [91/120    avg_loss:0.012, val_acc:0.986]
Epoch [92/120    avg_loss:0.018, val_acc:0.985]
Epoch [93/120    avg_loss:0.011, val_acc:0.985]
Epoch [94/120    avg_loss:0.010, val_acc:0.986]
Epoch [95/120    avg_loss:0.012, val_acc:0.986]
Epoch [96/120    avg_loss:0.011, val_acc:0.987]
Epoch [97/120    avg_loss:0.011, val_acc:0.987]
Epoch [98/120    avg_loss:0.014, val_acc:0.986]
Epoch [99/120    avg_loss:0.012, val_acc:0.986]
Epoch [100/120    avg_loss:0.013, val_acc:0.982]
Epoch [101/120    avg_loss:0.012, val_acc:0.982]
Epoch [102/120    avg_loss:0.013, val_acc:0.983]
Epoch [103/120    avg_loss:0.010, val_acc:0.985]
Epoch [104/120    avg_loss:0.012, val_acc:0.985]
Epoch [105/120    avg_loss:0.011, val_acc:0.982]
Epoch [106/120    avg_loss:0.010, val_acc:0.985]
Epoch [107/120    avg_loss:0.012, val_acc:0.982]
Epoch [108/120    avg_loss:0.010, val_acc:0.982]
Epoch [109/120    avg_loss:0.010, val_acc:0.985]
Epoch [110/120    avg_loss:0.009, val_acc:0.985]
Epoch [111/120    avg_loss:0.010, val_acc:0.985]
Epoch [112/120    avg_loss:0.010, val_acc:0.985]
Epoch [113/120    avg_loss:0.012, val_acc:0.985]
Epoch [114/120    avg_loss:0.010, val_acc:0.985]
Epoch [115/120    avg_loss:0.011, val_acc:0.985]
Epoch [116/120    avg_loss:0.009, val_acc:0.985]
Epoch [117/120    avg_loss:0.010, val_acc:0.985]
Epoch [118/120    avg_loss:0.009, val_acc:0.985]
Epoch [119/120    avg_loss:0.009, val_acc:0.985]
Epoch [120/120    avg_loss:0.011, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    3    1    0    0    0    0    0    0   21    0    0
     0    0    0]
 [   0    0    2  737    1    3    0    0    0    1    0    0    2    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  421    0    0    0    2    0    0    0    0
    12    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    5    0    0   13    0    0    0    0
     0    0    0]
 [   0    0    3   67    0    4    0    0    0    0  781   14    6    0
     0    0    0]
 [   0    0    1    0    0    0    3    0    0    0   23 2183    0    0
     0    0    0]
 [   0    0    0   11    0   10    0    0    0    0    0    3  508    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1138    1    0]
 [   0    0    0    0    0    0   28    0    0    0    0    0    0    0
    66  253    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.78048780487805

F1 scores:
[       nan 0.98765432 0.98746082 0.94185304 0.9953271  0.96449026
 0.97333333 1.         1.         0.76470588 0.93031566 0.98533063
 0.96761905 0.99730458 0.96645435 0.84193012 0.98823529]

Kappa:
0.9632633506862516
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc1646dbeb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.793, val_acc:0.296]
Epoch [2/120    avg_loss:2.639, val_acc:0.442]
Epoch [3/120    avg_loss:2.446, val_acc:0.517]
Epoch [4/120    avg_loss:2.220, val_acc:0.545]
Epoch [5/120    avg_loss:1.993, val_acc:0.565]
Epoch [6/120    avg_loss:1.784, val_acc:0.600]
Epoch [7/120    avg_loss:1.604, val_acc:0.633]
Epoch [8/120    avg_loss:1.429, val_acc:0.658]
Epoch [9/120    avg_loss:1.221, val_acc:0.695]
Epoch [10/120    avg_loss:1.082, val_acc:0.732]
Epoch [11/120    avg_loss:1.037, val_acc:0.740]
Epoch [12/120    avg_loss:0.894, val_acc:0.748]
Epoch [13/120    avg_loss:0.782, val_acc:0.778]
Epoch [14/120    avg_loss:0.702, val_acc:0.810]
Epoch [15/120    avg_loss:0.594, val_acc:0.830]
Epoch [16/120    avg_loss:0.539, val_acc:0.842]
Epoch [17/120    avg_loss:0.424, val_acc:0.852]
Epoch [18/120    avg_loss:0.416, val_acc:0.854]
Epoch [19/120    avg_loss:0.370, val_acc:0.877]
Epoch [20/120    avg_loss:0.321, val_acc:0.881]
Epoch [21/120    avg_loss:0.272, val_acc:0.888]
Epoch [22/120    avg_loss:0.264, val_acc:0.891]
Epoch [23/120    avg_loss:0.269, val_acc:0.904]
Epoch [24/120    avg_loss:0.255, val_acc:0.917]
Epoch [25/120    avg_loss:0.251, val_acc:0.888]
Epoch [26/120    avg_loss:0.203, val_acc:0.888]
Epoch [27/120    avg_loss:0.220, val_acc:0.934]
Epoch [28/120    avg_loss:0.164, val_acc:0.931]
Epoch [29/120    avg_loss:0.148, val_acc:0.944]
Epoch [30/120    avg_loss:0.161, val_acc:0.936]
Epoch [31/120    avg_loss:0.183, val_acc:0.920]
Epoch [32/120    avg_loss:0.173, val_acc:0.932]
Epoch [33/120    avg_loss:0.115, val_acc:0.943]
Epoch [34/120    avg_loss:0.108, val_acc:0.959]
Epoch [35/120    avg_loss:0.122, val_acc:0.935]
Epoch [36/120    avg_loss:0.097, val_acc:0.942]
Epoch [37/120    avg_loss:0.154, val_acc:0.943]
Epoch [38/120    avg_loss:0.143, val_acc:0.949]
Epoch [39/120    avg_loss:0.105, val_acc:0.939]
Epoch [40/120    avg_loss:0.108, val_acc:0.960]
Epoch [41/120    avg_loss:0.063, val_acc:0.967]
Epoch [42/120    avg_loss:0.078, val_acc:0.964]
Epoch [43/120    avg_loss:0.060, val_acc:0.963]
Epoch [44/120    avg_loss:0.062, val_acc:0.963]
Epoch [45/120    avg_loss:0.061, val_acc:0.965]
Epoch [46/120    avg_loss:0.064, val_acc:0.948]
Epoch [47/120    avg_loss:0.058, val_acc:0.968]
Epoch [48/120    avg_loss:0.049, val_acc:0.970]
Epoch [49/120    avg_loss:0.057, val_acc:0.971]
Epoch [50/120    avg_loss:0.064, val_acc:0.975]
Epoch [51/120    avg_loss:0.065, val_acc:0.975]
Epoch [52/120    avg_loss:0.073, val_acc:0.950]
Epoch [53/120    avg_loss:0.081, val_acc:0.974]
Epoch [54/120    avg_loss:0.054, val_acc:0.969]
Epoch [55/120    avg_loss:0.040, val_acc:0.968]
Epoch [56/120    avg_loss:0.063, val_acc:0.971]
Epoch [57/120    avg_loss:0.049, val_acc:0.971]
Epoch [58/120    avg_loss:0.040, val_acc:0.980]
Epoch [59/120    avg_loss:0.038, val_acc:0.975]
Epoch [60/120    avg_loss:0.032, val_acc:0.975]
Epoch [61/120    avg_loss:0.058, val_acc:0.969]
Epoch [62/120    avg_loss:0.184, val_acc:0.953]
Epoch [63/120    avg_loss:0.058, val_acc:0.973]
Epoch [64/120    avg_loss:0.054, val_acc:0.954]
Epoch [65/120    avg_loss:0.051, val_acc:0.978]
Epoch [66/120    avg_loss:0.045, val_acc:0.973]
Epoch [67/120    avg_loss:0.032, val_acc:0.980]
Epoch [68/120    avg_loss:0.029, val_acc:0.981]
Epoch [69/120    avg_loss:0.030, val_acc:0.985]
Epoch [70/120    avg_loss:0.024, val_acc:0.964]
Epoch [71/120    avg_loss:0.022, val_acc:0.984]
Epoch [72/120    avg_loss:0.032, val_acc:0.979]
Epoch [73/120    avg_loss:0.027, val_acc:0.967]
Epoch [74/120    avg_loss:0.024, val_acc:0.981]
Epoch [75/120    avg_loss:0.045, val_acc:0.987]
Epoch [76/120    avg_loss:0.025, val_acc:0.971]
Epoch [77/120    avg_loss:0.031, val_acc:0.979]
Epoch [78/120    avg_loss:0.020, val_acc:0.981]
Epoch [79/120    avg_loss:0.028, val_acc:0.979]
Epoch [80/120    avg_loss:0.018, val_acc:0.987]
Epoch [81/120    avg_loss:0.019, val_acc:0.989]
Epoch [82/120    avg_loss:0.014, val_acc:0.988]
Epoch [83/120    avg_loss:0.013, val_acc:0.977]
Epoch [84/120    avg_loss:0.010, val_acc:0.987]
Epoch [85/120    avg_loss:0.009, val_acc:0.984]
Epoch [86/120    avg_loss:0.010, val_acc:0.984]
Epoch [87/120    avg_loss:0.015, val_acc:0.988]
Epoch [88/120    avg_loss:0.014, val_acc:0.984]
Epoch [89/120    avg_loss:0.011, val_acc:0.988]
Epoch [90/120    avg_loss:0.021, val_acc:0.981]
Epoch [91/120    avg_loss:0.029, val_acc:0.983]
Epoch [92/120    avg_loss:0.012, val_acc:0.982]
Epoch [93/120    avg_loss:0.010, val_acc:0.983]
Epoch [94/120    avg_loss:0.008, val_acc:0.985]
Epoch [95/120    avg_loss:0.007, val_acc:0.987]
Epoch [96/120    avg_loss:0.007, val_acc:0.983]
Epoch [97/120    avg_loss:0.007, val_acc:0.983]
Epoch [98/120    avg_loss:0.009, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.987]
Epoch [100/120    avg_loss:0.007, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.989]
Epoch [103/120    avg_loss:0.007, val_acc:0.989]
Epoch [104/120    avg_loss:0.008, val_acc:0.989]
Epoch [105/120    avg_loss:0.006, val_acc:0.989]
Epoch [106/120    avg_loss:0.006, val_acc:0.989]
Epoch [107/120    avg_loss:0.007, val_acc:0.989]
Epoch [108/120    avg_loss:0.007, val_acc:0.989]
Epoch [109/120    avg_loss:0.007, val_acc:0.989]
Epoch [110/120    avg_loss:0.008, val_acc:0.989]
Epoch [111/120    avg_loss:0.006, val_acc:0.989]
Epoch [112/120    avg_loss:0.007, val_acc:0.989]
Epoch [113/120    avg_loss:0.006, val_acc:0.989]
Epoch [114/120    avg_loss:0.005, val_acc:0.989]
Epoch [115/120    avg_loss:0.007, val_acc:0.989]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.006, val_acc:0.989]
Epoch [118/120    avg_loss:0.008, val_acc:0.989]
Epoch [119/120    avg_loss:0.006, val_acc:0.989]
Epoch [120/120    avg_loss:0.007, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1264    0    0    0    0    0    0    0    0   15    6    0
     0    0    0]
 [   0    0    1  740    1    4    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  423    0    0    0    1    0    1    0    0
    10    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    1   44    0    6    1    0    0    0  814    3    1    0
     2    3    0]
 [   0    0    2    0    0    0    0    1    0    0   11 2196    0    0
     0    0    0]
 [   0    0    0    9    1    8    0    0    0    0    0    5  508    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    5    9    0    0    0    0    0    0    0
    98  235    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.15989159891599

F1 scores:
[       nan 0.98765432 0.9902076  0.96103896 0.99297424 0.96027242
 0.99017385 0.98039216 0.99883856 0.94444444 0.95708407 0.99097473
 0.96394687 1.         0.95349811 0.80068143 0.97590361]

Kappa:
0.9675824603184408
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc466ba4f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.771, val_acc:0.385]
Epoch [2/120    avg_loss:2.598, val_acc:0.477]
Epoch [3/120    avg_loss:2.386, val_acc:0.499]
Epoch [4/120    avg_loss:2.175, val_acc:0.555]
Epoch [5/120    avg_loss:1.992, val_acc:0.576]
Epoch [6/120    avg_loss:1.831, val_acc:0.606]
Epoch [7/120    avg_loss:1.650, val_acc:0.627]
Epoch [8/120    avg_loss:1.519, val_acc:0.676]
Epoch [9/120    avg_loss:1.334, val_acc:0.686]
Epoch [10/120    avg_loss:1.240, val_acc:0.713]
Epoch [11/120    avg_loss:1.086, val_acc:0.720]
Epoch [12/120    avg_loss:0.951, val_acc:0.723]
Epoch [13/120    avg_loss:0.847, val_acc:0.796]
Epoch [14/120    avg_loss:0.810, val_acc:0.756]
Epoch [15/120    avg_loss:0.762, val_acc:0.799]
Epoch [16/120    avg_loss:0.603, val_acc:0.813]
Epoch [17/120    avg_loss:0.603, val_acc:0.803]
Epoch [18/120    avg_loss:0.587, val_acc:0.801]
Epoch [19/120    avg_loss:0.509, val_acc:0.851]
Epoch [20/120    avg_loss:0.460, val_acc:0.846]
Epoch [21/120    avg_loss:0.430, val_acc:0.847]
Epoch [22/120    avg_loss:0.380, val_acc:0.844]
Epoch [23/120    avg_loss:0.384, val_acc:0.879]
Epoch [24/120    avg_loss:0.334, val_acc:0.900]
Epoch [25/120    avg_loss:0.318, val_acc:0.886]
Epoch [26/120    avg_loss:0.283, val_acc:0.908]
Epoch [27/120    avg_loss:0.277, val_acc:0.900]
Epoch [28/120    avg_loss:0.300, val_acc:0.904]
Epoch [29/120    avg_loss:0.237, val_acc:0.899]
Epoch [30/120    avg_loss:0.183, val_acc:0.917]
Epoch [31/120    avg_loss:0.165, val_acc:0.936]
Epoch [32/120    avg_loss:0.164, val_acc:0.920]
Epoch [33/120    avg_loss:0.153, val_acc:0.934]
Epoch [34/120    avg_loss:0.161, val_acc:0.890]
Epoch [35/120    avg_loss:0.147, val_acc:0.948]
Epoch [36/120    avg_loss:0.167, val_acc:0.895]
Epoch [37/120    avg_loss:0.149, val_acc:0.927]
Epoch [38/120    avg_loss:0.196, val_acc:0.933]
Epoch [39/120    avg_loss:0.156, val_acc:0.919]
Epoch [40/120    avg_loss:0.179, val_acc:0.882]
Epoch [41/120    avg_loss:0.194, val_acc:0.946]
Epoch [42/120    avg_loss:0.118, val_acc:0.934]
Epoch [43/120    avg_loss:0.090, val_acc:0.949]
Epoch [44/120    avg_loss:0.126, val_acc:0.937]
Epoch [45/120    avg_loss:0.157, val_acc:0.937]
Epoch [46/120    avg_loss:0.119, val_acc:0.934]
Epoch [47/120    avg_loss:0.124, val_acc:0.949]
Epoch [48/120    avg_loss:0.088, val_acc:0.958]
Epoch [49/120    avg_loss:0.060, val_acc:0.965]
Epoch [50/120    avg_loss:0.067, val_acc:0.960]
Epoch [51/120    avg_loss:0.069, val_acc:0.959]
Epoch [52/120    avg_loss:0.056, val_acc:0.962]
Epoch [53/120    avg_loss:0.059, val_acc:0.962]
Epoch [54/120    avg_loss:0.041, val_acc:0.968]
Epoch [55/120    avg_loss:0.055, val_acc:0.962]
Epoch [56/120    avg_loss:0.059, val_acc:0.960]
Epoch [57/120    avg_loss:0.051, val_acc:0.972]
Epoch [58/120    avg_loss:0.055, val_acc:0.963]
Epoch [59/120    avg_loss:0.041, val_acc:0.947]
Epoch [60/120    avg_loss:0.046, val_acc:0.967]
Epoch [61/120    avg_loss:0.048, val_acc:0.963]
Epoch [62/120    avg_loss:0.092, val_acc:0.960]
Epoch [63/120    avg_loss:0.051, val_acc:0.971]
Epoch [64/120    avg_loss:0.037, val_acc:0.976]
Epoch [65/120    avg_loss:0.041, val_acc:0.969]
Epoch [66/120    avg_loss:0.038, val_acc:0.970]
Epoch [67/120    avg_loss:0.028, val_acc:0.975]
Epoch [68/120    avg_loss:0.037, val_acc:0.971]
Epoch [69/120    avg_loss:0.033, val_acc:0.972]
Epoch [70/120    avg_loss:0.028, val_acc:0.970]
Epoch [71/120    avg_loss:0.027, val_acc:0.977]
Epoch [72/120    avg_loss:0.070, val_acc:0.916]
Epoch [73/120    avg_loss:0.039, val_acc:0.977]
Epoch [74/120    avg_loss:0.023, val_acc:0.980]
Epoch [75/120    avg_loss:0.031, val_acc:0.975]
Epoch [76/120    avg_loss:0.020, val_acc:0.976]
Epoch [77/120    avg_loss:0.017, val_acc:0.982]
Epoch [78/120    avg_loss:0.017, val_acc:0.977]
Epoch [79/120    avg_loss:0.017, val_acc:0.986]
Epoch [80/120    avg_loss:0.013, val_acc:0.979]
Epoch [81/120    avg_loss:0.014, val_acc:0.979]
Epoch [82/120    avg_loss:0.015, val_acc:0.982]
Epoch [83/120    avg_loss:0.021, val_acc:0.976]
Epoch [84/120    avg_loss:0.017, val_acc:0.983]
Epoch [85/120    avg_loss:0.024, val_acc:0.972]
Epoch [86/120    avg_loss:0.018, val_acc:0.972]
Epoch [87/120    avg_loss:0.019, val_acc:0.968]
Epoch [88/120    avg_loss:0.027, val_acc:0.979]
Epoch [89/120    avg_loss:0.012, val_acc:0.971]
Epoch [90/120    avg_loss:0.028, val_acc:0.976]
Epoch [91/120    avg_loss:0.026, val_acc:0.983]
Epoch [92/120    avg_loss:0.038, val_acc:0.845]
Epoch [93/120    avg_loss:0.082, val_acc:0.971]
Epoch [94/120    avg_loss:0.022, val_acc:0.979]
Epoch [95/120    avg_loss:0.017, val_acc:0.981]
Epoch [96/120    avg_loss:0.016, val_acc:0.981]
Epoch [97/120    avg_loss:0.031, val_acc:0.982]
Epoch [98/120    avg_loss:0.013, val_acc:0.985]
Epoch [99/120    avg_loss:0.013, val_acc:0.982]
Epoch [100/120    avg_loss:0.011, val_acc:0.986]
Epoch [101/120    avg_loss:0.012, val_acc:0.983]
Epoch [102/120    avg_loss:0.011, val_acc:0.986]
Epoch [103/120    avg_loss:0.008, val_acc:0.986]
Epoch [104/120    avg_loss:0.013, val_acc:0.985]
Epoch [105/120    avg_loss:0.009, val_acc:0.986]
Epoch [106/120    avg_loss:0.010, val_acc:0.986]
Epoch [107/120    avg_loss:0.010, val_acc:0.986]
Epoch [108/120    avg_loss:0.010, val_acc:0.987]
Epoch [109/120    avg_loss:0.011, val_acc:0.988]
Epoch [110/120    avg_loss:0.009, val_acc:0.988]
Epoch [111/120    avg_loss:0.010, val_acc:0.989]
Epoch [112/120    avg_loss:0.013, val_acc:0.989]
Epoch [113/120    avg_loss:0.010, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.008, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.010, val_acc:0.987]
Epoch [120/120    avg_loss:0.008, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1248    1    1    0    0    0    0    0    2   21    8    0
     0    4    0]
 [   0    0    3  718    0    0    0    0    0    1    0    0   24    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    2    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    5   76    0    6    0    0    0    0  780    1    7    0
     0    0    0]
 [   0    0    3    0    0    1    0    0    0    0    8 2196    2    0
     0    0    0]
 [   0    0    0   19    0   10    0    0    0    0    5    0  499    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    0    1    0
  1135    0    0]
 [   0    0    0    0    0    0   34    0    0    0    0    0    1    0
    72  240    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.34688346883469

F1 scores:
[       nan 0.98765432 0.98074656 0.918746   0.99765808 0.9738933
 0.97104677 1.         0.99883856 0.84848485 0.93301435 0.99097473
 0.9257885  1.         0.96513605 0.81218274 0.98203593]

Kappa:
0.9583184862234708
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f890ee16eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.809, val_acc:0.093]
Epoch [2/120    avg_loss:2.736, val_acc:0.247]
Epoch [3/120    avg_loss:2.652, val_acc:0.352]
Epoch [4/120    avg_loss:2.551, val_acc:0.509]
Epoch [5/120    avg_loss:2.433, val_acc:0.567]
Epoch [6/120    avg_loss:2.280, val_acc:0.580]
Epoch [7/120    avg_loss:2.160, val_acc:0.590]
Epoch [8/120    avg_loss:2.013, val_acc:0.609]
Epoch [9/120    avg_loss:1.766, val_acc:0.593]
Epoch [10/120    avg_loss:1.608, val_acc:0.632]
Epoch [11/120    avg_loss:1.482, val_acc:0.641]
Epoch [12/120    avg_loss:1.331, val_acc:0.623]
Epoch [13/120    avg_loss:1.163, val_acc:0.711]
Epoch [14/120    avg_loss:1.063, val_acc:0.752]
Epoch [15/120    avg_loss:0.937, val_acc:0.785]
Epoch [16/120    avg_loss:0.854, val_acc:0.798]
Epoch [17/120    avg_loss:0.690, val_acc:0.835]
Epoch [18/120    avg_loss:0.638, val_acc:0.768]
Epoch [19/120    avg_loss:0.615, val_acc:0.842]
Epoch [20/120    avg_loss:0.504, val_acc:0.875]
Epoch [21/120    avg_loss:0.387, val_acc:0.876]
Epoch [22/120    avg_loss:0.426, val_acc:0.855]
Epoch [23/120    avg_loss:0.340, val_acc:0.903]
Epoch [24/120    avg_loss:0.274, val_acc:0.942]
Epoch [25/120    avg_loss:0.224, val_acc:0.945]
Epoch [26/120    avg_loss:0.239, val_acc:0.931]
Epoch [27/120    avg_loss:0.166, val_acc:0.946]
Epoch [28/120    avg_loss:0.137, val_acc:0.953]
Epoch [29/120    avg_loss:0.157, val_acc:0.927]
Epoch [30/120    avg_loss:0.132, val_acc:0.965]
Epoch [31/120    avg_loss:0.116, val_acc:0.953]
Epoch [32/120    avg_loss:0.105, val_acc:0.953]
Epoch [33/120    avg_loss:0.114, val_acc:0.931]
Epoch [34/120    avg_loss:0.161, val_acc:0.923]
Epoch [35/120    avg_loss:0.183, val_acc:0.957]
Epoch [36/120    avg_loss:0.121, val_acc:0.920]
Epoch [37/120    avg_loss:0.151, val_acc:0.964]
Epoch [38/120    avg_loss:0.119, val_acc:0.947]
Epoch [39/120    avg_loss:0.114, val_acc:0.954]
Epoch [40/120    avg_loss:0.101, val_acc:0.961]
Epoch [41/120    avg_loss:0.084, val_acc:0.963]
Epoch [42/120    avg_loss:0.065, val_acc:0.963]
Epoch [43/120    avg_loss:0.064, val_acc:0.975]
Epoch [44/120    avg_loss:0.044, val_acc:0.976]
Epoch [45/120    avg_loss:0.042, val_acc:0.975]
Epoch [46/120    avg_loss:0.039, val_acc:0.977]
Epoch [47/120    avg_loss:0.045, val_acc:0.955]
Epoch [48/120    avg_loss:0.046, val_acc:0.974]
Epoch [49/120    avg_loss:0.062, val_acc:0.963]
Epoch [50/120    avg_loss:0.162, val_acc:0.863]
Epoch [51/120    avg_loss:0.275, val_acc:0.868]
Epoch [52/120    avg_loss:0.153, val_acc:0.968]
Epoch [53/120    avg_loss:0.148, val_acc:0.928]
Epoch [54/120    avg_loss:0.068, val_acc:0.974]
Epoch [55/120    avg_loss:0.063, val_acc:0.961]
Epoch [56/120    avg_loss:0.045, val_acc:0.958]
Epoch [57/120    avg_loss:0.040, val_acc:0.968]
Epoch [58/120    avg_loss:0.035, val_acc:0.959]
Epoch [59/120    avg_loss:0.062, val_acc:0.976]
Epoch [60/120    avg_loss:0.032, val_acc:0.980]
Epoch [61/120    avg_loss:0.027, val_acc:0.980]
Epoch [62/120    avg_loss:0.021, val_acc:0.982]
Epoch [63/120    avg_loss:0.025, val_acc:0.981]
Epoch [64/120    avg_loss:0.026, val_acc:0.979]
Epoch [65/120    avg_loss:0.021, val_acc:0.982]
Epoch [66/120    avg_loss:0.025, val_acc:0.982]
Epoch [67/120    avg_loss:0.021, val_acc:0.981]
Epoch [68/120    avg_loss:0.024, val_acc:0.982]
Epoch [69/120    avg_loss:0.020, val_acc:0.983]
Epoch [70/120    avg_loss:0.026, val_acc:0.983]
Epoch [71/120    avg_loss:0.022, val_acc:0.984]
Epoch [72/120    avg_loss:0.021, val_acc:0.984]
Epoch [73/120    avg_loss:0.019, val_acc:0.984]
Epoch [74/120    avg_loss:0.018, val_acc:0.986]
Epoch [75/120    avg_loss:0.019, val_acc:0.984]
Epoch [76/120    avg_loss:0.018, val_acc:0.985]
Epoch [77/120    avg_loss:0.019, val_acc:0.985]
Epoch [78/120    avg_loss:0.021, val_acc:0.985]
Epoch [79/120    avg_loss:0.020, val_acc:0.984]
Epoch [80/120    avg_loss:0.017, val_acc:0.984]
Epoch [81/120    avg_loss:0.021, val_acc:0.983]
Epoch [82/120    avg_loss:0.017, val_acc:0.984]
Epoch [83/120    avg_loss:0.018, val_acc:0.984]
Epoch [84/120    avg_loss:0.017, val_acc:0.986]
Epoch [85/120    avg_loss:0.017, val_acc:0.985]
Epoch [86/120    avg_loss:0.021, val_acc:0.984]
Epoch [87/120    avg_loss:0.018, val_acc:0.985]
Epoch [88/120    avg_loss:0.020, val_acc:0.985]
Epoch [89/120    avg_loss:0.019, val_acc:0.984]
Epoch [90/120    avg_loss:0.018, val_acc:0.985]
Epoch [91/120    avg_loss:0.017, val_acc:0.985]
Epoch [92/120    avg_loss:0.022, val_acc:0.985]
Epoch [93/120    avg_loss:0.015, val_acc:0.982]
Epoch [94/120    avg_loss:0.016, val_acc:0.984]
Epoch [95/120    avg_loss:0.018, val_acc:0.983]
Epoch [96/120    avg_loss:0.016, val_acc:0.983]
Epoch [97/120    avg_loss:0.019, val_acc:0.983]
Epoch [98/120    avg_loss:0.016, val_acc:0.983]
Epoch [99/120    avg_loss:0.016, val_acc:0.983]
Epoch [100/120    avg_loss:0.018, val_acc:0.984]
Epoch [101/120    avg_loss:0.016, val_acc:0.984]
Epoch [102/120    avg_loss:0.017, val_acc:0.984]
Epoch [103/120    avg_loss:0.016, val_acc:0.984]
Epoch [104/120    avg_loss:0.014, val_acc:0.984]
Epoch [105/120    avg_loss:0.018, val_acc:0.984]
Epoch [106/120    avg_loss:0.013, val_acc:0.984]
Epoch [107/120    avg_loss:0.017, val_acc:0.984]
Epoch [108/120    avg_loss:0.017, val_acc:0.984]
Epoch [109/120    avg_loss:0.015, val_acc:0.984]
Epoch [110/120    avg_loss:0.016, val_acc:0.984]
Epoch [111/120    avg_loss:0.015, val_acc:0.984]
Epoch [112/120    avg_loss:0.016, val_acc:0.984]
Epoch [113/120    avg_loss:0.015, val_acc:0.984]
Epoch [114/120    avg_loss:0.016, val_acc:0.984]
Epoch [115/120    avg_loss:0.019, val_acc:0.984]
Epoch [116/120    avg_loss:0.016, val_acc:0.984]
Epoch [117/120    avg_loss:0.018, val_acc:0.984]
Epoch [118/120    avg_loss:0.015, val_acc:0.984]
Epoch [119/120    avg_loss:0.016, val_acc:0.984]
Epoch [120/120    avg_loss:0.016, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    2    0    0    2    0    1    0    0    0
     0    0    0]
 [   0    0 1262    3    3    0    0    0    0    0    2   15    0    0
     0    0    0]
 [   0    0    0  707    3    1    0    0    0    5    4   13   13    0
     0    1    0]
 [   0    0    0    1  210    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  849   18    0    0
     2    0    0]
 [   0    4    6    0    0    0    0    0    9    0   18 2151   20    0
     0    1    1]
 [   0    0    1    1    0    0    0    0    0    0    0    6  522    0
     1    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    80  264    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.09485094850949

F1 scores:
[       nan 0.88888889 0.9859375  0.96915696 0.97902098 0.99310345
 0.99696049 1.         0.98737084 0.87804878 0.97084048 0.97484704
 0.95692026 1.         0.95748299 0.84210526 0.98245614]

Kappa:
0.9668675826212771
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3ae1cc8cc0>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.818, val_acc:0.290]
Epoch [2/120    avg_loss:2.735, val_acc:0.330]
Epoch [3/120    avg_loss:2.631, val_acc:0.333]
Epoch [4/120    avg_loss:2.514, val_acc:0.367]
Epoch [5/120    avg_loss:2.364, val_acc:0.431]
Epoch [6/120    avg_loss:2.255, val_acc:0.472]
Epoch [7/120    avg_loss:2.100, val_acc:0.473]
Epoch [8/120    avg_loss:1.970, val_acc:0.576]
Epoch [9/120    avg_loss:1.809, val_acc:0.633]
Epoch [10/120    avg_loss:1.600, val_acc:0.668]
Epoch [11/120    avg_loss:1.504, val_acc:0.694]
Epoch [12/120    avg_loss:1.271, val_acc:0.710]
Epoch [13/120    avg_loss:1.117, val_acc:0.746]
Epoch [14/120    avg_loss:1.053, val_acc:0.766]
Epoch [15/120    avg_loss:0.880, val_acc:0.784]
Epoch [16/120    avg_loss:0.767, val_acc:0.791]
Epoch [17/120    avg_loss:0.724, val_acc:0.828]
Epoch [18/120    avg_loss:0.622, val_acc:0.785]
Epoch [19/120    avg_loss:0.560, val_acc:0.832]
Epoch [20/120    avg_loss:0.490, val_acc:0.853]
Epoch [21/120    avg_loss:0.425, val_acc:0.854]
Epoch [22/120    avg_loss:0.331, val_acc:0.869]
Epoch [23/120    avg_loss:0.330, val_acc:0.868]
Epoch [24/120    avg_loss:0.312, val_acc:0.870]
Epoch [25/120    avg_loss:0.288, val_acc:0.885]
Epoch [26/120    avg_loss:0.221, val_acc:0.904]
Epoch [27/120    avg_loss:0.231, val_acc:0.919]
Epoch [28/120    avg_loss:0.307, val_acc:0.854]
Epoch [29/120    avg_loss:0.240, val_acc:0.882]
Epoch [30/120    avg_loss:0.212, val_acc:0.902]
Epoch [31/120    avg_loss:0.175, val_acc:0.928]
Epoch [32/120    avg_loss:0.165, val_acc:0.942]
Epoch [33/120    avg_loss:0.118, val_acc:0.927]
Epoch [34/120    avg_loss:0.248, val_acc:0.924]
Epoch [35/120    avg_loss:0.127, val_acc:0.936]
Epoch [36/120    avg_loss:0.092, val_acc:0.930]
Epoch [37/120    avg_loss:0.087, val_acc:0.948]
Epoch [38/120    avg_loss:0.134, val_acc:0.938]
Epoch [39/120    avg_loss:0.088, val_acc:0.948]
Epoch [40/120    avg_loss:0.174, val_acc:0.915]
Epoch [41/120    avg_loss:0.193, val_acc:0.936]
Epoch [42/120    avg_loss:0.085, val_acc:0.948]
Epoch [43/120    avg_loss:0.096, val_acc:0.946]
Epoch [44/120    avg_loss:0.074, val_acc:0.954]
Epoch [45/120    avg_loss:0.059, val_acc:0.956]
Epoch [46/120    avg_loss:0.069, val_acc:0.946]
Epoch [47/120    avg_loss:0.084, val_acc:0.942]
Epoch [48/120    avg_loss:0.069, val_acc:0.959]
Epoch [49/120    avg_loss:0.051, val_acc:0.960]
Epoch [50/120    avg_loss:0.055, val_acc:0.959]
Epoch [51/120    avg_loss:0.043, val_acc:0.952]
Epoch [52/120    avg_loss:0.044, val_acc:0.966]
Epoch [53/120    avg_loss:0.089, val_acc:0.946]
Epoch [54/120    avg_loss:0.044, val_acc:0.957]
Epoch [55/120    avg_loss:0.046, val_acc:0.955]
Epoch [56/120    avg_loss:0.044, val_acc:0.956]
Epoch [57/120    avg_loss:0.050, val_acc:0.960]
Epoch [58/120    avg_loss:0.037, val_acc:0.950]
Epoch [59/120    avg_loss:0.057, val_acc:0.958]
Epoch [60/120    avg_loss:0.047, val_acc:0.966]
Epoch [61/120    avg_loss:0.034, val_acc:0.964]
Epoch [62/120    avg_loss:0.021, val_acc:0.975]
Epoch [63/120    avg_loss:0.031, val_acc:0.977]
Epoch [64/120    avg_loss:0.035, val_acc:0.956]
Epoch [65/120    avg_loss:0.035, val_acc:0.975]
Epoch [66/120    avg_loss:0.028, val_acc:0.967]
Epoch [67/120    avg_loss:0.036, val_acc:0.971]
Epoch [68/120    avg_loss:0.039, val_acc:0.932]
Epoch [69/120    avg_loss:0.031, val_acc:0.967]
Epoch [70/120    avg_loss:0.025, val_acc:0.967]
Epoch [71/120    avg_loss:0.033, val_acc:0.973]
Epoch [72/120    avg_loss:0.023, val_acc:0.964]
Epoch [73/120    avg_loss:0.021, val_acc:0.971]
Epoch [74/120    avg_loss:0.017, val_acc:0.974]
Epoch [75/120    avg_loss:0.019, val_acc:0.968]
Epoch [76/120    avg_loss:0.029, val_acc:0.971]
Epoch [77/120    avg_loss:0.016, val_acc:0.976]
Epoch [78/120    avg_loss:0.016, val_acc:0.979]
Epoch [79/120    avg_loss:0.016, val_acc:0.978]
Epoch [80/120    avg_loss:0.012, val_acc:0.978]
Epoch [81/120    avg_loss:0.012, val_acc:0.976]
Epoch [82/120    avg_loss:0.010, val_acc:0.977]
Epoch [83/120    avg_loss:0.010, val_acc:0.977]
Epoch [84/120    avg_loss:0.012, val_acc:0.978]
Epoch [85/120    avg_loss:0.015, val_acc:0.979]
Epoch [86/120    avg_loss:0.011, val_acc:0.978]
Epoch [87/120    avg_loss:0.012, val_acc:0.978]
Epoch [88/120    avg_loss:0.010, val_acc:0.978]
Epoch [89/120    avg_loss:0.010, val_acc:0.979]
Epoch [90/120    avg_loss:0.011, val_acc:0.978]
Epoch [91/120    avg_loss:0.013, val_acc:0.978]
Epoch [92/120    avg_loss:0.010, val_acc:0.978]
Epoch [93/120    avg_loss:0.015, val_acc:0.977]
Epoch [94/120    avg_loss:0.011, val_acc:0.979]
Epoch [95/120    avg_loss:0.011, val_acc:0.979]
Epoch [96/120    avg_loss:0.009, val_acc:0.981]
Epoch [97/120    avg_loss:0.009, val_acc:0.979]
Epoch [98/120    avg_loss:0.010, val_acc:0.979]
Epoch [99/120    avg_loss:0.009, val_acc:0.979]
Epoch [100/120    avg_loss:0.013, val_acc:0.976]
Epoch [101/120    avg_loss:0.010, val_acc:0.979]
Epoch [102/120    avg_loss:0.009, val_acc:0.979]
Epoch [103/120    avg_loss:0.009, val_acc:0.978]
Epoch [104/120    avg_loss:0.010, val_acc:0.979]
Epoch [105/120    avg_loss:0.010, val_acc:0.979]
Epoch [106/120    avg_loss:0.008, val_acc:0.979]
Epoch [107/120    avg_loss:0.009, val_acc:0.979]
Epoch [108/120    avg_loss:0.011, val_acc:0.979]
Epoch [109/120    avg_loss:0.010, val_acc:0.979]
Epoch [110/120    avg_loss:0.009, val_acc:0.979]
Epoch [111/120    avg_loss:0.012, val_acc:0.979]
Epoch [112/120    avg_loss:0.009, val_acc:0.979]
Epoch [113/120    avg_loss:0.008, val_acc:0.979]
Epoch [114/120    avg_loss:0.009, val_acc:0.978]
Epoch [115/120    avg_loss:0.009, val_acc:0.978]
Epoch [116/120    avg_loss:0.010, val_acc:0.979]
Epoch [117/120    avg_loss:0.010, val_acc:0.979]
Epoch [118/120    avg_loss:0.008, val_acc:0.979]
Epoch [119/120    avg_loss:0.008, val_acc:0.979]
Epoch [120/120    avg_loss:0.009, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    1    0    1    0    0    0    0    0   16    0    0
     0    0    0]
 [   0    0    0  733    2    0    0    0    0    1    5    2    4    0
     0    0    0]
 [   0    0    0    1  209    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9    0    1    0    0    0    0    0  846   19    0    0
     0    0    0]
 [   0    0    9    0    1    0    1    0    0    0   25 2148   17    9
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    3    1  525    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    1    3    0    0    0    0    0    0    0
    81  262    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.45257452574526

F1 scores:
[       nan 1.         0.98599222 0.98853675 0.98122066 0.99770642
 0.9969651  1.         0.99649942 0.97297297 0.96465222 0.97725205
 0.96596136 0.9762533  0.96126011 0.84380032 0.98224852]

Kappa:
0.9709524049686901
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f42b3f49eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.812, val_acc:0.166]
Epoch [2/120    avg_loss:2.730, val_acc:0.291]
Epoch [3/120    avg_loss:2.632, val_acc:0.361]
Epoch [4/120    avg_loss:2.487, val_acc:0.387]
Epoch [5/120    avg_loss:2.351, val_acc:0.465]
Epoch [6/120    avg_loss:2.195, val_acc:0.455]
Epoch [7/120    avg_loss:2.037, val_acc:0.505]
Epoch [8/120    avg_loss:1.867, val_acc:0.631]
Epoch [9/120    avg_loss:1.674, val_acc:0.636]
Epoch [10/120    avg_loss:1.507, val_acc:0.662]
Epoch [11/120    avg_loss:1.351, val_acc:0.663]
Epoch [12/120    avg_loss:1.249, val_acc:0.679]
Epoch [13/120    avg_loss:1.093, val_acc:0.693]
Epoch [14/120    avg_loss:0.941, val_acc:0.747]
Epoch [15/120    avg_loss:0.821, val_acc:0.770]
Epoch [16/120    avg_loss:0.709, val_acc:0.780]
Epoch [17/120    avg_loss:0.740, val_acc:0.781]
Epoch [18/120    avg_loss:0.680, val_acc:0.823]
Epoch [19/120    avg_loss:0.545, val_acc:0.815]
Epoch [20/120    avg_loss:0.472, val_acc:0.853]
Epoch [21/120    avg_loss:0.401, val_acc:0.860]
Epoch [22/120    avg_loss:0.323, val_acc:0.881]
Epoch [23/120    avg_loss:0.294, val_acc:0.858]
Epoch [24/120    avg_loss:0.286, val_acc:0.878]
Epoch [25/120    avg_loss:0.274, val_acc:0.872]
Epoch [26/120    avg_loss:0.218, val_acc:0.918]
Epoch [27/120    avg_loss:0.207, val_acc:0.892]
Epoch [28/120    avg_loss:0.151, val_acc:0.897]
Epoch [29/120    avg_loss:0.207, val_acc:0.884]
Epoch [30/120    avg_loss:0.210, val_acc:0.905]
Epoch [31/120    avg_loss:0.176, val_acc:0.898]
Epoch [32/120    avg_loss:0.127, val_acc:0.912]
Epoch [33/120    avg_loss:0.271, val_acc:0.881]
Epoch [34/120    avg_loss:0.153, val_acc:0.890]
Epoch [35/120    avg_loss:0.153, val_acc:0.922]
Epoch [36/120    avg_loss:0.114, val_acc:0.927]
Epoch [37/120    avg_loss:0.106, val_acc:0.935]
Epoch [38/120    avg_loss:0.086, val_acc:0.929]
Epoch [39/120    avg_loss:0.089, val_acc:0.939]
Epoch [40/120    avg_loss:0.088, val_acc:0.910]
Epoch [41/120    avg_loss:0.062, val_acc:0.942]
Epoch [42/120    avg_loss:0.059, val_acc:0.928]
Epoch [43/120    avg_loss:0.062, val_acc:0.946]
Epoch [44/120    avg_loss:0.056, val_acc:0.942]
Epoch [45/120    avg_loss:0.056, val_acc:0.916]
Epoch [46/120    avg_loss:0.089, val_acc:0.939]
Epoch [47/120    avg_loss:0.048, val_acc:0.944]
Epoch [48/120    avg_loss:0.053, val_acc:0.949]
Epoch [49/120    avg_loss:0.068, val_acc:0.934]
Epoch [50/120    avg_loss:0.083, val_acc:0.909]
Epoch [51/120    avg_loss:0.067, val_acc:0.939]
Epoch [52/120    avg_loss:0.060, val_acc:0.957]
Epoch [53/120    avg_loss:0.033, val_acc:0.952]
Epoch [54/120    avg_loss:0.043, val_acc:0.958]
Epoch [55/120    avg_loss:0.046, val_acc:0.955]
Epoch [56/120    avg_loss:0.033, val_acc:0.959]
Epoch [57/120    avg_loss:0.033, val_acc:0.959]
Epoch [58/120    avg_loss:0.026, val_acc:0.961]
Epoch [59/120    avg_loss:0.026, val_acc:0.957]
Epoch [60/120    avg_loss:0.023, val_acc:0.969]
Epoch [61/120    avg_loss:0.025, val_acc:0.965]
Epoch [62/120    avg_loss:0.025, val_acc:0.948]
Epoch [63/120    avg_loss:0.019, val_acc:0.963]
Epoch [64/120    avg_loss:0.027, val_acc:0.966]
Epoch [65/120    avg_loss:0.022, val_acc:0.972]
Epoch [66/120    avg_loss:0.022, val_acc:0.968]
Epoch [67/120    avg_loss:0.017, val_acc:0.966]
Epoch [68/120    avg_loss:0.033, val_acc:0.962]
Epoch [69/120    avg_loss:0.052, val_acc:0.957]
Epoch [70/120    avg_loss:0.049, val_acc:0.950]
Epoch [71/120    avg_loss:0.040, val_acc:0.961]
Epoch [72/120    avg_loss:0.023, val_acc:0.968]
Epoch [73/120    avg_loss:0.026, val_acc:0.963]
Epoch [74/120    avg_loss:0.062, val_acc:0.947]
Epoch [75/120    avg_loss:0.034, val_acc:0.960]
Epoch [76/120    avg_loss:0.027, val_acc:0.960]
Epoch [77/120    avg_loss:0.023, val_acc:0.966]
Epoch [78/120    avg_loss:0.023, val_acc:0.972]
Epoch [79/120    avg_loss:0.029, val_acc:0.965]
Epoch [80/120    avg_loss:0.015, val_acc:0.971]
Epoch [81/120    avg_loss:0.015, val_acc:0.972]
Epoch [82/120    avg_loss:0.019, val_acc:0.963]
Epoch [83/120    avg_loss:0.026, val_acc:0.950]
Epoch [84/120    avg_loss:0.016, val_acc:0.970]
Epoch [85/120    avg_loss:0.016, val_acc:0.972]
Epoch [86/120    avg_loss:0.013, val_acc:0.973]
Epoch [87/120    avg_loss:0.012, val_acc:0.963]
Epoch [88/120    avg_loss:0.014, val_acc:0.970]
Epoch [89/120    avg_loss:0.010, val_acc:0.977]
Epoch [90/120    avg_loss:0.017, val_acc:0.973]
Epoch [91/120    avg_loss:0.011, val_acc:0.974]
Epoch [92/120    avg_loss:0.018, val_acc:0.962]
Epoch [93/120    avg_loss:0.012, val_acc:0.965]
Epoch [94/120    avg_loss:0.024, val_acc:0.969]
Epoch [95/120    avg_loss:0.014, val_acc:0.974]
Epoch [96/120    avg_loss:0.008, val_acc:0.974]
Epoch [97/120    avg_loss:0.009, val_acc:0.974]
Epoch [98/120    avg_loss:0.010, val_acc:0.974]
Epoch [99/120    avg_loss:0.009, val_acc:0.978]
Epoch [100/120    avg_loss:0.007, val_acc:0.974]
Epoch [101/120    avg_loss:0.007, val_acc:0.974]
Epoch [102/120    avg_loss:0.008, val_acc:0.980]
Epoch [103/120    avg_loss:0.009, val_acc:0.966]
Epoch [104/120    avg_loss:0.006, val_acc:0.973]
Epoch [105/120    avg_loss:0.007, val_acc:0.974]
Epoch [106/120    avg_loss:0.011, val_acc:0.974]
Epoch [107/120    avg_loss:0.006, val_acc:0.968]
Epoch [108/120    avg_loss:0.006, val_acc:0.974]
Epoch [109/120    avg_loss:0.006, val_acc:0.971]
Epoch [110/120    avg_loss:0.008, val_acc:0.974]
Epoch [111/120    avg_loss:0.008, val_acc:0.980]
Epoch [112/120    avg_loss:0.006, val_acc:0.975]
Epoch [113/120    avg_loss:0.006, val_acc:0.973]
Epoch [114/120    avg_loss:0.020, val_acc:0.972]
Epoch [115/120    avg_loss:0.015, val_acc:0.967]
Epoch [116/120    avg_loss:0.012, val_acc:0.974]
Epoch [117/120    avg_loss:0.006, val_acc:0.976]
Epoch [118/120    avg_loss:0.006, val_acc:0.975]
Epoch [119/120    avg_loss:0.030, val_acc:0.959]
Epoch [120/120    avg_loss:0.043, val_acc:0.959]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1241    8   17    0    0    0    0    0    4   15    0    0
     0    0    0]
 [   0    0    3  682   39    0    0    0    0    1    2    2   18    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    6  428    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    4    0    0    0  652    0    0    0    1    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   18    0    0    0    0    0    0    0  822   17   18    0
     0    0    0]
 [   0    0    5    0    0    0    3    0    0    0   17 2140   44    0
     0    1    0]
 [   0    0    0    2    0    0    0    0    0    0    0    2  530    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
  1122   15    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    95  246    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
95.94579945799458

F1 scores:
[       nan 0.975      0.97104851 0.94788047 0.87295082 0.99188876
 0.98787879 1.         0.99883586 0.97297297 0.95414974 0.97560976
 0.92173913 1.         0.9524618  0.80788177 0.96932515]

Kappa:
0.9538027593737759
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6923555ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.813, val_acc:0.143]
Epoch [2/120    avg_loss:2.697, val_acc:0.276]
Epoch [3/120    avg_loss:2.604, val_acc:0.390]
Epoch [4/120    avg_loss:2.471, val_acc:0.464]
Epoch [5/120    avg_loss:2.349, val_acc:0.495]
Epoch [6/120    avg_loss:2.247, val_acc:0.572]
Epoch [7/120    avg_loss:2.141, val_acc:0.598]
Epoch [8/120    avg_loss:2.016, val_acc:0.574]
Epoch [9/120    avg_loss:1.876, val_acc:0.668]
Epoch [10/120    avg_loss:1.737, val_acc:0.689]
Epoch [11/120    avg_loss:1.566, val_acc:0.649]
Epoch [12/120    avg_loss:1.472, val_acc:0.680]
Epoch [13/120    avg_loss:1.272, val_acc:0.742]
Epoch [14/120    avg_loss:1.156, val_acc:0.791]
Epoch [15/120    avg_loss:0.966, val_acc:0.787]
Epoch [16/120    avg_loss:0.847, val_acc:0.815]
Epoch [17/120    avg_loss:0.755, val_acc:0.826]
Epoch [18/120    avg_loss:1.588, val_acc:0.700]
Epoch [19/120    avg_loss:1.042, val_acc:0.780]
Epoch [20/120    avg_loss:0.751, val_acc:0.838]
Epoch [21/120    avg_loss:0.609, val_acc:0.847]
Epoch [22/120    avg_loss:0.662, val_acc:0.808]
Epoch [23/120    avg_loss:0.535, val_acc:0.801]
Epoch [24/120    avg_loss:0.454, val_acc:0.834]
Epoch [25/120    avg_loss:0.366, val_acc:0.863]
Epoch [26/120    avg_loss:0.337, val_acc:0.898]
Epoch [27/120    avg_loss:0.303, val_acc:0.894]
Epoch [28/120    avg_loss:0.360, val_acc:0.864]
Epoch [29/120    avg_loss:0.372, val_acc:0.899]
Epoch [30/120    avg_loss:0.374, val_acc:0.876]
Epoch [31/120    avg_loss:0.231, val_acc:0.928]
Epoch [32/120    avg_loss:0.188, val_acc:0.922]
Epoch [33/120    avg_loss:0.225, val_acc:0.901]
Epoch [34/120    avg_loss:0.232, val_acc:0.932]
Epoch [35/120    avg_loss:0.164, val_acc:0.935]
Epoch [36/120    avg_loss:0.143, val_acc:0.950]
Epoch [37/120    avg_loss:0.104, val_acc:0.947]
Epoch [38/120    avg_loss:0.107, val_acc:0.925]
Epoch [39/120    avg_loss:0.105, val_acc:0.948]
Epoch [40/120    avg_loss:0.090, val_acc:0.944]
Epoch [41/120    avg_loss:0.086, val_acc:0.954]
Epoch [42/120    avg_loss:0.068, val_acc:0.944]
Epoch [43/120    avg_loss:0.101, val_acc:0.936]
Epoch [44/120    avg_loss:0.082, val_acc:0.926]
Epoch [45/120    avg_loss:0.153, val_acc:0.938]
Epoch [46/120    avg_loss:0.094, val_acc:0.965]
Epoch [47/120    avg_loss:0.086, val_acc:0.956]
Epoch [48/120    avg_loss:0.068, val_acc:0.966]
Epoch [49/120    avg_loss:0.057, val_acc:0.957]
Epoch [50/120    avg_loss:0.054, val_acc:0.970]
Epoch [51/120    avg_loss:0.048, val_acc:0.964]
Epoch [52/120    avg_loss:0.049, val_acc:0.977]
Epoch [53/120    avg_loss:0.071, val_acc:0.957]
Epoch [54/120    avg_loss:0.051, val_acc:0.975]
Epoch [55/120    avg_loss:0.057, val_acc:0.977]
Epoch [56/120    avg_loss:0.050, val_acc:0.972]
Epoch [57/120    avg_loss:0.069, val_acc:0.955]
Epoch [58/120    avg_loss:0.053, val_acc:0.971]
Epoch [59/120    avg_loss:0.039, val_acc:0.976]
Epoch [60/120    avg_loss:0.062, val_acc:0.961]
Epoch [61/120    avg_loss:0.040, val_acc:0.976]
Epoch [62/120    avg_loss:0.040, val_acc:0.980]
Epoch [63/120    avg_loss:0.028, val_acc:0.961]
Epoch [64/120    avg_loss:0.043, val_acc:0.976]
Epoch [65/120    avg_loss:0.042, val_acc:0.970]
Epoch [66/120    avg_loss:0.046, val_acc:0.965]
Epoch [67/120    avg_loss:0.031, val_acc:0.977]
Epoch [68/120    avg_loss:0.024, val_acc:0.977]
Epoch [69/120    avg_loss:0.042, val_acc:0.977]
Epoch [70/120    avg_loss:0.034, val_acc:0.983]
Epoch [71/120    avg_loss:0.023, val_acc:0.968]
Epoch [72/120    avg_loss:0.021, val_acc:0.981]
Epoch [73/120    avg_loss:0.016, val_acc:0.979]
Epoch [74/120    avg_loss:0.016, val_acc:0.979]
Epoch [75/120    avg_loss:0.014, val_acc:0.982]
Epoch [76/120    avg_loss:0.027, val_acc:0.982]
Epoch [77/120    avg_loss:0.016, val_acc:0.984]
Epoch [78/120    avg_loss:0.014, val_acc:0.984]
Epoch [79/120    avg_loss:0.021, val_acc:0.982]
Epoch [80/120    avg_loss:0.017, val_acc:0.975]
Epoch [81/120    avg_loss:0.037, val_acc:0.974]
Epoch [82/120    avg_loss:0.024, val_acc:0.979]
Epoch [83/120    avg_loss:0.017, val_acc:0.974]
Epoch [84/120    avg_loss:0.013, val_acc:0.970]
Epoch [85/120    avg_loss:0.018, val_acc:0.982]
Epoch [86/120    avg_loss:0.049, val_acc:0.972]
Epoch [87/120    avg_loss:0.043, val_acc:0.977]
Epoch [88/120    avg_loss:0.069, val_acc:0.981]
Epoch [89/120    avg_loss:0.050, val_acc:0.968]
Epoch [90/120    avg_loss:0.044, val_acc:0.969]
Epoch [91/120    avg_loss:0.094, val_acc:0.981]
Epoch [92/120    avg_loss:0.042, val_acc:0.981]
Epoch [93/120    avg_loss:0.029, val_acc:0.982]
Epoch [94/120    avg_loss:0.021, val_acc:0.981]
Epoch [95/120    avg_loss:0.026, val_acc:0.985]
Epoch [96/120    avg_loss:0.018, val_acc:0.982]
Epoch [97/120    avg_loss:0.017, val_acc:0.984]
Epoch [98/120    avg_loss:0.019, val_acc:0.985]
Epoch [99/120    avg_loss:0.017, val_acc:0.986]
Epoch [100/120    avg_loss:0.016, val_acc:0.988]
Epoch [101/120    avg_loss:0.013, val_acc:0.986]
Epoch [102/120    avg_loss:0.013, val_acc:0.985]
Epoch [103/120    avg_loss:0.015, val_acc:0.986]
Epoch [104/120    avg_loss:0.015, val_acc:0.988]
Epoch [105/120    avg_loss:0.013, val_acc:0.989]
Epoch [106/120    avg_loss:0.011, val_acc:0.988]
Epoch [107/120    avg_loss:0.012, val_acc:0.990]
Epoch [108/120    avg_loss:0.013, val_acc:0.991]
Epoch [109/120    avg_loss:0.009, val_acc:0.990]
Epoch [110/120    avg_loss:0.010, val_acc:0.988]
Epoch [111/120    avg_loss:0.010, val_acc:0.989]
Epoch [112/120    avg_loss:0.011, val_acc:0.989]
Epoch [113/120    avg_loss:0.012, val_acc:0.990]
Epoch [114/120    avg_loss:0.012, val_acc:0.990]
Epoch [115/120    avg_loss:0.010, val_acc:0.990]
Epoch [116/120    avg_loss:0.011, val_acc:0.989]
Epoch [117/120    avg_loss:0.011, val_acc:0.990]
Epoch [118/120    avg_loss:0.009, val_acc:0.988]
Epoch [119/120    avg_loss:0.010, val_acc:0.989]
Epoch [120/120    avg_loss:0.011, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1279    0    0    0    0    0    0    0    1    4    1    0
     0    0    0]
 [   0    0    0  734    1    2    0    0    0    5    1    0    2    1
     1    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0  845   29    0    0
     0    0    0]
 [   0    0   11    0    0    0    0    0    0    0   26 2154   18    0
     1    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0  528    0
     1    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1132    6    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    97  250    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.52845528455285

F1 scores:
[       nan 0.975      0.99301242 0.99122215 0.99530516 0.99192618
 0.99771167 1.         0.99883856 0.87804878 0.96516276 0.97931348
 0.97237569 0.99730458 0.95246109 0.82644628 0.97619048]

Kappa:
0.9718065465030126
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:06:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff761546eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.797, val_acc:0.224]
Epoch [2/120    avg_loss:2.721, val_acc:0.286]
Epoch [3/120    avg_loss:2.613, val_acc:0.360]
Epoch [4/120    avg_loss:2.473, val_acc:0.435]
Epoch [5/120    avg_loss:2.351, val_acc:0.458]
Epoch [6/120    avg_loss:2.173, val_acc:0.434]
Epoch [7/120    avg_loss:2.008, val_acc:0.579]
Epoch [8/120    avg_loss:1.835, val_acc:0.518]
Epoch [9/120    avg_loss:1.673, val_acc:0.627]
Epoch [10/120    avg_loss:1.520, val_acc:0.659]
Epoch [11/120    avg_loss:1.319, val_acc:0.665]
Epoch [12/120    avg_loss:1.102, val_acc:0.685]
Epoch [13/120    avg_loss:0.960, val_acc:0.759]
Epoch [14/120    avg_loss:0.830, val_acc:0.771]
Epoch [15/120    avg_loss:0.716, val_acc:0.761]
Epoch [16/120    avg_loss:0.627, val_acc:0.821]
Epoch [17/120    avg_loss:0.528, val_acc:0.806]
Epoch [18/120    avg_loss:0.497, val_acc:0.842]
Epoch [19/120    avg_loss:0.509, val_acc:0.834]
Epoch [20/120    avg_loss:0.418, val_acc:0.748]
Epoch [21/120    avg_loss:0.457, val_acc:0.845]
Epoch [22/120    avg_loss:0.432, val_acc:0.871]
Epoch [23/120    avg_loss:0.333, val_acc:0.904]
Epoch [24/120    avg_loss:0.231, val_acc:0.885]
Epoch [25/120    avg_loss:0.228, val_acc:0.887]
Epoch [26/120    avg_loss:0.213, val_acc:0.934]
Epoch [27/120    avg_loss:0.203, val_acc:0.921]
Epoch [28/120    avg_loss:0.148, val_acc:0.935]
Epoch [29/120    avg_loss:0.130, val_acc:0.947]
Epoch [30/120    avg_loss:0.108, val_acc:0.936]
Epoch [31/120    avg_loss:0.125, val_acc:0.909]
Epoch [32/120    avg_loss:0.109, val_acc:0.955]
Epoch [33/120    avg_loss:0.106, val_acc:0.961]
Epoch [34/120    avg_loss:0.107, val_acc:0.951]
Epoch [35/120    avg_loss:0.104, val_acc:0.950]
Epoch [36/120    avg_loss:0.086, val_acc:0.955]
Epoch [37/120    avg_loss:0.073, val_acc:0.959]
Epoch [38/120    avg_loss:0.087, val_acc:0.969]
Epoch [39/120    avg_loss:0.082, val_acc:0.958]
Epoch [40/120    avg_loss:0.063, val_acc:0.964]
Epoch [41/120    avg_loss:0.070, val_acc:0.960]
Epoch [42/120    avg_loss:0.073, val_acc:0.940]
Epoch [43/120    avg_loss:0.087, val_acc:0.957]
Epoch [44/120    avg_loss:0.063, val_acc:0.964]
Epoch [45/120    avg_loss:0.048, val_acc:0.957]
Epoch [46/120    avg_loss:0.053, val_acc:0.957]
Epoch [47/120    avg_loss:0.049, val_acc:0.956]
Epoch [48/120    avg_loss:0.039, val_acc:0.974]
Epoch [49/120    avg_loss:0.029, val_acc:0.975]
Epoch [50/120    avg_loss:0.027, val_acc:0.971]
Epoch [51/120    avg_loss:0.030, val_acc:0.978]
Epoch [52/120    avg_loss:0.039, val_acc:0.971]
Epoch [53/120    avg_loss:0.102, val_acc:0.963]
Epoch [54/120    avg_loss:1.377, val_acc:0.518]
Epoch [55/120    avg_loss:0.776, val_acc:0.878]
Epoch [56/120    avg_loss:0.177, val_acc:0.939]
Epoch [57/120    avg_loss:0.107, val_acc:0.944]
Epoch [58/120    avg_loss:0.158, val_acc:0.908]
Epoch [59/120    avg_loss:0.111, val_acc:0.958]
Epoch [60/120    avg_loss:0.063, val_acc:0.969]
Epoch [61/120    avg_loss:0.076, val_acc:0.955]
Epoch [62/120    avg_loss:1.825, val_acc:0.157]
Epoch [63/120    avg_loss:2.463, val_acc:0.289]
Epoch [64/120    avg_loss:2.235, val_acc:0.388]
Epoch [65/120    avg_loss:2.173, val_acc:0.390]
Epoch [66/120    avg_loss:2.138, val_acc:0.392]
Epoch [67/120    avg_loss:2.101, val_acc:0.395]
Epoch [68/120    avg_loss:2.107, val_acc:0.398]
Epoch [69/120    avg_loss:2.073, val_acc:0.399]
Epoch [70/120    avg_loss:2.109, val_acc:0.402]
Epoch [71/120    avg_loss:2.032, val_acc:0.403]
Epoch [72/120    avg_loss:2.067, val_acc:0.405]
Epoch [73/120    avg_loss:2.045, val_acc:0.404]
Epoch [74/120    avg_loss:2.033, val_acc:0.402]
Epoch [75/120    avg_loss:1.995, val_acc:0.405]
Epoch [76/120    avg_loss:1.994, val_acc:0.411]
Epoch [77/120    avg_loss:1.993, val_acc:0.410]
Epoch [78/120    avg_loss:2.023, val_acc:0.409]
Epoch [79/120    avg_loss:1.958, val_acc:0.412]
Epoch [80/120    avg_loss:1.961, val_acc:0.412]
Epoch [81/120    avg_loss:1.973, val_acc:0.412]
Epoch [82/120    avg_loss:1.983, val_acc:0.412]
Epoch [83/120    avg_loss:1.947, val_acc:0.415]
Epoch [84/120    avg_loss:1.947, val_acc:0.415]
Epoch [85/120    avg_loss:1.962, val_acc:0.417]
Epoch [86/120    avg_loss:1.978, val_acc:0.417]
Epoch [87/120    avg_loss:1.964, val_acc:0.417]
Epoch [88/120    avg_loss:1.964, val_acc:0.416]
Epoch [89/120    avg_loss:1.972, val_acc:0.416]
Epoch [90/120    avg_loss:1.952, val_acc:0.417]
Epoch [91/120    avg_loss:1.922, val_acc:0.417]
Epoch [92/120    avg_loss:1.955, val_acc:0.417]
Epoch [93/120    avg_loss:1.934, val_acc:0.417]
Epoch [94/120    avg_loss:1.941, val_acc:0.417]
Epoch [95/120    avg_loss:1.977, val_acc:0.417]
Epoch [96/120    avg_loss:1.965, val_acc:0.417]
Epoch [97/120    avg_loss:1.967, val_acc:0.417]
Epoch [98/120    avg_loss:1.972, val_acc:0.417]
Epoch [99/120    avg_loss:1.972, val_acc:0.417]
Epoch [100/120    avg_loss:1.959, val_acc:0.417]
Epoch [101/120    avg_loss:1.957, val_acc:0.417]
Epoch [102/120    avg_loss:1.945, val_acc:0.417]
Epoch [103/120    avg_loss:1.947, val_acc:0.417]
Epoch [104/120    avg_loss:1.990, val_acc:0.417]
Epoch [105/120    avg_loss:1.958, val_acc:0.417]
Epoch [106/120    avg_loss:1.932, val_acc:0.417]
Epoch [107/120    avg_loss:1.948, val_acc:0.417]
Epoch [108/120    avg_loss:1.991, val_acc:0.417]
Epoch [109/120    avg_loss:1.983, val_acc:0.417]
Epoch [110/120    avg_loss:1.965, val_acc:0.417]
Epoch [111/120    avg_loss:1.942, val_acc:0.417]
Epoch [112/120    avg_loss:1.961, val_acc:0.417]
Epoch [113/120    avg_loss:1.943, val_acc:0.417]
Epoch [114/120    avg_loss:1.985, val_acc:0.417]
Epoch [115/120    avg_loss:1.939, val_acc:0.417]
Epoch [116/120    avg_loss:1.965, val_acc:0.417]
Epoch [117/120    avg_loss:1.956, val_acc:0.417]
Epoch [118/120    avg_loss:1.959, val_acc:0.417]
Epoch [119/120    avg_loss:1.958, val_acc:0.417]
Epoch [120/120    avg_loss:1.947, val_acc:0.417]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   6   0   2   0   6   0  26   0   0   0   0   0   1   0   0]
 [  0   0 313   6 339   3  15   0   0   0  91 341 155  20   0   2   0]
 [  0   0  72  72  87   0  45   0   0   0 179 126 137  28   0   1   0]
 [  0   0  31  42 122   0   5   0   0   0   0   0  10   3   0   0   0]
 [  0   0  20   0   5 153  36   0   0   0   6  28  11  33 143   0   0]
 [  0   0   0  74   0   0 149   0   0   0 125  97   2 160  32  18   0]
 [  0   0   0   0   0   7  18   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0   7   0 414   0   0   0   0   6   0   0   0]
 [  0   0   0   0   0   0   6   0   0   0   0   0   0  12   0   0   0]
 [  0   0  28  12   0   0  36   0   0   0 420 230 106  38   0   5   0]
 [  0   0 125  35  43  26  14   0   0   0 739 860 243  53  11  10  51]
 [  0   0  90   8  82   0   8   0   0   0  85  26 188  10   0   6  31]
 [  0   0   0   0   0   0   5   0   0   0  19  20   0 141   0   0   0]
 [  0   0   0   0   0   9   9   0   0   0   0  27   0 114 980   0   0]
 [  0   0  20   5   0   0  31   0   0   0 114   0  16  58  86  17   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  84]]

Accuracy:
42.417344173441734

F1 scores:
[       nan 0.         0.31457286 0.14385614 0.27232143 0.48341232
 0.28462273 0.         0.95172414 0.         0.31662269 0.43379571
 0.2681883  0.32752613 0.81939799 0.08374384 0.672     ]

Kappa:
0.35389029618068485
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faed99aae80>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.812, val_acc:0.278]
Epoch [2/120    avg_loss:2.719, val_acc:0.400]
Epoch [3/120    avg_loss:2.619, val_acc:0.454]
Epoch [4/120    avg_loss:2.500, val_acc:0.466]
Epoch [5/120    avg_loss:2.372, val_acc:0.482]
Epoch [6/120    avg_loss:2.270, val_acc:0.483]
Epoch [7/120    avg_loss:2.143, val_acc:0.534]
Epoch [8/120    avg_loss:2.009, val_acc:0.562]
Epoch [9/120    avg_loss:1.883, val_acc:0.588]
Epoch [10/120    avg_loss:1.715, val_acc:0.633]
Epoch [11/120    avg_loss:1.513, val_acc:0.698]
Epoch [12/120    avg_loss:1.304, val_acc:0.694]
Epoch [13/120    avg_loss:1.192, val_acc:0.723]
Epoch [14/120    avg_loss:0.979, val_acc:0.755]
Epoch [15/120    avg_loss:0.873, val_acc:0.726]
Epoch [16/120    avg_loss:0.764, val_acc:0.796]
Epoch [17/120    avg_loss:0.709, val_acc:0.805]
Epoch [18/120    avg_loss:0.577, val_acc:0.794]
Epoch [19/120    avg_loss:0.527, val_acc:0.833]
Epoch [20/120    avg_loss:0.424, val_acc:0.839]
Epoch [21/120    avg_loss:0.425, val_acc:0.833]
Epoch [22/120    avg_loss:0.407, val_acc:0.838]
Epoch [23/120    avg_loss:0.474, val_acc:0.826]
Epoch [24/120    avg_loss:0.286, val_acc:0.895]
Epoch [25/120    avg_loss:0.287, val_acc:0.868]
Epoch [26/120    avg_loss:0.206, val_acc:0.895]
Epoch [27/120    avg_loss:0.194, val_acc:0.875]
Epoch [28/120    avg_loss:0.422, val_acc:0.755]
Epoch [29/120    avg_loss:0.264, val_acc:0.912]
Epoch [30/120    avg_loss:0.195, val_acc:0.904]
Epoch [31/120    avg_loss:0.158, val_acc:0.908]
Epoch [32/120    avg_loss:0.154, val_acc:0.915]
Epoch [33/120    avg_loss:0.139, val_acc:0.921]
Epoch [34/120    avg_loss:0.099, val_acc:0.928]
Epoch [35/120    avg_loss:0.100, val_acc:0.932]
Epoch [36/120    avg_loss:0.076, val_acc:0.923]
Epoch [37/120    avg_loss:0.110, val_acc:0.911]
Epoch [38/120    avg_loss:0.085, val_acc:0.936]
Epoch [39/120    avg_loss:0.074, val_acc:0.944]
Epoch [40/120    avg_loss:0.066, val_acc:0.920]
Epoch [41/120    avg_loss:0.066, val_acc:0.926]
Epoch [42/120    avg_loss:0.060, val_acc:0.926]
Epoch [43/120    avg_loss:0.061, val_acc:0.933]
Epoch [44/120    avg_loss:0.063, val_acc:0.945]
Epoch [45/120    avg_loss:0.076, val_acc:0.941]
Epoch [46/120    avg_loss:0.053, val_acc:0.955]
Epoch [47/120    avg_loss:0.045, val_acc:0.956]
Epoch [48/120    avg_loss:0.040, val_acc:0.947]
Epoch [49/120    avg_loss:0.037, val_acc:0.948]
Epoch [50/120    avg_loss:0.037, val_acc:0.955]
Epoch [51/120    avg_loss:0.050, val_acc:0.943]
Epoch [52/120    avg_loss:0.038, val_acc:0.910]
Epoch [53/120    avg_loss:0.044, val_acc:0.940]
Epoch [54/120    avg_loss:0.029, val_acc:0.963]
Epoch [55/120    avg_loss:0.030, val_acc:0.965]
Epoch [56/120    avg_loss:0.027, val_acc:0.966]
Epoch [57/120    avg_loss:0.034, val_acc:0.959]
Epoch [58/120    avg_loss:0.026, val_acc:0.966]
Epoch [59/120    avg_loss:0.024, val_acc:0.957]
Epoch [60/120    avg_loss:0.025, val_acc:0.960]
Epoch [61/120    avg_loss:0.032, val_acc:0.969]
Epoch [62/120    avg_loss:0.024, val_acc:0.972]
Epoch [63/120    avg_loss:0.022, val_acc:0.958]
Epoch [64/120    avg_loss:0.032, val_acc:0.961]
Epoch [65/120    avg_loss:0.031, val_acc:0.957]
Epoch [66/120    avg_loss:0.018, val_acc:0.966]
Epoch [67/120    avg_loss:0.015, val_acc:0.969]
Epoch [68/120    avg_loss:0.035, val_acc:0.957]
Epoch [69/120    avg_loss:0.015, val_acc:0.964]
Epoch [70/120    avg_loss:0.013, val_acc:0.969]
Epoch [71/120    avg_loss:0.027, val_acc:0.966]
Epoch [72/120    avg_loss:0.032, val_acc:0.965]
Epoch [73/120    avg_loss:0.016, val_acc:0.969]
Epoch [74/120    avg_loss:0.013, val_acc:0.970]
Epoch [75/120    avg_loss:0.017, val_acc:0.967]
Epoch [76/120    avg_loss:0.011, val_acc:0.969]
Epoch [77/120    avg_loss:0.010, val_acc:0.968]
Epoch [78/120    avg_loss:0.011, val_acc:0.968]
Epoch [79/120    avg_loss:0.011, val_acc:0.969]
Epoch [80/120    avg_loss:0.008, val_acc:0.966]
Epoch [81/120    avg_loss:0.009, val_acc:0.968]
Epoch [82/120    avg_loss:0.009, val_acc:0.968]
Epoch [83/120    avg_loss:0.008, val_acc:0.970]
Epoch [84/120    avg_loss:0.009, val_acc:0.971]
Epoch [85/120    avg_loss:0.009, val_acc:0.966]
Epoch [86/120    avg_loss:0.008, val_acc:0.967]
Epoch [87/120    avg_loss:0.010, val_acc:0.968]
Epoch [88/120    avg_loss:0.009, val_acc:0.969]
Epoch [89/120    avg_loss:0.007, val_acc:0.969]
Epoch [90/120    avg_loss:0.009, val_acc:0.968]
Epoch [91/120    avg_loss:0.010, val_acc:0.968]
Epoch [92/120    avg_loss:0.008, val_acc:0.968]
Epoch [93/120    avg_loss:0.009, val_acc:0.967]
Epoch [94/120    avg_loss:0.008, val_acc:0.968]
Epoch [95/120    avg_loss:0.008, val_acc:0.967]
Epoch [96/120    avg_loss:0.008, val_acc:0.968]
Epoch [97/120    avg_loss:0.007, val_acc:0.967]
Epoch [98/120    avg_loss:0.008, val_acc:0.967]
Epoch [99/120    avg_loss:0.008, val_acc:0.967]
Epoch [100/120    avg_loss:0.008, val_acc:0.967]
Epoch [101/120    avg_loss:0.007, val_acc:0.967]
Epoch [102/120    avg_loss:0.007, val_acc:0.967]
Epoch [103/120    avg_loss:0.008, val_acc:0.967]
Epoch [104/120    avg_loss:0.008, val_acc:0.967]
Epoch [105/120    avg_loss:0.008, val_acc:0.967]
Epoch [106/120    avg_loss:0.008, val_acc:0.967]
Epoch [107/120    avg_loss:0.008, val_acc:0.967]
Epoch [108/120    avg_loss:0.007, val_acc:0.967]
Epoch [109/120    avg_loss:0.009, val_acc:0.967]
Epoch [110/120    avg_loss:0.010, val_acc:0.967]
Epoch [111/120    avg_loss:0.007, val_acc:0.967]
Epoch [112/120    avg_loss:0.008, val_acc:0.967]
Epoch [113/120    avg_loss:0.008, val_acc:0.968]
Epoch [114/120    avg_loss:0.009, val_acc:0.967]
Epoch [115/120    avg_loss:0.009, val_acc:0.967]
Epoch [116/120    avg_loss:0.008, val_acc:0.967]
Epoch [117/120    avg_loss:0.010, val_acc:0.967]
Epoch [118/120    avg_loss:0.008, val_acc:0.967]
Epoch [119/120    avg_loss:0.009, val_acc:0.968]
Epoch [120/120    avg_loss:0.008, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1252    0   12    0    0    0    0    0    4   17    0    0
     0    0    0]
 [   0    0    0  721   10    0    0    0    0    1    5    8    2    0
     0    0    0]
 [   0    0    0    3  209    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    6    0    0    1    0    0    0    0  828   40    0    0
     0    0    0]
 [   0    0   13    1    0    0    0    0    0    0   13 2171   12    0
     0    0    0]
 [   0    0    1    1    0    0    0    0    0    0   10    3  515    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    95  246    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.84552845528455

F1 scores:
[       nan 0.95348837 0.97927259 0.97895451 0.94144144 0.99423299
 0.99545455 1.         0.9953271  0.94444444 0.95446686 0.97551112
 0.96804511 1.         0.95302581 0.80921053 0.98823529]

Kappa:
0.9639913383640767
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe940bc2ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.800, val_acc:0.156]
Epoch [2/120    avg_loss:2.726, val_acc:0.301]
Epoch [3/120    avg_loss:2.630, val_acc:0.426]
Epoch [4/120    avg_loss:2.513, val_acc:0.502]
Epoch [5/120    avg_loss:2.347, val_acc:0.557]
Epoch [6/120    avg_loss:2.217, val_acc:0.590]
Epoch [7/120    avg_loss:2.039, val_acc:0.625]
Epoch [8/120    avg_loss:1.895, val_acc:0.660]
Epoch [9/120    avg_loss:1.708, val_acc:0.661]
Epoch [10/120    avg_loss:1.504, val_acc:0.711]
Epoch [11/120    avg_loss:1.335, val_acc:0.718]
Epoch [12/120    avg_loss:1.175, val_acc:0.718]
Epoch [13/120    avg_loss:0.985, val_acc:0.749]
Epoch [14/120    avg_loss:0.909, val_acc:0.764]
Epoch [15/120    avg_loss:0.842, val_acc:0.779]
Epoch [16/120    avg_loss:0.702, val_acc:0.770]
Epoch [17/120    avg_loss:0.621, val_acc:0.799]
Epoch [18/120    avg_loss:0.569, val_acc:0.795]
Epoch [19/120    avg_loss:0.441, val_acc:0.814]
Epoch [20/120    avg_loss:0.408, val_acc:0.835]
Epoch [21/120    avg_loss:0.484, val_acc:0.839]
Epoch [22/120    avg_loss:0.404, val_acc:0.850]
Epoch [23/120    avg_loss:0.303, val_acc:0.883]
Epoch [24/120    avg_loss:0.287, val_acc:0.889]
Epoch [25/120    avg_loss:0.257, val_acc:0.907]
Epoch [26/120    avg_loss:0.218, val_acc:0.912]
Epoch [27/120    avg_loss:0.219, val_acc:0.893]
Epoch [28/120    avg_loss:0.179, val_acc:0.923]
Epoch [29/120    avg_loss:0.171, val_acc:0.902]
Epoch [30/120    avg_loss:0.170, val_acc:0.933]
Epoch [31/120    avg_loss:0.137, val_acc:0.941]
Epoch [32/120    avg_loss:0.136, val_acc:0.932]
Epoch [33/120    avg_loss:0.132, val_acc:0.917]
Epoch [34/120    avg_loss:0.110, val_acc:0.897]
Epoch [35/120    avg_loss:0.141, val_acc:0.927]
Epoch [36/120    avg_loss:0.144, val_acc:0.917]
Epoch [37/120    avg_loss:0.091, val_acc:0.930]
Epoch [38/120    avg_loss:0.089, val_acc:0.955]
Epoch [39/120    avg_loss:0.100, val_acc:0.926]
Epoch [40/120    avg_loss:0.074, val_acc:0.944]
Epoch [41/120    avg_loss:0.070, val_acc:0.956]
Epoch [42/120    avg_loss:0.068, val_acc:0.948]
Epoch [43/120    avg_loss:0.059, val_acc:0.966]
Epoch [44/120    avg_loss:0.067, val_acc:0.961]
Epoch [45/120    avg_loss:0.056, val_acc:0.948]
Epoch [46/120    avg_loss:0.057, val_acc:0.953]
Epoch [47/120    avg_loss:0.072, val_acc:0.958]
Epoch [48/120    avg_loss:0.049, val_acc:0.958]
Epoch [49/120    avg_loss:0.056, val_acc:0.949]
Epoch [50/120    avg_loss:0.080, val_acc:0.950]
Epoch [51/120    avg_loss:0.067, val_acc:0.951]
Epoch [52/120    avg_loss:0.041, val_acc:0.956]
Epoch [53/120    avg_loss:0.032, val_acc:0.965]
Epoch [54/120    avg_loss:0.028, val_acc:0.957]
Epoch [55/120    avg_loss:0.032, val_acc:0.957]
Epoch [56/120    avg_loss:0.053, val_acc:0.946]
Epoch [57/120    avg_loss:0.039, val_acc:0.966]
Epoch [58/120    avg_loss:0.024, val_acc:0.965]
Epoch [59/120    avg_loss:0.025, val_acc:0.969]
Epoch [60/120    avg_loss:0.021, val_acc:0.966]
Epoch [61/120    avg_loss:0.020, val_acc:0.963]
Epoch [62/120    avg_loss:0.022, val_acc:0.964]
Epoch [63/120    avg_loss:0.019, val_acc:0.965]
Epoch [64/120    avg_loss:0.024, val_acc:0.967]
Epoch [65/120    avg_loss:0.016, val_acc:0.966]
Epoch [66/120    avg_loss:0.022, val_acc:0.964]
Epoch [67/120    avg_loss:0.022, val_acc:0.966]
Epoch [68/120    avg_loss:0.016, val_acc:0.966]
Epoch [69/120    avg_loss:0.017, val_acc:0.966]
Epoch [70/120    avg_loss:0.018, val_acc:0.967]
Epoch [71/120    avg_loss:0.017, val_acc:0.966]
Epoch [72/120    avg_loss:0.022, val_acc:0.967]
Epoch [73/120    avg_loss:0.018, val_acc:0.966]
Epoch [74/120    avg_loss:0.024, val_acc:0.966]
Epoch [75/120    avg_loss:0.017, val_acc:0.967]
Epoch [76/120    avg_loss:0.020, val_acc:0.967]
Epoch [77/120    avg_loss:0.018, val_acc:0.965]
Epoch [78/120    avg_loss:0.015, val_acc:0.966]
Epoch [79/120    avg_loss:0.017, val_acc:0.966]
Epoch [80/120    avg_loss:0.015, val_acc:0.966]
Epoch [81/120    avg_loss:0.014, val_acc:0.966]
Epoch [82/120    avg_loss:0.017, val_acc:0.966]
Epoch [83/120    avg_loss:0.017, val_acc:0.965]
Epoch [84/120    avg_loss:0.017, val_acc:0.965]
Epoch [85/120    avg_loss:0.018, val_acc:0.965]
Epoch [86/120    avg_loss:0.018, val_acc:0.965]
Epoch [87/120    avg_loss:0.017, val_acc:0.965]
Epoch [88/120    avg_loss:0.018, val_acc:0.965]
Epoch [89/120    avg_loss:0.015, val_acc:0.965]
Epoch [90/120    avg_loss:0.016, val_acc:0.965]
Epoch [91/120    avg_loss:0.016, val_acc:0.965]
Epoch [92/120    avg_loss:0.016, val_acc:0.965]
Epoch [93/120    avg_loss:0.017, val_acc:0.965]
Epoch [94/120    avg_loss:0.022, val_acc:0.965]
Epoch [95/120    avg_loss:0.016, val_acc:0.966]
Epoch [96/120    avg_loss:0.018, val_acc:0.966]
Epoch [97/120    avg_loss:0.014, val_acc:0.966]
Epoch [98/120    avg_loss:0.018, val_acc:0.966]
Epoch [99/120    avg_loss:0.016, val_acc:0.966]
Epoch [100/120    avg_loss:0.018, val_acc:0.966]
Epoch [101/120    avg_loss:0.018, val_acc:0.966]
Epoch [102/120    avg_loss:0.014, val_acc:0.966]
Epoch [103/120    avg_loss:0.016, val_acc:0.966]
Epoch [104/120    avg_loss:0.014, val_acc:0.966]
Epoch [105/120    avg_loss:0.017, val_acc:0.966]
Epoch [106/120    avg_loss:0.017, val_acc:0.966]
Epoch [107/120    avg_loss:0.015, val_acc:0.966]
Epoch [108/120    avg_loss:0.017, val_acc:0.966]
Epoch [109/120    avg_loss:0.015, val_acc:0.965]
Epoch [110/120    avg_loss:0.016, val_acc:0.966]
Epoch [111/120    avg_loss:0.016, val_acc:0.966]
Epoch [112/120    avg_loss:0.017, val_acc:0.966]
Epoch [113/120    avg_loss:0.015, val_acc:0.966]
Epoch [114/120    avg_loss:0.016, val_acc:0.966]
Epoch [115/120    avg_loss:0.017, val_acc:0.966]
Epoch [116/120    avg_loss:0.019, val_acc:0.966]
Epoch [117/120    avg_loss:0.016, val_acc:0.966]
Epoch [118/120    avg_loss:0.016, val_acc:0.965]
Epoch [119/120    avg_loss:0.017, val_acc:0.965]
Epoch [120/120    avg_loss:0.016, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1255   11    2    0    0    0    0    0    6   11    0    0
     0    0    0]
 [   0    0    5  734    0    0    0    0    0    2    1    2    2    0
     0    1    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    0    0    0
     5    1    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   11    0    0    1    0    0    0    0  838   25    0    0
     0    0    0]
 [   0    0    4    0    0    0    3    0    0    1   20 2162   19    0
     0    1    0]
 [   0    0    0    3    0    0    0    0    0    0    1    0  527    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1125   13    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    79  254    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.20325203252033

F1 scores:
[       nan 0.975      0.98046875 0.98128342 0.99297424 0.99190751
 0.98419865 1.         1.         0.92307692 0.96156053 0.9800544
 0.97053407 1.         0.95785441 0.82067851 0.96969697]

Kappa:
0.9680987600891918
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7b752a3e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.806, val_acc:0.188]
Epoch [2/120    avg_loss:2.725, val_acc:0.307]
Epoch [3/120    avg_loss:2.640, val_acc:0.425]
Epoch [4/120    avg_loss:2.525, val_acc:0.536]
Epoch [5/120    avg_loss:2.427, val_acc:0.545]
Epoch [6/120    avg_loss:2.313, val_acc:0.528]
Epoch [7/120    avg_loss:2.215, val_acc:0.599]
Epoch [8/120    avg_loss:2.080, val_acc:0.614]
Epoch [9/120    avg_loss:1.942, val_acc:0.643]
Epoch [10/120    avg_loss:1.825, val_acc:0.649]
Epoch [11/120    avg_loss:1.629, val_acc:0.700]
Epoch [12/120    avg_loss:1.510, val_acc:0.682]
Epoch [13/120    avg_loss:1.308, val_acc:0.708]
Epoch [14/120    avg_loss:1.088, val_acc:0.714]
Epoch [15/120    avg_loss:0.967, val_acc:0.762]
Epoch [16/120    avg_loss:0.828, val_acc:0.734]
Epoch [17/120    avg_loss:0.734, val_acc:0.820]
Epoch [18/120    avg_loss:0.696, val_acc:0.816]
Epoch [19/120    avg_loss:0.547, val_acc:0.815]
Epoch [20/120    avg_loss:0.482, val_acc:0.839]
Epoch [21/120    avg_loss:0.389, val_acc:0.879]
Epoch [22/120    avg_loss:0.322, val_acc:0.880]
Epoch [23/120    avg_loss:0.305, val_acc:0.891]
Epoch [24/120    avg_loss:0.308, val_acc:0.886]
Epoch [25/120    avg_loss:0.254, val_acc:0.898]
Epoch [26/120    avg_loss:0.219, val_acc:0.909]
Epoch [27/120    avg_loss:0.243, val_acc:0.922]
Epoch [28/120    avg_loss:0.158, val_acc:0.917]
Epoch [29/120    avg_loss:0.167, val_acc:0.923]
Epoch [30/120    avg_loss:0.132, val_acc:0.945]
Epoch [31/120    avg_loss:0.159, val_acc:0.923]
Epoch [32/120    avg_loss:0.124, val_acc:0.925]
Epoch [33/120    avg_loss:0.129, val_acc:0.941]
Epoch [34/120    avg_loss:0.101, val_acc:0.936]
Epoch [35/120    avg_loss:0.091, val_acc:0.941]
Epoch [36/120    avg_loss:0.117, val_acc:0.921]
Epoch [37/120    avg_loss:0.086, val_acc:0.949]
Epoch [38/120    avg_loss:0.070, val_acc:0.936]
Epoch [39/120    avg_loss:0.132, val_acc:0.924]
Epoch [40/120    avg_loss:0.088, val_acc:0.946]
Epoch [41/120    avg_loss:0.058, val_acc:0.952]
Epoch [42/120    avg_loss:0.051, val_acc:0.961]
Epoch [43/120    avg_loss:0.074, val_acc:0.957]
Epoch [44/120    avg_loss:0.051, val_acc:0.950]
Epoch [45/120    avg_loss:0.040, val_acc:0.957]
Epoch [46/120    avg_loss:0.033, val_acc:0.970]
Epoch [47/120    avg_loss:0.217, val_acc:0.929]
Epoch [48/120    avg_loss:0.174, val_acc:0.933]
Epoch [49/120    avg_loss:0.077, val_acc:0.949]
Epoch [50/120    avg_loss:0.050, val_acc:0.949]
Epoch [51/120    avg_loss:0.083, val_acc:0.920]
Epoch [52/120    avg_loss:0.178, val_acc:0.963]
Epoch [53/120    avg_loss:0.061, val_acc:0.972]
Epoch [54/120    avg_loss:0.077, val_acc:0.961]
Epoch [55/120    avg_loss:0.040, val_acc:0.968]
Epoch [56/120    avg_loss:0.028, val_acc:0.964]
Epoch [57/120    avg_loss:0.033, val_acc:0.969]
Epoch [58/120    avg_loss:0.025, val_acc:0.966]
Epoch [59/120    avg_loss:0.022, val_acc:0.978]
Epoch [60/120    avg_loss:0.023, val_acc:0.971]
Epoch [61/120    avg_loss:0.032, val_acc:0.966]
Epoch [62/120    avg_loss:0.023, val_acc:0.961]
Epoch [63/120    avg_loss:0.032, val_acc:0.965]
Epoch [64/120    avg_loss:0.025, val_acc:0.956]
Epoch [65/120    avg_loss:0.028, val_acc:0.968]
Epoch [66/120    avg_loss:0.018, val_acc:0.970]
Epoch [67/120    avg_loss:0.024, val_acc:0.945]
Epoch [68/120    avg_loss:0.019, val_acc:0.973]
Epoch [69/120    avg_loss:0.026, val_acc:0.971]
Epoch [70/120    avg_loss:0.048, val_acc:0.941]
Epoch [71/120    avg_loss:0.093, val_acc:0.966]
Epoch [72/120    avg_loss:0.139, val_acc:0.869]
Epoch [73/120    avg_loss:0.173, val_acc:0.943]
Epoch [74/120    avg_loss:0.071, val_acc:0.956]
Epoch [75/120    avg_loss:0.047, val_acc:0.960]
Epoch [76/120    avg_loss:0.046, val_acc:0.960]
Epoch [77/120    avg_loss:0.040, val_acc:0.963]
Epoch [78/120    avg_loss:0.036, val_acc:0.963]
Epoch [79/120    avg_loss:0.029, val_acc:0.963]
Epoch [80/120    avg_loss:0.025, val_acc:0.967]
Epoch [81/120    avg_loss:0.024, val_acc:0.966]
Epoch [82/120    avg_loss:0.025, val_acc:0.967]
Epoch [83/120    avg_loss:0.022, val_acc:0.967]
Epoch [84/120    avg_loss:0.021, val_acc:0.969]
Epoch [85/120    avg_loss:0.021, val_acc:0.967]
Epoch [86/120    avg_loss:0.018, val_acc:0.967]
Epoch [87/120    avg_loss:0.020, val_acc:0.967]
Epoch [88/120    avg_loss:0.020, val_acc:0.967]
Epoch [89/120    avg_loss:0.024, val_acc:0.967]
Epoch [90/120    avg_loss:0.018, val_acc:0.968]
Epoch [91/120    avg_loss:0.019, val_acc:0.968]
Epoch [92/120    avg_loss:0.019, val_acc:0.969]
Epoch [93/120    avg_loss:0.020, val_acc:0.969]
Epoch [94/120    avg_loss:0.022, val_acc:0.969]
Epoch [95/120    avg_loss:0.024, val_acc:0.969]
Epoch [96/120    avg_loss:0.019, val_acc:0.968]
Epoch [97/120    avg_loss:0.024, val_acc:0.967]
Epoch [98/120    avg_loss:0.019, val_acc:0.967]
Epoch [99/120    avg_loss:0.018, val_acc:0.967]
Epoch [100/120    avg_loss:0.023, val_acc:0.967]
Epoch [101/120    avg_loss:0.022, val_acc:0.967]
Epoch [102/120    avg_loss:0.021, val_acc:0.967]
Epoch [103/120    avg_loss:0.020, val_acc:0.967]
Epoch [104/120    avg_loss:0.019, val_acc:0.967]
Epoch [105/120    avg_loss:0.023, val_acc:0.967]
Epoch [106/120    avg_loss:0.021, val_acc:0.967]
Epoch [107/120    avg_loss:0.021, val_acc:0.967]
Epoch [108/120    avg_loss:0.019, val_acc:0.967]
Epoch [109/120    avg_loss:0.022, val_acc:0.967]
Epoch [110/120    avg_loss:0.025, val_acc:0.967]
Epoch [111/120    avg_loss:0.023, val_acc:0.967]
Epoch [112/120    avg_loss:0.019, val_acc:0.967]
Epoch [113/120    avg_loss:0.021, val_acc:0.967]
Epoch [114/120    avg_loss:0.029, val_acc:0.967]
Epoch [115/120    avg_loss:0.020, val_acc:0.967]
Epoch [116/120    avg_loss:0.023, val_acc:0.967]
Epoch [117/120    avg_loss:0.031, val_acc:0.967]
Epoch [118/120    avg_loss:0.022, val_acc:0.967]
Epoch [119/120    avg_loss:0.022, val_acc:0.967]
Epoch [120/120    avg_loss:0.022, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1242    7    5    3    0    0    0    0    3   24    1    0
     0    0    0]
 [   0    0    2  723    3    0    0    0    0    5    3    5    6    0
     0    0    0]
 [   0    0    0    0  205    0    1    0    0    0    0    1    6    0
     0    0    0]
 [   0    0    0    0    0  427    1    0    0    0    1    2    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    1    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    3    0    0    0  838   30    0    0
     0    0    0]
 [   0    0    6    0    0    2    2    0    0    0   10 2170   19    0
     1    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0    8  520    0
     0    3    1]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1121   17    0]
 [   0    0    0    0    0    0   18    0    0    0    0    0    0    0
    82  247    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.80216802168022

F1 scores:
[       nan 1.         0.97833793 0.97768763 0.96244131 0.98387097
 0.9798357  1.         0.99883586 0.85714286 0.96878613 0.97506178
 0.95764273 0.99728997 0.95526204 0.80456026 0.99408284]

Kappa:
0.9635048761615895
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f08433daf60>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.820, val_acc:0.117]
Epoch [2/120    avg_loss:2.746, val_acc:0.201]
Epoch [3/120    avg_loss:2.650, val_acc:0.334]
Epoch [4/120    avg_loss:2.538, val_acc:0.376]
Epoch [5/120    avg_loss:2.440, val_acc:0.393]
Epoch [6/120    avg_loss:2.313, val_acc:0.383]
Epoch [7/120    avg_loss:2.174, val_acc:0.467]
Epoch [8/120    avg_loss:2.087, val_acc:0.457]
Epoch [9/120    avg_loss:1.965, val_acc:0.551]
Epoch [10/120    avg_loss:1.848, val_acc:0.526]
Epoch [11/120    avg_loss:1.700, val_acc:0.637]
Epoch [12/120    avg_loss:1.576, val_acc:0.663]
Epoch [13/120    avg_loss:1.353, val_acc:0.665]
Epoch [14/120    avg_loss:1.237, val_acc:0.766]
Epoch [15/120    avg_loss:1.094, val_acc:0.752]
Epoch [16/120    avg_loss:0.975, val_acc:0.785]
Epoch [17/120    avg_loss:0.845, val_acc:0.752]
Epoch [18/120    avg_loss:0.753, val_acc:0.781]
Epoch [19/120    avg_loss:0.645, val_acc:0.835]
Epoch [20/120    avg_loss:0.606, val_acc:0.822]
Epoch [21/120    avg_loss:0.525, val_acc:0.841]
Epoch [22/120    avg_loss:0.464, val_acc:0.871]
Epoch [23/120    avg_loss:1.085, val_acc:0.655]
Epoch [24/120    avg_loss:0.857, val_acc:0.817]
Epoch [25/120    avg_loss:0.516, val_acc:0.838]
Epoch [26/120    avg_loss:0.373, val_acc:0.883]
Epoch [27/120    avg_loss:0.295, val_acc:0.855]
Epoch [28/120    avg_loss:0.437, val_acc:0.788]
Epoch [29/120    avg_loss:0.377, val_acc:0.863]
Epoch [30/120    avg_loss:0.301, val_acc:0.894]
Epoch [31/120    avg_loss:0.296, val_acc:0.880]
Epoch [32/120    avg_loss:0.232, val_acc:0.919]
Epoch [33/120    avg_loss:0.178, val_acc:0.875]
Epoch [34/120    avg_loss:0.155, val_acc:0.923]
Epoch [35/120    avg_loss:0.132, val_acc:0.934]
Epoch [36/120    avg_loss:0.129, val_acc:0.947]
Epoch [37/120    avg_loss:0.115, val_acc:0.943]
Epoch [38/120    avg_loss:0.134, val_acc:0.936]
Epoch [39/120    avg_loss:0.110, val_acc:0.944]
Epoch [40/120    avg_loss:0.110, val_acc:0.949]
Epoch [41/120    avg_loss:0.122, val_acc:0.878]
Epoch [42/120    avg_loss:0.103, val_acc:0.938]
Epoch [43/120    avg_loss:0.071, val_acc:0.926]
Epoch [44/120    avg_loss:0.081, val_acc:0.955]
Epoch [45/120    avg_loss:0.078, val_acc:0.927]
Epoch [46/120    avg_loss:0.085, val_acc:0.941]
Epoch [47/120    avg_loss:0.077, val_acc:0.945]
Epoch [48/120    avg_loss:0.070, val_acc:0.946]
Epoch [49/120    avg_loss:0.050, val_acc:0.965]
Epoch [50/120    avg_loss:0.051, val_acc:0.963]
Epoch [51/120    avg_loss:0.056, val_acc:0.940]
Epoch [52/120    avg_loss:0.064, val_acc:0.961]
Epoch [53/120    avg_loss:0.055, val_acc:0.958]
Epoch [54/120    avg_loss:0.045, val_acc:0.957]
Epoch [55/120    avg_loss:0.052, val_acc:0.946]
Epoch [56/120    avg_loss:0.051, val_acc:0.947]
Epoch [57/120    avg_loss:0.046, val_acc:0.946]
Epoch [58/120    avg_loss:0.044, val_acc:0.955]
Epoch [59/120    avg_loss:0.037, val_acc:0.968]
Epoch [60/120    avg_loss:0.026, val_acc:0.965]
Epoch [61/120    avg_loss:0.031, val_acc:0.951]
Epoch [62/120    avg_loss:0.099, val_acc:0.931]
Epoch [63/120    avg_loss:0.068, val_acc:0.923]
Epoch [64/120    avg_loss:0.054, val_acc:0.953]
Epoch [65/120    avg_loss:0.048, val_acc:0.965]
Epoch [66/120    avg_loss:0.041, val_acc:0.949]
Epoch [67/120    avg_loss:0.040, val_acc:0.960]
Epoch [68/120    avg_loss:0.023, val_acc:0.963]
Epoch [69/120    avg_loss:0.023, val_acc:0.956]
Epoch [70/120    avg_loss:0.042, val_acc:0.944]
Epoch [71/120    avg_loss:0.024, val_acc:0.963]
Epoch [72/120    avg_loss:0.024, val_acc:0.966]
Epoch [73/120    avg_loss:0.017, val_acc:0.967]
Epoch [74/120    avg_loss:0.014, val_acc:0.971]
Epoch [75/120    avg_loss:0.013, val_acc:0.973]
Epoch [76/120    avg_loss:0.017, val_acc:0.972]
Epoch [77/120    avg_loss:0.014, val_acc:0.971]
Epoch [78/120    avg_loss:0.013, val_acc:0.970]
Epoch [79/120    avg_loss:0.014, val_acc:0.971]
Epoch [80/120    avg_loss:0.014, val_acc:0.968]
Epoch [81/120    avg_loss:0.014, val_acc:0.970]
Epoch [82/120    avg_loss:0.013, val_acc:0.970]
Epoch [83/120    avg_loss:0.013, val_acc:0.970]
Epoch [84/120    avg_loss:0.012, val_acc:0.969]
Epoch [85/120    avg_loss:0.011, val_acc:0.969]
Epoch [86/120    avg_loss:0.011, val_acc:0.968]
Epoch [87/120    avg_loss:0.011, val_acc:0.969]
Epoch [88/120    avg_loss:0.017, val_acc:0.968]
Epoch [89/120    avg_loss:0.013, val_acc:0.968]
Epoch [90/120    avg_loss:0.015, val_acc:0.968]
Epoch [91/120    avg_loss:0.012, val_acc:0.968]
Epoch [92/120    avg_loss:0.010, val_acc:0.968]
Epoch [93/120    avg_loss:0.011, val_acc:0.968]
Epoch [94/120    avg_loss:0.013, val_acc:0.968]
Epoch [95/120    avg_loss:0.011, val_acc:0.968]
Epoch [96/120    avg_loss:0.012, val_acc:0.968]
Epoch [97/120    avg_loss:0.012, val_acc:0.968]
Epoch [98/120    avg_loss:0.014, val_acc:0.968]
Epoch [99/120    avg_loss:0.012, val_acc:0.967]
Epoch [100/120    avg_loss:0.014, val_acc:0.967]
Epoch [101/120    avg_loss:0.012, val_acc:0.967]
Epoch [102/120    avg_loss:0.011, val_acc:0.968]
Epoch [103/120    avg_loss:0.013, val_acc:0.967]
Epoch [104/120    avg_loss:0.014, val_acc:0.967]
Epoch [105/120    avg_loss:0.012, val_acc:0.967]
Epoch [106/120    avg_loss:0.012, val_acc:0.967]
Epoch [107/120    avg_loss:0.011, val_acc:0.967]
Epoch [108/120    avg_loss:0.013, val_acc:0.967]
Epoch [109/120    avg_loss:0.011, val_acc:0.967]
Epoch [110/120    avg_loss:0.013, val_acc:0.967]
Epoch [111/120    avg_loss:0.012, val_acc:0.967]
Epoch [112/120    avg_loss:0.011, val_acc:0.967]
Epoch [113/120    avg_loss:0.011, val_acc:0.967]
Epoch [114/120    avg_loss:0.011, val_acc:0.967]
Epoch [115/120    avg_loss:0.011, val_acc:0.967]
Epoch [116/120    avg_loss:0.012, val_acc:0.967]
Epoch [117/120    avg_loss:0.012, val_acc:0.967]
Epoch [118/120    avg_loss:0.011, val_acc:0.967]
Epoch [119/120    avg_loss:0.013, val_acc:0.967]
Epoch [120/120    avg_loss:0.011, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    2    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1236   10    3    0    0    0    0    0    9   27    0    0
     0    0    0]
 [   0    0    2  727    6    0    1    0    0    1    0    2    8    0
     0    0    0]
 [   0    0    0    0  211    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  433    1    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0  852   20    0    0
     1    0    0]
 [   0    0    9    1    0    0    0    0    0    0   12 2166   21    0
     1    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  532    0
     0    2    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
    88  258    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.30081300813008

F1 scores:
[       nan 0.975      0.97553275 0.97912458 0.97459584 0.9954023
 0.9946687  1.         1.         0.94444444 0.97482838 0.97831978
 0.96815287 1.         0.95677966 0.8363047  0.99401198]

Kappa:
0.9692074914985048
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3619299ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.795, val_acc:0.117]
Epoch [2/120    avg_loss:2.695, val_acc:0.281]
Epoch [3/120    avg_loss:2.589, val_acc:0.327]
Epoch [4/120    avg_loss:2.488, val_acc:0.366]
Epoch [5/120    avg_loss:2.381, val_acc:0.373]
Epoch [6/120    avg_loss:2.295, val_acc:0.463]
Epoch [7/120    avg_loss:2.170, val_acc:0.497]
Epoch [8/120    avg_loss:2.025, val_acc:0.487]
Epoch [9/120    avg_loss:1.840, val_acc:0.508]
Epoch [10/120    avg_loss:1.777, val_acc:0.496]
Epoch [11/120    avg_loss:1.530, val_acc:0.594]
Epoch [12/120    avg_loss:1.457, val_acc:0.522]
Epoch [13/120    avg_loss:1.322, val_acc:0.674]
Epoch [14/120    avg_loss:1.149, val_acc:0.669]
Epoch [15/120    avg_loss:1.027, val_acc:0.742]
Epoch [16/120    avg_loss:0.914, val_acc:0.711]
Epoch [17/120    avg_loss:0.870, val_acc:0.735]
Epoch [18/120    avg_loss:0.834, val_acc:0.750]
Epoch [19/120    avg_loss:0.666, val_acc:0.797]
Epoch [20/120    avg_loss:0.553, val_acc:0.836]
Epoch [21/120    avg_loss:0.496, val_acc:0.812]
Epoch [22/120    avg_loss:0.458, val_acc:0.871]
Epoch [23/120    avg_loss:0.387, val_acc:0.866]
Epoch [24/120    avg_loss:0.315, val_acc:0.907]
Epoch [25/120    avg_loss:0.343, val_acc:0.882]
Epoch [26/120    avg_loss:0.282, val_acc:0.899]
Epoch [27/120    avg_loss:0.269, val_acc:0.866]
Epoch [28/120    avg_loss:0.309, val_acc:0.849]
Epoch [29/120    avg_loss:0.318, val_acc:0.880]
Epoch [30/120    avg_loss:0.244, val_acc:0.892]
Epoch [31/120    avg_loss:0.192, val_acc:0.932]
Epoch [32/120    avg_loss:0.133, val_acc:0.949]
Epoch [33/120    avg_loss:0.119, val_acc:0.878]
Epoch [34/120    avg_loss:0.129, val_acc:0.954]
Epoch [35/120    avg_loss:0.101, val_acc:0.934]
Epoch [36/120    avg_loss:0.097, val_acc:0.900]
Epoch [37/120    avg_loss:0.273, val_acc:0.815]
Epoch [38/120    avg_loss:0.219, val_acc:0.887]
Epoch [39/120    avg_loss:0.156, val_acc:0.926]
Epoch [40/120    avg_loss:0.127, val_acc:0.933]
Epoch [41/120    avg_loss:0.115, val_acc:0.948]
Epoch [42/120    avg_loss:0.092, val_acc:0.945]
Epoch [43/120    avg_loss:0.078, val_acc:0.916]
Epoch [44/120    avg_loss:0.111, val_acc:0.877]
Epoch [45/120    avg_loss:0.104, val_acc:0.956]
Epoch [46/120    avg_loss:0.064, val_acc:0.971]
Epoch [47/120    avg_loss:0.065, val_acc:0.950]
Epoch [48/120    avg_loss:0.101, val_acc:0.914]
Epoch [49/120    avg_loss:0.067, val_acc:0.968]
Epoch [50/120    avg_loss:0.057, val_acc:0.964]
Epoch [51/120    avg_loss:0.078, val_acc:0.933]
Epoch [52/120    avg_loss:0.051, val_acc:0.963]
Epoch [53/120    avg_loss:0.047, val_acc:0.963]
Epoch [54/120    avg_loss:0.044, val_acc:0.964]
Epoch [55/120    avg_loss:0.049, val_acc:0.942]
Epoch [56/120    avg_loss:0.048, val_acc:0.963]
Epoch [57/120    avg_loss:0.052, val_acc:0.970]
Epoch [58/120    avg_loss:0.054, val_acc:0.963]
Epoch [59/120    avg_loss:0.054, val_acc:0.959]
Epoch [60/120    avg_loss:0.033, val_acc:0.969]
Epoch [61/120    avg_loss:0.027, val_acc:0.971]
Epoch [62/120    avg_loss:0.027, val_acc:0.969]
Epoch [63/120    avg_loss:0.025, val_acc:0.970]
Epoch [64/120    avg_loss:0.021, val_acc:0.969]
Epoch [65/120    avg_loss:0.023, val_acc:0.971]
Epoch [66/120    avg_loss:0.018, val_acc:0.971]
Epoch [67/120    avg_loss:0.020, val_acc:0.972]
Epoch [68/120    avg_loss:0.021, val_acc:0.972]
Epoch [69/120    avg_loss:0.020, val_acc:0.974]
Epoch [70/120    avg_loss:0.021, val_acc:0.975]
Epoch [71/120    avg_loss:0.020, val_acc:0.976]
Epoch [72/120    avg_loss:0.020, val_acc:0.975]
Epoch [73/120    avg_loss:0.021, val_acc:0.975]
Epoch [74/120    avg_loss:0.020, val_acc:0.976]
Epoch [75/120    avg_loss:0.021, val_acc:0.975]
Epoch [76/120    avg_loss:0.017, val_acc:0.976]
Epoch [77/120    avg_loss:0.017, val_acc:0.977]
Epoch [78/120    avg_loss:0.019, val_acc:0.976]
Epoch [79/120    avg_loss:0.018, val_acc:0.976]
Epoch [80/120    avg_loss:0.022, val_acc:0.976]
Epoch [81/120    avg_loss:0.019, val_acc:0.977]
Epoch [82/120    avg_loss:0.025, val_acc:0.977]
Epoch [83/120    avg_loss:0.017, val_acc:0.977]
Epoch [84/120    avg_loss:0.019, val_acc:0.979]
Epoch [85/120    avg_loss:0.017, val_acc:0.977]
Epoch [86/120    avg_loss:0.017, val_acc:0.977]
Epoch [87/120    avg_loss:0.018, val_acc:0.977]
Epoch [88/120    avg_loss:0.015, val_acc:0.977]
Epoch [89/120    avg_loss:0.017, val_acc:0.977]
Epoch [90/120    avg_loss:0.015, val_acc:0.978]
Epoch [91/120    avg_loss:0.019, val_acc:0.978]
Epoch [92/120    avg_loss:0.017, val_acc:0.978]
Epoch [93/120    avg_loss:0.015, val_acc:0.976]
Epoch [94/120    avg_loss:0.018, val_acc:0.979]
Epoch [95/120    avg_loss:0.016, val_acc:0.979]
Epoch [96/120    avg_loss:0.015, val_acc:0.978]
Epoch [97/120    avg_loss:0.016, val_acc:0.978]
Epoch [98/120    avg_loss:0.014, val_acc:0.978]
Epoch [99/120    avg_loss:0.018, val_acc:0.977]
Epoch [100/120    avg_loss:0.016, val_acc:0.977]
Epoch [101/120    avg_loss:0.018, val_acc:0.979]
Epoch [102/120    avg_loss:0.015, val_acc:0.979]
Epoch [103/120    avg_loss:0.016, val_acc:0.980]
Epoch [104/120    avg_loss:0.014, val_acc:0.979]
Epoch [105/120    avg_loss:0.017, val_acc:0.979]
Epoch [106/120    avg_loss:0.014, val_acc:0.979]
Epoch [107/120    avg_loss:0.015, val_acc:0.978]
Epoch [108/120    avg_loss:0.016, val_acc:0.979]
Epoch [109/120    avg_loss:0.017, val_acc:0.978]
Epoch [110/120    avg_loss:0.015, val_acc:0.977]
Epoch [111/120    avg_loss:0.015, val_acc:0.978]
Epoch [112/120    avg_loss:0.015, val_acc:0.978]
Epoch [113/120    avg_loss:0.014, val_acc:0.978]
Epoch [114/120    avg_loss:0.013, val_acc:0.978]
Epoch [115/120    avg_loss:0.015, val_acc:0.979]
Epoch [116/120    avg_loss:0.015, val_acc:0.978]
Epoch [117/120    avg_loss:0.014, val_acc:0.978]
Epoch [118/120    avg_loss:0.013, val_acc:0.978]
Epoch [119/120    avg_loss:0.014, val_acc:0.978]
Epoch [120/120    avg_loss:0.013, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0 1236    5    6    0    0    0    0    0    8   30    0    0
     0    0    0]
 [   0    0    1  730    5    0    0    0    0    3    1    3    3    0
     0    1    0]
 [   0    0    0    0  211    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    0    0    2    0    0
     4    1    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   12    0    0    0    0    0    0    0  837   24    1    0
     0    1    0]
 [   0    0   10    0    0    0    1    0    0    0   18 2163   17    0
     0    0    1]
 [   0    0    0    1    0    0    0    0    0    0    0    1  526    0
     0    5    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    81  266    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.10569105691057

F1 scores:
[       nan 0.98765432 0.97169811 0.9844909  0.97011494 0.99188876
 0.99618612 1.         1.         0.92307692 0.9626222  0.97476341
 0.96869245 1.         0.95962601 0.84310618 0.97005988]

Kappa:
0.9669763536976614
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1734eabf60>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.791, val_acc:0.124]
Epoch [2/120    avg_loss:2.687, val_acc:0.224]
Epoch [3/120    avg_loss:2.573, val_acc:0.268]
Epoch [4/120    avg_loss:2.449, val_acc:0.323]
Epoch [5/120    avg_loss:2.359, val_acc:0.352]
Epoch [6/120    avg_loss:2.209, val_acc:0.452]
Epoch [7/120    avg_loss:2.128, val_acc:0.521]
Epoch [8/120    avg_loss:2.043, val_acc:0.512]
Epoch [9/120    avg_loss:1.930, val_acc:0.610]
Epoch [10/120    avg_loss:1.811, val_acc:0.610]
Epoch [11/120    avg_loss:1.708, val_acc:0.647]
Epoch [12/120    avg_loss:1.527, val_acc:0.702]
Epoch [13/120    avg_loss:1.415, val_acc:0.653]
Epoch [14/120    avg_loss:1.312, val_acc:0.729]
Epoch [15/120    avg_loss:1.206, val_acc:0.767]
Epoch [16/120    avg_loss:0.993, val_acc:0.789]
Epoch [17/120    avg_loss:0.865, val_acc:0.762]
Epoch [18/120    avg_loss:0.806, val_acc:0.774]
Epoch [19/120    avg_loss:0.673, val_acc:0.841]
Epoch [20/120    avg_loss:0.643, val_acc:0.854]
Epoch [21/120    avg_loss:0.547, val_acc:0.857]
Epoch [22/120    avg_loss:0.469, val_acc:0.867]
Epoch [23/120    avg_loss:0.399, val_acc:0.868]
Epoch [24/120    avg_loss:0.348, val_acc:0.912]
Epoch [25/120    avg_loss:0.391, val_acc:0.875]
Epoch [26/120    avg_loss:0.329, val_acc:0.840]
Epoch [27/120    avg_loss:0.302, val_acc:0.899]
Epoch [28/120    avg_loss:0.218, val_acc:0.917]
Epoch [29/120    avg_loss:0.235, val_acc:0.910]
Epoch [30/120    avg_loss:0.228, val_acc:0.916]
Epoch [31/120    avg_loss:0.288, val_acc:0.851]
Epoch [32/120    avg_loss:0.321, val_acc:0.883]
Epoch [33/120    avg_loss:0.267, val_acc:0.913]
Epoch [34/120    avg_loss:0.196, val_acc:0.916]
Epoch [35/120    avg_loss:0.149, val_acc:0.928]
Epoch [36/120    avg_loss:0.135, val_acc:0.934]
Epoch [37/120    avg_loss:0.125, val_acc:0.917]
Epoch [38/120    avg_loss:0.132, val_acc:0.928]
Epoch [39/120    avg_loss:0.118, val_acc:0.918]
Epoch [40/120    avg_loss:0.085, val_acc:0.914]
Epoch [41/120    avg_loss:0.117, val_acc:0.933]
Epoch [42/120    avg_loss:0.128, val_acc:0.929]
Epoch [43/120    avg_loss:0.086, val_acc:0.930]
Epoch [44/120    avg_loss:0.068, val_acc:0.944]
Epoch [45/120    avg_loss:0.109, val_acc:0.919]
Epoch [46/120    avg_loss:0.089, val_acc:0.933]
Epoch [47/120    avg_loss:0.089, val_acc:0.943]
Epoch [48/120    avg_loss:0.081, val_acc:0.942]
Epoch [49/120    avg_loss:0.065, val_acc:0.952]
Epoch [50/120    avg_loss:0.060, val_acc:0.950]
Epoch [51/120    avg_loss:0.037, val_acc:0.959]
Epoch [52/120    avg_loss:0.053, val_acc:0.959]
Epoch [53/120    avg_loss:0.038, val_acc:0.956]
Epoch [54/120    avg_loss:0.035, val_acc:0.940]
Epoch [55/120    avg_loss:0.034, val_acc:0.962]
Epoch [56/120    avg_loss:0.023, val_acc:0.965]
Epoch [57/120    avg_loss:0.032, val_acc:0.956]
Epoch [58/120    avg_loss:0.024, val_acc:0.966]
Epoch [59/120    avg_loss:0.020, val_acc:0.971]
Epoch [60/120    avg_loss:0.021, val_acc:0.970]
Epoch [61/120    avg_loss:0.027, val_acc:0.963]
Epoch [62/120    avg_loss:0.027, val_acc:0.969]
Epoch [63/120    avg_loss:0.038, val_acc:0.957]
Epoch [64/120    avg_loss:0.026, val_acc:0.970]
Epoch [65/120    avg_loss:0.038, val_acc:0.974]
Epoch [66/120    avg_loss:0.028, val_acc:0.896]
Epoch [67/120    avg_loss:0.033, val_acc:0.962]
Epoch [68/120    avg_loss:0.033, val_acc:0.960]
Epoch [69/120    avg_loss:0.033, val_acc:0.969]
Epoch [70/120    avg_loss:0.020, val_acc:0.972]
Epoch [71/120    avg_loss:0.014, val_acc:0.974]
Epoch [72/120    avg_loss:0.019, val_acc:0.975]
Epoch [73/120    avg_loss:0.016, val_acc:0.972]
Epoch [74/120    avg_loss:0.014, val_acc:0.967]
Epoch [75/120    avg_loss:0.016, val_acc:0.966]
Epoch [76/120    avg_loss:0.015, val_acc:0.976]
Epoch [77/120    avg_loss:0.024, val_acc:0.968]
Epoch [78/120    avg_loss:0.023, val_acc:0.970]
Epoch [79/120    avg_loss:0.019, val_acc:0.977]
Epoch [80/120    avg_loss:0.013, val_acc:0.979]
Epoch [81/120    avg_loss:0.016, val_acc:0.975]
Epoch [82/120    avg_loss:0.014, val_acc:0.978]
Epoch [83/120    avg_loss:0.024, val_acc:0.970]
Epoch [84/120    avg_loss:0.017, val_acc:0.979]
Epoch [85/120    avg_loss:0.022, val_acc:0.963]
Epoch [86/120    avg_loss:0.027, val_acc:0.966]
Epoch [87/120    avg_loss:0.015, val_acc:0.977]
Epoch [88/120    avg_loss:0.013, val_acc:0.977]
Epoch [89/120    avg_loss:0.015, val_acc:0.972]
Epoch [90/120    avg_loss:0.014, val_acc:0.963]
Epoch [91/120    avg_loss:0.036, val_acc:0.966]
Epoch [92/120    avg_loss:0.024, val_acc:0.965]
Epoch [93/120    avg_loss:0.016, val_acc:0.972]
Epoch [94/120    avg_loss:0.010, val_acc:0.974]
Epoch [95/120    avg_loss:0.010, val_acc:0.976]
Epoch [96/120    avg_loss:0.010, val_acc:0.972]
Epoch [97/120    avg_loss:0.009, val_acc:0.979]
Epoch [98/120    avg_loss:0.009, val_acc:0.959]
Epoch [99/120    avg_loss:0.010, val_acc:0.976]
Epoch [100/120    avg_loss:0.011, val_acc:0.974]
Epoch [101/120    avg_loss:0.012, val_acc:0.980]
Epoch [102/120    avg_loss:0.026, val_acc:0.965]
Epoch [103/120    avg_loss:0.034, val_acc:0.971]
Epoch [104/120    avg_loss:0.013, val_acc:0.976]
Epoch [105/120    avg_loss:0.010, val_acc:0.978]
Epoch [106/120    avg_loss:0.010, val_acc:0.974]
Epoch [107/120    avg_loss:0.008, val_acc:0.981]
Epoch [108/120    avg_loss:0.005, val_acc:0.981]
Epoch [109/120    avg_loss:0.006, val_acc:0.978]
Epoch [110/120    avg_loss:0.005, val_acc:0.981]
Epoch [111/120    avg_loss:0.006, val_acc:0.980]
Epoch [112/120    avg_loss:0.014, val_acc:0.978]
Epoch [113/120    avg_loss:0.007, val_acc:0.981]
Epoch [114/120    avg_loss:0.006, val_acc:0.980]
Epoch [115/120    avg_loss:0.006, val_acc:0.982]
Epoch [116/120    avg_loss:0.007, val_acc:0.976]
Epoch [117/120    avg_loss:0.005, val_acc:0.980]
Epoch [118/120    avg_loss:0.005, val_acc:0.981]
Epoch [119/120    avg_loss:0.005, val_acc:0.981]
Epoch [120/120    avg_loss:0.008, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1233    1    4    0    3    0    0    0   14   30    0    0
     0    0    0]
 [   0    0    2  710    5    6    0    0    0    4    3    4   13    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    1    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    1    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    2    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  867    3    0    0
     0    0    0]
 [   0    0    1    0    0    0    1    2    0    0   14 2186    6    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    8    0  520    0
     2    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1137    2    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    23  323    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.14634146341463

F1 scores:
[       nan 0.98765432 0.97624703 0.97193703 0.97695853 0.98969072
 0.99318698 0.96153846 1.         0.75675676 0.97251823 0.98601714
 0.96924511 1.         0.98826597 0.96130952 0.97674419]

Kappa:
0.9788643931101358
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1b5f4f3f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.780, val_acc:0.236]
Epoch [2/120    avg_loss:2.702, val_acc:0.268]
Epoch [3/120    avg_loss:2.593, val_acc:0.374]
Epoch [4/120    avg_loss:2.481, val_acc:0.391]
Epoch [5/120    avg_loss:2.360, val_acc:0.395]
Epoch [6/120    avg_loss:2.243, val_acc:0.424]
Epoch [7/120    avg_loss:2.122, val_acc:0.496]
Epoch [8/120    avg_loss:1.997, val_acc:0.551]
Epoch [9/120    avg_loss:1.841, val_acc:0.603]
Epoch [10/120    avg_loss:1.736, val_acc:0.616]
Epoch [11/120    avg_loss:1.487, val_acc:0.672]
Epoch [12/120    avg_loss:1.322, val_acc:0.678]
Epoch [13/120    avg_loss:1.197, val_acc:0.683]
Epoch [14/120    avg_loss:1.110, val_acc:0.719]
Epoch [15/120    avg_loss:0.926, val_acc:0.736]
Epoch [16/120    avg_loss:0.879, val_acc:0.746]
Epoch [17/120    avg_loss:0.849, val_acc:0.727]
Epoch [18/120    avg_loss:0.719, val_acc:0.741]
Epoch [19/120    avg_loss:0.597, val_acc:0.767]
Epoch [20/120    avg_loss:0.596, val_acc:0.796]
Epoch [21/120    avg_loss:0.507, val_acc:0.816]
Epoch [22/120    avg_loss:0.449, val_acc:0.842]
Epoch [23/120    avg_loss:0.392, val_acc:0.846]
Epoch [24/120    avg_loss:0.319, val_acc:0.873]
Epoch [25/120    avg_loss:0.344, val_acc:0.875]
Epoch [26/120    avg_loss:0.269, val_acc:0.898]
Epoch [27/120    avg_loss:0.284, val_acc:0.881]
Epoch [28/120    avg_loss:0.275, val_acc:0.860]
Epoch [29/120    avg_loss:0.228, val_acc:0.918]
Epoch [30/120    avg_loss:0.205, val_acc:0.900]
Epoch [31/120    avg_loss:0.194, val_acc:0.922]
Epoch [32/120    avg_loss:0.155, val_acc:0.924]
Epoch [33/120    avg_loss:0.184, val_acc:0.891]
Epoch [34/120    avg_loss:0.186, val_acc:0.910]
Epoch [35/120    avg_loss:0.150, val_acc:0.912]
Epoch [36/120    avg_loss:0.141, val_acc:0.930]
Epoch [37/120    avg_loss:0.182, val_acc:0.927]
Epoch [38/120    avg_loss:0.114, val_acc:0.929]
Epoch [39/120    avg_loss:0.096, val_acc:0.940]
Epoch [40/120    avg_loss:0.167, val_acc:0.908]
Epoch [41/120    avg_loss:0.126, val_acc:0.931]
Epoch [42/120    avg_loss:0.100, val_acc:0.941]
Epoch [43/120    avg_loss:0.092, val_acc:0.942]
Epoch [44/120    avg_loss:0.084, val_acc:0.931]
Epoch [45/120    avg_loss:0.081, val_acc:0.938]
Epoch [46/120    avg_loss:0.107, val_acc:0.944]
Epoch [47/120    avg_loss:0.076, val_acc:0.935]
Epoch [48/120    avg_loss:0.119, val_acc:0.929]
Epoch [49/120    avg_loss:0.383, val_acc:0.833]
Epoch [50/120    avg_loss:0.300, val_acc:0.894]
Epoch [51/120    avg_loss:0.176, val_acc:0.938]
Epoch [52/120    avg_loss:0.134, val_acc:0.930]
Epoch [53/120    avg_loss:0.109, val_acc:0.957]
Epoch [54/120    avg_loss:0.105, val_acc:0.898]
Epoch [55/120    avg_loss:0.096, val_acc:0.941]
Epoch [56/120    avg_loss:0.078, val_acc:0.947]
Epoch [57/120    avg_loss:0.062, val_acc:0.954]
Epoch [58/120    avg_loss:0.059, val_acc:0.953]
Epoch [59/120    avg_loss:0.067, val_acc:0.936]
Epoch [60/120    avg_loss:0.072, val_acc:0.959]
Epoch [61/120    avg_loss:0.054, val_acc:0.938]
Epoch [62/120    avg_loss:0.051, val_acc:0.964]
Epoch [63/120    avg_loss:0.071, val_acc:0.948]
Epoch [64/120    avg_loss:0.067, val_acc:0.964]
Epoch [65/120    avg_loss:0.042, val_acc:0.966]
Epoch [66/120    avg_loss:0.039, val_acc:0.961]
Epoch [67/120    avg_loss:0.035, val_acc:0.964]
Epoch [68/120    avg_loss:0.051, val_acc:0.960]
Epoch [69/120    avg_loss:0.037, val_acc:0.969]
Epoch [70/120    avg_loss:0.025, val_acc:0.973]
Epoch [71/120    avg_loss:0.027, val_acc:0.965]
Epoch [72/120    avg_loss:0.027, val_acc:0.971]
Epoch [73/120    avg_loss:0.027, val_acc:0.977]
Epoch [74/120    avg_loss:0.030, val_acc:0.971]
Epoch [75/120    avg_loss:0.030, val_acc:0.964]
Epoch [76/120    avg_loss:0.039, val_acc:0.971]
Epoch [77/120    avg_loss:0.024, val_acc:0.966]
Epoch [78/120    avg_loss:0.027, val_acc:0.970]
Epoch [79/120    avg_loss:0.019, val_acc:0.973]
Epoch [80/120    avg_loss:0.035, val_acc:0.953]
Epoch [81/120    avg_loss:0.043, val_acc:0.925]
Epoch [82/120    avg_loss:0.046, val_acc:0.956]
Epoch [83/120    avg_loss:0.025, val_acc:0.966]
Epoch [84/120    avg_loss:0.021, val_acc:0.972]
Epoch [85/120    avg_loss:0.015, val_acc:0.974]
Epoch [86/120    avg_loss:0.018, val_acc:0.973]
Epoch [87/120    avg_loss:0.013, val_acc:0.976]
Epoch [88/120    avg_loss:0.012, val_acc:0.976]
Epoch [89/120    avg_loss:0.012, val_acc:0.975]
Epoch [90/120    avg_loss:0.011, val_acc:0.975]
Epoch [91/120    avg_loss:0.011, val_acc:0.976]
Epoch [92/120    avg_loss:0.011, val_acc:0.975]
Epoch [93/120    avg_loss:0.011, val_acc:0.975]
Epoch [94/120    avg_loss:0.010, val_acc:0.977]
Epoch [95/120    avg_loss:0.011, val_acc:0.978]
Epoch [96/120    avg_loss:0.012, val_acc:0.976]
Epoch [97/120    avg_loss:0.011, val_acc:0.976]
Epoch [98/120    avg_loss:0.012, val_acc:0.976]
Epoch [99/120    avg_loss:0.015, val_acc:0.975]
Epoch [100/120    avg_loss:0.012, val_acc:0.975]
Epoch [101/120    avg_loss:0.011, val_acc:0.975]
Epoch [102/120    avg_loss:0.011, val_acc:0.976]
Epoch [103/120    avg_loss:0.011, val_acc:0.976]
Epoch [104/120    avg_loss:0.011, val_acc:0.976]
Epoch [105/120    avg_loss:0.010, val_acc:0.977]
Epoch [106/120    avg_loss:0.011, val_acc:0.976]
Epoch [107/120    avg_loss:0.011, val_acc:0.976]
Epoch [108/120    avg_loss:0.013, val_acc:0.977]
Epoch [109/120    avg_loss:0.009, val_acc:0.977]
Epoch [110/120    avg_loss:0.010, val_acc:0.977]
Epoch [111/120    avg_loss:0.009, val_acc:0.976]
Epoch [112/120    avg_loss:0.009, val_acc:0.976]
Epoch [113/120    avg_loss:0.011, val_acc:0.976]
Epoch [114/120    avg_loss:0.010, val_acc:0.976]
Epoch [115/120    avg_loss:0.010, val_acc:0.977]
Epoch [116/120    avg_loss:0.010, val_acc:0.976]
Epoch [117/120    avg_loss:0.012, val_acc:0.976]
Epoch [118/120    avg_loss:0.010, val_acc:0.976]
Epoch [119/120    avg_loss:0.009, val_acc:0.976]
Epoch [120/120    avg_loss:0.012, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    0    1    2    0
     0    0    0]
 [   0    0 1245    2    5    0    1    0    0    0    2   30    0    0
     0    0    0]
 [   0    0    0  731    0    0    0    0    0    9    0    5    2    0
     0    0    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    3    1    0    0    0  838   28    0    0
     0    0    0]
 [   0    0    3    0    0    2    0    2    1    0    6 2186    8    0
     0    2    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0  528    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1125   14    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    65  269    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.57181571815718

F1 scores:
[       nan 0.96202532 0.98108747 0.98717083 0.98368298 0.99428571
 0.98795181 0.96153846 0.99767442 0.8        0.972722   0.98026906
 0.97959184 1.         0.96566524 0.84858044 0.97619048]

Kappa:
0.9722905857734498
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f12abf16ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.787, val_acc:0.230]
Epoch [2/120    avg_loss:2.695, val_acc:0.271]
Epoch [3/120    avg_loss:2.594, val_acc:0.333]
Epoch [4/120    avg_loss:2.452, val_acc:0.405]
Epoch [5/120    avg_loss:2.345, val_acc:0.435]
Epoch [6/120    avg_loss:2.241, val_acc:0.488]
Epoch [7/120    avg_loss:2.129, val_acc:0.533]
Epoch [8/120    avg_loss:1.993, val_acc:0.558]
Epoch [9/120    avg_loss:1.874, val_acc:0.541]
Epoch [10/120    avg_loss:1.730, val_acc:0.538]
Epoch [11/120    avg_loss:1.550, val_acc:0.612]
Epoch [12/120    avg_loss:1.392, val_acc:0.666]
Epoch [13/120    avg_loss:1.260, val_acc:0.706]
Epoch [14/120    avg_loss:1.142, val_acc:0.747]
Epoch [15/120    avg_loss:1.026, val_acc:0.784]
Epoch [16/120    avg_loss:0.862, val_acc:0.761]
Epoch [17/120    avg_loss:0.780, val_acc:0.804]
Epoch [18/120    avg_loss:0.763, val_acc:0.790]
Epoch [19/120    avg_loss:0.639, val_acc:0.849]
Epoch [20/120    avg_loss:0.539, val_acc:0.849]
Epoch [21/120    avg_loss:0.491, val_acc:0.857]
Epoch [22/120    avg_loss:0.422, val_acc:0.872]
Epoch [23/120    avg_loss:0.357, val_acc:0.908]
Epoch [24/120    avg_loss:0.335, val_acc:0.904]
Epoch [25/120    avg_loss:0.283, val_acc:0.892]
Epoch [26/120    avg_loss:0.248, val_acc:0.893]
Epoch [27/120    avg_loss:0.220, val_acc:0.918]
Epoch [28/120    avg_loss:0.208, val_acc:0.936]
Epoch [29/120    avg_loss:0.263, val_acc:0.886]
Epoch [30/120    avg_loss:0.196, val_acc:0.890]
Epoch [31/120    avg_loss:0.164, val_acc:0.945]
Epoch [32/120    avg_loss:0.131, val_acc:0.948]
Epoch [33/120    avg_loss:0.118, val_acc:0.958]
Epoch [34/120    avg_loss:0.115, val_acc:0.958]
Epoch [35/120    avg_loss:0.112, val_acc:0.946]
Epoch [36/120    avg_loss:0.085, val_acc:0.958]
Epoch [37/120    avg_loss:0.111, val_acc:0.951]
Epoch [38/120    avg_loss:0.096, val_acc:0.956]
Epoch [39/120    avg_loss:0.071, val_acc:0.962]
Epoch [40/120    avg_loss:0.075, val_acc:0.961]
Epoch [41/120    avg_loss:0.084, val_acc:0.956]
Epoch [42/120    avg_loss:0.070, val_acc:0.968]
Epoch [43/120    avg_loss:0.097, val_acc:0.919]
Epoch [44/120    avg_loss:0.129, val_acc:0.941]
Epoch [45/120    avg_loss:0.233, val_acc:0.914]
Epoch [46/120    avg_loss:0.203, val_acc:0.855]
Epoch [47/120    avg_loss:0.261, val_acc:0.945]
Epoch [48/120    avg_loss:0.125, val_acc:0.962]
Epoch [49/120    avg_loss:0.079, val_acc:0.957]
Epoch [50/120    avg_loss:0.078, val_acc:0.956]
Epoch [51/120    avg_loss:0.064, val_acc:0.960]
Epoch [52/120    avg_loss:0.042, val_acc:0.961]
Epoch [53/120    avg_loss:0.041, val_acc:0.967]
Epoch [54/120    avg_loss:0.048, val_acc:0.960]
Epoch [55/120    avg_loss:0.052, val_acc:0.962]
Epoch [56/120    avg_loss:0.032, val_acc:0.971]
Epoch [57/120    avg_loss:0.032, val_acc:0.971]
Epoch [58/120    avg_loss:0.024, val_acc:0.972]
Epoch [59/120    avg_loss:0.029, val_acc:0.976]
Epoch [60/120    avg_loss:0.025, val_acc:0.974]
Epoch [61/120    avg_loss:0.024, val_acc:0.974]
Epoch [62/120    avg_loss:0.021, val_acc:0.976]
Epoch [63/120    avg_loss:0.023, val_acc:0.977]
Epoch [64/120    avg_loss:0.023, val_acc:0.976]
Epoch [65/120    avg_loss:0.023, val_acc:0.978]
Epoch [66/120    avg_loss:0.021, val_acc:0.976]
Epoch [67/120    avg_loss:0.021, val_acc:0.976]
Epoch [68/120    avg_loss:0.023, val_acc:0.976]
Epoch [69/120    avg_loss:0.027, val_acc:0.976]
Epoch [70/120    avg_loss:0.025, val_acc:0.972]
Epoch [71/120    avg_loss:0.024, val_acc:0.977]
Epoch [72/120    avg_loss:0.020, val_acc:0.977]
Epoch [73/120    avg_loss:0.021, val_acc:0.978]
Epoch [74/120    avg_loss:0.021, val_acc:0.980]
Epoch [75/120    avg_loss:0.023, val_acc:0.976]
Epoch [76/120    avg_loss:0.020, val_acc:0.978]
Epoch [77/120    avg_loss:0.018, val_acc:0.977]
Epoch [78/120    avg_loss:0.019, val_acc:0.979]
Epoch [79/120    avg_loss:0.019, val_acc:0.981]
Epoch [80/120    avg_loss:0.020, val_acc:0.978]
Epoch [81/120    avg_loss:0.019, val_acc:0.980]
Epoch [82/120    avg_loss:0.018, val_acc:0.980]
Epoch [83/120    avg_loss:0.021, val_acc:0.981]
Epoch [84/120    avg_loss:0.018, val_acc:0.977]
Epoch [85/120    avg_loss:0.016, val_acc:0.979]
Epoch [86/120    avg_loss:0.018, val_acc:0.980]
Epoch [87/120    avg_loss:0.017, val_acc:0.980]
Epoch [88/120    avg_loss:0.021, val_acc:0.980]
Epoch [89/120    avg_loss:0.018, val_acc:0.980]
Epoch [90/120    avg_loss:0.018, val_acc:0.981]
Epoch [91/120    avg_loss:0.018, val_acc:0.981]
Epoch [92/120    avg_loss:0.017, val_acc:0.980]
Epoch [93/120    avg_loss:0.020, val_acc:0.980]
Epoch [94/120    avg_loss:0.017, val_acc:0.979]
Epoch [95/120    avg_loss:0.018, val_acc:0.980]
Epoch [96/120    avg_loss:0.020, val_acc:0.982]
Epoch [97/120    avg_loss:0.016, val_acc:0.983]
Epoch [98/120    avg_loss:0.014, val_acc:0.983]
Epoch [99/120    avg_loss:0.016, val_acc:0.981]
Epoch [100/120    avg_loss:0.015, val_acc:0.983]
Epoch [101/120    avg_loss:0.017, val_acc:0.984]
Epoch [102/120    avg_loss:0.017, val_acc:0.985]
Epoch [103/120    avg_loss:0.018, val_acc:0.981]
Epoch [104/120    avg_loss:0.015, val_acc:0.982]
Epoch [105/120    avg_loss:0.016, val_acc:0.982]
Epoch [106/120    avg_loss:0.016, val_acc:0.984]
Epoch [107/120    avg_loss:0.016, val_acc:0.982]
Epoch [108/120    avg_loss:0.016, val_acc:0.983]
Epoch [109/120    avg_loss:0.015, val_acc:0.983]
Epoch [110/120    avg_loss:0.013, val_acc:0.983]
Epoch [111/120    avg_loss:0.016, val_acc:0.984]
Epoch [112/120    avg_loss:0.016, val_acc:0.983]
Epoch [113/120    avg_loss:0.013, val_acc:0.981]
Epoch [114/120    avg_loss:0.017, val_acc:0.985]
Epoch [115/120    avg_loss:0.015, val_acc:0.982]
Epoch [116/120    avg_loss:0.014, val_acc:0.981]
Epoch [117/120    avg_loss:0.018, val_acc:0.980]
Epoch [118/120    avg_loss:0.015, val_acc:0.983]
Epoch [119/120    avg_loss:0.015, val_acc:0.985]
Epoch [120/120    avg_loss:0.016, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1257    0    5    1    0    0    0    0    5   17    0    0
     0    0    0]
 [   0    0    1  724    4    0    0    0    0    0    1   13    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    9    0    0    9    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    1    0    0    0    0  851   20    0    0
     0    0    0]
 [   0    0   17    0    0    1    0    1    0    0   10 2163   17    0
     0    1    0]
 [   0    0    0    2    2    2    0    0    0    0    6    2  516    0
     2    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    26  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.91869918699187

F1 scores:
[       nan 1.         0.98088178 0.98302783 0.97482838 0.99198167
 0.99018868 0.98039216 1.         0.66666667 0.97368421 0.97718545
 0.96358543 1.         0.9825784  0.9380531  0.99408284]

Kappa:
0.9762660625840024
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f695bfb8f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.800, val_acc:0.194]
Epoch [2/120    avg_loss:2.713, val_acc:0.269]
Epoch [3/120    avg_loss:2.633, val_acc:0.294]
Epoch [4/120    avg_loss:2.533, val_acc:0.361]
Epoch [5/120    avg_loss:2.414, val_acc:0.485]
Epoch [6/120    avg_loss:2.344, val_acc:0.525]
Epoch [7/120    avg_loss:2.198, val_acc:0.571]
Epoch [8/120    avg_loss:2.073, val_acc:0.590]
Epoch [9/120    avg_loss:1.941, val_acc:0.590]
Epoch [10/120    avg_loss:1.818, val_acc:0.626]
Epoch [11/120    avg_loss:1.693, val_acc:0.623]
Epoch [12/120    avg_loss:1.593, val_acc:0.667]
Epoch [13/120    avg_loss:1.472, val_acc:0.669]
Epoch [14/120    avg_loss:1.310, val_acc:0.690]
Epoch [15/120    avg_loss:1.184, val_acc:0.698]
Epoch [16/120    avg_loss:1.018, val_acc:0.771]
Epoch [17/120    avg_loss:0.908, val_acc:0.704]
Epoch [18/120    avg_loss:0.900, val_acc:0.741]
Epoch [19/120    avg_loss:0.776, val_acc:0.782]
Epoch [20/120    avg_loss:0.690, val_acc:0.818]
Epoch [21/120    avg_loss:0.609, val_acc:0.794]
Epoch [22/120    avg_loss:0.498, val_acc:0.834]
Epoch [23/120    avg_loss:0.464, val_acc:0.819]
Epoch [24/120    avg_loss:0.433, val_acc:0.871]
Epoch [25/120    avg_loss:0.452, val_acc:0.823]
Epoch [26/120    avg_loss:0.474, val_acc:0.848]
Epoch [27/120    avg_loss:0.487, val_acc:0.694]
Epoch [28/120    avg_loss:0.675, val_acc:0.861]
Epoch [29/120    avg_loss:0.355, val_acc:0.909]
Epoch [30/120    avg_loss:0.288, val_acc:0.858]
Epoch [31/120    avg_loss:0.272, val_acc:0.916]
Epoch [32/120    avg_loss:0.206, val_acc:0.909]
Epoch [33/120    avg_loss:0.277, val_acc:0.889]
Epoch [34/120    avg_loss:0.209, val_acc:0.916]
Epoch [35/120    avg_loss:0.150, val_acc:0.931]
Epoch [36/120    avg_loss:0.124, val_acc:0.940]
Epoch [37/120    avg_loss:0.126, val_acc:0.931]
Epoch [38/120    avg_loss:0.111, val_acc:0.936]
Epoch [39/120    avg_loss:0.116, val_acc:0.887]
Epoch [40/120    avg_loss:0.125, val_acc:0.917]
Epoch [41/120    avg_loss:0.135, val_acc:0.908]
Epoch [42/120    avg_loss:0.078, val_acc:0.943]
Epoch [43/120    avg_loss:0.113, val_acc:0.952]
Epoch [44/120    avg_loss:0.111, val_acc:0.956]
Epoch [45/120    avg_loss:0.064, val_acc:0.952]
Epoch [46/120    avg_loss:0.065, val_acc:0.966]
Epoch [47/120    avg_loss:0.064, val_acc:0.965]
Epoch [48/120    avg_loss:0.044, val_acc:0.956]
Epoch [49/120    avg_loss:0.044, val_acc:0.960]
Epoch [50/120    avg_loss:0.057, val_acc:0.961]
Epoch [51/120    avg_loss:0.045, val_acc:0.943]
Epoch [52/120    avg_loss:0.044, val_acc:0.965]
Epoch [53/120    avg_loss:0.045, val_acc:0.968]
Epoch [54/120    avg_loss:0.037, val_acc:0.968]
Epoch [55/120    avg_loss:0.044, val_acc:0.966]
Epoch [56/120    avg_loss:0.039, val_acc:0.964]
Epoch [57/120    avg_loss:0.041, val_acc:0.971]
Epoch [58/120    avg_loss:0.060, val_acc:0.093]
Epoch [59/120    avg_loss:2.368, val_acc:0.458]
Epoch [60/120    avg_loss:1.874, val_acc:0.506]
Epoch [61/120    avg_loss:1.713, val_acc:0.526]
Epoch [62/120    avg_loss:1.555, val_acc:0.586]
Epoch [63/120    avg_loss:1.345, val_acc:0.580]
Epoch [64/120    avg_loss:1.192, val_acc:0.724]
Epoch [65/120    avg_loss:0.998, val_acc:0.689]
Epoch [66/120    avg_loss:0.863, val_acc:0.745]
Epoch [67/120    avg_loss:0.756, val_acc:0.782]
Epoch [68/120    avg_loss:0.604, val_acc:0.770]
Epoch [69/120    avg_loss:0.505, val_acc:0.819]
Epoch [70/120    avg_loss:0.483, val_acc:0.822]
Epoch [71/120    avg_loss:0.382, val_acc:0.857]
Epoch [72/120    avg_loss:0.300, val_acc:0.873]
Epoch [73/120    avg_loss:0.285, val_acc:0.879]
Epoch [74/120    avg_loss:0.258, val_acc:0.876]
Epoch [75/120    avg_loss:0.261, val_acc:0.889]
Epoch [76/120    avg_loss:0.271, val_acc:0.894]
Epoch [77/120    avg_loss:0.247, val_acc:0.901]
Epoch [78/120    avg_loss:0.246, val_acc:0.896]
Epoch [79/120    avg_loss:0.230, val_acc:0.892]
Epoch [80/120    avg_loss:0.204, val_acc:0.909]
Epoch [81/120    avg_loss:0.213, val_acc:0.907]
Epoch [82/120    avg_loss:0.205, val_acc:0.907]
Epoch [83/120    avg_loss:0.198, val_acc:0.914]
Epoch [84/120    avg_loss:0.205, val_acc:0.912]
Epoch [85/120    avg_loss:0.210, val_acc:0.914]
Epoch [86/120    avg_loss:0.208, val_acc:0.916]
Epoch [87/120    avg_loss:0.200, val_acc:0.915]
Epoch [88/120    avg_loss:0.188, val_acc:0.915]
Epoch [89/120    avg_loss:0.187, val_acc:0.915]
Epoch [90/120    avg_loss:0.204, val_acc:0.918]
Epoch [91/120    avg_loss:0.181, val_acc:0.916]
Epoch [92/120    avg_loss:0.192, val_acc:0.918]
Epoch [93/120    avg_loss:0.197, val_acc:0.919]
Epoch [94/120    avg_loss:0.190, val_acc:0.919]
Epoch [95/120    avg_loss:0.181, val_acc:0.921]
Epoch [96/120    avg_loss:0.194, val_acc:0.918]
Epoch [97/120    avg_loss:0.213, val_acc:0.919]
Epoch [98/120    avg_loss:0.178, val_acc:0.919]
Epoch [99/120    avg_loss:0.183, val_acc:0.919]
Epoch [100/120    avg_loss:0.185, val_acc:0.919]
Epoch [101/120    avg_loss:0.175, val_acc:0.919]
Epoch [102/120    avg_loss:0.207, val_acc:0.919]
Epoch [103/120    avg_loss:0.192, val_acc:0.918]
Epoch [104/120    avg_loss:0.202, val_acc:0.917]
Epoch [105/120    avg_loss:0.187, val_acc:0.919]
Epoch [106/120    avg_loss:0.176, val_acc:0.917]
Epoch [107/120    avg_loss:0.194, val_acc:0.917]
Epoch [108/120    avg_loss:0.191, val_acc:0.917]
Epoch [109/120    avg_loss:0.189, val_acc:0.917]
Epoch [110/120    avg_loss:0.188, val_acc:0.917]
Epoch [111/120    avg_loss:0.204, val_acc:0.917]
Epoch [112/120    avg_loss:0.183, val_acc:0.917]
Epoch [113/120    avg_loss:0.186, val_acc:0.917]
Epoch [114/120    avg_loss:0.189, val_acc:0.917]
Epoch [115/120    avg_loss:0.189, val_acc:0.917]
Epoch [116/120    avg_loss:0.199, val_acc:0.917]
Epoch [117/120    avg_loss:0.191, val_acc:0.917]
Epoch [118/120    avg_loss:0.168, val_acc:0.917]
Epoch [119/120    avg_loss:0.191, val_acc:0.917]
Epoch [120/120    avg_loss:0.180, val_acc:0.917]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    1    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1179    5   21    0    0    0    0    3   50   19    3    0
     5    0    0]
 [   0    0   38  635    8    2    0    0    0    6    3   28   21    0
     6    0    0]
 [   0    0    5    0  206    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  399    1   18    0    1    0    0    0    0
    13    3    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   10    0    0    0    0    0    0  420    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    1    0    0   13    0    0    2    0
     0    0    0]
 [   0    0   45    3    0   10    3    0    0    0  768   20    9    0
    13    4    0]
 [   0    0   48   35    1   17    1    0    3    0   27 2001   60    1
    10    0    6]
 [   0    0    0    0    0    0    0    0    0    0    3    1  522    0
     0    5    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1114   24    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    62  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
92.43360433604336

F1 scores:
[       nan 0.85393258 0.90692308 0.88997898 0.91555556 0.92361111
 0.99316629 0.73529412 0.98475967 0.63414634 0.88888889 0.93482831
 0.90467938 0.99730458 0.94286923 0.85329341 0.94318182]

Kappa:
0.9139679722042418
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f328d38aef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.758, val_acc:0.301]
Epoch [2/120    avg_loss:2.648, val_acc:0.320]
Epoch [3/120    avg_loss:2.519, val_acc:0.344]
Epoch [4/120    avg_loss:2.420, val_acc:0.418]
Epoch [5/120    avg_loss:2.291, val_acc:0.445]
Epoch [6/120    avg_loss:2.205, val_acc:0.498]
Epoch [7/120    avg_loss:2.134, val_acc:0.457]
Epoch [8/120    avg_loss:2.034, val_acc:0.595]
Epoch [9/120    avg_loss:1.956, val_acc:0.611]
Epoch [10/120    avg_loss:1.808, val_acc:0.643]
Epoch [11/120    avg_loss:1.629, val_acc:0.643]
Epoch [12/120    avg_loss:1.445, val_acc:0.710]
Epoch [13/120    avg_loss:1.251, val_acc:0.621]
Epoch [14/120    avg_loss:1.120, val_acc:0.749]
Epoch [15/120    avg_loss:0.982, val_acc:0.730]
Epoch [16/120    avg_loss:0.901, val_acc:0.776]
Epoch [17/120    avg_loss:0.789, val_acc:0.817]
Epoch [18/120    avg_loss:0.632, val_acc:0.819]
Epoch [19/120    avg_loss:0.567, val_acc:0.855]
Epoch [20/120    avg_loss:0.559, val_acc:0.834]
Epoch [21/120    avg_loss:0.474, val_acc:0.818]
Epoch [22/120    avg_loss:0.488, val_acc:0.873]
Epoch [23/120    avg_loss:0.830, val_acc:0.439]
Epoch [24/120    avg_loss:1.192, val_acc:0.778]
Epoch [25/120    avg_loss:0.487, val_acc:0.873]
Epoch [26/120    avg_loss:0.414, val_acc:0.839]
Epoch [27/120    avg_loss:0.319, val_acc:0.894]
Epoch [28/120    avg_loss:0.304, val_acc:0.903]
Epoch [29/120    avg_loss:0.254, val_acc:0.908]
Epoch [30/120    avg_loss:0.221, val_acc:0.930]
Epoch [31/120    avg_loss:0.248, val_acc:0.920]
Epoch [32/120    avg_loss:0.183, val_acc:0.925]
Epoch [33/120    avg_loss:0.173, val_acc:0.939]
Epoch [34/120    avg_loss:0.143, val_acc:0.922]
Epoch [35/120    avg_loss:0.133, val_acc:0.928]
Epoch [36/120    avg_loss:0.126, val_acc:0.932]
Epoch [37/120    avg_loss:0.134, val_acc:0.918]
Epoch [38/120    avg_loss:0.102, val_acc:0.952]
Epoch [39/120    avg_loss:0.144, val_acc:0.910]
Epoch [40/120    avg_loss:0.119, val_acc:0.953]
Epoch [41/120    avg_loss:0.082, val_acc:0.947]
Epoch [42/120    avg_loss:0.095, val_acc:0.943]
Epoch [43/120    avg_loss:0.076, val_acc:0.959]
Epoch [44/120    avg_loss:0.070, val_acc:0.950]
Epoch [45/120    avg_loss:0.068, val_acc:0.957]
Epoch [46/120    avg_loss:0.058, val_acc:0.955]
Epoch [47/120    avg_loss:0.052, val_acc:0.969]
Epoch [48/120    avg_loss:0.058, val_acc:0.939]
Epoch [49/120    avg_loss:0.052, val_acc:0.957]
Epoch [50/120    avg_loss:0.050, val_acc:0.967]
Epoch [51/120    avg_loss:0.048, val_acc:0.970]
Epoch [52/120    avg_loss:0.069, val_acc:0.967]
Epoch [53/120    avg_loss:0.069, val_acc:0.976]
Epoch [54/120    avg_loss:0.045, val_acc:0.967]
Epoch [55/120    avg_loss:0.053, val_acc:0.965]
Epoch [56/120    avg_loss:0.046, val_acc:0.975]
Epoch [57/120    avg_loss:0.037, val_acc:0.970]
Epoch [58/120    avg_loss:0.031, val_acc:0.974]
Epoch [59/120    avg_loss:0.058, val_acc:0.961]
Epoch [60/120    avg_loss:0.034, val_acc:0.967]
Epoch [61/120    avg_loss:0.026, val_acc:0.967]
Epoch [62/120    avg_loss:0.033, val_acc:0.968]
Epoch [63/120    avg_loss:0.028, val_acc:0.964]
Epoch [64/120    avg_loss:0.026, val_acc:0.964]
Epoch [65/120    avg_loss:0.028, val_acc:0.968]
Epoch [66/120    avg_loss:0.034, val_acc:0.967]
Epoch [67/120    avg_loss:0.022, val_acc:0.973]
Epoch [68/120    avg_loss:0.017, val_acc:0.972]
Epoch [69/120    avg_loss:0.017, val_acc:0.974]
Epoch [70/120    avg_loss:0.015, val_acc:0.975]
Epoch [71/120    avg_loss:0.013, val_acc:0.975]
Epoch [72/120    avg_loss:0.015, val_acc:0.976]
Epoch [73/120    avg_loss:0.016, val_acc:0.979]
Epoch [74/120    avg_loss:0.012, val_acc:0.977]
Epoch [75/120    avg_loss:0.017, val_acc:0.976]
Epoch [76/120    avg_loss:0.013, val_acc:0.977]
Epoch [77/120    avg_loss:0.017, val_acc:0.976]
Epoch [78/120    avg_loss:0.015, val_acc:0.976]
Epoch [79/120    avg_loss:0.014, val_acc:0.975]
Epoch [80/120    avg_loss:0.012, val_acc:0.978]
Epoch [81/120    avg_loss:0.013, val_acc:0.978]
Epoch [82/120    avg_loss:0.012, val_acc:0.978]
Epoch [83/120    avg_loss:0.013, val_acc:0.979]
Epoch [84/120    avg_loss:0.013, val_acc:0.979]
Epoch [85/120    avg_loss:0.010, val_acc:0.979]
Epoch [86/120    avg_loss:0.017, val_acc:0.978]
Epoch [87/120    avg_loss:0.014, val_acc:0.980]
Epoch [88/120    avg_loss:0.013, val_acc:0.979]
Epoch [89/120    avg_loss:0.014, val_acc:0.978]
Epoch [90/120    avg_loss:0.012, val_acc:0.979]
Epoch [91/120    avg_loss:0.012, val_acc:0.980]
Epoch [92/120    avg_loss:0.009, val_acc:0.978]
Epoch [93/120    avg_loss:0.013, val_acc:0.978]
Epoch [94/120    avg_loss:0.009, val_acc:0.979]
Epoch [95/120    avg_loss:0.011, val_acc:0.978]
Epoch [96/120    avg_loss:0.011, val_acc:0.978]
Epoch [97/120    avg_loss:0.013, val_acc:0.978]
Epoch [98/120    avg_loss:0.010, val_acc:0.978]
Epoch [99/120    avg_loss:0.014, val_acc:0.978]
Epoch [100/120    avg_loss:0.009, val_acc:0.978]
Epoch [101/120    avg_loss:0.011, val_acc:0.978]
Epoch [102/120    avg_loss:0.011, val_acc:0.978]
Epoch [103/120    avg_loss:0.020, val_acc:0.979]
Epoch [104/120    avg_loss:0.010, val_acc:0.980]
Epoch [105/120    avg_loss:0.013, val_acc:0.979]
Epoch [106/120    avg_loss:0.012, val_acc:0.978]
Epoch [107/120    avg_loss:0.010, val_acc:0.978]
Epoch [108/120    avg_loss:0.012, val_acc:0.977]
Epoch [109/120    avg_loss:0.012, val_acc:0.979]
Epoch [110/120    avg_loss:0.010, val_acc:0.980]
Epoch [111/120    avg_loss:0.009, val_acc:0.981]
Epoch [112/120    avg_loss:0.011, val_acc:0.980]
Epoch [113/120    avg_loss:0.018, val_acc:0.974]
Epoch [114/120    avg_loss:0.011, val_acc:0.975]
Epoch [115/120    avg_loss:0.008, val_acc:0.978]
Epoch [116/120    avg_loss:0.011, val_acc:0.978]
Epoch [117/120    avg_loss:0.014, val_acc:0.979]
Epoch [118/120    avg_loss:0.012, val_acc:0.978]
Epoch [119/120    avg_loss:0.015, val_acc:0.981]
Epoch [120/120    avg_loss:0.010, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    1    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1242    1    3    5    2    0    0    0    4   28    0    0
     0    0    0]
 [   0    0    4  714    1    0    2    0    0   10    8    4    4    0
     0    0    0]
 [   0    0    0    3  205    0    0    0    0    0    0    0    5    0
     0    0    0]
 [   0    0    0    0    0  427    0    7    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  428    0    0    0    1    0
     0    0    0]
 [   0    0    0    2    0    0    1    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    8    2    0    2    1    0    0    1  843   17    0    0
     0    1    0]
 [   0    0    7    0    0    1    1    1    0    0    5 2191    4    0
     0    0    0]
 [   0    0    1    1    0    1    0    0    0    0    0    6  519    0
     2    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1137    1    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    44  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.57181571815718

F1 scores:
[       nan 0.95       0.97526502 0.97142857 0.97156398 0.9793578
 0.98648649 0.86206897 0.997669   0.68181818 0.97008055 0.98339318
 0.97191011 1.         0.97890659 0.91107644 0.97076023]

Kappa:
0.972292017729086
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f18171cff28>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.807, val_acc:0.392]
Epoch [2/120    avg_loss:2.718, val_acc:0.468]
Epoch [3/120    avg_loss:2.611, val_acc:0.480]
Epoch [4/120    avg_loss:2.500, val_acc:0.506]
Epoch [5/120    avg_loss:2.402, val_acc:0.547]
Epoch [6/120    avg_loss:2.257, val_acc:0.592]
Epoch [7/120    avg_loss:2.125, val_acc:0.607]
Epoch [8/120    avg_loss:2.001, val_acc:0.621]
Epoch [9/120    avg_loss:1.865, val_acc:0.658]
Epoch [10/120    avg_loss:1.660, val_acc:0.664]
Epoch [11/120    avg_loss:1.475, val_acc:0.684]
Epoch [12/120    avg_loss:1.308, val_acc:0.702]
Epoch [13/120    avg_loss:1.218, val_acc:0.724]
Epoch [14/120    avg_loss:1.002, val_acc:0.786]
Epoch [15/120    avg_loss:0.899, val_acc:0.791]
Epoch [16/120    avg_loss:0.853, val_acc:0.811]
Epoch [17/120    avg_loss:0.711, val_acc:0.831]
Epoch [18/120    avg_loss:0.588, val_acc:0.781]
Epoch [19/120    avg_loss:0.535, val_acc:0.836]
Epoch [20/120    avg_loss:0.484, val_acc:0.800]
Epoch [21/120    avg_loss:0.404, val_acc:0.847]
Epoch [22/120    avg_loss:0.309, val_acc:0.863]
Epoch [23/120    avg_loss:0.299, val_acc:0.858]
Epoch [24/120    avg_loss:0.351, val_acc:0.816]
Epoch [25/120    avg_loss:0.355, val_acc:0.831]
Epoch [26/120    avg_loss:0.279, val_acc:0.897]
Epoch [27/120    avg_loss:0.224, val_acc:0.878]
Epoch [28/120    avg_loss:0.199, val_acc:0.905]
Epoch [29/120    avg_loss:0.198, val_acc:0.903]
Epoch [30/120    avg_loss:0.189, val_acc:0.914]
Epoch [31/120    avg_loss:0.153, val_acc:0.918]
Epoch [32/120    avg_loss:0.133, val_acc:0.935]
Epoch [33/120    avg_loss:0.128, val_acc:0.933]
Epoch [34/120    avg_loss:0.125, val_acc:0.916]
Epoch [35/120    avg_loss:0.130, val_acc:0.925]
Epoch [36/120    avg_loss:0.155, val_acc:0.907]
Epoch [37/120    avg_loss:0.115, val_acc:0.915]
Epoch [38/120    avg_loss:0.123, val_acc:0.935]
Epoch [39/120    avg_loss:0.179, val_acc:0.866]
Epoch [40/120    avg_loss:0.106, val_acc:0.940]
Epoch [41/120    avg_loss:0.088, val_acc:0.938]
Epoch [42/120    avg_loss:0.101, val_acc:0.922]
Epoch [43/120    avg_loss:0.183, val_acc:0.923]
Epoch [44/120    avg_loss:0.093, val_acc:0.951]
Epoch [45/120    avg_loss:0.085, val_acc:0.930]
Epoch [46/120    avg_loss:0.077, val_acc:0.935]
Epoch [47/120    avg_loss:0.065, val_acc:0.951]
Epoch [48/120    avg_loss:0.048, val_acc:0.947]
Epoch [49/120    avg_loss:0.078, val_acc:0.948]
Epoch [50/120    avg_loss:0.050, val_acc:0.945]
Epoch [51/120    avg_loss:0.063, val_acc:0.947]
Epoch [52/120    avg_loss:0.040, val_acc:0.954]
Epoch [53/120    avg_loss:0.047, val_acc:0.963]
Epoch [54/120    avg_loss:0.045, val_acc:0.943]
Epoch [55/120    avg_loss:0.046, val_acc:0.965]
Epoch [56/120    avg_loss:0.049, val_acc:0.943]
Epoch [57/120    avg_loss:0.060, val_acc:0.940]
Epoch [58/120    avg_loss:0.066, val_acc:0.951]
Epoch [59/120    avg_loss:0.084, val_acc:0.911]
Epoch [60/120    avg_loss:0.093, val_acc:0.943]
Epoch [61/120    avg_loss:0.050, val_acc:0.955]
Epoch [62/120    avg_loss:0.053, val_acc:0.947]
Epoch [63/120    avg_loss:0.039, val_acc:0.964]
Epoch [64/120    avg_loss:0.032, val_acc:0.952]
Epoch [65/120    avg_loss:0.032, val_acc:0.961]
Epoch [66/120    avg_loss:0.024, val_acc:0.969]
Epoch [67/120    avg_loss:0.023, val_acc:0.966]
Epoch [68/120    avg_loss:0.049, val_acc:0.952]
Epoch [69/120    avg_loss:0.041, val_acc:0.967]
Epoch [70/120    avg_loss:0.029, val_acc:0.954]
Epoch [71/120    avg_loss:0.032, val_acc:0.957]
Epoch [72/120    avg_loss:0.023, val_acc:0.959]
Epoch [73/120    avg_loss:0.024, val_acc:0.960]
Epoch [74/120    avg_loss:0.032, val_acc:0.965]
Epoch [75/120    avg_loss:0.021, val_acc:0.967]
Epoch [76/120    avg_loss:0.021, val_acc:0.972]
Epoch [77/120    avg_loss:0.024, val_acc:0.965]
Epoch [78/120    avg_loss:0.013, val_acc:0.970]
Epoch [79/120    avg_loss:0.014, val_acc:0.968]
Epoch [80/120    avg_loss:0.015, val_acc:0.961]
Epoch [81/120    avg_loss:0.018, val_acc:0.972]
Epoch [82/120    avg_loss:0.011, val_acc:0.972]
Epoch [83/120    avg_loss:0.010, val_acc:0.972]
Epoch [84/120    avg_loss:0.016, val_acc:0.964]
Epoch [85/120    avg_loss:0.014, val_acc:0.969]
Epoch [86/120    avg_loss:0.016, val_acc:0.972]
Epoch [87/120    avg_loss:0.016, val_acc:0.976]
Epoch [88/120    avg_loss:0.023, val_acc:0.955]
Epoch [89/120    avg_loss:0.024, val_acc:0.954]
Epoch [90/120    avg_loss:0.027, val_acc:0.975]
Epoch [91/120    avg_loss:0.016, val_acc:0.968]
Epoch [92/120    avg_loss:0.010, val_acc:0.975]
Epoch [93/120    avg_loss:0.011, val_acc:0.974]
Epoch [94/120    avg_loss:0.009, val_acc:0.971]
Epoch [95/120    avg_loss:0.008, val_acc:0.971]
Epoch [96/120    avg_loss:0.006, val_acc:0.974]
Epoch [97/120    avg_loss:0.018, val_acc:0.970]
Epoch [98/120    avg_loss:0.021, val_acc:0.973]
Epoch [99/120    avg_loss:0.020, val_acc:0.973]
Epoch [100/120    avg_loss:0.010, val_acc:0.971]
Epoch [101/120    avg_loss:0.007, val_acc:0.972]
Epoch [102/120    avg_loss:0.008, val_acc:0.969]
Epoch [103/120    avg_loss:0.008, val_acc:0.973]
Epoch [104/120    avg_loss:0.009, val_acc:0.971]
Epoch [105/120    avg_loss:0.008, val_acc:0.971]
Epoch [106/120    avg_loss:0.012, val_acc:0.973]
Epoch [107/120    avg_loss:0.007, val_acc:0.973]
Epoch [108/120    avg_loss:0.006, val_acc:0.973]
Epoch [109/120    avg_loss:0.010, val_acc:0.972]
Epoch [110/120    avg_loss:0.007, val_acc:0.972]
Epoch [111/120    avg_loss:0.009, val_acc:0.973]
Epoch [112/120    avg_loss:0.008, val_acc:0.974]
Epoch [113/120    avg_loss:0.006, val_acc:0.973]
Epoch [114/120    avg_loss:0.007, val_acc:0.973]
Epoch [115/120    avg_loss:0.007, val_acc:0.972]
Epoch [116/120    avg_loss:0.006, val_acc:0.972]
Epoch [117/120    avg_loss:0.007, val_acc:0.972]
Epoch [118/120    avg_loss:0.005, val_acc:0.973]
Epoch [119/120    avg_loss:0.009, val_acc:0.973]
Epoch [120/120    avg_loss:0.007, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1262    0    6    0    0    0    0    0    3   14    0    0
     0    0    0]
 [   0    0    0  725    2    0    0    0    0   10    5    1    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    3    0    0    0  856    9    1    0
     0    1    0]
 [   0    0    6    0    0    0    0    0    0    0   13 2178   13    0
     0    0    0]
 [   0    0    0    1    0    1    0    0    0    0    1    1  526    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1134    5    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    44  280    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.94037940379404

F1 scores:
[       nan 0.975      0.98670837 0.98438561 0.98156682 0.9954023
 0.9798357  0.97959184 0.9953271  0.7826087  0.97549858 0.98663647
 0.96958525 0.99728997 0.97716502 0.88467615 0.97005988]

Kappa:
0.9765136350826839
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feffd639ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.773, val_acc:0.246]
Epoch [2/120    avg_loss:2.664, val_acc:0.289]
Epoch [3/120    avg_loss:2.537, val_acc:0.385]
Epoch [4/120    avg_loss:2.418, val_acc:0.423]
Epoch [5/120    avg_loss:2.342, val_acc:0.463]
Epoch [6/120    avg_loss:2.222, val_acc:0.479]
Epoch [7/120    avg_loss:2.134, val_acc:0.524]
Epoch [8/120    avg_loss:2.040, val_acc:0.603]
Epoch [9/120    avg_loss:1.925, val_acc:0.591]
Epoch [10/120    avg_loss:1.874, val_acc:0.627]
Epoch [11/120    avg_loss:1.745, val_acc:0.642]
Epoch [12/120    avg_loss:1.604, val_acc:0.643]
Epoch [13/120    avg_loss:1.494, val_acc:0.678]
Epoch [14/120    avg_loss:1.389, val_acc:0.678]
Epoch [15/120    avg_loss:1.172, val_acc:0.721]
Epoch [16/120    avg_loss:0.977, val_acc:0.798]
Epoch [17/120    avg_loss:0.825, val_acc:0.748]
Epoch [18/120    avg_loss:0.795, val_acc:0.810]
Epoch [19/120    avg_loss:0.670, val_acc:0.860]
Epoch [20/120    avg_loss:0.586, val_acc:0.836]
Epoch [21/120    avg_loss:0.496, val_acc:0.877]
Epoch [22/120    avg_loss:0.493, val_acc:0.846]
Epoch [23/120    avg_loss:0.542, val_acc:0.810]
Epoch [24/120    avg_loss:0.408, val_acc:0.898]
Epoch [25/120    avg_loss:0.433, val_acc:0.874]
Epoch [26/120    avg_loss:0.389, val_acc:0.888]
Epoch [27/120    avg_loss:0.351, val_acc:0.899]
Epoch [28/120    avg_loss:0.239, val_acc:0.924]
Epoch [29/120    avg_loss:0.560, val_acc:0.549]
Epoch [30/120    avg_loss:1.123, val_acc:0.843]
Epoch [31/120    avg_loss:0.360, val_acc:0.897]
Epoch [32/120    avg_loss:0.238, val_acc:0.910]
Epoch [33/120    avg_loss:0.251, val_acc:0.928]
Epoch [34/120    avg_loss:0.153, val_acc:0.935]
Epoch [35/120    avg_loss:0.157, val_acc:0.921]
Epoch [36/120    avg_loss:0.141, val_acc:0.933]
Epoch [37/120    avg_loss:0.120, val_acc:0.948]
Epoch [38/120    avg_loss:0.104, val_acc:0.947]
Epoch [39/120    avg_loss:0.083, val_acc:0.951]
Epoch [40/120    avg_loss:0.099, val_acc:0.938]
Epoch [41/120    avg_loss:0.147, val_acc:0.940]
Epoch [42/120    avg_loss:0.093, val_acc:0.958]
Epoch [43/120    avg_loss:0.067, val_acc:0.949]
Epoch [44/120    avg_loss:0.083, val_acc:0.948]
Epoch [45/120    avg_loss:0.085, val_acc:0.931]
Epoch [46/120    avg_loss:0.071, val_acc:0.957]
Epoch [47/120    avg_loss:0.050, val_acc:0.963]
Epoch [48/120    avg_loss:0.042, val_acc:0.968]
Epoch [49/120    avg_loss:0.042, val_acc:0.967]
Epoch [50/120    avg_loss:0.062, val_acc:0.960]
Epoch [51/120    avg_loss:0.053, val_acc:0.953]
Epoch [52/120    avg_loss:0.130, val_acc:0.882]
Epoch [53/120    avg_loss:0.168, val_acc:0.923]
Epoch [54/120    avg_loss:0.391, val_acc:0.858]
Epoch [55/120    avg_loss:0.404, val_acc:0.887]
Epoch [56/120    avg_loss:0.168, val_acc:0.945]
Epoch [57/120    avg_loss:0.095, val_acc:0.946]
Epoch [58/120    avg_loss:0.070, val_acc:0.952]
Epoch [59/120    avg_loss:0.102, val_acc:0.958]
Epoch [60/120    avg_loss:0.056, val_acc:0.953]
Epoch [61/120    avg_loss:0.051, val_acc:0.957]
Epoch [62/120    avg_loss:0.051, val_acc:0.962]
Epoch [63/120    avg_loss:0.033, val_acc:0.963]
Epoch [64/120    avg_loss:0.033, val_acc:0.965]
Epoch [65/120    avg_loss:0.029, val_acc:0.966]
Epoch [66/120    avg_loss:0.032, val_acc:0.965]
Epoch [67/120    avg_loss:0.025, val_acc:0.966]
Epoch [68/120    avg_loss:0.028, val_acc:0.966]
Epoch [69/120    avg_loss:0.031, val_acc:0.968]
Epoch [70/120    avg_loss:0.026, val_acc:0.967]
Epoch [71/120    avg_loss:0.028, val_acc:0.968]
Epoch [72/120    avg_loss:0.028, val_acc:0.968]
Epoch [73/120    avg_loss:0.025, val_acc:0.968]
Epoch [74/120    avg_loss:0.027, val_acc:0.970]
Epoch [75/120    avg_loss:0.027, val_acc:0.972]
Epoch [76/120    avg_loss:0.025, val_acc:0.969]
Epoch [77/120    avg_loss:0.023, val_acc:0.975]
Epoch [78/120    avg_loss:0.026, val_acc:0.974]
Epoch [79/120    avg_loss:0.027, val_acc:0.972]
Epoch [80/120    avg_loss:0.027, val_acc:0.972]
Epoch [81/120    avg_loss:0.029, val_acc:0.967]
Epoch [82/120    avg_loss:0.021, val_acc:0.969]
Epoch [83/120    avg_loss:0.023, val_acc:0.966]
Epoch [84/120    avg_loss:0.024, val_acc:0.971]
Epoch [85/120    avg_loss:0.027, val_acc:0.971]
Epoch [86/120    avg_loss:0.025, val_acc:0.970]
Epoch [87/120    avg_loss:0.019, val_acc:0.972]
Epoch [88/120    avg_loss:0.029, val_acc:0.972]
Epoch [89/120    avg_loss:0.025, val_acc:0.975]
Epoch [90/120    avg_loss:0.025, val_acc:0.972]
Epoch [91/120    avg_loss:0.023, val_acc:0.972]
Epoch [92/120    avg_loss:0.032, val_acc:0.975]
Epoch [93/120    avg_loss:0.029, val_acc:0.974]
Epoch [94/120    avg_loss:0.030, val_acc:0.974]
Epoch [95/120    avg_loss:0.024, val_acc:0.974]
Epoch [96/120    avg_loss:0.022, val_acc:0.975]
Epoch [97/120    avg_loss:0.024, val_acc:0.976]
Epoch [98/120    avg_loss:0.025, val_acc:0.975]
Epoch [99/120    avg_loss:0.019, val_acc:0.976]
Epoch [100/120    avg_loss:0.023, val_acc:0.977]
Epoch [101/120    avg_loss:0.022, val_acc:0.977]
Epoch [102/120    avg_loss:0.022, val_acc:0.974]
Epoch [103/120    avg_loss:0.022, val_acc:0.974]
Epoch [104/120    avg_loss:0.022, val_acc:0.974]
Epoch [105/120    avg_loss:0.018, val_acc:0.971]
Epoch [106/120    avg_loss:0.021, val_acc:0.970]
Epoch [107/120    avg_loss:0.018, val_acc:0.971]
Epoch [108/120    avg_loss:0.021, val_acc:0.975]
Epoch [109/120    avg_loss:0.020, val_acc:0.976]
Epoch [110/120    avg_loss:0.019, val_acc:0.974]
Epoch [111/120    avg_loss:0.020, val_acc:0.971]
Epoch [112/120    avg_loss:0.019, val_acc:0.974]
Epoch [113/120    avg_loss:0.020, val_acc:0.974]
Epoch [114/120    avg_loss:0.018, val_acc:0.978]
Epoch [115/120    avg_loss:0.020, val_acc:0.977]
Epoch [116/120    avg_loss:0.019, val_acc:0.976]
Epoch [117/120    avg_loss:0.024, val_acc:0.972]
Epoch [118/120    avg_loss:0.018, val_acc:0.975]
Epoch [119/120    avg_loss:0.016, val_acc:0.972]
Epoch [120/120    avg_loss:0.015, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1255    2    0    0    0    0    0    1    4   23    0    0
     0    0    0]
 [   0    0    0  720    4    0    0    0    0    3    3   11    6    0
     0    0    0]
 [   0    0    0    0  207    1    0    0    0    0    0    0    5    0
     0    0    0]
 [   0    0    0    0    0  428    0    2    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   10    0    0    0    0    0    0  420    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  855   16    0    0
     0    0    0]
 [   0    0    8    0    0    0    2    0    0    0   15 2154   28    0
     0    3    0]
 [   0    0    0    2    0    0    0    0    0    0    4    0  523    0
     1    3    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
  1118   11    0]
 [   0    0    0    0    0    0   32    0    0    0    0    0    0    0
    48  267    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.06233062330624

F1 scores:
[       nan 0.87912088 0.98354232 0.9789259  0.97641509 0.99074074
 0.96759941 0.96153846 0.98823529 0.9        0.97324986 0.9759855
 0.95264117 1.         0.96754652 0.84627575 0.98203593]

Kappa:
0.966505084237407
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd1e9ff9ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.800, val_acc:0.220]
Epoch [2/120    avg_loss:2.700, val_acc:0.385]
Epoch [3/120    avg_loss:2.584, val_acc:0.410]
Epoch [4/120    avg_loss:2.479, val_acc:0.507]
Epoch [5/120    avg_loss:2.345, val_acc:0.541]
Epoch [6/120    avg_loss:2.203, val_acc:0.562]
Epoch [7/120    avg_loss:2.028, val_acc:0.572]
Epoch [8/120    avg_loss:1.889, val_acc:0.566]
Epoch [9/120    avg_loss:1.756, val_acc:0.608]
Epoch [10/120    avg_loss:1.646, val_acc:0.640]
Epoch [11/120    avg_loss:1.481, val_acc:0.688]
Epoch [12/120    avg_loss:1.315, val_acc:0.703]
Epoch [13/120    avg_loss:1.193, val_acc:0.739]
Epoch [14/120    avg_loss:1.105, val_acc:0.753]
Epoch [15/120    avg_loss:0.974, val_acc:0.650]
Epoch [16/120    avg_loss:0.864, val_acc:0.793]
Epoch [17/120    avg_loss:0.771, val_acc:0.809]
Epoch [18/120    avg_loss:0.719, val_acc:0.794]
Epoch [19/120    avg_loss:0.844, val_acc:0.309]
Epoch [20/120    avg_loss:1.843, val_acc:0.362]
Epoch [21/120    avg_loss:1.958, val_acc:0.511]
Epoch [22/120    avg_loss:1.807, val_acc:0.565]
Epoch [23/120    avg_loss:1.567, val_acc:0.551]
Epoch [24/120    avg_loss:1.415, val_acc:0.630]
Epoch [25/120    avg_loss:1.327, val_acc:0.662]
Epoch [26/120    avg_loss:1.154, val_acc:0.664]
Epoch [27/120    avg_loss:0.998, val_acc:0.671]
Epoch [28/120    avg_loss:0.976, val_acc:0.750]
Epoch [29/120    avg_loss:0.893, val_acc:0.737]
Epoch [30/120    avg_loss:0.795, val_acc:0.769]
Epoch [31/120    avg_loss:0.656, val_acc:0.789]
Epoch [32/120    avg_loss:0.636, val_acc:0.805]
Epoch [33/120    avg_loss:0.632, val_acc:0.793]
Epoch [34/120    avg_loss:0.613, val_acc:0.801]
Epoch [35/120    avg_loss:0.604, val_acc:0.814]
Epoch [36/120    avg_loss:0.598, val_acc:0.812]
Epoch [37/120    avg_loss:0.575, val_acc:0.806]
Epoch [38/120    avg_loss:0.577, val_acc:0.807]
Epoch [39/120    avg_loss:0.571, val_acc:0.812]
Epoch [40/120    avg_loss:0.525, val_acc:0.822]
Epoch [41/120    avg_loss:0.515, val_acc:0.819]
Epoch [42/120    avg_loss:0.523, val_acc:0.828]
Epoch [43/120    avg_loss:0.511, val_acc:0.827]
Epoch [44/120    avg_loss:0.499, val_acc:0.831]
Epoch [45/120    avg_loss:0.504, val_acc:0.825]
Epoch [46/120    avg_loss:0.519, val_acc:0.828]
Epoch [47/120    avg_loss:0.477, val_acc:0.840]
Epoch [48/120    avg_loss:0.484, val_acc:0.836]
Epoch [49/120    avg_loss:0.445, val_acc:0.845]
Epoch [50/120    avg_loss:0.480, val_acc:0.841]
Epoch [51/120    avg_loss:0.458, val_acc:0.851]
Epoch [52/120    avg_loss:0.447, val_acc:0.861]
Epoch [53/120    avg_loss:0.434, val_acc:0.853]
Epoch [54/120    avg_loss:0.418, val_acc:0.851]
Epoch [55/120    avg_loss:0.431, val_acc:0.865]
Epoch [56/120    avg_loss:0.407, val_acc:0.862]
Epoch [57/120    avg_loss:0.410, val_acc:0.859]
Epoch [58/120    avg_loss:0.405, val_acc:0.871]
Epoch [59/120    avg_loss:0.396, val_acc:0.868]
Epoch [60/120    avg_loss:0.372, val_acc:0.880]
Epoch [61/120    avg_loss:0.389, val_acc:0.883]
Epoch [62/120    avg_loss:0.360, val_acc:0.882]
Epoch [63/120    avg_loss:0.364, val_acc:0.887]
Epoch [64/120    avg_loss:0.358, val_acc:0.880]
Epoch [65/120    avg_loss:0.364, val_acc:0.890]
Epoch [66/120    avg_loss:0.344, val_acc:0.895]
Epoch [67/120    avg_loss:0.342, val_acc:0.896]
Epoch [68/120    avg_loss:0.319, val_acc:0.903]
Epoch [69/120    avg_loss:0.325, val_acc:0.908]
Epoch [70/120    avg_loss:0.320, val_acc:0.895]
Epoch [71/120    avg_loss:0.315, val_acc:0.895]
Epoch [72/120    avg_loss:0.298, val_acc:0.896]
Epoch [73/120    avg_loss:0.305, val_acc:0.912]
Epoch [74/120    avg_loss:0.286, val_acc:0.913]
Epoch [75/120    avg_loss:0.286, val_acc:0.908]
Epoch [76/120    avg_loss:0.305, val_acc:0.911]
Epoch [77/120    avg_loss:0.285, val_acc:0.919]
Epoch [78/120    avg_loss:0.275, val_acc:0.925]
Epoch [79/120    avg_loss:0.266, val_acc:0.926]
Epoch [80/120    avg_loss:0.279, val_acc:0.916]
Epoch [81/120    avg_loss:0.277, val_acc:0.913]
Epoch [82/120    avg_loss:0.257, val_acc:0.927]
Epoch [83/120    avg_loss:0.272, val_acc:0.924]
Epoch [84/120    avg_loss:0.256, val_acc:0.926]
Epoch [85/120    avg_loss:0.228, val_acc:0.918]
Epoch [86/120    avg_loss:0.254, val_acc:0.932]
Epoch [87/120    avg_loss:0.218, val_acc:0.929]
Epoch [88/120    avg_loss:0.236, val_acc:0.927]
Epoch [89/120    avg_loss:0.226, val_acc:0.918]
Epoch [90/120    avg_loss:0.221, val_acc:0.929]
Epoch [91/120    avg_loss:0.212, val_acc:0.941]
Epoch [92/120    avg_loss:0.221, val_acc:0.938]
Epoch [93/120    avg_loss:0.205, val_acc:0.943]
Epoch [94/120    avg_loss:0.203, val_acc:0.909]
Epoch [95/120    avg_loss:0.202, val_acc:0.943]
Epoch [96/120    avg_loss:0.193, val_acc:0.943]
Epoch [97/120    avg_loss:0.177, val_acc:0.939]
Epoch [98/120    avg_loss:0.213, val_acc:0.935]
Epoch [99/120    avg_loss:0.184, val_acc:0.941]
Epoch [100/120    avg_loss:0.196, val_acc:0.942]
Epoch [101/120    avg_loss:0.197, val_acc:0.931]
Epoch [102/120    avg_loss:0.192, val_acc:0.944]
Epoch [103/120    avg_loss:0.187, val_acc:0.933]
Epoch [104/120    avg_loss:0.188, val_acc:0.931]
Epoch [105/120    avg_loss:0.175, val_acc:0.945]
Epoch [106/120    avg_loss:0.175, val_acc:0.952]
Epoch [107/120    avg_loss:0.182, val_acc:0.949]
Epoch [108/120    avg_loss:0.161, val_acc:0.956]
Epoch [109/120    avg_loss:0.166, val_acc:0.944]
Epoch [110/120    avg_loss:0.166, val_acc:0.941]
Epoch [111/120    avg_loss:0.156, val_acc:0.935]
Epoch [112/120    avg_loss:0.154, val_acc:0.948]
Epoch [113/120    avg_loss:0.157, val_acc:0.956]
Epoch [114/120    avg_loss:0.158, val_acc:0.930]
Epoch [115/120    avg_loss:0.153, val_acc:0.954]
Epoch [116/120    avg_loss:0.150, val_acc:0.950]
Epoch [117/120    avg_loss:0.147, val_acc:0.951]
Epoch [118/120    avg_loss:0.143, val_acc:0.949]
Epoch [119/120    avg_loss:0.162, val_acc:0.943]
Epoch [120/120    avg_loss:0.159, val_acc:0.952]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1252    0    2    0    0    0    0    0    6   22    0    0
     0    3    0]
 [   0    0   33  690    0    0    0    0    0    3    0   13    8    0
     0    0    0]
 [   0    0    0    0  211    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  405    0   19    0    0    0    0    0    0
    11    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   50    0    0    8    3    0    0    3  801    0    0    0
     0   10    0]
 [   0    0   94    9    0    3    0    1    0    5   39 2038   12    0
     0    4    5]
 [   0    0    0    1    0    0    0    0    0    0    6    0  522    0
     1    2    2]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0  183
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1109   29    0]
 [   0    0    0    0    0    0   17    0    0    1    0    0    0    0
     9  320    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.19783197831978

F1 scores:
[       nan 1.         0.92262343 0.9536973  0.99061033 0.94847775
 0.98277154 0.71428571 1.         0.69565217 0.92762015 0.95166939
 0.96756256 0.99456522 0.97709251 0.8951049  0.95402299]

Kappa:
0.9453689519910369
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3072908f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.805, val_acc:0.298]
Epoch [2/120    avg_loss:2.707, val_acc:0.390]
Epoch [3/120    avg_loss:2.611, val_acc:0.472]
Epoch [4/120    avg_loss:2.506, val_acc:0.522]
Epoch [5/120    avg_loss:2.404, val_acc:0.579]
Epoch [6/120    avg_loss:2.284, val_acc:0.576]
Epoch [7/120    avg_loss:2.155, val_acc:0.568]
Epoch [8/120    avg_loss:2.047, val_acc:0.605]
Epoch [9/120    avg_loss:1.949, val_acc:0.633]
Epoch [10/120    avg_loss:1.826, val_acc:0.626]
Epoch [11/120    avg_loss:1.699, val_acc:0.678]
Epoch [12/120    avg_loss:1.560, val_acc:0.652]
Epoch [13/120    avg_loss:1.432, val_acc:0.707]
Epoch [14/120    avg_loss:1.305, val_acc:0.704]
Epoch [15/120    avg_loss:1.124, val_acc:0.766]
Epoch [16/120    avg_loss:1.043, val_acc:0.746]
Epoch [17/120    avg_loss:0.912, val_acc:0.753]
Epoch [18/120    avg_loss:0.801, val_acc:0.824]
Epoch [19/120    avg_loss:0.700, val_acc:0.834]
Epoch [20/120    avg_loss:0.618, val_acc:0.809]
Epoch [21/120    avg_loss:0.561, val_acc:0.810]
Epoch [22/120    avg_loss:0.489, val_acc:0.820]
Epoch [23/120    avg_loss:0.520, val_acc:0.830]
Epoch [24/120    avg_loss:0.502, val_acc:0.852]
Epoch [25/120    avg_loss:0.417, val_acc:0.875]
Epoch [26/120    avg_loss:0.329, val_acc:0.877]
Epoch [27/120    avg_loss:0.341, val_acc:0.892]
Epoch [28/120    avg_loss:0.267, val_acc:0.904]
Epoch [29/120    avg_loss:0.283, val_acc:0.822]
Epoch [30/120    avg_loss:0.240, val_acc:0.919]
Epoch [31/120    avg_loss:0.202, val_acc:0.878]
Epoch [32/120    avg_loss:0.204, val_acc:0.884]
Epoch [33/120    avg_loss:0.176, val_acc:0.879]
Epoch [34/120    avg_loss:0.140, val_acc:0.932]
Epoch [35/120    avg_loss:0.171, val_acc:0.877]
Epoch [36/120    avg_loss:0.202, val_acc:0.905]
Epoch [37/120    avg_loss:0.144, val_acc:0.902]
Epoch [38/120    avg_loss:0.107, val_acc:0.935]
Epoch [39/120    avg_loss:0.112, val_acc:0.946]
Epoch [40/120    avg_loss:0.107, val_acc:0.912]
Epoch [41/120    avg_loss:0.108, val_acc:0.925]
Epoch [42/120    avg_loss:0.092, val_acc:0.942]
Epoch [43/120    avg_loss:0.107, val_acc:0.936]
Epoch [44/120    avg_loss:0.093, val_acc:0.944]
Epoch [45/120    avg_loss:0.073, val_acc:0.939]
Epoch [46/120    avg_loss:0.076, val_acc:0.958]
Epoch [47/120    avg_loss:0.061, val_acc:0.951]
Epoch [48/120    avg_loss:0.078, val_acc:0.953]
Epoch [49/120    avg_loss:0.061, val_acc:0.963]
Epoch [50/120    avg_loss:0.049, val_acc:0.960]
Epoch [51/120    avg_loss:0.040, val_acc:0.965]
Epoch [52/120    avg_loss:0.040, val_acc:0.964]
Epoch [53/120    avg_loss:0.038, val_acc:0.957]
Epoch [54/120    avg_loss:0.044, val_acc:0.960]
Epoch [55/120    avg_loss:0.033, val_acc:0.968]
Epoch [56/120    avg_loss:0.038, val_acc:0.945]
Epoch [57/120    avg_loss:0.053, val_acc:0.965]
Epoch [58/120    avg_loss:0.044, val_acc:0.957]
Epoch [59/120    avg_loss:1.433, val_acc:0.801]
Epoch [60/120    avg_loss:0.295, val_acc:0.938]
Epoch [61/120    avg_loss:0.124, val_acc:0.935]
Epoch [62/120    avg_loss:0.078, val_acc:0.955]
Epoch [63/120    avg_loss:0.061, val_acc:0.960]
Epoch [64/120    avg_loss:0.087, val_acc:0.936]
Epoch [65/120    avg_loss:0.073, val_acc:0.946]
Epoch [66/120    avg_loss:0.054, val_acc:0.955]
Epoch [67/120    avg_loss:0.050, val_acc:0.956]
Epoch [68/120    avg_loss:0.036, val_acc:0.973]
Epoch [69/120    avg_loss:0.036, val_acc:0.974]
Epoch [70/120    avg_loss:0.030, val_acc:0.969]
Epoch [71/120    avg_loss:0.028, val_acc:0.972]
Epoch [72/120    avg_loss:0.032, val_acc:0.965]
Epoch [73/120    avg_loss:0.039, val_acc:0.963]
Epoch [74/120    avg_loss:0.048, val_acc:0.972]
Epoch [75/120    avg_loss:0.031, val_acc:0.973]
Epoch [76/120    avg_loss:0.022, val_acc:0.970]
Epoch [77/120    avg_loss:0.021, val_acc:0.968]
Epoch [78/120    avg_loss:0.024, val_acc:0.968]
Epoch [79/120    avg_loss:0.019, val_acc:0.974]
Epoch [80/120    avg_loss:0.018, val_acc:0.975]
Epoch [81/120    avg_loss:0.014, val_acc:0.974]
Epoch [82/120    avg_loss:0.034, val_acc:0.964]
Epoch [83/120    avg_loss:0.036, val_acc:0.972]
Epoch [84/120    avg_loss:0.018, val_acc:0.969]
Epoch [85/120    avg_loss:0.022, val_acc:0.956]
Epoch [86/120    avg_loss:0.016, val_acc:0.972]
Epoch [87/120    avg_loss:0.015, val_acc:0.960]
Epoch [88/120    avg_loss:0.023, val_acc:0.963]
Epoch [89/120    avg_loss:0.015, val_acc:0.968]
Epoch [90/120    avg_loss:0.012, val_acc:0.970]
Epoch [91/120    avg_loss:0.014, val_acc:0.974]
Epoch [92/120    avg_loss:0.011, val_acc:0.974]
Epoch [93/120    avg_loss:0.012, val_acc:0.977]
Epoch [94/120    avg_loss:0.015, val_acc:0.976]
Epoch [95/120    avg_loss:0.009, val_acc:0.974]
Epoch [96/120    avg_loss:0.010, val_acc:0.975]
Epoch [97/120    avg_loss:0.016, val_acc:0.971]
Epoch [98/120    avg_loss:0.015, val_acc:0.978]
Epoch [99/120    avg_loss:0.011, val_acc:0.968]
Epoch [100/120    avg_loss:0.096, val_acc:0.941]
Epoch [101/120    avg_loss:0.129, val_acc:0.955]
Epoch [102/120    avg_loss:0.053, val_acc:0.963]
Epoch [103/120    avg_loss:0.310, val_acc:0.354]
Epoch [104/120    avg_loss:0.710, val_acc:0.905]
Epoch [105/120    avg_loss:0.123, val_acc:0.955]
Epoch [106/120    avg_loss:0.071, val_acc:0.966]
Epoch [107/120    avg_loss:0.049, val_acc:0.966]
Epoch [108/120    avg_loss:0.036, val_acc:0.956]
Epoch [109/120    avg_loss:0.030, val_acc:0.972]
Epoch [110/120    avg_loss:0.026, val_acc:0.959]
Epoch [111/120    avg_loss:0.019, val_acc:0.980]
Epoch [112/120    avg_loss:0.017, val_acc:0.974]
Epoch [113/120    avg_loss:0.017, val_acc:0.978]
Epoch [114/120    avg_loss:0.016, val_acc:0.970]
Epoch [115/120    avg_loss:0.017, val_acc:0.973]
Epoch [116/120    avg_loss:0.028, val_acc:0.971]
Epoch [117/120    avg_loss:0.020, val_acc:0.972]
Epoch [118/120    avg_loss:0.014, val_acc:0.973]
Epoch [119/120    avg_loss:0.011, val_acc:0.979]
Epoch [120/120    avg_loss:0.008, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1268    0   10    0    0    0    0    0    0    7    0    0
     0    0    0]
 [   0    0    0  738    1    0    0    0    0    2    1    0    4    0
     1    0    0]
 [   0    0    0    0  210    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  858   11    1    0
     0    0    0]
 [   0    0   10    0    0    0    0    0    0    0   64 2112   24    0
     0    0    0]
 [   0    0    0    7    0    0    0    0    0    0    5    3  512    0
     0    4    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1136    3    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    71  274    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.24661246612466

F1 scores:
[       nan 0.98765432 0.98753894 0.98861353 0.96774194 0.99421965
 0.99542683 1.         1.         0.91891892 0.95121951 0.97192821
 0.94902688 1.         0.96557586 0.87261146 0.97647059]

Kappa:
0.9686310922738949
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f65ce7e8ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.807, val_acc:0.302]
Epoch [2/120    avg_loss:2.713, val_acc:0.413]
Epoch [3/120    avg_loss:2.603, val_acc:0.445]
Epoch [4/120    avg_loss:2.476, val_acc:0.471]
Epoch [5/120    avg_loss:2.382, val_acc:0.487]
Epoch [6/120    avg_loss:2.282, val_acc:0.515]
Epoch [7/120    avg_loss:2.180, val_acc:0.561]
Epoch [8/120    avg_loss:2.100, val_acc:0.558]
Epoch [9/120    avg_loss:2.024, val_acc:0.608]
Epoch [10/120    avg_loss:1.865, val_acc:0.618]
Epoch [11/120    avg_loss:1.762, val_acc:0.643]
Epoch [12/120    avg_loss:1.634, val_acc:0.672]
Epoch [13/120    avg_loss:1.518, val_acc:0.674]
Epoch [14/120    avg_loss:1.394, val_acc:0.690]
Epoch [15/120    avg_loss:1.234, val_acc:0.710]
Epoch [16/120    avg_loss:1.140, val_acc:0.779]
Epoch [17/120    avg_loss:1.030, val_acc:0.753]
Epoch [18/120    avg_loss:0.851, val_acc:0.751]
Epoch [19/120    avg_loss:0.888, val_acc:0.749]
Epoch [20/120    avg_loss:0.854, val_acc:0.788]
Epoch [21/120    avg_loss:0.684, val_acc:0.839]
Epoch [22/120    avg_loss:0.595, val_acc:0.816]
Epoch [23/120    avg_loss:0.583, val_acc:0.815]
Epoch [24/120    avg_loss:0.519, val_acc:0.851]
Epoch [25/120    avg_loss:0.556, val_acc:0.811]
Epoch [26/120    avg_loss:0.525, val_acc:0.733]
Epoch [27/120    avg_loss:0.525, val_acc:0.837]
Epoch [28/120    avg_loss:0.440, val_acc:0.875]
Epoch [29/120    avg_loss:0.291, val_acc:0.881]
Epoch [30/120    avg_loss:0.266, val_acc:0.889]
Epoch [31/120    avg_loss:0.237, val_acc:0.910]
Epoch [32/120    avg_loss:0.203, val_acc:0.908]
Epoch [33/120    avg_loss:0.198, val_acc:0.913]
Epoch [34/120    avg_loss:0.157, val_acc:0.927]
Epoch [35/120    avg_loss:0.135, val_acc:0.910]
Epoch [36/120    avg_loss:0.125, val_acc:0.929]
Epoch [37/120    avg_loss:0.115, val_acc:0.930]
Epoch [38/120    avg_loss:0.103, val_acc:0.951]
Epoch [39/120    avg_loss:0.094, val_acc:0.919]
Epoch [40/120    avg_loss:0.149, val_acc:0.933]
Epoch [41/120    avg_loss:0.127, val_acc:0.927]
Epoch [42/120    avg_loss:0.121, val_acc:0.874]
Epoch [43/120    avg_loss:0.125, val_acc:0.935]
Epoch [44/120    avg_loss:0.146, val_acc:0.959]
Epoch [45/120    avg_loss:0.093, val_acc:0.950]
Epoch [46/120    avg_loss:0.069, val_acc:0.956]
Epoch [47/120    avg_loss:0.067, val_acc:0.943]
Epoch [48/120    avg_loss:0.073, val_acc:0.949]
Epoch [49/120    avg_loss:0.062, val_acc:0.959]
Epoch [50/120    avg_loss:0.056, val_acc:0.943]
Epoch [51/120    avg_loss:0.049, val_acc:0.966]
Epoch [52/120    avg_loss:0.055, val_acc:0.951]
Epoch [53/120    avg_loss:0.075, val_acc:0.944]
Epoch [54/120    avg_loss:0.065, val_acc:0.949]
Epoch [55/120    avg_loss:0.072, val_acc:0.956]
Epoch [56/120    avg_loss:0.152, val_acc:0.927]
Epoch [57/120    avg_loss:0.178, val_acc:0.938]
Epoch [58/120    avg_loss:0.076, val_acc:0.954]
Epoch [59/120    avg_loss:0.071, val_acc:0.947]
Epoch [60/120    avg_loss:0.089, val_acc:0.939]
Epoch [61/120    avg_loss:0.067, val_acc:0.941]
Epoch [62/120    avg_loss:0.055, val_acc:0.960]
Epoch [63/120    avg_loss:0.039, val_acc:0.972]
Epoch [64/120    avg_loss:0.045, val_acc:0.957]
Epoch [65/120    avg_loss:0.051, val_acc:0.957]
Epoch [66/120    avg_loss:0.032, val_acc:0.956]
Epoch [67/120    avg_loss:0.043, val_acc:0.962]
Epoch [68/120    avg_loss:0.036, val_acc:0.956]
Epoch [69/120    avg_loss:0.030, val_acc:0.967]
Epoch [70/120    avg_loss:0.022, val_acc:0.968]
Epoch [71/120    avg_loss:0.022, val_acc:0.964]
Epoch [72/120    avg_loss:0.032, val_acc:0.960]
Epoch [73/120    avg_loss:0.027, val_acc:0.969]
Epoch [74/120    avg_loss:0.016, val_acc:0.971]
Epoch [75/120    avg_loss:0.013, val_acc:0.967]
Epoch [76/120    avg_loss:0.015, val_acc:0.961]
Epoch [77/120    avg_loss:0.015, val_acc:0.972]
Epoch [78/120    avg_loss:0.012, val_acc:0.971]
Epoch [79/120    avg_loss:0.012, val_acc:0.974]
Epoch [80/120    avg_loss:0.012, val_acc:0.971]
Epoch [81/120    avg_loss:0.011, val_acc:0.972]
Epoch [82/120    avg_loss:0.013, val_acc:0.972]
Epoch [83/120    avg_loss:0.010, val_acc:0.974]
Epoch [84/120    avg_loss:0.015, val_acc:0.972]
Epoch [85/120    avg_loss:0.012, val_acc:0.974]
Epoch [86/120    avg_loss:0.010, val_acc:0.972]
Epoch [87/120    avg_loss:0.015, val_acc:0.975]
Epoch [88/120    avg_loss:0.010, val_acc:0.975]
Epoch [89/120    avg_loss:0.013, val_acc:0.970]
Epoch [90/120    avg_loss:0.009, val_acc:0.970]
Epoch [91/120    avg_loss:0.010, val_acc:0.970]
Epoch [92/120    avg_loss:0.012, val_acc:0.972]
Epoch [93/120    avg_loss:0.012, val_acc:0.972]
Epoch [94/120    avg_loss:0.011, val_acc:0.976]
Epoch [95/120    avg_loss:0.010, val_acc:0.972]
Epoch [96/120    avg_loss:0.011, val_acc:0.971]
Epoch [97/120    avg_loss:0.010, val_acc:0.974]
Epoch [98/120    avg_loss:0.009, val_acc:0.975]
Epoch [99/120    avg_loss:0.011, val_acc:0.976]
Epoch [100/120    avg_loss:0.009, val_acc:0.976]
Epoch [101/120    avg_loss:0.009, val_acc:0.974]
Epoch [102/120    avg_loss:0.008, val_acc:0.972]
Epoch [103/120    avg_loss:0.008, val_acc:0.972]
Epoch [104/120    avg_loss:0.010, val_acc:0.974]
Epoch [105/120    avg_loss:0.009, val_acc:0.975]
Epoch [106/120    avg_loss:0.009, val_acc:0.971]
Epoch [107/120    avg_loss:0.010, val_acc:0.974]
Epoch [108/120    avg_loss:0.008, val_acc:0.975]
Epoch [109/120    avg_loss:0.009, val_acc:0.974]
Epoch [110/120    avg_loss:0.010, val_acc:0.974]
Epoch [111/120    avg_loss:0.010, val_acc:0.972]
Epoch [112/120    avg_loss:0.009, val_acc:0.974]
Epoch [113/120    avg_loss:0.011, val_acc:0.974]
Epoch [114/120    avg_loss:0.008, val_acc:0.974]
Epoch [115/120    avg_loss:0.010, val_acc:0.974]
Epoch [116/120    avg_loss:0.008, val_acc:0.974]
Epoch [117/120    avg_loss:0.010, val_acc:0.974]
Epoch [118/120    avg_loss:0.010, val_acc:0.974]
Epoch [119/120    avg_loss:0.009, val_acc:0.974]
Epoch [120/120    avg_loss:0.010, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1259    0    5    1    0    0    0    0    5   15    0    0
     0    0    0]
 [   0    0    0  716   11    3    0    0    0    1    3    3    9    0
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    0    0    0    0    0    0    0
    10    1    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    1    1    0    0    0  851   18    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    1    0    0   18 2188    1    0
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    3    2  526    0
     1    0    1]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1133    5    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    91  253    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.53929539295393

F1 scores:
[       nan 0.98765432 0.98745098 0.97814208 0.96145125 0.97921478
 0.99544765 0.98039216 1.         0.94444444 0.96869664 0.9864743
 0.98317757 0.99728997 0.9537037  0.8349835  0.99408284]

Kappa:
0.9719158238217463
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f78d4c18f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.804, val_acc:0.115]
Epoch [2/120    avg_loss:2.724, val_acc:0.343]
Epoch [3/120    avg_loss:2.628, val_acc:0.422]
Epoch [4/120    avg_loss:2.516, val_acc:0.445]
Epoch [5/120    avg_loss:2.418, val_acc:0.354]
Epoch [6/120    avg_loss:2.271, val_acc:0.452]
Epoch [7/120    avg_loss:2.179, val_acc:0.487]
Epoch [8/120    avg_loss:2.051, val_acc:0.516]
Epoch [9/120    avg_loss:1.975, val_acc:0.561]
Epoch [10/120    avg_loss:1.834, val_acc:0.605]
Epoch [11/120    avg_loss:1.699, val_acc:0.672]
Epoch [12/120    avg_loss:1.544, val_acc:0.706]
Epoch [13/120    avg_loss:1.429, val_acc:0.700]
Epoch [14/120    avg_loss:1.236, val_acc:0.718]
Epoch [15/120    avg_loss:1.130, val_acc:0.754]
Epoch [16/120    avg_loss:1.030, val_acc:0.766]
Epoch [17/120    avg_loss:0.895, val_acc:0.730]
Epoch [18/120    avg_loss:0.772, val_acc:0.802]
Epoch [19/120    avg_loss:0.654, val_acc:0.879]
Epoch [20/120    avg_loss:0.605, val_acc:0.842]
Epoch [21/120    avg_loss:0.501, val_acc:0.873]
Epoch [22/120    avg_loss:0.500, val_acc:0.889]
Epoch [23/120    avg_loss:0.495, val_acc:0.879]
Epoch [24/120    avg_loss:0.399, val_acc:0.880]
Epoch [25/120    avg_loss:0.346, val_acc:0.880]
Epoch [26/120    avg_loss:0.280, val_acc:0.906]
Epoch [27/120    avg_loss:0.272, val_acc:0.919]
Epoch [28/120    avg_loss:0.293, val_acc:0.908]
Epoch [29/120    avg_loss:0.240, val_acc:0.909]
Epoch [30/120    avg_loss:0.233, val_acc:0.929]
Epoch [31/120    avg_loss:0.167, val_acc:0.854]
Epoch [32/120    avg_loss:0.152, val_acc:0.929]
Epoch [33/120    avg_loss:0.172, val_acc:0.943]
Epoch [34/120    avg_loss:0.124, val_acc:0.944]
Epoch [35/120    avg_loss:0.120, val_acc:0.922]
Epoch [36/120    avg_loss:0.120, val_acc:0.936]
Epoch [37/120    avg_loss:0.126, val_acc:0.954]
Epoch [38/120    avg_loss:0.103, val_acc:0.942]
Epoch [39/120    avg_loss:0.086, val_acc:0.953]
Epoch [40/120    avg_loss:0.102, val_acc:0.968]
Epoch [41/120    avg_loss:0.075, val_acc:0.947]
Epoch [42/120    avg_loss:0.063, val_acc:0.954]
Epoch [43/120    avg_loss:0.055, val_acc:0.967]
Epoch [44/120    avg_loss:0.065, val_acc:0.954]
Epoch [45/120    avg_loss:0.086, val_acc:0.958]
Epoch [46/120    avg_loss:0.056, val_acc:0.957]
Epoch [47/120    avg_loss:0.061, val_acc:0.960]
Epoch [48/120    avg_loss:0.050, val_acc:0.962]
Epoch [49/120    avg_loss:0.057, val_acc:0.964]
Epoch [50/120    avg_loss:0.029, val_acc:0.975]
Epoch [51/120    avg_loss:0.025, val_acc:0.967]
Epoch [52/120    avg_loss:0.035, val_acc:0.975]
Epoch [53/120    avg_loss:0.053, val_acc:0.971]
Epoch [54/120    avg_loss:0.156, val_acc:0.885]
Epoch [55/120    avg_loss:0.128, val_acc:0.948]
Epoch [56/120    avg_loss:0.082, val_acc:0.962]
Epoch [57/120    avg_loss:0.051, val_acc:0.953]
Epoch [58/120    avg_loss:0.056, val_acc:0.952]
Epoch [59/120    avg_loss:0.046, val_acc:0.971]
Epoch [60/120    avg_loss:0.035, val_acc:0.985]
Epoch [61/120    avg_loss:0.037, val_acc:0.964]
Epoch [62/120    avg_loss:0.051, val_acc:0.954]
Epoch [63/120    avg_loss:0.037, val_acc:0.958]
Epoch [64/120    avg_loss:0.032, val_acc:0.962]
Epoch [65/120    avg_loss:0.041, val_acc:0.950]
Epoch [66/120    avg_loss:0.048, val_acc:0.974]
Epoch [67/120    avg_loss:0.056, val_acc:0.966]
Epoch [68/120    avg_loss:0.026, val_acc:0.977]
Epoch [69/120    avg_loss:0.022, val_acc:0.975]
Epoch [70/120    avg_loss:0.015, val_acc:0.980]
Epoch [71/120    avg_loss:0.021, val_acc:0.977]
Epoch [72/120    avg_loss:0.015, val_acc:0.977]
Epoch [73/120    avg_loss:0.013, val_acc:0.977]
Epoch [74/120    avg_loss:0.015, val_acc:0.978]
Epoch [75/120    avg_loss:0.011, val_acc:0.978]
Epoch [76/120    avg_loss:0.011, val_acc:0.977]
Epoch [77/120    avg_loss:0.010, val_acc:0.977]
Epoch [78/120    avg_loss:0.009, val_acc:0.979]
Epoch [79/120    avg_loss:0.009, val_acc:0.979]
Epoch [80/120    avg_loss:0.010, val_acc:0.979]
Epoch [81/120    avg_loss:0.011, val_acc:0.981]
Epoch [82/120    avg_loss:0.010, val_acc:0.982]
Epoch [83/120    avg_loss:0.013, val_acc:0.983]
Epoch [84/120    avg_loss:0.010, val_acc:0.981]
Epoch [85/120    avg_loss:0.010, val_acc:0.982]
Epoch [86/120    avg_loss:0.011, val_acc:0.983]
Epoch [87/120    avg_loss:0.008, val_acc:0.983]
Epoch [88/120    avg_loss:0.010, val_acc:0.983]
Epoch [89/120    avg_loss:0.009, val_acc:0.983]
Epoch [90/120    avg_loss:0.010, val_acc:0.983]
Epoch [91/120    avg_loss:0.021, val_acc:0.983]
Epoch [92/120    avg_loss:0.011, val_acc:0.982]
Epoch [93/120    avg_loss:0.010, val_acc:0.983]
Epoch [94/120    avg_loss:0.009, val_acc:0.983]
Epoch [95/120    avg_loss:0.010, val_acc:0.984]
Epoch [96/120    avg_loss:0.010, val_acc:0.984]
Epoch [97/120    avg_loss:0.010, val_acc:0.984]
Epoch [98/120    avg_loss:0.010, val_acc:0.984]
Epoch [99/120    avg_loss:0.010, val_acc:0.984]
Epoch [100/120    avg_loss:0.010, val_acc:0.984]
Epoch [101/120    avg_loss:0.010, val_acc:0.984]
Epoch [102/120    avg_loss:0.010, val_acc:0.984]
Epoch [103/120    avg_loss:0.011, val_acc:0.984]
Epoch [104/120    avg_loss:0.017, val_acc:0.984]
Epoch [105/120    avg_loss:0.011, val_acc:0.984]
Epoch [106/120    avg_loss:0.010, val_acc:0.984]
Epoch [107/120    avg_loss:0.009, val_acc:0.984]
Epoch [108/120    avg_loss:0.010, val_acc:0.984]
Epoch [109/120    avg_loss:0.008, val_acc:0.984]
Epoch [110/120    avg_loss:0.016, val_acc:0.984]
Epoch [111/120    avg_loss:0.009, val_acc:0.984]
Epoch [112/120    avg_loss:0.011, val_acc:0.984]
Epoch [113/120    avg_loss:0.008, val_acc:0.984]
Epoch [114/120    avg_loss:0.010, val_acc:0.984]
Epoch [115/120    avg_loss:0.011, val_acc:0.984]
Epoch [116/120    avg_loss:0.013, val_acc:0.984]
Epoch [117/120    avg_loss:0.010, val_acc:0.984]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.984]
Epoch [120/120    avg_loss:0.010, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1250    1    6    0    1    0    0    1    7   19    0    0
     0    0    0]
 [   0    0    1  702    8    6    2    0    0    5    3    0   19    1
     0    0    0]
 [   0    0    0    6  207    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    1    0    0    0    0  851   13    1    0
     0    0    0]
 [   0    0   13    0    0    1    0    0    2    0    7 2182    4    0
     1    0    0]
 [   0    0    0    0    0    2    0    0    0    0    1    0  528    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1135    3    0]
 [   0    0    1    0    0    0   18    0    0    0    0    0    0    0
    59  269    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.42005420054201

F1 scores:
[       nan 1.         0.97694412 0.96428571 0.95391705 0.98285714
 0.98206278 1.         0.99534884 0.8        0.97591743 0.98621469
 0.96791934 0.99730458 0.97050021 0.86914378 0.97005988]

Kappa:
0.9705709643084929
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fefb2b71f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.787, val_acc:0.229]
Epoch [2/120    avg_loss:2.688, val_acc:0.318]
Epoch [3/120    avg_loss:2.548, val_acc:0.299]
Epoch [4/120    avg_loss:2.400, val_acc:0.337]
Epoch [5/120    avg_loss:2.332, val_acc:0.387]
Epoch [6/120    avg_loss:2.218, val_acc:0.373]
Epoch [7/120    avg_loss:2.166, val_acc:0.407]
Epoch [8/120    avg_loss:2.063, val_acc:0.501]
Epoch [9/120    avg_loss:1.927, val_acc:0.583]
Epoch [10/120    avg_loss:1.833, val_acc:0.647]
Epoch [11/120    avg_loss:1.678, val_acc:0.650]
Epoch [12/120    avg_loss:1.475, val_acc:0.698]
Epoch [13/120    avg_loss:1.343, val_acc:0.713]
Epoch [14/120    avg_loss:1.200, val_acc:0.737]
Epoch [15/120    avg_loss:1.110, val_acc:0.730]
Epoch [16/120    avg_loss:0.938, val_acc:0.784]
Epoch [17/120    avg_loss:0.796, val_acc:0.768]
Epoch [18/120    avg_loss:0.725, val_acc:0.784]
Epoch [19/120    avg_loss:0.707, val_acc:0.837]
Epoch [20/120    avg_loss:0.570, val_acc:0.837]
Epoch [21/120    avg_loss:0.508, val_acc:0.850]
Epoch [22/120    avg_loss:0.463, val_acc:0.857]
Epoch [23/120    avg_loss:0.435, val_acc:0.869]
Epoch [24/120    avg_loss:0.411, val_acc:0.889]
Epoch [25/120    avg_loss:0.389, val_acc:0.870]
Epoch [26/120    avg_loss:0.349, val_acc:0.910]
Epoch [27/120    avg_loss:0.254, val_acc:0.907]
Epoch [28/120    avg_loss:0.228, val_acc:0.932]
Epoch [29/120    avg_loss:0.214, val_acc:0.906]
Epoch [30/120    avg_loss:0.213, val_acc:0.929]
Epoch [31/120    avg_loss:0.153, val_acc:0.929]
Epoch [32/120    avg_loss:0.157, val_acc:0.925]
Epoch [33/120    avg_loss:0.145, val_acc:0.923]
Epoch [34/120    avg_loss:0.129, val_acc:0.935]
Epoch [35/120    avg_loss:0.122, val_acc:0.946]
Epoch [36/120    avg_loss:0.135, val_acc:0.951]
Epoch [37/120    avg_loss:0.101, val_acc:0.941]
Epoch [38/120    avg_loss:0.240, val_acc:0.928]
Epoch [39/120    avg_loss:0.195, val_acc:0.924]
Epoch [40/120    avg_loss:0.145, val_acc:0.946]
Epoch [41/120    avg_loss:0.102, val_acc:0.967]
Epoch [42/120    avg_loss:0.090, val_acc:0.954]
Epoch [43/120    avg_loss:0.106, val_acc:0.957]
Epoch [44/120    avg_loss:0.095, val_acc:0.945]
Epoch [45/120    avg_loss:0.084, val_acc:0.968]
Epoch [46/120    avg_loss:0.082, val_acc:0.927]
Epoch [47/120    avg_loss:0.098, val_acc:0.947]
Epoch [48/120    avg_loss:0.074, val_acc:0.966]
Epoch [49/120    avg_loss:0.076, val_acc:0.961]
Epoch [50/120    avg_loss:0.072, val_acc:0.972]
Epoch [51/120    avg_loss:0.064, val_acc:0.967]
Epoch [52/120    avg_loss:0.067, val_acc:0.972]
Epoch [53/120    avg_loss:0.078, val_acc:0.972]
Epoch [54/120    avg_loss:0.060, val_acc:0.966]
Epoch [55/120    avg_loss:0.039, val_acc:0.981]
Epoch [56/120    avg_loss:0.047, val_acc:0.966]
Epoch [57/120    avg_loss:0.041, val_acc:0.965]
Epoch [58/120    avg_loss:0.032, val_acc:0.980]
Epoch [59/120    avg_loss:0.036, val_acc:0.970]
Epoch [60/120    avg_loss:0.045, val_acc:0.969]
Epoch [61/120    avg_loss:0.034, val_acc:0.983]
Epoch [62/120    avg_loss:0.036, val_acc:0.975]
Epoch [63/120    avg_loss:0.033, val_acc:0.978]
Epoch [64/120    avg_loss:0.028, val_acc:0.968]
Epoch [65/120    avg_loss:0.032, val_acc:0.985]
Epoch [66/120    avg_loss:0.027, val_acc:0.983]
Epoch [67/120    avg_loss:0.037, val_acc:0.980]
Epoch [68/120    avg_loss:0.016, val_acc:0.984]
Epoch [69/120    avg_loss:0.020, val_acc:0.980]
Epoch [70/120    avg_loss:0.018, val_acc:0.987]
Epoch [71/120    avg_loss:0.017, val_acc:0.988]
Epoch [72/120    avg_loss:0.013, val_acc:0.986]
Epoch [73/120    avg_loss:0.014, val_acc:0.975]
Epoch [74/120    avg_loss:0.031, val_acc:0.968]
Epoch [75/120    avg_loss:0.042, val_acc:0.984]
Epoch [76/120    avg_loss:0.020, val_acc:0.987]
Epoch [77/120    avg_loss:0.025, val_acc:0.982]
Epoch [78/120    avg_loss:0.013, val_acc:0.982]
Epoch [79/120    avg_loss:0.013, val_acc:0.981]
Epoch [80/120    avg_loss:0.016, val_acc:0.987]
Epoch [81/120    avg_loss:0.016, val_acc:0.978]
Epoch [82/120    avg_loss:0.026, val_acc:0.984]
Epoch [83/120    avg_loss:0.025, val_acc:0.976]
Epoch [84/120    avg_loss:0.038, val_acc:0.977]
Epoch [85/120    avg_loss:0.033, val_acc:0.981]
Epoch [86/120    avg_loss:0.022, val_acc:0.983]
Epoch [87/120    avg_loss:0.013, val_acc:0.983]
Epoch [88/120    avg_loss:0.012, val_acc:0.984]
Epoch [89/120    avg_loss:0.011, val_acc:0.985]
Epoch [90/120    avg_loss:0.011, val_acc:0.986]
Epoch [91/120    avg_loss:0.014, val_acc:0.987]
Epoch [92/120    avg_loss:0.013, val_acc:0.987]
Epoch [93/120    avg_loss:0.014, val_acc:0.988]
Epoch [94/120    avg_loss:0.010, val_acc:0.987]
Epoch [95/120    avg_loss:0.009, val_acc:0.987]
Epoch [96/120    avg_loss:0.009, val_acc:0.987]
Epoch [97/120    avg_loss:0.013, val_acc:0.987]
Epoch [98/120    avg_loss:0.011, val_acc:0.988]
Epoch [99/120    avg_loss:0.010, val_acc:0.988]
Epoch [100/120    avg_loss:0.010, val_acc:0.988]
Epoch [101/120    avg_loss:0.009, val_acc:0.988]
Epoch [102/120    avg_loss:0.009, val_acc:0.987]
Epoch [103/120    avg_loss:0.009, val_acc:0.989]
Epoch [104/120    avg_loss:0.011, val_acc:0.990]
Epoch [105/120    avg_loss:0.008, val_acc:0.988]
Epoch [106/120    avg_loss:0.009, val_acc:0.989]
Epoch [107/120    avg_loss:0.009, val_acc:0.989]
Epoch [108/120    avg_loss:0.010, val_acc:0.987]
Epoch [109/120    avg_loss:0.011, val_acc:0.988]
Epoch [110/120    avg_loss:0.010, val_acc:0.990]
Epoch [111/120    avg_loss:0.009, val_acc:0.988]
Epoch [112/120    avg_loss:0.011, val_acc:0.990]
Epoch [113/120    avg_loss:0.008, val_acc:0.992]
Epoch [114/120    avg_loss:0.008, val_acc:0.990]
Epoch [115/120    avg_loss:0.008, val_acc:0.993]
Epoch [116/120    avg_loss:0.009, val_acc:0.990]
Epoch [117/120    avg_loss:0.007, val_acc:0.990]
Epoch [118/120    avg_loss:0.007, val_acc:0.992]
Epoch [119/120    avg_loss:0.009, val_acc:0.989]
Epoch [120/120    avg_loss:0.008, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1268    0    0    0    0    0    0    0    7    8    2    0
     0    0    0]
 [   0    0    1  722    5    0    0    0    0    5    2    2   10    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    5    0    0    0    0  836   28    1    0
     0    0    0]
 [   0    0   16    1    0    0    1    0    0    0   19 2169    3    0
     0    1    0]
 [   0    0    0    2    0    3    0    0    0    0    0    0  527    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1134    5    0]
 [   0    0    0    0    0    0   28    0    0    0    0    0    0    0
    30  289    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.81029810298104

F1 scores:
[       nan 0.975      0.98485437 0.97964722 0.98839907 0.98858447
 0.97764531 1.         0.99883586 0.8        0.9603676  0.98167006
 0.97592593 1.         0.98480243 0.90031153 0.97619048]

Kappa:
0.9750287629199971
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc6f6e16f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.792, val_acc:0.069]
Epoch [2/120    avg_loss:2.718, val_acc:0.239]
Epoch [3/120    avg_loss:2.623, val_acc:0.297]
Epoch [4/120    avg_loss:2.536, val_acc:0.297]
Epoch [5/120    avg_loss:2.449, val_acc:0.307]
Epoch [6/120    avg_loss:2.319, val_acc:0.362]
Epoch [7/120    avg_loss:2.209, val_acc:0.429]
Epoch [8/120    avg_loss:2.150, val_acc:0.478]
Epoch [9/120    avg_loss:2.040, val_acc:0.597]
Epoch [10/120    avg_loss:1.920, val_acc:0.641]
Epoch [11/120    avg_loss:1.803, val_acc:0.663]
Epoch [12/120    avg_loss:1.681, val_acc:0.668]
Epoch [13/120    avg_loss:1.535, val_acc:0.684]
Epoch [14/120    avg_loss:1.492, val_acc:0.665]
Epoch [15/120    avg_loss:1.331, val_acc:0.688]
Epoch [16/120    avg_loss:1.237, val_acc:0.746]
Epoch [17/120    avg_loss:1.055, val_acc:0.787]
Epoch [18/120    avg_loss:0.912, val_acc:0.750]
Epoch [19/120    avg_loss:0.832, val_acc:0.754]
Epoch [20/120    avg_loss:0.754, val_acc:0.810]
Epoch [21/120    avg_loss:0.697, val_acc:0.797]
Epoch [22/120    avg_loss:0.604, val_acc:0.830]
Epoch [23/120    avg_loss:0.518, val_acc:0.846]
Epoch [24/120    avg_loss:0.522, val_acc:0.872]
Epoch [25/120    avg_loss:0.428, val_acc:0.862]
Epoch [26/120    avg_loss:0.412, val_acc:0.846]
Epoch [27/120    avg_loss:0.316, val_acc:0.916]
Epoch [28/120    avg_loss:0.275, val_acc:0.892]
Epoch [29/120    avg_loss:0.257, val_acc:0.912]
Epoch [30/120    avg_loss:0.202, val_acc:0.906]
Epoch [31/120    avg_loss:0.191, val_acc:0.907]
Epoch [32/120    avg_loss:0.200, val_acc:0.908]
Epoch [33/120    avg_loss:0.220, val_acc:0.897]
Epoch [34/120    avg_loss:0.179, val_acc:0.925]
Epoch [35/120    avg_loss:0.218, val_acc:0.879]
Epoch [36/120    avg_loss:0.206, val_acc:0.897]
Epoch [37/120    avg_loss:0.138, val_acc:0.944]
Epoch [38/120    avg_loss:0.118, val_acc:0.947]
Epoch [39/120    avg_loss:0.114, val_acc:0.932]
Epoch [40/120    avg_loss:0.103, val_acc:0.956]
Epoch [41/120    avg_loss:0.078, val_acc:0.956]
Epoch [42/120    avg_loss:0.075, val_acc:0.963]
Epoch [43/120    avg_loss:0.063, val_acc:0.968]
Epoch [44/120    avg_loss:0.061, val_acc:0.968]
Epoch [45/120    avg_loss:0.058, val_acc:0.976]
Epoch [46/120    avg_loss:0.074, val_acc:0.950]
Epoch [47/120    avg_loss:0.061, val_acc:0.969]
Epoch [48/120    avg_loss:0.041, val_acc:0.968]
Epoch [49/120    avg_loss:0.036, val_acc:0.968]
Epoch [50/120    avg_loss:0.040, val_acc:0.972]
Epoch [51/120    avg_loss:0.033, val_acc:0.972]
Epoch [52/120    avg_loss:0.033, val_acc:0.972]
Epoch [53/120    avg_loss:0.036, val_acc:0.947]
Epoch [54/120    avg_loss:0.035, val_acc:0.957]
Epoch [55/120    avg_loss:0.047, val_acc:0.944]
Epoch [56/120    avg_loss:0.143, val_acc:0.938]
Epoch [57/120    avg_loss:0.101, val_acc:0.950]
Epoch [58/120    avg_loss:0.059, val_acc:0.968]
Epoch [59/120    avg_loss:0.043, val_acc:0.974]
Epoch [60/120    avg_loss:0.036, val_acc:0.976]
Epoch [61/120    avg_loss:0.027, val_acc:0.978]
Epoch [62/120    avg_loss:0.030, val_acc:0.980]
Epoch [63/120    avg_loss:0.031, val_acc:0.978]
Epoch [64/120    avg_loss:0.029, val_acc:0.980]
Epoch [65/120    avg_loss:0.025, val_acc:0.980]
Epoch [66/120    avg_loss:0.025, val_acc:0.978]
Epoch [67/120    avg_loss:0.030, val_acc:0.978]
Epoch [68/120    avg_loss:0.025, val_acc:0.978]
Epoch [69/120    avg_loss:0.025, val_acc:0.980]
Epoch [70/120    avg_loss:0.027, val_acc:0.981]
Epoch [71/120    avg_loss:0.024, val_acc:0.978]
Epoch [72/120    avg_loss:0.023, val_acc:0.981]
Epoch [73/120    avg_loss:0.025, val_acc:0.980]
Epoch [74/120    avg_loss:0.023, val_acc:0.981]
Epoch [75/120    avg_loss:0.026, val_acc:0.981]
Epoch [76/120    avg_loss:0.027, val_acc:0.978]
Epoch [77/120    avg_loss:0.021, val_acc:0.980]
Epoch [78/120    avg_loss:0.022, val_acc:0.982]
Epoch [79/120    avg_loss:0.022, val_acc:0.978]
Epoch [80/120    avg_loss:0.023, val_acc:0.980]
Epoch [81/120    avg_loss:0.024, val_acc:0.980]
Epoch [82/120    avg_loss:0.022, val_acc:0.980]
Epoch [83/120    avg_loss:0.020, val_acc:0.980]
Epoch [84/120    avg_loss:0.019, val_acc:0.980]
Epoch [85/120    avg_loss:0.020, val_acc:0.978]
Epoch [86/120    avg_loss:0.020, val_acc:0.980]
Epoch [87/120    avg_loss:0.020, val_acc:0.980]
Epoch [88/120    avg_loss:0.019, val_acc:0.980]
Epoch [89/120    avg_loss:0.020, val_acc:0.981]
Epoch [90/120    avg_loss:0.020, val_acc:0.981]
Epoch [91/120    avg_loss:0.017, val_acc:0.978]
Epoch [92/120    avg_loss:0.018, val_acc:0.978]
Epoch [93/120    avg_loss:0.019, val_acc:0.978]
Epoch [94/120    avg_loss:0.017, val_acc:0.978]
Epoch [95/120    avg_loss:0.023, val_acc:0.978]
Epoch [96/120    avg_loss:0.020, val_acc:0.978]
Epoch [97/120    avg_loss:0.020, val_acc:0.980]
Epoch [98/120    avg_loss:0.020, val_acc:0.981]
Epoch [99/120    avg_loss:0.017, val_acc:0.981]
Epoch [100/120    avg_loss:0.021, val_acc:0.981]
Epoch [101/120    avg_loss:0.020, val_acc:0.981]
Epoch [102/120    avg_loss:0.020, val_acc:0.981]
Epoch [103/120    avg_loss:0.017, val_acc:0.981]
Epoch [104/120    avg_loss:0.021, val_acc:0.981]
Epoch [105/120    avg_loss:0.019, val_acc:0.981]
Epoch [106/120    avg_loss:0.020, val_acc:0.981]
Epoch [107/120    avg_loss:0.019, val_acc:0.981]
Epoch [108/120    avg_loss:0.019, val_acc:0.981]
Epoch [109/120    avg_loss:0.018, val_acc:0.981]
Epoch [110/120    avg_loss:0.019, val_acc:0.981]
Epoch [111/120    avg_loss:0.019, val_acc:0.981]
Epoch [112/120    avg_loss:0.019, val_acc:0.981]
Epoch [113/120    avg_loss:0.018, val_acc:0.981]
Epoch [114/120    avg_loss:0.018, val_acc:0.981]
Epoch [115/120    avg_loss:0.019, val_acc:0.981]
Epoch [116/120    avg_loss:0.016, val_acc:0.981]
Epoch [117/120    avg_loss:0.020, val_acc:0.981]
Epoch [118/120    avg_loss:0.021, val_acc:0.981]
Epoch [119/120    avg_loss:0.020, val_acc:0.981]
Epoch [120/120    avg_loss:0.020, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1257    0    8    0    0    0    0    0    3   16    1    0
     0    0    0]
 [   0    0    3  717    5    8    0    0    0    1    3    1    9    0
     0    0    0]
 [   0    0    0    6  203    0    0    0    0    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    3    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    8    1    0    2    0    0    0    0  843   19    0    0
     0    2    0]
 [   0    0    1    0    0    0    3    1    0    0   12 2186    7    0
     0    0    0]
 [   0    0    1    2    0   10    0    0    0    0    3    4  511    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1135    4    0]
 [   0    0    0    0    0    0   32    0    0    0    0    0    0    0
    60  255    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.22493224932249

F1 scores:
[       nan 0.98765432 0.98356808 0.97352342 0.93981481 0.97175141
 0.97181009 0.98039216 0.99649942 0.91891892 0.96952271 0.98535046
 0.95782568 1.         0.97050021 0.83881579 0.97647059]

Kappa:
0.9683352405576441
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f71c6823860>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.799, val_acc:0.167]
Epoch [2/120    avg_loss:2.726, val_acc:0.308]
Epoch [3/120    avg_loss:2.646, val_acc:0.374]
Epoch [4/120    avg_loss:2.551, val_acc:0.401]
Epoch [5/120    avg_loss:2.457, val_acc:0.447]
Epoch [6/120    avg_loss:2.352, val_acc:0.482]
Epoch [7/120    avg_loss:2.190, val_acc:0.497]
Epoch [8/120    avg_loss:2.138, val_acc:0.548]
Epoch [9/120    avg_loss:2.005, val_acc:0.560]
Epoch [10/120    avg_loss:1.890, val_acc:0.635]
Epoch [11/120    avg_loss:1.768, val_acc:0.659]
Epoch [12/120    avg_loss:1.678, val_acc:0.686]
Epoch [13/120    avg_loss:1.517, val_acc:0.696]
Epoch [14/120    avg_loss:1.290, val_acc:0.735]
Epoch [15/120    avg_loss:1.142, val_acc:0.747]
Epoch [16/120    avg_loss:1.106, val_acc:0.739]
Epoch [17/120    avg_loss:0.910, val_acc:0.768]
Epoch [18/120    avg_loss:0.833, val_acc:0.782]
Epoch [19/120    avg_loss:0.738, val_acc:0.820]
Epoch [20/120    avg_loss:0.643, val_acc:0.818]
Epoch [21/120    avg_loss:0.559, val_acc:0.843]
Epoch [22/120    avg_loss:0.471, val_acc:0.842]
Epoch [23/120    avg_loss:0.445, val_acc:0.864]
Epoch [24/120    avg_loss:0.358, val_acc:0.888]
Epoch [25/120    avg_loss:0.381, val_acc:0.883]
Epoch [26/120    avg_loss:0.394, val_acc:0.904]
Epoch [27/120    avg_loss:0.318, val_acc:0.904]
Epoch [28/120    avg_loss:0.232, val_acc:0.923]
Epoch [29/120    avg_loss:0.235, val_acc:0.873]
Epoch [30/120    avg_loss:0.216, val_acc:0.940]
Epoch [31/120    avg_loss:0.185, val_acc:0.941]
Epoch [32/120    avg_loss:0.153, val_acc:0.942]
Epoch [33/120    avg_loss:0.157, val_acc:0.914]
Epoch [34/120    avg_loss:0.139, val_acc:0.943]
Epoch [35/120    avg_loss:0.107, val_acc:0.904]
Epoch [36/120    avg_loss:0.149, val_acc:0.952]
Epoch [37/120    avg_loss:0.122, val_acc:0.952]
Epoch [38/120    avg_loss:0.105, val_acc:0.945]
Epoch [39/120    avg_loss:0.090, val_acc:0.960]
Epoch [40/120    avg_loss:0.076, val_acc:0.964]
Epoch [41/120    avg_loss:0.067, val_acc:0.962]
Epoch [42/120    avg_loss:0.083, val_acc:0.956]
Epoch [43/120    avg_loss:0.083, val_acc:0.958]
Epoch [44/120    avg_loss:0.073, val_acc:0.950]
Epoch [45/120    avg_loss:0.081, val_acc:0.949]
Epoch [46/120    avg_loss:0.072, val_acc:0.969]
Epoch [47/120    avg_loss:0.052, val_acc:0.974]
Epoch [48/120    avg_loss:0.053, val_acc:0.970]
Epoch [49/120    avg_loss:0.035, val_acc:0.970]
Epoch [50/120    avg_loss:0.055, val_acc:0.962]
Epoch [51/120    avg_loss:0.057, val_acc:0.956]
Epoch [52/120    avg_loss:0.076, val_acc:0.955]
Epoch [53/120    avg_loss:0.059, val_acc:0.961]
Epoch [54/120    avg_loss:0.053, val_acc:0.969]
Epoch [55/120    avg_loss:0.045, val_acc:0.962]
Epoch [56/120    avg_loss:0.043, val_acc:0.934]
Epoch [57/120    avg_loss:0.035, val_acc:0.963]
Epoch [58/120    avg_loss:0.042, val_acc:0.946]
Epoch [59/120    avg_loss:0.042, val_acc:0.969]
Epoch [60/120    avg_loss:0.037, val_acc:0.968]
Epoch [61/120    avg_loss:0.032, val_acc:0.972]
Epoch [62/120    avg_loss:0.022, val_acc:0.973]
Epoch [63/120    avg_loss:0.022, val_acc:0.974]
Epoch [64/120    avg_loss:0.020, val_acc:0.976]
Epoch [65/120    avg_loss:0.018, val_acc:0.975]
Epoch [66/120    avg_loss:0.018, val_acc:0.974]
Epoch [67/120    avg_loss:0.018, val_acc:0.976]
Epoch [68/120    avg_loss:0.018, val_acc:0.974]
Epoch [69/120    avg_loss:0.017, val_acc:0.973]
Epoch [70/120    avg_loss:0.019, val_acc:0.976]
Epoch [71/120    avg_loss:0.019, val_acc:0.976]
Epoch [72/120    avg_loss:0.016, val_acc:0.976]
Epoch [73/120    avg_loss:0.017, val_acc:0.975]
Epoch [74/120    avg_loss:0.014, val_acc:0.976]
Epoch [75/120    avg_loss:0.018, val_acc:0.975]
Epoch [76/120    avg_loss:0.017, val_acc:0.977]
Epoch [77/120    avg_loss:0.017, val_acc:0.977]
Epoch [78/120    avg_loss:0.014, val_acc:0.976]
Epoch [79/120    avg_loss:0.015, val_acc:0.977]
Epoch [80/120    avg_loss:0.018, val_acc:0.980]
Epoch [81/120    avg_loss:0.015, val_acc:0.978]
Epoch [82/120    avg_loss:0.015, val_acc:0.978]
Epoch [83/120    avg_loss:0.017, val_acc:0.977]
Epoch [84/120    avg_loss:0.014, val_acc:0.980]
Epoch [85/120    avg_loss:0.016, val_acc:0.978]
Epoch [86/120    avg_loss:0.015, val_acc:0.977]
Epoch [87/120    avg_loss:0.014, val_acc:0.978]
Epoch [88/120    avg_loss:0.013, val_acc:0.978]
Epoch [89/120    avg_loss:0.014, val_acc:0.978]
Epoch [90/120    avg_loss:0.013, val_acc:0.981]
Epoch [91/120    avg_loss:0.020, val_acc:0.981]
Epoch [92/120    avg_loss:0.015, val_acc:0.982]
Epoch [93/120    avg_loss:0.016, val_acc:0.982]
Epoch [94/120    avg_loss:0.014, val_acc:0.978]
Epoch [95/120    avg_loss:0.017, val_acc:0.978]
Epoch [96/120    avg_loss:0.016, val_acc:0.978]
Epoch [97/120    avg_loss:0.018, val_acc:0.980]
Epoch [98/120    avg_loss:0.013, val_acc:0.981]
Epoch [99/120    avg_loss:0.014, val_acc:0.980]
Epoch [100/120    avg_loss:0.013, val_acc:0.981]
Epoch [101/120    avg_loss:0.012, val_acc:0.980]
Epoch [102/120    avg_loss:0.013, val_acc:0.980]
Epoch [103/120    avg_loss:0.012, val_acc:0.978]
Epoch [104/120    avg_loss:0.015, val_acc:0.978]
Epoch [105/120    avg_loss:0.015, val_acc:0.976]
Epoch [106/120    avg_loss:0.014, val_acc:0.977]
Epoch [107/120    avg_loss:0.019, val_acc:0.977]
Epoch [108/120    avg_loss:0.024, val_acc:0.978]
Epoch [109/120    avg_loss:0.015, val_acc:0.978]
Epoch [110/120    avg_loss:0.012, val_acc:0.978]
Epoch [111/120    avg_loss:0.012, val_acc:0.977]
Epoch [112/120    avg_loss:0.012, val_acc:0.976]
Epoch [113/120    avg_loss:0.017, val_acc:0.976]
Epoch [114/120    avg_loss:0.012, val_acc:0.976]
Epoch [115/120    avg_loss:0.013, val_acc:0.976]
Epoch [116/120    avg_loss:0.013, val_acc:0.976]
Epoch [117/120    avg_loss:0.014, val_acc:0.976]
Epoch [118/120    avg_loss:0.016, val_acc:0.976]
Epoch [119/120    avg_loss:0.012, val_acc:0.976]
Epoch [120/120    avg_loss:0.013, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1244    1    7    0    0    0    0    0    9   22    2    0
     0    0    0]
 [   0    0    2  702    2    0    0    0    0   19    4    1   17    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    2    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    6    0    0    3    0    0    0    0  853   13    0    0
     0    0    0]
 [   0    0   10    0    0    0    0    0    0    0   19 2177    4    0
     0    0    0]
 [   0    0    0    0    6    3    0    0    0    0    3    0  513    0
     2    4    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    4    0    0
  1130    4    0]
 [   0    0    0    0    0    1   17    0    0    1    0    0    0    0
    28  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.45257452574526

F1 scores:
[       nan 0.96202532 0.97683549 0.96827586 0.96598639 0.98390805
 0.98646617 0.98039216 0.99416569 0.59649123 0.96602492 0.98351028
 0.9535316  1.         0.98090278 0.91603053 0.97647059]

Kappa:
0.9709569335216115
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc10f6a6ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.801, val_acc:0.212]
Epoch [2/120    avg_loss:2.722, val_acc:0.258]
Epoch [3/120    avg_loss:2.637, val_acc:0.311]
Epoch [4/120    avg_loss:2.511, val_acc:0.345]
Epoch [5/120    avg_loss:2.407, val_acc:0.397]
Epoch [6/120    avg_loss:2.292, val_acc:0.470]
Epoch [7/120    avg_loss:2.197, val_acc:0.516]
Epoch [8/120    avg_loss:2.083, val_acc:0.567]
Epoch [9/120    avg_loss:1.973, val_acc:0.597]
Epoch [10/120    avg_loss:1.867, val_acc:0.612]
Epoch [11/120    avg_loss:1.723, val_acc:0.647]
Epoch [12/120    avg_loss:1.539, val_acc:0.718]
Epoch [13/120    avg_loss:1.374, val_acc:0.733]
Epoch [14/120    avg_loss:1.280, val_acc:0.766]
Epoch [15/120    avg_loss:1.134, val_acc:0.774]
Epoch [16/120    avg_loss:1.001, val_acc:0.777]
Epoch [17/120    avg_loss:0.852, val_acc:0.814]
Epoch [18/120    avg_loss:0.796, val_acc:0.832]
Epoch [19/120    avg_loss:0.783, val_acc:0.839]
Epoch [20/120    avg_loss:0.666, val_acc:0.847]
Epoch [21/120    avg_loss:0.679, val_acc:0.856]
Epoch [22/120    avg_loss:0.467, val_acc:0.892]
Epoch [23/120    avg_loss:0.492, val_acc:0.842]
Epoch [24/120    avg_loss:0.445, val_acc:0.879]
Epoch [25/120    avg_loss:0.397, val_acc:0.887]
Epoch [26/120    avg_loss:0.310, val_acc:0.887]
Epoch [27/120    avg_loss:0.242, val_acc:0.913]
Epoch [28/120    avg_loss:0.243, val_acc:0.900]
Epoch [29/120    avg_loss:0.202, val_acc:0.919]
Epoch [30/120    avg_loss:0.205, val_acc:0.934]
Epoch [31/120    avg_loss:0.171, val_acc:0.931]
Epoch [32/120    avg_loss:0.160, val_acc:0.947]
Epoch [33/120    avg_loss:0.139, val_acc:0.947]
Epoch [34/120    avg_loss:0.156, val_acc:0.939]
Epoch [35/120    avg_loss:0.151, val_acc:0.941]
Epoch [36/120    avg_loss:0.113, val_acc:0.955]
Epoch [37/120    avg_loss:0.086, val_acc:0.957]
Epoch [38/120    avg_loss:0.116, val_acc:0.948]
Epoch [39/120    avg_loss:0.085, val_acc:0.954]
Epoch [40/120    avg_loss:0.077, val_acc:0.960]
Epoch [41/120    avg_loss:0.079, val_acc:0.968]
Epoch [42/120    avg_loss:0.073, val_acc:0.961]
Epoch [43/120    avg_loss:0.070, val_acc:0.969]
Epoch [44/120    avg_loss:0.046, val_acc:0.978]
Epoch [45/120    avg_loss:0.066, val_acc:0.966]
Epoch [46/120    avg_loss:0.056, val_acc:0.949]
Epoch [47/120    avg_loss:0.073, val_acc:0.952]
Epoch [48/120    avg_loss:0.073, val_acc:0.960]
Epoch [49/120    avg_loss:0.086, val_acc:0.953]
Epoch [50/120    avg_loss:0.077, val_acc:0.953]
Epoch [51/120    avg_loss:0.071, val_acc:0.956]
Epoch [52/120    avg_loss:0.068, val_acc:0.963]
Epoch [53/120    avg_loss:0.052, val_acc:0.967]
Epoch [54/120    avg_loss:0.048, val_acc:0.964]
Epoch [55/120    avg_loss:0.040, val_acc:0.970]
Epoch [56/120    avg_loss:0.031, val_acc:0.973]
Epoch [57/120    avg_loss:0.029, val_acc:0.980]
Epoch [58/120    avg_loss:0.033, val_acc:0.968]
Epoch [59/120    avg_loss:0.035, val_acc:0.981]
Epoch [60/120    avg_loss:0.024, val_acc:0.978]
Epoch [61/120    avg_loss:0.192, val_acc:0.874]
Epoch [62/120    avg_loss:0.279, val_acc:0.860]
Epoch [63/120    avg_loss:0.170, val_acc:0.938]
Epoch [64/120    avg_loss:0.125, val_acc:0.921]
Epoch [65/120    avg_loss:0.127, val_acc:0.934]
Epoch [66/120    avg_loss:0.099, val_acc:0.957]
Epoch [67/120    avg_loss:0.065, val_acc:0.967]
Epoch [68/120    avg_loss:0.051, val_acc:0.970]
Epoch [69/120    avg_loss:0.046, val_acc:0.953]
Epoch [70/120    avg_loss:0.061, val_acc:0.972]
Epoch [71/120    avg_loss:0.045, val_acc:0.961]
Epoch [72/120    avg_loss:0.031, val_acc:0.968]
Epoch [73/120    avg_loss:0.031, val_acc:0.974]
Epoch [74/120    avg_loss:0.021, val_acc:0.980]
Epoch [75/120    avg_loss:0.020, val_acc:0.977]
Epoch [76/120    avg_loss:0.023, val_acc:0.976]
Epoch [77/120    avg_loss:0.021, val_acc:0.977]
Epoch [78/120    avg_loss:0.019, val_acc:0.976]
Epoch [79/120    avg_loss:0.019, val_acc:0.977]
Epoch [80/120    avg_loss:0.021, val_acc:0.980]
Epoch [81/120    avg_loss:0.024, val_acc:0.981]
Epoch [82/120    avg_loss:0.021, val_acc:0.981]
Epoch [83/120    avg_loss:0.020, val_acc:0.982]
Epoch [84/120    avg_loss:0.026, val_acc:0.981]
Epoch [85/120    avg_loss:0.020, val_acc:0.981]
Epoch [86/120    avg_loss:0.018, val_acc:0.980]
Epoch [87/120    avg_loss:0.015, val_acc:0.983]
Epoch [88/120    avg_loss:0.020, val_acc:0.983]
Epoch [89/120    avg_loss:0.016, val_acc:0.983]
Epoch [90/120    avg_loss:0.020, val_acc:0.983]
Epoch [91/120    avg_loss:0.022, val_acc:0.984]
Epoch [92/120    avg_loss:0.023, val_acc:0.985]
Epoch [93/120    avg_loss:0.017, val_acc:0.982]
Epoch [94/120    avg_loss:0.017, val_acc:0.985]
Epoch [95/120    avg_loss:0.020, val_acc:0.985]
Epoch [96/120    avg_loss:0.014, val_acc:0.985]
Epoch [97/120    avg_loss:0.019, val_acc:0.983]
Epoch [98/120    avg_loss:0.014, val_acc:0.985]
Epoch [99/120    avg_loss:0.015, val_acc:0.984]
Epoch [100/120    avg_loss:0.014, val_acc:0.984]
Epoch [101/120    avg_loss:0.016, val_acc:0.984]
Epoch [102/120    avg_loss:0.017, val_acc:0.983]
Epoch [103/120    avg_loss:0.017, val_acc:0.984]
Epoch [104/120    avg_loss:0.014, val_acc:0.983]
Epoch [105/120    avg_loss:0.015, val_acc:0.984]
Epoch [106/120    avg_loss:0.016, val_acc:0.984]
Epoch [107/120    avg_loss:0.014, val_acc:0.986]
Epoch [108/120    avg_loss:0.018, val_acc:0.985]
Epoch [109/120    avg_loss:0.015, val_acc:0.983]
Epoch [110/120    avg_loss:0.014, val_acc:0.984]
Epoch [111/120    avg_loss:0.014, val_acc:0.983]
Epoch [112/120    avg_loss:0.012, val_acc:0.984]
Epoch [113/120    avg_loss:0.015, val_acc:0.984]
Epoch [114/120    avg_loss:0.013, val_acc:0.985]
Epoch [115/120    avg_loss:0.013, val_acc:0.983]
Epoch [116/120    avg_loss:0.013, val_acc:0.984]
Epoch [117/120    avg_loss:0.015, val_acc:0.983]
Epoch [118/120    avg_loss:0.015, val_acc:0.983]
Epoch [119/120    avg_loss:0.020, val_acc:0.980]
Epoch [120/120    avg_loss:0.015, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1262    2   11    0    0    0    0    0    2    8    0    0
     0    0    0]
 [   0    0    1  714    9    0    0    0    0    6    0    3   13    0
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    3    0    0    0    0    1    0    0
     6    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    4    0    0    0    0  857   12    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0    6 2195    0    0
     0    3    0]
 [   0    0    0    1    0    0    0    0    0    0    0    0  527    0
     1    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1138    1    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    53  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.04878048780488

F1 scores:
[       nan 1.         0.98748044 0.97407913 0.95280899 0.9837963
 0.98422239 1.         1.         0.82926829 0.98505747 0.99074701
 0.98137803 1.         0.97348161 0.87974684 0.98823529]

Kappa:
0.9777425334007434
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a84fc0eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.810, val_acc:0.185]
Epoch [2/120    avg_loss:2.731, val_acc:0.392]
Epoch [3/120    avg_loss:2.629, val_acc:0.419]
Epoch [4/120    avg_loss:2.519, val_acc:0.539]
Epoch [5/120    avg_loss:2.403, val_acc:0.561]
Epoch [6/120    avg_loss:2.300, val_acc:0.606]
Epoch [7/120    avg_loss:2.197, val_acc:0.593]
Epoch [8/120    avg_loss:2.075, val_acc:0.601]
Epoch [9/120    avg_loss:1.923, val_acc:0.621]
Epoch [10/120    avg_loss:1.764, val_acc:0.663]
Epoch [11/120    avg_loss:1.619, val_acc:0.683]
Epoch [12/120    avg_loss:1.430, val_acc:0.668]
Epoch [13/120    avg_loss:1.255, val_acc:0.683]
Epoch [14/120    avg_loss:1.123, val_acc:0.724]
Epoch [15/120    avg_loss:1.069, val_acc:0.725]
Epoch [16/120    avg_loss:0.869, val_acc:0.782]
Epoch [17/120    avg_loss:0.778, val_acc:0.779]
Epoch [18/120    avg_loss:0.655, val_acc:0.837]
Epoch [19/120    avg_loss:0.623, val_acc:0.834]
Epoch [20/120    avg_loss:0.505, val_acc:0.873]
Epoch [21/120    avg_loss:0.468, val_acc:0.865]
Epoch [22/120    avg_loss:0.474, val_acc:0.854]
Epoch [23/120    avg_loss:0.342, val_acc:0.879]
Epoch [24/120    avg_loss:0.380, val_acc:0.875]
Epoch [25/120    avg_loss:0.352, val_acc:0.887]
Epoch [26/120    avg_loss:0.296, val_acc:0.865]
Epoch [27/120    avg_loss:0.234, val_acc:0.926]
Epoch [28/120    avg_loss:0.208, val_acc:0.906]
Epoch [29/120    avg_loss:0.192, val_acc:0.916]
Epoch [30/120    avg_loss:0.169, val_acc:0.936]
Epoch [31/120    avg_loss:0.160, val_acc:0.943]
Epoch [32/120    avg_loss:0.126, val_acc:0.945]
Epoch [33/120    avg_loss:0.212, val_acc:0.933]
Epoch [34/120    avg_loss:0.151, val_acc:0.942]
Epoch [35/120    avg_loss:0.105, val_acc:0.951]
Epoch [36/120    avg_loss:0.104, val_acc:0.949]
Epoch [37/120    avg_loss:0.103, val_acc:0.949]
Epoch [38/120    avg_loss:0.091, val_acc:0.950]
Epoch [39/120    avg_loss:0.134, val_acc:0.935]
Epoch [40/120    avg_loss:0.179, val_acc:0.896]
Epoch [41/120    avg_loss:0.137, val_acc:0.948]
Epoch [42/120    avg_loss:0.096, val_acc:0.945]
Epoch [43/120    avg_loss:0.081, val_acc:0.958]
Epoch [44/120    avg_loss:0.065, val_acc:0.935]
Epoch [45/120    avg_loss:0.080, val_acc:0.958]
Epoch [46/120    avg_loss:0.058, val_acc:0.963]
Epoch [47/120    avg_loss:0.061, val_acc:0.956]
Epoch [48/120    avg_loss:0.054, val_acc:0.964]
Epoch [49/120    avg_loss:0.056, val_acc:0.971]
Epoch [50/120    avg_loss:0.039, val_acc:0.972]
Epoch [51/120    avg_loss:0.040, val_acc:0.948]
Epoch [52/120    avg_loss:0.047, val_acc:0.961]
Epoch [53/120    avg_loss:0.046, val_acc:0.959]
Epoch [54/120    avg_loss:0.046, val_acc:0.966]
Epoch [55/120    avg_loss:0.035, val_acc:0.968]
Epoch [56/120    avg_loss:0.037, val_acc:0.968]
Epoch [57/120    avg_loss:0.036, val_acc:0.972]
Epoch [58/120    avg_loss:0.038, val_acc:0.965]
Epoch [59/120    avg_loss:0.029, val_acc:0.978]
Epoch [60/120    avg_loss:0.029, val_acc:0.972]
Epoch [61/120    avg_loss:0.068, val_acc:0.954]
Epoch [62/120    avg_loss:1.104, val_acc:0.719]
Epoch [63/120    avg_loss:0.709, val_acc:0.850]
Epoch [64/120    avg_loss:0.268, val_acc:0.908]
Epoch [65/120    avg_loss:0.161, val_acc:0.951]
Epoch [66/120    avg_loss:0.135, val_acc:0.915]
Epoch [67/120    avg_loss:0.130, val_acc:0.931]
Epoch [68/120    avg_loss:0.081, val_acc:0.952]
Epoch [69/120    avg_loss:0.061, val_acc:0.968]
Epoch [70/120    avg_loss:0.042, val_acc:0.965]
Epoch [71/120    avg_loss:0.045, val_acc:0.954]
Epoch [72/120    avg_loss:0.042, val_acc:0.950]
Epoch [73/120    avg_loss:0.031, val_acc:0.978]
Epoch [74/120    avg_loss:0.026, val_acc:0.976]
Epoch [75/120    avg_loss:0.027, val_acc:0.976]
Epoch [76/120    avg_loss:0.022, val_acc:0.975]
Epoch [77/120    avg_loss:0.020, val_acc:0.976]
Epoch [78/120    avg_loss:0.022, val_acc:0.975]
Epoch [79/120    avg_loss:0.028, val_acc:0.976]
Epoch [80/120    avg_loss:0.022, val_acc:0.975]
Epoch [81/120    avg_loss:0.023, val_acc:0.975]
Epoch [82/120    avg_loss:0.021, val_acc:0.975]
Epoch [83/120    avg_loss:0.024, val_acc:0.978]
Epoch [84/120    avg_loss:0.024, val_acc:0.977]
Epoch [85/120    avg_loss:0.016, val_acc:0.977]
Epoch [86/120    avg_loss:0.022, val_acc:0.977]
Epoch [87/120    avg_loss:0.022, val_acc:0.975]
Epoch [88/120    avg_loss:0.020, val_acc:0.976]
Epoch [89/120    avg_loss:0.019, val_acc:0.978]
Epoch [90/120    avg_loss:0.020, val_acc:0.977]
Epoch [91/120    avg_loss:0.022, val_acc:0.978]
Epoch [92/120    avg_loss:0.018, val_acc:0.977]
Epoch [93/120    avg_loss:0.020, val_acc:0.977]
Epoch [94/120    avg_loss:0.020, val_acc:0.976]
Epoch [95/120    avg_loss:0.021, val_acc:0.978]
Epoch [96/120    avg_loss:0.019, val_acc:0.977]
Epoch [97/120    avg_loss:0.017, val_acc:0.978]
Epoch [98/120    avg_loss:0.018, val_acc:0.979]
Epoch [99/120    avg_loss:0.017, val_acc:0.977]
Epoch [100/120    avg_loss:0.015, val_acc:0.977]
Epoch [101/120    avg_loss:0.019, val_acc:0.977]
Epoch [102/120    avg_loss:0.016, val_acc:0.975]
Epoch [103/120    avg_loss:0.016, val_acc:0.977]
Epoch [104/120    avg_loss:0.019, val_acc:0.979]
Epoch [105/120    avg_loss:0.019, val_acc:0.978]
Epoch [106/120    avg_loss:0.017, val_acc:0.978]
Epoch [107/120    avg_loss:0.017, val_acc:0.979]
Epoch [108/120    avg_loss:0.014, val_acc:0.978]
Epoch [109/120    avg_loss:0.015, val_acc:0.979]
Epoch [110/120    avg_loss:0.016, val_acc:0.978]
Epoch [111/120    avg_loss:0.017, val_acc:0.979]
Epoch [112/120    avg_loss:0.015, val_acc:0.980]
Epoch [113/120    avg_loss:0.016, val_acc:0.980]
Epoch [114/120    avg_loss:0.017, val_acc:0.979]
Epoch [115/120    avg_loss:0.016, val_acc:0.977]
Epoch [116/120    avg_loss:0.015, val_acc:0.977]
Epoch [117/120    avg_loss:0.013, val_acc:0.979]
Epoch [118/120    avg_loss:0.015, val_acc:0.979]
Epoch [119/120    avg_loss:0.016, val_acc:0.980]
Epoch [120/120    avg_loss:0.015, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1251    4    8    4    1    0    0    0    1   16    0    0
     0    0    0]
 [   0    0    2  721   14    0    0    0    0    3    1    3    3    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    7    1    0    6    0    0    0    0  843   16    0    0
     2    0    0]
 [   0    0    9    5    0    0    0    0    0    0   16 2176    3    0
     1    0    0]
 [   0    0    0    0    1    0    0    0    0    1    0    6  515    0
     7    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1134    5    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    51  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.60433604336043

F1 scores:
[       nan 1.         0.97963978 0.97432432 0.94407159 0.98169336
 0.99392097 1.         0.99649942 0.87179487 0.97119816 0.9828365
 0.97261568 1.         0.96798976 0.9068323  0.97076023]

Kappa:
0.9726752362108202
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:07:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7b6510aef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.815, val_acc:0.123]
Epoch [2/120    avg_loss:2.745, val_acc:0.351]
Epoch [3/120    avg_loss:2.662, val_acc:0.468]
Epoch [4/120    avg_loss:2.566, val_acc:0.498]
Epoch [5/120    avg_loss:2.440, val_acc:0.490]
Epoch [6/120    avg_loss:2.320, val_acc:0.518]
Epoch [7/120    avg_loss:2.182, val_acc:0.545]
Epoch [8/120    avg_loss:2.103, val_acc:0.572]
Epoch [9/120    avg_loss:2.012, val_acc:0.609]
Epoch [10/120    avg_loss:1.872, val_acc:0.630]
Epoch [11/120    avg_loss:1.681, val_acc:0.679]
Epoch [12/120    avg_loss:1.599, val_acc:0.652]
Epoch [13/120    avg_loss:1.465, val_acc:0.690]
Epoch [14/120    avg_loss:1.387, val_acc:0.712]
Epoch [15/120    avg_loss:1.238, val_acc:0.737]
Epoch [16/120    avg_loss:1.067, val_acc:0.759]
Epoch [17/120    avg_loss:0.924, val_acc:0.755]
Epoch [18/120    avg_loss:0.891, val_acc:0.797]
Epoch [19/120    avg_loss:0.789, val_acc:0.783]
Epoch [20/120    avg_loss:0.748, val_acc:0.802]
Epoch [21/120    avg_loss:0.725, val_acc:0.795]
Epoch [22/120    avg_loss:0.671, val_acc:0.809]
Epoch [23/120    avg_loss:0.571, val_acc:0.829]
Epoch [24/120    avg_loss:0.513, val_acc:0.842]
Epoch [25/120    avg_loss:0.422, val_acc:0.869]
Epoch [26/120    avg_loss:0.443, val_acc:0.839]
Epoch [27/120    avg_loss:0.423, val_acc:0.830]
Epoch [28/120    avg_loss:0.369, val_acc:0.846]
Epoch [29/120    avg_loss:0.526, val_acc:0.864]
Epoch [30/120    avg_loss:0.324, val_acc:0.870]
Epoch [31/120    avg_loss:0.260, val_acc:0.887]
Epoch [32/120    avg_loss:0.245, val_acc:0.905]
Epoch [33/120    avg_loss:0.184, val_acc:0.901]
Epoch [34/120    avg_loss:0.249, val_acc:0.847]
Epoch [35/120    avg_loss:0.239, val_acc:0.919]
Epoch [36/120    avg_loss:0.180, val_acc:0.878]
Epoch [37/120    avg_loss:0.175, val_acc:0.879]
Epoch [38/120    avg_loss:0.150, val_acc:0.914]
Epoch [39/120    avg_loss:0.179, val_acc:0.912]
Epoch [40/120    avg_loss:0.152, val_acc:0.932]
Epoch [41/120    avg_loss:0.112, val_acc:0.938]
Epoch [42/120    avg_loss:0.107, val_acc:0.916]
Epoch [43/120    avg_loss:0.135, val_acc:0.933]
Epoch [44/120    avg_loss:0.101, val_acc:0.915]
Epoch [45/120    avg_loss:0.111, val_acc:0.947]
Epoch [46/120    avg_loss:0.128, val_acc:0.915]
Epoch [47/120    avg_loss:0.114, val_acc:0.944]
Epoch [48/120    avg_loss:0.070, val_acc:0.943]
Epoch [49/120    avg_loss:0.069, val_acc:0.940]
Epoch [50/120    avg_loss:0.122, val_acc:0.915]
Epoch [51/120    avg_loss:0.152, val_acc:0.934]
Epoch [52/120    avg_loss:0.071, val_acc:0.946]
Epoch [53/120    avg_loss:0.113, val_acc:0.946]
Epoch [54/120    avg_loss:0.057, val_acc:0.943]
Epoch [55/120    avg_loss:0.054, val_acc:0.939]
Epoch [56/120    avg_loss:0.084, val_acc:0.953]
Epoch [57/120    avg_loss:0.082, val_acc:0.948]
Epoch [58/120    avg_loss:0.176, val_acc:0.838]
Epoch [59/120    avg_loss:0.325, val_acc:0.944]
Epoch [60/120    avg_loss:0.088, val_acc:0.926]
Epoch [61/120    avg_loss:0.069, val_acc:0.950]
Epoch [62/120    avg_loss:0.083, val_acc:0.925]
Epoch [63/120    avg_loss:0.064, val_acc:0.957]
Epoch [64/120    avg_loss:0.184, val_acc:0.930]
Epoch [65/120    avg_loss:0.086, val_acc:0.958]
Epoch [66/120    avg_loss:0.064, val_acc:0.954]
Epoch [67/120    avg_loss:0.047, val_acc:0.950]
Epoch [68/120    avg_loss:0.042, val_acc:0.956]
Epoch [69/120    avg_loss:0.050, val_acc:0.950]
Epoch [70/120    avg_loss:0.055, val_acc:0.958]
Epoch [71/120    avg_loss:0.030, val_acc:0.964]
Epoch [72/120    avg_loss:0.033, val_acc:0.967]
Epoch [73/120    avg_loss:0.029, val_acc:0.968]
Epoch [74/120    avg_loss:0.023, val_acc:0.962]
Epoch [75/120    avg_loss:0.023, val_acc:0.971]
Epoch [76/120    avg_loss:0.026, val_acc:0.974]
Epoch [77/120    avg_loss:0.036, val_acc:0.970]
Epoch [78/120    avg_loss:0.041, val_acc:0.964]
Epoch [79/120    avg_loss:0.031, val_acc:0.968]
Epoch [80/120    avg_loss:0.032, val_acc:0.964]
Epoch [81/120    avg_loss:0.025, val_acc:0.970]
Epoch [82/120    avg_loss:0.019, val_acc:0.963]
Epoch [83/120    avg_loss:0.021, val_acc:0.978]
Epoch [84/120    avg_loss:0.025, val_acc:0.974]
Epoch [85/120    avg_loss:0.019, val_acc:0.967]
Epoch [86/120    avg_loss:0.024, val_acc:0.968]
Epoch [87/120    avg_loss:0.024, val_acc:0.972]
Epoch [88/120    avg_loss:0.021, val_acc:0.975]
Epoch [89/120    avg_loss:0.016, val_acc:0.963]
Epoch [90/120    avg_loss:0.020, val_acc:0.972]
Epoch [91/120    avg_loss:0.018, val_acc:0.976]
Epoch [92/120    avg_loss:0.024, val_acc:0.972]
Epoch [93/120    avg_loss:0.014, val_acc:0.981]
Epoch [94/120    avg_loss:0.014, val_acc:0.978]
Epoch [95/120    avg_loss:0.024, val_acc:0.971]
Epoch [96/120    avg_loss:0.028, val_acc:0.966]
Epoch [97/120    avg_loss:0.013, val_acc:0.972]
Epoch [98/120    avg_loss:0.018, val_acc:0.976]
Epoch [99/120    avg_loss:0.010, val_acc:0.977]
Epoch [100/120    avg_loss:0.011, val_acc:0.981]
Epoch [101/120    avg_loss:0.015, val_acc:0.962]
Epoch [102/120    avg_loss:0.013, val_acc:0.977]
Epoch [103/120    avg_loss:0.011, val_acc:0.983]
Epoch [104/120    avg_loss:0.016, val_acc:0.966]
Epoch [105/120    avg_loss:0.021, val_acc:0.969]
Epoch [106/120    avg_loss:0.013, val_acc:0.976]
Epoch [107/120    avg_loss:0.012, val_acc:0.973]
Epoch [108/120    avg_loss:0.010, val_acc:0.971]
Epoch [109/120    avg_loss:0.007, val_acc:0.977]
Epoch [110/120    avg_loss:0.010, val_acc:0.972]
Epoch [111/120    avg_loss:0.007, val_acc:0.977]
Epoch [112/120    avg_loss:0.011, val_acc:0.977]
Epoch [113/120    avg_loss:0.011, val_acc:0.970]
Epoch [114/120    avg_loss:0.013, val_acc:0.968]
Epoch [115/120    avg_loss:0.023, val_acc:0.961]
Epoch [116/120    avg_loss:0.011, val_acc:0.970]
Epoch [117/120    avg_loss:0.011, val_acc:0.975]
Epoch [118/120    avg_loss:0.009, val_acc:0.980]
Epoch [119/120    avg_loss:0.007, val_acc:0.980]
Epoch [120/120    avg_loss:0.008, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1253    0    6    0    0    0    0    0    4   20    2    0
     0    0    0]
 [   0    0    0  709    5   14    0    0    0    9    1    0    7    0
     0    2    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   10    1    0    7    0    0    0    0  822   33    1    0
     0    1    0]
 [   0    0    2    0    0    1    2    0    0    0    3 2190   12    0
     0    0    0]
 [   0    0    0    0    2    3    0    0    0    0    4    7  514    0
     2    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    2    0    0
  1133    4    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    28  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.50677506775068

F1 scores:
[       nan 1.         0.9827451  0.97323267 0.97038724 0.96629213
 0.97907324 1.         1.         0.72727273 0.96196606 0.9811828
 0.95895522 1.         0.98265395 0.9093702  0.98809524]

Kappa:
0.971554197408601
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3de0849f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.783, val_acc:0.144]
Epoch [2/120    avg_loss:2.700, val_acc:0.166]
Epoch [3/120    avg_loss:2.611, val_acc:0.296]
Epoch [4/120    avg_loss:2.515, val_acc:0.479]
Epoch [5/120    avg_loss:2.424, val_acc:0.516]
Epoch [6/120    avg_loss:2.312, val_acc:0.517]
Epoch [7/120    avg_loss:2.204, val_acc:0.569]
Epoch [8/120    avg_loss:2.079, val_acc:0.591]
Epoch [9/120    avg_loss:1.996, val_acc:0.626]
Epoch [10/120    avg_loss:1.860, val_acc:0.658]
Epoch [11/120    avg_loss:1.679, val_acc:0.655]
Epoch [12/120    avg_loss:1.555, val_acc:0.671]
Epoch [13/120    avg_loss:1.473, val_acc:0.684]
Epoch [14/120    avg_loss:1.266, val_acc:0.694]
Epoch [15/120    avg_loss:1.200, val_acc:0.725]
Epoch [16/120    avg_loss:1.032, val_acc:0.747]
Epoch [17/120    avg_loss:0.909, val_acc:0.774]
Epoch [18/120    avg_loss:0.832, val_acc:0.826]
Epoch [19/120    avg_loss:0.760, val_acc:0.788]
Epoch [20/120    avg_loss:0.692, val_acc:0.836]
Epoch [21/120    avg_loss:0.571, val_acc:0.849]
Epoch [22/120    avg_loss:0.609, val_acc:0.825]
Epoch [23/120    avg_loss:0.540, val_acc:0.820]
Epoch [24/120    avg_loss:0.500, val_acc:0.856]
Epoch [25/120    avg_loss:0.449, val_acc:0.864]
Epoch [26/120    avg_loss:0.395, val_acc:0.865]
Epoch [27/120    avg_loss:0.399, val_acc:0.840]
Epoch [28/120    avg_loss:0.348, val_acc:0.890]
Epoch [29/120    avg_loss:0.327, val_acc:0.887]
Epoch [30/120    avg_loss:0.338, val_acc:0.892]
Epoch [31/120    avg_loss:0.264, val_acc:0.900]
Epoch [32/120    avg_loss:0.239, val_acc:0.890]
Epoch [33/120    avg_loss:0.220, val_acc:0.893]
Epoch [34/120    avg_loss:0.200, val_acc:0.934]
Epoch [35/120    avg_loss:0.183, val_acc:0.899]
Epoch [36/120    avg_loss:0.179, val_acc:0.929]
Epoch [37/120    avg_loss:0.146, val_acc:0.917]
Epoch [38/120    avg_loss:0.193, val_acc:0.895]
Epoch [39/120    avg_loss:0.183, val_acc:0.940]
Epoch [40/120    avg_loss:0.162, val_acc:0.911]
Epoch [41/120    avg_loss:0.185, val_acc:0.915]
Epoch [42/120    avg_loss:0.174, val_acc:0.913]
Epoch [43/120    avg_loss:0.173, val_acc:0.947]
Epoch [44/120    avg_loss:0.120, val_acc:0.943]
Epoch [45/120    avg_loss:0.105, val_acc:0.953]
Epoch [46/120    avg_loss:0.099, val_acc:0.945]
Epoch [47/120    avg_loss:0.129, val_acc:0.958]
Epoch [48/120    avg_loss:0.096, val_acc:0.960]
Epoch [49/120    avg_loss:0.092, val_acc:0.957]
Epoch [50/120    avg_loss:0.084, val_acc:0.966]
Epoch [51/120    avg_loss:0.088, val_acc:0.962]
Epoch [52/120    avg_loss:0.061, val_acc:0.967]
Epoch [53/120    avg_loss:0.102, val_acc:0.922]
Epoch [54/120    avg_loss:0.143, val_acc:0.951]
Epoch [55/120    avg_loss:0.126, val_acc:0.951]
Epoch [56/120    avg_loss:0.079, val_acc:0.957]
Epoch [57/120    avg_loss:0.110, val_acc:0.922]
Epoch [58/120    avg_loss:0.181, val_acc:0.942]
Epoch [59/120    avg_loss:0.095, val_acc:0.968]
Epoch [60/120    avg_loss:0.062, val_acc:0.969]
Epoch [61/120    avg_loss:0.059, val_acc:0.959]
Epoch [62/120    avg_loss:0.063, val_acc:0.965]
Epoch [63/120    avg_loss:0.058, val_acc:0.981]
Epoch [64/120    avg_loss:0.049, val_acc:0.977]
Epoch [65/120    avg_loss:0.042, val_acc:0.947]
Epoch [66/120    avg_loss:0.045, val_acc:0.960]
Epoch [67/120    avg_loss:0.043, val_acc:0.978]
Epoch [68/120    avg_loss:0.034, val_acc:0.972]
Epoch [69/120    avg_loss:0.031, val_acc:0.978]
Epoch [70/120    avg_loss:0.030, val_acc:0.976]
Epoch [71/120    avg_loss:0.027, val_acc:0.976]
Epoch [72/120    avg_loss:0.034, val_acc:0.981]
Epoch [73/120    avg_loss:0.051, val_acc:0.978]
Epoch [74/120    avg_loss:0.039, val_acc:0.972]
Epoch [75/120    avg_loss:0.045, val_acc:0.978]
Epoch [76/120    avg_loss:0.036, val_acc:0.969]
Epoch [77/120    avg_loss:0.034, val_acc:0.964]
Epoch [78/120    avg_loss:0.038, val_acc:0.969]
Epoch [79/120    avg_loss:0.042, val_acc:0.977]
Epoch [80/120    avg_loss:0.032, val_acc:0.980]
Epoch [81/120    avg_loss:0.030, val_acc:0.980]
Epoch [82/120    avg_loss:0.027, val_acc:0.981]
Epoch [83/120    avg_loss:0.064, val_acc:0.939]
Epoch [84/120    avg_loss:0.143, val_acc:0.956]
Epoch [85/120    avg_loss:0.053, val_acc:0.963]
Epoch [86/120    avg_loss:0.032, val_acc:0.974]
Epoch [87/120    avg_loss:0.033, val_acc:0.969]
Epoch [88/120    avg_loss:0.022, val_acc:0.980]
Epoch [89/120    avg_loss:0.019, val_acc:0.985]
Epoch [90/120    avg_loss:0.012, val_acc:0.980]
Epoch [91/120    avg_loss:0.015, val_acc:0.986]
Epoch [92/120    avg_loss:0.023, val_acc:0.979]
Epoch [93/120    avg_loss:0.014, val_acc:0.976]
Epoch [94/120    avg_loss:0.014, val_acc:0.983]
Epoch [95/120    avg_loss:0.022, val_acc:0.980]
Epoch [96/120    avg_loss:0.021, val_acc:0.978]
Epoch [97/120    avg_loss:0.015, val_acc:0.988]
Epoch [98/120    avg_loss:0.012, val_acc:0.979]
Epoch [99/120    avg_loss:0.014, val_acc:0.983]
Epoch [100/120    avg_loss:0.009, val_acc:0.983]
Epoch [101/120    avg_loss:0.010, val_acc:0.981]
Epoch [102/120    avg_loss:0.008, val_acc:0.983]
Epoch [103/120    avg_loss:0.013, val_acc:0.979]
Epoch [104/120    avg_loss:0.013, val_acc:0.983]
Epoch [105/120    avg_loss:0.013, val_acc:0.986]
Epoch [106/120    avg_loss:0.011, val_acc:0.977]
Epoch [107/120    avg_loss:0.008, val_acc:0.980]
Epoch [108/120    avg_loss:0.009, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.008, val_acc:0.987]
Epoch [113/120    avg_loss:0.010, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.986]
Epoch [119/120    avg_loss:0.010, val_acc:0.985]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1252    0    3    0    0    0    0    0    2   28    0    0
     0    0    0]
 [   0    0    0  738    0    2    0    0    0    1    1    1    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    2    0    0    1    0    0    0    0  857   13    0    0
     0    2    0]
 [   0    0    3    0    0    0    0    0    5    0   18 2184    0    0
     0    0    0]
 [   0    0    4    0    0    8    0    0    0    0    3    0  516    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1132    7    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    53  284    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.02710027100271

F1 scores:
[       nan 0.98765432 0.98350353 0.99393939 0.99300699 0.98521047
 0.99244713 1.         0.99071926 0.91891892 0.97552647 0.98467087
 0.97450425 1.         0.97376344 0.88611544 0.98224852]

Kappa:
0.9774931903478755
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f95961eef28>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.795, val_acc:0.179]
Epoch [2/120    avg_loss:2.710, val_acc:0.369]
Epoch [3/120    avg_loss:2.592, val_acc:0.373]
Epoch [4/120    avg_loss:2.480, val_acc:0.419]
Epoch [5/120    avg_loss:2.361, val_acc:0.436]
Epoch [6/120    avg_loss:2.245, val_acc:0.459]
Epoch [7/120    avg_loss:2.112, val_acc:0.498]
Epoch [8/120    avg_loss:2.092, val_acc:0.533]
Epoch [9/120    avg_loss:1.965, val_acc:0.595]
Epoch [10/120    avg_loss:1.850, val_acc:0.614]
Epoch [11/120    avg_loss:1.747, val_acc:0.632]
Epoch [12/120    avg_loss:1.573, val_acc:0.637]
Epoch [13/120    avg_loss:1.498, val_acc:0.695]
Epoch [14/120    avg_loss:1.311, val_acc:0.718]
Epoch [15/120    avg_loss:1.216, val_acc:0.713]
Epoch [16/120    avg_loss:1.058, val_acc:0.738]
Epoch [17/120    avg_loss:0.943, val_acc:0.701]
Epoch [18/120    avg_loss:0.785, val_acc:0.779]
Epoch [19/120    avg_loss:0.856, val_acc:0.776]
Epoch [20/120    avg_loss:0.709, val_acc:0.803]
Epoch [21/120    avg_loss:0.666, val_acc:0.754]
Epoch [22/120    avg_loss:0.543, val_acc:0.834]
Epoch [23/120    avg_loss:0.520, val_acc:0.844]
Epoch [24/120    avg_loss:0.407, val_acc:0.867]
Epoch [25/120    avg_loss:0.411, val_acc:0.852]
Epoch [26/120    avg_loss:0.366, val_acc:0.863]
Epoch [27/120    avg_loss:0.348, val_acc:0.875]
Epoch [28/120    avg_loss:0.340, val_acc:0.847]
Epoch [29/120    avg_loss:0.263, val_acc:0.887]
Epoch [30/120    avg_loss:0.248, val_acc:0.915]
Epoch [31/120    avg_loss:0.270, val_acc:0.867]
Epoch [32/120    avg_loss:0.214, val_acc:0.911]
Epoch [33/120    avg_loss:0.174, val_acc:0.935]
Epoch [34/120    avg_loss:0.154, val_acc:0.946]
Epoch [35/120    avg_loss:0.147, val_acc:0.915]
Epoch [36/120    avg_loss:0.177, val_acc:0.938]
Epoch [37/120    avg_loss:0.134, val_acc:0.936]
Epoch [38/120    avg_loss:0.164, val_acc:0.935]
Epoch [39/120    avg_loss:0.133, val_acc:0.943]
Epoch [40/120    avg_loss:0.129, val_acc:0.942]
Epoch [41/120    avg_loss:0.124, val_acc:0.952]
Epoch [42/120    avg_loss:0.090, val_acc:0.960]
Epoch [43/120    avg_loss:0.071, val_acc:0.949]
Epoch [44/120    avg_loss:0.084, val_acc:0.927]
Epoch [45/120    avg_loss:0.106, val_acc:0.933]
Epoch [46/120    avg_loss:0.102, val_acc:0.963]
Epoch [47/120    avg_loss:0.081, val_acc:0.943]
Epoch [48/120    avg_loss:0.064, val_acc:0.962]
Epoch [49/120    avg_loss:0.093, val_acc:0.936]
Epoch [50/120    avg_loss:0.090, val_acc:0.960]
Epoch [51/120    avg_loss:0.064, val_acc:0.958]
Epoch [52/120    avg_loss:0.048, val_acc:0.968]
Epoch [53/120    avg_loss:0.046, val_acc:0.966]
Epoch [54/120    avg_loss:0.055, val_acc:0.967]
Epoch [55/120    avg_loss:0.062, val_acc:0.960]
Epoch [56/120    avg_loss:0.057, val_acc:0.961]
Epoch [57/120    avg_loss:0.054, val_acc:0.963]
Epoch [58/120    avg_loss:0.045, val_acc:0.970]
Epoch [59/120    avg_loss:0.034, val_acc:0.972]
Epoch [60/120    avg_loss:0.035, val_acc:0.948]
Epoch [61/120    avg_loss:0.048, val_acc:0.966]
Epoch [62/120    avg_loss:0.038, val_acc:0.967]
Epoch [63/120    avg_loss:0.032, val_acc:0.962]
Epoch [64/120    avg_loss:0.035, val_acc:0.975]
Epoch [65/120    avg_loss:0.031, val_acc:0.965]
Epoch [66/120    avg_loss:0.027, val_acc:0.961]
Epoch [67/120    avg_loss:0.022, val_acc:0.979]
Epoch [68/120    avg_loss:0.029, val_acc:0.977]
Epoch [69/120    avg_loss:0.032, val_acc:0.958]
Epoch [70/120    avg_loss:0.024, val_acc:0.978]
Epoch [71/120    avg_loss:0.023, val_acc:0.970]
Epoch [72/120    avg_loss:0.023, val_acc:0.980]
Epoch [73/120    avg_loss:0.016, val_acc:0.979]
Epoch [74/120    avg_loss:0.023, val_acc:0.980]
Epoch [75/120    avg_loss:0.024, val_acc:0.976]
Epoch [76/120    avg_loss:0.022, val_acc:0.974]
Epoch [77/120    avg_loss:0.032, val_acc:0.971]
Epoch [78/120    avg_loss:0.019, val_acc:0.970]
Epoch [79/120    avg_loss:0.014, val_acc:0.977]
Epoch [80/120    avg_loss:0.014, val_acc:0.976]
Epoch [81/120    avg_loss:0.029, val_acc:0.967]
Epoch [82/120    avg_loss:0.026, val_acc:0.968]
Epoch [83/120    avg_loss:0.027, val_acc:0.974]
Epoch [84/120    avg_loss:0.029, val_acc:0.965]
Epoch [85/120    avg_loss:0.022, val_acc:0.975]
Epoch [86/120    avg_loss:0.018, val_acc:0.976]
Epoch [87/120    avg_loss:0.019, val_acc:0.967]
Epoch [88/120    avg_loss:0.015, val_acc:0.972]
Epoch [89/120    avg_loss:0.014, val_acc:0.976]
Epoch [90/120    avg_loss:0.014, val_acc:0.978]
Epoch [91/120    avg_loss:0.013, val_acc:0.975]
Epoch [92/120    avg_loss:0.011, val_acc:0.976]
Epoch [93/120    avg_loss:0.010, val_acc:0.980]
Epoch [94/120    avg_loss:0.010, val_acc:0.979]
Epoch [95/120    avg_loss:0.011, val_acc:0.978]
Epoch [96/120    avg_loss:0.010, val_acc:0.978]
Epoch [97/120    avg_loss:0.009, val_acc:0.978]
Epoch [98/120    avg_loss:0.010, val_acc:0.979]
Epoch [99/120    avg_loss:0.011, val_acc:0.978]
Epoch [100/120    avg_loss:0.009, val_acc:0.979]
Epoch [101/120    avg_loss:0.010, val_acc:0.978]
Epoch [102/120    avg_loss:0.010, val_acc:0.978]
Epoch [103/120    avg_loss:0.009, val_acc:0.978]
Epoch [104/120    avg_loss:0.010, val_acc:0.979]
Epoch [105/120    avg_loss:0.010, val_acc:0.978]
Epoch [106/120    avg_loss:0.008, val_acc:0.979]
Epoch [107/120    avg_loss:0.010, val_acc:0.980]
Epoch [108/120    avg_loss:0.016, val_acc:0.980]
Epoch [109/120    avg_loss:0.009, val_acc:0.980]
Epoch [110/120    avg_loss:0.009, val_acc:0.980]
Epoch [111/120    avg_loss:0.008, val_acc:0.980]
Epoch [112/120    avg_loss:0.009, val_acc:0.980]
Epoch [113/120    avg_loss:0.010, val_acc:0.980]
Epoch [114/120    avg_loss:0.008, val_acc:0.980]
Epoch [115/120    avg_loss:0.009, val_acc:0.980]
Epoch [116/120    avg_loss:0.010, val_acc:0.980]
Epoch [117/120    avg_loss:0.007, val_acc:0.980]
Epoch [118/120    avg_loss:0.011, val_acc:0.980]
Epoch [119/120    avg_loss:0.009, val_acc:0.980]
Epoch [120/120    avg_loss:0.009, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1249    2    7    5    1    0    0    0    6   15    0    0
     0    0    0]
 [   0    0    0  727    5    8    0    0    0    7    0    0    0    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  425    0    1    0    0    0    0    0    0
     9    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    5    0    0   12    0    0    0    0
     0    0    0]
 [   0    0    5    2    0    3    2    0    0    0  835   22    1    0
     5    0    0]
 [   0    0    8    2    0    0    0    0    0    0   12 2181    6    1
     0    0    0]
 [   0    0    0    1    0    5    0    0    0    0    7    5  511    0
     0    1    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1138    1    0]
 [   0    0    1    0    0    0   17    0    0    0    0    0    0    0
    33  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.59349593495935

F1 scores:
[       nan 0.96202532 0.98037677 0.98110661 0.97025172 0.96481271
 0.98056801 0.98039216 1.         0.64864865 0.96087457 0.98398376
 0.96963947 0.99730458 0.97892473 0.91782946 0.97076023]

Kappa:
0.9725514222054471
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f867ba1ff28>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.818, val_acc:0.282]
Epoch [2/120    avg_loss:2.730, val_acc:0.395]
Epoch [3/120    avg_loss:2.619, val_acc:0.413]
Epoch [4/120    avg_loss:2.517, val_acc:0.455]
Epoch [5/120    avg_loss:2.390, val_acc:0.498]
Epoch [6/120    avg_loss:2.289, val_acc:0.495]
Epoch [7/120    avg_loss:2.194, val_acc:0.529]
Epoch [8/120    avg_loss:2.083, val_acc:0.560]
Epoch [9/120    avg_loss:1.965, val_acc:0.560]
Epoch [10/120    avg_loss:1.833, val_acc:0.615]
Epoch [11/120    avg_loss:1.702, val_acc:0.636]
Epoch [12/120    avg_loss:1.587, val_acc:0.696]
Epoch [13/120    avg_loss:1.449, val_acc:0.643]
Epoch [14/120    avg_loss:1.307, val_acc:0.683]
Epoch [15/120    avg_loss:1.292, val_acc:0.720]
Epoch [16/120    avg_loss:1.118, val_acc:0.697]
Epoch [17/120    avg_loss:0.976, val_acc:0.758]
Epoch [18/120    avg_loss:0.914, val_acc:0.759]
Epoch [19/120    avg_loss:0.860, val_acc:0.783]
Epoch [20/120    avg_loss:0.768, val_acc:0.814]
Epoch [21/120    avg_loss:0.713, val_acc:0.839]
Epoch [22/120    avg_loss:0.574, val_acc:0.849]
Epoch [23/120    avg_loss:0.552, val_acc:0.829]
Epoch [24/120    avg_loss:0.525, val_acc:0.848]
Epoch [25/120    avg_loss:0.415, val_acc:0.888]
Epoch [26/120    avg_loss:0.401, val_acc:0.867]
Epoch [27/120    avg_loss:0.423, val_acc:0.839]
Epoch [28/120    avg_loss:0.409, val_acc:0.884]
Epoch [29/120    avg_loss:0.318, val_acc:0.900]
Epoch [30/120    avg_loss:0.336, val_acc:0.876]
Epoch [31/120    avg_loss:0.342, val_acc:0.885]
Epoch [32/120    avg_loss:0.287, val_acc:0.913]
Epoch [33/120    avg_loss:0.191, val_acc:0.898]
Epoch [34/120    avg_loss:0.240, val_acc:0.904]
Epoch [35/120    avg_loss:0.221, val_acc:0.933]
Epoch [36/120    avg_loss:0.225, val_acc:0.922]
Epoch [37/120    avg_loss:0.185, val_acc:0.903]
Epoch [38/120    avg_loss:0.203, val_acc:0.931]
Epoch [39/120    avg_loss:0.241, val_acc:0.909]
Epoch [40/120    avg_loss:0.147, val_acc:0.923]
Epoch [41/120    avg_loss:0.169, val_acc:0.930]
Epoch [42/120    avg_loss:0.144, val_acc:0.934]
Epoch [43/120    avg_loss:0.115, val_acc:0.928]
Epoch [44/120    avg_loss:0.103, val_acc:0.943]
Epoch [45/120    avg_loss:0.103, val_acc:0.942]
Epoch [46/120    avg_loss:0.112, val_acc:0.954]
Epoch [47/120    avg_loss:0.101, val_acc:0.941]
Epoch [48/120    avg_loss:0.116, val_acc:0.949]
Epoch [49/120    avg_loss:0.081, val_acc:0.962]
Epoch [50/120    avg_loss:0.059, val_acc:0.952]
Epoch [51/120    avg_loss:0.071, val_acc:0.931]
Epoch [52/120    avg_loss:0.088, val_acc:0.954]
Epoch [53/120    avg_loss:0.090, val_acc:0.947]
Epoch [54/120    avg_loss:0.074, val_acc:0.949]
Epoch [55/120    avg_loss:0.061, val_acc:0.960]
Epoch [56/120    avg_loss:0.048, val_acc:0.954]
Epoch [57/120    avg_loss:0.065, val_acc:0.941]
Epoch [58/120    avg_loss:0.056, val_acc:0.958]
Epoch [59/120    avg_loss:0.043, val_acc:0.958]
Epoch [60/120    avg_loss:0.061, val_acc:0.909]
Epoch [61/120    avg_loss:0.064, val_acc:0.953]
Epoch [62/120    avg_loss:0.056, val_acc:0.963]
Epoch [63/120    avg_loss:0.061, val_acc:0.954]
Epoch [64/120    avg_loss:0.056, val_acc:0.961]
Epoch [65/120    avg_loss:0.040, val_acc:0.966]
Epoch [66/120    avg_loss:0.032, val_acc:0.938]
Epoch [67/120    avg_loss:0.041, val_acc:0.959]
Epoch [68/120    avg_loss:0.058, val_acc:0.953]
Epoch [69/120    avg_loss:0.078, val_acc:0.946]
Epoch [70/120    avg_loss:0.064, val_acc:0.954]
Epoch [71/120    avg_loss:0.059, val_acc:0.957]
Epoch [72/120    avg_loss:0.039, val_acc:0.952]
Epoch [73/120    avg_loss:0.039, val_acc:0.964]
Epoch [74/120    avg_loss:0.046, val_acc:0.959]
Epoch [75/120    avg_loss:0.040, val_acc:0.959]
Epoch [76/120    avg_loss:0.044, val_acc:0.963]
Epoch [77/120    avg_loss:0.025, val_acc:0.974]
Epoch [78/120    avg_loss:0.046, val_acc:0.957]
Epoch [79/120    avg_loss:0.031, val_acc:0.969]
Epoch [80/120    avg_loss:0.018, val_acc:0.978]
Epoch [81/120    avg_loss:0.024, val_acc:0.966]
Epoch [82/120    avg_loss:0.020, val_acc:0.968]
Epoch [83/120    avg_loss:0.030, val_acc:0.957]
Epoch [84/120    avg_loss:0.021, val_acc:0.958]
Epoch [85/120    avg_loss:0.027, val_acc:0.971]
Epoch [86/120    avg_loss:0.020, val_acc:0.975]
Epoch [87/120    avg_loss:0.017, val_acc:0.973]
Epoch [88/120    avg_loss:0.014, val_acc:0.966]
Epoch [89/120    avg_loss:0.024, val_acc:0.971]
Epoch [90/120    avg_loss:0.025, val_acc:0.960]
Epoch [91/120    avg_loss:0.023, val_acc:0.963]
Epoch [92/120    avg_loss:0.022, val_acc:0.971]
Epoch [93/120    avg_loss:0.015, val_acc:0.971]
Epoch [94/120    avg_loss:0.012, val_acc:0.976]
Epoch [95/120    avg_loss:0.010, val_acc:0.976]
Epoch [96/120    avg_loss:0.011, val_acc:0.976]
Epoch [97/120    avg_loss:0.010, val_acc:0.975]
Epoch [98/120    avg_loss:0.011, val_acc:0.976]
Epoch [99/120    avg_loss:0.010, val_acc:0.977]
Epoch [100/120    avg_loss:0.010, val_acc:0.976]
Epoch [101/120    avg_loss:0.011, val_acc:0.975]
Epoch [102/120    avg_loss:0.008, val_acc:0.976]
Epoch [103/120    avg_loss:0.009, val_acc:0.976]
Epoch [104/120    avg_loss:0.009, val_acc:0.977]
Epoch [105/120    avg_loss:0.009, val_acc:0.976]
Epoch [106/120    avg_loss:0.009, val_acc:0.976]
Epoch [107/120    avg_loss:0.008, val_acc:0.976]
Epoch [108/120    avg_loss:0.011, val_acc:0.977]
Epoch [109/120    avg_loss:0.009, val_acc:0.977]
Epoch [110/120    avg_loss:0.009, val_acc:0.977]
Epoch [111/120    avg_loss:0.009, val_acc:0.977]
Epoch [112/120    avg_loss:0.007, val_acc:0.977]
Epoch [113/120    avg_loss:0.009, val_acc:0.977]
Epoch [114/120    avg_loss:0.010, val_acc:0.977]
Epoch [115/120    avg_loss:0.009, val_acc:0.977]
Epoch [116/120    avg_loss:0.010, val_acc:0.977]
Epoch [117/120    avg_loss:0.010, val_acc:0.977]
Epoch [118/120    avg_loss:0.011, val_acc:0.976]
Epoch [119/120    avg_loss:0.009, val_acc:0.976]
Epoch [120/120    avg_loss:0.009, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1253    0    2    0    2    0    0    2    5   21    0    0
     0    0    0]
 [   0    0    7  690    1    0    0    0    0    9    2    1   37    0
     0    0    0]
 [   0    0    0   14  199    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    1    0
     0    2    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    4    5    0    5    0    0    0    0  849    8    0    0
     0    4    0]
 [   0    0    7    0    0    0    2    0    0    0   10 2185    5    0
     0    1    0]
 [   0    0    0    0    0    7    0    0    0    0    7    0  515    0
     1    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1134    2    0]
 [   0    0    0    0    0    0   35    0    0    0    0    0    0    0
    12  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.47425474254743

F1 scores:
[       nan 1.         0.98043818 0.9478022  0.95903614 0.97959184
 0.96969697 1.         0.99649942 0.72340426 0.97139588 0.98734749
 0.94322344 1.         0.99125874 0.91463415 0.97674419]

Kappa:
0.9712027069515755
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f77d34c3eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.806, val_acc:0.270]
Epoch [2/120    avg_loss:2.728, val_acc:0.345]
Epoch [3/120    avg_loss:2.644, val_acc:0.366]
Epoch [4/120    avg_loss:2.543, val_acc:0.483]
Epoch [5/120    avg_loss:2.428, val_acc:0.516]
Epoch [6/120    avg_loss:2.321, val_acc:0.536]
Epoch [7/120    avg_loss:2.186, val_acc:0.579]
Epoch [8/120    avg_loss:2.090, val_acc:0.592]
Epoch [9/120    avg_loss:1.990, val_acc:0.628]
Epoch [10/120    avg_loss:1.862, val_acc:0.648]
Epoch [11/120    avg_loss:1.682, val_acc:0.690]
Epoch [12/120    avg_loss:1.579, val_acc:0.672]
Epoch [13/120    avg_loss:1.420, val_acc:0.717]
Epoch [14/120    avg_loss:1.307, val_acc:0.735]
Epoch [15/120    avg_loss:1.201, val_acc:0.763]
Epoch [16/120    avg_loss:1.028, val_acc:0.744]
Epoch [17/120    avg_loss:0.987, val_acc:0.759]
Epoch [18/120    avg_loss:0.885, val_acc:0.746]
Epoch [19/120    avg_loss:0.845, val_acc:0.784]
Epoch [20/120    avg_loss:0.704, val_acc:0.793]
Epoch [21/120    avg_loss:0.622, val_acc:0.815]
Epoch [22/120    avg_loss:0.581, val_acc:0.848]
Epoch [23/120    avg_loss:0.541, val_acc:0.857]
Epoch [24/120    avg_loss:0.496, val_acc:0.884]
Epoch [25/120    avg_loss:0.458, val_acc:0.842]
Epoch [26/120    avg_loss:0.478, val_acc:0.877]
Epoch [27/120    avg_loss:0.413, val_acc:0.886]
Epoch [28/120    avg_loss:0.356, val_acc:0.911]
Epoch [29/120    avg_loss:0.327, val_acc:0.904]
Epoch [30/120    avg_loss:0.316, val_acc:0.909]
Epoch [31/120    avg_loss:0.278, val_acc:0.901]
Epoch [32/120    avg_loss:0.267, val_acc:0.886]
Epoch [33/120    avg_loss:0.245, val_acc:0.901]
Epoch [34/120    avg_loss:0.213, val_acc:0.925]
Epoch [35/120    avg_loss:0.163, val_acc:0.915]
Epoch [36/120    avg_loss:0.210, val_acc:0.916]
Epoch [37/120    avg_loss:0.197, val_acc:0.923]
Epoch [38/120    avg_loss:0.200, val_acc:0.935]
Epoch [39/120    avg_loss:0.156, val_acc:0.933]
Epoch [40/120    avg_loss:0.317, val_acc:0.908]
Epoch [41/120    avg_loss:0.204, val_acc:0.931]
Epoch [42/120    avg_loss:0.184, val_acc:0.917]
Epoch [43/120    avg_loss:0.156, val_acc:0.930]
Epoch [44/120    avg_loss:0.141, val_acc:0.900]
Epoch [45/120    avg_loss:0.167, val_acc:0.925]
Epoch [46/120    avg_loss:0.099, val_acc:0.940]
Epoch [47/120    avg_loss:0.088, val_acc:0.945]
Epoch [48/120    avg_loss:0.096, val_acc:0.936]
Epoch [49/120    avg_loss:0.084, val_acc:0.927]
Epoch [50/120    avg_loss:0.098, val_acc:0.932]
Epoch [51/120    avg_loss:0.087, val_acc:0.955]
Epoch [52/120    avg_loss:0.059, val_acc:0.956]
Epoch [53/120    avg_loss:0.083, val_acc:0.954]
Epoch [54/120    avg_loss:0.055, val_acc:0.953]
Epoch [55/120    avg_loss:0.073, val_acc:0.948]
Epoch [56/120    avg_loss:0.053, val_acc:0.961]
Epoch [57/120    avg_loss:0.057, val_acc:0.959]
Epoch [58/120    avg_loss:0.051, val_acc:0.956]
Epoch [59/120    avg_loss:0.056, val_acc:0.958]
Epoch [60/120    avg_loss:0.064, val_acc:0.948]
Epoch [61/120    avg_loss:0.045, val_acc:0.961]
Epoch [62/120    avg_loss:0.046, val_acc:0.962]
Epoch [63/120    avg_loss:0.037, val_acc:0.961]
Epoch [64/120    avg_loss:0.036, val_acc:0.961]
Epoch [65/120    avg_loss:0.038, val_acc:0.967]
Epoch [66/120    avg_loss:0.032, val_acc:0.971]
Epoch [67/120    avg_loss:0.033, val_acc:0.968]
Epoch [68/120    avg_loss:0.037, val_acc:0.963]
Epoch [69/120    avg_loss:0.032, val_acc:0.964]
Epoch [70/120    avg_loss:0.039, val_acc:0.957]
Epoch [71/120    avg_loss:0.062, val_acc:0.948]
Epoch [72/120    avg_loss:0.066, val_acc:0.938]
Epoch [73/120    avg_loss:0.156, val_acc:0.952]
Epoch [74/120    avg_loss:0.081, val_acc:0.948]
Epoch [75/120    avg_loss:0.070, val_acc:0.940]
Epoch [76/120    avg_loss:0.066, val_acc:0.963]
Epoch [77/120    avg_loss:0.056, val_acc:0.961]
Epoch [78/120    avg_loss:0.029, val_acc:0.964]
Epoch [79/120    avg_loss:0.027, val_acc:0.961]
Epoch [80/120    avg_loss:0.021, val_acc:0.970]
Epoch [81/120    avg_loss:0.027, val_acc:0.971]
Epoch [82/120    avg_loss:0.026, val_acc:0.970]
Epoch [83/120    avg_loss:0.020, val_acc:0.968]
Epoch [84/120    avg_loss:0.015, val_acc:0.970]
Epoch [85/120    avg_loss:0.022, val_acc:0.970]
Epoch [86/120    avg_loss:0.021, val_acc:0.970]
Epoch [87/120    avg_loss:0.018, val_acc:0.970]
Epoch [88/120    avg_loss:0.017, val_acc:0.969]
Epoch [89/120    avg_loss:0.019, val_acc:0.970]
Epoch [90/120    avg_loss:0.023, val_acc:0.970]
Epoch [91/120    avg_loss:0.019, val_acc:0.969]
Epoch [92/120    avg_loss:0.018, val_acc:0.970]
Epoch [93/120    avg_loss:0.018, val_acc:0.970]
Epoch [94/120    avg_loss:0.016, val_acc:0.973]
Epoch [95/120    avg_loss:0.017, val_acc:0.973]
Epoch [96/120    avg_loss:0.019, val_acc:0.972]
Epoch [97/120    avg_loss:0.016, val_acc:0.972]
Epoch [98/120    avg_loss:0.018, val_acc:0.971]
Epoch [99/120    avg_loss:0.018, val_acc:0.971]
Epoch [100/120    avg_loss:0.017, val_acc:0.972]
Epoch [101/120    avg_loss:0.019, val_acc:0.971]
Epoch [102/120    avg_loss:0.017, val_acc:0.974]
Epoch [103/120    avg_loss:0.013, val_acc:0.973]
Epoch [104/120    avg_loss:0.015, val_acc:0.973]
Epoch [105/120    avg_loss:0.017, val_acc:0.974]
Epoch [106/120    avg_loss:0.017, val_acc:0.972]
Epoch [107/120    avg_loss:0.017, val_acc:0.973]
Epoch [108/120    avg_loss:0.016, val_acc:0.974]
Epoch [109/120    avg_loss:0.017, val_acc:0.975]
Epoch [110/120    avg_loss:0.015, val_acc:0.975]
Epoch [111/120    avg_loss:0.014, val_acc:0.975]
Epoch [112/120    avg_loss:0.012, val_acc:0.975]
Epoch [113/120    avg_loss:0.015, val_acc:0.975]
Epoch [114/120    avg_loss:0.014, val_acc:0.974]
Epoch [115/120    avg_loss:0.018, val_acc:0.973]
Epoch [116/120    avg_loss:0.014, val_acc:0.973]
Epoch [117/120    avg_loss:0.016, val_acc:0.973]
Epoch [118/120    avg_loss:0.014, val_acc:0.973]
Epoch [119/120    avg_loss:0.012, val_acc:0.975]
Epoch [120/120    avg_loss:0.015, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1242    4    2    0    0    0    0    0   13   23    1    0
     0    0    0]
 [   0    0    2  713    0    4    0    0    0   15    1    8    2    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    2    0    1    0    0    0    0
     4    0    0]
 [   0    0   11    0    0    0  643    0    0    1    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    3    0    0    3    0    0   12    0    0    0    0
     0    0    0]
 [   0    0    8   41    0    3    1    0    0    0  807    7    0    0
     1    7    0]
 [   0    0    4    0    0    0    5    0    0    0   12 2188    1    0
     0    0    0]
 [   0    0    0   23    2    5    0    0    0    2   13    2  483    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0    1    0    0    1    0    0    0    0
    51  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
96.69376693766938

F1 scores:
[       nan 1.         0.97335423 0.93141737 0.99069767 0.97828571
 0.98167939 0.96153846 0.99649942 0.48       0.93728223 0.98558559
 0.9406037  0.99462366 0.97556794 0.90461538 0.97005988]

Kappa:
0.9622809838768324
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff199582f98>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.804, val_acc:0.282]
Epoch [2/120    avg_loss:2.721, val_acc:0.331]
Epoch [3/120    avg_loss:2.612, val_acc:0.349]
Epoch [4/120    avg_loss:2.504, val_acc:0.445]
Epoch [5/120    avg_loss:2.418, val_acc:0.504]
Epoch [6/120    avg_loss:2.315, val_acc:0.515]
Epoch [7/120    avg_loss:2.205, val_acc:0.527]
Epoch [8/120    avg_loss:2.125, val_acc:0.540]
Epoch [9/120    avg_loss:2.053, val_acc:0.539]
Epoch [10/120    avg_loss:1.980, val_acc:0.566]
Epoch [11/120    avg_loss:1.861, val_acc:0.572]
Epoch [12/120    avg_loss:1.794, val_acc:0.611]
Epoch [13/120    avg_loss:1.708, val_acc:0.672]
Epoch [14/120    avg_loss:1.636, val_acc:0.688]
Epoch [15/120    avg_loss:1.457, val_acc:0.717]
Epoch [16/120    avg_loss:1.320, val_acc:0.736]
Epoch [17/120    avg_loss:1.171, val_acc:0.741]
Epoch [18/120    avg_loss:0.999, val_acc:0.739]
Epoch [19/120    avg_loss:0.908, val_acc:0.776]
Epoch [20/120    avg_loss:0.792, val_acc:0.793]
Epoch [21/120    avg_loss:0.754, val_acc:0.767]
Epoch [22/120    avg_loss:0.788, val_acc:0.836]
Epoch [23/120    avg_loss:0.677, val_acc:0.823]
Epoch [24/120    avg_loss:0.659, val_acc:0.843]
Epoch [25/120    avg_loss:0.508, val_acc:0.863]
Epoch [26/120    avg_loss:0.472, val_acc:0.887]
Epoch [27/120    avg_loss:0.472, val_acc:0.888]
Epoch [28/120    avg_loss:0.398, val_acc:0.904]
Epoch [29/120    avg_loss:0.351, val_acc:0.874]
Epoch [30/120    avg_loss:0.305, val_acc:0.892]
Epoch [31/120    avg_loss:0.250, val_acc:0.921]
Epoch [32/120    avg_loss:0.329, val_acc:0.908]
Epoch [33/120    avg_loss:0.267, val_acc:0.914]
Epoch [34/120    avg_loss:0.191, val_acc:0.933]
Epoch [35/120    avg_loss:0.221, val_acc:0.913]
Epoch [36/120    avg_loss:0.286, val_acc:0.889]
Epoch [37/120    avg_loss:0.181, val_acc:0.923]
Epoch [38/120    avg_loss:0.144, val_acc:0.936]
Epoch [39/120    avg_loss:0.151, val_acc:0.932]
Epoch [40/120    avg_loss:0.121, val_acc:0.949]
Epoch [41/120    avg_loss:0.112, val_acc:0.946]
Epoch [42/120    avg_loss:0.111, val_acc:0.952]
Epoch [43/120    avg_loss:0.173, val_acc:0.944]
Epoch [44/120    avg_loss:0.128, val_acc:0.950]
Epoch [45/120    avg_loss:0.102, val_acc:0.945]
Epoch [46/120    avg_loss:0.081, val_acc:0.949]
Epoch [47/120    avg_loss:0.217, val_acc:0.946]
Epoch [48/120    avg_loss:0.095, val_acc:0.952]
Epoch [49/120    avg_loss:0.078, val_acc:0.946]
Epoch [50/120    avg_loss:0.079, val_acc:0.952]
Epoch [51/120    avg_loss:0.075, val_acc:0.923]
Epoch [52/120    avg_loss:0.078, val_acc:0.955]
Epoch [53/120    avg_loss:0.049, val_acc:0.968]
Epoch [54/120    avg_loss:0.064, val_acc:0.969]
Epoch [55/120    avg_loss:0.050, val_acc:0.970]
Epoch [56/120    avg_loss:0.064, val_acc:0.947]
Epoch [57/120    avg_loss:0.048, val_acc:0.950]
Epoch [58/120    avg_loss:0.044, val_acc:0.968]
Epoch [59/120    avg_loss:0.053, val_acc:0.967]
Epoch [60/120    avg_loss:0.037, val_acc:0.963]
Epoch [61/120    avg_loss:0.033, val_acc:0.973]
Epoch [62/120    avg_loss:0.032, val_acc:0.961]
Epoch [63/120    avg_loss:0.048, val_acc:0.972]
Epoch [64/120    avg_loss:0.045, val_acc:0.961]
Epoch [65/120    avg_loss:0.029, val_acc:0.977]
Epoch [66/120    avg_loss:0.032, val_acc:0.969]
Epoch [67/120    avg_loss:0.023, val_acc:0.980]
Epoch [68/120    avg_loss:0.026, val_acc:0.978]
Epoch [69/120    avg_loss:0.029, val_acc:0.971]
Epoch [70/120    avg_loss:0.030, val_acc:0.973]
Epoch [71/120    avg_loss:0.025, val_acc:0.976]
Epoch [72/120    avg_loss:0.024, val_acc:0.982]
Epoch [73/120    avg_loss:0.024, val_acc:0.977]
Epoch [74/120    avg_loss:0.020, val_acc:0.981]
Epoch [75/120    avg_loss:0.017, val_acc:0.982]
Epoch [76/120    avg_loss:0.019, val_acc:0.985]
Epoch [77/120    avg_loss:0.027, val_acc:0.975]
Epoch [78/120    avg_loss:0.016, val_acc:0.977]
Epoch [79/120    avg_loss:0.039, val_acc:0.971]
Epoch [80/120    avg_loss:0.029, val_acc:0.981]
Epoch [81/120    avg_loss:0.020, val_acc:0.980]
Epoch [82/120    avg_loss:0.018, val_acc:0.976]
Epoch [83/120    avg_loss:0.021, val_acc:0.976]
Epoch [84/120    avg_loss:0.022, val_acc:0.977]
Epoch [85/120    avg_loss:0.018, val_acc:0.981]
Epoch [86/120    avg_loss:0.067, val_acc:0.891]
Epoch [87/120    avg_loss:0.126, val_acc:0.950]
Epoch [88/120    avg_loss:0.062, val_acc:0.967]
Epoch [89/120    avg_loss:0.034, val_acc:0.973]
Epoch [90/120    avg_loss:0.021, val_acc:0.980]
Epoch [91/120    avg_loss:0.025, val_acc:0.980]
Epoch [92/120    avg_loss:0.020, val_acc:0.980]
Epoch [93/120    avg_loss:0.025, val_acc:0.983]
Epoch [94/120    avg_loss:0.020, val_acc:0.982]
Epoch [95/120    avg_loss:0.018, val_acc:0.982]
Epoch [96/120    avg_loss:0.021, val_acc:0.980]
Epoch [97/120    avg_loss:0.019, val_acc:0.982]
Epoch [98/120    avg_loss:0.018, val_acc:0.984]
Epoch [99/120    avg_loss:0.019, val_acc:0.987]
Epoch [100/120    avg_loss:0.019, val_acc:0.984]
Epoch [101/120    avg_loss:0.018, val_acc:0.985]
Epoch [102/120    avg_loss:0.016, val_acc:0.984]
Epoch [103/120    avg_loss:0.016, val_acc:0.984]
Epoch [104/120    avg_loss:0.017, val_acc:0.984]
Epoch [105/120    avg_loss:0.026, val_acc:0.985]
Epoch [106/120    avg_loss:0.018, val_acc:0.985]
Epoch [107/120    avg_loss:0.015, val_acc:0.984]
Epoch [108/120    avg_loss:0.017, val_acc:0.983]
Epoch [109/120    avg_loss:0.019, val_acc:0.987]
Epoch [110/120    avg_loss:0.014, val_acc:0.987]
Epoch [111/120    avg_loss:0.022, val_acc:0.986]
Epoch [112/120    avg_loss:0.013, val_acc:0.987]
Epoch [113/120    avg_loss:0.018, val_acc:0.985]
Epoch [114/120    avg_loss:0.015, val_acc:0.984]
Epoch [115/120    avg_loss:0.013, val_acc:0.986]
Epoch [116/120    avg_loss:0.013, val_acc:0.986]
Epoch [117/120    avg_loss:0.014, val_acc:0.987]
Epoch [118/120    avg_loss:0.012, val_acc:0.986]
Epoch [119/120    avg_loss:0.014, val_acc:0.987]
Epoch [120/120    avg_loss:0.016, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1254    1    2    0    1    0    0    0    3   23    1    0
     0    0    0]
 [   0    0    3  716    7   14    0    0    0    3    0    2    2    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7   47    0    3    0    0    0    0  809    8    0    0
     1    0    0]
 [   0    0    3    0    0    1    0    0    0    0   10 2184   12    0
     0    0    0]
 [   0    0    0   15    0   11    0    0    0    0    4   11  490    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   22    0    0    3    0    0    0    0
    99  223    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.42276422764228

F1 scores:
[       nan 1.         0.98275862 0.93717277 0.97459584 0.96659243
 0.97974494 1.         0.99883856 0.8372093  0.95120517 0.98334084
 0.94321463 1.         0.95750947 0.78108581 0.98823529]

Kappa:
0.9591673128293265
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9911f55f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.815, val_acc:0.236]
Epoch [2/120    avg_loss:2.761, val_acc:0.364]
Epoch [3/120    avg_loss:2.706, val_acc:0.405]
Epoch [4/120    avg_loss:2.637, val_acc:0.416]
Epoch [5/120    avg_loss:2.577, val_acc:0.448]
Epoch [6/120    avg_loss:2.471, val_acc:0.487]
Epoch [7/120    avg_loss:2.340, val_acc:0.512]
Epoch [8/120    avg_loss:2.235, val_acc:0.542]
Epoch [9/120    avg_loss:2.117, val_acc:0.561]
Epoch [10/120    avg_loss:2.018, val_acc:0.581]
Epoch [11/120    avg_loss:1.928, val_acc:0.602]
Epoch [12/120    avg_loss:1.793, val_acc:0.604]
Epoch [13/120    avg_loss:1.685, val_acc:0.668]
Epoch [14/120    avg_loss:1.567, val_acc:0.681]
Epoch [15/120    avg_loss:1.405, val_acc:0.715]
Epoch [16/120    avg_loss:1.214, val_acc:0.741]
Epoch [17/120    avg_loss:1.103, val_acc:0.746]
Epoch [18/120    avg_loss:1.047, val_acc:0.749]
Epoch [19/120    avg_loss:0.916, val_acc:0.792]
Epoch [20/120    avg_loss:0.902, val_acc:0.783]
Epoch [21/120    avg_loss:0.768, val_acc:0.829]
Epoch [22/120    avg_loss:0.664, val_acc:0.760]
Epoch [23/120    avg_loss:0.635, val_acc:0.823]
Epoch [24/120    avg_loss:0.479, val_acc:0.877]
Epoch [25/120    avg_loss:0.454, val_acc:0.837]
Epoch [26/120    avg_loss:0.429, val_acc:0.887]
Epoch [27/120    avg_loss:0.404, val_acc:0.891]
Epoch [28/120    avg_loss:0.351, val_acc:0.889]
Epoch [29/120    avg_loss:0.313, val_acc:0.906]
Epoch [30/120    avg_loss:0.326, val_acc:0.914]
Epoch [31/120    avg_loss:0.288, val_acc:0.910]
Epoch [32/120    avg_loss:0.223, val_acc:0.911]
Epoch [33/120    avg_loss:0.194, val_acc:0.944]
Epoch [34/120    avg_loss:0.215, val_acc:0.920]
Epoch [35/120    avg_loss:0.161, val_acc:0.933]
Epoch [36/120    avg_loss:0.181, val_acc:0.912]
Epoch [37/120    avg_loss:0.162, val_acc:0.954]
Epoch [38/120    avg_loss:0.141, val_acc:0.931]
Epoch [39/120    avg_loss:0.139, val_acc:0.951]
Epoch [40/120    avg_loss:0.117, val_acc:0.933]
Epoch [41/120    avg_loss:0.112, val_acc:0.958]
Epoch [42/120    avg_loss:0.095, val_acc:0.956]
Epoch [43/120    avg_loss:0.072, val_acc:0.954]
Epoch [44/120    avg_loss:0.077, val_acc:0.936]
Epoch [45/120    avg_loss:0.154, val_acc:0.957]
Epoch [46/120    avg_loss:0.091, val_acc:0.959]
Epoch [47/120    avg_loss:0.103, val_acc:0.959]
Epoch [48/120    avg_loss:0.098, val_acc:0.910]
Epoch [49/120    avg_loss:0.138, val_acc:0.946]
Epoch [50/120    avg_loss:0.101, val_acc:0.891]
Epoch [51/120    avg_loss:0.129, val_acc:0.959]
Epoch [52/120    avg_loss:0.069, val_acc:0.956]
Epoch [53/120    avg_loss:0.067, val_acc:0.946]
Epoch [54/120    avg_loss:0.058, val_acc:0.942]
Epoch [55/120    avg_loss:0.059, val_acc:0.966]
Epoch [56/120    avg_loss:0.052, val_acc:0.968]
Epoch [57/120    avg_loss:0.054, val_acc:0.961]
Epoch [58/120    avg_loss:0.055, val_acc:0.969]
Epoch [59/120    avg_loss:0.038, val_acc:0.975]
Epoch [60/120    avg_loss:0.039, val_acc:0.970]
Epoch [61/120    avg_loss:0.055, val_acc:0.964]
Epoch [62/120    avg_loss:0.043, val_acc:0.976]
Epoch [63/120    avg_loss:0.039, val_acc:0.966]
Epoch [64/120    avg_loss:0.081, val_acc:0.965]
Epoch [65/120    avg_loss:0.034, val_acc:0.979]
Epoch [66/120    avg_loss:0.036, val_acc:0.975]
Epoch [67/120    avg_loss:0.031, val_acc:0.963]
Epoch [68/120    avg_loss:0.056, val_acc:0.959]
Epoch [69/120    avg_loss:0.054, val_acc:0.970]
Epoch [70/120    avg_loss:0.037, val_acc:0.965]
Epoch [71/120    avg_loss:0.025, val_acc:0.981]
Epoch [72/120    avg_loss:0.026, val_acc:0.971]
Epoch [73/120    avg_loss:0.032, val_acc:0.974]
Epoch [74/120    avg_loss:0.024, val_acc:0.977]
Epoch [75/120    avg_loss:0.024, val_acc:0.948]
Epoch [76/120    avg_loss:0.027, val_acc:0.975]
Epoch [77/120    avg_loss:0.019, val_acc:0.974]
Epoch [78/120    avg_loss:0.018, val_acc:0.980]
Epoch [79/120    avg_loss:0.017, val_acc:0.978]
Epoch [80/120    avg_loss:0.027, val_acc:0.979]
Epoch [81/120    avg_loss:0.025, val_acc:0.970]
Epoch [82/120    avg_loss:0.019, val_acc:0.979]
Epoch [83/120    avg_loss:0.023, val_acc:0.973]
Epoch [84/120    avg_loss:0.021, val_acc:0.971]
Epoch [85/120    avg_loss:0.017, val_acc:0.971]
Epoch [86/120    avg_loss:0.014, val_acc:0.975]
Epoch [87/120    avg_loss:0.011, val_acc:0.981]
Epoch [88/120    avg_loss:0.010, val_acc:0.981]
Epoch [89/120    avg_loss:0.013, val_acc:0.984]
Epoch [90/120    avg_loss:0.009, val_acc:0.985]
Epoch [91/120    avg_loss:0.011, val_acc:0.985]
Epoch [92/120    avg_loss:0.011, val_acc:0.985]
Epoch [93/120    avg_loss:0.012, val_acc:0.981]
Epoch [94/120    avg_loss:0.011, val_acc:0.985]
Epoch [95/120    avg_loss:0.009, val_acc:0.984]
Epoch [96/120    avg_loss:0.011, val_acc:0.985]
Epoch [97/120    avg_loss:0.010, val_acc:0.986]
Epoch [98/120    avg_loss:0.012, val_acc:0.987]
Epoch [99/120    avg_loss:0.010, val_acc:0.987]
Epoch [100/120    avg_loss:0.009, val_acc:0.986]
Epoch [101/120    avg_loss:0.012, val_acc:0.986]
Epoch [102/120    avg_loss:0.012, val_acc:0.986]
Epoch [103/120    avg_loss:0.010, val_acc:0.986]
Epoch [104/120    avg_loss:0.012, val_acc:0.986]
Epoch [105/120    avg_loss:0.009, val_acc:0.985]
Epoch [106/120    avg_loss:0.009, val_acc:0.985]
Epoch [107/120    avg_loss:0.011, val_acc:0.987]
Epoch [108/120    avg_loss:0.010, val_acc:0.987]
Epoch [109/120    avg_loss:0.010, val_acc:0.986]
Epoch [110/120    avg_loss:0.009, val_acc:0.986]
Epoch [111/120    avg_loss:0.009, val_acc:0.988]
Epoch [112/120    avg_loss:0.011, val_acc:0.988]
Epoch [113/120    avg_loss:0.008, val_acc:0.988]
Epoch [114/120    avg_loss:0.010, val_acc:0.987]
Epoch [115/120    avg_loss:0.010, val_acc:0.986]
Epoch [116/120    avg_loss:0.010, val_acc:0.987]
Epoch [117/120    avg_loss:0.011, val_acc:0.985]
Epoch [118/120    avg_loss:0.012, val_acc:0.985]
Epoch [119/120    avg_loss:0.011, val_acc:0.986]
Epoch [120/120    avg_loss:0.010, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1252    0    0    0    0    0    0    0    3   21    9    0
     0    0    0]
 [   0    0    0  715   12    4    0    0    0    8    0    1    6    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  421    0    0    0    3    0    0    0    0
    11    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   16    0    0    1    0
     0    0    0]
 [   0    0    3   29    0    3    0    0    0    0  831    4    0    0
     0    5    0]
 [   0    0    4    0    0    0    1    0    0    0    5 2198    2    0
     0    0    0]
 [   0    0    4    2    0    1    0    0    0    0    7    5  508    0
     2    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1137    1    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    0    0
    58  274    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.38753387533875

F1 scores:
[       nan 0.96202532 0.98273155 0.95780308 0.97260274 0.97453704
 0.99848024 1.         1.         0.52459016 0.96403712 0.99031313
 0.95849057 0.99730458 0.96889646 0.87400319 0.97109827]

Kappa:
0.9701981525770138
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3aa97daf60>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.800, val_acc:0.121]
Epoch [2/120    avg_loss:2.710, val_acc:0.268]
Epoch [3/120    avg_loss:2.597, val_acc:0.373]
Epoch [4/120    avg_loss:2.488, val_acc:0.451]
Epoch [5/120    avg_loss:2.357, val_acc:0.474]
Epoch [6/120    avg_loss:2.282, val_acc:0.522]
Epoch [7/120    avg_loss:2.165, val_acc:0.550]
Epoch [8/120    avg_loss:2.084, val_acc:0.565]
Epoch [9/120    avg_loss:2.008, val_acc:0.584]
Epoch [10/120    avg_loss:1.873, val_acc:0.611]
Epoch [11/120    avg_loss:1.796, val_acc:0.605]
Epoch [12/120    avg_loss:1.735, val_acc:0.648]
Epoch [13/120    avg_loss:1.616, val_acc:0.668]
Epoch [14/120    avg_loss:1.487, val_acc:0.684]
Epoch [15/120    avg_loss:1.360, val_acc:0.713]
Epoch [16/120    avg_loss:1.259, val_acc:0.735]
Epoch [17/120    avg_loss:1.149, val_acc:0.726]
Epoch [18/120    avg_loss:1.016, val_acc:0.746]
Epoch [19/120    avg_loss:0.971, val_acc:0.742]
Epoch [20/120    avg_loss:0.819, val_acc:0.779]
Epoch [21/120    avg_loss:0.672, val_acc:0.796]
Epoch [22/120    avg_loss:0.597, val_acc:0.837]
Epoch [23/120    avg_loss:0.597, val_acc:0.829]
Epoch [24/120    avg_loss:0.681, val_acc:0.810]
Epoch [25/120    avg_loss:0.507, val_acc:0.867]
Epoch [26/120    avg_loss:0.478, val_acc:0.874]
Epoch [27/120    avg_loss:0.407, val_acc:0.849]
Epoch [28/120    avg_loss:0.392, val_acc:0.907]
Epoch [29/120    avg_loss:0.356, val_acc:0.867]
Epoch [30/120    avg_loss:0.328, val_acc:0.893]
Epoch [31/120    avg_loss:0.288, val_acc:0.882]
Epoch [32/120    avg_loss:0.262, val_acc:0.899]
Epoch [33/120    avg_loss:0.212, val_acc:0.930]
Epoch [34/120    avg_loss:0.218, val_acc:0.932]
Epoch [35/120    avg_loss:0.278, val_acc:0.912]
Epoch [36/120    avg_loss:0.246, val_acc:0.932]
Epoch [37/120    avg_loss:0.200, val_acc:0.933]
Epoch [38/120    avg_loss:0.148, val_acc:0.932]
Epoch [39/120    avg_loss:0.141, val_acc:0.924]
Epoch [40/120    avg_loss:0.158, val_acc:0.927]
Epoch [41/120    avg_loss:0.140, val_acc:0.929]
Epoch [42/120    avg_loss:0.115, val_acc:0.931]
Epoch [43/120    avg_loss:0.143, val_acc:0.938]
Epoch [44/120    avg_loss:0.115, val_acc:0.944]
Epoch [45/120    avg_loss:0.085, val_acc:0.946]
Epoch [46/120    avg_loss:0.098, val_acc:0.919]
Epoch [47/120    avg_loss:0.119, val_acc:0.947]
Epoch [48/120    avg_loss:0.113, val_acc:0.945]
Epoch [49/120    avg_loss:0.089, val_acc:0.923]
Epoch [50/120    avg_loss:0.084, val_acc:0.933]
Epoch [51/120    avg_loss:0.086, val_acc:0.956]
Epoch [52/120    avg_loss:0.068, val_acc:0.954]
Epoch [53/120    avg_loss:0.073, val_acc:0.952]
Epoch [54/120    avg_loss:0.077, val_acc:0.957]
Epoch [55/120    avg_loss:0.080, val_acc:0.950]
Epoch [56/120    avg_loss:0.071, val_acc:0.951]
Epoch [57/120    avg_loss:0.053, val_acc:0.959]
Epoch [58/120    avg_loss:0.053, val_acc:0.957]
Epoch [59/120    avg_loss:0.047, val_acc:0.965]
Epoch [60/120    avg_loss:0.062, val_acc:0.955]
Epoch [61/120    avg_loss:0.049, val_acc:0.966]
Epoch [62/120    avg_loss:0.035, val_acc:0.971]
Epoch [63/120    avg_loss:0.034, val_acc:0.965]
Epoch [64/120    avg_loss:0.032, val_acc:0.974]
Epoch [65/120    avg_loss:0.043, val_acc:0.969]
Epoch [66/120    avg_loss:0.032, val_acc:0.970]
Epoch [67/120    avg_loss:0.029, val_acc:0.966]
Epoch [68/120    avg_loss:0.027, val_acc:0.973]
Epoch [69/120    avg_loss:0.037, val_acc:0.943]
Epoch [70/120    avg_loss:0.040, val_acc:0.966]
Epoch [71/120    avg_loss:0.026, val_acc:0.967]
Epoch [72/120    avg_loss:0.024, val_acc:0.973]
Epoch [73/120    avg_loss:0.033, val_acc:0.954]
Epoch [74/120    avg_loss:0.064, val_acc:0.939]
Epoch [75/120    avg_loss:0.040, val_acc:0.969]
Epoch [76/120    avg_loss:0.031, val_acc:0.978]
Epoch [77/120    avg_loss:0.019, val_acc:0.978]
Epoch [78/120    avg_loss:0.021, val_acc:0.973]
Epoch [79/120    avg_loss:0.028, val_acc:0.966]
Epoch [80/120    avg_loss:0.025, val_acc:0.968]
Epoch [81/120    avg_loss:0.022, val_acc:0.975]
Epoch [82/120    avg_loss:0.024, val_acc:0.967]
Epoch [83/120    avg_loss:0.023, val_acc:0.974]
Epoch [84/120    avg_loss:0.025, val_acc:0.975]
Epoch [85/120    avg_loss:0.028, val_acc:0.971]
Epoch [86/120    avg_loss:0.028, val_acc:0.974]
Epoch [87/120    avg_loss:0.025, val_acc:0.974]
Epoch [88/120    avg_loss:0.032, val_acc:0.963]
Epoch [89/120    avg_loss:0.025, val_acc:0.967]
Epoch [90/120    avg_loss:0.018, val_acc:0.969]
Epoch [91/120    avg_loss:0.018, val_acc:0.973]
Epoch [92/120    avg_loss:0.014, val_acc:0.969]
Epoch [93/120    avg_loss:0.013, val_acc:0.975]
Epoch [94/120    avg_loss:0.011, val_acc:0.974]
Epoch [95/120    avg_loss:0.012, val_acc:0.974]
Epoch [96/120    avg_loss:0.014, val_acc:0.974]
Epoch [97/120    avg_loss:0.010, val_acc:0.975]
Epoch [98/120    avg_loss:0.011, val_acc:0.976]
Epoch [99/120    avg_loss:0.012, val_acc:0.976]
Epoch [100/120    avg_loss:0.013, val_acc:0.977]
Epoch [101/120    avg_loss:0.012, val_acc:0.978]
Epoch [102/120    avg_loss:0.012, val_acc:0.977]
Epoch [103/120    avg_loss:0.011, val_acc:0.977]
Epoch [104/120    avg_loss:0.011, val_acc:0.978]
Epoch [105/120    avg_loss:0.009, val_acc:0.980]
Epoch [106/120    avg_loss:0.011, val_acc:0.980]
Epoch [107/120    avg_loss:0.010, val_acc:0.980]
Epoch [108/120    avg_loss:0.010, val_acc:0.980]
Epoch [109/120    avg_loss:0.011, val_acc:0.979]
Epoch [110/120    avg_loss:0.011, val_acc:0.980]
Epoch [111/120    avg_loss:0.009, val_acc:0.980]
Epoch [112/120    avg_loss:0.009, val_acc:0.979]
Epoch [113/120    avg_loss:0.012, val_acc:0.979]
Epoch [114/120    avg_loss:0.010, val_acc:0.977]
Epoch [115/120    avg_loss:0.013, val_acc:0.977]
Epoch [116/120    avg_loss:0.009, val_acc:0.980]
Epoch [117/120    avg_loss:0.010, val_acc:0.979]
Epoch [118/120    avg_loss:0.010, val_acc:0.980]
Epoch [119/120    avg_loss:0.010, val_acc:0.979]
Epoch [120/120    avg_loss:0.009, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1263    2    5    0    0    0    0    0    0   10    5    0
     0    0    0]
 [   0    0    1  725    1    9    0    0    0    6    0    0    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    5    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1   30    0    4    0    0    0    0  823   16    0    0
     0    1    0]
 [   0    0    7    0    0    0    0    0    0    0   12 2190    1    0
     0    0    0]
 [   0    0    0    9    0    6    0    0    0    0    8   20  488    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    1   19    0    0    2    0    0    0    0
    99  226    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.78048780487805

F1 scores:
[       nan 0.94871795 0.98787642 0.95836087 0.98611111 0.97058824
 0.98574644 1.         0.99649942 0.73469388 0.95586527 0.9851552
 0.94117647 1.         0.95794786 0.78472222 0.98809524]

Kappa:
0.9632426960617949
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f315be28e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.793, val_acc:0.276]
Epoch [2/120    avg_loss:2.713, val_acc:0.425]
Epoch [3/120    avg_loss:2.641, val_acc:0.460]
Epoch [4/120    avg_loss:2.551, val_acc:0.475]
Epoch [5/120    avg_loss:2.450, val_acc:0.524]
Epoch [6/120    avg_loss:2.335, val_acc:0.525]
Epoch [7/120    avg_loss:2.233, val_acc:0.558]
Epoch [8/120    avg_loss:2.146, val_acc:0.579]
Epoch [9/120    avg_loss:2.017, val_acc:0.616]
Epoch [10/120    avg_loss:1.904, val_acc:0.626]
Epoch [11/120    avg_loss:1.784, val_acc:0.657]
Epoch [12/120    avg_loss:1.699, val_acc:0.691]
Epoch [13/120    avg_loss:1.577, val_acc:0.663]
Epoch [14/120    avg_loss:1.456, val_acc:0.719]
Epoch [15/120    avg_loss:1.355, val_acc:0.714]
Epoch [16/120    avg_loss:1.175, val_acc:0.765]
Epoch [17/120    avg_loss:1.075, val_acc:0.779]
Epoch [18/120    avg_loss:0.942, val_acc:0.807]
Epoch [19/120    avg_loss:0.875, val_acc:0.805]
Epoch [20/120    avg_loss:1.046, val_acc:0.504]
Epoch [21/120    avg_loss:1.451, val_acc:0.726]
Epoch [22/120    avg_loss:0.897, val_acc:0.786]
Epoch [23/120    avg_loss:0.730, val_acc:0.816]
Epoch [24/120    avg_loss:0.625, val_acc:0.774]
Epoch [25/120    avg_loss:0.586, val_acc:0.846]
Epoch [26/120    avg_loss:0.465, val_acc:0.869]
Epoch [27/120    avg_loss:0.467, val_acc:0.872]
Epoch [28/120    avg_loss:0.367, val_acc:0.890]
Epoch [29/120    avg_loss:0.382, val_acc:0.906]
Epoch [30/120    avg_loss:0.365, val_acc:0.892]
Epoch [31/120    avg_loss:0.311, val_acc:0.899]
Epoch [32/120    avg_loss:0.278, val_acc:0.915]
Epoch [33/120    avg_loss:0.249, val_acc:0.916]
Epoch [34/120    avg_loss:0.221, val_acc:0.930]
Epoch [35/120    avg_loss:0.187, val_acc:0.928]
Epoch [36/120    avg_loss:0.178, val_acc:0.916]
Epoch [37/120    avg_loss:0.195, val_acc:0.929]
Epoch [38/120    avg_loss:0.194, val_acc:0.926]
Epoch [39/120    avg_loss:0.144, val_acc:0.944]
Epoch [40/120    avg_loss:0.152, val_acc:0.923]
Epoch [41/120    avg_loss:0.234, val_acc:0.928]
Epoch [42/120    avg_loss:0.193, val_acc:0.932]
Epoch [43/120    avg_loss:0.143, val_acc:0.936]
Epoch [44/120    avg_loss:0.117, val_acc:0.944]
Epoch [45/120    avg_loss:0.112, val_acc:0.952]
Epoch [46/120    avg_loss:0.092, val_acc:0.939]
Epoch [47/120    avg_loss:0.082, val_acc:0.966]
Epoch [48/120    avg_loss:0.073, val_acc:0.932]
Epoch [49/120    avg_loss:0.079, val_acc:0.960]
Epoch [50/120    avg_loss:0.066, val_acc:0.955]
Epoch [51/120    avg_loss:0.076, val_acc:0.964]
Epoch [52/120    avg_loss:0.071, val_acc:0.960]
Epoch [53/120    avg_loss:0.051, val_acc:0.970]
Epoch [54/120    avg_loss:0.052, val_acc:0.954]
Epoch [55/120    avg_loss:0.063, val_acc:0.954]
Epoch [56/120    avg_loss:0.083, val_acc:0.942]
Epoch [57/120    avg_loss:0.070, val_acc:0.969]
Epoch [58/120    avg_loss:0.047, val_acc:0.971]
Epoch [59/120    avg_loss:0.039, val_acc:0.960]
Epoch [60/120    avg_loss:0.041, val_acc:0.956]
Epoch [61/120    avg_loss:0.044, val_acc:0.974]
Epoch [62/120    avg_loss:0.031, val_acc:0.971]
Epoch [63/120    avg_loss:0.031, val_acc:0.960]
Epoch [64/120    avg_loss:0.035, val_acc:0.963]
Epoch [65/120    avg_loss:0.038, val_acc:0.964]
Epoch [66/120    avg_loss:0.048, val_acc:0.964]
Epoch [67/120    avg_loss:0.058, val_acc:0.960]
Epoch [68/120    avg_loss:0.043, val_acc:0.962]
Epoch [69/120    avg_loss:0.034, val_acc:0.973]
Epoch [70/120    avg_loss:0.033, val_acc:0.976]
Epoch [71/120    avg_loss:0.024, val_acc:0.971]
Epoch [72/120    avg_loss:0.021, val_acc:0.966]
Epoch [73/120    avg_loss:0.041, val_acc:0.961]
Epoch [74/120    avg_loss:0.092, val_acc:0.946]
Epoch [75/120    avg_loss:0.076, val_acc:0.962]
Epoch [76/120    avg_loss:0.026, val_acc:0.966]
Epoch [77/120    avg_loss:0.020, val_acc:0.962]
Epoch [78/120    avg_loss:0.019, val_acc:0.974]
Epoch [79/120    avg_loss:0.023, val_acc:0.976]
Epoch [80/120    avg_loss:0.025, val_acc:0.971]
Epoch [81/120    avg_loss:0.020, val_acc:0.972]
Epoch [82/120    avg_loss:0.021, val_acc:0.968]
Epoch [83/120    avg_loss:0.016, val_acc:0.971]
Epoch [84/120    avg_loss:0.016, val_acc:0.975]
Epoch [85/120    avg_loss:0.012, val_acc:0.971]
Epoch [86/120    avg_loss:0.018, val_acc:0.973]
Epoch [87/120    avg_loss:0.015, val_acc:0.967]
Epoch [88/120    avg_loss:0.021, val_acc:0.972]
Epoch [89/120    avg_loss:0.011, val_acc:0.969]
Epoch [90/120    avg_loss:0.013, val_acc:0.976]
Epoch [91/120    avg_loss:0.015, val_acc:0.972]
Epoch [92/120    avg_loss:0.016, val_acc:0.970]
Epoch [93/120    avg_loss:0.012, val_acc:0.969]
Epoch [94/120    avg_loss:0.014, val_acc:0.974]
Epoch [95/120    avg_loss:0.011, val_acc:0.970]
Epoch [96/120    avg_loss:0.012, val_acc:0.969]
Epoch [97/120    avg_loss:0.010, val_acc:0.977]
Epoch [98/120    avg_loss:0.008, val_acc:0.973]
Epoch [99/120    avg_loss:0.012, val_acc:0.975]
Epoch [100/120    avg_loss:0.012, val_acc:0.968]
Epoch [101/120    avg_loss:0.013, val_acc:0.978]
Epoch [102/120    avg_loss:0.011, val_acc:0.977]
Epoch [103/120    avg_loss:0.011, val_acc:0.973]
Epoch [104/120    avg_loss:0.007, val_acc:0.973]
Epoch [105/120    avg_loss:0.012, val_acc:0.975]
Epoch [106/120    avg_loss:0.037, val_acc:0.967]
Epoch [107/120    avg_loss:0.015, val_acc:0.971]
Epoch [108/120    avg_loss:0.010, val_acc:0.972]
Epoch [109/120    avg_loss:0.021, val_acc:0.973]
Epoch [110/120    avg_loss:0.022, val_acc:0.970]
Epoch [111/120    avg_loss:0.009, val_acc:0.974]
Epoch [112/120    avg_loss:0.008, val_acc:0.975]
Epoch [113/120    avg_loss:0.008, val_acc:0.970]
Epoch [114/120    avg_loss:0.008, val_acc:0.974]
Epoch [115/120    avg_loss:0.006, val_acc:0.972]
Epoch [116/120    avg_loss:0.007, val_acc:0.974]
Epoch [117/120    avg_loss:0.006, val_acc:0.972]
Epoch [118/120    avg_loss:0.008, val_acc:0.975]
Epoch [119/120    avg_loss:0.008, val_acc:0.974]
Epoch [120/120    avg_loss:0.006, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1262    0   11    4    0    0    0    0    2    6    0    0
     0    0    0]
 [   0    0    4  726    0   11    0    0    0    0    1    2    2    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    5    0    0   13    0    0    0    0
     0    0    0]
 [   0    0    8   59    0    5    0    0    0    0  793    9    1    0
     0    0    0]
 [   0    0    4    2    0    0    1    0    0    0   15 2188    0    0
     0    0    0]
 [   0    0    0    3    0   18    0    0    0    0    1    5  505    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    88  255    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.98644986449864

F1 scores:
[       nan 1.         0.98478346 0.94469746 0.97482838 0.95469613
 0.99244713 1.         0.99883586 0.83870968 0.94013041 0.99004525
 0.9683605  0.99730458 0.96158717 0.84717608 0.98823529]

Kappa:
0.9656196027800269
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3b3b6bbf60>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.812, val_acc:0.180]
Epoch [2/120    avg_loss:2.723, val_acc:0.350]
Epoch [3/120    avg_loss:2.630, val_acc:0.458]
Epoch [4/120    avg_loss:2.539, val_acc:0.507]
Epoch [5/120    avg_loss:2.426, val_acc:0.521]
Epoch [6/120    avg_loss:2.293, val_acc:0.537]
Epoch [7/120    avg_loss:2.202, val_acc:0.515]
Epoch [8/120    avg_loss:2.089, val_acc:0.541]
Epoch [9/120    avg_loss:1.971, val_acc:0.605]
Epoch [10/120    avg_loss:1.823, val_acc:0.588]
Epoch [11/120    avg_loss:1.723, val_acc:0.600]
Epoch [12/120    avg_loss:1.600, val_acc:0.608]
Epoch [13/120    avg_loss:1.498, val_acc:0.641]
Epoch [14/120    avg_loss:1.354, val_acc:0.692]
Epoch [15/120    avg_loss:1.320, val_acc:0.695]
Epoch [16/120    avg_loss:1.150, val_acc:0.741]
Epoch [17/120    avg_loss:1.008, val_acc:0.743]
Epoch [18/120    avg_loss:0.891, val_acc:0.758]
Epoch [19/120    avg_loss:0.826, val_acc:0.725]
Epoch [20/120    avg_loss:0.740, val_acc:0.787]
Epoch [21/120    avg_loss:0.659, val_acc:0.831]
Epoch [22/120    avg_loss:0.617, val_acc:0.790]
Epoch [23/120    avg_loss:0.544, val_acc:0.847]
Epoch [24/120    avg_loss:0.443, val_acc:0.872]
Epoch [25/120    avg_loss:0.396, val_acc:0.890]
Epoch [26/120    avg_loss:0.386, val_acc:0.891]
Epoch [27/120    avg_loss:0.390, val_acc:0.883]
Epoch [28/120    avg_loss:0.366, val_acc:0.890]
Epoch [29/120    avg_loss:0.335, val_acc:0.881]
Epoch [30/120    avg_loss:0.254, val_acc:0.908]
Epoch [31/120    avg_loss:0.364, val_acc:0.881]
Epoch [32/120    avg_loss:0.297, val_acc:0.905]
Epoch [33/120    avg_loss:0.232, val_acc:0.903]
Epoch [34/120    avg_loss:0.228, val_acc:0.927]
Epoch [35/120    avg_loss:0.203, val_acc:0.892]
Epoch [36/120    avg_loss:0.196, val_acc:0.924]
Epoch [37/120    avg_loss:0.147, val_acc:0.927]
Epoch [38/120    avg_loss:0.106, val_acc:0.921]
Epoch [39/120    avg_loss:0.137, val_acc:0.935]
Epoch [40/120    avg_loss:0.105, val_acc:0.933]
Epoch [41/120    avg_loss:0.087, val_acc:0.940]
Epoch [42/120    avg_loss:0.116, val_acc:0.934]
Epoch [43/120    avg_loss:0.085, val_acc:0.936]
Epoch [44/120    avg_loss:0.070, val_acc:0.941]
Epoch [45/120    avg_loss:0.098, val_acc:0.939]
Epoch [46/120    avg_loss:0.075, val_acc:0.942]
Epoch [47/120    avg_loss:0.062, val_acc:0.953]
Epoch [48/120    avg_loss:0.059, val_acc:0.948]
Epoch [49/120    avg_loss:0.044, val_acc:0.941]
Epoch [50/120    avg_loss:0.055, val_acc:0.953]
Epoch [51/120    avg_loss:0.075, val_acc:0.961]
Epoch [52/120    avg_loss:0.048, val_acc:0.955]
Epoch [53/120    avg_loss:0.057, val_acc:0.942]
Epoch [54/120    avg_loss:0.061, val_acc:0.946]
Epoch [55/120    avg_loss:0.066, val_acc:0.955]
Epoch [56/120    avg_loss:0.051, val_acc:0.959]
Epoch [57/120    avg_loss:0.044, val_acc:0.969]
Epoch [58/120    avg_loss:0.052, val_acc:0.935]
Epoch [59/120    avg_loss:0.035, val_acc:0.963]
Epoch [60/120    avg_loss:0.036, val_acc:0.968]
Epoch [61/120    avg_loss:0.035, val_acc:0.968]
Epoch [62/120    avg_loss:0.028, val_acc:0.979]
Epoch [63/120    avg_loss:0.026, val_acc:0.970]
Epoch [64/120    avg_loss:0.023, val_acc:0.977]
Epoch [65/120    avg_loss:0.024, val_acc:0.974]
Epoch [66/120    avg_loss:0.026, val_acc:0.977]
Epoch [67/120    avg_loss:0.020, val_acc:0.977]
Epoch [68/120    avg_loss:0.023, val_acc:0.935]
Epoch [69/120    avg_loss:0.047, val_acc:0.964]
Epoch [70/120    avg_loss:0.032, val_acc:0.971]
Epoch [71/120    avg_loss:0.039, val_acc:0.959]
Epoch [72/120    avg_loss:0.037, val_acc:0.914]
Epoch [73/120    avg_loss:0.050, val_acc:0.963]
Epoch [74/120    avg_loss:0.033, val_acc:0.975]
Epoch [75/120    avg_loss:0.022, val_acc:0.978]
Epoch [76/120    avg_loss:0.015, val_acc:0.977]
Epoch [77/120    avg_loss:0.015, val_acc:0.977]
Epoch [78/120    avg_loss:0.012, val_acc:0.977]
Epoch [79/120    avg_loss:0.012, val_acc:0.977]
Epoch [80/120    avg_loss:0.012, val_acc:0.975]
Epoch [81/120    avg_loss:0.014, val_acc:0.979]
Epoch [82/120    avg_loss:0.014, val_acc:0.977]
Epoch [83/120    avg_loss:0.012, val_acc:0.977]
Epoch [84/120    avg_loss:0.013, val_acc:0.977]
Epoch [85/120    avg_loss:0.012, val_acc:0.975]
Epoch [86/120    avg_loss:0.015, val_acc:0.977]
Epoch [87/120    avg_loss:0.014, val_acc:0.978]
Epoch [88/120    avg_loss:0.013, val_acc:0.978]
Epoch [89/120    avg_loss:0.012, val_acc:0.978]
Epoch [90/120    avg_loss:0.013, val_acc:0.978]
Epoch [91/120    avg_loss:0.012, val_acc:0.979]
Epoch [92/120    avg_loss:0.011, val_acc:0.978]
Epoch [93/120    avg_loss:0.010, val_acc:0.979]
Epoch [94/120    avg_loss:0.011, val_acc:0.979]
Epoch [95/120    avg_loss:0.017, val_acc:0.980]
Epoch [96/120    avg_loss:0.010, val_acc:0.978]
Epoch [97/120    avg_loss:0.014, val_acc:0.980]
Epoch [98/120    avg_loss:0.012, val_acc:0.978]
Epoch [99/120    avg_loss:0.013, val_acc:0.980]
Epoch [100/120    avg_loss:0.012, val_acc:0.981]
Epoch [101/120    avg_loss:0.013, val_acc:0.980]
Epoch [102/120    avg_loss:0.013, val_acc:0.980]
Epoch [103/120    avg_loss:0.011, val_acc:0.979]
Epoch [104/120    avg_loss:0.011, val_acc:0.979]
Epoch [105/120    avg_loss:0.010, val_acc:0.979]
Epoch [106/120    avg_loss:0.011, val_acc:0.979]
Epoch [107/120    avg_loss:0.009, val_acc:0.982]
Epoch [108/120    avg_loss:0.013, val_acc:0.980]
Epoch [109/120    avg_loss:0.011, val_acc:0.980]
Epoch [110/120    avg_loss:0.010, val_acc:0.979]
Epoch [111/120    avg_loss:0.014, val_acc:0.981]
Epoch [112/120    avg_loss:0.010, val_acc:0.980]
Epoch [113/120    avg_loss:0.010, val_acc:0.981]
Epoch [114/120    avg_loss:0.010, val_acc:0.979]
Epoch [115/120    avg_loss:0.013, val_acc:0.978]
Epoch [116/120    avg_loss:0.010, val_acc:0.978]
Epoch [117/120    avg_loss:0.009, val_acc:0.978]
Epoch [118/120    avg_loss:0.013, val_acc:0.980]
Epoch [119/120    avg_loss:0.010, val_acc:0.978]
Epoch [120/120    avg_loss:0.011, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1274    0    1    0    1    0    0    0    0    9    0    0
     0    0    0]
 [   0    0    4  709   18    6    0    0    0    4    0    0    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    3    0    2    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    5    0    0   12    0    0    1    0
     0    0    0]
 [   0    0    7   16    0    3    0    0    0    0  845    3    0    0
     1    0    0]
 [   0    0    3    0    0    0    2    0    3    0   12 2185    1    0
     1    3    0]
 [   0    0    0    2   13    4    0    0    0    2    1    9  500    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1135    3    0]
 [   0    0    0    0    0    0   21    0    0    3    0    0    0    0
    45  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.47425474254743

F1 scores:
[       nan 0.96202532 0.99028372 0.96200814 0.930131   0.97371429
 0.97840655 0.94339623 0.99302326 0.58536585 0.9735023  0.98958333
 0.95602294 1.         0.97634409 0.87835703 0.98809524]

Kappa:
0.9711978328758616
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7760694ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.811, val_acc:0.181]
Epoch [2/120    avg_loss:2.730, val_acc:0.338]
Epoch [3/120    avg_loss:2.649, val_acc:0.350]
Epoch [4/120    avg_loss:2.564, val_acc:0.427]
Epoch [5/120    avg_loss:2.446, val_acc:0.444]
Epoch [6/120    avg_loss:2.331, val_acc:0.456]
Epoch [7/120    avg_loss:2.245, val_acc:0.500]
Epoch [8/120    avg_loss:2.134, val_acc:0.519]
Epoch [9/120    avg_loss:2.010, val_acc:0.553]
Epoch [10/120    avg_loss:1.911, val_acc:0.556]
Epoch [11/120    avg_loss:1.869, val_acc:0.567]
Epoch [12/120    avg_loss:1.747, val_acc:0.581]
Epoch [13/120    avg_loss:1.595, val_acc:0.629]
Epoch [14/120    avg_loss:1.460, val_acc:0.650]
Epoch [15/120    avg_loss:1.421, val_acc:0.663]
Epoch [16/120    avg_loss:1.209, val_acc:0.667]
Epoch [17/120    avg_loss:1.127, val_acc:0.636]
Epoch [18/120    avg_loss:1.038, val_acc:0.719]
Epoch [19/120    avg_loss:0.902, val_acc:0.741]
Epoch [20/120    avg_loss:0.844, val_acc:0.759]
Epoch [21/120    avg_loss:0.761, val_acc:0.777]
Epoch [22/120    avg_loss:0.735, val_acc:0.768]
Epoch [23/120    avg_loss:0.646, val_acc:0.818]
Epoch [24/120    avg_loss:0.636, val_acc:0.776]
Epoch [25/120    avg_loss:0.546, val_acc:0.821]
Epoch [26/120    avg_loss:0.510, val_acc:0.833]
Epoch [27/120    avg_loss:0.491, val_acc:0.817]
Epoch [28/120    avg_loss:0.397, val_acc:0.854]
Epoch [29/120    avg_loss:0.327, val_acc:0.867]
Epoch [30/120    avg_loss:0.303, val_acc:0.863]
Epoch [31/120    avg_loss:0.455, val_acc:0.862]
Epoch [32/120    avg_loss:0.330, val_acc:0.891]
Epoch [33/120    avg_loss:0.233, val_acc:0.899]
Epoch [34/120    avg_loss:0.243, val_acc:0.886]
Epoch [35/120    avg_loss:0.231, val_acc:0.885]
Epoch [36/120    avg_loss:0.216, val_acc:0.905]
Epoch [37/120    avg_loss:0.200, val_acc:0.889]
Epoch [38/120    avg_loss:0.144, val_acc:0.902]
Epoch [39/120    avg_loss:0.135, val_acc:0.910]
Epoch [40/120    avg_loss:0.151, val_acc:0.911]
Epoch [41/120    avg_loss:0.139, val_acc:0.923]
Epoch [42/120    avg_loss:0.133, val_acc:0.923]
Epoch [43/120    avg_loss:0.164, val_acc:0.918]
Epoch [44/120    avg_loss:0.148, val_acc:0.911]
Epoch [45/120    avg_loss:0.154, val_acc:0.913]
Epoch [46/120    avg_loss:0.110, val_acc:0.918]
Epoch [47/120    avg_loss:0.116, val_acc:0.883]
Epoch [48/120    avg_loss:0.226, val_acc:0.133]
Epoch [49/120    avg_loss:2.237, val_acc:0.423]
Epoch [50/120    avg_loss:1.986, val_acc:0.440]
Epoch [51/120    avg_loss:1.828, val_acc:0.524]
Epoch [52/120    avg_loss:1.610, val_acc:0.544]
Epoch [53/120    avg_loss:1.459, val_acc:0.634]
Epoch [54/120    avg_loss:1.244, val_acc:0.647]
Epoch [55/120    avg_loss:1.170, val_acc:0.673]
Epoch [56/120    avg_loss:1.032, val_acc:0.693]
Epoch [57/120    avg_loss:0.912, val_acc:0.705]
Epoch [58/120    avg_loss:0.906, val_acc:0.720]
Epoch [59/120    avg_loss:0.870, val_acc:0.732]
Epoch [60/120    avg_loss:0.812, val_acc:0.737]
Epoch [61/120    avg_loss:0.805, val_acc:0.743]
Epoch [62/120    avg_loss:0.786, val_acc:0.764]
Epoch [63/120    avg_loss:0.761, val_acc:0.758]
Epoch [64/120    avg_loss:0.707, val_acc:0.760]
Epoch [65/120    avg_loss:0.698, val_acc:0.771]
Epoch [66/120    avg_loss:0.720, val_acc:0.769]
Epoch [67/120    avg_loss:0.696, val_acc:0.777]
Epoch [68/120    avg_loss:0.673, val_acc:0.787]
Epoch [69/120    avg_loss:0.654, val_acc:0.791]
Epoch [70/120    avg_loss:0.619, val_acc:0.781]
Epoch [71/120    avg_loss:0.597, val_acc:0.786]
Epoch [72/120    avg_loss:0.674, val_acc:0.788]
Epoch [73/120    avg_loss:0.607, val_acc:0.787]
Epoch [74/120    avg_loss:0.632, val_acc:0.793]
Epoch [75/120    avg_loss:0.591, val_acc:0.793]
Epoch [76/120    avg_loss:0.593, val_acc:0.793]
Epoch [77/120    avg_loss:0.598, val_acc:0.795]
Epoch [78/120    avg_loss:0.605, val_acc:0.794]
Epoch [79/120    avg_loss:0.586, val_acc:0.795]
Epoch [80/120    avg_loss:0.612, val_acc:0.797]
Epoch [81/120    avg_loss:0.628, val_acc:0.795]
Epoch [82/120    avg_loss:0.591, val_acc:0.795]
Epoch [83/120    avg_loss:0.596, val_acc:0.797]
Epoch [84/120    avg_loss:0.600, val_acc:0.797]
Epoch [85/120    avg_loss:0.602, val_acc:0.797]
Epoch [86/120    avg_loss:0.567, val_acc:0.797]
Epoch [87/120    avg_loss:0.579, val_acc:0.797]
Epoch [88/120    avg_loss:0.600, val_acc:0.798]
Epoch [89/120    avg_loss:0.595, val_acc:0.798]
Epoch [90/120    avg_loss:0.593, val_acc:0.797]
Epoch [91/120    avg_loss:0.602, val_acc:0.797]
Epoch [92/120    avg_loss:0.596, val_acc:0.797]
Epoch [93/120    avg_loss:0.593, val_acc:0.798]
Epoch [94/120    avg_loss:0.643, val_acc:0.795]
Epoch [95/120    avg_loss:0.611, val_acc:0.795]
Epoch [96/120    avg_loss:0.608, val_acc:0.795]
Epoch [97/120    avg_loss:0.637, val_acc:0.795]
Epoch [98/120    avg_loss:0.596, val_acc:0.795]
Epoch [99/120    avg_loss:0.600, val_acc:0.795]
Epoch [100/120    avg_loss:0.577, val_acc:0.795]
Epoch [101/120    avg_loss:0.574, val_acc:0.795]
Epoch [102/120    avg_loss:0.602, val_acc:0.796]
Epoch [103/120    avg_loss:0.606, val_acc:0.796]
Epoch [104/120    avg_loss:0.579, val_acc:0.796]
Epoch [105/120    avg_loss:0.606, val_acc:0.796]
Epoch [106/120    avg_loss:0.611, val_acc:0.797]
Epoch [107/120    avg_loss:0.602, val_acc:0.797]
Epoch [108/120    avg_loss:0.622, val_acc:0.797]
Epoch [109/120    avg_loss:0.581, val_acc:0.797]
Epoch [110/120    avg_loss:0.611, val_acc:0.797]
Epoch [111/120    avg_loss:0.635, val_acc:0.797]
Epoch [112/120    avg_loss:0.579, val_acc:0.797]
Epoch [113/120    avg_loss:0.587, val_acc:0.797]
Epoch [114/120    avg_loss:0.608, val_acc:0.797]
Epoch [115/120    avg_loss:0.557, val_acc:0.797]
Epoch [116/120    avg_loss:0.592, val_acc:0.797]
Epoch [117/120    avg_loss:0.601, val_acc:0.797]
Epoch [118/120    avg_loss:0.589, val_acc:0.797]
Epoch [119/120    avg_loss:0.568, val_acc:0.797]
Epoch [120/120    avg_loss:0.595, val_acc:0.797]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   34    0    0    0    0    0    3    0    0    4    0    0    0
     0    0    0]
 [   0    0  905   96   13    6   18    1    0    0   72  135   17    0
    14    8    0]
 [   0    0   17  604    8    3   13    0    0    6    0   22   47   27
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  331   11   42    0    0    5    0    0    9
    37    0    0]
 [   0    1    2    2    1    0  620    0    0    0    0   17    0    0
    12    2    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   30    0    0    0    0    0    0  400    0    0    0    0    0
     0    0    0]
 [   0    0    0    7    0    0   10    0    0    1    0    0    0    0
     0    0    0]
 [   0    2   24   15    0   22   18    0    0    8  617  101   20   24
     1   23    0]
 [   0    0   91   55   11   22   39    0   13    0   14 1787  123   22
     0   33    0]
 [   0    0    0   10   26   14    0    0    0    0    0    0  457    1
     0    5   21]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    3    0    3    8   18    5    0    0   17    0    0    0    0
  1082    3    0]
 [   0    0    4    0  105    0   45    0    0    1    2    0    1    0
    87  102    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
80.72628726287263

F1 scores:
[       nan 0.61261261 0.77749141 0.78492528 0.71237458 0.77790834
 0.86350975 0.52083333 0.9489917  0.03921569 0.77658905 0.83661049
 0.76230192 0.81677704 0.91231029 0.39005736 0.88888889]

Kappa:
0.7817437057229628
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5c1dea5ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.836, val_acc:0.024]
Epoch [2/120    avg_loss:2.759, val_acc:0.214]
Epoch [3/120    avg_loss:2.686, val_acc:0.374]
Epoch [4/120    avg_loss:2.593, val_acc:0.430]
Epoch [5/120    avg_loss:2.487, val_acc:0.467]
Epoch [6/120    avg_loss:2.350, val_acc:0.512]
Epoch [7/120    avg_loss:2.239, val_acc:0.534]
Epoch [8/120    avg_loss:2.150, val_acc:0.570]
Epoch [9/120    avg_loss:2.066, val_acc:0.547]
Epoch [10/120    avg_loss:1.945, val_acc:0.633]
Epoch [11/120    avg_loss:1.844, val_acc:0.648]
Epoch [12/120    avg_loss:1.641, val_acc:0.675]
Epoch [13/120    avg_loss:1.551, val_acc:0.688]
Epoch [14/120    avg_loss:1.354, val_acc:0.682]
Epoch [15/120    avg_loss:1.272, val_acc:0.718]
Epoch [16/120    avg_loss:1.149, val_acc:0.707]
Epoch [17/120    avg_loss:0.988, val_acc:0.772]
Epoch [18/120    avg_loss:0.870, val_acc:0.794]
Epoch [19/120    avg_loss:0.794, val_acc:0.821]
Epoch [20/120    avg_loss:0.770, val_acc:0.807]
Epoch [21/120    avg_loss:0.766, val_acc:0.805]
Epoch [22/120    avg_loss:0.660, val_acc:0.828]
Epoch [23/120    avg_loss:0.558, val_acc:0.865]
Epoch [24/120    avg_loss:0.567, val_acc:0.836]
Epoch [25/120    avg_loss:0.481, val_acc:0.879]
Epoch [26/120    avg_loss:0.381, val_acc:0.895]
Epoch [27/120    avg_loss:0.416, val_acc:0.879]
Epoch [28/120    avg_loss:0.357, val_acc:0.904]
Epoch [29/120    avg_loss:0.296, val_acc:0.929]
Epoch [30/120    avg_loss:0.263, val_acc:0.933]
Epoch [31/120    avg_loss:0.244, val_acc:0.921]
Epoch [32/120    avg_loss:0.219, val_acc:0.932]
Epoch [33/120    avg_loss:0.221, val_acc:0.896]
Epoch [34/120    avg_loss:0.199, val_acc:0.931]
Epoch [35/120    avg_loss:0.170, val_acc:0.948]
Epoch [36/120    avg_loss:0.155, val_acc:0.943]
Epoch [37/120    avg_loss:0.135, val_acc:0.954]
Epoch [38/120    avg_loss:0.122, val_acc:0.948]
Epoch [39/120    avg_loss:0.105, val_acc:0.958]
Epoch [40/120    avg_loss:0.107, val_acc:0.950]
Epoch [41/120    avg_loss:0.086, val_acc:0.955]
Epoch [42/120    avg_loss:0.092, val_acc:0.961]
Epoch [43/120    avg_loss:0.079, val_acc:0.938]
Epoch [44/120    avg_loss:0.092, val_acc:0.956]
Epoch [45/120    avg_loss:0.102, val_acc:0.947]
Epoch [46/120    avg_loss:0.128, val_acc:0.953]
Epoch [47/120    avg_loss:0.200, val_acc:0.925]
Epoch [48/120    avg_loss:0.161, val_acc:0.939]
Epoch [49/120    avg_loss:0.117, val_acc:0.944]
Epoch [50/120    avg_loss:0.087, val_acc:0.958]
Epoch [51/120    avg_loss:0.145, val_acc:0.947]
Epoch [52/120    avg_loss:0.083, val_acc:0.954]
Epoch [53/120    avg_loss:0.080, val_acc:0.948]
Epoch [54/120    avg_loss:0.096, val_acc:0.956]
Epoch [55/120    avg_loss:0.057, val_acc:0.966]
Epoch [56/120    avg_loss:0.066, val_acc:0.961]
Epoch [57/120    avg_loss:0.045, val_acc:0.963]
Epoch [58/120    avg_loss:0.046, val_acc:0.969]
Epoch [59/120    avg_loss:0.038, val_acc:0.967]
Epoch [60/120    avg_loss:0.027, val_acc:0.970]
Epoch [61/120    avg_loss:0.028, val_acc:0.968]
Epoch [62/120    avg_loss:0.031, val_acc:0.968]
Epoch [63/120    avg_loss:0.032, val_acc:0.974]
Epoch [64/120    avg_loss:0.028, val_acc:0.975]
Epoch [65/120    avg_loss:0.021, val_acc:0.965]
Epoch [66/120    avg_loss:0.030, val_acc:0.973]
Epoch [67/120    avg_loss:0.028, val_acc:0.975]
Epoch [68/120    avg_loss:0.023, val_acc:0.977]
Epoch [69/120    avg_loss:0.030, val_acc:0.969]
Epoch [70/120    avg_loss:0.028, val_acc:0.970]
Epoch [71/120    avg_loss:0.017, val_acc:0.977]
Epoch [72/120    avg_loss:0.025, val_acc:0.976]
Epoch [73/120    avg_loss:0.020, val_acc:0.979]
Epoch [74/120    avg_loss:0.017, val_acc:0.979]
Epoch [75/120    avg_loss:0.017, val_acc:0.977]
Epoch [76/120    avg_loss:0.015, val_acc:0.985]
Epoch [77/120    avg_loss:0.016, val_acc:0.980]
Epoch [78/120    avg_loss:0.031, val_acc:0.977]
Epoch [79/120    avg_loss:0.020, val_acc:0.980]
Epoch [80/120    avg_loss:0.016, val_acc:0.984]
Epoch [81/120    avg_loss:0.016, val_acc:0.971]
Epoch [82/120    avg_loss:0.013, val_acc:0.980]
Epoch [83/120    avg_loss:0.016, val_acc:0.981]
Epoch [84/120    avg_loss:0.012, val_acc:0.977]
Epoch [85/120    avg_loss:0.015, val_acc:0.979]
Epoch [86/120    avg_loss:0.010, val_acc:0.981]
Epoch [87/120    avg_loss:0.011, val_acc:0.980]
Epoch [88/120    avg_loss:0.014, val_acc:0.984]
Epoch [89/120    avg_loss:0.014, val_acc:0.979]
Epoch [90/120    avg_loss:0.012, val_acc:0.982]
Epoch [91/120    avg_loss:0.012, val_acc:0.981]
Epoch [92/120    avg_loss:0.012, val_acc:0.982]
Epoch [93/120    avg_loss:0.010, val_acc:0.982]
Epoch [94/120    avg_loss:0.009, val_acc:0.985]
Epoch [95/120    avg_loss:0.012, val_acc:0.984]
Epoch [96/120    avg_loss:0.010, val_acc:0.984]
Epoch [97/120    avg_loss:0.009, val_acc:0.984]
Epoch [98/120    avg_loss:0.011, val_acc:0.985]
Epoch [99/120    avg_loss:0.008, val_acc:0.984]
Epoch [100/120    avg_loss:0.009, val_acc:0.982]
Epoch [101/120    avg_loss:0.011, val_acc:0.982]
Epoch [102/120    avg_loss:0.009, val_acc:0.982]
Epoch [103/120    avg_loss:0.008, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.985]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.010, val_acc:0.984]
Epoch [107/120    avg_loss:0.012, val_acc:0.984]
Epoch [108/120    avg_loss:0.007, val_acc:0.984]
Epoch [109/120    avg_loss:0.008, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.985]
Epoch [111/120    avg_loss:0.008, val_acc:0.984]
Epoch [112/120    avg_loss:0.009, val_acc:0.986]
Epoch [113/120    avg_loss:0.009, val_acc:0.982]
Epoch [114/120    avg_loss:0.008, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.008, val_acc:0.985]
Epoch [117/120    avg_loss:0.009, val_acc:0.985]
Epoch [118/120    avg_loss:0.009, val_acc:0.985]
Epoch [119/120    avg_loss:0.007, val_acc:0.985]
Epoch [120/120    avg_loss:0.008, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1262    0    5    0    1    0    0    0    6   11    0    0
     0    0    0]
 [   0    0    1  737    0    3    0    0    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2   15    0    1    0    0    0    0  856    0    0    0
     0    1    0]
 [   0    0    1    0    0    0    0    0    4    0   18 2183    3    0
     1    0    0]
 [   0    0    0    5    0    4    0    0    0    0   10    3  511    0
     0    1    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0    8    0    0    4    0    0    0    0
    70  265    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.95121951219512

F1 scores:
[       nan 0.98765432 0.98902821 0.98005319 0.98839907 0.98976109
 0.99167298 1.         0.99537037 0.7826087  0.96997167 0.99024722
 0.9742612  0.99728997 0.96977437 0.86319218 0.99401198]

Kappa:
0.9766335002934139
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f79aca2ee48>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.789, val_acc:0.209]
Epoch [2/120    avg_loss:2.701, val_acc:0.216]
Epoch [3/120    avg_loss:2.593, val_acc:0.361]
Epoch [4/120    avg_loss:2.492, val_acc:0.400]
Epoch [5/120    avg_loss:2.375, val_acc:0.420]
Epoch [6/120    avg_loss:2.274, val_acc:0.422]
Epoch [7/120    avg_loss:2.193, val_acc:0.434]
Epoch [8/120    avg_loss:2.110, val_acc:0.433]
Epoch [9/120    avg_loss:2.001, val_acc:0.481]
Epoch [10/120    avg_loss:1.906, val_acc:0.541]
Epoch [11/120    avg_loss:1.822, val_acc:0.606]
Epoch [12/120    avg_loss:1.711, val_acc:0.634]
Epoch [13/120    avg_loss:1.610, val_acc:0.618]
Epoch [14/120    avg_loss:1.504, val_acc:0.594]
Epoch [15/120    avg_loss:1.392, val_acc:0.674]
Epoch [16/120    avg_loss:1.229, val_acc:0.708]
Epoch [17/120    avg_loss:1.086, val_acc:0.734]
Epoch [18/120    avg_loss:0.993, val_acc:0.745]
Epoch [19/120    avg_loss:0.921, val_acc:0.753]
Epoch [20/120    avg_loss:0.830, val_acc:0.725]
Epoch [21/120    avg_loss:0.778, val_acc:0.781]
Epoch [22/120    avg_loss:0.751, val_acc:0.777]
Epoch [23/120    avg_loss:0.737, val_acc:0.775]
Epoch [24/120    avg_loss:0.661, val_acc:0.794]
Epoch [25/120    avg_loss:0.569, val_acc:0.823]
Epoch [26/120    avg_loss:0.509, val_acc:0.814]
Epoch [27/120    avg_loss:0.520, val_acc:0.826]
Epoch [28/120    avg_loss:0.451, val_acc:0.844]
Epoch [29/120    avg_loss:0.368, val_acc:0.855]
Epoch [30/120    avg_loss:0.400, val_acc:0.879]
Epoch [31/120    avg_loss:0.315, val_acc:0.874]
Epoch [32/120    avg_loss:0.317, val_acc:0.870]
Epoch [33/120    avg_loss:0.299, val_acc:0.894]
Epoch [34/120    avg_loss:0.297, val_acc:0.891]
Epoch [35/120    avg_loss:0.309, val_acc:0.875]
Epoch [36/120    avg_loss:0.217, val_acc:0.909]
Epoch [37/120    avg_loss:0.189, val_acc:0.907]
Epoch [38/120    avg_loss:0.160, val_acc:0.923]
Epoch [39/120    avg_loss:0.163, val_acc:0.922]
Epoch [40/120    avg_loss:0.142, val_acc:0.923]
Epoch [41/120    avg_loss:0.121, val_acc:0.928]
Epoch [42/120    avg_loss:0.120, val_acc:0.933]
Epoch [43/120    avg_loss:0.125, val_acc:0.935]
Epoch [44/120    avg_loss:0.140, val_acc:0.925]
Epoch [45/120    avg_loss:0.115, val_acc:0.914]
Epoch [46/120    avg_loss:0.100, val_acc:0.948]
Epoch [47/120    avg_loss:0.114, val_acc:0.928]
Epoch [48/120    avg_loss:0.102, val_acc:0.944]
Epoch [49/120    avg_loss:0.084, val_acc:0.943]
Epoch [50/120    avg_loss:0.095, val_acc:0.954]
Epoch [51/120    avg_loss:0.059, val_acc:0.957]
Epoch [52/120    avg_loss:0.071, val_acc:0.935]
Epoch [53/120    avg_loss:0.078, val_acc:0.940]
Epoch [54/120    avg_loss:0.088, val_acc:0.945]
Epoch [55/120    avg_loss:0.228, val_acc:0.936]
Epoch [56/120    avg_loss:0.117, val_acc:0.921]
Epoch [57/120    avg_loss:0.094, val_acc:0.944]
Epoch [58/120    avg_loss:0.068, val_acc:0.938]
Epoch [59/120    avg_loss:0.067, val_acc:0.964]
Epoch [60/120    avg_loss:0.058, val_acc:0.963]
Epoch [61/120    avg_loss:0.050, val_acc:0.964]
Epoch [62/120    avg_loss:0.039, val_acc:0.970]
Epoch [63/120    avg_loss:0.046, val_acc:0.967]
Epoch [64/120    avg_loss:0.041, val_acc:0.958]
Epoch [65/120    avg_loss:0.037, val_acc:0.954]
Epoch [66/120    avg_loss:0.032, val_acc:0.971]
Epoch [67/120    avg_loss:0.028, val_acc:0.974]
Epoch [68/120    avg_loss:0.029, val_acc:0.974]
Epoch [69/120    avg_loss:0.031, val_acc:0.968]
Epoch [70/120    avg_loss:0.035, val_acc:0.980]
Epoch [71/120    avg_loss:0.030, val_acc:0.976]
Epoch [72/120    avg_loss:0.032, val_acc:0.981]
Epoch [73/120    avg_loss:0.027, val_acc:0.971]
Epoch [74/120    avg_loss:0.037, val_acc:0.971]
Epoch [75/120    avg_loss:0.021, val_acc:0.966]
Epoch [76/120    avg_loss:0.026, val_acc:0.981]
Epoch [77/120    avg_loss:0.029, val_acc:0.973]
Epoch [78/120    avg_loss:0.023, val_acc:0.977]
Epoch [79/120    avg_loss:0.029, val_acc:0.970]
Epoch [80/120    avg_loss:0.042, val_acc:0.973]
Epoch [81/120    avg_loss:0.019, val_acc:0.977]
Epoch [82/120    avg_loss:0.027, val_acc:0.975]
Epoch [83/120    avg_loss:0.017, val_acc:0.978]
Epoch [84/120    avg_loss:0.023, val_acc:0.970]
Epoch [85/120    avg_loss:0.026, val_acc:0.977]
Epoch [86/120    avg_loss:0.019, val_acc:0.977]
Epoch [87/120    avg_loss:0.022, val_acc:0.970]
Epoch [88/120    avg_loss:0.019, val_acc:0.975]
Epoch [89/120    avg_loss:0.041, val_acc:0.958]
Epoch [90/120    avg_loss:0.026, val_acc:0.969]
Epoch [91/120    avg_loss:0.016, val_acc:0.974]
Epoch [92/120    avg_loss:0.013, val_acc:0.973]
Epoch [93/120    avg_loss:0.015, val_acc:0.979]
Epoch [94/120    avg_loss:0.012, val_acc:0.976]
Epoch [95/120    avg_loss:0.015, val_acc:0.977]
Epoch [96/120    avg_loss:0.012, val_acc:0.980]
Epoch [97/120    avg_loss:0.014, val_acc:0.981]
Epoch [98/120    avg_loss:0.011, val_acc:0.979]
Epoch [99/120    avg_loss:0.012, val_acc:0.981]
Epoch [100/120    avg_loss:0.014, val_acc:0.982]
Epoch [101/120    avg_loss:0.010, val_acc:0.981]
Epoch [102/120    avg_loss:0.010, val_acc:0.981]
Epoch [103/120    avg_loss:0.010, val_acc:0.982]
Epoch [104/120    avg_loss:0.011, val_acc:0.981]
Epoch [105/120    avg_loss:0.010, val_acc:0.980]
Epoch [106/120    avg_loss:0.009, val_acc:0.982]
Epoch [107/120    avg_loss:0.010, val_acc:0.982]
Epoch [108/120    avg_loss:0.013, val_acc:0.981]
Epoch [109/120    avg_loss:0.010, val_acc:0.981]
Epoch [110/120    avg_loss:0.009, val_acc:0.982]
Epoch [111/120    avg_loss:0.016, val_acc:0.981]
Epoch [112/120    avg_loss:0.008, val_acc:0.980]
Epoch [113/120    avg_loss:0.010, val_acc:0.982]
Epoch [114/120    avg_loss:0.010, val_acc:0.984]
Epoch [115/120    avg_loss:0.011, val_acc:0.982]
Epoch [116/120    avg_loss:0.009, val_acc:0.981]
Epoch [117/120    avg_loss:0.009, val_acc:0.981]
Epoch [118/120    avg_loss:0.011, val_acc:0.982]
Epoch [119/120    avg_loss:0.011, val_acc:0.981]
Epoch [120/120    avg_loss:0.009, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1262    2    4    0    0    0    0    0    1   16    0    0
     0    0    0]
 [   0    0    3  717    3    0    1    0    0   19    0    3    1    0
     0    0    0]
 [   0    0    0    7  206    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    1    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8   41    0    6    0    0    0    0  816    3    1    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0   18 2183    3    0
     0    0    0]
 [   0    0    0    1    0   14    0    0    0    0    0    8  509    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    50  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.34417344173441

F1 scores:
[       nan 0.98765432 0.98401559 0.94653465 0.96713615 0.97058824
 0.98720843 1.         1.         0.64285714 0.95438596 0.98711282
 0.97044805 1.         0.97427101 0.90189873 0.98809524]

Kappa:
0.9697102086658214
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe97c4a5e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.795, val_acc:0.312]
Epoch [2/120    avg_loss:2.705, val_acc:0.405]
Epoch [3/120    avg_loss:2.608, val_acc:0.453]
Epoch [4/120    avg_loss:2.508, val_acc:0.468]
Epoch [5/120    avg_loss:2.428, val_acc:0.478]
Epoch [6/120    avg_loss:2.328, val_acc:0.498]
Epoch [7/120    avg_loss:2.253, val_acc:0.516]
Epoch [8/120    avg_loss:2.137, val_acc:0.552]
Epoch [9/120    avg_loss:2.059, val_acc:0.548]
Epoch [10/120    avg_loss:1.958, val_acc:0.577]
Epoch [11/120    avg_loss:1.892, val_acc:0.582]
Epoch [12/120    avg_loss:1.753, val_acc:0.604]
Epoch [13/120    avg_loss:1.647, val_acc:0.620]
Epoch [14/120    avg_loss:1.487, val_acc:0.684]
Epoch [15/120    avg_loss:1.346, val_acc:0.685]
Epoch [16/120    avg_loss:1.235, val_acc:0.704]
Epoch [17/120    avg_loss:1.135, val_acc:0.732]
Epoch [18/120    avg_loss:0.962, val_acc:0.757]
Epoch [19/120    avg_loss:0.911, val_acc:0.780]
Epoch [20/120    avg_loss:0.824, val_acc:0.759]
Epoch [21/120    avg_loss:0.729, val_acc:0.786]
Epoch [22/120    avg_loss:0.649, val_acc:0.815]
Epoch [23/120    avg_loss:0.577, val_acc:0.842]
Epoch [24/120    avg_loss:0.520, val_acc:0.838]
Epoch [25/120    avg_loss:0.507, val_acc:0.832]
Epoch [26/120    avg_loss:0.445, val_acc:0.868]
Epoch [27/120    avg_loss:0.346, val_acc:0.884]
Epoch [28/120    avg_loss:0.367, val_acc:0.846]
Epoch [29/120    avg_loss:0.292, val_acc:0.893]
Epoch [30/120    avg_loss:0.259, val_acc:0.911]
Epoch [31/120    avg_loss:0.323, val_acc:0.782]
Epoch [32/120    avg_loss:0.387, val_acc:0.893]
Epoch [33/120    avg_loss:0.286, val_acc:0.861]
Epoch [34/120    avg_loss:0.254, val_acc:0.910]
Epoch [35/120    avg_loss:0.286, val_acc:0.905]
Epoch [36/120    avg_loss:0.234, val_acc:0.928]
Epoch [37/120    avg_loss:0.185, val_acc:0.928]
Epoch [38/120    avg_loss:0.166, val_acc:0.936]
Epoch [39/120    avg_loss:0.171, val_acc:0.928]
Epoch [40/120    avg_loss:0.166, val_acc:0.940]
Epoch [41/120    avg_loss:0.129, val_acc:0.942]
Epoch [42/120    avg_loss:0.127, val_acc:0.929]
Epoch [43/120    avg_loss:0.116, val_acc:0.934]
Epoch [44/120    avg_loss:0.138, val_acc:0.936]
Epoch [45/120    avg_loss:0.128, val_acc:0.932]
Epoch [46/120    avg_loss:0.090, val_acc:0.941]
Epoch [47/120    avg_loss:0.108, val_acc:0.934]
Epoch [48/120    avg_loss:0.084, val_acc:0.956]
Epoch [49/120    avg_loss:0.076, val_acc:0.966]
Epoch [50/120    avg_loss:0.090, val_acc:0.944]
Epoch [51/120    avg_loss:0.082, val_acc:0.950]
Epoch [52/120    avg_loss:0.058, val_acc:0.959]
Epoch [53/120    avg_loss:0.090, val_acc:0.946]
Epoch [54/120    avg_loss:0.074, val_acc:0.957]
Epoch [55/120    avg_loss:0.075, val_acc:0.962]
Epoch [56/120    avg_loss:0.058, val_acc:0.953]
Epoch [57/120    avg_loss:0.063, val_acc:0.961]
Epoch [58/120    avg_loss:0.054, val_acc:0.966]
Epoch [59/120    avg_loss:0.053, val_acc:0.962]
Epoch [60/120    avg_loss:0.050, val_acc:0.947]
Epoch [61/120    avg_loss:0.064, val_acc:0.957]
Epoch [62/120    avg_loss:0.057, val_acc:0.966]
Epoch [63/120    avg_loss:0.048, val_acc:0.970]
Epoch [64/120    avg_loss:0.039, val_acc:0.970]
Epoch [65/120    avg_loss:0.057, val_acc:0.955]
Epoch [66/120    avg_loss:0.047, val_acc:0.969]
Epoch [67/120    avg_loss:0.046, val_acc:0.968]
Epoch [68/120    avg_loss:0.029, val_acc:0.973]
Epoch [69/120    avg_loss:0.039, val_acc:0.967]
Epoch [70/120    avg_loss:0.033, val_acc:0.956]
Epoch [71/120    avg_loss:0.120, val_acc:0.961]
Epoch [72/120    avg_loss:0.083, val_acc:0.959]
Epoch [73/120    avg_loss:0.042, val_acc:0.968]
Epoch [74/120    avg_loss:0.043, val_acc:0.969]
Epoch [75/120    avg_loss:0.037, val_acc:0.969]
Epoch [76/120    avg_loss:0.026, val_acc:0.973]
Epoch [77/120    avg_loss:0.021, val_acc:0.977]
Epoch [78/120    avg_loss:0.023, val_acc:0.978]
Epoch [79/120    avg_loss:0.035, val_acc:0.970]
Epoch [80/120    avg_loss:0.033, val_acc:0.970]
Epoch [81/120    avg_loss:0.038, val_acc:0.975]
Epoch [82/120    avg_loss:0.027, val_acc:0.975]
Epoch [83/120    avg_loss:0.031, val_acc:0.985]
Epoch [84/120    avg_loss:0.024, val_acc:0.970]
Epoch [85/120    avg_loss:0.026, val_acc:0.970]
Epoch [86/120    avg_loss:0.017, val_acc:0.980]
Epoch [87/120    avg_loss:0.028, val_acc:0.977]
Epoch [88/120    avg_loss:0.034, val_acc:0.974]
Epoch [89/120    avg_loss:0.022, val_acc:0.976]
Epoch [90/120    avg_loss:0.020, val_acc:0.978]
Epoch [91/120    avg_loss:0.020, val_acc:0.974]
Epoch [92/120    avg_loss:0.027, val_acc:0.975]
Epoch [93/120    avg_loss:0.017, val_acc:0.973]
Epoch [94/120    avg_loss:0.013, val_acc:0.984]
Epoch [95/120    avg_loss:0.010, val_acc:0.981]
Epoch [96/120    avg_loss:0.016, val_acc:0.981]
Epoch [97/120    avg_loss:0.018, val_acc:0.986]
Epoch [98/120    avg_loss:0.014, val_acc:0.985]
Epoch [99/120    avg_loss:0.011, val_acc:0.986]
Epoch [100/120    avg_loss:0.010, val_acc:0.986]
Epoch [101/120    avg_loss:0.009, val_acc:0.986]
Epoch [102/120    avg_loss:0.010, val_acc:0.986]
Epoch [103/120    avg_loss:0.010, val_acc:0.986]
Epoch [104/120    avg_loss:0.010, val_acc:0.987]
Epoch [105/120    avg_loss:0.010, val_acc:0.988]
Epoch [106/120    avg_loss:0.008, val_acc:0.988]
Epoch [107/120    avg_loss:0.011, val_acc:0.987]
Epoch [108/120    avg_loss:0.012, val_acc:0.988]
Epoch [109/120    avg_loss:0.013, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.988]
Epoch [111/120    avg_loss:0.009, val_acc:0.988]
Epoch [112/120    avg_loss:0.009, val_acc:0.987]
Epoch [113/120    avg_loss:0.012, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.988]
Epoch [115/120    avg_loss:0.010, val_acc:0.985]
Epoch [116/120    avg_loss:0.009, val_acc:0.985]
Epoch [117/120    avg_loss:0.012, val_acc:0.987]
Epoch [118/120    avg_loss:0.010, val_acc:0.986]
Epoch [119/120    avg_loss:0.008, val_acc:0.987]
Epoch [120/120    avg_loss:0.011, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1243    1    6    0    0    0    0    0    9   26    0    0
     0    0    0]
 [   0    0    3  742    0    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    4    0    0   13    0    1    0    0
     0    0    0]
 [   0    0    8   12    0    4    1    0    0    0  829   14    4    0
     0    3    0]
 [   0    0    1    0    0    0    1    0    0    0   14 2194    0    0
     0    0    0]
 [   0    0    0    3    2    4    0    0    0    0    0    2  521    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    1    0    0
  1137    0    0]
 [   0    0    0    0    0    0   29    0    0    0    0    0    0    0
    88  230    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.24661246612466

F1 scores:
[       nan 0.975      0.97874016 0.98604651 0.98156682 0.98858447
 0.97253155 1.         1.         0.78787879 0.9583815  0.98651079
 0.98024459 1.         0.96111581 0.79173838 0.98203593]

Kappa:
0.9685675415138444
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8622509f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.809, val_acc:0.189]
Epoch [2/120    avg_loss:2.735, val_acc:0.206]
Epoch [3/120    avg_loss:2.667, val_acc:0.283]
Epoch [4/120    avg_loss:2.565, val_acc:0.383]
Epoch [5/120    avg_loss:2.460, val_acc:0.356]
Epoch [6/120    avg_loss:2.325, val_acc:0.409]
Epoch [7/120    avg_loss:2.208, val_acc:0.436]
Epoch [8/120    avg_loss:2.139, val_acc:0.506]
Epoch [9/120    avg_loss:2.013, val_acc:0.536]
Epoch [10/120    avg_loss:1.902, val_acc:0.551]
Epoch [11/120    avg_loss:1.798, val_acc:0.613]
Epoch [12/120    avg_loss:1.655, val_acc:0.617]
Epoch [13/120    avg_loss:1.554, val_acc:0.635]
Epoch [14/120    avg_loss:1.428, val_acc:0.669]
Epoch [15/120    avg_loss:1.241, val_acc:0.664]
Epoch [16/120    avg_loss:1.226, val_acc:0.715]
Epoch [17/120    avg_loss:1.124, val_acc:0.738]
Epoch [18/120    avg_loss:1.113, val_acc:0.764]
Epoch [19/120    avg_loss:0.953, val_acc:0.739]
Epoch [20/120    avg_loss:0.840, val_acc:0.784]
Epoch [21/120    avg_loss:0.769, val_acc:0.808]
Epoch [22/120    avg_loss:0.688, val_acc:0.823]
Epoch [23/120    avg_loss:0.642, val_acc:0.774]
Epoch [24/120    avg_loss:0.662, val_acc:0.832]
Epoch [25/120    avg_loss:0.606, val_acc:0.826]
Epoch [26/120    avg_loss:0.554, val_acc:0.852]
Epoch [27/120    avg_loss:0.430, val_acc:0.875]
Epoch [28/120    avg_loss:0.413, val_acc:0.876]
Epoch [29/120    avg_loss:0.394, val_acc:0.893]
Epoch [30/120    avg_loss:0.379, val_acc:0.822]
Epoch [31/120    avg_loss:0.408, val_acc:0.881]
Epoch [32/120    avg_loss:0.345, val_acc:0.895]
Epoch [33/120    avg_loss:0.385, val_acc:0.884]
Epoch [34/120    avg_loss:0.294, val_acc:0.874]
Epoch [35/120    avg_loss:0.284, val_acc:0.878]
Epoch [36/120    avg_loss:0.259, val_acc:0.875]
Epoch [37/120    avg_loss:0.244, val_acc:0.887]
Epoch [38/120    avg_loss:0.252, val_acc:0.916]
Epoch [39/120    avg_loss:0.209, val_acc:0.924]
Epoch [40/120    avg_loss:0.246, val_acc:0.885]
Epoch [41/120    avg_loss:0.230, val_acc:0.901]
Epoch [42/120    avg_loss:0.203, val_acc:0.908]
Epoch [43/120    avg_loss:0.251, val_acc:0.933]
Epoch [44/120    avg_loss:0.209, val_acc:0.901]
Epoch [45/120    avg_loss:0.159, val_acc:0.932]
Epoch [46/120    avg_loss:0.162, val_acc:0.918]
Epoch [47/120    avg_loss:0.128, val_acc:0.932]
Epoch [48/120    avg_loss:0.111, val_acc:0.933]
Epoch [49/120    avg_loss:0.136, val_acc:0.938]
Epoch [50/120    avg_loss:0.135, val_acc:0.951]
Epoch [51/120    avg_loss:0.131, val_acc:0.936]
Epoch [52/120    avg_loss:0.120, val_acc:0.941]
Epoch [53/120    avg_loss:0.123, val_acc:0.951]
Epoch [54/120    avg_loss:0.101, val_acc:0.955]
Epoch [55/120    avg_loss:0.091, val_acc:0.943]
Epoch [56/120    avg_loss:0.096, val_acc:0.938]
Epoch [57/120    avg_loss:0.076, val_acc:0.958]
Epoch [58/120    avg_loss:0.099, val_acc:0.951]
Epoch [59/120    avg_loss:0.090, val_acc:0.919]
Epoch [60/120    avg_loss:0.119, val_acc:0.935]
Epoch [61/120    avg_loss:0.150, val_acc:0.932]
Epoch [62/120    avg_loss:0.112, val_acc:0.922]
Epoch [63/120    avg_loss:0.114, val_acc:0.938]
Epoch [64/120    avg_loss:0.093, val_acc:0.942]
Epoch [65/120    avg_loss:0.080, val_acc:0.950]
Epoch [66/120    avg_loss:0.081, val_acc:0.948]
Epoch [67/120    avg_loss:0.065, val_acc:0.958]
Epoch [68/120    avg_loss:0.067, val_acc:0.944]
Epoch [69/120    avg_loss:0.064, val_acc:0.960]
Epoch [70/120    avg_loss:0.062, val_acc:0.953]
Epoch [71/120    avg_loss:0.065, val_acc:0.958]
Epoch [72/120    avg_loss:0.047, val_acc:0.959]
Epoch [73/120    avg_loss:0.049, val_acc:0.960]
Epoch [74/120    avg_loss:0.063, val_acc:0.958]
Epoch [75/120    avg_loss:0.047, val_acc:0.958]
Epoch [76/120    avg_loss:0.048, val_acc:0.959]
Epoch [77/120    avg_loss:0.045, val_acc:0.966]
Epoch [78/120    avg_loss:0.049, val_acc:0.961]
Epoch [79/120    avg_loss:0.035, val_acc:0.964]
Epoch [80/120    avg_loss:0.043, val_acc:0.966]
Epoch [81/120    avg_loss:0.039, val_acc:0.965]
Epoch [82/120    avg_loss:0.035, val_acc:0.966]
Epoch [83/120    avg_loss:0.026, val_acc:0.969]
Epoch [84/120    avg_loss:0.045, val_acc:0.970]
Epoch [85/120    avg_loss:0.032, val_acc:0.965]
Epoch [86/120    avg_loss:0.034, val_acc:0.940]
Epoch [87/120    avg_loss:0.041, val_acc:0.949]
Epoch [88/120    avg_loss:0.036, val_acc:0.958]
Epoch [89/120    avg_loss:0.034, val_acc:0.966]
Epoch [90/120    avg_loss:0.027, val_acc:0.968]
Epoch [91/120    avg_loss:0.029, val_acc:0.974]
Epoch [92/120    avg_loss:0.028, val_acc:0.963]
Epoch [93/120    avg_loss:0.028, val_acc:0.964]
Epoch [94/120    avg_loss:0.033, val_acc:0.967]
Epoch [95/120    avg_loss:0.051, val_acc:0.968]
Epoch [96/120    avg_loss:0.040, val_acc:0.967]
Epoch [97/120    avg_loss:0.034, val_acc:0.973]
Epoch [98/120    avg_loss:0.027, val_acc:0.959]
Epoch [99/120    avg_loss:0.028, val_acc:0.969]
Epoch [100/120    avg_loss:0.026, val_acc:0.967]
Epoch [101/120    avg_loss:0.033, val_acc:0.975]
Epoch [102/120    avg_loss:0.028, val_acc:0.967]
Epoch [103/120    avg_loss:0.030, val_acc:0.970]
Epoch [104/120    avg_loss:0.019, val_acc:0.973]
Epoch [105/120    avg_loss:0.023, val_acc:0.970]
Epoch [106/120    avg_loss:0.022, val_acc:0.974]
Epoch [107/120    avg_loss:0.026, val_acc:0.974]
Epoch [108/120    avg_loss:0.018, val_acc:0.977]
Epoch [109/120    avg_loss:0.020, val_acc:0.980]
Epoch [110/120    avg_loss:0.019, val_acc:0.975]
Epoch [111/120    avg_loss:0.021, val_acc:0.976]
Epoch [112/120    avg_loss:0.021, val_acc:0.978]
Epoch [113/120    avg_loss:0.014, val_acc:0.976]
Epoch [114/120    avg_loss:0.015, val_acc:0.978]
Epoch [115/120    avg_loss:0.022, val_acc:0.976]
Epoch [116/120    avg_loss:0.019, val_acc:0.980]
Epoch [117/120    avg_loss:0.021, val_acc:0.972]
Epoch [118/120    avg_loss:0.019, val_acc:0.973]
Epoch [119/120    avg_loss:0.057, val_acc:0.939]
Epoch [120/120    avg_loss:0.068, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    2 1226    5    1    0   12    0    0    0   12   23    1    0
     0    1    2]
 [   0    0    5  727    0    1    0    0    0    9    0    0    4    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  397    3    5    0    4    0    1    0    0
    25    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    6    0    0    8    0    0    0    0
     0    0    0]
 [   0    0   36   62    0    2    0    0    0    0  748   17    0    0
     2    8    0]
 [   0    0   15    0    0    0   13    0    6    0    3 2166    6    0
     0    1    0]
 [   0    0    0   34    3   11    0    0    0    2    8    0  470    0
     0    3    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    2    0    3    0    1    0    0    0
  1133    0    0]
 [   0    0    0    0    0    0   41    0    0    3    0    0    0    0
   119  184    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0   21    0
     0    0   63]]

Accuracy:
94.00542005420054

F1 scores:
[       nan 0.96385542 0.95520062 0.92083597 0.99069767 0.93853428
 0.94464414 0.90909091 0.98964327 0.36363636 0.90776699 0.98075617
 0.90733591 0.99730458 0.93713813 0.67647059 0.82894737]

Kappa:
0.9315664037252248
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7bead71f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.802, val_acc:0.106]
Epoch [2/120    avg_loss:2.723, val_acc:0.215]
Epoch [3/120    avg_loss:2.621, val_acc:0.327]
Epoch [4/120    avg_loss:2.516, val_acc:0.372]
Epoch [5/120    avg_loss:2.432, val_acc:0.439]
Epoch [6/120    avg_loss:2.322, val_acc:0.455]
Epoch [7/120    avg_loss:2.225, val_acc:0.540]
Epoch [8/120    avg_loss:2.148, val_acc:0.578]
Epoch [9/120    avg_loss:2.007, val_acc:0.566]
Epoch [10/120    avg_loss:1.924, val_acc:0.583]
Epoch [11/120    avg_loss:1.759, val_acc:0.605]
Epoch [12/120    avg_loss:1.703, val_acc:0.655]
Epoch [13/120    avg_loss:1.527, val_acc:0.652]
Epoch [14/120    avg_loss:1.410, val_acc:0.682]
Epoch [15/120    avg_loss:1.267, val_acc:0.664]
Epoch [16/120    avg_loss:1.205, val_acc:0.724]
Epoch [17/120    avg_loss:1.120, val_acc:0.703]
Epoch [18/120    avg_loss:1.007, val_acc:0.718]
Epoch [19/120    avg_loss:0.913, val_acc:0.715]
Epoch [20/120    avg_loss:0.798, val_acc:0.807]
Epoch [21/120    avg_loss:0.701, val_acc:0.795]
Epoch [22/120    avg_loss:0.737, val_acc:0.811]
Epoch [23/120    avg_loss:0.645, val_acc:0.817]
Epoch [24/120    avg_loss:0.536, val_acc:0.850]
Epoch [25/120    avg_loss:0.552, val_acc:0.859]
Epoch [26/120    avg_loss:0.475, val_acc:0.845]
Epoch [27/120    avg_loss:0.453, val_acc:0.873]
Epoch [28/120    avg_loss:0.390, val_acc:0.884]
Epoch [29/120    avg_loss:0.356, val_acc:0.871]
Epoch [30/120    avg_loss:0.322, val_acc:0.890]
Epoch [31/120    avg_loss:0.374, val_acc:0.888]
Epoch [32/120    avg_loss:0.325, val_acc:0.893]
Epoch [33/120    avg_loss:0.225, val_acc:0.927]
Epoch [34/120    avg_loss:0.218, val_acc:0.908]
Epoch [35/120    avg_loss:0.232, val_acc:0.929]
Epoch [36/120    avg_loss:0.194, val_acc:0.910]
Epoch [37/120    avg_loss:0.161, val_acc:0.917]
Epoch [38/120    avg_loss:0.170, val_acc:0.933]
Epoch [39/120    avg_loss:0.139, val_acc:0.934]
Epoch [40/120    avg_loss:0.137, val_acc:0.932]
Epoch [41/120    avg_loss:0.118, val_acc:0.949]
Epoch [42/120    avg_loss:0.127, val_acc:0.941]
Epoch [43/120    avg_loss:0.100, val_acc:0.942]
Epoch [44/120    avg_loss:0.101, val_acc:0.940]
Epoch [45/120    avg_loss:0.089, val_acc:0.953]
Epoch [46/120    avg_loss:0.089, val_acc:0.949]
Epoch [47/120    avg_loss:0.097, val_acc:0.950]
Epoch [48/120    avg_loss:0.094, val_acc:0.949]
Epoch [49/120    avg_loss:0.116, val_acc:0.949]
Epoch [50/120    avg_loss:0.074, val_acc:0.961]
Epoch [51/120    avg_loss:0.064, val_acc:0.962]
Epoch [52/120    avg_loss:0.064, val_acc:0.949]
Epoch [53/120    avg_loss:0.080, val_acc:0.930]
Epoch [54/120    avg_loss:0.069, val_acc:0.951]
Epoch [55/120    avg_loss:0.071, val_acc:0.960]
Epoch [56/120    avg_loss:0.054, val_acc:0.962]
Epoch [57/120    avg_loss:0.054, val_acc:0.933]
Epoch [58/120    avg_loss:0.048, val_acc:0.960]
Epoch [59/120    avg_loss:0.045, val_acc:0.963]
Epoch [60/120    avg_loss:0.039, val_acc:0.972]
Epoch [61/120    avg_loss:0.047, val_acc:0.933]
Epoch [62/120    avg_loss:0.102, val_acc:0.942]
Epoch [63/120    avg_loss:0.084, val_acc:0.921]
Epoch [64/120    avg_loss:0.084, val_acc:0.933]
Epoch [65/120    avg_loss:0.118, val_acc:0.943]
Epoch [66/120    avg_loss:0.084, val_acc:0.958]
Epoch [67/120    avg_loss:0.048, val_acc:0.951]
Epoch [68/120    avg_loss:0.057, val_acc:0.963]
Epoch [69/120    avg_loss:0.039, val_acc:0.960]
Epoch [70/120    avg_loss:0.038, val_acc:0.975]
Epoch [71/120    avg_loss:0.048, val_acc:0.963]
Epoch [72/120    avg_loss:0.039, val_acc:0.953]
Epoch [73/120    avg_loss:0.032, val_acc:0.950]
Epoch [74/120    avg_loss:0.034, val_acc:0.979]
Epoch [75/120    avg_loss:0.028, val_acc:0.971]
Epoch [76/120    avg_loss:0.047, val_acc:0.946]
Epoch [77/120    avg_loss:0.024, val_acc:0.972]
Epoch [78/120    avg_loss:0.025, val_acc:0.975]
Epoch [79/120    avg_loss:0.027, val_acc:0.980]
Epoch [80/120    avg_loss:0.023, val_acc:0.975]
Epoch [81/120    avg_loss:0.017, val_acc:0.977]
Epoch [82/120    avg_loss:0.017, val_acc:0.980]
Epoch [83/120    avg_loss:0.019, val_acc:0.967]
Epoch [84/120    avg_loss:0.020, val_acc:0.980]
Epoch [85/120    avg_loss:0.024, val_acc:0.972]
Epoch [86/120    avg_loss:0.019, val_acc:0.967]
Epoch [87/120    avg_loss:0.013, val_acc:0.975]
Epoch [88/120    avg_loss:0.026, val_acc:0.982]
Epoch [89/120    avg_loss:0.023, val_acc:0.977]
Epoch [90/120    avg_loss:0.019, val_acc:0.969]
Epoch [91/120    avg_loss:0.020, val_acc:0.980]
Epoch [92/120    avg_loss:0.015, val_acc:0.979]
Epoch [93/120    avg_loss:0.013, val_acc:0.978]
Epoch [94/120    avg_loss:0.010, val_acc:0.978]
Epoch [95/120    avg_loss:0.010, val_acc:0.982]
Epoch [96/120    avg_loss:0.011, val_acc:0.977]
Epoch [97/120    avg_loss:0.012, val_acc:0.982]
Epoch [98/120    avg_loss:0.012, val_acc:0.980]
Epoch [99/120    avg_loss:0.012, val_acc:0.980]
Epoch [100/120    avg_loss:0.012, val_acc:0.969]
Epoch [101/120    avg_loss:0.022, val_acc:0.975]
Epoch [102/120    avg_loss:0.024, val_acc:0.971]
Epoch [103/120    avg_loss:0.032, val_acc:0.965]
Epoch [104/120    avg_loss:0.024, val_acc:0.975]
Epoch [105/120    avg_loss:0.053, val_acc:0.954]
Epoch [106/120    avg_loss:0.044, val_acc:0.959]
Epoch [107/120    avg_loss:0.028, val_acc:0.974]
Epoch [108/120    avg_loss:0.019, val_acc:0.978]
Epoch [109/120    avg_loss:0.014, val_acc:0.980]
Epoch [110/120    avg_loss:0.012, val_acc:0.979]
Epoch [111/120    avg_loss:0.010, val_acc:0.982]
Epoch [112/120    avg_loss:0.008, val_acc:0.982]
Epoch [113/120    avg_loss:0.009, val_acc:0.983]
Epoch [114/120    avg_loss:0.010, val_acc:0.985]
Epoch [115/120    avg_loss:0.011, val_acc:0.987]
Epoch [116/120    avg_loss:0.012, val_acc:0.983]
Epoch [117/120    avg_loss:0.010, val_acc:0.985]
Epoch [118/120    avg_loss:0.008, val_acc:0.987]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.008, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1255    2    0    0    0    0    0    0    2   26    0    0
     0    0    0]
 [   0    0    1  738    1    0    0    0    0    6    0    0    1    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    2    0    0    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3   87    0    2    1    0    0    0  770   11    1    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0   10 2188    7    0
     0    0    0]
 [   0    0    0    6    0    1    0    0    0    0    9   11  503    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    1    0    0    0    0    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   40    0    0    0    0    0    0    0
    52  255    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.71544715447155

F1 scores:
[       nan 1.         0.98469988 0.93358634 0.99530516 0.9837963
 0.96902655 0.96153846 1.         0.85714286 0.92436975 0.98425551
 0.9608405  1.         0.9734589  0.84577114 0.97647059]

Kappa:
0.9625176911790848
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3a04637f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.792, val_acc:0.210]
Epoch [2/120    avg_loss:2.709, val_acc:0.439]
Epoch [3/120    avg_loss:2.615, val_acc:0.461]
Epoch [4/120    avg_loss:2.513, val_acc:0.493]
Epoch [5/120    avg_loss:2.393, val_acc:0.506]
Epoch [6/120    avg_loss:2.283, val_acc:0.537]
Epoch [7/120    avg_loss:2.211, val_acc:0.572]
Epoch [8/120    avg_loss:2.117, val_acc:0.601]
Epoch [9/120    avg_loss:2.023, val_acc:0.616]
Epoch [10/120    avg_loss:1.926, val_acc:0.602]
Epoch [11/120    avg_loss:1.795, val_acc:0.644]
Epoch [12/120    avg_loss:1.707, val_acc:0.659]
Epoch [13/120    avg_loss:1.531, val_acc:0.710]
Epoch [14/120    avg_loss:1.353, val_acc:0.709]
Epoch [15/120    avg_loss:1.266, val_acc:0.716]
Epoch [16/120    avg_loss:1.030, val_acc:0.743]
Epoch [17/120    avg_loss:0.919, val_acc:0.756]
Epoch [18/120    avg_loss:0.884, val_acc:0.797]
Epoch [19/120    avg_loss:0.789, val_acc:0.812]
Epoch [20/120    avg_loss:0.782, val_acc:0.785]
Epoch [21/120    avg_loss:0.686, val_acc:0.777]
Epoch [22/120    avg_loss:0.616, val_acc:0.841]
Epoch [23/120    avg_loss:0.578, val_acc:0.841]
Epoch [24/120    avg_loss:0.556, val_acc:0.849]
Epoch [25/120    avg_loss:0.556, val_acc:0.817]
Epoch [26/120    avg_loss:0.489, val_acc:0.848]
Epoch [27/120    avg_loss:0.427, val_acc:0.880]
Epoch [28/120    avg_loss:0.405, val_acc:0.893]
Epoch [29/120    avg_loss:0.370, val_acc:0.892]
Epoch [30/120    avg_loss:0.323, val_acc:0.894]
Epoch [31/120    avg_loss:0.326, val_acc:0.872]
Epoch [32/120    avg_loss:0.621, val_acc:0.842]
Epoch [33/120    avg_loss:0.380, val_acc:0.887]
Epoch [34/120    avg_loss:0.284, val_acc:0.852]
Epoch [35/120    avg_loss:0.254, val_acc:0.914]
Epoch [36/120    avg_loss:0.264, val_acc:0.936]
Epoch [37/120    avg_loss:0.241, val_acc:0.922]
Epoch [38/120    avg_loss:0.237, val_acc:0.926]
Epoch [39/120    avg_loss:0.212, val_acc:0.922]
Epoch [40/120    avg_loss:0.186, val_acc:0.928]
Epoch [41/120    avg_loss:0.165, val_acc:0.938]
Epoch [42/120    avg_loss:0.156, val_acc:0.943]
Epoch [43/120    avg_loss:0.155, val_acc:0.941]
Epoch [44/120    avg_loss:0.130, val_acc:0.922]
Epoch [45/120    avg_loss:0.151, val_acc:0.940]
Epoch [46/120    avg_loss:0.119, val_acc:0.955]
Epoch [47/120    avg_loss:0.121, val_acc:0.938]
Epoch [48/120    avg_loss:0.121, val_acc:0.935]
Epoch [49/120    avg_loss:0.109, val_acc:0.952]
Epoch [50/120    avg_loss:0.095, val_acc:0.958]
Epoch [51/120    avg_loss:0.075, val_acc:0.945]
Epoch [52/120    avg_loss:0.082, val_acc:0.956]
Epoch [53/120    avg_loss:0.075, val_acc:0.968]
Epoch [54/120    avg_loss:0.067, val_acc:0.951]
Epoch [55/120    avg_loss:0.777, val_acc:0.836]
Epoch [56/120    avg_loss:0.237, val_acc:0.957]
Epoch [57/120    avg_loss:0.158, val_acc:0.943]
Epoch [58/120    avg_loss:0.152, val_acc:0.939]
Epoch [59/120    avg_loss:0.092, val_acc:0.948]
Epoch [60/120    avg_loss:0.114, val_acc:0.892]
Epoch [61/120    avg_loss:0.162, val_acc:0.925]
Epoch [62/120    avg_loss:0.106, val_acc:0.959]
Epoch [63/120    avg_loss:0.097, val_acc:0.947]
Epoch [64/120    avg_loss:0.076, val_acc:0.944]
Epoch [65/120    avg_loss:0.071, val_acc:0.955]
Epoch [66/120    avg_loss:0.064, val_acc:0.966]
Epoch [67/120    avg_loss:0.050, val_acc:0.969]
Epoch [68/120    avg_loss:0.044, val_acc:0.970]
Epoch [69/120    avg_loss:0.041, val_acc:0.972]
Epoch [70/120    avg_loss:0.034, val_acc:0.975]
Epoch [71/120    avg_loss:0.035, val_acc:0.974]
Epoch [72/120    avg_loss:0.035, val_acc:0.973]
Epoch [73/120    avg_loss:0.036, val_acc:0.974]
Epoch [74/120    avg_loss:0.036, val_acc:0.974]
Epoch [75/120    avg_loss:0.029, val_acc:0.973]
Epoch [76/120    avg_loss:0.036, val_acc:0.972]
Epoch [77/120    avg_loss:0.030, val_acc:0.975]
Epoch [78/120    avg_loss:0.034, val_acc:0.973]
Epoch [79/120    avg_loss:0.034, val_acc:0.972]
Epoch [80/120    avg_loss:0.027, val_acc:0.970]
Epoch [81/120    avg_loss:0.032, val_acc:0.974]
Epoch [82/120    avg_loss:0.033, val_acc:0.972]
Epoch [83/120    avg_loss:0.032, val_acc:0.974]
Epoch [84/120    avg_loss:0.032, val_acc:0.973]
Epoch [85/120    avg_loss:0.035, val_acc:0.974]
Epoch [86/120    avg_loss:0.031, val_acc:0.975]
Epoch [87/120    avg_loss:0.031, val_acc:0.977]
Epoch [88/120    avg_loss:0.029, val_acc:0.972]
Epoch [89/120    avg_loss:0.034, val_acc:0.975]
Epoch [90/120    avg_loss:0.030, val_acc:0.974]
Epoch [91/120    avg_loss:0.028, val_acc:0.974]
Epoch [92/120    avg_loss:0.034, val_acc:0.972]
Epoch [93/120    avg_loss:0.029, val_acc:0.972]
Epoch [94/120    avg_loss:0.030, val_acc:0.969]
Epoch [95/120    avg_loss:0.032, val_acc:0.974]
Epoch [96/120    avg_loss:0.028, val_acc:0.973]
Epoch [97/120    avg_loss:0.027, val_acc:0.977]
Epoch [98/120    avg_loss:0.032, val_acc:0.972]
Epoch [99/120    avg_loss:0.027, val_acc:0.975]
Epoch [100/120    avg_loss:0.032, val_acc:0.974]
Epoch [101/120    avg_loss:0.028, val_acc:0.975]
Epoch [102/120    avg_loss:0.027, val_acc:0.970]
Epoch [103/120    avg_loss:0.029, val_acc:0.969]
Epoch [104/120    avg_loss:0.027, val_acc:0.973]
Epoch [105/120    avg_loss:0.024, val_acc:0.972]
Epoch [106/120    avg_loss:0.024, val_acc:0.975]
Epoch [107/120    avg_loss:0.029, val_acc:0.975]
Epoch [108/120    avg_loss:0.028, val_acc:0.975]
Epoch [109/120    avg_loss:0.028, val_acc:0.975]
Epoch [110/120    avg_loss:0.033, val_acc:0.974]
Epoch [111/120    avg_loss:0.029, val_acc:0.975]
Epoch [112/120    avg_loss:0.024, val_acc:0.980]
Epoch [113/120    avg_loss:0.026, val_acc:0.980]
Epoch [114/120    avg_loss:0.027, val_acc:0.978]
Epoch [115/120    avg_loss:0.025, val_acc:0.978]
Epoch [116/120    avg_loss:0.024, val_acc:0.978]
Epoch [117/120    avg_loss:0.024, val_acc:0.978]
Epoch [118/120    avg_loss:0.025, val_acc:0.978]
Epoch [119/120    avg_loss:0.022, val_acc:0.978]
Epoch [120/120    avg_loss:0.028, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1243    1    0    0    1    0    0    0    4   32    3    0
     0    1    0]
 [   0    0    4  721    0    5    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    3    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   20   90    0    3    0    0    0    0  757    5    0    0
     0    0    0]
 [   0    0    2    0    3    0    1    0    1    0    2 2196    3    1
     1    0    0]
 [   0    0    0    1    0    6    0    0    0    0    4   40  480    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    0    0    0    0
  1137    0    0]
 [   0    0    0    0    0    1   27    0    0    0    0    0    0    0
   116  203    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.59891598915989

F1 scores:
[       nan 0.975      0.9733751  0.92435897 0.99300699 0.9784336
 0.97840655 1.         0.99652375 0.65454545 0.92092457 0.9794826
 0.94025465 0.99730458 0.95027163 0.73550725 0.98823529]

Kappa:
0.9497155745017263
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff546939f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.818, val_acc:0.300]
Epoch [2/120    avg_loss:2.744, val_acc:0.355]
Epoch [3/120    avg_loss:2.664, val_acc:0.378]
Epoch [4/120    avg_loss:2.566, val_acc:0.459]
Epoch [5/120    avg_loss:2.452, val_acc:0.499]
Epoch [6/120    avg_loss:2.384, val_acc:0.517]
Epoch [7/120    avg_loss:2.274, val_acc:0.535]
Epoch [8/120    avg_loss:2.179, val_acc:0.520]
Epoch [9/120    avg_loss:2.082, val_acc:0.597]
Epoch [10/120    avg_loss:2.040, val_acc:0.622]
Epoch [11/120    avg_loss:1.930, val_acc:0.613]
Epoch [12/120    avg_loss:1.806, val_acc:0.657]
Epoch [13/120    avg_loss:1.666, val_acc:0.651]
Epoch [14/120    avg_loss:1.537, val_acc:0.707]
Epoch [15/120    avg_loss:1.427, val_acc:0.716]
Epoch [16/120    avg_loss:1.250, val_acc:0.695]
Epoch [17/120    avg_loss:1.112, val_acc:0.724]
Epoch [18/120    avg_loss:1.047, val_acc:0.760]
Epoch [19/120    avg_loss:0.954, val_acc:0.741]
Epoch [20/120    avg_loss:0.825, val_acc:0.807]
Epoch [21/120    avg_loss:0.718, val_acc:0.808]
Epoch [22/120    avg_loss:0.672, val_acc:0.835]
Epoch [23/120    avg_loss:0.594, val_acc:0.835]
Epoch [24/120    avg_loss:0.530, val_acc:0.868]
Epoch [25/120    avg_loss:0.532, val_acc:0.858]
Epoch [26/120    avg_loss:0.471, val_acc:0.874]
Epoch [27/120    avg_loss:0.462, val_acc:0.870]
Epoch [28/120    avg_loss:0.393, val_acc:0.872]
Epoch [29/120    avg_loss:0.383, val_acc:0.864]
Epoch [30/120    avg_loss:0.297, val_acc:0.900]
Epoch [31/120    avg_loss:0.312, val_acc:0.897]
Epoch [32/120    avg_loss:0.264, val_acc:0.890]
Epoch [33/120    avg_loss:0.264, val_acc:0.907]
Epoch [34/120    avg_loss:0.260, val_acc:0.890]
Epoch [35/120    avg_loss:0.220, val_acc:0.895]
Epoch [36/120    avg_loss:0.258, val_acc:0.909]
Epoch [37/120    avg_loss:0.260, val_acc:0.859]
Epoch [38/120    avg_loss:0.216, val_acc:0.918]
Epoch [39/120    avg_loss:0.213, val_acc:0.902]
Epoch [40/120    avg_loss:0.217, val_acc:0.893]
Epoch [41/120    avg_loss:0.182, val_acc:0.912]
Epoch [42/120    avg_loss:0.149, val_acc:0.901]
Epoch [43/120    avg_loss:0.166, val_acc:0.933]
Epoch [44/120    avg_loss:0.124, val_acc:0.933]
Epoch [45/120    avg_loss:0.115, val_acc:0.938]
Epoch [46/120    avg_loss:0.126, val_acc:0.935]
Epoch [47/120    avg_loss:0.116, val_acc:0.943]
Epoch [48/120    avg_loss:0.105, val_acc:0.942]
Epoch [49/120    avg_loss:0.101, val_acc:0.942]
Epoch [50/120    avg_loss:0.084, val_acc:0.944]
Epoch [51/120    avg_loss:0.088, val_acc:0.945]
Epoch [52/120    avg_loss:0.100, val_acc:0.943]
Epoch [53/120    avg_loss:0.107, val_acc:0.943]
Epoch [54/120    avg_loss:0.084, val_acc:0.947]
Epoch [55/120    avg_loss:0.069, val_acc:0.958]
Epoch [56/120    avg_loss:0.076, val_acc:0.951]
Epoch [57/120    avg_loss:0.069, val_acc:0.945]
Epoch [58/120    avg_loss:0.067, val_acc:0.957]
Epoch [59/120    avg_loss:0.069, val_acc:0.951]
Epoch [60/120    avg_loss:0.062, val_acc:0.943]
Epoch [61/120    avg_loss:0.088, val_acc:0.943]
Epoch [62/120    avg_loss:0.092, val_acc:0.955]
Epoch [63/120    avg_loss:0.059, val_acc:0.926]
Epoch [64/120    avg_loss:0.057, val_acc:0.952]
Epoch [65/120    avg_loss:0.043, val_acc:0.958]
Epoch [66/120    avg_loss:0.055, val_acc:0.953]
Epoch [67/120    avg_loss:0.040, val_acc:0.963]
Epoch [68/120    avg_loss:0.044, val_acc:0.944]
Epoch [69/120    avg_loss:0.038, val_acc:0.970]
Epoch [70/120    avg_loss:0.043, val_acc:0.938]
Epoch [71/120    avg_loss:0.076, val_acc:0.958]
Epoch [72/120    avg_loss:0.057, val_acc:0.961]
Epoch [73/120    avg_loss:0.052, val_acc:0.964]
Epoch [74/120    avg_loss:0.043, val_acc:0.959]
Epoch [75/120    avg_loss:0.044, val_acc:0.970]
Epoch [76/120    avg_loss:0.038, val_acc:0.967]
Epoch [77/120    avg_loss:0.040, val_acc:0.967]
Epoch [78/120    avg_loss:0.031, val_acc:0.967]
Epoch [79/120    avg_loss:0.045, val_acc:0.961]
Epoch [80/120    avg_loss:0.030, val_acc:0.968]
Epoch [81/120    avg_loss:0.027, val_acc:0.964]
Epoch [82/120    avg_loss:0.042, val_acc:0.958]
Epoch [83/120    avg_loss:0.042, val_acc:0.958]
Epoch [84/120    avg_loss:0.041, val_acc:0.963]
Epoch [85/120    avg_loss:0.041, val_acc:0.965]
Epoch [86/120    avg_loss:0.029, val_acc:0.970]
Epoch [87/120    avg_loss:0.027, val_acc:0.973]
Epoch [88/120    avg_loss:0.022, val_acc:0.976]
Epoch [89/120    avg_loss:0.029, val_acc:0.969]
Epoch [90/120    avg_loss:0.020, val_acc:0.980]
Epoch [91/120    avg_loss:0.025, val_acc:0.965]
Epoch [92/120    avg_loss:0.035, val_acc:0.969]
Epoch [93/120    avg_loss:0.028, val_acc:0.974]
Epoch [94/120    avg_loss:0.028, val_acc:0.973]
Epoch [95/120    avg_loss:0.023, val_acc:0.977]
Epoch [96/120    avg_loss:0.021, val_acc:0.967]
Epoch [97/120    avg_loss:0.030, val_acc:0.976]
Epoch [98/120    avg_loss:0.025, val_acc:0.967]
Epoch [99/120    avg_loss:0.025, val_acc:0.974]
Epoch [100/120    avg_loss:0.021, val_acc:0.972]
Epoch [101/120    avg_loss:0.019, val_acc:0.975]
Epoch [102/120    avg_loss:0.031, val_acc:0.970]
Epoch [103/120    avg_loss:0.022, val_acc:0.973]
Epoch [104/120    avg_loss:0.020, val_acc:0.974]
Epoch [105/120    avg_loss:0.016, val_acc:0.977]
Epoch [106/120    avg_loss:0.013, val_acc:0.976]
Epoch [107/120    avg_loss:0.015, val_acc:0.976]
Epoch [108/120    avg_loss:0.012, val_acc:0.975]
Epoch [109/120    avg_loss:0.011, val_acc:0.975]
Epoch [110/120    avg_loss:0.016, val_acc:0.976]
Epoch [111/120    avg_loss:0.014, val_acc:0.976]
Epoch [112/120    avg_loss:0.015, val_acc:0.976]
Epoch [113/120    avg_loss:0.010, val_acc:0.976]
Epoch [114/120    avg_loss:0.014, val_acc:0.975]
Epoch [115/120    avg_loss:0.010, val_acc:0.976]
Epoch [116/120    avg_loss:0.011, val_acc:0.977]
Epoch [117/120    avg_loss:0.010, val_acc:0.977]
Epoch [118/120    avg_loss:0.013, val_acc:0.977]
Epoch [119/120    avg_loss:0.010, val_acc:0.977]
Epoch [120/120    avg_loss:0.011, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1241    0    0    0    1    0    0    0    8   29    3    0
     0    3    0]
 [   0    0    1  730    0    5    0    0    0    6    0    0    1    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    1    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    2    0    0   13    0    0    1    0
     0    0    0]
 [   0    0   10   90    0    4    0    0    0    0  754    4    3    0
     0   10    0]
 [   0    0    8    0    0    0    6    0    0   14   12 2166    2    1
     1    0    0]
 [   0    0    0    0    0    6    0    0    0    0    5   14  506    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    0    0    0    0
  1133    0    0]
 [   0    0    0    0    0    2   20    0    0    2    0    0    1    0
    85  237    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.8048780487805

F1 scores:
[       nan 0.975      0.97524558 0.930529   1.         0.96723164
 0.97764531 0.98039216 0.99883856 0.48148148 0.91062802 0.97920434
 0.96106363 0.98666667 0.95895049 0.79396985 0.9704142 ]

Kappa:
0.9521511191919665
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2dc88586a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.813, val_acc:0.253]
Epoch [2/120    avg_loss:2.739, val_acc:0.397]
Epoch [3/120    avg_loss:2.644, val_acc:0.383]
Epoch [4/120    avg_loss:2.545, val_acc:0.388]
Epoch [5/120    avg_loss:2.454, val_acc:0.438]
Epoch [6/120    avg_loss:2.361, val_acc:0.453]
Epoch [7/120    avg_loss:2.282, val_acc:0.497]
Epoch [8/120    avg_loss:2.189, val_acc:0.518]
Epoch [9/120    avg_loss:2.115, val_acc:0.550]
Epoch [10/120    avg_loss:2.064, val_acc:0.537]
Epoch [11/120    avg_loss:1.971, val_acc:0.575]
Epoch [12/120    avg_loss:1.880, val_acc:0.585]
Epoch [13/120    avg_loss:1.803, val_acc:0.627]
Epoch [14/120    avg_loss:1.715, val_acc:0.634]
Epoch [15/120    avg_loss:1.655, val_acc:0.636]
Epoch [16/120    avg_loss:1.542, val_acc:0.666]
Epoch [17/120    avg_loss:1.443, val_acc:0.649]
Epoch [18/120    avg_loss:1.271, val_acc:0.670]
Epoch [19/120    avg_loss:1.224, val_acc:0.703]
Epoch [20/120    avg_loss:1.129, val_acc:0.723]
Epoch [21/120    avg_loss:1.112, val_acc:0.733]
Epoch [22/120    avg_loss:0.995, val_acc:0.747]
Epoch [23/120    avg_loss:0.955, val_acc:0.774]
Epoch [24/120    avg_loss:0.874, val_acc:0.768]
Epoch [25/120    avg_loss:0.787, val_acc:0.760]
Epoch [26/120    avg_loss:0.868, val_acc:0.748]
Epoch [27/120    avg_loss:0.727, val_acc:0.824]
Epoch [28/120    avg_loss:0.622, val_acc:0.832]
Epoch [29/120    avg_loss:0.537, val_acc:0.844]
Epoch [30/120    avg_loss:0.535, val_acc:0.859]
Epoch [31/120    avg_loss:0.496, val_acc:0.882]
Epoch [32/120    avg_loss:0.469, val_acc:0.847]
Epoch [33/120    avg_loss:0.412, val_acc:0.881]
Epoch [34/120    avg_loss:0.331, val_acc:0.903]
Epoch [35/120    avg_loss:0.328, val_acc:0.880]
Epoch [36/120    avg_loss:0.291, val_acc:0.910]
Epoch [37/120    avg_loss:0.253, val_acc:0.917]
Epoch [38/120    avg_loss:0.233, val_acc:0.911]
Epoch [39/120    avg_loss:0.238, val_acc:0.816]
Epoch [40/120    avg_loss:0.368, val_acc:0.902]
Epoch [41/120    avg_loss:0.232, val_acc:0.919]
Epoch [42/120    avg_loss:0.188, val_acc:0.943]
Epoch [43/120    avg_loss:0.168, val_acc:0.915]
Epoch [44/120    avg_loss:0.182, val_acc:0.931]
Epoch [45/120    avg_loss:0.153, val_acc:0.938]
Epoch [46/120    avg_loss:0.149, val_acc:0.928]
Epoch [47/120    avg_loss:0.132, val_acc:0.940]
Epoch [48/120    avg_loss:0.149, val_acc:0.940]
Epoch [49/120    avg_loss:0.110, val_acc:0.950]
Epoch [50/120    avg_loss:0.109, val_acc:0.951]
Epoch [51/120    avg_loss:0.096, val_acc:0.942]
Epoch [52/120    avg_loss:0.106, val_acc:0.951]
Epoch [53/120    avg_loss:0.132, val_acc:0.947]
Epoch [54/120    avg_loss:0.086, val_acc:0.955]
Epoch [55/120    avg_loss:0.085, val_acc:0.953]
Epoch [56/120    avg_loss:0.079, val_acc:0.928]
Epoch [57/120    avg_loss:0.135, val_acc:0.925]
Epoch [58/120    avg_loss:0.153, val_acc:0.940]
Epoch [59/120    avg_loss:0.103, val_acc:0.944]
Epoch [60/120    avg_loss:0.087, val_acc:0.951]
Epoch [61/120    avg_loss:0.075, val_acc:0.958]
Epoch [62/120    avg_loss:0.092, val_acc:0.957]
Epoch [63/120    avg_loss:0.071, val_acc:0.960]
Epoch [64/120    avg_loss:0.072, val_acc:0.934]
Epoch [65/120    avg_loss:0.072, val_acc:0.964]
Epoch [66/120    avg_loss:0.062, val_acc:0.952]
Epoch [67/120    avg_loss:0.052, val_acc:0.964]
Epoch [68/120    avg_loss:0.052, val_acc:0.966]
Epoch [69/120    avg_loss:0.047, val_acc:0.964]
Epoch [70/120    avg_loss:0.049, val_acc:0.964]
Epoch [71/120    avg_loss:0.038, val_acc:0.961]
Epoch [72/120    avg_loss:0.048, val_acc:0.973]
Epoch [73/120    avg_loss:0.036, val_acc:0.975]
Epoch [74/120    avg_loss:0.041, val_acc:0.967]
Epoch [75/120    avg_loss:0.033, val_acc:0.970]
Epoch [76/120    avg_loss:0.037, val_acc:0.970]
Epoch [77/120    avg_loss:0.030, val_acc:0.967]
Epoch [78/120    avg_loss:0.043, val_acc:0.963]
Epoch [79/120    avg_loss:0.033, val_acc:0.958]
Epoch [80/120    avg_loss:0.035, val_acc:0.967]
Epoch [81/120    avg_loss:0.050, val_acc:0.963]
Epoch [82/120    avg_loss:0.049, val_acc:0.972]
Epoch [83/120    avg_loss:0.038, val_acc:0.963]
Epoch [84/120    avg_loss:0.039, val_acc:0.982]
Epoch [85/120    avg_loss:0.027, val_acc:0.967]
Epoch [86/120    avg_loss:0.032, val_acc:0.967]
Epoch [87/120    avg_loss:0.031, val_acc:0.974]
Epoch [88/120    avg_loss:0.030, val_acc:0.972]
Epoch [89/120    avg_loss:0.030, val_acc:0.947]
Epoch [90/120    avg_loss:0.025, val_acc:0.966]
Epoch [91/120    avg_loss:0.024, val_acc:0.977]
Epoch [92/120    avg_loss:0.023, val_acc:0.976]
Epoch [93/120    avg_loss:0.019, val_acc:0.975]
Epoch [94/120    avg_loss:0.027, val_acc:0.969]
Epoch [95/120    avg_loss:0.038, val_acc:0.973]
Epoch [96/120    avg_loss:0.032, val_acc:0.965]
Epoch [97/120    avg_loss:0.041, val_acc:0.958]
Epoch [98/120    avg_loss:0.032, val_acc:0.975]
Epoch [99/120    avg_loss:0.022, val_acc:0.975]
Epoch [100/120    avg_loss:0.019, val_acc:0.977]
Epoch [101/120    avg_loss:0.018, val_acc:0.975]
Epoch [102/120    avg_loss:0.020, val_acc:0.975]
Epoch [103/120    avg_loss:0.023, val_acc:0.974]
Epoch [104/120    avg_loss:0.015, val_acc:0.974]
Epoch [105/120    avg_loss:0.016, val_acc:0.974]
Epoch [106/120    avg_loss:0.017, val_acc:0.976]
Epoch [107/120    avg_loss:0.015, val_acc:0.975]
Epoch [108/120    avg_loss:0.016, val_acc:0.975]
Epoch [109/120    avg_loss:0.016, val_acc:0.976]
Epoch [110/120    avg_loss:0.018, val_acc:0.980]
Epoch [111/120    avg_loss:0.017, val_acc:0.980]
Epoch [112/120    avg_loss:0.014, val_acc:0.980]
Epoch [113/120    avg_loss:0.015, val_acc:0.980]
Epoch [114/120    avg_loss:0.013, val_acc:0.980]
Epoch [115/120    avg_loss:0.014, val_acc:0.980]
Epoch [116/120    avg_loss:0.018, val_acc:0.980]
Epoch [117/120    avg_loss:0.021, val_acc:0.980]
Epoch [118/120    avg_loss:0.016, val_acc:0.980]
Epoch [119/120    avg_loss:0.016, val_acc:0.980]
Epoch [120/120    avg_loss:0.014, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1236    0    0    2    1    0    0    0    5   32    1    0
     0    7    1]
 [   0    0    8  721    1    6    0    0    0    2    0    0    9    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  419    0    5    0    1    0    0    0    0
    10    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    5   84    0    5    1    0    0    0  752   24    1    0
     0    3    0]
 [   0    0    6    0    0    1    4    0    0    0    3 2195    1    0
     0    0    0]
 [   0    0    0   34    7    1    0    0    0    0   17    1  470    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    1    0    0
  1136    0    0]
 [   0    0    0    0    0    0   30    0    0    0    0    0    0    0
    79  238    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
95.50135501355014

F1 scores:
[       nan 0.975      0.97322835 0.90806045 0.97921478 0.96321839
 0.97181009 0.90909091 1.         0.89473684 0.90876133 0.98364329
 0.92156863 1.         0.9602705  0.8        0.94674556]

Kappa:
0.94863717643551
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2952914eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.813, val_acc:0.166]
Epoch [2/120    avg_loss:2.750, val_acc:0.210]
Epoch [3/120    avg_loss:2.686, val_acc:0.233]
Epoch [4/120    avg_loss:2.610, val_acc:0.282]
Epoch [5/120    avg_loss:2.511, val_acc:0.411]
Epoch [6/120    avg_loss:2.437, val_acc:0.441]
Epoch [7/120    avg_loss:2.332, val_acc:0.523]
Epoch [8/120    avg_loss:2.241, val_acc:0.516]
Epoch [9/120    avg_loss:2.158, val_acc:0.526]
Epoch [10/120    avg_loss:2.068, val_acc:0.558]
Epoch [11/120    avg_loss:1.988, val_acc:0.598]
Epoch [12/120    avg_loss:1.884, val_acc:0.640]
Epoch [13/120    avg_loss:1.809, val_acc:0.641]
Epoch [14/120    avg_loss:1.691, val_acc:0.651]
Epoch [15/120    avg_loss:1.611, val_acc:0.665]
Epoch [16/120    avg_loss:1.496, val_acc:0.698]
Epoch [17/120    avg_loss:1.355, val_acc:0.697]
Epoch [18/120    avg_loss:1.364, val_acc:0.701]
Epoch [19/120    avg_loss:1.159, val_acc:0.749]
Epoch [20/120    avg_loss:1.048, val_acc:0.717]
Epoch [21/120    avg_loss:0.968, val_acc:0.781]
Epoch [22/120    avg_loss:0.842, val_acc:0.772]
Epoch [23/120    avg_loss:0.808, val_acc:0.785]
Epoch [24/120    avg_loss:0.739, val_acc:0.815]
Epoch [25/120    avg_loss:0.670, val_acc:0.825]
Epoch [26/120    avg_loss:0.650, val_acc:0.809]
Epoch [27/120    avg_loss:0.559, val_acc:0.841]
Epoch [28/120    avg_loss:0.491, val_acc:0.852]
Epoch [29/120    avg_loss:0.437, val_acc:0.868]
Epoch [30/120    avg_loss:0.439, val_acc:0.874]
Epoch [31/120    avg_loss:0.414, val_acc:0.880]
Epoch [32/120    avg_loss:0.394, val_acc:0.869]
Epoch [33/120    avg_loss:0.365, val_acc:0.850]
Epoch [34/120    avg_loss:0.336, val_acc:0.893]
Epoch [35/120    avg_loss:0.326, val_acc:0.873]
Epoch [36/120    avg_loss:0.281, val_acc:0.869]
Epoch [37/120    avg_loss:0.226, val_acc:0.907]
Epoch [38/120    avg_loss:0.204, val_acc:0.881]
Epoch [39/120    avg_loss:0.289, val_acc:0.914]
Epoch [40/120    avg_loss:0.232, val_acc:0.906]
Epoch [41/120    avg_loss:0.187, val_acc:0.926]
Epoch [42/120    avg_loss:0.211, val_acc:0.923]
Epoch [43/120    avg_loss:0.186, val_acc:0.919]
Epoch [44/120    avg_loss:0.259, val_acc:0.897]
Epoch [45/120    avg_loss:0.193, val_acc:0.909]
Epoch [46/120    avg_loss:0.130, val_acc:0.938]
Epoch [47/120    avg_loss:0.138, val_acc:0.942]
Epoch [48/120    avg_loss:0.116, val_acc:0.930]
Epoch [49/120    avg_loss:0.131, val_acc:0.940]
Epoch [50/120    avg_loss:0.143, val_acc:0.925]
Epoch [51/120    avg_loss:0.133, val_acc:0.930]
Epoch [52/120    avg_loss:0.130, val_acc:0.926]
Epoch [53/120    avg_loss:0.108, val_acc:0.935]
Epoch [54/120    avg_loss:0.093, val_acc:0.947]
Epoch [55/120    avg_loss:0.102, val_acc:0.947]
Epoch [56/120    avg_loss:0.131, val_acc:0.940]
Epoch [57/120    avg_loss:0.115, val_acc:0.930]
Epoch [58/120    avg_loss:0.092, val_acc:0.950]
Epoch [59/120    avg_loss:0.093, val_acc:0.949]
Epoch [60/120    avg_loss:0.068, val_acc:0.953]
Epoch [61/120    avg_loss:0.072, val_acc:0.950]
Epoch [62/120    avg_loss:0.072, val_acc:0.943]
Epoch [63/120    avg_loss:0.064, val_acc:0.949]
Epoch [64/120    avg_loss:0.083, val_acc:0.953]
Epoch [65/120    avg_loss:0.074, val_acc:0.956]
Epoch [66/120    avg_loss:0.051, val_acc:0.961]
Epoch [67/120    avg_loss:0.059, val_acc:0.953]
Epoch [68/120    avg_loss:0.061, val_acc:0.956]
Epoch [69/120    avg_loss:0.059, val_acc:0.955]
Epoch [70/120    avg_loss:0.074, val_acc:0.944]
Epoch [71/120    avg_loss:0.065, val_acc:0.957]
Epoch [72/120    avg_loss:0.046, val_acc:0.959]
Epoch [73/120    avg_loss:0.040, val_acc:0.964]
Epoch [74/120    avg_loss:0.051, val_acc:0.956]
Epoch [75/120    avg_loss:0.078, val_acc:0.949]
Epoch [76/120    avg_loss:0.044, val_acc:0.956]
Epoch [77/120    avg_loss:0.046, val_acc:0.949]
Epoch [78/120    avg_loss:0.043, val_acc:0.951]
Epoch [79/120    avg_loss:0.043, val_acc:0.956]
Epoch [80/120    avg_loss:0.040, val_acc:0.960]
Epoch [81/120    avg_loss:0.041, val_acc:0.964]
Epoch [82/120    avg_loss:0.042, val_acc:0.968]
Epoch [83/120    avg_loss:0.033, val_acc:0.966]
Epoch [84/120    avg_loss:0.047, val_acc:0.960]
Epoch [85/120    avg_loss:0.058, val_acc:0.963]
Epoch [86/120    avg_loss:0.040, val_acc:0.966]
Epoch [87/120    avg_loss:0.033, val_acc:0.961]
Epoch [88/120    avg_loss:0.029, val_acc:0.964]
Epoch [89/120    avg_loss:0.057, val_acc:0.951]
Epoch [90/120    avg_loss:0.054, val_acc:0.955]
Epoch [91/120    avg_loss:0.036, val_acc:0.947]
Epoch [92/120    avg_loss:0.058, val_acc:0.958]
Epoch [93/120    avg_loss:0.037, val_acc:0.964]
Epoch [94/120    avg_loss:0.033, val_acc:0.958]
Epoch [95/120    avg_loss:0.041, val_acc:0.966]
Epoch [96/120    avg_loss:0.031, val_acc:0.966]
Epoch [97/120    avg_loss:0.024, val_acc:0.967]
Epoch [98/120    avg_loss:0.026, val_acc:0.968]
Epoch [99/120    avg_loss:0.024, val_acc:0.969]
Epoch [100/120    avg_loss:0.021, val_acc:0.968]
Epoch [101/120    avg_loss:0.023, val_acc:0.969]
Epoch [102/120    avg_loss:0.025, val_acc:0.969]
Epoch [103/120    avg_loss:0.026, val_acc:0.969]
Epoch [104/120    avg_loss:0.019, val_acc:0.970]
Epoch [105/120    avg_loss:0.021, val_acc:0.972]
Epoch [106/120    avg_loss:0.019, val_acc:0.970]
Epoch [107/120    avg_loss:0.021, val_acc:0.973]
Epoch [108/120    avg_loss:0.019, val_acc:0.968]
Epoch [109/120    avg_loss:0.023, val_acc:0.974]
Epoch [110/120    avg_loss:0.018, val_acc:0.973]
Epoch [111/120    avg_loss:0.021, val_acc:0.974]
Epoch [112/120    avg_loss:0.021, val_acc:0.974]
Epoch [113/120    avg_loss:0.020, val_acc:0.973]
Epoch [114/120    avg_loss:0.018, val_acc:0.974]
Epoch [115/120    avg_loss:0.018, val_acc:0.975]
Epoch [116/120    avg_loss:0.017, val_acc:0.975]
Epoch [117/120    avg_loss:0.020, val_acc:0.975]
Epoch [118/120    avg_loss:0.018, val_acc:0.974]
Epoch [119/120    avg_loss:0.023, val_acc:0.972]
Epoch [120/120    avg_loss:0.017, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1241    0    7    0    2    0    0    0    0   34    0    0
     0    1    0]
 [   0    0    0  713    0   18    0    0    0    4    0    0   10    2
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    0    0    3    0    0    0    0
    10    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   16   89    0    1    0    0    0    0  763    5    0    0
     0    1    0]
 [   0    0   13    0    0    1    7    0    0    0   13 2174    0    2
     0    0    0]
 [   0    0    4   35    4    8    0    0    0    0   13    0  464    0
     0    0    6]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0  183
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    1    0    0
  1137    0    0]
 [   0    0    0    0    0    0   22    0    0    4    0    0    0    0
   106  215    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.11111111111111

F1 scores:
[       nan 0.98765432 0.96953125 0.89911728 0.97247706 0.95152198
 0.97619048 1.         1.         0.71111111 0.91651652 0.98259887
 0.91881188 0.98387097 0.9506689  0.76241135 0.95953757]

Kappa:
0.9442050759349945
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6182a20ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.810, val_acc:0.060]
Epoch [2/120    avg_loss:2.746, val_acc:0.275]
Epoch [3/120    avg_loss:2.679, val_acc:0.377]
Epoch [4/120    avg_loss:2.598, val_acc:0.367]
Epoch [5/120    avg_loss:2.500, val_acc:0.456]
Epoch [6/120    avg_loss:2.385, val_acc:0.491]
Epoch [7/120    avg_loss:2.289, val_acc:0.504]
Epoch [8/120    avg_loss:2.157, val_acc:0.549]
Epoch [9/120    avg_loss:2.059, val_acc:0.560]
Epoch [10/120    avg_loss:1.973, val_acc:0.588]
Epoch [11/120    avg_loss:1.877, val_acc:0.616]
Epoch [12/120    avg_loss:1.742, val_acc:0.623]
Epoch [13/120    avg_loss:1.564, val_acc:0.659]
Epoch [14/120    avg_loss:1.409, val_acc:0.670]
Epoch [15/120    avg_loss:1.334, val_acc:0.659]
Epoch [16/120    avg_loss:1.204, val_acc:0.707]
Epoch [17/120    avg_loss:1.071, val_acc:0.717]
Epoch [18/120    avg_loss:0.957, val_acc:0.752]
Epoch [19/120    avg_loss:0.857, val_acc:0.740]
Epoch [20/120    avg_loss:0.871, val_acc:0.793]
Epoch [21/120    avg_loss:0.770, val_acc:0.751]
Epoch [22/120    avg_loss:0.682, val_acc:0.802]
Epoch [23/120    avg_loss:0.646, val_acc:0.783]
Epoch [24/120    avg_loss:0.608, val_acc:0.833]
Epoch [25/120    avg_loss:0.551, val_acc:0.876]
Epoch [26/120    avg_loss:0.514, val_acc:0.861]
Epoch [27/120    avg_loss:0.516, val_acc:0.841]
Epoch [28/120    avg_loss:0.404, val_acc:0.870]
Epoch [29/120    avg_loss:0.448, val_acc:0.882]
Epoch [30/120    avg_loss:0.356, val_acc:0.900]
Epoch [31/120    avg_loss:0.306, val_acc:0.890]
Epoch [32/120    avg_loss:0.330, val_acc:0.898]
Epoch [33/120    avg_loss:0.271, val_acc:0.919]
Epoch [34/120    avg_loss:0.218, val_acc:0.880]
Epoch [35/120    avg_loss:0.317, val_acc:0.889]
Epoch [36/120    avg_loss:0.223, val_acc:0.904]
Epoch [37/120    avg_loss:0.188, val_acc:0.928]
Epoch [38/120    avg_loss:0.202, val_acc:0.911]
Epoch [39/120    avg_loss:0.170, val_acc:0.936]
Epoch [40/120    avg_loss:0.142, val_acc:0.940]
Epoch [41/120    avg_loss:0.129, val_acc:0.928]
Epoch [42/120    avg_loss:0.161, val_acc:0.929]
Epoch [43/120    avg_loss:0.163, val_acc:0.939]
Epoch [44/120    avg_loss:0.132, val_acc:0.925]
Epoch [45/120    avg_loss:0.129, val_acc:0.940]
Epoch [46/120    avg_loss:0.184, val_acc:0.950]
Epoch [47/120    avg_loss:0.111, val_acc:0.942]
Epoch [48/120    avg_loss:0.103, val_acc:0.956]
Epoch [49/120    avg_loss:0.099, val_acc:0.945]
Epoch [50/120    avg_loss:0.123, val_acc:0.951]
Epoch [51/120    avg_loss:0.085, val_acc:0.958]
Epoch [52/120    avg_loss:0.072, val_acc:0.956]
Epoch [53/120    avg_loss:0.093, val_acc:0.950]
Epoch [54/120    avg_loss:0.090, val_acc:0.956]
Epoch [55/120    avg_loss:0.072, val_acc:0.967]
Epoch [56/120    avg_loss:0.071, val_acc:0.948]
Epoch [57/120    avg_loss:0.052, val_acc:0.966]
Epoch [58/120    avg_loss:0.047, val_acc:0.955]
Epoch [59/120    avg_loss:0.050, val_acc:0.964]
Epoch [60/120    avg_loss:0.053, val_acc:0.964]
Epoch [61/120    avg_loss:0.068, val_acc:0.961]
Epoch [62/120    avg_loss:0.047, val_acc:0.959]
Epoch [63/120    avg_loss:0.056, val_acc:0.963]
Epoch [64/120    avg_loss:0.196, val_acc:0.943]
Epoch [65/120    avg_loss:0.155, val_acc:0.943]
Epoch [66/120    avg_loss:0.092, val_acc:0.950]
Epoch [67/120    avg_loss:0.076, val_acc:0.961]
Epoch [68/120    avg_loss:0.065, val_acc:0.969]
Epoch [69/120    avg_loss:0.046, val_acc:0.962]
Epoch [70/120    avg_loss:0.061, val_acc:0.957]
Epoch [71/120    avg_loss:0.065, val_acc:0.966]
Epoch [72/120    avg_loss:0.045, val_acc:0.961]
Epoch [73/120    avg_loss:0.033, val_acc:0.961]
Epoch [74/120    avg_loss:0.040, val_acc:0.974]
Epoch [75/120    avg_loss:0.040, val_acc:0.962]
Epoch [76/120    avg_loss:0.046, val_acc:0.970]
Epoch [77/120    avg_loss:0.034, val_acc:0.976]
Epoch [78/120    avg_loss:0.067, val_acc:0.912]
Epoch [79/120    avg_loss:0.149, val_acc:0.962]
Epoch [80/120    avg_loss:0.063, val_acc:0.966]
Epoch [81/120    avg_loss:0.043, val_acc:0.963]
Epoch [82/120    avg_loss:0.041, val_acc:0.966]
Epoch [83/120    avg_loss:0.035, val_acc:0.978]
Epoch [84/120    avg_loss:0.030, val_acc:0.969]
Epoch [85/120    avg_loss:0.022, val_acc:0.978]
Epoch [86/120    avg_loss:0.026, val_acc:0.981]
Epoch [87/120    avg_loss:0.024, val_acc:0.984]
Epoch [88/120    avg_loss:0.018, val_acc:0.978]
Epoch [89/120    avg_loss:0.023, val_acc:0.986]
Epoch [90/120    avg_loss:0.032, val_acc:0.980]
Epoch [91/120    avg_loss:0.029, val_acc:0.984]
Epoch [92/120    avg_loss:0.024, val_acc:0.982]
Epoch [93/120    avg_loss:0.026, val_acc:0.978]
Epoch [94/120    avg_loss:0.019, val_acc:0.984]
Epoch [95/120    avg_loss:0.025, val_acc:0.988]
Epoch [96/120    avg_loss:0.027, val_acc:0.980]
Epoch [97/120    avg_loss:0.027, val_acc:0.984]
Epoch [98/120    avg_loss:0.020, val_acc:0.987]
Epoch [99/120    avg_loss:0.018, val_acc:0.982]
Epoch [100/120    avg_loss:0.016, val_acc:0.981]
Epoch [101/120    avg_loss:0.012, val_acc:0.982]
Epoch [102/120    avg_loss:0.015, val_acc:0.985]
Epoch [103/120    avg_loss:0.017, val_acc:0.977]
Epoch [104/120    avg_loss:0.015, val_acc:0.977]
Epoch [105/120    avg_loss:0.014, val_acc:0.987]
Epoch [106/120    avg_loss:0.018, val_acc:0.974]
Epoch [107/120    avg_loss:0.016, val_acc:0.987]
Epoch [108/120    avg_loss:0.014, val_acc:0.989]
Epoch [109/120    avg_loss:0.012, val_acc:0.985]
Epoch [110/120    avg_loss:0.015, val_acc:0.987]
Epoch [111/120    avg_loss:0.017, val_acc:0.985]
Epoch [112/120    avg_loss:0.014, val_acc:0.985]
Epoch [113/120    avg_loss:0.013, val_acc:0.986]
Epoch [114/120    avg_loss:0.013, val_acc:0.981]
Epoch [115/120    avg_loss:0.015, val_acc:0.973]
Epoch [116/120    avg_loss:0.016, val_acc:0.985]
Epoch [117/120    avg_loss:0.012, val_acc:0.990]
Epoch [118/120    avg_loss:0.009, val_acc:0.981]
Epoch [119/120    avg_loss:0.009, val_acc:0.985]
Epoch [120/120    avg_loss:0.011, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1255    2    2    0    0    0    0    0    9   16    1    0
     0    0    0]
 [   0    0    2  723    4    0    0    0    0    7    0    0   10    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    3    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    1    0    0   14    0    0    1    0
     0    0    0]
 [   0    0    0   63    0    4    0    0    0    0  775   30    0    0
     0    3    0]
 [   0    0    2    0    0    0    2    0    0    0    5 2199    2    0
     0    0    0]
 [   0    0    2   21    0    4    0    0    0    0    6    3  495    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    2   56    0    0    0    0    0    0    0
    73  216    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.17344173441734

F1 scores:
[       nan 0.98765432 0.98586017 0.92811297 0.98611111 0.97818599
 0.95626822 1.         0.99883856 0.66666667 0.92703349 0.9863198
 0.94827586 0.99730458 0.9656051  0.76190476 0.98224852]

Kappa:
0.9563115383059635
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:08:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2a14b77f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.782, val_acc:0.184]
Epoch [2/120    avg_loss:2.687, val_acc:0.260]
Epoch [3/120    avg_loss:2.597, val_acc:0.299]
Epoch [4/120    avg_loss:2.501, val_acc:0.343]
Epoch [5/120    avg_loss:2.406, val_acc:0.387]
Epoch [6/120    avg_loss:2.290, val_acc:0.436]
Epoch [7/120    avg_loss:2.252, val_acc:0.456]
Epoch [8/120    avg_loss:2.203, val_acc:0.484]
Epoch [9/120    avg_loss:2.097, val_acc:0.499]
Epoch [10/120    avg_loss:2.011, val_acc:0.574]
Epoch [11/120    avg_loss:1.944, val_acc:0.588]
Epoch [12/120    avg_loss:1.859, val_acc:0.604]
Epoch [13/120    avg_loss:1.747, val_acc:0.656]
Epoch [14/120    avg_loss:1.609, val_acc:0.682]
Epoch [15/120    avg_loss:1.491, val_acc:0.713]
Epoch [16/120    avg_loss:1.351, val_acc:0.729]
Epoch [17/120    avg_loss:1.211, val_acc:0.771]
Epoch [18/120    avg_loss:1.196, val_acc:0.733]
Epoch [19/120    avg_loss:1.000, val_acc:0.776]
Epoch [20/120    avg_loss:0.930, val_acc:0.801]
Epoch [21/120    avg_loss:0.845, val_acc:0.799]
Epoch [22/120    avg_loss:0.782, val_acc:0.808]
Epoch [23/120    avg_loss:0.695, val_acc:0.840]
Epoch [24/120    avg_loss:0.582, val_acc:0.847]
Epoch [25/120    avg_loss:0.538, val_acc:0.852]
Epoch [26/120    avg_loss:0.461, val_acc:0.873]
Epoch [27/120    avg_loss:0.478, val_acc:0.874]
Epoch [28/120    avg_loss:0.522, val_acc:0.827]
Epoch [29/120    avg_loss:0.466, val_acc:0.853]
Epoch [30/120    avg_loss:0.362, val_acc:0.876]
Epoch [31/120    avg_loss:0.331, val_acc:0.882]
Epoch [32/120    avg_loss:0.326, val_acc:0.914]
Epoch [33/120    avg_loss:0.272, val_acc:0.922]
Epoch [34/120    avg_loss:0.286, val_acc:0.883]
Epoch [35/120    avg_loss:0.313, val_acc:0.905]
Epoch [36/120    avg_loss:0.223, val_acc:0.885]
Epoch [37/120    avg_loss:0.265, val_acc:0.882]
Epoch [38/120    avg_loss:0.232, val_acc:0.917]
Epoch [39/120    avg_loss:0.203, val_acc:0.922]
Epoch [40/120    avg_loss:0.185, val_acc:0.914]
Epoch [41/120    avg_loss:0.179, val_acc:0.922]
Epoch [42/120    avg_loss:0.178, val_acc:0.908]
Epoch [43/120    avg_loss:0.155, val_acc:0.941]
Epoch [44/120    avg_loss:0.143, val_acc:0.914]
Epoch [45/120    avg_loss:0.163, val_acc:0.933]
Epoch [46/120    avg_loss:0.128, val_acc:0.941]
Epoch [47/120    avg_loss:0.104, val_acc:0.939]
Epoch [48/120    avg_loss:0.095, val_acc:0.950]
Epoch [49/120    avg_loss:0.087, val_acc:0.956]
Epoch [50/120    avg_loss:0.084, val_acc:0.948]
Epoch [51/120    avg_loss:0.084, val_acc:0.948]
Epoch [52/120    avg_loss:0.102, val_acc:0.948]
Epoch [53/120    avg_loss:0.125, val_acc:0.946]
Epoch [54/120    avg_loss:0.076, val_acc:0.955]
Epoch [55/120    avg_loss:0.077, val_acc:0.954]
Epoch [56/120    avg_loss:0.067, val_acc:0.954]
Epoch [57/120    avg_loss:0.075, val_acc:0.953]
Epoch [58/120    avg_loss:0.055, val_acc:0.968]
Epoch [59/120    avg_loss:0.056, val_acc:0.948]
Epoch [60/120    avg_loss:0.071, val_acc:0.949]
Epoch [61/120    avg_loss:0.055, val_acc:0.965]
Epoch [62/120    avg_loss:0.061, val_acc:0.951]
Epoch [63/120    avg_loss:0.060, val_acc:0.967]
Epoch [64/120    avg_loss:0.040, val_acc:0.955]
Epoch [65/120    avg_loss:0.044, val_acc:0.962]
Epoch [66/120    avg_loss:0.066, val_acc:0.953]
Epoch [67/120    avg_loss:0.047, val_acc:0.965]
Epoch [68/120    avg_loss:0.034, val_acc:0.971]
Epoch [69/120    avg_loss:0.027, val_acc:0.969]
Epoch [70/120    avg_loss:0.028, val_acc:0.958]
Epoch [71/120    avg_loss:0.034, val_acc:0.964]
Epoch [72/120    avg_loss:0.034, val_acc:0.962]
Epoch [73/120    avg_loss:0.035, val_acc:0.969]
Epoch [74/120    avg_loss:0.073, val_acc:0.919]
Epoch [75/120    avg_loss:0.062, val_acc:0.967]
Epoch [76/120    avg_loss:0.043, val_acc:0.960]
Epoch [77/120    avg_loss:0.047, val_acc:0.965]
Epoch [78/120    avg_loss:0.028, val_acc:0.969]
Epoch [79/120    avg_loss:0.040, val_acc:0.964]
Epoch [80/120    avg_loss:0.061, val_acc:0.956]
Epoch [81/120    avg_loss:0.041, val_acc:0.975]
Epoch [82/120    avg_loss:0.023, val_acc:0.973]
Epoch [83/120    avg_loss:0.022, val_acc:0.967]
Epoch [84/120    avg_loss:0.023, val_acc:0.975]
Epoch [85/120    avg_loss:0.026, val_acc:0.973]
Epoch [86/120    avg_loss:0.035, val_acc:0.973]
Epoch [87/120    avg_loss:0.027, val_acc:0.963]
Epoch [88/120    avg_loss:0.025, val_acc:0.972]
Epoch [89/120    avg_loss:0.023, val_acc:0.971]
Epoch [90/120    avg_loss:0.023, val_acc:0.971]
Epoch [91/120    avg_loss:0.022, val_acc:0.968]
Epoch [92/120    avg_loss:0.021, val_acc:0.967]
Epoch [93/120    avg_loss:0.022, val_acc:0.973]
Epoch [94/120    avg_loss:0.019, val_acc:0.975]
Epoch [95/120    avg_loss:0.022, val_acc:0.968]
Epoch [96/120    avg_loss:0.014, val_acc:0.970]
Epoch [97/120    avg_loss:0.020, val_acc:0.974]
Epoch [98/120    avg_loss:0.020, val_acc:0.973]
Epoch [99/120    avg_loss:0.025, val_acc:0.969]
Epoch [100/120    avg_loss:0.029, val_acc:0.977]
Epoch [101/120    avg_loss:0.026, val_acc:0.967]
Epoch [102/120    avg_loss:0.016, val_acc:0.979]
Epoch [103/120    avg_loss:0.011, val_acc:0.980]
Epoch [104/120    avg_loss:0.019, val_acc:0.973]
Epoch [105/120    avg_loss:0.017, val_acc:0.979]
Epoch [106/120    avg_loss:0.017, val_acc:0.974]
Epoch [107/120    avg_loss:0.013, val_acc:0.975]
Epoch [108/120    avg_loss:0.015, val_acc:0.981]
Epoch [109/120    avg_loss:0.011, val_acc:0.982]
Epoch [110/120    avg_loss:0.008, val_acc:0.975]
Epoch [111/120    avg_loss:0.011, val_acc:0.975]
Epoch [112/120    avg_loss:0.019, val_acc:0.970]
Epoch [113/120    avg_loss:0.018, val_acc:0.975]
Epoch [114/120    avg_loss:0.011, val_acc:0.975]
Epoch [115/120    avg_loss:0.011, val_acc:0.975]
Epoch [116/120    avg_loss:0.014, val_acc:0.978]
Epoch [117/120    avg_loss:0.012, val_acc:0.973]
Epoch [118/120    avg_loss:0.012, val_acc:0.975]
Epoch [119/120    avg_loss:0.011, val_acc:0.977]
Epoch [120/120    avg_loss:0.013, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1259    5    0    3    0    0    0    0    3    6    0    0
     0    9    0]
 [   0    0    2  719    1   10    0    0    0    8    0    0    6    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5   90    0    6    0    0    0    0  756   12    3    0
     0    3    0]
 [   0    0   24    0    0    1    7    0    0    0   24 2150    1    1
     0    2    0]
 [   0    0    0   21    9    8    0    0    0    1    7    0  486    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1138    1    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    2    0
    66  274    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.05420054200542

F1 scores:
[       nan 0.94871795 0.97786408 0.90897598 0.97706422 0.96536313
 0.99018868 1.         0.99883586 0.8        0.9059317  0.98218365
 0.94094869 0.99730458 0.9693356  0.86163522 0.98823529]

Kappa:
0.9550326576679576
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f77c4522ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.810, val_acc:0.179]
Epoch [2/120    avg_loss:2.734, val_acc:0.360]
Epoch [3/120    avg_loss:2.647, val_acc:0.493]
Epoch [4/120    avg_loss:2.533, val_acc:0.504]
Epoch [5/120    avg_loss:2.385, val_acc:0.527]
Epoch [6/120    avg_loss:2.272, val_acc:0.546]
Epoch [7/120    avg_loss:2.144, val_acc:0.536]
Epoch [8/120    avg_loss:2.063, val_acc:0.592]
Epoch [9/120    avg_loss:1.910, val_acc:0.606]
Epoch [10/120    avg_loss:1.777, val_acc:0.606]
Epoch [11/120    avg_loss:1.628, val_acc:0.641]
Epoch [12/120    avg_loss:1.496, val_acc:0.629]
Epoch [13/120    avg_loss:1.405, val_acc:0.705]
Epoch [14/120    avg_loss:1.194, val_acc:0.695]
Epoch [15/120    avg_loss:1.211, val_acc:0.694]
Epoch [16/120    avg_loss:1.087, val_acc:0.756]
Epoch [17/120    avg_loss:0.960, val_acc:0.737]
Epoch [18/120    avg_loss:0.832, val_acc:0.792]
Epoch [19/120    avg_loss:0.749, val_acc:0.799]
Epoch [20/120    avg_loss:0.688, val_acc:0.819]
Epoch [21/120    avg_loss:0.686, val_acc:0.810]
Epoch [22/120    avg_loss:0.636, val_acc:0.843]
Epoch [23/120    avg_loss:0.563, val_acc:0.836]
Epoch [24/120    avg_loss:0.475, val_acc:0.820]
Epoch [25/120    avg_loss:0.482, val_acc:0.892]
Epoch [26/120    avg_loss:0.436, val_acc:0.867]
Epoch [27/120    avg_loss:0.388, val_acc:0.871]
Epoch [28/120    avg_loss:0.384, val_acc:0.888]
Epoch [29/120    avg_loss:0.332, val_acc:0.891]
Epoch [30/120    avg_loss:0.312, val_acc:0.900]
Epoch [31/120    avg_loss:0.259, val_acc:0.911]
Epoch [32/120    avg_loss:0.237, val_acc:0.934]
Epoch [33/120    avg_loss:0.223, val_acc:0.939]
Epoch [34/120    avg_loss:0.207, val_acc:0.922]
Epoch [35/120    avg_loss:0.216, val_acc:0.912]
Epoch [36/120    avg_loss:0.185, val_acc:0.954]
Epoch [37/120    avg_loss:0.167, val_acc:0.932]
Epoch [38/120    avg_loss:0.130, val_acc:0.951]
Epoch [39/120    avg_loss:0.122, val_acc:0.954]
Epoch [40/120    avg_loss:0.154, val_acc:0.934]
Epoch [41/120    avg_loss:0.164, val_acc:0.950]
Epoch [42/120    avg_loss:0.122, val_acc:0.936]
Epoch [43/120    avg_loss:0.116, val_acc:0.934]
Epoch [44/120    avg_loss:0.107, val_acc:0.942]
Epoch [45/120    avg_loss:0.114, val_acc:0.961]
Epoch [46/120    avg_loss:0.092, val_acc:0.934]
Epoch [47/120    avg_loss:0.118, val_acc:0.964]
Epoch [48/120    avg_loss:0.086, val_acc:0.941]
Epoch [49/120    avg_loss:0.079, val_acc:0.959]
Epoch [50/120    avg_loss:0.069, val_acc:0.953]
Epoch [51/120    avg_loss:0.080, val_acc:0.927]
Epoch [52/120    avg_loss:0.085, val_acc:0.960]
Epoch [53/120    avg_loss:0.103, val_acc:0.951]
Epoch [54/120    avg_loss:0.114, val_acc:0.960]
Epoch [55/120    avg_loss:0.091, val_acc:0.969]
Epoch [56/120    avg_loss:0.090, val_acc:0.972]
Epoch [57/120    avg_loss:0.124, val_acc:0.958]
Epoch [58/120    avg_loss:0.068, val_acc:0.934]
Epoch [59/120    avg_loss:0.070, val_acc:0.970]
Epoch [60/120    avg_loss:0.049, val_acc:0.972]
Epoch [61/120    avg_loss:0.044, val_acc:0.982]
Epoch [62/120    avg_loss:0.042, val_acc:0.980]
Epoch [63/120    avg_loss:0.035, val_acc:0.978]
Epoch [64/120    avg_loss:0.036, val_acc:0.975]
Epoch [65/120    avg_loss:0.037, val_acc:0.975]
Epoch [66/120    avg_loss:0.040, val_acc:0.971]
Epoch [67/120    avg_loss:0.031, val_acc:0.971]
Epoch [68/120    avg_loss:0.020, val_acc:0.975]
Epoch [69/120    avg_loss:0.030, val_acc:0.971]
Epoch [70/120    avg_loss:0.035, val_acc:0.982]
Epoch [71/120    avg_loss:0.022, val_acc:0.983]
Epoch [72/120    avg_loss:0.024, val_acc:0.981]
Epoch [73/120    avg_loss:0.020, val_acc:0.980]
Epoch [74/120    avg_loss:0.026, val_acc:0.981]
Epoch [75/120    avg_loss:0.031, val_acc:0.983]
Epoch [76/120    avg_loss:0.027, val_acc:0.981]
Epoch [77/120    avg_loss:0.024, val_acc:0.979]
Epoch [78/120    avg_loss:0.022, val_acc:0.983]
Epoch [79/120    avg_loss:0.021, val_acc:0.968]
Epoch [80/120    avg_loss:0.018, val_acc:0.985]
Epoch [81/120    avg_loss:0.017, val_acc:0.946]
Epoch [82/120    avg_loss:0.016, val_acc:0.983]
Epoch [83/120    avg_loss:0.019, val_acc:0.980]
Epoch [84/120    avg_loss:0.042, val_acc:0.969]
Epoch [85/120    avg_loss:0.029, val_acc:0.987]
Epoch [86/120    avg_loss:0.021, val_acc:0.978]
Epoch [87/120    avg_loss:0.021, val_acc:0.982]
Epoch [88/120    avg_loss:0.019, val_acc:0.988]
Epoch [89/120    avg_loss:0.019, val_acc:0.975]
Epoch [90/120    avg_loss:0.030, val_acc:0.978]
Epoch [91/120    avg_loss:0.058, val_acc:0.953]
Epoch [92/120    avg_loss:0.035, val_acc:0.982]
Epoch [93/120    avg_loss:0.023, val_acc:0.985]
Epoch [94/120    avg_loss:0.017, val_acc:0.985]
Epoch [95/120    avg_loss:0.014, val_acc:0.988]
Epoch [96/120    avg_loss:0.018, val_acc:0.984]
Epoch [97/120    avg_loss:0.017, val_acc:0.988]
Epoch [98/120    avg_loss:0.016, val_acc:0.980]
Epoch [99/120    avg_loss:0.010, val_acc:0.980]
Epoch [100/120    avg_loss:0.010, val_acc:0.987]
Epoch [101/120    avg_loss:0.014, val_acc:0.969]
Epoch [102/120    avg_loss:0.012, val_acc:0.985]
Epoch [103/120    avg_loss:0.012, val_acc:0.982]
Epoch [104/120    avg_loss:0.012, val_acc:0.977]
Epoch [105/120    avg_loss:0.011, val_acc:0.984]
Epoch [106/120    avg_loss:0.014, val_acc:0.988]
Epoch [107/120    avg_loss:0.021, val_acc:0.982]
Epoch [108/120    avg_loss:0.016, val_acc:0.990]
Epoch [109/120    avg_loss:0.014, val_acc:0.988]
Epoch [110/120    avg_loss:0.011, val_acc:0.985]
Epoch [111/120    avg_loss:0.008, val_acc:0.984]
Epoch [112/120    avg_loss:0.013, val_acc:0.985]
Epoch [113/120    avg_loss:0.031, val_acc:0.973]
Epoch [114/120    avg_loss:0.023, val_acc:0.980]
Epoch [115/120    avg_loss:0.014, val_acc:0.984]
Epoch [116/120    avg_loss:0.014, val_acc:0.989]
Epoch [117/120    avg_loss:0.009, val_acc:0.987]
Epoch [118/120    avg_loss:0.009, val_acc:0.985]
Epoch [119/120    avg_loss:0.013, val_acc:0.984]
Epoch [120/120    avg_loss:0.008, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1217    2    0    0    2    0    0    2   28   26    7    0
     0    1    0]
 [   0    0    0  729    1    8    0    0    0    4    0    3    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    1    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    6    0    0    8    0    0    0    0
     0    0    0]
 [   0    0   24   90    0    6    0    0    0    0  749    3    1    0
     2    0    0]
 [   0    0   12    0    0    0    7    0    0    0    9 2182    0    0
     0    0    0]
 [   0    0    0   22    6   12    0    0    0    0    0   10  482    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    1    0    0
  1137    0    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    98  233    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.3821138211382

F1 scores:
[       nan 0.98765432 0.95902285 0.91468005 0.98383372 0.96404494
 0.97695167 1.         0.99883856 0.48484848 0.90132371 0.98376916
 0.93865628 1.         0.9550609  0.8020654  0.98809524]

Kappa:
0.9472960931731205
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efea58b3ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.779, val_acc:0.239]
Epoch [2/120    avg_loss:2.655, val_acc:0.394]
Epoch [3/120    avg_loss:2.532, val_acc:0.404]
Epoch [4/120    avg_loss:2.408, val_acc:0.448]
Epoch [5/120    avg_loss:2.326, val_acc:0.491]
Epoch [6/120    avg_loss:2.212, val_acc:0.504]
Epoch [7/120    avg_loss:2.113, val_acc:0.504]
Epoch [8/120    avg_loss:2.073, val_acc:0.566]
Epoch [9/120    avg_loss:1.946, val_acc:0.576]
Epoch [10/120    avg_loss:1.870, val_acc:0.566]
Epoch [11/120    avg_loss:1.782, val_acc:0.628]
Epoch [12/120    avg_loss:1.700, val_acc:0.661]
Epoch [13/120    avg_loss:1.516, val_acc:0.675]
Epoch [14/120    avg_loss:1.364, val_acc:0.706]
Epoch [15/120    avg_loss:1.245, val_acc:0.735]
Epoch [16/120    avg_loss:1.129, val_acc:0.757]
Epoch [17/120    avg_loss:1.018, val_acc:0.763]
Epoch [18/120    avg_loss:0.974, val_acc:0.815]
Epoch [19/120    avg_loss:0.842, val_acc:0.791]
Epoch [20/120    avg_loss:0.810, val_acc:0.779]
Epoch [21/120    avg_loss:0.710, val_acc:0.817]
Epoch [22/120    avg_loss:0.726, val_acc:0.748]
Epoch [23/120    avg_loss:0.618, val_acc:0.829]
Epoch [24/120    avg_loss:0.575, val_acc:0.818]
Epoch [25/120    avg_loss:0.569, val_acc:0.862]
Epoch [26/120    avg_loss:0.536, val_acc:0.850]
Epoch [27/120    avg_loss:0.465, val_acc:0.868]
Epoch [28/120    avg_loss:0.419, val_acc:0.874]
Epoch [29/120    avg_loss:0.426, val_acc:0.869]
Epoch [30/120    avg_loss:0.392, val_acc:0.890]
Epoch [31/120    avg_loss:0.354, val_acc:0.882]
Epoch [32/120    avg_loss:0.329, val_acc:0.879]
Epoch [33/120    avg_loss:0.313, val_acc:0.897]
Epoch [34/120    avg_loss:0.297, val_acc:0.903]
Epoch [35/120    avg_loss:0.244, val_acc:0.882]
Epoch [36/120    avg_loss:0.215, val_acc:0.920]
Epoch [37/120    avg_loss:0.179, val_acc:0.919]
Epoch [38/120    avg_loss:0.182, val_acc:0.898]
Epoch [39/120    avg_loss:0.174, val_acc:0.873]
Epoch [40/120    avg_loss:0.264, val_acc:0.916]
Epoch [41/120    avg_loss:0.189, val_acc:0.924]
Epoch [42/120    avg_loss:0.138, val_acc:0.943]
Epoch [43/120    avg_loss:0.154, val_acc:0.933]
Epoch [44/120    avg_loss:0.161, val_acc:0.922]
Epoch [45/120    avg_loss:0.155, val_acc:0.879]
Epoch [46/120    avg_loss:0.157, val_acc:0.874]
Epoch [47/120    avg_loss:0.149, val_acc:0.934]
Epoch [48/120    avg_loss:0.110, val_acc:0.924]
Epoch [49/120    avg_loss:0.127, val_acc:0.938]
Epoch [50/120    avg_loss:0.095, val_acc:0.939]
Epoch [51/120    avg_loss:0.099, val_acc:0.952]
Epoch [52/120    avg_loss:0.085, val_acc:0.952]
Epoch [53/120    avg_loss:0.078, val_acc:0.941]
Epoch [54/120    avg_loss:0.079, val_acc:0.939]
Epoch [55/120    avg_loss:0.063, val_acc:0.950]
Epoch [56/120    avg_loss:0.063, val_acc:0.954]
Epoch [57/120    avg_loss:0.053, val_acc:0.952]
Epoch [58/120    avg_loss:0.066, val_acc:0.958]
Epoch [59/120    avg_loss:0.062, val_acc:0.962]
Epoch [60/120    avg_loss:0.072, val_acc:0.960]
Epoch [61/120    avg_loss:0.064, val_acc:0.952]
Epoch [62/120    avg_loss:0.065, val_acc:0.958]
Epoch [63/120    avg_loss:0.062, val_acc:0.965]
Epoch [64/120    avg_loss:0.049, val_acc:0.967]
Epoch [65/120    avg_loss:0.058, val_acc:0.939]
Epoch [66/120    avg_loss:0.064, val_acc:0.960]
Epoch [67/120    avg_loss:0.062, val_acc:0.952]
Epoch [68/120    avg_loss:0.043, val_acc:0.967]
Epoch [69/120    avg_loss:0.038, val_acc:0.969]
Epoch [70/120    avg_loss:0.033, val_acc:0.963]
Epoch [71/120    avg_loss:0.025, val_acc:0.972]
Epoch [72/120    avg_loss:0.030, val_acc:0.971]
Epoch [73/120    avg_loss:0.031, val_acc:0.958]
Epoch [74/120    avg_loss:0.023, val_acc:0.970]
Epoch [75/120    avg_loss:0.019, val_acc:0.977]
Epoch [76/120    avg_loss:0.029, val_acc:0.963]
Epoch [77/120    avg_loss:0.039, val_acc:0.975]
Epoch [78/120    avg_loss:0.028, val_acc:0.975]
Epoch [79/120    avg_loss:0.025, val_acc:0.978]
Epoch [80/120    avg_loss:0.019, val_acc:0.968]
Epoch [81/120    avg_loss:0.038, val_acc:0.969]
Epoch [82/120    avg_loss:0.031, val_acc:0.971]
Epoch [83/120    avg_loss:0.028, val_acc:0.979]
Epoch [84/120    avg_loss:0.024, val_acc:0.971]
Epoch [85/120    avg_loss:0.035, val_acc:0.961]
Epoch [86/120    avg_loss:0.022, val_acc:0.972]
Epoch [87/120    avg_loss:0.017, val_acc:0.971]
Epoch [88/120    avg_loss:0.018, val_acc:0.973]
Epoch [89/120    avg_loss:0.015, val_acc:0.961]
Epoch [90/120    avg_loss:0.018, val_acc:0.963]
Epoch [91/120    avg_loss:0.012, val_acc:0.975]
Epoch [92/120    avg_loss:0.014, val_acc:0.978]
Epoch [93/120    avg_loss:0.013, val_acc:0.967]
Epoch [94/120    avg_loss:0.019, val_acc:0.967]
Epoch [95/120    avg_loss:0.020, val_acc:0.969]
Epoch [96/120    avg_loss:0.016, val_acc:0.975]
Epoch [97/120    avg_loss:0.015, val_acc:0.977]
Epoch [98/120    avg_loss:0.013, val_acc:0.978]
Epoch [99/120    avg_loss:0.012, val_acc:0.978]
Epoch [100/120    avg_loss:0.012, val_acc:0.977]
Epoch [101/120    avg_loss:0.015, val_acc:0.979]
Epoch [102/120    avg_loss:0.010, val_acc:0.979]
Epoch [103/120    avg_loss:0.012, val_acc:0.981]
Epoch [104/120    avg_loss:0.014, val_acc:0.979]
Epoch [105/120    avg_loss:0.010, val_acc:0.980]
Epoch [106/120    avg_loss:0.010, val_acc:0.981]
Epoch [107/120    avg_loss:0.012, val_acc:0.980]
Epoch [108/120    avg_loss:0.010, val_acc:0.980]
Epoch [109/120    avg_loss:0.009, val_acc:0.981]
Epoch [110/120    avg_loss:0.013, val_acc:0.980]
Epoch [111/120    avg_loss:0.010, val_acc:0.978]
Epoch [112/120    avg_loss:0.015, val_acc:0.978]
Epoch [113/120    avg_loss:0.014, val_acc:0.980]
Epoch [114/120    avg_loss:0.011, val_acc:0.979]
Epoch [115/120    avg_loss:0.009, val_acc:0.980]
Epoch [116/120    avg_loss:0.012, val_acc:0.980]
Epoch [117/120    avg_loss:0.009, val_acc:0.981]
Epoch [118/120    avg_loss:0.010, val_acc:0.980]
Epoch [119/120    avg_loss:0.010, val_acc:0.980]
Epoch [120/120    avg_loss:0.009, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1264    0    0    0    0    0    0    0    0   11    1    0
     0    9    0]
 [   0    0   10  719    3    0    0    0    0    7    0    0    5    0
     0    3    0]
 [   0    0    0    6  207    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    1    0    4    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    9   63    0    2    4    0    0    0  788    8    0    1
     0    0    0]
 [   0    0   13    0    0    0    1    0    2    0    9 2181    2    0
     2    0    0]
 [   0    0    0    8    0   11    0    0    0    0    1    5  507    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    2    2    0    0    5    0    0    0    0
   132  206    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.16260162601625

F1 scores:
[       nan 0.98765432 0.97908598 0.93195075 0.9787234  0.97149373
 0.99318698 0.98039216 0.99767981 0.66666667 0.94202032 0.98777174
 0.96479543 0.99730458 0.94244306 0.72920354 0.97619048]

Kappa:
0.956199525809721
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4c7acbbeb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.802, val_acc:0.173]
Epoch [2/120    avg_loss:2.719, val_acc:0.260]
Epoch [3/120    avg_loss:2.667, val_acc:0.303]
Epoch [4/120    avg_loss:2.580, val_acc:0.365]
Epoch [5/120    avg_loss:2.517, val_acc:0.427]
Epoch [6/120    avg_loss:2.453, val_acc:0.424]
Epoch [7/120    avg_loss:2.337, val_acc:0.459]
Epoch [8/120    avg_loss:2.268, val_acc:0.497]
Epoch [9/120    avg_loss:2.214, val_acc:0.493]
Epoch [10/120    avg_loss:2.154, val_acc:0.511]
Epoch [11/120    avg_loss:2.086, val_acc:0.544]
Epoch [12/120    avg_loss:2.007, val_acc:0.593]
Epoch [13/120    avg_loss:1.910, val_acc:0.618]
Epoch [14/120    avg_loss:1.863, val_acc:0.654]
Epoch [15/120    avg_loss:1.798, val_acc:0.648]
Epoch [16/120    avg_loss:1.660, val_acc:0.655]
Epoch [17/120    avg_loss:1.610, val_acc:0.691]
Epoch [18/120    avg_loss:1.443, val_acc:0.765]
Epoch [19/120    avg_loss:1.293, val_acc:0.786]
Epoch [20/120    avg_loss:1.116, val_acc:0.827]
Epoch [21/120    avg_loss:1.137, val_acc:0.800]
Epoch [22/120    avg_loss:1.042, val_acc:0.819]
Epoch [23/120    avg_loss:0.825, val_acc:0.810]
Epoch [24/120    avg_loss:0.706, val_acc:0.806]
Epoch [25/120    avg_loss:0.671, val_acc:0.843]
Epoch [26/120    avg_loss:0.537, val_acc:0.869]
Epoch [27/120    avg_loss:0.439, val_acc:0.895]
Epoch [28/120    avg_loss:0.408, val_acc:0.877]
Epoch [29/120    avg_loss:0.362, val_acc:0.883]
Epoch [30/120    avg_loss:0.340, val_acc:0.876]
Epoch [31/120    avg_loss:0.312, val_acc:0.901]
Epoch [32/120    avg_loss:0.239, val_acc:0.904]
Epoch [33/120    avg_loss:0.227, val_acc:0.927]
Epoch [34/120    avg_loss:0.188, val_acc:0.916]
Epoch [35/120    avg_loss:0.175, val_acc:0.946]
Epoch [36/120    avg_loss:0.154, val_acc:0.939]
Epoch [37/120    avg_loss:0.168, val_acc:0.897]
Epoch [38/120    avg_loss:0.156, val_acc:0.926]
Epoch [39/120    avg_loss:0.138, val_acc:0.926]
Epoch [40/120    avg_loss:0.130, val_acc:0.929]
Epoch [41/120    avg_loss:0.142, val_acc:0.932]
Epoch [42/120    avg_loss:0.112, val_acc:0.955]
Epoch [43/120    avg_loss:0.117, val_acc:0.947]
Epoch [44/120    avg_loss:0.099, val_acc:0.953]
Epoch [45/120    avg_loss:0.074, val_acc:0.956]
Epoch [46/120    avg_loss:0.099, val_acc:0.953]
Epoch [47/120    avg_loss:0.217, val_acc:0.908]
Epoch [48/120    avg_loss:0.330, val_acc:0.912]
Epoch [49/120    avg_loss:0.560, val_acc:0.797]
Epoch [50/120    avg_loss:0.698, val_acc:0.366]
Epoch [51/120    avg_loss:0.658, val_acc:0.921]
Epoch [52/120    avg_loss:0.182, val_acc:0.914]
Epoch [53/120    avg_loss:0.139, val_acc:0.905]
Epoch [54/120    avg_loss:0.125, val_acc:0.936]
Epoch [55/120    avg_loss:0.104, val_acc:0.925]
Epoch [56/120    avg_loss:0.105, val_acc:0.940]
Epoch [57/120    avg_loss:0.086, val_acc:0.955]
Epoch [58/120    avg_loss:0.064, val_acc:0.949]
Epoch [59/120    avg_loss:0.050, val_acc:0.961]
Epoch [60/120    avg_loss:0.053, val_acc:0.964]
Epoch [61/120    avg_loss:0.044, val_acc:0.964]
Epoch [62/120    avg_loss:0.041, val_acc:0.968]
Epoch [63/120    avg_loss:0.041, val_acc:0.967]
Epoch [64/120    avg_loss:0.032, val_acc:0.967]
Epoch [65/120    avg_loss:0.032, val_acc:0.967]
Epoch [66/120    avg_loss:0.036, val_acc:0.970]
Epoch [67/120    avg_loss:0.037, val_acc:0.969]
Epoch [68/120    avg_loss:0.036, val_acc:0.970]
Epoch [69/120    avg_loss:0.030, val_acc:0.972]
Epoch [70/120    avg_loss:0.031, val_acc:0.970]
Epoch [71/120    avg_loss:0.036, val_acc:0.970]
Epoch [72/120    avg_loss:0.037, val_acc:0.969]
Epoch [73/120    avg_loss:0.030, val_acc:0.968]
Epoch [74/120    avg_loss:0.033, val_acc:0.968]
Epoch [75/120    avg_loss:0.034, val_acc:0.968]
Epoch [76/120    avg_loss:0.035, val_acc:0.968]
Epoch [77/120    avg_loss:0.030, val_acc:0.972]
Epoch [78/120    avg_loss:0.027, val_acc:0.969]
Epoch [79/120    avg_loss:0.033, val_acc:0.969]
Epoch [80/120    avg_loss:0.031, val_acc:0.969]
Epoch [81/120    avg_loss:0.033, val_acc:0.970]
Epoch [82/120    avg_loss:0.032, val_acc:0.971]
Epoch [83/120    avg_loss:0.034, val_acc:0.971]
Epoch [84/120    avg_loss:0.034, val_acc:0.972]
Epoch [85/120    avg_loss:0.029, val_acc:0.972]
Epoch [86/120    avg_loss:0.035, val_acc:0.971]
Epoch [87/120    avg_loss:0.031, val_acc:0.974]
Epoch [88/120    avg_loss:0.029, val_acc:0.973]
Epoch [89/120    avg_loss:0.031, val_acc:0.972]
Epoch [90/120    avg_loss:0.029, val_acc:0.972]
Epoch [91/120    avg_loss:0.026, val_acc:0.972]
Epoch [92/120    avg_loss:0.031, val_acc:0.971]
Epoch [93/120    avg_loss:0.028, val_acc:0.973]
Epoch [94/120    avg_loss:0.029, val_acc:0.974]
Epoch [95/120    avg_loss:0.027, val_acc:0.970]
Epoch [96/120    avg_loss:0.028, val_acc:0.970]
Epoch [97/120    avg_loss:0.023, val_acc:0.972]
Epoch [98/120    avg_loss:0.026, val_acc:0.972]
Epoch [99/120    avg_loss:0.028, val_acc:0.973]
Epoch [100/120    avg_loss:0.025, val_acc:0.974]
Epoch [101/120    avg_loss:0.029, val_acc:0.974]
Epoch [102/120    avg_loss:0.025, val_acc:0.972]
Epoch [103/120    avg_loss:0.027, val_acc:0.973]
Epoch [104/120    avg_loss:0.030, val_acc:0.974]
Epoch [105/120    avg_loss:0.027, val_acc:0.975]
Epoch [106/120    avg_loss:0.026, val_acc:0.975]
Epoch [107/120    avg_loss:0.025, val_acc:0.976]
Epoch [108/120    avg_loss:0.027, val_acc:0.973]
Epoch [109/120    avg_loss:0.025, val_acc:0.973]
Epoch [110/120    avg_loss:0.026, val_acc:0.974]
Epoch [111/120    avg_loss:0.030, val_acc:0.972]
Epoch [112/120    avg_loss:0.024, val_acc:0.975]
Epoch [113/120    avg_loss:0.024, val_acc:0.974]
Epoch [114/120    avg_loss:0.024, val_acc:0.974]
Epoch [115/120    avg_loss:0.028, val_acc:0.974]
Epoch [116/120    avg_loss:0.022, val_acc:0.973]
Epoch [117/120    avg_loss:0.027, val_acc:0.973]
Epoch [118/120    avg_loss:0.022, val_acc:0.974]
Epoch [119/120    avg_loss:0.025, val_acc:0.975]
Epoch [120/120    avg_loss:0.022, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    2    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1245    7    3    2    0    0    0    0    7   21    0    0
     0    0    0]
 [   0    0    1  730    4    0    2    0    0    2    2    3    3    0
     0    0    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  854   16    0    0
     1    0    0]
 [   0    0   22    0    0    0    0    0    0    0   12 2145   30    0
     1    0    0]
 [   0    0    0    3    1    0    0    0    0    0    0    3  522    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    88  257    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.02981029810299

F1 scores:
[       nan 0.92857143 0.97379742 0.9811828  0.97235023 0.99076212
 0.99620349 1.         0.9953271  0.94736842 0.976      0.97522164
 0.95692026 1.         0.95383312 0.82903226 0.98224852]

Kappa:
0.9661308058520618
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faed6a4fe48>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.828, val_acc:0.164]
Epoch [2/120    avg_loss:2.772, val_acc:0.242]
Epoch [3/120    avg_loss:2.698, val_acc:0.278]
Epoch [4/120    avg_loss:2.630, val_acc:0.300]
Epoch [5/120    avg_loss:2.524, val_acc:0.311]
Epoch [6/120    avg_loss:2.460, val_acc:0.377]
Epoch [7/120    avg_loss:2.373, val_acc:0.451]
Epoch [8/120    avg_loss:2.307, val_acc:0.484]
Epoch [9/120    avg_loss:2.240, val_acc:0.508]
Epoch [10/120    avg_loss:2.126, val_acc:0.522]
Epoch [11/120    avg_loss:2.037, val_acc:0.465]
Epoch [12/120    avg_loss:1.929, val_acc:0.555]
Epoch [13/120    avg_loss:1.786, val_acc:0.544]
Epoch [14/120    avg_loss:1.678, val_acc:0.541]
Epoch [15/120    avg_loss:1.596, val_acc:0.622]
Epoch [16/120    avg_loss:1.492, val_acc:0.616]
Epoch [17/120    avg_loss:1.321, val_acc:0.758]
Epoch [18/120    avg_loss:1.272, val_acc:0.764]
Epoch [19/120    avg_loss:1.160, val_acc:0.766]
Epoch [20/120    avg_loss:1.063, val_acc:0.811]
Epoch [21/120    avg_loss:0.859, val_acc:0.829]
Epoch [22/120    avg_loss:0.791, val_acc:0.770]
Epoch [23/120    avg_loss:0.698, val_acc:0.818]
Epoch [24/120    avg_loss:0.607, val_acc:0.814]
Epoch [25/120    avg_loss:0.551, val_acc:0.845]
Epoch [26/120    avg_loss:0.451, val_acc:0.900]
Epoch [27/120    avg_loss:0.457, val_acc:0.893]
Epoch [28/120    avg_loss:0.384, val_acc:0.878]
Epoch [29/120    avg_loss:0.336, val_acc:0.894]
Epoch [30/120    avg_loss:0.289, val_acc:0.902]
Epoch [31/120    avg_loss:0.339, val_acc:0.852]
Epoch [32/120    avg_loss:0.248, val_acc:0.902]
Epoch [33/120    avg_loss:0.243, val_acc:0.887]
Epoch [34/120    avg_loss:0.204, val_acc:0.914]
Epoch [35/120    avg_loss:0.188, val_acc:0.927]
Epoch [36/120    avg_loss:0.206, val_acc:0.877]
Epoch [37/120    avg_loss:0.169, val_acc:0.935]
Epoch [38/120    avg_loss:0.159, val_acc:0.930]
Epoch [39/120    avg_loss:0.136, val_acc:0.943]
Epoch [40/120    avg_loss:0.128, val_acc:0.941]
Epoch [41/120    avg_loss:0.121, val_acc:0.927]
Epoch [42/120    avg_loss:0.120, val_acc:0.939]
Epoch [43/120    avg_loss:0.135, val_acc:0.953]
Epoch [44/120    avg_loss:0.114, val_acc:0.927]
Epoch [45/120    avg_loss:0.105, val_acc:0.946]
Epoch [46/120    avg_loss:0.082, val_acc:0.950]
Epoch [47/120    avg_loss:0.078, val_acc:0.961]
Epoch [48/120    avg_loss:0.067, val_acc:0.955]
Epoch [49/120    avg_loss:0.069, val_acc:0.934]
Epoch [50/120    avg_loss:0.074, val_acc:0.935]
Epoch [51/120    avg_loss:0.071, val_acc:0.949]
Epoch [52/120    avg_loss:0.062, val_acc:0.957]
Epoch [53/120    avg_loss:0.065, val_acc:0.952]
Epoch [54/120    avg_loss:0.053, val_acc:0.963]
Epoch [55/120    avg_loss:0.047, val_acc:0.949]
Epoch [56/120    avg_loss:0.044, val_acc:0.966]
Epoch [57/120    avg_loss:0.038, val_acc:0.967]
Epoch [58/120    avg_loss:0.035, val_acc:0.969]
Epoch [59/120    avg_loss:0.044, val_acc:0.964]
Epoch [60/120    avg_loss:0.056, val_acc:0.955]
Epoch [61/120    avg_loss:0.034, val_acc:0.967]
Epoch [62/120    avg_loss:0.025, val_acc:0.963]
Epoch [63/120    avg_loss:0.021, val_acc:0.968]
Epoch [64/120    avg_loss:0.027, val_acc:0.970]
Epoch [65/120    avg_loss:0.028, val_acc:0.960]
Epoch [66/120    avg_loss:0.031, val_acc:0.972]
Epoch [67/120    avg_loss:0.038, val_acc:0.970]
Epoch [68/120    avg_loss:0.065, val_acc:0.951]
Epoch [69/120    avg_loss:0.043, val_acc:0.959]
Epoch [70/120    avg_loss:0.031, val_acc:0.959]
Epoch [71/120    avg_loss:0.037, val_acc:0.967]
Epoch [72/120    avg_loss:0.048, val_acc:0.964]
Epoch [73/120    avg_loss:0.027, val_acc:0.963]
Epoch [74/120    avg_loss:0.027, val_acc:0.971]
Epoch [75/120    avg_loss:0.019, val_acc:0.974]
Epoch [76/120    avg_loss:0.019, val_acc:0.972]
Epoch [77/120    avg_loss:0.022, val_acc:0.965]
Epoch [78/120    avg_loss:0.032, val_acc:0.969]
Epoch [79/120    avg_loss:0.041, val_acc:0.955]
Epoch [80/120    avg_loss:0.032, val_acc:0.963]
Epoch [81/120    avg_loss:0.021, val_acc:0.965]
Epoch [82/120    avg_loss:0.028, val_acc:0.960]
Epoch [83/120    avg_loss:0.041, val_acc:0.950]
Epoch [84/120    avg_loss:0.025, val_acc:0.967]
Epoch [85/120    avg_loss:0.019, val_acc:0.968]
Epoch [86/120    avg_loss:0.016, val_acc:0.968]
Epoch [87/120    avg_loss:0.013, val_acc:0.976]
Epoch [88/120    avg_loss:0.015, val_acc:0.971]
Epoch [89/120    avg_loss:0.012, val_acc:0.976]
Epoch [90/120    avg_loss:0.011, val_acc:0.972]
Epoch [91/120    avg_loss:0.014, val_acc:0.967]
Epoch [92/120    avg_loss:0.011, val_acc:0.968]
Epoch [93/120    avg_loss:0.010, val_acc:0.968]
Epoch [94/120    avg_loss:0.010, val_acc:0.971]
Epoch [95/120    avg_loss:0.009, val_acc:0.974]
Epoch [96/120    avg_loss:0.011, val_acc:0.970]
Epoch [97/120    avg_loss:0.008, val_acc:0.975]
Epoch [98/120    avg_loss:0.010, val_acc:0.970]
Epoch [99/120    avg_loss:0.012, val_acc:0.973]
Epoch [100/120    avg_loss:0.029, val_acc:0.974]
Epoch [101/120    avg_loss:0.025, val_acc:0.969]
Epoch [102/120    avg_loss:0.020, val_acc:0.973]
Epoch [103/120    avg_loss:0.011, val_acc:0.976]
Epoch [104/120    avg_loss:0.013, val_acc:0.975]
Epoch [105/120    avg_loss:0.010, val_acc:0.975]
Epoch [106/120    avg_loss:0.009, val_acc:0.974]
Epoch [107/120    avg_loss:0.010, val_acc:0.975]
Epoch [108/120    avg_loss:0.008, val_acc:0.975]
Epoch [109/120    avg_loss:0.009, val_acc:0.974]
Epoch [110/120    avg_loss:0.009, val_acc:0.974]
Epoch [111/120    avg_loss:0.008, val_acc:0.975]
Epoch [112/120    avg_loss:0.007, val_acc:0.975]
Epoch [113/120    avg_loss:0.009, val_acc:0.975]
Epoch [114/120    avg_loss:0.009, val_acc:0.975]
Epoch [115/120    avg_loss:0.008, val_acc:0.975]
Epoch [116/120    avg_loss:0.010, val_acc:0.976]
Epoch [117/120    avg_loss:0.008, val_acc:0.976]
Epoch [118/120    avg_loss:0.008, val_acc:0.977]
Epoch [119/120    avg_loss:0.008, val_acc:0.977]
Epoch [120/120    avg_loss:0.008, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1262    7    4    0    0    0    0    0    1   11    0    0
     0    0    0]
 [   0    0    2  733    1    0    1    0    0    0    0    9    1    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    4    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    1    0    0    0  859    8    0    0
     0    0    0]
 [   0    0    6    0    0    0    1    0    0    0   15 2165   23    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  532    0
     1    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1127   11    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    69  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.8970189701897

F1 scores:
[       nan 1.         0.98516784 0.98521505 0.98604651 0.99071926
 0.99696049 0.92592593 0.99767442 1.         0.98171429 0.98275079
 0.97346752 1.         0.96407186 0.87284144 0.98795181]

Kappa:
0.9760193433913583
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f78c0072f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.808, val_acc:0.098]
Epoch [2/120    avg_loss:2.722, val_acc:0.354]
Epoch [3/120    avg_loss:2.649, val_acc:0.397]
Epoch [4/120    avg_loss:2.569, val_acc:0.411]
Epoch [5/120    avg_loss:2.469, val_acc:0.443]
Epoch [6/120    avg_loss:2.374, val_acc:0.484]
Epoch [7/120    avg_loss:2.293, val_acc:0.533]
Epoch [8/120    avg_loss:2.192, val_acc:0.580]
Epoch [9/120    avg_loss:2.073, val_acc:0.608]
Epoch [10/120    avg_loss:2.001, val_acc:0.657]
Epoch [11/120    avg_loss:1.906, val_acc:0.686]
Epoch [12/120    avg_loss:1.771, val_acc:0.676]
Epoch [13/120    avg_loss:1.663, val_acc:0.688]
Epoch [14/120    avg_loss:1.492, val_acc:0.658]
Epoch [15/120    avg_loss:1.304, val_acc:0.740]
Epoch [16/120    avg_loss:1.229, val_acc:0.695]
Epoch [17/120    avg_loss:1.103, val_acc:0.762]
Epoch [18/120    avg_loss:0.957, val_acc:0.786]
Epoch [19/120    avg_loss:0.864, val_acc:0.791]
Epoch [20/120    avg_loss:0.735, val_acc:0.784]
Epoch [21/120    avg_loss:0.687, val_acc:0.781]
Epoch [22/120    avg_loss:0.608, val_acc:0.853]
Epoch [23/120    avg_loss:0.513, val_acc:0.819]
Epoch [24/120    avg_loss:0.472, val_acc:0.829]
Epoch [25/120    avg_loss:0.381, val_acc:0.852]
Epoch [26/120    avg_loss:0.365, val_acc:0.869]
Epoch [27/120    avg_loss:0.339, val_acc:0.891]
Epoch [28/120    avg_loss:0.293, val_acc:0.883]
Epoch [29/120    avg_loss:0.246, val_acc:0.886]
Epoch [30/120    avg_loss:0.237, val_acc:0.907]
Epoch [31/120    avg_loss:0.206, val_acc:0.894]
Epoch [32/120    avg_loss:0.194, val_acc:0.927]
Epoch [33/120    avg_loss:0.181, val_acc:0.914]
Epoch [34/120    avg_loss:0.189, val_acc:0.905]
Epoch [35/120    avg_loss:0.174, val_acc:0.932]
Epoch [36/120    avg_loss:0.132, val_acc:0.939]
Epoch [37/120    avg_loss:0.145, val_acc:0.935]
Epoch [38/120    avg_loss:0.124, val_acc:0.934]
Epoch [39/120    avg_loss:0.110, val_acc:0.941]
Epoch [40/120    avg_loss:0.104, val_acc:0.934]
Epoch [41/120    avg_loss:0.096, val_acc:0.950]
Epoch [42/120    avg_loss:0.105, val_acc:0.942]
Epoch [43/120    avg_loss:0.116, val_acc:0.909]
Epoch [44/120    avg_loss:0.097, val_acc:0.946]
Epoch [45/120    avg_loss:0.082, val_acc:0.944]
Epoch [46/120    avg_loss:0.077, val_acc:0.948]
Epoch [47/120    avg_loss:0.067, val_acc:0.957]
Epoch [48/120    avg_loss:0.057, val_acc:0.965]
Epoch [49/120    avg_loss:0.047, val_acc:0.958]
Epoch [50/120    avg_loss:0.060, val_acc:0.960]
Epoch [51/120    avg_loss:0.051, val_acc:0.951]
Epoch [52/120    avg_loss:0.050, val_acc:0.955]
Epoch [53/120    avg_loss:0.051, val_acc:0.951]
Epoch [54/120    avg_loss:0.069, val_acc:0.928]
Epoch [55/120    avg_loss:0.067, val_acc:0.931]
Epoch [56/120    avg_loss:0.055, val_acc:0.944]
Epoch [57/120    avg_loss:0.085, val_acc:0.869]
Epoch [58/120    avg_loss:0.087, val_acc:0.959]
Epoch [59/120    avg_loss:0.055, val_acc:0.954]
Epoch [60/120    avg_loss:0.045, val_acc:0.957]
Epoch [61/120    avg_loss:0.060, val_acc:0.944]
Epoch [62/120    avg_loss:0.050, val_acc:0.961]
Epoch [63/120    avg_loss:0.038, val_acc:0.965]
Epoch [64/120    avg_loss:0.032, val_acc:0.965]
Epoch [65/120    avg_loss:0.028, val_acc:0.967]
Epoch [66/120    avg_loss:0.029, val_acc:0.965]
Epoch [67/120    avg_loss:0.035, val_acc:0.965]
Epoch [68/120    avg_loss:0.024, val_acc:0.966]
Epoch [69/120    avg_loss:0.025, val_acc:0.967]
Epoch [70/120    avg_loss:0.026, val_acc:0.968]
Epoch [71/120    avg_loss:0.025, val_acc:0.968]
Epoch [72/120    avg_loss:0.026, val_acc:0.968]
Epoch [73/120    avg_loss:0.024, val_acc:0.965]
Epoch [74/120    avg_loss:0.023, val_acc:0.966]
Epoch [75/120    avg_loss:0.024, val_acc:0.968]
Epoch [76/120    avg_loss:0.024, val_acc:0.965]
Epoch [77/120    avg_loss:0.021, val_acc:0.965]
Epoch [78/120    avg_loss:0.023, val_acc:0.966]
Epoch [79/120    avg_loss:0.023, val_acc:0.966]
Epoch [80/120    avg_loss:0.022, val_acc:0.966]
Epoch [81/120    avg_loss:0.023, val_acc:0.966]
Epoch [82/120    avg_loss:0.020, val_acc:0.964]
Epoch [83/120    avg_loss:0.022, val_acc:0.964]
Epoch [84/120    avg_loss:0.028, val_acc:0.965]
Epoch [85/120    avg_loss:0.023, val_acc:0.966]
Epoch [86/120    avg_loss:0.024, val_acc:0.966]
Epoch [87/120    avg_loss:0.021, val_acc:0.966]
Epoch [88/120    avg_loss:0.025, val_acc:0.965]
Epoch [89/120    avg_loss:0.022, val_acc:0.966]
Epoch [90/120    avg_loss:0.020, val_acc:0.965]
Epoch [91/120    avg_loss:0.021, val_acc:0.966]
Epoch [92/120    avg_loss:0.019, val_acc:0.967]
Epoch [93/120    avg_loss:0.020, val_acc:0.967]
Epoch [94/120    avg_loss:0.017, val_acc:0.967]
Epoch [95/120    avg_loss:0.026, val_acc:0.967]
Epoch [96/120    avg_loss:0.022, val_acc:0.967]
Epoch [97/120    avg_loss:0.019, val_acc:0.967]
Epoch [98/120    avg_loss:0.023, val_acc:0.967]
Epoch [99/120    avg_loss:0.018, val_acc:0.967]
Epoch [100/120    avg_loss:0.020, val_acc:0.967]
Epoch [101/120    avg_loss:0.019, val_acc:0.967]
Epoch [102/120    avg_loss:0.019, val_acc:0.967]
Epoch [103/120    avg_loss:0.020, val_acc:0.967]
Epoch [104/120    avg_loss:0.018, val_acc:0.967]
Epoch [105/120    avg_loss:0.018, val_acc:0.967]
Epoch [106/120    avg_loss:0.017, val_acc:0.967]
Epoch [107/120    avg_loss:0.018, val_acc:0.967]
Epoch [108/120    avg_loss:0.020, val_acc:0.967]
Epoch [109/120    avg_loss:0.021, val_acc:0.967]
Epoch [110/120    avg_loss:0.019, val_acc:0.967]
Epoch [111/120    avg_loss:0.020, val_acc:0.967]
Epoch [112/120    avg_loss:0.019, val_acc:0.967]
Epoch [113/120    avg_loss:0.018, val_acc:0.967]
Epoch [114/120    avg_loss:0.018, val_acc:0.967]
Epoch [115/120    avg_loss:0.019, val_acc:0.967]
Epoch [116/120    avg_loss:0.022, val_acc:0.967]
Epoch [117/120    avg_loss:0.020, val_acc:0.967]
Epoch [118/120    avg_loss:0.021, val_acc:0.967]
Epoch [119/120    avg_loss:0.024, val_acc:0.967]
Epoch [120/120    avg_loss:0.022, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0 1255    2   13    0    0    0    0    0    2   13    0    0
     0    0    0]
 [   0    0    0  726    2    0    0    0    0    2    1   13    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    1    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  850   19    1    0
     1    0    0]
 [   0    0    6    0    0    0    0    0    0    0   17 2165   19    0
     3    0    0]
 [   0    0    0    7    0    0    0    0    0    0    0    2  523    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    1    0    0    0   13    0    0    0    0    0    0    0
    76  257    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.34417344173441

F1 scores:
[       nan 0.98765432 0.98392787 0.97975709 0.96598639 0.99769585
 0.98716981 1.         0.99883586 0.91891892 0.97421203 0.97830999
 0.96583564 1.         0.9595228  0.83306321 0.97619048]

Kappa:
0.9697031091697865
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8170068eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.841, val_acc:0.092]
Epoch [2/120    avg_loss:2.762, val_acc:0.193]
Epoch [3/120    avg_loss:2.682, val_acc:0.271]
Epoch [4/120    avg_loss:2.607, val_acc:0.378]
Epoch [5/120    avg_loss:2.478, val_acc:0.458]
Epoch [6/120    avg_loss:2.414, val_acc:0.482]
Epoch [7/120    avg_loss:2.311, val_acc:0.494]
Epoch [8/120    avg_loss:2.246, val_acc:0.508]
Epoch [9/120    avg_loss:2.093, val_acc:0.530]
Epoch [10/120    avg_loss:1.994, val_acc:0.545]
Epoch [11/120    avg_loss:1.886, val_acc:0.534]
Epoch [12/120    avg_loss:1.750, val_acc:0.571]
Epoch [13/120    avg_loss:1.640, val_acc:0.610]
Epoch [14/120    avg_loss:1.540, val_acc:0.657]
Epoch [15/120    avg_loss:1.364, val_acc:0.719]
Epoch [16/120    avg_loss:1.177, val_acc:0.726]
Epoch [17/120    avg_loss:1.064, val_acc:0.752]
Epoch [18/120    avg_loss:0.926, val_acc:0.755]
Epoch [19/120    avg_loss:0.850, val_acc:0.757]
Epoch [20/120    avg_loss:0.727, val_acc:0.776]
Epoch [21/120    avg_loss:0.659, val_acc:0.790]
Epoch [22/120    avg_loss:0.595, val_acc:0.823]
Epoch [23/120    avg_loss:0.556, val_acc:0.724]
Epoch [24/120    avg_loss:0.515, val_acc:0.816]
Epoch [25/120    avg_loss:0.458, val_acc:0.814]
Epoch [26/120    avg_loss:0.429, val_acc:0.851]
Epoch [27/120    avg_loss:0.409, val_acc:0.861]
Epoch [28/120    avg_loss:0.327, val_acc:0.857]
Epoch [29/120    avg_loss:0.247, val_acc:0.874]
Epoch [30/120    avg_loss:0.221, val_acc:0.885]
Epoch [31/120    avg_loss:0.205, val_acc:0.898]
Epoch [32/120    avg_loss:0.205, val_acc:0.892]
Epoch [33/120    avg_loss:0.197, val_acc:0.895]
Epoch [34/120    avg_loss:0.188, val_acc:0.910]
Epoch [35/120    avg_loss:0.209, val_acc:0.897]
Epoch [36/120    avg_loss:0.167, val_acc:0.914]
Epoch [37/120    avg_loss:0.161, val_acc:0.889]
Epoch [38/120    avg_loss:0.138, val_acc:0.907]
Epoch [39/120    avg_loss:0.131, val_acc:0.927]
Epoch [40/120    avg_loss:0.115, val_acc:0.941]
Epoch [41/120    avg_loss:0.098, val_acc:0.914]
Epoch [42/120    avg_loss:0.121, val_acc:0.933]
Epoch [43/120    avg_loss:0.074, val_acc:0.940]
Epoch [44/120    avg_loss:0.098, val_acc:0.942]
Epoch [45/120    avg_loss:0.083, val_acc:0.941]
Epoch [46/120    avg_loss:0.063, val_acc:0.940]
Epoch [47/120    avg_loss:0.084, val_acc:0.944]
Epoch [48/120    avg_loss:0.124, val_acc:0.887]
Epoch [49/120    avg_loss:0.098, val_acc:0.926]
Epoch [50/120    avg_loss:0.078, val_acc:0.934]
Epoch [51/120    avg_loss:0.072, val_acc:0.950]
Epoch [52/120    avg_loss:0.082, val_acc:0.941]
Epoch [53/120    avg_loss:0.094, val_acc:0.940]
Epoch [54/120    avg_loss:0.091, val_acc:0.952]
Epoch [55/120    avg_loss:0.130, val_acc:0.916]
Epoch [56/120    avg_loss:0.093, val_acc:0.920]
Epoch [57/120    avg_loss:0.065, val_acc:0.949]
Epoch [58/120    avg_loss:0.078, val_acc:0.939]
Epoch [59/120    avg_loss:0.066, val_acc:0.950]
Epoch [60/120    avg_loss:0.058, val_acc:0.940]
Epoch [61/120    avg_loss:0.079, val_acc:0.950]
Epoch [62/120    avg_loss:0.115, val_acc:0.943]
Epoch [63/120    avg_loss:0.068, val_acc:0.949]
Epoch [64/120    avg_loss:0.052, val_acc:0.918]
Epoch [65/120    avg_loss:0.056, val_acc:0.949]
Epoch [66/120    avg_loss:0.045, val_acc:0.956]
Epoch [67/120    avg_loss:0.038, val_acc:0.953]
Epoch [68/120    avg_loss:0.048, val_acc:0.947]
Epoch [69/120    avg_loss:0.046, val_acc:0.953]
Epoch [70/120    avg_loss:0.046, val_acc:0.953]
Epoch [71/120    avg_loss:0.035, val_acc:0.961]
Epoch [72/120    avg_loss:0.030, val_acc:0.961]
Epoch [73/120    avg_loss:0.027, val_acc:0.966]
Epoch [74/120    avg_loss:0.027, val_acc:0.956]
Epoch [75/120    avg_loss:0.029, val_acc:0.972]
Epoch [76/120    avg_loss:0.026, val_acc:0.921]
Epoch [77/120    avg_loss:0.033, val_acc:0.958]
Epoch [78/120    avg_loss:0.027, val_acc:0.969]
Epoch [79/120    avg_loss:0.019, val_acc:0.973]
Epoch [80/120    avg_loss:0.019, val_acc:0.963]
Epoch [81/120    avg_loss:0.018, val_acc:0.968]
Epoch [82/120    avg_loss:0.017, val_acc:0.970]
Epoch [83/120    avg_loss:0.014, val_acc:0.972]
Epoch [84/120    avg_loss:0.013, val_acc:0.973]
Epoch [85/120    avg_loss:0.021, val_acc:0.959]
Epoch [86/120    avg_loss:0.020, val_acc:0.968]
Epoch [87/120    avg_loss:0.020, val_acc:0.953]
Epoch [88/120    avg_loss:0.020, val_acc:0.965]
Epoch [89/120    avg_loss:0.020, val_acc:0.967]
Epoch [90/120    avg_loss:0.017, val_acc:0.970]
Epoch [91/120    avg_loss:0.013, val_acc:0.975]
Epoch [92/120    avg_loss:0.014, val_acc:0.977]
Epoch [93/120    avg_loss:0.041, val_acc:0.972]
Epoch [94/120    avg_loss:0.023, val_acc:0.968]
Epoch [95/120    avg_loss:0.017, val_acc:0.970]
Epoch [96/120    avg_loss:0.013, val_acc:0.972]
Epoch [97/120    avg_loss:0.011, val_acc:0.974]
Epoch [98/120    avg_loss:0.021, val_acc:0.969]
Epoch [99/120    avg_loss:0.022, val_acc:0.969]
Epoch [100/120    avg_loss:0.014, val_acc:0.974]
Epoch [101/120    avg_loss:0.011, val_acc:0.977]
Epoch [102/120    avg_loss:0.012, val_acc:0.976]
Epoch [103/120    avg_loss:0.009, val_acc:0.973]
Epoch [104/120    avg_loss:0.014, val_acc:0.973]
Epoch [105/120    avg_loss:0.012, val_acc:0.972]
Epoch [106/120    avg_loss:0.012, val_acc:0.974]
Epoch [107/120    avg_loss:0.013, val_acc:0.976]
Epoch [108/120    avg_loss:0.008, val_acc:0.975]
Epoch [109/120    avg_loss:0.008, val_acc:0.967]
Epoch [110/120    avg_loss:0.009, val_acc:0.975]
Epoch [111/120    avg_loss:0.008, val_acc:0.975]
Epoch [112/120    avg_loss:0.009, val_acc:0.974]
Epoch [113/120    avg_loss:0.023, val_acc:0.963]
Epoch [114/120    avg_loss:0.011, val_acc:0.975]
Epoch [115/120    avg_loss:0.010, val_acc:0.975]
Epoch [116/120    avg_loss:0.008, val_acc:0.974]
Epoch [117/120    avg_loss:0.007, val_acc:0.974]
Epoch [118/120    avg_loss:0.009, val_acc:0.974]
Epoch [119/120    avg_loss:0.007, val_acc:0.973]
Epoch [120/120    avg_loss:0.009, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1252    4    3    0    0    0    0    0    1   25    0    0
     0    0    0]
 [   0    0    1  730    0    0    0    0    0    3    1    9    3    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    2    0    0    0    0    0    0
     1    0    0]
 [   0    0    2    0    0    0  654    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    0    0
     0    1    0]
 [   0    0    8    0    0    0    0    0    0    0  843   24    0    0
     0    0    0]
 [   0    0    9    0    0    0    0    0    0    0    8 2180   13    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0    0    5    3  517    0
     2    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1129    9    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
    41  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.64769647696477

F1 scores:
[       nan 0.98765432 0.97927259 0.9811828  0.9882904  0.99653979
 0.97830965 0.96153846 0.99883586 0.86486486 0.97231834 0.97933513
 0.96816479 1.         0.9766436  0.87949922 0.98823529]

Kappa:
0.9731568055297203
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f843a391eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.803, val_acc:0.164]
Epoch [2/120    avg_loss:2.731, val_acc:0.314]
Epoch [3/120    avg_loss:2.627, val_acc:0.366]
Epoch [4/120    avg_loss:2.545, val_acc:0.349]
Epoch [5/120    avg_loss:2.463, val_acc:0.401]
Epoch [6/120    avg_loss:2.377, val_acc:0.425]
Epoch [7/120    avg_loss:2.325, val_acc:0.463]
Epoch [8/120    avg_loss:2.259, val_acc:0.487]
Epoch [9/120    avg_loss:2.213, val_acc:0.546]
Epoch [10/120    avg_loss:2.115, val_acc:0.569]
Epoch [11/120    avg_loss:2.017, val_acc:0.581]
Epoch [12/120    avg_loss:1.922, val_acc:0.637]
Epoch [13/120    avg_loss:1.830, val_acc:0.683]
Epoch [14/120    avg_loss:1.711, val_acc:0.692]
Epoch [15/120    avg_loss:1.571, val_acc:0.704]
Epoch [16/120    avg_loss:1.441, val_acc:0.715]
Epoch [17/120    avg_loss:1.258, val_acc:0.758]
Epoch [18/120    avg_loss:1.114, val_acc:0.759]
Epoch [19/120    avg_loss:1.008, val_acc:0.769]
Epoch [20/120    avg_loss:0.885, val_acc:0.782]
Epoch [21/120    avg_loss:0.797, val_acc:0.828]
Epoch [22/120    avg_loss:0.657, val_acc:0.821]
Epoch [23/120    avg_loss:0.615, val_acc:0.828]
Epoch [24/120    avg_loss:0.551, val_acc:0.779]
Epoch [25/120    avg_loss:0.784, val_acc:0.845]
Epoch [26/120    avg_loss:0.501, val_acc:0.847]
Epoch [27/120    avg_loss:0.486, val_acc:0.846]
Epoch [28/120    avg_loss:0.416, val_acc:0.819]
Epoch [29/120    avg_loss:0.343, val_acc:0.866]
Epoch [30/120    avg_loss:0.346, val_acc:0.868]
Epoch [31/120    avg_loss:0.292, val_acc:0.878]
Epoch [32/120    avg_loss:0.262, val_acc:0.895]
Epoch [33/120    avg_loss:0.228, val_acc:0.910]
Epoch [34/120    avg_loss:0.183, val_acc:0.911]
Epoch [35/120    avg_loss:0.181, val_acc:0.918]
Epoch [36/120    avg_loss:0.174, val_acc:0.908]
Epoch [37/120    avg_loss:0.138, val_acc:0.939]
Epoch [38/120    avg_loss:0.124, val_acc:0.934]
Epoch [39/120    avg_loss:0.118, val_acc:0.920]
Epoch [40/120    avg_loss:0.124, val_acc:0.933]
Epoch [41/120    avg_loss:0.166, val_acc:0.907]
Epoch [42/120    avg_loss:0.144, val_acc:0.910]
Epoch [43/120    avg_loss:0.109, val_acc:0.912]
Epoch [44/120    avg_loss:0.122, val_acc:0.923]
Epoch [45/120    avg_loss:0.087, val_acc:0.940]
Epoch [46/120    avg_loss:0.081, val_acc:0.948]
Epoch [47/120    avg_loss:0.076, val_acc:0.957]
Epoch [48/120    avg_loss:0.084, val_acc:0.941]
Epoch [49/120    avg_loss:0.069, val_acc:0.954]
Epoch [50/120    avg_loss:0.048, val_acc:0.958]
Epoch [51/120    avg_loss:0.060, val_acc:0.920]
Epoch [52/120    avg_loss:0.065, val_acc:0.952]
Epoch [53/120    avg_loss:0.062, val_acc:0.948]
Epoch [54/120    avg_loss:0.062, val_acc:0.959]
Epoch [55/120    avg_loss:0.048, val_acc:0.964]
Epoch [56/120    avg_loss:0.050, val_acc:0.957]
Epoch [57/120    avg_loss:0.035, val_acc:0.969]
Epoch [58/120    avg_loss:0.056, val_acc:0.952]
Epoch [59/120    avg_loss:0.062, val_acc:0.935]
Epoch [60/120    avg_loss:0.055, val_acc:0.959]
Epoch [61/120    avg_loss:0.044, val_acc:0.948]
Epoch [62/120    avg_loss:0.067, val_acc:0.961]
Epoch [63/120    avg_loss:0.059, val_acc:0.964]
Epoch [64/120    avg_loss:0.033, val_acc:0.958]
Epoch [65/120    avg_loss:0.107, val_acc:0.927]
Epoch [66/120    avg_loss:0.075, val_acc:0.950]
Epoch [67/120    avg_loss:0.051, val_acc:0.957]
Epoch [68/120    avg_loss:0.044, val_acc:0.969]
Epoch [69/120    avg_loss:0.028, val_acc:0.974]
Epoch [70/120    avg_loss:0.029, val_acc:0.970]
Epoch [71/120    avg_loss:0.038, val_acc:0.947]
Epoch [72/120    avg_loss:0.034, val_acc:0.959]
Epoch [73/120    avg_loss:0.039, val_acc:0.922]
Epoch [74/120    avg_loss:0.048, val_acc:0.952]
Epoch [75/120    avg_loss:0.026, val_acc:0.971]
Epoch [76/120    avg_loss:0.022, val_acc:0.974]
Epoch [77/120    avg_loss:0.020, val_acc:0.975]
Epoch [78/120    avg_loss:0.024, val_acc:0.971]
Epoch [79/120    avg_loss:0.023, val_acc:0.967]
Epoch [80/120    avg_loss:0.025, val_acc:0.978]
Epoch [81/120    avg_loss:0.019, val_acc:0.976]
Epoch [82/120    avg_loss:0.019, val_acc:0.971]
Epoch [83/120    avg_loss:0.018, val_acc:0.978]
Epoch [84/120    avg_loss:0.014, val_acc:0.973]
Epoch [85/120    avg_loss:0.022, val_acc:0.974]
Epoch [86/120    avg_loss:0.014, val_acc:0.976]
Epoch [87/120    avg_loss:0.014, val_acc:0.977]
Epoch [88/120    avg_loss:0.013, val_acc:0.966]
Epoch [89/120    avg_loss:0.014, val_acc:0.979]
Epoch [90/120    avg_loss:0.012, val_acc:0.976]
Epoch [91/120    avg_loss:0.010, val_acc:0.979]
Epoch [92/120    avg_loss:0.012, val_acc:0.978]
Epoch [93/120    avg_loss:0.018, val_acc:0.980]
Epoch [94/120    avg_loss:0.015, val_acc:0.981]
Epoch [95/120    avg_loss:0.012, val_acc:0.978]
Epoch [96/120    avg_loss:0.013, val_acc:0.977]
Epoch [97/120    avg_loss:0.012, val_acc:0.979]
Epoch [98/120    avg_loss:0.011, val_acc:0.979]
Epoch [99/120    avg_loss:0.012, val_acc:0.978]
Epoch [100/120    avg_loss:0.020, val_acc:0.978]
Epoch [101/120    avg_loss:0.103, val_acc:0.952]
Epoch [102/120    avg_loss:0.223, val_acc:0.881]
Epoch [103/120    avg_loss:0.130, val_acc:0.942]
Epoch [104/120    avg_loss:0.081, val_acc:0.946]
Epoch [105/120    avg_loss:0.122, val_acc:0.938]
Epoch [106/120    avg_loss:0.060, val_acc:0.946]
Epoch [107/120    avg_loss:0.039, val_acc:0.961]
Epoch [108/120    avg_loss:0.029, val_acc:0.966]
Epoch [109/120    avg_loss:0.023, val_acc:0.967]
Epoch [110/120    avg_loss:0.022, val_acc:0.967]
Epoch [111/120    avg_loss:0.025, val_acc:0.968]
Epoch [112/120    avg_loss:0.021, val_acc:0.969]
Epoch [113/120    avg_loss:0.022, val_acc:0.971]
Epoch [114/120    avg_loss:0.020, val_acc:0.971]
Epoch [115/120    avg_loss:0.024, val_acc:0.972]
Epoch [116/120    avg_loss:0.018, val_acc:0.973]
Epoch [117/120    avg_loss:0.018, val_acc:0.972]
Epoch [118/120    avg_loss:0.018, val_acc:0.971]
Epoch [119/120    avg_loss:0.017, val_acc:0.973]
Epoch [120/120    avg_loss:0.017, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    1    1    0    0
     0    0    0]
 [   0    0 1239    3    3    3    0    0    0    0    5   32    0    0
     0    0    0]
 [   0    0    0  722    9    1    0    0    0    0    1   10    4    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    4    0    0    1    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  649    0    0    0    0    6    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   13    0    0    0    5
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    2  853   13    0    0
     1    1    0]
 [   0    0    3    0    0    0    0    0    0    0   26 2155   25    0
     0    1    0]
 [   0    0    0    3    0    0    0    0    0    0    2    3  521    0
     0    2    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1123   15    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    56  280    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.11653116531166

F1 scores:
[       nan 0.975      0.97867299 0.97831978 0.97025172 0.98964327
 0.98557327 0.92592593 0.99883586 0.76470588 0.96712018 0.97291196
 0.96036866 0.98666667 0.96768634 0.86687307 0.98245614]

Kappa:
0.9671178721470346
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe5d3a19f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.786, val_acc:0.165]
Epoch [2/120    avg_loss:2.743, val_acc:0.221]
Epoch [3/120    avg_loss:2.672, val_acc:0.271]
Epoch [4/120    avg_loss:2.605, val_acc:0.336]
Epoch [5/120    avg_loss:2.519, val_acc:0.389]
Epoch [6/120    avg_loss:2.427, val_acc:0.396]
Epoch [7/120    avg_loss:2.329, val_acc:0.408]
Epoch [8/120    avg_loss:2.276, val_acc:0.483]
Epoch [9/120    avg_loss:2.187, val_acc:0.470]
Epoch [10/120    avg_loss:2.101, val_acc:0.533]
Epoch [11/120    avg_loss:1.967, val_acc:0.572]
Epoch [12/120    avg_loss:1.866, val_acc:0.593]
Epoch [13/120    avg_loss:1.760, val_acc:0.637]
Epoch [14/120    avg_loss:1.637, val_acc:0.599]
Epoch [15/120    avg_loss:1.578, val_acc:0.628]
Epoch [16/120    avg_loss:1.390, val_acc:0.666]
Epoch [17/120    avg_loss:1.326, val_acc:0.720]
Epoch [18/120    avg_loss:1.218, val_acc:0.704]
Epoch [19/120    avg_loss:1.142, val_acc:0.714]
Epoch [20/120    avg_loss:1.009, val_acc:0.773]
Epoch [21/120    avg_loss:0.906, val_acc:0.819]
Epoch [22/120    avg_loss:0.920, val_acc:0.811]
Epoch [23/120    avg_loss:0.730, val_acc:0.825]
Epoch [24/120    avg_loss:0.662, val_acc:0.858]
Epoch [25/120    avg_loss:0.638, val_acc:0.830]
Epoch [26/120    avg_loss:0.585, val_acc:0.868]
Epoch [27/120    avg_loss:0.492, val_acc:0.864]
Epoch [28/120    avg_loss:0.439, val_acc:0.866]
Epoch [29/120    avg_loss:0.424, val_acc:0.873]
Epoch [30/120    avg_loss:0.335, val_acc:0.900]
Epoch [31/120    avg_loss:0.347, val_acc:0.893]
Epoch [32/120    avg_loss:0.330, val_acc:0.877]
Epoch [33/120    avg_loss:0.275, val_acc:0.889]
Epoch [34/120    avg_loss:0.234, val_acc:0.899]
Epoch [35/120    avg_loss:0.231, val_acc:0.908]
Epoch [36/120    avg_loss:0.226, val_acc:0.910]
Epoch [37/120    avg_loss:0.232, val_acc:0.907]
Epoch [38/120    avg_loss:0.279, val_acc:0.842]
Epoch [39/120    avg_loss:0.373, val_acc:0.890]
Epoch [40/120    avg_loss:0.266, val_acc:0.887]
Epoch [41/120    avg_loss:0.182, val_acc:0.911]
Epoch [42/120    avg_loss:0.165, val_acc:0.920]
Epoch [43/120    avg_loss:0.136, val_acc:0.890]
Epoch [44/120    avg_loss:0.132, val_acc:0.939]
Epoch [45/120    avg_loss:0.146, val_acc:0.891]
Epoch [46/120    avg_loss:0.109, val_acc:0.945]
Epoch [47/120    avg_loss:0.107, val_acc:0.930]
Epoch [48/120    avg_loss:0.118, val_acc:0.931]
Epoch [49/120    avg_loss:0.115, val_acc:0.904]
Epoch [50/120    avg_loss:0.097, val_acc:0.947]
Epoch [51/120    avg_loss:0.098, val_acc:0.940]
Epoch [52/120    avg_loss:0.136, val_acc:0.944]
Epoch [53/120    avg_loss:0.098, val_acc:0.952]
Epoch [54/120    avg_loss:0.111, val_acc:0.948]
Epoch [55/120    avg_loss:0.095, val_acc:0.942]
Epoch [56/120    avg_loss:0.072, val_acc:0.955]
Epoch [57/120    avg_loss:0.101, val_acc:0.945]
Epoch [58/120    avg_loss:0.082, val_acc:0.954]
Epoch [59/120    avg_loss:0.080, val_acc:0.966]
Epoch [60/120    avg_loss:0.070, val_acc:0.940]
Epoch [61/120    avg_loss:0.082, val_acc:0.952]
Epoch [62/120    avg_loss:0.069, val_acc:0.960]
Epoch [63/120    avg_loss:0.053, val_acc:0.963]
Epoch [64/120    avg_loss:0.040, val_acc:0.967]
Epoch [65/120    avg_loss:0.038, val_acc:0.976]
Epoch [66/120    avg_loss:0.039, val_acc:0.940]
Epoch [67/120    avg_loss:0.045, val_acc:0.966]
Epoch [68/120    avg_loss:0.052, val_acc:0.967]
Epoch [69/120    avg_loss:0.045, val_acc:0.960]
Epoch [70/120    avg_loss:0.048, val_acc:0.966]
Epoch [71/120    avg_loss:0.040, val_acc:0.970]
Epoch [72/120    avg_loss:0.030, val_acc:0.974]
Epoch [73/120    avg_loss:0.036, val_acc:0.975]
Epoch [74/120    avg_loss:0.026, val_acc:0.975]
Epoch [75/120    avg_loss:0.032, val_acc:0.977]
Epoch [76/120    avg_loss:0.025, val_acc:0.968]
Epoch [77/120    avg_loss:0.021, val_acc:0.974]
Epoch [78/120    avg_loss:0.018, val_acc:0.977]
Epoch [79/120    avg_loss:0.020, val_acc:0.964]
Epoch [80/120    avg_loss:0.031, val_acc:0.969]
Epoch [81/120    avg_loss:0.059, val_acc:0.965]
Epoch [82/120    avg_loss:0.052, val_acc:0.977]
Epoch [83/120    avg_loss:0.039, val_acc:0.967]
Epoch [84/120    avg_loss:0.029, val_acc:0.975]
Epoch [85/120    avg_loss:0.030, val_acc:0.975]
Epoch [86/120    avg_loss:0.034, val_acc:0.978]
Epoch [87/120    avg_loss:0.027, val_acc:0.971]
Epoch [88/120    avg_loss:0.024, val_acc:0.972]
Epoch [89/120    avg_loss:0.023, val_acc:0.975]
Epoch [90/120    avg_loss:0.022, val_acc:0.979]
Epoch [91/120    avg_loss:0.020, val_acc:0.978]
Epoch [92/120    avg_loss:0.017, val_acc:0.983]
Epoch [93/120    avg_loss:0.016, val_acc:0.974]
Epoch [94/120    avg_loss:0.024, val_acc:0.974]
Epoch [95/120    avg_loss:0.023, val_acc:0.970]
Epoch [96/120    avg_loss:0.018, val_acc:0.979]
Epoch [97/120    avg_loss:0.017, val_acc:0.977]
Epoch [98/120    avg_loss:0.026, val_acc:0.980]
Epoch [99/120    avg_loss:0.014, val_acc:0.978]
Epoch [100/120    avg_loss:0.011, val_acc:0.976]
Epoch [101/120    avg_loss:0.012, val_acc:0.985]
Epoch [102/120    avg_loss:0.018, val_acc:0.975]
Epoch [103/120    avg_loss:0.013, val_acc:0.980]
Epoch [104/120    avg_loss:0.025, val_acc:0.978]
Epoch [105/120    avg_loss:0.020, val_acc:0.980]
Epoch [106/120    avg_loss:0.014, val_acc:0.985]
Epoch [107/120    avg_loss:0.015, val_acc:0.982]
Epoch [108/120    avg_loss:0.021, val_acc:0.975]
Epoch [109/120    avg_loss:0.017, val_acc:0.984]
Epoch [110/120    avg_loss:0.011, val_acc:0.980]
Epoch [111/120    avg_loss:0.009, val_acc:0.982]
Epoch [112/120    avg_loss:0.009, val_acc:0.980]
Epoch [113/120    avg_loss:0.014, val_acc:0.980]
Epoch [114/120    avg_loss:0.013, val_acc:0.981]
Epoch [115/120    avg_loss:0.009, val_acc:0.979]
Epoch [116/120    avg_loss:0.009, val_acc:0.981]
Epoch [117/120    avg_loss:0.008, val_acc:0.981]
Epoch [118/120    avg_loss:0.012, val_acc:0.975]
Epoch [119/120    avg_loss:0.019, val_acc:0.974]
Epoch [120/120    avg_loss:0.013, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    0    3    0    0
     0    0    0]
 [   0    0 1255    5    4    1    1    0    0    0    0   19    0    0
     0    0    0]
 [   0    0    0  733    8    0    0    0    0    2    0    1    2    0
     1    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   11    0    0    1    0    0    0    0  847   15    1    0
     0    0    0]
 [   0    0   21    0    0    2    0    0    0    1    9 2162   15    0
     0    0    0]
 [   0    0    0    3    0    1    0    0    0    0    3    2  521    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0    0
  1123   15    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    85  262    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.37669376693766

F1 scores:
[       nan 0.96202532 0.97589425 0.98323273 0.96788991 0.99428571
 0.99771516 1.         1.         0.89473684 0.97693195 0.97961033
 0.97110904 1.         0.95574468 0.83974359 0.98245614]

Kappa:
0.9700769863669707
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc43d183ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.799, val_acc:0.122]
Epoch [2/120    avg_loss:2.743, val_acc:0.319]
Epoch [3/120    avg_loss:2.661, val_acc:0.449]
Epoch [4/120    avg_loss:2.579, val_acc:0.456]
Epoch [5/120    avg_loss:2.493, val_acc:0.379]
Epoch [6/120    avg_loss:2.412, val_acc:0.401]
Epoch [7/120    avg_loss:2.347, val_acc:0.460]
Epoch [8/120    avg_loss:2.258, val_acc:0.496]
Epoch [9/120    avg_loss:2.165, val_acc:0.504]
Epoch [10/120    avg_loss:2.026, val_acc:0.509]
Epoch [11/120    avg_loss:1.968, val_acc:0.547]
Epoch [12/120    avg_loss:1.797, val_acc:0.548]
Epoch [13/120    avg_loss:1.732, val_acc:0.605]
Epoch [14/120    avg_loss:1.615, val_acc:0.611]
Epoch [15/120    avg_loss:1.501, val_acc:0.619]
Epoch [16/120    avg_loss:1.418, val_acc:0.658]
Epoch [17/120    avg_loss:1.358, val_acc:0.693]
Epoch [18/120    avg_loss:1.249, val_acc:0.710]
Epoch [19/120    avg_loss:1.143, val_acc:0.724]
Epoch [20/120    avg_loss:1.088, val_acc:0.758]
Epoch [21/120    avg_loss:0.996, val_acc:0.747]
Epoch [22/120    avg_loss:0.933, val_acc:0.820]
Epoch [23/120    avg_loss:0.857, val_acc:0.821]
Epoch [24/120    avg_loss:0.745, val_acc:0.853]
Epoch [25/120    avg_loss:0.709, val_acc:0.808]
Epoch [26/120    avg_loss:0.672, val_acc:0.850]
Epoch [27/120    avg_loss:0.614, val_acc:0.829]
Epoch [28/120    avg_loss:0.538, val_acc:0.855]
Epoch [29/120    avg_loss:0.483, val_acc:0.844]
Epoch [30/120    avg_loss:0.446, val_acc:0.878]
Epoch [31/120    avg_loss:0.394, val_acc:0.803]
Epoch [32/120    avg_loss:0.478, val_acc:0.868]
Epoch [33/120    avg_loss:0.451, val_acc:0.871]
Epoch [34/120    avg_loss:0.373, val_acc:0.871]
Epoch [35/120    avg_loss:0.331, val_acc:0.875]
Epoch [36/120    avg_loss:0.295, val_acc:0.911]
Epoch [37/120    avg_loss:0.246, val_acc:0.875]
Epoch [38/120    avg_loss:0.255, val_acc:0.923]
Epoch [39/120    avg_loss:0.251, val_acc:0.941]
Epoch [40/120    avg_loss:0.251, val_acc:0.896]
Epoch [41/120    avg_loss:0.206, val_acc:0.923]
Epoch [42/120    avg_loss:0.165, val_acc:0.933]
Epoch [43/120    avg_loss:0.162, val_acc:0.941]
Epoch [44/120    avg_loss:0.147, val_acc:0.950]
Epoch [45/120    avg_loss:0.136, val_acc:0.941]
Epoch [46/120    avg_loss:0.132, val_acc:0.956]
Epoch [47/120    avg_loss:0.141, val_acc:0.935]
Epoch [48/120    avg_loss:0.145, val_acc:0.902]
Epoch [49/120    avg_loss:0.143, val_acc:0.936]
Epoch [50/120    avg_loss:0.139, val_acc:0.934]
Epoch [51/120    avg_loss:0.116, val_acc:0.930]
Epoch [52/120    avg_loss:0.095, val_acc:0.947]
Epoch [53/120    avg_loss:0.102, val_acc:0.928]
Epoch [54/120    avg_loss:0.095, val_acc:0.951]
Epoch [55/120    avg_loss:0.084, val_acc:0.951]
Epoch [56/120    avg_loss:0.093, val_acc:0.957]
Epoch [57/120    avg_loss:0.075, val_acc:0.938]
Epoch [58/120    avg_loss:0.057, val_acc:0.958]
Epoch [59/120    avg_loss:0.072, val_acc:0.955]
Epoch [60/120    avg_loss:0.068, val_acc:0.930]
Epoch [61/120    avg_loss:0.107, val_acc:0.960]
Epoch [62/120    avg_loss:0.068, val_acc:0.956]
Epoch [63/120    avg_loss:0.055, val_acc:0.948]
Epoch [64/120    avg_loss:0.080, val_acc:0.954]
Epoch [65/120    avg_loss:0.061, val_acc:0.960]
Epoch [66/120    avg_loss:0.057, val_acc:0.961]
Epoch [67/120    avg_loss:0.047, val_acc:0.966]
Epoch [68/120    avg_loss:0.048, val_acc:0.954]
Epoch [69/120    avg_loss:0.059, val_acc:0.965]
Epoch [70/120    avg_loss:0.055, val_acc:0.961]
Epoch [71/120    avg_loss:0.049, val_acc:0.969]
Epoch [72/120    avg_loss:0.052, val_acc:0.953]
Epoch [73/120    avg_loss:0.040, val_acc:0.964]
Epoch [74/120    avg_loss:0.032, val_acc:0.974]
Epoch [75/120    avg_loss:0.034, val_acc:0.969]
Epoch [76/120    avg_loss:0.030, val_acc:0.974]
Epoch [77/120    avg_loss:0.022, val_acc:0.972]
Epoch [78/120    avg_loss:0.024, val_acc:0.977]
Epoch [79/120    avg_loss:0.033, val_acc:0.973]
Epoch [80/120    avg_loss:0.030, val_acc:0.959]
Epoch [81/120    avg_loss:0.028, val_acc:0.976]
Epoch [82/120    avg_loss:0.026, val_acc:0.972]
Epoch [83/120    avg_loss:0.029, val_acc:0.971]
Epoch [84/120    avg_loss:0.020, val_acc:0.970]
Epoch [85/120    avg_loss:0.021, val_acc:0.972]
Epoch [86/120    avg_loss:0.019, val_acc:0.975]
Epoch [87/120    avg_loss:0.020, val_acc:0.967]
Epoch [88/120    avg_loss:0.018, val_acc:0.976]
Epoch [89/120    avg_loss:0.020, val_acc:0.978]
Epoch [90/120    avg_loss:0.016, val_acc:0.982]
Epoch [91/120    avg_loss:0.019, val_acc:0.977]
Epoch [92/120    avg_loss:0.021, val_acc:0.982]
Epoch [93/120    avg_loss:0.018, val_acc:0.979]
Epoch [94/120    avg_loss:0.019, val_acc:0.973]
Epoch [95/120    avg_loss:0.019, val_acc:0.974]
Epoch [96/120    avg_loss:0.016, val_acc:0.972]
Epoch [97/120    avg_loss:0.012, val_acc:0.980]
Epoch [98/120    avg_loss:0.017, val_acc:0.971]
Epoch [99/120    avg_loss:0.015, val_acc:0.982]
Epoch [100/120    avg_loss:0.015, val_acc:0.975]
Epoch [101/120    avg_loss:0.015, val_acc:0.974]
Epoch [102/120    avg_loss:0.015, val_acc:0.977]
Epoch [103/120    avg_loss:0.019, val_acc:0.978]
Epoch [104/120    avg_loss:0.030, val_acc:0.970]
Epoch [105/120    avg_loss:0.051, val_acc:0.940]
Epoch [106/120    avg_loss:0.123, val_acc:0.931]
Epoch [107/120    avg_loss:0.064, val_acc:0.970]
Epoch [108/120    avg_loss:0.041, val_acc:0.961]
Epoch [109/120    avg_loss:0.064, val_acc:0.935]
Epoch [110/120    avg_loss:0.080, val_acc:0.946]
Epoch [111/120    avg_loss:0.064, val_acc:0.947]
Epoch [112/120    avg_loss:0.062, val_acc:0.964]
Epoch [113/120    avg_loss:0.027, val_acc:0.973]
Epoch [114/120    avg_loss:0.025, val_acc:0.970]
Epoch [115/120    avg_loss:0.025, val_acc:0.970]
Epoch [116/120    avg_loss:0.019, val_acc:0.974]
Epoch [117/120    avg_loss:0.020, val_acc:0.971]
Epoch [118/120    avg_loss:0.020, val_acc:0.974]
Epoch [119/120    avg_loss:0.017, val_acc:0.971]
Epoch [120/120    avg_loss:0.017, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    2    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0 1247    7    1    1    1    0    0    0    2   26    0    0
     0    0    0]
 [   0    0    0  730    6    1    0    0    0    0    0    7    1    2
     0    0    0]
 [   0    0    0    0  203   10    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    1  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    1    0    0    0    0  844   18    4    0
     0    1    0]
 [   0    0   10    0    0    0    9    0    0    0   34 2134   23    0
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    7  522    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1137    1    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    68  256    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.88888888888889

F1 scores:
[       nan 0.95       0.97842291 0.98382749 0.95981087 0.97732426
 0.97401633 0.98039216 0.997669   1.         0.96182336 0.96933909
 0.96221198 0.99462366 0.96848382 0.84349259 0.98823529]

Kappa:
0.9645224989592891
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fce4652af60>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.832, val_acc:0.172]
Epoch [2/120    avg_loss:2.772, val_acc:0.204]
Epoch [3/120    avg_loss:2.700, val_acc:0.202]
Epoch [4/120    avg_loss:2.620, val_acc:0.206]
Epoch [5/120    avg_loss:2.526, val_acc:0.344]
Epoch [6/120    avg_loss:2.438, val_acc:0.410]
Epoch [7/120    avg_loss:2.364, val_acc:0.465]
Epoch [8/120    avg_loss:2.265, val_acc:0.500]
Epoch [9/120    avg_loss:2.205, val_acc:0.504]
Epoch [10/120    avg_loss:2.046, val_acc:0.491]
Epoch [11/120    avg_loss:1.899, val_acc:0.511]
Epoch [12/120    avg_loss:1.769, val_acc:0.533]
Epoch [13/120    avg_loss:1.605, val_acc:0.606]
Epoch [14/120    avg_loss:1.461, val_acc:0.595]
Epoch [15/120    avg_loss:1.378, val_acc:0.645]
Epoch [16/120    avg_loss:1.181, val_acc:0.596]
Epoch [17/120    avg_loss:1.051, val_acc:0.667]
Epoch [18/120    avg_loss:0.942, val_acc:0.726]
Epoch [19/120    avg_loss:0.815, val_acc:0.759]
Epoch [20/120    avg_loss:0.743, val_acc:0.807]
Epoch [21/120    avg_loss:0.686, val_acc:0.810]
Epoch [22/120    avg_loss:0.568, val_acc:0.829]
Epoch [23/120    avg_loss:0.567, val_acc:0.816]
Epoch [24/120    avg_loss:0.467, val_acc:0.828]
Epoch [25/120    avg_loss:0.407, val_acc:0.832]
Epoch [26/120    avg_loss:0.356, val_acc:0.863]
Epoch [27/120    avg_loss:0.314, val_acc:0.870]
Epoch [28/120    avg_loss:0.241, val_acc:0.886]
Epoch [29/120    avg_loss:0.233, val_acc:0.883]
Epoch [30/120    avg_loss:0.248, val_acc:0.856]
Epoch [31/120    avg_loss:0.244, val_acc:0.892]
Epoch [32/120    avg_loss:0.237, val_acc:0.879]
Epoch [33/120    avg_loss:0.266, val_acc:0.883]
Epoch [34/120    avg_loss:0.205, val_acc:0.898]
Epoch [35/120    avg_loss:0.203, val_acc:0.861]
Epoch [36/120    avg_loss:0.218, val_acc:0.887]
Epoch [37/120    avg_loss:0.156, val_acc:0.908]
Epoch [38/120    avg_loss:0.154, val_acc:0.923]
Epoch [39/120    avg_loss:0.150, val_acc:0.920]
Epoch [40/120    avg_loss:0.113, val_acc:0.932]
Epoch [41/120    avg_loss:0.104, val_acc:0.933]
Epoch [42/120    avg_loss:0.089, val_acc:0.949]
Epoch [43/120    avg_loss:0.090, val_acc:0.942]
Epoch [44/120    avg_loss:0.100, val_acc:0.943]
Epoch [45/120    avg_loss:0.076, val_acc:0.953]
Epoch [46/120    avg_loss:0.064, val_acc:0.956]
Epoch [47/120    avg_loss:0.061, val_acc:0.960]
Epoch [48/120    avg_loss:0.066, val_acc:0.953]
Epoch [49/120    avg_loss:0.051, val_acc:0.950]
Epoch [50/120    avg_loss:0.057, val_acc:0.956]
Epoch [51/120    avg_loss:0.058, val_acc:0.952]
Epoch [52/120    avg_loss:0.052, val_acc:0.956]
Epoch [53/120    avg_loss:0.054, val_acc:0.946]
Epoch [54/120    avg_loss:0.044, val_acc:0.941]
Epoch [55/120    avg_loss:0.064, val_acc:0.963]
Epoch [56/120    avg_loss:0.047, val_acc:0.960]
Epoch [57/120    avg_loss:0.039, val_acc:0.966]
Epoch [58/120    avg_loss:0.044, val_acc:0.955]
Epoch [59/120    avg_loss:0.038, val_acc:0.967]
Epoch [60/120    avg_loss:0.052, val_acc:0.964]
Epoch [61/120    avg_loss:0.048, val_acc:0.956]
Epoch [62/120    avg_loss:0.050, val_acc:0.964]
Epoch [63/120    avg_loss:0.033, val_acc:0.974]
Epoch [64/120    avg_loss:0.033, val_acc:0.936]
Epoch [65/120    avg_loss:0.033, val_acc:0.959]
Epoch [66/120    avg_loss:0.025, val_acc:0.974]
Epoch [67/120    avg_loss:0.027, val_acc:0.960]
Epoch [68/120    avg_loss:0.023, val_acc:0.967]
Epoch [69/120    avg_loss:0.021, val_acc:0.965]
Epoch [70/120    avg_loss:0.021, val_acc:0.963]
Epoch [71/120    avg_loss:0.031, val_acc:0.968]
Epoch [72/120    avg_loss:0.026, val_acc:0.966]
Epoch [73/120    avg_loss:0.022, val_acc:0.961]
Epoch [74/120    avg_loss:0.022, val_acc:0.969]
Epoch [75/120    avg_loss:0.019, val_acc:0.973]
Epoch [76/120    avg_loss:0.018, val_acc:0.972]
Epoch [77/120    avg_loss:0.021, val_acc:0.970]
Epoch [78/120    avg_loss:0.020, val_acc:0.975]
Epoch [79/120    avg_loss:0.020, val_acc:0.974]
Epoch [80/120    avg_loss:0.016, val_acc:0.977]
Epoch [81/120    avg_loss:0.015, val_acc:0.979]
Epoch [82/120    avg_loss:0.021, val_acc:0.966]
Epoch [83/120    avg_loss:0.019, val_acc:0.976]
Epoch [84/120    avg_loss:0.012, val_acc:0.973]
Epoch [85/120    avg_loss:0.016, val_acc:0.972]
Epoch [86/120    avg_loss:0.015, val_acc:0.973]
Epoch [87/120    avg_loss:0.016, val_acc:0.973]
Epoch [88/120    avg_loss:0.014, val_acc:0.970]
Epoch [89/120    avg_loss:0.013, val_acc:0.977]
Epoch [90/120    avg_loss:0.013, val_acc:0.979]
Epoch [91/120    avg_loss:0.012, val_acc:0.974]
Epoch [92/120    avg_loss:0.011, val_acc:0.978]
Epoch [93/120    avg_loss:0.019, val_acc:0.973]
Epoch [94/120    avg_loss:0.014, val_acc:0.975]
Epoch [95/120    avg_loss:0.023, val_acc:0.972]
Epoch [96/120    avg_loss:0.017, val_acc:0.971]
Epoch [97/120    avg_loss:0.012, val_acc:0.976]
Epoch [98/120    avg_loss:0.011, val_acc:0.980]
Epoch [99/120    avg_loss:0.008, val_acc:0.980]
Epoch [100/120    avg_loss:0.008, val_acc:0.981]
Epoch [101/120    avg_loss:0.010, val_acc:0.976]
Epoch [102/120    avg_loss:0.009, val_acc:0.973]
Epoch [103/120    avg_loss:0.008, val_acc:0.976]
Epoch [104/120    avg_loss:0.009, val_acc:0.975]
Epoch [105/120    avg_loss:0.009, val_acc:0.974]
Epoch [106/120    avg_loss:0.010, val_acc:0.970]
Epoch [107/120    avg_loss:0.020, val_acc:0.972]
Epoch [108/120    avg_loss:0.014, val_acc:0.974]
Epoch [109/120    avg_loss:0.027, val_acc:0.967]
Epoch [110/120    avg_loss:0.010, val_acc:0.977]
Epoch [111/120    avg_loss:0.014, val_acc:0.966]
Epoch [112/120    avg_loss:0.020, val_acc:0.973]
Epoch [113/120    avg_loss:0.019, val_acc:0.982]
Epoch [114/120    avg_loss:0.015, val_acc:0.966]
Epoch [115/120    avg_loss:0.010, val_acc:0.971]
Epoch [116/120    avg_loss:0.010, val_acc:0.978]
Epoch [117/120    avg_loss:0.007, val_acc:0.974]
Epoch [118/120    avg_loss:0.012, val_acc:0.979]
Epoch [119/120    avg_loss:0.009, val_acc:0.978]
Epoch [120/120    avg_loss:0.011, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    2    0    1    0    0    0
     0    0    0]
 [   0    0 1229   19   12    0    0    0    0    0    6   19    0    0
     0    0    0]
 [   0    0    0  743    1    0    0    0    0    0    1    2    0    0
     0    0    0]
 [   0    0    0    2  209    0    0    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    0    0    0    0    0  842   25    0    0
     0    0    0]
 [   0    0   25    1    0    0    0    0    0    0   11 2162   10    0
     0    1    0]
 [   0    0    0    7    0    1    0    0    0    0    5    2  517    0
     0    2    0]
 [   0    0    0    1    0    1    0    0    0    0    0    0    0  183
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
   104  236    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.83468834688347

F1 scores:
[       nan 0.96202532 0.96505693 0.97763158 0.96091954 0.9954023
 0.99393939 0.98039216 0.99767981 1.         0.9672602  0.977617
 0.97455231 0.99456522 0.95109612 0.79061977 1.        ]

Kappa:
0.9638813425472716
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f94e9786e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.833, val_acc:0.204]
Epoch [2/120    avg_loss:2.765, val_acc:0.251]
Epoch [3/120    avg_loss:2.697, val_acc:0.388]
Epoch [4/120    avg_loss:2.608, val_acc:0.411]
Epoch [5/120    avg_loss:2.524, val_acc:0.421]
Epoch [6/120    avg_loss:2.429, val_acc:0.466]
Epoch [7/120    avg_loss:2.291, val_acc:0.484]
Epoch [8/120    avg_loss:2.184, val_acc:0.490]
Epoch [9/120    avg_loss:2.109, val_acc:0.525]
Epoch [10/120    avg_loss:1.958, val_acc:0.527]
Epoch [11/120    avg_loss:1.819, val_acc:0.579]
Epoch [12/120    avg_loss:1.694, val_acc:0.582]
Epoch [13/120    avg_loss:1.555, val_acc:0.586]
Epoch [14/120    avg_loss:1.454, val_acc:0.595]
Epoch [15/120    avg_loss:1.373, val_acc:0.681]
Epoch [16/120    avg_loss:1.184, val_acc:0.703]
Epoch [17/120    avg_loss:1.096, val_acc:0.734]
Epoch [18/120    avg_loss:0.968, val_acc:0.740]
Epoch [19/120    avg_loss:0.891, val_acc:0.762]
Epoch [20/120    avg_loss:0.836, val_acc:0.765]
Epoch [21/120    avg_loss:0.747, val_acc:0.738]
Epoch [22/120    avg_loss:0.683, val_acc:0.791]
Epoch [23/120    avg_loss:0.542, val_acc:0.847]
Epoch [24/120    avg_loss:0.525, val_acc:0.864]
Epoch [25/120    avg_loss:0.526, val_acc:0.841]
Epoch [26/120    avg_loss:0.428, val_acc:0.864]
Epoch [27/120    avg_loss:0.366, val_acc:0.880]
Epoch [28/120    avg_loss:0.323, val_acc:0.904]
Epoch [29/120    avg_loss:0.299, val_acc:0.905]
Epoch [30/120    avg_loss:0.254, val_acc:0.917]
Epoch [31/120    avg_loss:0.232, val_acc:0.916]
Epoch [32/120    avg_loss:0.210, val_acc:0.915]
Epoch [33/120    avg_loss:0.173, val_acc:0.935]
Epoch [34/120    avg_loss:0.173, val_acc:0.923]
Epoch [35/120    avg_loss:0.174, val_acc:0.926]
Epoch [36/120    avg_loss:0.184, val_acc:0.903]
Epoch [37/120    avg_loss:0.192, val_acc:0.915]
Epoch [38/120    avg_loss:0.179, val_acc:0.910]
Epoch [39/120    avg_loss:0.152, val_acc:0.920]
Epoch [40/120    avg_loss:0.143, val_acc:0.930]
Epoch [41/120    avg_loss:0.116, val_acc:0.946]
Epoch [42/120    avg_loss:0.094, val_acc:0.952]
Epoch [43/120    avg_loss:0.102, val_acc:0.934]
Epoch [44/120    avg_loss:0.109, val_acc:0.945]
Epoch [45/120    avg_loss:0.099, val_acc:0.921]
Epoch [46/120    avg_loss:0.098, val_acc:0.939]
Epoch [47/120    avg_loss:0.074, val_acc:0.944]
Epoch [48/120    avg_loss:0.061, val_acc:0.959]
Epoch [49/120    avg_loss:0.057, val_acc:0.949]
Epoch [50/120    avg_loss:0.068, val_acc:0.963]
Epoch [51/120    avg_loss:0.062, val_acc:0.954]
Epoch [52/120    avg_loss:0.061, val_acc:0.947]
Epoch [53/120    avg_loss:0.057, val_acc:0.953]
Epoch [54/120    avg_loss:0.044, val_acc:0.959]
Epoch [55/120    avg_loss:0.061, val_acc:0.939]
Epoch [56/120    avg_loss:0.072, val_acc:0.952]
Epoch [57/120    avg_loss:0.065, val_acc:0.961]
Epoch [58/120    avg_loss:0.050, val_acc:0.957]
Epoch [59/120    avg_loss:0.063, val_acc:0.945]
Epoch [60/120    avg_loss:0.059, val_acc:0.918]
Epoch [61/120    avg_loss:0.064, val_acc:0.930]
Epoch [62/120    avg_loss:0.076, val_acc:0.960]
Epoch [63/120    avg_loss:0.045, val_acc:0.965]
Epoch [64/120    avg_loss:0.038, val_acc:0.959]
Epoch [65/120    avg_loss:0.032, val_acc:0.958]
Epoch [66/120    avg_loss:0.032, val_acc:0.964]
Epoch [67/120    avg_loss:0.037, val_acc:0.961]
Epoch [68/120    avg_loss:0.027, val_acc:0.970]
Epoch [69/120    avg_loss:0.028, val_acc:0.963]
Epoch [70/120    avg_loss:0.040, val_acc:0.959]
Epoch [71/120    avg_loss:0.048, val_acc:0.932]
Epoch [72/120    avg_loss:0.046, val_acc:0.956]
Epoch [73/120    avg_loss:0.034, val_acc:0.957]
Epoch [74/120    avg_loss:0.029, val_acc:0.970]
Epoch [75/120    avg_loss:0.024, val_acc:0.966]
Epoch [76/120    avg_loss:0.019, val_acc:0.969]
Epoch [77/120    avg_loss:0.020, val_acc:0.958]
Epoch [78/120    avg_loss:0.018, val_acc:0.965]
Epoch [79/120    avg_loss:0.019, val_acc:0.966]
Epoch [80/120    avg_loss:0.018, val_acc:0.969]
Epoch [81/120    avg_loss:0.013, val_acc:0.968]
Epoch [82/120    avg_loss:0.016, val_acc:0.966]
Epoch [83/120    avg_loss:0.016, val_acc:0.965]
Epoch [84/120    avg_loss:0.020, val_acc:0.954]
Epoch [85/120    avg_loss:0.025, val_acc:0.963]
Epoch [86/120    avg_loss:0.024, val_acc:0.964]
Epoch [87/120    avg_loss:0.017, val_acc:0.975]
Epoch [88/120    avg_loss:0.019, val_acc:0.974]
Epoch [89/120    avg_loss:0.018, val_acc:0.973]
Epoch [90/120    avg_loss:0.015, val_acc:0.966]
Epoch [91/120    avg_loss:0.011, val_acc:0.969]
Epoch [92/120    avg_loss:0.011, val_acc:0.970]
Epoch [93/120    avg_loss:0.011, val_acc:0.967]
Epoch [94/120    avg_loss:0.016, val_acc:0.957]
Epoch [95/120    avg_loss:0.015, val_acc:0.973]
Epoch [96/120    avg_loss:0.012, val_acc:0.966]
Epoch [97/120    avg_loss:0.016, val_acc:0.967]
Epoch [98/120    avg_loss:0.013, val_acc:0.972]
Epoch [99/120    avg_loss:0.014, val_acc:0.954]
Epoch [100/120    avg_loss:0.025, val_acc:0.969]
Epoch [101/120    avg_loss:0.017, val_acc:0.973]
Epoch [102/120    avg_loss:0.012, val_acc:0.973]
Epoch [103/120    avg_loss:0.011, val_acc:0.975]
Epoch [104/120    avg_loss:0.011, val_acc:0.975]
Epoch [105/120    avg_loss:0.010, val_acc:0.974]
Epoch [106/120    avg_loss:0.013, val_acc:0.976]
Epoch [107/120    avg_loss:0.008, val_acc:0.976]
Epoch [108/120    avg_loss:0.011, val_acc:0.977]
Epoch [109/120    avg_loss:0.010, val_acc:0.977]
Epoch [110/120    avg_loss:0.011, val_acc:0.976]
Epoch [111/120    avg_loss:0.010, val_acc:0.977]
Epoch [112/120    avg_loss:0.008, val_acc:0.977]
Epoch [113/120    avg_loss:0.010, val_acc:0.977]
Epoch [114/120    avg_loss:0.008, val_acc:0.977]
Epoch [115/120    avg_loss:0.009, val_acc:0.977]
Epoch [116/120    avg_loss:0.009, val_acc:0.978]
Epoch [117/120    avg_loss:0.009, val_acc:0.978]
Epoch [118/120    avg_loss:0.008, val_acc:0.977]
Epoch [119/120    avg_loss:0.008, val_acc:0.977]
Epoch [120/120    avg_loss:0.007, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1261    3    6    0    0    0    0    0    0   15    0    0
     0    0    0]
 [   0    0    1  721    0    1    0    0    0    1    0   19    4    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  426    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    1    0    0    0    0  838   32    0    0
     0    0    0]
 [   0    0   14    0    0    0    0    0    0    0   20 2166   10    0
     0    0    0]
 [   0    0    0    6    0    1    3    0    0    0    5    4  514    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1128   10    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    47  286    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.44173441734418

F1 scores:
[       nan 0.95121951 0.98285269 0.97630332 0.9837587  0.99078341
 0.98572502 1.         0.9953271  0.97297297 0.96377228 0.97435897
 0.96525822 1.         0.97241379 0.88958009 0.99408284]

Kappa:
0.970806744766994
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f171880eef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.816, val_acc:0.210]
Epoch [2/120    avg_loss:2.743, val_acc:0.193]
Epoch [3/120    avg_loss:2.681, val_acc:0.263]
Epoch [4/120    avg_loss:2.605, val_acc:0.310]
Epoch [5/120    avg_loss:2.520, val_acc:0.357]
Epoch [6/120    avg_loss:2.441, val_acc:0.401]
Epoch [7/120    avg_loss:2.355, val_acc:0.443]
Epoch [8/120    avg_loss:2.226, val_acc:0.468]
Epoch [9/120    avg_loss:2.137, val_acc:0.500]
Epoch [10/120    avg_loss:2.023, val_acc:0.511]
Epoch [11/120    avg_loss:1.891, val_acc:0.535]
Epoch [12/120    avg_loss:1.754, val_acc:0.546]
Epoch [13/120    avg_loss:1.638, val_acc:0.604]
Epoch [14/120    avg_loss:1.575, val_acc:0.589]
Epoch [15/120    avg_loss:1.498, val_acc:0.654]
Epoch [16/120    avg_loss:1.387, val_acc:0.669]
Epoch [17/120    avg_loss:1.257, val_acc:0.740]
Epoch [18/120    avg_loss:1.127, val_acc:0.734]
Epoch [19/120    avg_loss:1.090, val_acc:0.657]
Epoch [20/120    avg_loss:0.960, val_acc:0.774]
Epoch [21/120    avg_loss:0.815, val_acc:0.805]
Epoch [22/120    avg_loss:0.690, val_acc:0.796]
Epoch [23/120    avg_loss:0.631, val_acc:0.797]
Epoch [24/120    avg_loss:0.678, val_acc:0.826]
Epoch [25/120    avg_loss:0.510, val_acc:0.834]
Epoch [26/120    avg_loss:0.490, val_acc:0.863]
Epoch [27/120    avg_loss:0.428, val_acc:0.835]
Epoch [28/120    avg_loss:0.336, val_acc:0.875]
Epoch [29/120    avg_loss:0.426, val_acc:0.869]
Epoch [30/120    avg_loss:0.329, val_acc:0.891]
Epoch [31/120    avg_loss:0.251, val_acc:0.877]
Epoch [32/120    avg_loss:0.222, val_acc:0.884]
Epoch [33/120    avg_loss:0.206, val_acc:0.912]
Epoch [34/120    avg_loss:0.200, val_acc:0.898]
Epoch [35/120    avg_loss:0.374, val_acc:0.870]
Epoch [36/120    avg_loss:0.242, val_acc:0.886]
Epoch [37/120    avg_loss:0.202, val_acc:0.804]
Epoch [38/120    avg_loss:0.208, val_acc:0.877]
Epoch [39/120    avg_loss:0.180, val_acc:0.906]
Epoch [40/120    avg_loss:0.163, val_acc:0.894]
Epoch [41/120    avg_loss:0.145, val_acc:0.917]
Epoch [42/120    avg_loss:0.111, val_acc:0.936]
Epoch [43/120    avg_loss:0.115, val_acc:0.920]
Epoch [44/120    avg_loss:0.123, val_acc:0.924]
Epoch [45/120    avg_loss:0.119, val_acc:0.943]
Epoch [46/120    avg_loss:0.095, val_acc:0.930]
Epoch [47/120    avg_loss:0.067, val_acc:0.950]
Epoch [48/120    avg_loss:0.064, val_acc:0.940]
Epoch [49/120    avg_loss:0.078, val_acc:0.911]
Epoch [50/120    avg_loss:1.114, val_acc:0.806]
Epoch [51/120    avg_loss:0.325, val_acc:0.882]
Epoch [52/120    avg_loss:0.226, val_acc:0.897]
Epoch [53/120    avg_loss:0.194, val_acc:0.920]
Epoch [54/120    avg_loss:0.172, val_acc:0.900]
Epoch [55/120    avg_loss:0.136, val_acc:0.907]
Epoch [56/120    avg_loss:0.132, val_acc:0.938]
Epoch [57/120    avg_loss:0.099, val_acc:0.945]
Epoch [58/120    avg_loss:0.088, val_acc:0.955]
Epoch [59/120    avg_loss:0.080, val_acc:0.930]
Epoch [60/120    avg_loss:0.088, val_acc:0.961]
Epoch [61/120    avg_loss:0.057, val_acc:0.949]
Epoch [62/120    avg_loss:0.057, val_acc:0.948]
Epoch [63/120    avg_loss:0.082, val_acc:0.941]
Epoch [64/120    avg_loss:0.090, val_acc:0.946]
Epoch [65/120    avg_loss:0.054, val_acc:0.961]
Epoch [66/120    avg_loss:0.055, val_acc:0.956]
Epoch [67/120    avg_loss:0.048, val_acc:0.961]
Epoch [68/120    avg_loss:0.045, val_acc:0.957]
Epoch [69/120    avg_loss:0.048, val_acc:0.956]
Epoch [70/120    avg_loss:0.048, val_acc:0.952]
Epoch [71/120    avg_loss:0.042, val_acc:0.963]
Epoch [72/120    avg_loss:0.037, val_acc:0.970]
Epoch [73/120    avg_loss:0.039, val_acc:0.966]
Epoch [74/120    avg_loss:0.034, val_acc:0.966]
Epoch [75/120    avg_loss:0.035, val_acc:0.964]
Epoch [76/120    avg_loss:0.030, val_acc:0.971]
Epoch [77/120    avg_loss:0.023, val_acc:0.969]
Epoch [78/120    avg_loss:0.019, val_acc:0.972]
Epoch [79/120    avg_loss:0.019, val_acc:0.976]
Epoch [80/120    avg_loss:0.016, val_acc:0.974]
Epoch [81/120    avg_loss:0.019, val_acc:0.961]
Epoch [82/120    avg_loss:0.031, val_acc:0.971]
Epoch [83/120    avg_loss:0.022, val_acc:0.973]
Epoch [84/120    avg_loss:0.015, val_acc:0.972]
Epoch [85/120    avg_loss:0.018, val_acc:0.967]
Epoch [86/120    avg_loss:0.025, val_acc:0.974]
Epoch [87/120    avg_loss:0.024, val_acc:0.971]
Epoch [88/120    avg_loss:0.017, val_acc:0.970]
Epoch [89/120    avg_loss:0.015, val_acc:0.972]
Epoch [90/120    avg_loss:0.012, val_acc:0.975]
Epoch [91/120    avg_loss:0.020, val_acc:0.972]
Epoch [92/120    avg_loss:0.017, val_acc:0.970]
Epoch [93/120    avg_loss:0.010, val_acc:0.974]
Epoch [94/120    avg_loss:0.012, val_acc:0.977]
Epoch [95/120    avg_loss:0.009, val_acc:0.975]
Epoch [96/120    avg_loss:0.010, val_acc:0.977]
Epoch [97/120    avg_loss:0.011, val_acc:0.976]
Epoch [98/120    avg_loss:0.009, val_acc:0.976]
Epoch [99/120    avg_loss:0.010, val_acc:0.973]
Epoch [100/120    avg_loss:0.009, val_acc:0.974]
Epoch [101/120    avg_loss:0.009, val_acc:0.974]
Epoch [102/120    avg_loss:0.009, val_acc:0.974]
Epoch [103/120    avg_loss:0.008, val_acc:0.975]
Epoch [104/120    avg_loss:0.009, val_acc:0.974]
Epoch [105/120    avg_loss:0.010, val_acc:0.973]
Epoch [106/120    avg_loss:0.009, val_acc:0.974]
Epoch [107/120    avg_loss:0.010, val_acc:0.974]
Epoch [108/120    avg_loss:0.009, val_acc:0.976]
Epoch [109/120    avg_loss:0.009, val_acc:0.975]
Epoch [110/120    avg_loss:0.009, val_acc:0.975]
Epoch [111/120    avg_loss:0.009, val_acc:0.975]
Epoch [112/120    avg_loss:0.008, val_acc:0.975]
Epoch [113/120    avg_loss:0.008, val_acc:0.975]
Epoch [114/120    avg_loss:0.008, val_acc:0.974]
Epoch [115/120    avg_loss:0.009, val_acc:0.975]
Epoch [116/120    avg_loss:0.009, val_acc:0.975]
Epoch [117/120    avg_loss:0.009, val_acc:0.975]
Epoch [118/120    avg_loss:0.010, val_acc:0.974]
Epoch [119/120    avg_loss:0.010, val_acc:0.974]
Epoch [120/120    avg_loss:0.008, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1248    9    1    0    0    0    0    0    3   24    0    0
     0    0    0]
 [   0    0    0  727    8    0    1    0    0    2    1    8    0    0
     0    0    0]
 [   0    0    0    1  210    0    0    0    0    0    0    1    1    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  849   21    0    0
     0    0    0]
 [   0    0    5    0    0    0    4    0    0    1   22 2154   15    0
     0    9    0]
 [   0    0    0    1    0    0    0    0    0    0    4    2  523    0
     0    3    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1128   10    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    66  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.39837398373983

F1 scores:
[       nan 0.975      0.98113208 0.97912458 0.97222222 0.99884925
 0.99394856 1.         1.         0.87804878 0.96752137 0.97466063
 0.97121634 1.         0.96699529 0.85935085 0.96969697]

Kappa:
0.9703306591751626
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff2d9e64f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.802, val_acc:0.268]
Epoch [2/120    avg_loss:2.752, val_acc:0.360]
Epoch [3/120    avg_loss:2.693, val_acc:0.422]
Epoch [4/120    avg_loss:2.630, val_acc:0.542]
Epoch [5/120    avg_loss:2.523, val_acc:0.553]
Epoch [6/120    avg_loss:2.416, val_acc:0.557]
Epoch [7/120    avg_loss:2.332, val_acc:0.546]
Epoch [8/120    avg_loss:2.308, val_acc:0.524]
Epoch [9/120    avg_loss:2.181, val_acc:0.559]
Epoch [10/120    avg_loss:2.102, val_acc:0.548]
Epoch [11/120    avg_loss:2.002, val_acc:0.572]
Epoch [12/120    avg_loss:1.892, val_acc:0.621]
Epoch [13/120    avg_loss:1.748, val_acc:0.554]
Epoch [14/120    avg_loss:1.637, val_acc:0.637]
Epoch [15/120    avg_loss:1.517, val_acc:0.623]
Epoch [16/120    avg_loss:1.357, val_acc:0.733]
Epoch [17/120    avg_loss:1.252, val_acc:0.706]
Epoch [18/120    avg_loss:1.155, val_acc:0.690]
Epoch [19/120    avg_loss:0.995, val_acc:0.757]
Epoch [20/120    avg_loss:0.857, val_acc:0.768]
Epoch [21/120    avg_loss:0.822, val_acc:0.794]
Epoch [22/120    avg_loss:0.726, val_acc:0.810]
Epoch [23/120    avg_loss:0.618, val_acc:0.829]
Epoch [24/120    avg_loss:0.600, val_acc:0.821]
Epoch [25/120    avg_loss:0.521, val_acc:0.886]
Epoch [26/120    avg_loss:0.420, val_acc:0.878]
Epoch [27/120    avg_loss:0.432, val_acc:0.868]
Epoch [28/120    avg_loss:0.369, val_acc:0.854]
Epoch [29/120    avg_loss:0.369, val_acc:0.887]
Epoch [30/120    avg_loss:0.308, val_acc:0.907]
Epoch [31/120    avg_loss:0.306, val_acc:0.908]
Epoch [32/120    avg_loss:0.437, val_acc:0.901]
Epoch [33/120    avg_loss:0.292, val_acc:0.913]
Epoch [34/120    avg_loss:0.225, val_acc:0.938]
Epoch [35/120    avg_loss:0.251, val_acc:0.864]
Epoch [36/120    avg_loss:0.238, val_acc:0.928]
Epoch [37/120    avg_loss:0.174, val_acc:0.925]
Epoch [38/120    avg_loss:0.213, val_acc:0.940]
Epoch [39/120    avg_loss:0.143, val_acc:0.942]
Epoch [40/120    avg_loss:0.137, val_acc:0.944]
Epoch [41/120    avg_loss:0.103, val_acc:0.956]
Epoch [42/120    avg_loss:0.087, val_acc:0.968]
Epoch [43/120    avg_loss:0.092, val_acc:0.960]
Epoch [44/120    avg_loss:0.082, val_acc:0.953]
Epoch [45/120    avg_loss:0.072, val_acc:0.959]
Epoch [46/120    avg_loss:0.082, val_acc:0.955]
Epoch [47/120    avg_loss:0.068, val_acc:0.965]
Epoch [48/120    avg_loss:0.062, val_acc:0.967]
Epoch [49/120    avg_loss:0.125, val_acc:0.953]
Epoch [50/120    avg_loss:0.125, val_acc:0.940]
Epoch [51/120    avg_loss:0.091, val_acc:0.952]
Epoch [52/120    avg_loss:0.079, val_acc:0.958]
Epoch [53/120    avg_loss:0.062, val_acc:0.972]
Epoch [54/120    avg_loss:0.081, val_acc:0.951]
Epoch [55/120    avg_loss:0.072, val_acc:0.951]
Epoch [56/120    avg_loss:0.064, val_acc:0.956]
Epoch [57/120    avg_loss:0.049, val_acc:0.971]
Epoch [58/120    avg_loss:0.043, val_acc:0.971]
Epoch [59/120    avg_loss:0.044, val_acc:0.974]
Epoch [60/120    avg_loss:0.041, val_acc:0.970]
Epoch [61/120    avg_loss:0.039, val_acc:0.973]
Epoch [62/120    avg_loss:0.032, val_acc:0.965]
Epoch [63/120    avg_loss:0.049, val_acc:0.955]
Epoch [64/120    avg_loss:0.042, val_acc:0.973]
Epoch [65/120    avg_loss:0.031, val_acc:0.971]
Epoch [66/120    avg_loss:0.023, val_acc:0.979]
Epoch [67/120    avg_loss:0.022, val_acc:0.976]
Epoch [68/120    avg_loss:0.027, val_acc:0.970]
Epoch [69/120    avg_loss:0.030, val_acc:0.978]
Epoch [70/120    avg_loss:0.022, val_acc:0.976]
Epoch [71/120    avg_loss:0.037, val_acc:0.972]
Epoch [72/120    avg_loss:0.025, val_acc:0.979]
Epoch [73/120    avg_loss:0.021, val_acc:0.974]
Epoch [74/120    avg_loss:0.033, val_acc:0.962]
Epoch [75/120    avg_loss:0.039, val_acc:0.965]
Epoch [76/120    avg_loss:0.024, val_acc:0.975]
Epoch [77/120    avg_loss:0.018, val_acc:0.976]
Epoch [78/120    avg_loss:0.021, val_acc:0.966]
Epoch [79/120    avg_loss:0.030, val_acc:0.972]
Epoch [80/120    avg_loss:0.023, val_acc:0.973]
Epoch [81/120    avg_loss:0.015, val_acc:0.979]
Epoch [82/120    avg_loss:0.015, val_acc:0.978]
Epoch [83/120    avg_loss:0.014, val_acc:0.975]
Epoch [84/120    avg_loss:0.017, val_acc:0.982]
Epoch [85/120    avg_loss:0.012, val_acc:0.980]
Epoch [86/120    avg_loss:0.012, val_acc:0.981]
Epoch [87/120    avg_loss:0.014, val_acc:0.979]
Epoch [88/120    avg_loss:0.014, val_acc:0.976]
Epoch [89/120    avg_loss:0.022, val_acc:0.981]
Epoch [90/120    avg_loss:0.013, val_acc:0.978]
Epoch [91/120    avg_loss:0.013, val_acc:0.981]
Epoch [92/120    avg_loss:0.011, val_acc:0.975]
Epoch [93/120    avg_loss:0.011, val_acc:0.980]
Epoch [94/120    avg_loss:0.017, val_acc:0.980]
Epoch [95/120    avg_loss:0.014, val_acc:0.981]
Epoch [96/120    avg_loss:0.012, val_acc:0.983]
Epoch [97/120    avg_loss:0.017, val_acc:0.958]
Epoch [98/120    avg_loss:0.028, val_acc:0.970]
Epoch [99/120    avg_loss:0.022, val_acc:0.984]
Epoch [100/120    avg_loss:0.017, val_acc:0.973]
Epoch [101/120    avg_loss:0.016, val_acc:0.981]
Epoch [102/120    avg_loss:0.013, val_acc:0.980]
Epoch [103/120    avg_loss:0.012, val_acc:0.981]
Epoch [104/120    avg_loss:0.010, val_acc:0.978]
Epoch [105/120    avg_loss:0.012, val_acc:0.980]
Epoch [106/120    avg_loss:0.019, val_acc:0.970]
Epoch [107/120    avg_loss:0.023, val_acc:0.971]
Epoch [108/120    avg_loss:0.019, val_acc:0.975]
Epoch [109/120    avg_loss:0.013, val_acc:0.974]
Epoch [110/120    avg_loss:0.014, val_acc:0.981]
Epoch [111/120    avg_loss:0.010, val_acc:0.979]
Epoch [112/120    avg_loss:0.012, val_acc:0.981]
Epoch [113/120    avg_loss:0.008, val_acc:0.982]
Epoch [114/120    avg_loss:0.009, val_acc:0.981]
Epoch [115/120    avg_loss:0.008, val_acc:0.981]
Epoch [116/120    avg_loss:0.008, val_acc:0.980]
Epoch [117/120    avg_loss:0.007, val_acc:0.982]
Epoch [118/120    avg_loss:0.008, val_acc:0.984]
Epoch [119/120    avg_loss:0.008, val_acc:0.981]
Epoch [120/120    avg_loss:0.006, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1256    5    3    0    0    0    0    0    2   19    0    0
     0    0    0]
 [   0    0    5  717    0    6    1    0    0    6    2    1    3    6
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    3    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    3    0    0    0    1    0    0    0  837   32    0    0
     1    1    0]
 [   0    0    2    0    0    0    0    4    0    0    9 2178   16    1
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    1    6    7  512    0
     2    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    1    0    0    0    0    0    0
  1132    6    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    22  309    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.73441734417344

F1 scores:
[       nan 1.         0.98471188 0.97418478 0.99300699 0.98853211
 0.98419865 0.86206897 0.99649942 0.79069767 0.96707106 0.9788764
 0.95700935 0.98143236 0.98563343 0.9321267  0.9704142 ]

Kappa:
0.9741590297291817
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3cc3e3ef60>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.816, val_acc:0.170]
Epoch [2/120    avg_loss:2.753, val_acc:0.317]
Epoch [3/120    avg_loss:2.682, val_acc:0.377]
Epoch [4/120    avg_loss:2.611, val_acc:0.407]
Epoch [5/120    avg_loss:2.533, val_acc:0.420]
Epoch [6/120    avg_loss:2.454, val_acc:0.446]
Epoch [7/120    avg_loss:2.395, val_acc:0.460]
Epoch [8/120    avg_loss:2.298, val_acc:0.475]
Epoch [9/120    avg_loss:2.209, val_acc:0.505]
Epoch [10/120    avg_loss:2.140, val_acc:0.514]
Epoch [11/120    avg_loss:2.119, val_acc:0.529]
Epoch [12/120    avg_loss:1.982, val_acc:0.606]
Epoch [13/120    avg_loss:1.888, val_acc:0.609]
Epoch [14/120    avg_loss:1.793, val_acc:0.626]
Epoch [15/120    avg_loss:1.658, val_acc:0.598]
Epoch [16/120    avg_loss:1.559, val_acc:0.713]
Epoch [17/120    avg_loss:1.420, val_acc:0.724]
Epoch [18/120    avg_loss:1.293, val_acc:0.741]
Epoch [19/120    avg_loss:1.145, val_acc:0.761]
Epoch [20/120    avg_loss:1.101, val_acc:0.765]
Epoch [21/120    avg_loss:0.905, val_acc:0.809]
Epoch [22/120    avg_loss:0.799, val_acc:0.843]
Epoch [23/120    avg_loss:0.839, val_acc:0.797]
Epoch [24/120    avg_loss:0.732, val_acc:0.833]
Epoch [25/120    avg_loss:0.586, val_acc:0.842]
Epoch [26/120    avg_loss:0.528, val_acc:0.850]
Epoch [27/120    avg_loss:0.495, val_acc:0.861]
Epoch [28/120    avg_loss:0.455, val_acc:0.864]
Epoch [29/120    avg_loss:0.606, val_acc:0.838]
Epoch [30/120    avg_loss:0.415, val_acc:0.885]
Epoch [31/120    avg_loss:0.330, val_acc:0.886]
Epoch [32/120    avg_loss:0.286, val_acc:0.891]
Epoch [33/120    avg_loss:0.271, val_acc:0.897]
Epoch [34/120    avg_loss:0.248, val_acc:0.894]
Epoch [35/120    avg_loss:0.246, val_acc:0.917]
Epoch [36/120    avg_loss:0.192, val_acc:0.917]
Epoch [37/120    avg_loss:0.161, val_acc:0.910]
Epoch [38/120    avg_loss:0.147, val_acc:0.933]
Epoch [39/120    avg_loss:0.135, val_acc:0.936]
Epoch [40/120    avg_loss:0.125, val_acc:0.949]
Epoch [41/120    avg_loss:0.129, val_acc:0.948]
Epoch [42/120    avg_loss:0.119, val_acc:0.942]
Epoch [43/120    avg_loss:0.088, val_acc:0.952]
Epoch [44/120    avg_loss:0.101, val_acc:0.916]
Epoch [45/120    avg_loss:0.096, val_acc:0.945]
Epoch [46/120    avg_loss:0.080, val_acc:0.952]
Epoch [47/120    avg_loss:0.081, val_acc:0.958]
Epoch [48/120    avg_loss:0.065, val_acc:0.959]
Epoch [49/120    avg_loss:0.086, val_acc:0.953]
Epoch [50/120    avg_loss:0.067, val_acc:0.960]
Epoch [51/120    avg_loss:0.062, val_acc:0.954]
Epoch [52/120    avg_loss:0.061, val_acc:0.955]
Epoch [53/120    avg_loss:0.065, val_acc:0.952]
Epoch [54/120    avg_loss:0.058, val_acc:0.950]
Epoch [55/120    avg_loss:0.052, val_acc:0.957]
Epoch [56/120    avg_loss:0.060, val_acc:0.973]
Epoch [57/120    avg_loss:0.052, val_acc:0.968]
Epoch [58/120    avg_loss:0.037, val_acc:0.972]
Epoch [59/120    avg_loss:0.048, val_acc:0.964]
Epoch [60/120    avg_loss:0.044, val_acc:0.960]
Epoch [61/120    avg_loss:0.051, val_acc:0.970]
Epoch [62/120    avg_loss:0.060, val_acc:0.970]
Epoch [63/120    avg_loss:0.045, val_acc:0.957]
Epoch [64/120    avg_loss:0.073, val_acc:0.951]
Epoch [65/120    avg_loss:0.046, val_acc:0.948]
Epoch [66/120    avg_loss:0.038, val_acc:0.975]
Epoch [67/120    avg_loss:0.071, val_acc:0.961]
Epoch [68/120    avg_loss:0.061, val_acc:0.968]
Epoch [69/120    avg_loss:0.034, val_acc:0.949]
Epoch [70/120    avg_loss:0.034, val_acc:0.964]
Epoch [71/120    avg_loss:0.048, val_acc:0.974]
Epoch [72/120    avg_loss:0.040, val_acc:0.976]
Epoch [73/120    avg_loss:0.029, val_acc:0.969]
Epoch [74/120    avg_loss:0.027, val_acc:0.974]
Epoch [75/120    avg_loss:0.028, val_acc:0.973]
Epoch [76/120    avg_loss:0.022, val_acc:0.976]
Epoch [77/120    avg_loss:0.029, val_acc:0.981]
Epoch [78/120    avg_loss:0.020, val_acc:0.974]
Epoch [79/120    avg_loss:0.032, val_acc:0.967]
Epoch [80/120    avg_loss:0.022, val_acc:0.971]
Epoch [81/120    avg_loss:0.023, val_acc:0.972]
Epoch [82/120    avg_loss:0.027, val_acc:0.973]
Epoch [83/120    avg_loss:0.019, val_acc:0.975]
Epoch [84/120    avg_loss:0.017, val_acc:0.973]
Epoch [85/120    avg_loss:0.017, val_acc:0.979]
Epoch [86/120    avg_loss:0.028, val_acc:0.964]
Epoch [87/120    avg_loss:0.020, val_acc:0.971]
Epoch [88/120    avg_loss:0.016, val_acc:0.977]
Epoch [89/120    avg_loss:0.020, val_acc:0.979]
Epoch [90/120    avg_loss:0.015, val_acc:0.981]
Epoch [91/120    avg_loss:0.012, val_acc:0.975]
Epoch [92/120    avg_loss:0.011, val_acc:0.979]
Epoch [93/120    avg_loss:0.011, val_acc:0.980]
Epoch [94/120    avg_loss:0.016, val_acc:0.980]
Epoch [95/120    avg_loss:0.017, val_acc:0.976]
Epoch [96/120    avg_loss:0.014, val_acc:0.972]
Epoch [97/120    avg_loss:0.017, val_acc:0.977]
Epoch [98/120    avg_loss:0.016, val_acc:0.977]
Epoch [99/120    avg_loss:0.015, val_acc:0.974]
Epoch [100/120    avg_loss:0.015, val_acc:0.974]
Epoch [101/120    avg_loss:0.010, val_acc:0.980]
Epoch [102/120    avg_loss:0.012, val_acc:0.970]
Epoch [103/120    avg_loss:0.012, val_acc:0.982]
Epoch [104/120    avg_loss:0.050, val_acc:0.955]
Epoch [105/120    avg_loss:0.050, val_acc:0.970]
Epoch [106/120    avg_loss:0.030, val_acc:0.972]
Epoch [107/120    avg_loss:0.028, val_acc:0.969]
Epoch [108/120    avg_loss:0.014, val_acc:0.977]
Epoch [109/120    avg_loss:0.011, val_acc:0.979]
Epoch [110/120    avg_loss:0.009, val_acc:0.977]
Epoch [111/120    avg_loss:0.020, val_acc:0.968]
Epoch [112/120    avg_loss:0.014, val_acc:0.974]
Epoch [113/120    avg_loss:0.026, val_acc:0.971]
Epoch [114/120    avg_loss:0.023, val_acc:0.959]
Epoch [115/120    avg_loss:0.085, val_acc:0.916]
Epoch [116/120    avg_loss:0.134, val_acc:0.948]
Epoch [117/120    avg_loss:0.074, val_acc:0.961]
Epoch [118/120    avg_loss:0.036, val_acc:0.969]
Epoch [119/120    avg_loss:0.036, val_acc:0.972]
Epoch [120/120    avg_loss:0.025, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1253    0    3    0    0    0    0    0    1   26    0    0
     0    2    0]
 [   0    0    2  732    0    0    0    0    0    1    4    1    7    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   13    0    0    0    0    0    0  417    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   20    4    0    0    2    0    0    0  829   17    0    0
     0    3    0]
 [   0    0   10    0    0    1    0    0    0    1   14 2162   21    0
     0    1    0]
 [   0    0    4    0    0    2    0    0    0    1    3    6  512    0
     1    3    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    75  267    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.00813008130082

F1 scores:
[       nan 0.86315789 0.97358197 0.98585859 0.9882904  0.9908046
 0.99164768 1.         0.98465171 0.92307692 0.96060255 0.977617
 0.95344507 0.99728997 0.96088435 0.84493671 0.98823529]

Kappa:
0.9658683896849424
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6925b75f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.829, val_acc:0.054]
Epoch [2/120    avg_loss:2.774, val_acc:0.170]
Epoch [3/120    avg_loss:2.731, val_acc:0.272]
Epoch [4/120    avg_loss:2.643, val_acc:0.303]
Epoch [5/120    avg_loss:2.547, val_acc:0.317]
Epoch [6/120    avg_loss:2.481, val_acc:0.324]
Epoch [7/120    avg_loss:2.382, val_acc:0.319]
Epoch [8/120    avg_loss:2.307, val_acc:0.334]
Epoch [9/120    avg_loss:2.269, val_acc:0.353]
Epoch [10/120    avg_loss:2.163, val_acc:0.391]
Epoch [11/120    avg_loss:2.052, val_acc:0.454]
Epoch [12/120    avg_loss:1.938, val_acc:0.440]
Epoch [13/120    avg_loss:1.833, val_acc:0.488]
Epoch [14/120    avg_loss:1.730, val_acc:0.530]
Epoch [15/120    avg_loss:1.602, val_acc:0.565]
Epoch [16/120    avg_loss:1.524, val_acc:0.599]
Epoch [17/120    avg_loss:1.343, val_acc:0.662]
Epoch [18/120    avg_loss:1.236, val_acc:0.671]
Epoch [19/120    avg_loss:1.080, val_acc:0.732]
Epoch [20/120    avg_loss:0.997, val_acc:0.746]
Epoch [21/120    avg_loss:0.902, val_acc:0.762]
Epoch [22/120    avg_loss:0.793, val_acc:0.768]
Epoch [23/120    avg_loss:0.711, val_acc:0.731]
Epoch [24/120    avg_loss:0.624, val_acc:0.827]
Epoch [25/120    avg_loss:0.583, val_acc:0.830]
Epoch [26/120    avg_loss:0.489, val_acc:0.850]
Epoch [27/120    avg_loss:0.453, val_acc:0.850]
Epoch [28/120    avg_loss:0.428, val_acc:0.875]
Epoch [29/120    avg_loss:0.354, val_acc:0.837]
Epoch [30/120    avg_loss:0.320, val_acc:0.880]
Epoch [31/120    avg_loss:0.310, val_acc:0.892]
Epoch [32/120    avg_loss:0.266, val_acc:0.911]
Epoch [33/120    avg_loss:0.248, val_acc:0.913]
Epoch [34/120    avg_loss:0.215, val_acc:0.926]
Epoch [35/120    avg_loss:0.173, val_acc:0.934]
Epoch [36/120    avg_loss:0.175, val_acc:0.928]
Epoch [37/120    avg_loss:0.158, val_acc:0.925]
Epoch [38/120    avg_loss:0.146, val_acc:0.947]
Epoch [39/120    avg_loss:0.137, val_acc:0.931]
Epoch [40/120    avg_loss:0.159, val_acc:0.906]
Epoch [41/120    avg_loss:0.161, val_acc:0.916]
Epoch [42/120    avg_loss:0.131, val_acc:0.953]
Epoch [43/120    avg_loss:0.110, val_acc:0.934]
Epoch [44/120    avg_loss:0.140, val_acc:0.923]
Epoch [45/120    avg_loss:0.192, val_acc:0.909]
Epoch [46/120    avg_loss:0.173, val_acc:0.934]
Epoch [47/120    avg_loss:0.142, val_acc:0.920]
Epoch [48/120    avg_loss:0.119, val_acc:0.934]
Epoch [49/120    avg_loss:0.137, val_acc:0.921]
Epoch [50/120    avg_loss:0.111, val_acc:0.926]
Epoch [51/120    avg_loss:0.089, val_acc:0.944]
Epoch [52/120    avg_loss:0.070, val_acc:0.946]
Epoch [53/120    avg_loss:0.064, val_acc:0.953]
Epoch [54/120    avg_loss:0.080, val_acc:0.957]
Epoch [55/120    avg_loss:0.067, val_acc:0.963]
Epoch [56/120    avg_loss:0.057, val_acc:0.963]
Epoch [57/120    avg_loss:0.052, val_acc:0.960]
Epoch [58/120    avg_loss:0.074, val_acc:0.949]
Epoch [59/120    avg_loss:0.067, val_acc:0.955]
Epoch [60/120    avg_loss:0.063, val_acc:0.956]
Epoch [61/120    avg_loss:0.051, val_acc:0.957]
Epoch [62/120    avg_loss:0.043, val_acc:0.943]
Epoch [63/120    avg_loss:0.039, val_acc:0.947]
Epoch [64/120    avg_loss:0.036, val_acc:0.965]
Epoch [65/120    avg_loss:0.029, val_acc:0.969]
Epoch [66/120    avg_loss:0.024, val_acc:0.973]
Epoch [67/120    avg_loss:0.036, val_acc:0.963]
Epoch [68/120    avg_loss:0.037, val_acc:0.965]
Epoch [69/120    avg_loss:0.032, val_acc:0.972]
Epoch [70/120    avg_loss:0.032, val_acc:0.964]
Epoch [71/120    avg_loss:0.026, val_acc:0.963]
Epoch [72/120    avg_loss:0.027, val_acc:0.966]
Epoch [73/120    avg_loss:0.023, val_acc:0.958]
Epoch [74/120    avg_loss:0.029, val_acc:0.968]
Epoch [75/120    avg_loss:0.084, val_acc:0.881]
Epoch [76/120    avg_loss:0.290, val_acc:0.888]
Epoch [77/120    avg_loss:0.131, val_acc:0.931]
Epoch [78/120    avg_loss:0.107, val_acc:0.925]
Epoch [79/120    avg_loss:0.088, val_acc:0.952]
Epoch [80/120    avg_loss:0.050, val_acc:0.970]
Epoch [81/120    avg_loss:0.040, val_acc:0.970]
Epoch [82/120    avg_loss:0.033, val_acc:0.971]
Epoch [83/120    avg_loss:0.033, val_acc:0.972]
Epoch [84/120    avg_loss:0.033, val_acc:0.974]
Epoch [85/120    avg_loss:0.030, val_acc:0.975]
Epoch [86/120    avg_loss:0.031, val_acc:0.975]
Epoch [87/120    avg_loss:0.032, val_acc:0.976]
Epoch [88/120    avg_loss:0.030, val_acc:0.973]
Epoch [89/120    avg_loss:0.029, val_acc:0.974]
Epoch [90/120    avg_loss:0.026, val_acc:0.975]
Epoch [91/120    avg_loss:0.027, val_acc:0.978]
Epoch [92/120    avg_loss:0.031, val_acc:0.976]
Epoch [93/120    avg_loss:0.026, val_acc:0.974]
Epoch [94/120    avg_loss:0.028, val_acc:0.975]
Epoch [95/120    avg_loss:0.029, val_acc:0.980]
Epoch [96/120    avg_loss:0.026, val_acc:0.975]
Epoch [97/120    avg_loss:0.024, val_acc:0.972]
Epoch [98/120    avg_loss:0.024, val_acc:0.978]
Epoch [99/120    avg_loss:0.028, val_acc:0.978]
Epoch [100/120    avg_loss:0.023, val_acc:0.975]
Epoch [101/120    avg_loss:0.028, val_acc:0.976]
Epoch [102/120    avg_loss:0.024, val_acc:0.974]
Epoch [103/120    avg_loss:0.025, val_acc:0.975]
Epoch [104/120    avg_loss:0.022, val_acc:0.975]
Epoch [105/120    avg_loss:0.021, val_acc:0.976]
Epoch [106/120    avg_loss:0.022, val_acc:0.975]
Epoch [107/120    avg_loss:0.027, val_acc:0.979]
Epoch [108/120    avg_loss:0.025, val_acc:0.978]
Epoch [109/120    avg_loss:0.024, val_acc:0.978]
Epoch [110/120    avg_loss:0.023, val_acc:0.978]
Epoch [111/120    avg_loss:0.024, val_acc:0.978]
Epoch [112/120    avg_loss:0.023, val_acc:0.978]
Epoch [113/120    avg_loss:0.022, val_acc:0.978]
Epoch [114/120    avg_loss:0.020, val_acc:0.976]
Epoch [115/120    avg_loss:0.024, val_acc:0.976]
Epoch [116/120    avg_loss:0.035, val_acc:0.976]
Epoch [117/120    avg_loss:0.022, val_acc:0.976]
Epoch [118/120    avg_loss:0.021, val_acc:0.976]
Epoch [119/120    avg_loss:0.021, val_acc:0.976]
Epoch [120/120    avg_loss:0.021, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1232    2    7    1    0    0    0    0    4   38    1    0
     0    0    0]
 [   0    0    2  712    9    0    0    0    0    2    1   19    2    0
     0    0    0]
 [   0    0    0    1  210    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  422    0    0    0    8    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    9    0    0    3    0    0    0    0  843   19    0    0
     1    0    0]
 [   0    0   10    0    0    0    0    0    0    0   13 2177    6    0
     4    0    0]
 [   0    0    0    5    0    0    0    0    0    0    2    2  519    0
     0    3    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0    0
  1130    8    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    84  257    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.91056910569105

F1 scores:
[       nan 0.98765432 0.97084318 0.97068848 0.95671982 0.99311927
 0.9939302  0.98039216 0.99061033 0.91891892 0.96952271 0.97426717
 0.96738117 1.         0.95803306 0.83577236 0.97647059]

Kappa:
0.964726699184984
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f614e603fd0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.816, val_acc:0.113]
Epoch [2/120    avg_loss:2.756, val_acc:0.260]
Epoch [3/120    avg_loss:2.688, val_acc:0.395]
Epoch [4/120    avg_loss:2.601, val_acc:0.475]
Epoch [5/120    avg_loss:2.528, val_acc:0.466]
Epoch [6/120    avg_loss:2.439, val_acc:0.490]
Epoch [7/120    avg_loss:2.378, val_acc:0.487]
Epoch [8/120    avg_loss:2.291, val_acc:0.508]
Epoch [9/120    avg_loss:2.219, val_acc:0.522]
Epoch [10/120    avg_loss:2.142, val_acc:0.545]
Epoch [11/120    avg_loss:2.048, val_acc:0.558]
Epoch [12/120    avg_loss:1.940, val_acc:0.558]
Epoch [13/120    avg_loss:1.814, val_acc:0.565]
Epoch [14/120    avg_loss:1.663, val_acc:0.628]
Epoch [15/120    avg_loss:1.503, val_acc:0.649]
Epoch [16/120    avg_loss:1.353, val_acc:0.677]
Epoch [17/120    avg_loss:1.180, val_acc:0.668]
Epoch [18/120    avg_loss:1.091, val_acc:0.693]
Epoch [19/120    avg_loss:0.959, val_acc:0.775]
Epoch [20/120    avg_loss:0.918, val_acc:0.750]
Epoch [21/120    avg_loss:0.799, val_acc:0.773]
Epoch [22/120    avg_loss:0.757, val_acc:0.775]
Epoch [23/120    avg_loss:0.718, val_acc:0.842]
Epoch [24/120    avg_loss:0.605, val_acc:0.848]
Epoch [25/120    avg_loss:0.527, val_acc:0.855]
Epoch [26/120    avg_loss:0.512, val_acc:0.852]
Epoch [27/120    avg_loss:0.459, val_acc:0.887]
Epoch [28/120    avg_loss:0.387, val_acc:0.882]
Epoch [29/120    avg_loss:0.381, val_acc:0.885]
Epoch [30/120    avg_loss:0.329, val_acc:0.895]
Epoch [31/120    avg_loss:0.309, val_acc:0.909]
Epoch [32/120    avg_loss:0.307, val_acc:0.939]
Epoch [33/120    avg_loss:0.249, val_acc:0.934]
Epoch [34/120    avg_loss:0.214, val_acc:0.949]
Epoch [35/120    avg_loss:0.217, val_acc:0.922]
Epoch [36/120    avg_loss:0.205, val_acc:0.921]
Epoch [37/120    avg_loss:0.186, val_acc:0.940]
Epoch [38/120    avg_loss:0.164, val_acc:0.947]
Epoch [39/120    avg_loss:0.127, val_acc:0.959]
Epoch [40/120    avg_loss:0.114, val_acc:0.963]
Epoch [41/120    avg_loss:0.116, val_acc:0.953]
Epoch [42/120    avg_loss:0.121, val_acc:0.936]
Epoch [43/120    avg_loss:0.123, val_acc:0.940]
Epoch [44/120    avg_loss:0.124, val_acc:0.943]
Epoch [45/120    avg_loss:0.120, val_acc:0.944]
Epoch [46/120    avg_loss:0.141, val_acc:0.951]
Epoch [47/120    avg_loss:0.119, val_acc:0.961]
Epoch [48/120    avg_loss:0.086, val_acc:0.964]
Epoch [49/120    avg_loss:0.092, val_acc:0.963]
Epoch [50/120    avg_loss:0.127, val_acc:0.960]
Epoch [51/120    avg_loss:0.243, val_acc:0.923]
Epoch [52/120    avg_loss:0.197, val_acc:0.936]
Epoch [53/120    avg_loss:0.140, val_acc:0.954]
Epoch [54/120    avg_loss:0.106, val_acc:0.922]
Epoch [55/120    avg_loss:0.104, val_acc:0.953]
Epoch [56/120    avg_loss:0.082, val_acc:0.952]
Epoch [57/120    avg_loss:0.065, val_acc:0.976]
Epoch [58/120    avg_loss:0.047, val_acc:0.973]
Epoch [59/120    avg_loss:0.049, val_acc:0.973]
Epoch [60/120    avg_loss:0.058, val_acc:0.977]
Epoch [61/120    avg_loss:0.060, val_acc:0.941]
Epoch [62/120    avg_loss:0.085, val_acc:0.971]
Epoch [63/120    avg_loss:0.065, val_acc:0.963]
Epoch [64/120    avg_loss:0.059, val_acc:0.980]
Epoch [65/120    avg_loss:0.042, val_acc:0.976]
Epoch [66/120    avg_loss:0.032, val_acc:0.977]
Epoch [67/120    avg_loss:0.028, val_acc:0.979]
Epoch [68/120    avg_loss:0.036, val_acc:0.969]
Epoch [69/120    avg_loss:0.026, val_acc:0.983]
Epoch [70/120    avg_loss:0.032, val_acc:0.978]
Epoch [71/120    avg_loss:0.022, val_acc:0.984]
Epoch [72/120    avg_loss:0.022, val_acc:0.988]
Epoch [73/120    avg_loss:0.022, val_acc:0.984]
Epoch [74/120    avg_loss:0.023, val_acc:0.985]
Epoch [75/120    avg_loss:0.024, val_acc:0.985]
Epoch [76/120    avg_loss:0.022, val_acc:0.979]
Epoch [77/120    avg_loss:0.018, val_acc:0.978]
Epoch [78/120    avg_loss:0.020, val_acc:0.977]
Epoch [79/120    avg_loss:0.024, val_acc:0.976]
Epoch [80/120    avg_loss:0.020, val_acc:0.978]
Epoch [81/120    avg_loss:0.014, val_acc:0.985]
Epoch [82/120    avg_loss:0.013, val_acc:0.986]
Epoch [83/120    avg_loss:0.033, val_acc:0.975]
Epoch [84/120    avg_loss:0.028, val_acc:0.984]
Epoch [85/120    avg_loss:0.022, val_acc:0.981]
Epoch [86/120    avg_loss:0.014, val_acc:0.988]
Epoch [87/120    avg_loss:0.015, val_acc:0.989]
Epoch [88/120    avg_loss:0.012, val_acc:0.989]
Epoch [89/120    avg_loss:0.013, val_acc:0.986]
Epoch [90/120    avg_loss:0.013, val_acc:0.988]
Epoch [91/120    avg_loss:0.012, val_acc:0.988]
Epoch [92/120    avg_loss:0.011, val_acc:0.986]
Epoch [93/120    avg_loss:0.012, val_acc:0.988]
Epoch [94/120    avg_loss:0.012, val_acc:0.986]
Epoch [95/120    avg_loss:0.011, val_acc:0.989]
Epoch [96/120    avg_loss:0.010, val_acc:0.988]
Epoch [97/120    avg_loss:0.011, val_acc:0.989]
Epoch [98/120    avg_loss:0.011, val_acc:0.989]
Epoch [99/120    avg_loss:0.011, val_acc:0.989]
Epoch [100/120    avg_loss:0.010, val_acc:0.988]
Epoch [101/120    avg_loss:0.010, val_acc:0.986]
Epoch [102/120    avg_loss:0.011, val_acc:0.988]
Epoch [103/120    avg_loss:0.011, val_acc:0.988]
Epoch [104/120    avg_loss:0.012, val_acc:0.986]
Epoch [105/120    avg_loss:0.015, val_acc:0.986]
Epoch [106/120    avg_loss:0.011, val_acc:0.985]
Epoch [107/120    avg_loss:0.011, val_acc:0.986]
Epoch [108/120    avg_loss:0.010, val_acc:0.988]
Epoch [109/120    avg_loss:0.010, val_acc:0.989]
Epoch [110/120    avg_loss:0.010, val_acc:0.988]
Epoch [111/120    avg_loss:0.010, val_acc:0.989]
Epoch [112/120    avg_loss:0.010, val_acc:0.989]
Epoch [113/120    avg_loss:0.010, val_acc:0.989]
Epoch [114/120    avg_loss:0.012, val_acc:0.985]
Epoch [115/120    avg_loss:0.010, val_acc:0.989]
Epoch [116/120    avg_loss:0.011, val_acc:0.988]
Epoch [117/120    avg_loss:0.010, val_acc:0.989]
Epoch [118/120    avg_loss:0.011, val_acc:0.989]
Epoch [119/120    avg_loss:0.010, val_acc:0.988]
Epoch [120/120    avg_loss:0.011, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1266    5    6    0    0    0    0    0    0    8    0    0
     0    0    0]
 [   0    0    4  722    9    0    0    0    0    2    1    0    2    7
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    1    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    2    0    0    0    0  856   14    1    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    1    0   19 2170   13    0
     0    0    0]
 [   0    0    0    3    0    1    0    0    0    0    2    0  526    0
     0    2    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1124   14    0]
 [   0    0    0    0    0    0   18    0    0    0    0    0    0    0
    41  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.87533875338754

F1 scores:
[       nan 0.95348837 0.9875195  0.97765741 0.96598639 0.99078341
 0.98498498 1.         0.99416569 0.94736842 0.97605473 0.98569157
 0.97678737 0.98143236 0.97400347 0.88479263 0.99401198]

Kappa:
0.9757821731238296
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f47a880cf28>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.819, val_acc:0.132]
Epoch [2/120    avg_loss:2.744, val_acc:0.231]
Epoch [3/120    avg_loss:2.662, val_acc:0.278]
Epoch [4/120    avg_loss:2.576, val_acc:0.302]
Epoch [5/120    avg_loss:2.468, val_acc:0.325]
Epoch [6/120    avg_loss:2.392, val_acc:0.353]
Epoch [7/120    avg_loss:2.307, val_acc:0.401]
Epoch [8/120    avg_loss:2.210, val_acc:0.370]
Epoch [9/120    avg_loss:2.129, val_acc:0.472]
Epoch [10/120    avg_loss:2.039, val_acc:0.532]
Epoch [11/120    avg_loss:1.962, val_acc:0.534]
Epoch [12/120    avg_loss:1.836, val_acc:0.543]
Epoch [13/120    avg_loss:1.736, val_acc:0.628]
Epoch [14/120    avg_loss:1.590, val_acc:0.614]
Epoch [15/120    avg_loss:1.463, val_acc:0.631]
Epoch [16/120    avg_loss:1.305, val_acc:0.674]
Epoch [17/120    avg_loss:1.305, val_acc:0.674]
Epoch [18/120    avg_loss:1.190, val_acc:0.681]
Epoch [19/120    avg_loss:1.083, val_acc:0.709]
Epoch [20/120    avg_loss:0.958, val_acc:0.756]
Epoch [21/120    avg_loss:0.911, val_acc:0.780]
Epoch [22/120    avg_loss:0.812, val_acc:0.795]
Epoch [23/120    avg_loss:0.731, val_acc:0.795]
Epoch [24/120    avg_loss:0.592, val_acc:0.818]
Epoch [25/120    avg_loss:0.569, val_acc:0.828]
Epoch [26/120    avg_loss:0.570, val_acc:0.846]
Epoch [27/120    avg_loss:0.488, val_acc:0.866]
Epoch [28/120    avg_loss:0.455, val_acc:0.856]
Epoch [29/120    avg_loss:0.416, val_acc:0.873]
Epoch [30/120    avg_loss:0.368, val_acc:0.871]
Epoch [31/120    avg_loss:0.371, val_acc:0.858]
Epoch [32/120    avg_loss:0.347, val_acc:0.877]
Epoch [33/120    avg_loss:0.308, val_acc:0.892]
Epoch [34/120    avg_loss:0.242, val_acc:0.911]
Epoch [35/120    avg_loss:0.188, val_acc:0.925]
Epoch [36/120    avg_loss:0.200, val_acc:0.931]
Epoch [37/120    avg_loss:0.163, val_acc:0.922]
Epoch [38/120    avg_loss:0.144, val_acc:0.934]
Epoch [39/120    avg_loss:0.138, val_acc:0.939]
Epoch [40/120    avg_loss:0.143, val_acc:0.935]
Epoch [41/120    avg_loss:0.124, val_acc:0.936]
Epoch [42/120    avg_loss:0.110, val_acc:0.936]
Epoch [43/120    avg_loss:0.095, val_acc:0.940]
Epoch [44/120    avg_loss:0.095, val_acc:0.957]
Epoch [45/120    avg_loss:0.104, val_acc:0.937]
Epoch [46/120    avg_loss:0.096, val_acc:0.956]
Epoch [47/120    avg_loss:0.083, val_acc:0.946]
Epoch [48/120    avg_loss:0.079, val_acc:0.964]
Epoch [49/120    avg_loss:0.086, val_acc:0.943]
Epoch [50/120    avg_loss:0.081, val_acc:0.953]
Epoch [51/120    avg_loss:0.073, val_acc:0.952]
Epoch [52/120    avg_loss:0.072, val_acc:0.964]
Epoch [53/120    avg_loss:0.055, val_acc:0.964]
Epoch [54/120    avg_loss:0.067, val_acc:0.956]
Epoch [55/120    avg_loss:0.054, val_acc:0.966]
Epoch [56/120    avg_loss:0.048, val_acc:0.964]
Epoch [57/120    avg_loss:0.053, val_acc:0.953]
Epoch [58/120    avg_loss:0.054, val_acc:0.971]
Epoch [59/120    avg_loss:0.042, val_acc:0.969]
Epoch [60/120    avg_loss:0.043, val_acc:0.967]
Epoch [61/120    avg_loss:0.040, val_acc:0.941]
Epoch [62/120    avg_loss:0.046, val_acc:0.969]
Epoch [63/120    avg_loss:0.036, val_acc:0.970]
Epoch [64/120    avg_loss:0.032, val_acc:0.974]
Epoch [65/120    avg_loss:0.041, val_acc:0.973]
Epoch [66/120    avg_loss:0.031, val_acc:0.972]
Epoch [67/120    avg_loss:0.039, val_acc:0.981]
Epoch [68/120    avg_loss:0.031, val_acc:0.975]
Epoch [69/120    avg_loss:0.038, val_acc:0.971]
Epoch [70/120    avg_loss:0.051, val_acc:0.953]
Epoch [71/120    avg_loss:0.084, val_acc:0.969]
Epoch [72/120    avg_loss:0.037, val_acc:0.969]
Epoch [73/120    avg_loss:0.025, val_acc:0.979]
Epoch [74/120    avg_loss:0.022, val_acc:0.975]
Epoch [75/120    avg_loss:0.021, val_acc:0.984]
Epoch [76/120    avg_loss:0.023, val_acc:0.978]
Epoch [77/120    avg_loss:0.021, val_acc:0.984]
Epoch [78/120    avg_loss:0.017, val_acc:0.973]
Epoch [79/120    avg_loss:0.017, val_acc:0.981]
Epoch [80/120    avg_loss:0.016, val_acc:0.984]
Epoch [81/120    avg_loss:0.018, val_acc:0.982]
Epoch [82/120    avg_loss:0.015, val_acc:0.980]
Epoch [83/120    avg_loss:0.016, val_acc:0.988]
Epoch [84/120    avg_loss:0.016, val_acc:0.979]
Epoch [85/120    avg_loss:0.017, val_acc:0.981]
Epoch [86/120    avg_loss:0.014, val_acc:0.985]
Epoch [87/120    avg_loss:0.015, val_acc:0.986]
Epoch [88/120    avg_loss:0.015, val_acc:0.980]
Epoch [89/120    avg_loss:0.017, val_acc:0.982]
Epoch [90/120    avg_loss:0.020, val_acc:0.979]
Epoch [91/120    avg_loss:0.015, val_acc:0.983]
Epoch [92/120    avg_loss:0.015, val_acc:0.985]
Epoch [93/120    avg_loss:0.023, val_acc:0.974]
Epoch [94/120    avg_loss:0.013, val_acc:0.987]
Epoch [95/120    avg_loss:0.014, val_acc:0.990]
Epoch [96/120    avg_loss:0.047, val_acc:0.978]
Epoch [97/120    avg_loss:0.032, val_acc:0.976]
Epoch [98/120    avg_loss:0.015, val_acc:0.989]
Epoch [99/120    avg_loss:0.012, val_acc:0.984]
Epoch [100/120    avg_loss:0.014, val_acc:0.987]
Epoch [101/120    avg_loss:0.013, val_acc:0.985]
Epoch [102/120    avg_loss:0.017, val_acc:0.984]
Epoch [103/120    avg_loss:0.014, val_acc:0.980]
Epoch [104/120    avg_loss:0.013, val_acc:0.986]
Epoch [105/120    avg_loss:0.010, val_acc:0.985]
Epoch [106/120    avg_loss:0.010, val_acc:0.988]
Epoch [107/120    avg_loss:0.008, val_acc:0.987]
Epoch [108/120    avg_loss:0.008, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.008, val_acc:0.989]
Epoch [111/120    avg_loss:0.007, val_acc:0.989]
Epoch [112/120    avg_loss:0.008, val_acc:0.989]
Epoch [113/120    avg_loss:0.010, val_acc:0.991]
Epoch [114/120    avg_loss:0.007, val_acc:0.993]
Epoch [115/120    avg_loss:0.008, val_acc:0.991]
Epoch [116/120    avg_loss:0.007, val_acc:0.990]
Epoch [117/120    avg_loss:0.008, val_acc:0.991]
Epoch [118/120    avg_loss:0.008, val_acc:0.990]
Epoch [119/120    avg_loss:0.008, val_acc:0.990]
Epoch [120/120    avg_loss:0.008, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1265    0    0    0    0    0    0    0    2   18    0    0
     0    0    0]
 [   0    0    0  729    2    1    0    0    0    6    1    4    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    1  851   19    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0    3 2183   20    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    5    2  521    2
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1131    7    0]
 [   0    0    0    0    0    0   34    0    0    0    0    0    0    0
    23  290    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.07046070460704

F1 scores:
[       nan 0.95348837 0.98905395 0.98713609 0.9953271  0.99423299
 0.97405486 1.         0.9953271  0.75555556 0.97985032 0.9839982
 0.96481481 0.99191375 0.98562092 0.89783282 0.98809524]

Kappa:
0.9779932161128497
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7ba5840f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.809, val_acc:0.109]
Epoch [2/120    avg_loss:2.757, val_acc:0.155]
Epoch [3/120    avg_loss:2.693, val_acc:0.317]
Epoch [4/120    avg_loss:2.624, val_acc:0.375]
Epoch [5/120    avg_loss:2.536, val_acc:0.394]
Epoch [6/120    avg_loss:2.440, val_acc:0.412]
Epoch [7/120    avg_loss:2.378, val_acc:0.422]
Epoch [8/120    avg_loss:2.315, val_acc:0.485]
Epoch [9/120    avg_loss:2.195, val_acc:0.534]
Epoch [10/120    avg_loss:2.105, val_acc:0.588]
Epoch [11/120    avg_loss:2.017, val_acc:0.548]
Epoch [12/120    avg_loss:1.958, val_acc:0.611]
Epoch [13/120    avg_loss:1.815, val_acc:0.642]
Epoch [14/120    avg_loss:1.705, val_acc:0.655]
Epoch [15/120    avg_loss:1.546, val_acc:0.728]
Epoch [16/120    avg_loss:1.391, val_acc:0.711]
Epoch [17/120    avg_loss:1.263, val_acc:0.652]
Epoch [18/120    avg_loss:1.156, val_acc:0.771]
Epoch [19/120    avg_loss:1.050, val_acc:0.794]
Epoch [20/120    avg_loss:0.941, val_acc:0.784]
Epoch [21/120    avg_loss:0.790, val_acc:0.815]
Epoch [22/120    avg_loss:0.703, val_acc:0.824]
Epoch [23/120    avg_loss:0.615, val_acc:0.861]
Epoch [24/120    avg_loss:0.555, val_acc:0.875]
Epoch [25/120    avg_loss:0.490, val_acc:0.882]
Epoch [26/120    avg_loss:0.418, val_acc:0.854]
Epoch [27/120    avg_loss:0.394, val_acc:0.907]
Epoch [28/120    avg_loss:0.318, val_acc:0.914]
Epoch [29/120    avg_loss:0.297, val_acc:0.903]
Epoch [30/120    avg_loss:0.316, val_acc:0.916]
Epoch [31/120    avg_loss:0.237, val_acc:0.939]
Epoch [32/120    avg_loss:0.220, val_acc:0.911]
Epoch [33/120    avg_loss:0.201, val_acc:0.943]
Epoch [34/120    avg_loss:0.200, val_acc:0.925]
Epoch [35/120    avg_loss:0.199, val_acc:0.954]
Epoch [36/120    avg_loss:0.166, val_acc:0.947]
Epoch [37/120    avg_loss:0.133, val_acc:0.946]
Epoch [38/120    avg_loss:0.140, val_acc:0.948]
Epoch [39/120    avg_loss:0.135, val_acc:0.950]
Epoch [40/120    avg_loss:0.825, val_acc:0.908]
Epoch [41/120    avg_loss:0.211, val_acc:0.949]
Epoch [42/120    avg_loss:0.178, val_acc:0.928]
Epoch [43/120    avg_loss:0.202, val_acc:0.842]
Epoch [44/120    avg_loss:0.273, val_acc:0.932]
Epoch [45/120    avg_loss:0.144, val_acc:0.946]
Epoch [46/120    avg_loss:0.120, val_acc:0.938]
Epoch [47/120    avg_loss:0.124, val_acc:0.952]
Epoch [48/120    avg_loss:0.101, val_acc:0.958]
Epoch [49/120    avg_loss:0.072, val_acc:0.967]
Epoch [50/120    avg_loss:0.068, val_acc:0.964]
Epoch [51/120    avg_loss:0.093, val_acc:0.953]
Epoch [52/120    avg_loss:0.066, val_acc:0.961]
Epoch [53/120    avg_loss:0.064, val_acc:0.981]
Epoch [54/120    avg_loss:0.048, val_acc:0.975]
Epoch [55/120    avg_loss:0.046, val_acc:0.973]
Epoch [56/120    avg_loss:0.046, val_acc:0.976]
Epoch [57/120    avg_loss:0.048, val_acc:0.979]
Epoch [58/120    avg_loss:0.049, val_acc:0.979]
Epoch [59/120    avg_loss:0.034, val_acc:0.978]
Epoch [60/120    avg_loss:0.041, val_acc:0.980]
Epoch [61/120    avg_loss:0.031, val_acc:0.984]
Epoch [62/120    avg_loss:0.035, val_acc:0.975]
Epoch [63/120    avg_loss:0.034, val_acc:0.978]
Epoch [64/120    avg_loss:0.030, val_acc:0.979]
Epoch [65/120    avg_loss:0.024, val_acc:0.982]
Epoch [66/120    avg_loss:0.025, val_acc:0.977]
Epoch [67/120    avg_loss:0.029, val_acc:0.974]
Epoch [68/120    avg_loss:0.033, val_acc:0.971]
Epoch [69/120    avg_loss:0.024, val_acc:0.983]
Epoch [70/120    avg_loss:0.021, val_acc:0.984]
Epoch [71/120    avg_loss:0.022, val_acc:0.986]
Epoch [72/120    avg_loss:0.023, val_acc:0.988]
Epoch [73/120    avg_loss:0.195, val_acc:0.923]
Epoch [74/120    avg_loss:0.110, val_acc:0.964]
Epoch [75/120    avg_loss:0.087, val_acc:0.946]
Epoch [76/120    avg_loss:0.091, val_acc:0.954]
Epoch [77/120    avg_loss:0.061, val_acc:0.969]
Epoch [78/120    avg_loss:0.037, val_acc:0.978]
Epoch [79/120    avg_loss:0.035, val_acc:0.973]
Epoch [80/120    avg_loss:0.026, val_acc:0.980]
Epoch [81/120    avg_loss:0.023, val_acc:0.981]
Epoch [82/120    avg_loss:0.018, val_acc:0.981]
Epoch [83/120    avg_loss:0.015, val_acc:0.982]
Epoch [84/120    avg_loss:0.015, val_acc:0.980]
Epoch [85/120    avg_loss:0.024, val_acc:0.976]
Epoch [86/120    avg_loss:0.015, val_acc:0.978]
Epoch [87/120    avg_loss:0.016, val_acc:0.980]
Epoch [88/120    avg_loss:0.013, val_acc:0.980]
Epoch [89/120    avg_loss:0.016, val_acc:0.981]
Epoch [90/120    avg_loss:0.014, val_acc:0.982]
Epoch [91/120    avg_loss:0.012, val_acc:0.983]
Epoch [92/120    avg_loss:0.014, val_acc:0.982]
Epoch [93/120    avg_loss:0.011, val_acc:0.983]
Epoch [94/120    avg_loss:0.014, val_acc:0.983]
Epoch [95/120    avg_loss:0.015, val_acc:0.983]
Epoch [96/120    avg_loss:0.013, val_acc:0.983]
Epoch [97/120    avg_loss:0.013, val_acc:0.981]
Epoch [98/120    avg_loss:0.013, val_acc:0.981]
Epoch [99/120    avg_loss:0.013, val_acc:0.982]
Epoch [100/120    avg_loss:0.012, val_acc:0.981]
Epoch [101/120    avg_loss:0.012, val_acc:0.982]
Epoch [102/120    avg_loss:0.013, val_acc:0.981]
Epoch [103/120    avg_loss:0.012, val_acc:0.982]
Epoch [104/120    avg_loss:0.012, val_acc:0.983]
Epoch [105/120    avg_loss:0.013, val_acc:0.982]
Epoch [106/120    avg_loss:0.012, val_acc:0.982]
Epoch [107/120    avg_loss:0.013, val_acc:0.982]
Epoch [108/120    avg_loss:0.013, val_acc:0.982]
Epoch [109/120    avg_loss:0.014, val_acc:0.982]
Epoch [110/120    avg_loss:0.012, val_acc:0.982]
Epoch [111/120    avg_loss:0.012, val_acc:0.982]
Epoch [112/120    avg_loss:0.010, val_acc:0.982]
Epoch [113/120    avg_loss:0.011, val_acc:0.982]
Epoch [114/120    avg_loss:0.012, val_acc:0.982]
Epoch [115/120    avg_loss:0.013, val_acc:0.982]
Epoch [116/120    avg_loss:0.011, val_acc:0.982]
Epoch [117/120    avg_loss:0.012, val_acc:0.982]
Epoch [118/120    avg_loss:0.012, val_acc:0.982]
Epoch [119/120    avg_loss:0.011, val_acc:0.982]
Epoch [120/120    avg_loss:0.013, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1244    1    6    0    1    0    0    0    3   30    0    0
     0    0    0]
 [   0    0    3  720    3    0    0    0    0    6    3    5    7    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    1    2    0    0    0  846   18    0    0
     1    0    0]
 [   0    0    0    0    0    0    4    0    7    0    5 2174   20    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    3    0  522    0
     0    3    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1127   11    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    58  270    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.39837398373983

F1 scores:
[       nan 0.98765432 0.97952756 0.98092643 0.97931034 0.99427262
 0.97834205 1.         0.99192618 0.8372093  0.97521614 0.9799414
 0.96221198 1.         0.96862914 0.85578447 0.9704142 ]

Kappa:
0.9703262500144249
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6b2de2af60>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.812, val_acc:0.135]
Epoch [2/120    avg_loss:2.763, val_acc:0.234]
Epoch [3/120    avg_loss:2.710, val_acc:0.308]
Epoch [4/120    avg_loss:2.655, val_acc:0.392]
Epoch [5/120    avg_loss:2.558, val_acc:0.450]
Epoch [6/120    avg_loss:2.499, val_acc:0.504]
Epoch [7/120    avg_loss:2.412, val_acc:0.540]
Epoch [8/120    avg_loss:2.332, val_acc:0.577]
Epoch [9/120    avg_loss:2.266, val_acc:0.600]
Epoch [10/120    avg_loss:2.141, val_acc:0.555]
Epoch [11/120    avg_loss:2.069, val_acc:0.614]
Epoch [12/120    avg_loss:1.995, val_acc:0.652]
Epoch [13/120    avg_loss:1.905, val_acc:0.646]
Epoch [14/120    avg_loss:1.829, val_acc:0.665]
Epoch [15/120    avg_loss:1.691, val_acc:0.719]
Epoch [16/120    avg_loss:1.545, val_acc:0.692]
Epoch [17/120    avg_loss:1.421, val_acc:0.722]
Epoch [18/120    avg_loss:1.282, val_acc:0.730]
Epoch [19/120    avg_loss:1.180, val_acc:0.778]
Epoch [20/120    avg_loss:1.005, val_acc:0.776]
Epoch [21/120    avg_loss:0.948, val_acc:0.807]
Epoch [22/120    avg_loss:0.783, val_acc:0.819]
Epoch [23/120    avg_loss:0.662, val_acc:0.829]
Epoch [24/120    avg_loss:0.689, val_acc:0.805]
Epoch [25/120    avg_loss:0.548, val_acc:0.875]
Epoch [26/120    avg_loss:0.620, val_acc:0.845]
Epoch [27/120    avg_loss:0.476, val_acc:0.876]
Epoch [28/120    avg_loss:0.411, val_acc:0.893]
Epoch [29/120    avg_loss:0.404, val_acc:0.881]
Epoch [30/120    avg_loss:0.303, val_acc:0.916]
Epoch [31/120    avg_loss:0.289, val_acc:0.894]
Epoch [32/120    avg_loss:0.234, val_acc:0.920]
Epoch [33/120    avg_loss:0.218, val_acc:0.909]
Epoch [34/120    avg_loss:0.199, val_acc:0.926]
Epoch [35/120    avg_loss:0.175, val_acc:0.946]
Epoch [36/120    avg_loss:0.160, val_acc:0.935]
Epoch [37/120    avg_loss:0.174, val_acc:0.940]
Epoch [38/120    avg_loss:0.169, val_acc:0.945]
Epoch [39/120    avg_loss:0.130, val_acc:0.945]
Epoch [40/120    avg_loss:0.119, val_acc:0.960]
Epoch [41/120    avg_loss:0.098, val_acc:0.949]
Epoch [42/120    avg_loss:0.140, val_acc:0.946]
Epoch [43/120    avg_loss:0.118, val_acc:0.954]
Epoch [44/120    avg_loss:0.107, val_acc:0.938]
Epoch [45/120    avg_loss:0.154, val_acc:0.949]
Epoch [46/120    avg_loss:0.111, val_acc:0.951]
Epoch [47/120    avg_loss:0.094, val_acc:0.950]
Epoch [48/120    avg_loss:0.081, val_acc:0.955]
Epoch [49/120    avg_loss:0.077, val_acc:0.953]
Epoch [50/120    avg_loss:0.078, val_acc:0.965]
Epoch [51/120    avg_loss:0.082, val_acc:0.968]
Epoch [52/120    avg_loss:0.066, val_acc:0.970]
Epoch [53/120    avg_loss:0.064, val_acc:0.967]
Epoch [54/120    avg_loss:0.055, val_acc:0.958]
Epoch [55/120    avg_loss:0.061, val_acc:0.964]
Epoch [56/120    avg_loss:0.051, val_acc:0.964]
Epoch [57/120    avg_loss:0.050, val_acc:0.965]
Epoch [58/120    avg_loss:0.052, val_acc:0.967]
Epoch [59/120    avg_loss:0.039, val_acc:0.970]
Epoch [60/120    avg_loss:0.035, val_acc:0.971]
Epoch [61/120    avg_loss:0.043, val_acc:0.973]
Epoch [62/120    avg_loss:0.038, val_acc:0.966]
Epoch [63/120    avg_loss:0.041, val_acc:0.959]
Epoch [64/120    avg_loss:0.047, val_acc:0.973]
Epoch [65/120    avg_loss:0.058, val_acc:0.969]
Epoch [66/120    avg_loss:0.035, val_acc:0.974]
Epoch [67/120    avg_loss:0.031, val_acc:0.967]
Epoch [68/120    avg_loss:0.034, val_acc:0.959]
Epoch [69/120    avg_loss:0.032, val_acc:0.981]
Epoch [70/120    avg_loss:0.025, val_acc:0.978]
Epoch [71/120    avg_loss:0.039, val_acc:0.977]
Epoch [72/120    avg_loss:0.026, val_acc:0.966]
Epoch [73/120    avg_loss:0.026, val_acc:0.966]
Epoch [74/120    avg_loss:0.027, val_acc:0.976]
Epoch [75/120    avg_loss:0.020, val_acc:0.972]
Epoch [76/120    avg_loss:0.022, val_acc:0.975]
Epoch [77/120    avg_loss:0.016, val_acc:0.983]
Epoch [78/120    avg_loss:0.018, val_acc:0.976]
Epoch [79/120    avg_loss:0.017, val_acc:0.979]
Epoch [80/120    avg_loss:0.016, val_acc:0.972]
Epoch [81/120    avg_loss:0.021, val_acc:0.979]
Epoch [82/120    avg_loss:0.017, val_acc:0.975]
Epoch [83/120    avg_loss:0.018, val_acc:0.981]
Epoch [84/120    avg_loss:0.018, val_acc:0.980]
Epoch [85/120    avg_loss:0.017, val_acc:0.977]
Epoch [86/120    avg_loss:0.015, val_acc:0.975]
Epoch [87/120    avg_loss:0.038, val_acc:0.968]
Epoch [88/120    avg_loss:0.020, val_acc:0.976]
Epoch [89/120    avg_loss:0.014, val_acc:0.972]
Epoch [90/120    avg_loss:0.018, val_acc:0.976]
Epoch [91/120    avg_loss:0.012, val_acc:0.978]
Epoch [92/120    avg_loss:0.012, val_acc:0.981]
Epoch [93/120    avg_loss:0.010, val_acc:0.981]
Epoch [94/120    avg_loss:0.013, val_acc:0.980]
Epoch [95/120    avg_loss:0.009, val_acc:0.980]
Epoch [96/120    avg_loss:0.009, val_acc:0.980]
Epoch [97/120    avg_loss:0.011, val_acc:0.980]
Epoch [98/120    avg_loss:0.010, val_acc:0.980]
Epoch [99/120    avg_loss:0.010, val_acc:0.979]
Epoch [100/120    avg_loss:0.011, val_acc:0.979]
Epoch [101/120    avg_loss:0.009, val_acc:0.980]
Epoch [102/120    avg_loss:0.011, val_acc:0.980]
Epoch [103/120    avg_loss:0.012, val_acc:0.980]
Epoch [104/120    avg_loss:0.010, val_acc:0.980]
Epoch [105/120    avg_loss:0.010, val_acc:0.980]
Epoch [106/120    avg_loss:0.011, val_acc:0.980]
Epoch [107/120    avg_loss:0.009, val_acc:0.980]
Epoch [108/120    avg_loss:0.011, val_acc:0.980]
Epoch [109/120    avg_loss:0.011, val_acc:0.980]
Epoch [110/120    avg_loss:0.010, val_acc:0.980]
Epoch [111/120    avg_loss:0.011, val_acc:0.980]
Epoch [112/120    avg_loss:0.010, val_acc:0.980]
Epoch [113/120    avg_loss:0.009, val_acc:0.980]
Epoch [114/120    avg_loss:0.010, val_acc:0.980]
Epoch [115/120    avg_loss:0.011, val_acc:0.980]
Epoch [116/120    avg_loss:0.011, val_acc:0.980]
Epoch [117/120    avg_loss:0.010, val_acc:0.980]
Epoch [118/120    avg_loss:0.010, val_acc:0.980]
Epoch [119/120    avg_loss:0.011, val_acc:0.980]
Epoch [120/120    avg_loss:0.009, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1257    0    4    1    2    0    0    0    1   19    1    0
     0    0    0]
 [   0    0    1  722    4    0    1    0    0    8    2    7    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    1    0    0
     2    1    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    0    0
     0    1    0]
 [   0    0    0    0    0    0    6    0    0   12    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    1    2    0    0    0  858    6    2    0
     0    1    0]
 [   0    0   20    0    0    0    0    2    0    0    9 2164   14    0
     0    1    0]
 [   0    0    0    0    0    2    0    0    0    0    4   10  513    0
     1    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1136    3    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    56  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.66937669376694

F1 scores:
[       nan 0.975      0.97859089 0.98298162 0.98156682 0.9908046
 0.98722765 0.96153846 0.99883586 0.63157895 0.98057143 0.97985058
 0.96247655 1.         0.97343616 0.88923557 0.98823529]

Kappa:
0.9734179699897241
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1e60827f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.801, val_acc:0.138]
Epoch [2/120    avg_loss:2.738, val_acc:0.149]
Epoch [3/120    avg_loss:2.685, val_acc:0.155]
Epoch [4/120    avg_loss:2.599, val_acc:0.198]
Epoch [5/120    avg_loss:2.516, val_acc:0.337]
Epoch [6/120    avg_loss:2.461, val_acc:0.417]
Epoch [7/120    avg_loss:2.361, val_acc:0.425]
Epoch [8/120    avg_loss:2.290, val_acc:0.469]
Epoch [9/120    avg_loss:2.243, val_acc:0.495]
Epoch [10/120    avg_loss:2.127, val_acc:0.510]
Epoch [11/120    avg_loss:2.082, val_acc:0.560]
Epoch [12/120    avg_loss:1.976, val_acc:0.591]
Epoch [13/120    avg_loss:1.937, val_acc:0.609]
Epoch [14/120    avg_loss:1.860, val_acc:0.623]
Epoch [15/120    avg_loss:1.783, val_acc:0.667]
Epoch [16/120    avg_loss:1.662, val_acc:0.684]
Epoch [17/120    avg_loss:1.559, val_acc:0.710]
Epoch [18/120    avg_loss:1.452, val_acc:0.693]
Epoch [19/120    avg_loss:1.370, val_acc:0.730]
Epoch [20/120    avg_loss:1.243, val_acc:0.739]
Epoch [21/120    avg_loss:1.144, val_acc:0.730]
Epoch [22/120    avg_loss:0.996, val_acc:0.757]
Epoch [23/120    avg_loss:0.941, val_acc:0.770]
Epoch [24/120    avg_loss:0.900, val_acc:0.783]
Epoch [25/120    avg_loss:0.785, val_acc:0.806]
Epoch [26/120    avg_loss:0.689, val_acc:0.776]
Epoch [27/120    avg_loss:0.641, val_acc:0.831]
Epoch [28/120    avg_loss:0.607, val_acc:0.822]
Epoch [29/120    avg_loss:0.510, val_acc:0.846]
Epoch [30/120    avg_loss:0.482, val_acc:0.859]
Epoch [31/120    avg_loss:0.413, val_acc:0.870]
Epoch [32/120    avg_loss:0.362, val_acc:0.895]
Epoch [33/120    avg_loss:0.344, val_acc:0.832]
Epoch [34/120    avg_loss:0.450, val_acc:0.887]
Epoch [35/120    avg_loss:0.323, val_acc:0.902]
Epoch [36/120    avg_loss:0.271, val_acc:0.875]
Epoch [37/120    avg_loss:0.238, val_acc:0.885]
Epoch [38/120    avg_loss:0.225, val_acc:0.897]
Epoch [39/120    avg_loss:0.184, val_acc:0.913]
Epoch [40/120    avg_loss:0.147, val_acc:0.934]
Epoch [41/120    avg_loss:0.158, val_acc:0.923]
Epoch [42/120    avg_loss:0.173, val_acc:0.917]
Epoch [43/120    avg_loss:0.240, val_acc:0.923]
Epoch [44/120    avg_loss:0.144, val_acc:0.925]
Epoch [45/120    avg_loss:0.159, val_acc:0.922]
Epoch [46/120    avg_loss:0.151, val_acc:0.928]
Epoch [47/120    avg_loss:0.185, val_acc:0.892]
Epoch [48/120    avg_loss:0.307, val_acc:0.885]
Epoch [49/120    avg_loss:0.200, val_acc:0.931]
Epoch [50/120    avg_loss:0.112, val_acc:0.928]
Epoch [51/120    avg_loss:0.107, val_acc:0.931]
Epoch [52/120    avg_loss:0.115, val_acc:0.935]
Epoch [53/120    avg_loss:0.093, val_acc:0.940]
Epoch [54/120    avg_loss:0.082, val_acc:0.943]
Epoch [55/120    avg_loss:0.076, val_acc:0.956]
Epoch [56/120    avg_loss:0.082, val_acc:0.940]
Epoch [57/120    avg_loss:0.075, val_acc:0.953]
Epoch [58/120    avg_loss:0.061, val_acc:0.958]
Epoch [59/120    avg_loss:0.059, val_acc:0.934]
Epoch [60/120    avg_loss:0.057, val_acc:0.957]
Epoch [61/120    avg_loss:0.070, val_acc:0.953]
Epoch [62/120    avg_loss:0.067, val_acc:0.955]
Epoch [63/120    avg_loss:0.115, val_acc:0.955]
Epoch [64/120    avg_loss:0.058, val_acc:0.944]
Epoch [65/120    avg_loss:0.058, val_acc:0.964]
Epoch [66/120    avg_loss:0.053, val_acc:0.966]
Epoch [67/120    avg_loss:0.048, val_acc:0.966]
Epoch [68/120    avg_loss:0.040, val_acc:0.965]
Epoch [69/120    avg_loss:0.036, val_acc:0.973]
Epoch [70/120    avg_loss:0.029, val_acc:0.969]
Epoch [71/120    avg_loss:0.030, val_acc:0.962]
Epoch [72/120    avg_loss:0.029, val_acc:0.972]
Epoch [73/120    avg_loss:0.032, val_acc:0.975]
Epoch [74/120    avg_loss:0.025, val_acc:0.967]
Epoch [75/120    avg_loss:0.029, val_acc:0.970]
Epoch [76/120    avg_loss:0.036, val_acc:0.955]
Epoch [77/120    avg_loss:0.034, val_acc:0.969]
Epoch [78/120    avg_loss:0.028, val_acc:0.969]
Epoch [79/120    avg_loss:0.029, val_acc:0.968]
Epoch [80/120    avg_loss:0.024, val_acc:0.969]
Epoch [81/120    avg_loss:0.024, val_acc:0.978]
Epoch [82/120    avg_loss:0.020, val_acc:0.981]
Epoch [83/120    avg_loss:0.030, val_acc:0.975]
Epoch [84/120    avg_loss:0.023, val_acc:0.975]
Epoch [85/120    avg_loss:0.026, val_acc:0.964]
Epoch [86/120    avg_loss:0.023, val_acc:0.976]
Epoch [87/120    avg_loss:0.019, val_acc:0.979]
Epoch [88/120    avg_loss:0.020, val_acc:0.980]
Epoch [89/120    avg_loss:0.026, val_acc:0.972]
Epoch [90/120    avg_loss:0.018, val_acc:0.974]
Epoch [91/120    avg_loss:0.020, val_acc:0.966]
Epoch [92/120    avg_loss:0.042, val_acc:0.950]
Epoch [93/120    avg_loss:0.078, val_acc:0.959]
Epoch [94/120    avg_loss:0.040, val_acc:0.972]
Epoch [95/120    avg_loss:0.026, val_acc:0.970]
Epoch [96/120    avg_loss:0.021, val_acc:0.978]
Epoch [97/120    avg_loss:0.019, val_acc:0.979]
Epoch [98/120    avg_loss:0.019, val_acc:0.980]
Epoch [99/120    avg_loss:0.014, val_acc:0.979]
Epoch [100/120    avg_loss:0.014, val_acc:0.978]
Epoch [101/120    avg_loss:0.017, val_acc:0.978]
Epoch [102/120    avg_loss:0.014, val_acc:0.979]
Epoch [103/120    avg_loss:0.013, val_acc:0.979]
Epoch [104/120    avg_loss:0.013, val_acc:0.979]
Epoch [105/120    avg_loss:0.016, val_acc:0.978]
Epoch [106/120    avg_loss:0.015, val_acc:0.978]
Epoch [107/120    avg_loss:0.011, val_acc:0.979]
Epoch [108/120    avg_loss:0.012, val_acc:0.980]
Epoch [109/120    avg_loss:0.012, val_acc:0.980]
Epoch [110/120    avg_loss:0.011, val_acc:0.980]
Epoch [111/120    avg_loss:0.016, val_acc:0.980]
Epoch [112/120    avg_loss:0.017, val_acc:0.980]
Epoch [113/120    avg_loss:0.014, val_acc:0.980]
Epoch [114/120    avg_loss:0.012, val_acc:0.980]
Epoch [115/120    avg_loss:0.015, val_acc:0.980]
Epoch [116/120    avg_loss:0.011, val_acc:0.980]
Epoch [117/120    avg_loss:0.015, val_acc:0.980]
Epoch [118/120    avg_loss:0.013, val_acc:0.980]
Epoch [119/120    avg_loss:0.011, val_acc:0.980]
Epoch [120/120    avg_loss:0.014, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    4    6    0    3    0    0    0    1   10    0    0
     0    0    0]
 [   0    0    1  720    0    0    3    0    0   15    2    3    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    5    4    0    0    0  846   14    0    0
     0    0    0]
 [   0    0    1    0    0    0    2    0    0    0    6 2180   20    0
     0    1    0]
 [   0    0    0    0    0    1    0    0    0    0    1    0  527    0
     2    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    2    0    0
  1130    7    0]
 [   0    0    0    0    0    0   21    0    0    0    0    0    0    0
    54  272    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.70189701897019

F1 scores:
[       nan 0.98795181 0.98747063 0.9789259  0.98611111 0.98737084
 0.97473997 1.         0.99883586 0.69230769 0.97746967 0.98664856
 0.96875    1.         0.96995708 0.8676236  0.95808383]

Kappa:
0.9737951037055053
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9fd9b05e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.824, val_acc:0.034]
Epoch [2/120    avg_loss:2.769, val_acc:0.179]
Epoch [3/120    avg_loss:2.716, val_acc:0.215]
Epoch [4/120    avg_loss:2.644, val_acc:0.228]
Epoch [5/120    avg_loss:2.574, val_acc:0.260]
Epoch [6/120    avg_loss:2.482, val_acc:0.309]
Epoch [7/120    avg_loss:2.391, val_acc:0.381]
Epoch [8/120    avg_loss:2.326, val_acc:0.441]
Epoch [9/120    avg_loss:2.225, val_acc:0.511]
Epoch [10/120    avg_loss:2.104, val_acc:0.571]
Epoch [11/120    avg_loss:2.036, val_acc:0.577]
Epoch [12/120    avg_loss:1.930, val_acc:0.614]
Epoch [13/120    avg_loss:1.832, val_acc:0.629]
Epoch [14/120    avg_loss:1.686, val_acc:0.616]
Epoch [15/120    avg_loss:1.608, val_acc:0.651]
Epoch [16/120    avg_loss:1.424, val_acc:0.696]
Epoch [17/120    avg_loss:1.363, val_acc:0.707]
Epoch [18/120    avg_loss:1.218, val_acc:0.725]
Epoch [19/120    avg_loss:1.130, val_acc:0.753]
Epoch [20/120    avg_loss:0.971, val_acc:0.796]
Epoch [21/120    avg_loss:0.876, val_acc:0.787]
Epoch [22/120    avg_loss:0.739, val_acc:0.809]
Epoch [23/120    avg_loss:0.635, val_acc:0.806]
Epoch [24/120    avg_loss:0.559, val_acc:0.828]
Epoch [25/120    avg_loss:0.591, val_acc:0.834]
Epoch [26/120    avg_loss:0.507, val_acc:0.833]
Epoch [27/120    avg_loss:0.458, val_acc:0.845]
Epoch [28/120    avg_loss:0.385, val_acc:0.849]
Epoch [29/120    avg_loss:0.421, val_acc:0.844]
Epoch [30/120    avg_loss:0.504, val_acc:0.832]
Epoch [31/120    avg_loss:0.400, val_acc:0.869]
Epoch [32/120    avg_loss:0.332, val_acc:0.873]
Epoch [33/120    avg_loss:0.286, val_acc:0.886]
Epoch [34/120    avg_loss:0.251, val_acc:0.919]
Epoch [35/120    avg_loss:0.197, val_acc:0.909]
Epoch [36/120    avg_loss:0.220, val_acc:0.914]
Epoch [37/120    avg_loss:0.180, val_acc:0.922]
Epoch [38/120    avg_loss:0.159, val_acc:0.918]
Epoch [39/120    avg_loss:0.163, val_acc:0.931]
Epoch [40/120    avg_loss:0.139, val_acc:0.929]
Epoch [41/120    avg_loss:0.130, val_acc:0.944]
Epoch [42/120    avg_loss:0.107, val_acc:0.948]
Epoch [43/120    avg_loss:0.205, val_acc:0.917]
Epoch [44/120    avg_loss:0.331, val_acc:0.889]
Epoch [45/120    avg_loss:0.200, val_acc:0.898]
Epoch [46/120    avg_loss:0.147, val_acc:0.919]
Epoch [47/120    avg_loss:0.154, val_acc:0.934]
Epoch [48/120    avg_loss:0.130, val_acc:0.952]
Epoch [49/120    avg_loss:0.094, val_acc:0.951]
Epoch [50/120    avg_loss:0.105, val_acc:0.939]
Epoch [51/120    avg_loss:0.112, val_acc:0.943]
Epoch [52/120    avg_loss:0.111, val_acc:0.954]
Epoch [53/120    avg_loss:0.068, val_acc:0.953]
Epoch [54/120    avg_loss:0.058, val_acc:0.958]
Epoch [55/120    avg_loss:0.052, val_acc:0.957]
Epoch [56/120    avg_loss:0.047, val_acc:0.966]
Epoch [57/120    avg_loss:0.059, val_acc:0.960]
Epoch [58/120    avg_loss:0.073, val_acc:0.959]
Epoch [59/120    avg_loss:0.058, val_acc:0.959]
Epoch [60/120    avg_loss:0.069, val_acc:0.955]
Epoch [61/120    avg_loss:0.049, val_acc:0.967]
Epoch [62/120    avg_loss:0.053, val_acc:0.968]
Epoch [63/120    avg_loss:0.040, val_acc:0.968]
Epoch [64/120    avg_loss:0.040, val_acc:0.965]
Epoch [65/120    avg_loss:0.043, val_acc:0.965]
Epoch [66/120    avg_loss:0.042, val_acc:0.967]
Epoch [67/120    avg_loss:0.062, val_acc:0.972]
Epoch [68/120    avg_loss:0.061, val_acc:0.970]
Epoch [69/120    avg_loss:0.045, val_acc:0.958]
Epoch [70/120    avg_loss:0.031, val_acc:0.967]
Epoch [71/120    avg_loss:0.031, val_acc:0.970]
Epoch [72/120    avg_loss:0.022, val_acc:0.970]
Epoch [73/120    avg_loss:0.028, val_acc:0.955]
Epoch [74/120    avg_loss:0.024, val_acc:0.974]
Epoch [75/120    avg_loss:0.020, val_acc:0.976]
Epoch [76/120    avg_loss:0.026, val_acc:0.976]
Epoch [77/120    avg_loss:0.024, val_acc:0.971]
Epoch [78/120    avg_loss:0.039, val_acc:0.963]
Epoch [79/120    avg_loss:0.034, val_acc:0.968]
Epoch [80/120    avg_loss:0.028, val_acc:0.968]
Epoch [81/120    avg_loss:0.023, val_acc:0.975]
Epoch [82/120    avg_loss:0.017, val_acc:0.973]
Epoch [83/120    avg_loss:0.022, val_acc:0.976]
Epoch [84/120    avg_loss:0.019, val_acc:0.975]
Epoch [85/120    avg_loss:0.021, val_acc:0.975]
Epoch [86/120    avg_loss:0.017, val_acc:0.973]
Epoch [87/120    avg_loss:0.031, val_acc:0.971]
Epoch [88/120    avg_loss:0.026, val_acc:0.973]
Epoch [89/120    avg_loss:0.033, val_acc:0.961]
Epoch [90/120    avg_loss:0.025, val_acc:0.972]
Epoch [91/120    avg_loss:0.019, val_acc:0.972]
Epoch [92/120    avg_loss:0.016, val_acc:0.974]
Epoch [93/120    avg_loss:0.020, val_acc:0.972]
Epoch [94/120    avg_loss:0.014, val_acc:0.975]
Epoch [95/120    avg_loss:0.013, val_acc:0.978]
Epoch [96/120    avg_loss:0.014, val_acc:0.977]
Epoch [97/120    avg_loss:0.010, val_acc:0.969]
Epoch [98/120    avg_loss:0.022, val_acc:0.969]
Epoch [99/120    avg_loss:0.145, val_acc:0.945]
Epoch [100/120    avg_loss:0.185, val_acc:0.914]
Epoch [101/120    avg_loss:0.152, val_acc:0.921]
Epoch [102/120    avg_loss:0.062, val_acc:0.958]
Epoch [103/120    avg_loss:0.070, val_acc:0.963]
Epoch [104/120    avg_loss:0.038, val_acc:0.961]
Epoch [105/120    avg_loss:0.039, val_acc:0.959]
Epoch [106/120    avg_loss:0.033, val_acc:0.966]
Epoch [107/120    avg_loss:0.027, val_acc:0.967]
Epoch [108/120    avg_loss:0.021, val_acc:0.970]
Epoch [109/120    avg_loss:0.018, val_acc:0.972]
Epoch [110/120    avg_loss:0.017, val_acc:0.971]
Epoch [111/120    avg_loss:0.021, val_acc:0.971]
Epoch [112/120    avg_loss:0.014, val_acc:0.972]
Epoch [113/120    avg_loss:0.013, val_acc:0.971]
Epoch [114/120    avg_loss:0.015, val_acc:0.974]
Epoch [115/120    avg_loss:0.014, val_acc:0.974]
Epoch [116/120    avg_loss:0.011, val_acc:0.973]
Epoch [117/120    avg_loss:0.012, val_acc:0.973]
Epoch [118/120    avg_loss:0.013, val_acc:0.972]
Epoch [119/120    avg_loss:0.012, val_acc:0.973]
Epoch [120/120    avg_loss:0.011, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1258    1    9    0    0    0    0    0    5   12    0    0
     0    0    0]
 [   0    0    0  722   12    0    2    0    0    0    4    3    4    0
     0    0    0]
 [   0    0    0    0  210    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    0    1    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0    8    0    0    0    0    0    0    0  851   16    0    0
     0    0    0]
 [   0    0   20    0    0    0    4    0    0    1   28 2145   11    0
     1    0    0]
 [   0    0    0    0    3    0    0    0    0    0    3    2  519    0
     6    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1133    5    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    61  282    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.33333333333333

F1 scores:
[       nan 0.94117647 0.97822706 0.98231293 0.93959732 0.99188876
 0.99093656 0.98039216 0.9953271  0.91428571 0.96321449 0.97766636
 0.96648045 1.         0.9658994  0.88958991 0.98809524]

Kappa:
0.969599598439008
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7d7f6fbf28>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.821, val_acc:0.134]
Epoch [2/120    avg_loss:2.754, val_acc:0.239]
Epoch [3/120    avg_loss:2.677, val_acc:0.373]
Epoch [4/120    avg_loss:2.596, val_acc:0.380]
Epoch [5/120    avg_loss:2.502, val_acc:0.429]
Epoch [6/120    avg_loss:2.441, val_acc:0.441]
Epoch [7/120    avg_loss:2.343, val_acc:0.465]
Epoch [8/120    avg_loss:2.282, val_acc:0.503]
Epoch [9/120    avg_loss:2.233, val_acc:0.536]
Epoch [10/120    avg_loss:2.172, val_acc:0.573]
Epoch [11/120    avg_loss:2.067, val_acc:0.541]
Epoch [12/120    avg_loss:2.005, val_acc:0.606]
Epoch [13/120    avg_loss:1.912, val_acc:0.618]
Epoch [14/120    avg_loss:1.803, val_acc:0.636]
Epoch [15/120    avg_loss:1.695, val_acc:0.658]
Epoch [16/120    avg_loss:1.572, val_acc:0.652]
Epoch [17/120    avg_loss:1.462, val_acc:0.745]
Epoch [18/120    avg_loss:1.278, val_acc:0.717]
Epoch [19/120    avg_loss:1.187, val_acc:0.743]
Epoch [20/120    avg_loss:1.078, val_acc:0.780]
Epoch [21/120    avg_loss:0.941, val_acc:0.813]
Epoch [22/120    avg_loss:0.862, val_acc:0.817]
Epoch [23/120    avg_loss:0.763, val_acc:0.838]
Epoch [24/120    avg_loss:0.715, val_acc:0.859]
Epoch [25/120    avg_loss:0.712, val_acc:0.801]
Epoch [26/120    avg_loss:0.734, val_acc:0.859]
Epoch [27/120    avg_loss:0.536, val_acc:0.817]
Epoch [28/120    avg_loss:0.493, val_acc:0.838]
Epoch [29/120    avg_loss:0.470, val_acc:0.843]
Epoch [30/120    avg_loss:0.393, val_acc:0.868]
Epoch [31/120    avg_loss:0.383, val_acc:0.861]
Epoch [32/120    avg_loss:0.386, val_acc:0.884]
Epoch [33/120    avg_loss:0.354, val_acc:0.895]
Epoch [34/120    avg_loss:0.247, val_acc:0.874]
Epoch [35/120    avg_loss:0.204, val_acc:0.884]
Epoch [36/120    avg_loss:0.211, val_acc:0.918]
Epoch [37/120    avg_loss:0.234, val_acc:0.885]
Epoch [38/120    avg_loss:0.181, val_acc:0.924]
Epoch [39/120    avg_loss:0.136, val_acc:0.928]
Epoch [40/120    avg_loss:0.130, val_acc:0.936]
Epoch [41/120    avg_loss:0.116, val_acc:0.939]
Epoch [42/120    avg_loss:0.119, val_acc:0.948]
Epoch [43/120    avg_loss:0.123, val_acc:0.940]
Epoch [44/120    avg_loss:0.103, val_acc:0.947]
Epoch [45/120    avg_loss:0.107, val_acc:0.928]
Epoch [46/120    avg_loss:0.097, val_acc:0.960]
Epoch [47/120    avg_loss:0.082, val_acc:0.948]
Epoch [48/120    avg_loss:0.085, val_acc:0.948]
Epoch [49/120    avg_loss:0.121, val_acc:0.849]
Epoch [50/120    avg_loss:1.773, val_acc:0.618]
Epoch [51/120    avg_loss:1.203, val_acc:0.788]
Epoch [52/120    avg_loss:0.585, val_acc:0.627]
Epoch [53/120    avg_loss:0.449, val_acc:0.897]
Epoch [54/120    avg_loss:0.265, val_acc:0.889]
Epoch [55/120    avg_loss:0.212, val_acc:0.908]
Epoch [56/120    avg_loss:0.156, val_acc:0.925]
Epoch [57/120    avg_loss:0.158, val_acc:0.941]
Epoch [58/120    avg_loss:0.109, val_acc:0.938]
Epoch [59/120    avg_loss:0.129, val_acc:0.936]
Epoch [60/120    avg_loss:0.086, val_acc:0.938]
Epoch [61/120    avg_loss:0.078, val_acc:0.946]
Epoch [62/120    avg_loss:0.067, val_acc:0.952]
Epoch [63/120    avg_loss:0.072, val_acc:0.952]
Epoch [64/120    avg_loss:0.065, val_acc:0.953]
Epoch [65/120    avg_loss:0.068, val_acc:0.952]
Epoch [66/120    avg_loss:0.056, val_acc:0.952]
Epoch [67/120    avg_loss:0.058, val_acc:0.955]
Epoch [68/120    avg_loss:0.058, val_acc:0.958]
Epoch [69/120    avg_loss:0.062, val_acc:0.958]
Epoch [70/120    avg_loss:0.057, val_acc:0.962]
Epoch [71/120    avg_loss:0.055, val_acc:0.956]
Epoch [72/120    avg_loss:0.056, val_acc:0.960]
Epoch [73/120    avg_loss:0.059, val_acc:0.962]
Epoch [74/120    avg_loss:0.050, val_acc:0.962]
Epoch [75/120    avg_loss:0.052, val_acc:0.962]
Epoch [76/120    avg_loss:0.052, val_acc:0.959]
Epoch [77/120    avg_loss:0.044, val_acc:0.959]
Epoch [78/120    avg_loss:0.048, val_acc:0.962]
Epoch [79/120    avg_loss:0.045, val_acc:0.962]
Epoch [80/120    avg_loss:0.045, val_acc:0.959]
Epoch [81/120    avg_loss:0.047, val_acc:0.960]
Epoch [82/120    avg_loss:0.048, val_acc:0.957]
Epoch [83/120    avg_loss:0.046, val_acc:0.956]
Epoch [84/120    avg_loss:0.047, val_acc:0.959]
Epoch [85/120    avg_loss:0.042, val_acc:0.957]
Epoch [86/120    avg_loss:0.051, val_acc:0.964]
Epoch [87/120    avg_loss:0.043, val_acc:0.960]
Epoch [88/120    avg_loss:0.044, val_acc:0.963]
Epoch [89/120    avg_loss:0.038, val_acc:0.962]
Epoch [90/120    avg_loss:0.053, val_acc:0.960]
Epoch [91/120    avg_loss:0.046, val_acc:0.959]
Epoch [92/120    avg_loss:0.047, val_acc:0.960]
Epoch [93/120    avg_loss:0.043, val_acc:0.960]
Epoch [94/120    avg_loss:0.043, val_acc:0.967]
Epoch [95/120    avg_loss:0.041, val_acc:0.964]
Epoch [96/120    avg_loss:0.039, val_acc:0.963]
Epoch [97/120    avg_loss:0.040, val_acc:0.963]
Epoch [98/120    avg_loss:0.050, val_acc:0.963]
Epoch [99/120    avg_loss:0.040, val_acc:0.962]
Epoch [100/120    avg_loss:0.039, val_acc:0.963]
Epoch [101/120    avg_loss:0.037, val_acc:0.963]
Epoch [102/120    avg_loss:0.041, val_acc:0.964]
Epoch [103/120    avg_loss:0.038, val_acc:0.964]
Epoch [104/120    avg_loss:0.039, val_acc:0.963]
Epoch [105/120    avg_loss:0.037, val_acc:0.960]
Epoch [106/120    avg_loss:0.035, val_acc:0.967]
Epoch [107/120    avg_loss:0.041, val_acc:0.965]
Epoch [108/120    avg_loss:0.037, val_acc:0.966]
Epoch [109/120    avg_loss:0.039, val_acc:0.965]
Epoch [110/120    avg_loss:0.032, val_acc:0.965]
Epoch [111/120    avg_loss:0.040, val_acc:0.965]
Epoch [112/120    avg_loss:0.036, val_acc:0.966]
Epoch [113/120    avg_loss:0.034, val_acc:0.959]
Epoch [114/120    avg_loss:0.034, val_acc:0.965]
Epoch [115/120    avg_loss:0.037, val_acc:0.969]
Epoch [116/120    avg_loss:0.037, val_acc:0.965]
Epoch [117/120    avg_loss:0.035, val_acc:0.966]
Epoch [118/120    avg_loss:0.033, val_acc:0.968]
Epoch [119/120    avg_loss:0.031, val_acc:0.967]
Epoch [120/120    avg_loss:0.034, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1244    1    4    0    0    0    0    0    8   27    1    0
     0    0    0]
 [   0    0    1  719    2    0    2    0    0   12    1    0   10    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  417    3    0    0    0    0    1    0    0
    14    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   10    0    0    0    3    0    0    0  845   13    3    0
     0    1    0]
 [   0    0    6    0    0    0    2    0    0    0   16 2160   24    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    1    2    0  528    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0   33    0    0    0    0    0    0    0
    49  265    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.04065040650407

F1 scores:
[       nan 1.         0.97721917 0.97956403 0.9837587  0.97887324
 0.96678967 1.         1.         0.70833333 0.96737264 0.97914778
 0.95825771 1.         0.96652361 0.84529506 0.97647059]

Kappa:
0.9662588634917962
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb26b69df60>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.815, val_acc:0.130]
Epoch [2/120    avg_loss:2.732, val_acc:0.236]
Epoch [3/120    avg_loss:2.660, val_acc:0.317]
Epoch [4/120    avg_loss:2.590, val_acc:0.467]
Epoch [5/120    avg_loss:2.500, val_acc:0.491]
Epoch [6/120    avg_loss:2.419, val_acc:0.515]
Epoch [7/120    avg_loss:2.357, val_acc:0.513]
Epoch [8/120    avg_loss:2.274, val_acc:0.544]
Epoch [9/120    avg_loss:2.212, val_acc:0.582]
Epoch [10/120    avg_loss:2.099, val_acc:0.548]
Epoch [11/120    avg_loss:2.068, val_acc:0.621]
Epoch [12/120    avg_loss:1.966, val_acc:0.641]
Epoch [13/120    avg_loss:1.886, val_acc:0.644]
Epoch [14/120    avg_loss:1.822, val_acc:0.655]
Epoch [15/120    avg_loss:1.749, val_acc:0.658]
Epoch [16/120    avg_loss:1.650, val_acc:0.683]
Epoch [17/120    avg_loss:1.519, val_acc:0.709]
Epoch [18/120    avg_loss:1.368, val_acc:0.763]
Epoch [19/120    avg_loss:1.245, val_acc:0.758]
Epoch [20/120    avg_loss:1.144, val_acc:0.758]
Epoch [21/120    avg_loss:1.031, val_acc:0.797]
Epoch [22/120    avg_loss:0.916, val_acc:0.809]
Epoch [23/120    avg_loss:0.864, val_acc:0.784]
Epoch [24/120    avg_loss:0.765, val_acc:0.839]
Epoch [25/120    avg_loss:0.670, val_acc:0.872]
Epoch [26/120    avg_loss:0.587, val_acc:0.865]
Epoch [27/120    avg_loss:0.520, val_acc:0.875]
Epoch [28/120    avg_loss:0.473, val_acc:0.880]
Epoch [29/120    avg_loss:0.407, val_acc:0.889]
Epoch [30/120    avg_loss:0.391, val_acc:0.897]
Epoch [31/120    avg_loss:0.337, val_acc:0.910]
Epoch [32/120    avg_loss:0.278, val_acc:0.904]
Epoch [33/120    avg_loss:0.269, val_acc:0.918]
Epoch [34/120    avg_loss:0.233, val_acc:0.938]
Epoch [35/120    avg_loss:0.197, val_acc:0.920]
Epoch [36/120    avg_loss:0.178, val_acc:0.934]
Epoch [37/120    avg_loss:0.217, val_acc:0.888]
Epoch [38/120    avg_loss:0.209, val_acc:0.925]
Epoch [39/120    avg_loss:0.182, val_acc:0.927]
Epoch [40/120    avg_loss:0.145, val_acc:0.942]
Epoch [41/120    avg_loss:0.139, val_acc:0.938]
Epoch [42/120    avg_loss:0.117, val_acc:0.958]
Epoch [43/120    avg_loss:0.097, val_acc:0.950]
Epoch [44/120    avg_loss:0.102, val_acc:0.962]
Epoch [45/120    avg_loss:0.151, val_acc:0.943]
Epoch [46/120    avg_loss:0.107, val_acc:0.950]
Epoch [47/120    avg_loss:0.098, val_acc:0.948]
Epoch [48/120    avg_loss:0.102, val_acc:0.951]
Epoch [49/120    avg_loss:0.086, val_acc:0.951]
Epoch [50/120    avg_loss:0.076, val_acc:0.941]
Epoch [51/120    avg_loss:0.089, val_acc:0.918]
Epoch [52/120    avg_loss:0.106, val_acc:0.959]
Epoch [53/120    avg_loss:0.079, val_acc:0.963]
Epoch [54/120    avg_loss:0.099, val_acc:0.913]
Epoch [55/120    avg_loss:0.098, val_acc:0.950]
Epoch [56/120    avg_loss:0.067, val_acc:0.957]
Epoch [57/120    avg_loss:0.055, val_acc:0.962]
Epoch [58/120    avg_loss:0.052, val_acc:0.967]
Epoch [59/120    avg_loss:0.042, val_acc:0.974]
Epoch [60/120    avg_loss:0.039, val_acc:0.967]
Epoch [61/120    avg_loss:0.043, val_acc:0.944]
Epoch [62/120    avg_loss:0.039, val_acc:0.969]
Epoch [63/120    avg_loss:0.029, val_acc:0.966]
Epoch [64/120    avg_loss:0.038, val_acc:0.967]
Epoch [65/120    avg_loss:0.033, val_acc:0.971]
Epoch [66/120    avg_loss:0.032, val_acc:0.970]
Epoch [67/120    avg_loss:0.042, val_acc:0.976]
Epoch [68/120    avg_loss:0.032, val_acc:0.982]
Epoch [69/120    avg_loss:0.037, val_acc:0.973]
Epoch [70/120    avg_loss:0.029, val_acc:0.975]
Epoch [71/120    avg_loss:0.028, val_acc:0.973]
Epoch [72/120    avg_loss:0.030, val_acc:0.976]
Epoch [73/120    avg_loss:0.023, val_acc:0.971]
Epoch [74/120    avg_loss:0.021, val_acc:0.973]
Epoch [75/120    avg_loss:0.053, val_acc:0.952]
Epoch [76/120    avg_loss:0.076, val_acc:0.958]
Epoch [77/120    avg_loss:0.058, val_acc:0.958]
Epoch [78/120    avg_loss:0.040, val_acc:0.967]
Epoch [79/120    avg_loss:0.038, val_acc:0.965]
Epoch [80/120    avg_loss:0.034, val_acc:0.975]
Epoch [81/120    avg_loss:0.064, val_acc:0.951]
Epoch [82/120    avg_loss:0.092, val_acc:0.966]
Epoch [83/120    avg_loss:0.052, val_acc:0.971]
Epoch [84/120    avg_loss:0.036, val_acc:0.976]
Epoch [85/120    avg_loss:0.034, val_acc:0.974]
Epoch [86/120    avg_loss:0.030, val_acc:0.975]
Epoch [87/120    avg_loss:0.028, val_acc:0.970]
Epoch [88/120    avg_loss:0.030, val_acc:0.973]
Epoch [89/120    avg_loss:0.029, val_acc:0.976]
Epoch [90/120    avg_loss:0.024, val_acc:0.976]
Epoch [91/120    avg_loss:0.020, val_acc:0.978]
Epoch [92/120    avg_loss:0.020, val_acc:0.979]
Epoch [93/120    avg_loss:0.020, val_acc:0.977]
Epoch [94/120    avg_loss:0.018, val_acc:0.982]
Epoch [95/120    avg_loss:0.017, val_acc:0.979]
Epoch [96/120    avg_loss:0.027, val_acc:0.977]
Epoch [97/120    avg_loss:0.017, val_acc:0.979]
Epoch [98/120    avg_loss:0.021, val_acc:0.979]
Epoch [99/120    avg_loss:0.018, val_acc:0.979]
Epoch [100/120    avg_loss:0.023, val_acc:0.979]
Epoch [101/120    avg_loss:0.015, val_acc:0.980]
Epoch [102/120    avg_loss:0.016, val_acc:0.981]
Epoch [103/120    avg_loss:0.020, val_acc:0.981]
Epoch [104/120    avg_loss:0.015, val_acc:0.982]
Epoch [105/120    avg_loss:0.018, val_acc:0.981]
Epoch [106/120    avg_loss:0.015, val_acc:0.981]
Epoch [107/120    avg_loss:0.022, val_acc:0.980]
Epoch [108/120    avg_loss:0.016, val_acc:0.982]
Epoch [109/120    avg_loss:0.015, val_acc:0.982]
Epoch [110/120    avg_loss:0.014, val_acc:0.982]
Epoch [111/120    avg_loss:0.020, val_acc:0.981]
Epoch [112/120    avg_loss:0.015, val_acc:0.980]
Epoch [113/120    avg_loss:0.014, val_acc:0.980]
Epoch [114/120    avg_loss:0.018, val_acc:0.980]
Epoch [115/120    avg_loss:0.016, val_acc:0.980]
Epoch [116/120    avg_loss:0.018, val_acc:0.979]
Epoch [117/120    avg_loss:0.014, val_acc:0.978]
Epoch [118/120    avg_loss:0.013, val_acc:0.980]
Epoch [119/120    avg_loss:0.016, val_acc:0.982]
Epoch [120/120    avg_loss:0.016, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1236    2    6    0    0    0    0    0   14   27    0    0
     0    0    0]
 [   0    0    7  698    1    2    0    0    0   22    0    1   16    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    9    0    0    9    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    4    0    0    0    0  854   13    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    1   10 2183   15    0
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    4    2  521    0
     0    0    1]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1135    4    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    15  320    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.71273712737127

F1 scores:
[       nan 0.92857143 0.97591788 0.96475466 0.98383372 0.98181818
 0.98350825 1.         0.9953271  0.36       0.97100625 0.98422002
 0.95860166 0.99728997 0.98996947 0.9538003  0.98809524]

Kappa:
0.9739240903305761
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb38af8def0>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.826, val_acc:0.116]
Epoch [2/120    avg_loss:2.787, val_acc:0.186]
Epoch [3/120    avg_loss:2.746, val_acc:0.228]
Epoch [4/120    avg_loss:2.698, val_acc:0.312]
Epoch [5/120    avg_loss:2.646, val_acc:0.325]
Epoch [6/120    avg_loss:2.580, val_acc:0.384]
Epoch [7/120    avg_loss:2.528, val_acc:0.393]
Epoch [8/120    avg_loss:2.445, val_acc:0.376]
Epoch [9/120    avg_loss:2.362, val_acc:0.410]
Epoch [10/120    avg_loss:2.293, val_acc:0.471]
Epoch [11/120    avg_loss:2.198, val_acc:0.560]
Epoch [12/120    avg_loss:2.113, val_acc:0.562]
Epoch [13/120    avg_loss:2.037, val_acc:0.529]
Epoch [14/120    avg_loss:1.925, val_acc:0.571]
Epoch [15/120    avg_loss:1.882, val_acc:0.606]
Epoch [16/120    avg_loss:1.743, val_acc:0.625]
Epoch [17/120    avg_loss:1.690, val_acc:0.655]
Epoch [18/120    avg_loss:1.506, val_acc:0.713]
Epoch [19/120    avg_loss:1.465, val_acc:0.688]
Epoch [20/120    avg_loss:1.321, val_acc:0.716]
Epoch [21/120    avg_loss:1.210, val_acc:0.742]
Epoch [22/120    avg_loss:1.104, val_acc:0.738]
Epoch [23/120    avg_loss:1.159, val_acc:0.706]
Epoch [24/120    avg_loss:1.042, val_acc:0.759]
Epoch [25/120    avg_loss:0.824, val_acc:0.770]
Epoch [26/120    avg_loss:0.774, val_acc:0.784]
Epoch [27/120    avg_loss:0.711, val_acc:0.783]
Epoch [28/120    avg_loss:0.670, val_acc:0.810]
Epoch [29/120    avg_loss:0.614, val_acc:0.809]
Epoch [30/120    avg_loss:0.574, val_acc:0.827]
Epoch [31/120    avg_loss:0.513, val_acc:0.851]
Epoch [32/120    avg_loss:0.419, val_acc:0.872]
Epoch [33/120    avg_loss:0.455, val_acc:0.825]
Epoch [34/120    avg_loss:0.413, val_acc:0.872]
Epoch [35/120    avg_loss:0.348, val_acc:0.896]
Epoch [36/120    avg_loss:0.350, val_acc:0.880]
Epoch [37/120    avg_loss:0.290, val_acc:0.913]
Epoch [38/120    avg_loss:0.261, val_acc:0.906]
Epoch [39/120    avg_loss:0.275, val_acc:0.876]
Epoch [40/120    avg_loss:0.241, val_acc:0.912]
Epoch [41/120    avg_loss:0.234, val_acc:0.924]
Epoch [42/120    avg_loss:0.200, val_acc:0.928]
Epoch [43/120    avg_loss:0.147, val_acc:0.939]
Epoch [44/120    avg_loss:0.173, val_acc:0.920]
Epoch [45/120    avg_loss:0.146, val_acc:0.938]
Epoch [46/120    avg_loss:0.169, val_acc:0.917]
Epoch [47/120    avg_loss:0.150, val_acc:0.932]
Epoch [48/120    avg_loss:0.131, val_acc:0.935]
Epoch [49/120    avg_loss:0.119, val_acc:0.927]
Epoch [50/120    avg_loss:0.120, val_acc:0.950]
Epoch [51/120    avg_loss:0.127, val_acc:0.917]
Epoch [52/120    avg_loss:0.122, val_acc:0.947]
Epoch [53/120    avg_loss:0.088, val_acc:0.957]
Epoch [54/120    avg_loss:0.086, val_acc:0.951]
Epoch [55/120    avg_loss:0.081, val_acc:0.948]
Epoch [56/120    avg_loss:0.082, val_acc:0.952]
Epoch [57/120    avg_loss:0.087, val_acc:0.963]
Epoch [58/120    avg_loss:0.095, val_acc:0.935]
Epoch [59/120    avg_loss:0.099, val_acc:0.958]
Epoch [60/120    avg_loss:0.063, val_acc:0.963]
Epoch [61/120    avg_loss:0.081, val_acc:0.946]
Epoch [62/120    avg_loss:0.092, val_acc:0.928]
Epoch [63/120    avg_loss:0.073, val_acc:0.962]
Epoch [64/120    avg_loss:0.064, val_acc:0.954]
Epoch [65/120    avg_loss:0.077, val_acc:0.959]
Epoch [66/120    avg_loss:0.071, val_acc:0.955]
Epoch [67/120    avg_loss:0.073, val_acc:0.925]
Epoch [68/120    avg_loss:0.056, val_acc:0.953]
Epoch [69/120    avg_loss:0.103, val_acc:0.951]
Epoch [70/120    avg_loss:0.106, val_acc:0.944]
Epoch [71/120    avg_loss:0.105, val_acc:0.954]
Epoch [72/120    avg_loss:0.080, val_acc:0.954]
Epoch [73/120    avg_loss:0.070, val_acc:0.969]
Epoch [74/120    avg_loss:0.070, val_acc:0.957]
Epoch [75/120    avg_loss:0.056, val_acc:0.974]
Epoch [76/120    avg_loss:0.050, val_acc:0.969]
Epoch [77/120    avg_loss:0.045, val_acc:0.963]
Epoch [78/120    avg_loss:0.032, val_acc:0.962]
Epoch [79/120    avg_loss:0.059, val_acc:0.957]
Epoch [80/120    avg_loss:0.065, val_acc:0.962]
Epoch [81/120    avg_loss:0.035, val_acc:0.969]
Epoch [82/120    avg_loss:0.036, val_acc:0.969]
Epoch [83/120    avg_loss:0.036, val_acc:0.969]
Epoch [84/120    avg_loss:0.047, val_acc:0.976]
Epoch [85/120    avg_loss:0.054, val_acc:0.955]
Epoch [86/120    avg_loss:0.086, val_acc:0.933]
Epoch [87/120    avg_loss:0.071, val_acc:0.970]
Epoch [88/120    avg_loss:0.050, val_acc:0.974]
Epoch [89/120    avg_loss:0.036, val_acc:0.974]
Epoch [90/120    avg_loss:0.031, val_acc:0.968]
Epoch [91/120    avg_loss:0.025, val_acc:0.974]
Epoch [92/120    avg_loss:0.027, val_acc:0.977]
Epoch [93/120    avg_loss:0.030, val_acc:0.968]
Epoch [94/120    avg_loss:0.021, val_acc:0.978]
Epoch [95/120    avg_loss:0.019, val_acc:0.971]
Epoch [96/120    avg_loss:0.019, val_acc:0.979]
Epoch [97/120    avg_loss:0.025, val_acc:0.976]
Epoch [98/120    avg_loss:0.021, val_acc:0.968]
Epoch [99/120    avg_loss:0.033, val_acc:0.971]
Epoch [100/120    avg_loss:0.025, val_acc:0.973]
Epoch [101/120    avg_loss:0.019, val_acc:0.979]
Epoch [102/120    avg_loss:0.019, val_acc:0.984]
Epoch [103/120    avg_loss:0.018, val_acc:0.975]
Epoch [104/120    avg_loss:0.021, val_acc:0.971]
Epoch [105/120    avg_loss:0.026, val_acc:0.979]
Epoch [106/120    avg_loss:0.014, val_acc:0.981]
Epoch [107/120    avg_loss:0.014, val_acc:0.979]
Epoch [108/120    avg_loss:0.018, val_acc:0.976]
Epoch [109/120    avg_loss:0.013, val_acc:0.978]
Epoch [110/120    avg_loss:0.024, val_acc:0.979]
Epoch [111/120    avg_loss:0.018, val_acc:0.977]
Epoch [112/120    avg_loss:0.020, val_acc:0.978]
Epoch [113/120    avg_loss:0.017, val_acc:0.979]
Epoch [114/120    avg_loss:0.012, val_acc:0.982]
Epoch [115/120    avg_loss:0.018, val_acc:0.977]
Epoch [116/120    avg_loss:0.015, val_acc:0.981]
Epoch [117/120    avg_loss:0.011, val_acc:0.985]
Epoch [118/120    avg_loss:0.009, val_acc:0.985]
Epoch [119/120    avg_loss:0.010, val_acc:0.984]
Epoch [120/120    avg_loss:0.009, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1264    1    0    0    1    0    0    0    3   16    0    0
     0    0    0]
 [   0    0    0  725    1    3    0    0    0   14    2    0    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   13    0    0    4    0    0    0    0  849    8    0    0
     0    1    0]
 [   0    0    7    0    0    0    2    0    0    0   12 2183    6    0
     0    0    0]
 [   0    0    0    3    0    5    0    0    0    4    3    1  517    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1134    5    0]
 [   0    0    0    0    0    0   40    0    0    0    0    0    0    0
    28  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.92953929539296

F1 scores:
[       nan 0.98765432 0.98404048 0.98238482 0.99765808 0.98524404
 0.96683861 1.         1.         0.62962963 0.9730659  0.98800634
 0.9754717  1.         0.98565841 0.88291139 0.98809524]

Kappa:
0.9763908742654319
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb3a7303f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.811, val_acc:0.076]
Epoch [2/120    avg_loss:2.732, val_acc:0.183]
Epoch [3/120    avg_loss:2.651, val_acc:0.228]
Epoch [4/120    avg_loss:2.557, val_acc:0.229]
Epoch [5/120    avg_loss:2.488, val_acc:0.276]
Epoch [6/120    avg_loss:2.409, val_acc:0.279]
Epoch [7/120    avg_loss:2.316, val_acc:0.405]
Epoch [8/120    avg_loss:2.272, val_acc:0.438]
Epoch [9/120    avg_loss:2.218, val_acc:0.468]
Epoch [10/120    avg_loss:2.147, val_acc:0.490]
Epoch [11/120    avg_loss:2.069, val_acc:0.569]
Epoch [12/120    avg_loss:1.993, val_acc:0.592]
Epoch [13/120    avg_loss:1.935, val_acc:0.629]
Epoch [14/120    avg_loss:1.844, val_acc:0.655]
Epoch [15/120    avg_loss:1.756, val_acc:0.668]
Epoch [16/120    avg_loss:1.667, val_acc:0.664]
Epoch [17/120    avg_loss:1.509, val_acc:0.675]
Epoch [18/120    avg_loss:1.341, val_acc:0.719]
Epoch [19/120    avg_loss:1.320, val_acc:0.714]
Epoch [20/120    avg_loss:1.189, val_acc:0.746]
Epoch [21/120    avg_loss:1.036, val_acc:0.742]
Epoch [22/120    avg_loss:0.964, val_acc:0.743]
Epoch [23/120    avg_loss:0.887, val_acc:0.780]
Epoch [24/120    avg_loss:0.791, val_acc:0.789]
Epoch [25/120    avg_loss:0.683, val_acc:0.820]
Epoch [26/120    avg_loss:0.627, val_acc:0.862]
Epoch [27/120    avg_loss:0.573, val_acc:0.865]
Epoch [28/120    avg_loss:0.571, val_acc:0.845]
Epoch [29/120    avg_loss:0.556, val_acc:0.866]
Epoch [30/120    avg_loss:0.458, val_acc:0.843]
Epoch [31/120    avg_loss:0.423, val_acc:0.844]
Epoch [32/120    avg_loss:0.405, val_acc:0.885]
Epoch [33/120    avg_loss:0.361, val_acc:0.874]
Epoch [34/120    avg_loss:0.345, val_acc:0.895]
Epoch [35/120    avg_loss:0.287, val_acc:0.916]
Epoch [36/120    avg_loss:0.235, val_acc:0.913]
Epoch [37/120    avg_loss:0.213, val_acc:0.907]
Epoch [38/120    avg_loss:0.224, val_acc:0.929]
Epoch [39/120    avg_loss:0.201, val_acc:0.928]
Epoch [40/120    avg_loss:0.171, val_acc:0.933]
Epoch [41/120    avg_loss:0.155, val_acc:0.913]
Epoch [42/120    avg_loss:0.143, val_acc:0.932]
Epoch [43/120    avg_loss:0.161, val_acc:0.913]
Epoch [44/120    avg_loss:0.157, val_acc:0.934]
Epoch [45/120    avg_loss:0.149, val_acc:0.947]
Epoch [46/120    avg_loss:0.124, val_acc:0.947]
Epoch [47/120    avg_loss:0.116, val_acc:0.927]
Epoch [48/120    avg_loss:0.112, val_acc:0.945]
Epoch [49/120    avg_loss:0.083, val_acc:0.947]
Epoch [50/120    avg_loss:0.090, val_acc:0.950]
Epoch [51/120    avg_loss:0.068, val_acc:0.961]
Epoch [52/120    avg_loss:0.069, val_acc:0.961]
Epoch [53/120    avg_loss:0.085, val_acc:0.954]
Epoch [54/120    avg_loss:0.086, val_acc:0.934]
Epoch [55/120    avg_loss:0.077, val_acc:0.947]
Epoch [56/120    avg_loss:0.083, val_acc:0.930]
Epoch [57/120    avg_loss:0.075, val_acc:0.952]
Epoch [58/120    avg_loss:0.066, val_acc:0.951]
Epoch [59/120    avg_loss:0.074, val_acc:0.967]
Epoch [60/120    avg_loss:0.075, val_acc:0.965]
Epoch [61/120    avg_loss:0.059, val_acc:0.964]
Epoch [62/120    avg_loss:0.075, val_acc:0.957]
Epoch [63/120    avg_loss:0.067, val_acc:0.938]
Epoch [64/120    avg_loss:0.131, val_acc:0.930]
Epoch [65/120    avg_loss:0.077, val_acc:0.957]
Epoch [66/120    avg_loss:0.045, val_acc:0.958]
Epoch [67/120    avg_loss:0.049, val_acc:0.964]
Epoch [68/120    avg_loss:0.033, val_acc:0.965]
Epoch [69/120    avg_loss:0.053, val_acc:0.955]
Epoch [70/120    avg_loss:0.043, val_acc:0.968]
Epoch [71/120    avg_loss:0.040, val_acc:0.964]
Epoch [72/120    avg_loss:0.043, val_acc:0.954]
Epoch [73/120    avg_loss:0.044, val_acc:0.957]
Epoch [74/120    avg_loss:0.037, val_acc:0.971]
Epoch [75/120    avg_loss:0.032, val_acc:0.968]
Epoch [76/120    avg_loss:0.032, val_acc:0.973]
Epoch [77/120    avg_loss:0.026, val_acc:0.969]
Epoch [78/120    avg_loss:0.026, val_acc:0.963]
Epoch [79/120    avg_loss:0.031, val_acc:0.965]
Epoch [80/120    avg_loss:0.026, val_acc:0.975]
Epoch [81/120    avg_loss:0.017, val_acc:0.977]
Epoch [82/120    avg_loss:0.026, val_acc:0.966]
Epoch [83/120    avg_loss:0.028, val_acc:0.973]
Epoch [84/120    avg_loss:0.024, val_acc:0.977]
Epoch [85/120    avg_loss:0.019, val_acc:0.970]
Epoch [86/120    avg_loss:0.018, val_acc:0.974]
Epoch [87/120    avg_loss:0.018, val_acc:0.979]
Epoch [88/120    avg_loss:0.020, val_acc:0.978]
Epoch [89/120    avg_loss:0.019, val_acc:0.975]
Epoch [90/120    avg_loss:0.019, val_acc:0.976]
Epoch [91/120    avg_loss:0.017, val_acc:0.985]
Epoch [92/120    avg_loss:0.031, val_acc:0.959]
Epoch [93/120    avg_loss:0.028, val_acc:0.978]
Epoch [94/120    avg_loss:0.047, val_acc:0.976]
Epoch [95/120    avg_loss:0.029, val_acc:0.966]
Epoch [96/120    avg_loss:0.030, val_acc:0.980]
Epoch [97/120    avg_loss:0.020, val_acc:0.977]
Epoch [98/120    avg_loss:0.014, val_acc:0.977]
Epoch [99/120    avg_loss:0.014, val_acc:0.977]
Epoch [100/120    avg_loss:0.015, val_acc:0.977]
Epoch [101/120    avg_loss:0.015, val_acc:0.967]
Epoch [102/120    avg_loss:0.018, val_acc:0.981]
Epoch [103/120    avg_loss:0.013, val_acc:0.985]
Epoch [104/120    avg_loss:0.011, val_acc:0.979]
Epoch [105/120    avg_loss:0.010, val_acc:0.980]
Epoch [106/120    avg_loss:0.010, val_acc:0.985]
Epoch [107/120    avg_loss:0.016, val_acc:0.979]
Epoch [108/120    avg_loss:0.012, val_acc:0.980]
Epoch [109/120    avg_loss:0.010, val_acc:0.987]
Epoch [110/120    avg_loss:0.010, val_acc:0.980]
Epoch [111/120    avg_loss:0.018, val_acc:0.977]
Epoch [112/120    avg_loss:0.016, val_acc:0.977]
Epoch [113/120    avg_loss:0.023, val_acc:0.957]
Epoch [114/120    avg_loss:0.087, val_acc:0.962]
Epoch [115/120    avg_loss:0.150, val_acc:0.931]
Epoch [116/120    avg_loss:0.080, val_acc:0.962]
Epoch [117/120    avg_loss:0.050, val_acc:0.952]
Epoch [118/120    avg_loss:0.028, val_acc:0.956]
Epoch [119/120    avg_loss:0.025, val_acc:0.958]
Epoch [120/120    avg_loss:0.046, val_acc:0.947]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1223    7   10    1    0    0    0    0   12   32    0    0
     0    0    0]
 [   0    0    0  717    4    0    0    0    0    6    2    3   13    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    0    0    1    0    0
     0    0    0]
 [   0    0  108    0    1    0  519    0    0    7    0   14    0    0
     8    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  423    0    0    0    7    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    2    0    0    0    0  855   15    1    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0    8 2190    8    0
     0    0    0]
 [   0    0    1   13    4    1    0    0    0    0   11    0  496    0
     0    6    2]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0   13    0    0    0   10    0    0    0    0
  1076   40    0]
 [   0    0    2    0    0    1    0    0    0    4    0    0    0    0
    44  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.29539295392954

F1 scores:
[       nan 0.98765432 0.93180952 0.96565657 0.95730337 0.97742664
 0.88265306 0.98039216 0.99179367 0.5483871  0.96938776 0.98074339
 0.93673277 0.99191375 0.94927217 0.85921626 0.98823529]

Kappa:
0.9463078728417438
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f54ddfddf28>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.799, val_acc:0.232]
Epoch [2/120    avg_loss:2.717, val_acc:0.285]
Epoch [3/120    avg_loss:2.640, val_acc:0.322]
Epoch [4/120    avg_loss:2.554, val_acc:0.338]
Epoch [5/120    avg_loss:2.483, val_acc:0.424]
Epoch [6/120    avg_loss:2.409, val_acc:0.504]
Epoch [7/120    avg_loss:2.367, val_acc:0.526]
Epoch [8/120    avg_loss:2.306, val_acc:0.557]
Epoch [9/120    avg_loss:2.230, val_acc:0.565]
Epoch [10/120    avg_loss:2.174, val_acc:0.557]
Epoch [11/120    avg_loss:2.132, val_acc:0.574]
Epoch [12/120    avg_loss:2.051, val_acc:0.595]
Epoch [13/120    avg_loss:1.974, val_acc:0.638]
Epoch [14/120    avg_loss:1.945, val_acc:0.606]
Epoch [15/120    avg_loss:1.854, val_acc:0.634]
Epoch [16/120    avg_loss:1.759, val_acc:0.673]
Epoch [17/120    avg_loss:1.635, val_acc:0.690]
Epoch [18/120    avg_loss:1.622, val_acc:0.694]
Epoch [19/120    avg_loss:1.509, val_acc:0.703]
Epoch [20/120    avg_loss:1.411, val_acc:0.698]
Epoch [21/120    avg_loss:1.365, val_acc:0.740]
Epoch [22/120    avg_loss:1.259, val_acc:0.655]
Epoch [23/120    avg_loss:1.184, val_acc:0.762]
Epoch [24/120    avg_loss:1.100, val_acc:0.804]
Epoch [25/120    avg_loss:0.984, val_acc:0.772]
Epoch [26/120    avg_loss:0.937, val_acc:0.781]
Epoch [27/120    avg_loss:0.890, val_acc:0.784]
Epoch [28/120    avg_loss:0.797, val_acc:0.784]
Epoch [29/120    avg_loss:0.740, val_acc:0.810]
Epoch [30/120    avg_loss:0.688, val_acc:0.821]
Epoch [31/120    avg_loss:0.562, val_acc:0.828]
Epoch [32/120    avg_loss:0.531, val_acc:0.854]
Epoch [33/120    avg_loss:0.496, val_acc:0.843]
Epoch [34/120    avg_loss:0.430, val_acc:0.869]
Epoch [35/120    avg_loss:0.411, val_acc:0.870]
Epoch [36/120    avg_loss:0.367, val_acc:0.849]
Epoch [37/120    avg_loss:0.380, val_acc:0.843]
Epoch [38/120    avg_loss:0.280, val_acc:0.878]
Epoch [39/120    avg_loss:0.240, val_acc:0.887]
Epoch [40/120    avg_loss:0.257, val_acc:0.886]
Epoch [41/120    avg_loss:0.247, val_acc:0.893]
Epoch [42/120    avg_loss:0.217, val_acc:0.908]
Epoch [43/120    avg_loss:0.234, val_acc:0.899]
Epoch [44/120    avg_loss:0.207, val_acc:0.909]
Epoch [45/120    avg_loss:0.198, val_acc:0.911]
Epoch [46/120    avg_loss:0.149, val_acc:0.905]
Epoch [47/120    avg_loss:0.131, val_acc:0.927]
Epoch [48/120    avg_loss:0.139, val_acc:0.920]
Epoch [49/120    avg_loss:0.129, val_acc:0.935]
Epoch [50/120    avg_loss:0.115, val_acc:0.903]
Epoch [51/120    avg_loss:0.134, val_acc:0.917]
Epoch [52/120    avg_loss:0.150, val_acc:0.918]
Epoch [53/120    avg_loss:0.137, val_acc:0.917]
Epoch [54/120    avg_loss:0.108, val_acc:0.915]
Epoch [55/120    avg_loss:0.115, val_acc:0.918]
Epoch [56/120    avg_loss:0.127, val_acc:0.936]
Epoch [57/120    avg_loss:0.091, val_acc:0.928]
Epoch [58/120    avg_loss:0.068, val_acc:0.928]
Epoch [59/120    avg_loss:0.066, val_acc:0.936]
Epoch [60/120    avg_loss:0.068, val_acc:0.935]
Epoch [61/120    avg_loss:0.071, val_acc:0.953]
Epoch [62/120    avg_loss:0.130, val_acc:0.928]
Epoch [63/120    avg_loss:0.098, val_acc:0.941]
Epoch [64/120    avg_loss:0.078, val_acc:0.940]
Epoch [65/120    avg_loss:0.060, val_acc:0.942]
Epoch [66/120    avg_loss:0.053, val_acc:0.933]
Epoch [67/120    avg_loss:0.062, val_acc:0.936]
Epoch [68/120    avg_loss:0.062, val_acc:0.952]
Epoch [69/120    avg_loss:0.059, val_acc:0.935]
Epoch [70/120    avg_loss:0.056, val_acc:0.942]
Epoch [71/120    avg_loss:0.048, val_acc:0.953]
Epoch [72/120    avg_loss:0.047, val_acc:0.951]
Epoch [73/120    avg_loss:0.034, val_acc:0.955]
Epoch [74/120    avg_loss:0.026, val_acc:0.955]
Epoch [75/120    avg_loss:0.050, val_acc:0.943]
Epoch [76/120    avg_loss:0.034, val_acc:0.948]
Epoch [77/120    avg_loss:0.028, val_acc:0.960]
Epoch [78/120    avg_loss:0.032, val_acc:0.953]
Epoch [79/120    avg_loss:0.033, val_acc:0.944]
Epoch [80/120    avg_loss:0.028, val_acc:0.958]
Epoch [81/120    avg_loss:0.024, val_acc:0.958]
Epoch [82/120    avg_loss:0.041, val_acc:0.952]
Epoch [83/120    avg_loss:0.035, val_acc:0.946]
Epoch [84/120    avg_loss:0.035, val_acc:0.962]
Epoch [85/120    avg_loss:0.035, val_acc:0.947]
Epoch [86/120    avg_loss:0.108, val_acc:0.931]
Epoch [87/120    avg_loss:0.062, val_acc:0.951]
Epoch [88/120    avg_loss:0.047, val_acc:0.952]
Epoch [89/120    avg_loss:0.031, val_acc:0.951]
Epoch [90/120    avg_loss:0.035, val_acc:0.948]
Epoch [91/120    avg_loss:0.031, val_acc:0.962]
Epoch [92/120    avg_loss:0.026, val_acc:0.964]
Epoch [93/120    avg_loss:0.044, val_acc:0.939]
Epoch [94/120    avg_loss:0.041, val_acc:0.953]
Epoch [95/120    avg_loss:0.032, val_acc:0.958]
Epoch [96/120    avg_loss:0.033, val_acc:0.957]
Epoch [97/120    avg_loss:0.049, val_acc:0.949]
Epoch [98/120    avg_loss:0.047, val_acc:0.949]
Epoch [99/120    avg_loss:0.034, val_acc:0.962]
Epoch [100/120    avg_loss:0.025, val_acc:0.963]
Epoch [101/120    avg_loss:0.019, val_acc:0.964]
Epoch [102/120    avg_loss:0.020, val_acc:0.958]
Epoch [103/120    avg_loss:0.018, val_acc:0.964]
Epoch [104/120    avg_loss:0.024, val_acc:0.959]
Epoch [105/120    avg_loss:0.021, val_acc:0.969]
Epoch [106/120    avg_loss:0.019, val_acc:0.970]
Epoch [107/120    avg_loss:0.021, val_acc:0.958]
Epoch [108/120    avg_loss:0.015, val_acc:0.975]
Epoch [109/120    avg_loss:0.017, val_acc:0.967]
Epoch [110/120    avg_loss:0.019, val_acc:0.953]
Epoch [111/120    avg_loss:0.019, val_acc:0.964]
Epoch [112/120    avg_loss:0.020, val_acc:0.965]
Epoch [113/120    avg_loss:0.025, val_acc:0.970]
Epoch [114/120    avg_loss:0.018, val_acc:0.959]
Epoch [115/120    avg_loss:0.020, val_acc:0.953]
Epoch [116/120    avg_loss:0.023, val_acc:0.970]
Epoch [117/120    avg_loss:0.013, val_acc:0.963]
Epoch [118/120    avg_loss:0.010, val_acc:0.973]
Epoch [119/120    avg_loss:0.011, val_acc:0.972]
Epoch [120/120    avg_loss:0.012, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1241    4    4    0    3    0    0    0    1   28    0    0
     0    4    0]
 [   0    0    2  690    5   10    0    0    0   14    2    0   20    3
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    1    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    0    0
     0    2    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    5    0    0    0    0  858    9    0    0
     0    1    0]
 [   0    4    9    0    0    2    3    4    0    0   12 2167    8    1
     0    0    0]
 [   0    0    0    3    2   11    0    0    0    2    0    0  507    0
     2    1    6]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   30    0    0    0    0    0    0    0
    11  306    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.39837398373983

F1 scores:
[       nan 0.92857143 0.97755022 0.95435685 0.97482838 0.96659243
 0.9710897  0.92592593 0.997669   0.64       0.98001142 0.98143116
 0.94855005 0.98659517 0.99388646 0.9244713  0.96551724]

Kappa:
0.9703559427825779
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f597c269f98>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.797, val_acc:0.208]
Epoch [2/120    avg_loss:2.747, val_acc:0.238]
Epoch [3/120    avg_loss:2.681, val_acc:0.248]
Epoch [4/120    avg_loss:2.599, val_acc:0.283]
Epoch [5/120    avg_loss:2.534, val_acc:0.320]
Epoch [6/120    avg_loss:2.465, val_acc:0.346]
Epoch [7/120    avg_loss:2.365, val_acc:0.388]
Epoch [8/120    avg_loss:2.261, val_acc:0.458]
Epoch [9/120    avg_loss:2.188, val_acc:0.503]
Epoch [10/120    avg_loss:2.111, val_acc:0.542]
Epoch [11/120    avg_loss:2.045, val_acc:0.558]
Epoch [12/120    avg_loss:1.924, val_acc:0.615]
Epoch [13/120    avg_loss:1.821, val_acc:0.649]
Epoch [14/120    avg_loss:1.731, val_acc:0.660]
Epoch [15/120    avg_loss:1.611, val_acc:0.694]
Epoch [16/120    avg_loss:1.434, val_acc:0.701]
Epoch [17/120    avg_loss:1.374, val_acc:0.713]
Epoch [18/120    avg_loss:1.218, val_acc:0.717]
Epoch [19/120    avg_loss:1.099, val_acc:0.741]
Epoch [20/120    avg_loss:1.029, val_acc:0.755]
Epoch [21/120    avg_loss:0.969, val_acc:0.735]
Epoch [22/120    avg_loss:0.881, val_acc:0.783]
Epoch [23/120    avg_loss:0.797, val_acc:0.827]
Epoch [24/120    avg_loss:0.750, val_acc:0.829]
Epoch [25/120    avg_loss:0.730, val_acc:0.814]
Epoch [26/120    avg_loss:0.663, val_acc:0.822]
Epoch [27/120    avg_loss:0.612, val_acc:0.850]
Epoch [28/120    avg_loss:0.557, val_acc:0.870]
Epoch [29/120    avg_loss:0.478, val_acc:0.871]
Epoch [30/120    avg_loss:0.483, val_acc:0.885]
Epoch [31/120    avg_loss:0.412, val_acc:0.875]
Epoch [32/120    avg_loss:0.366, val_acc:0.865]
Epoch [33/120    avg_loss:0.383, val_acc:0.867]
Epoch [34/120    avg_loss:0.368, val_acc:0.891]
Epoch [35/120    avg_loss:0.301, val_acc:0.865]
Epoch [36/120    avg_loss:0.301, val_acc:0.905]
Epoch [37/120    avg_loss:0.248, val_acc:0.906]
Epoch [38/120    avg_loss:0.229, val_acc:0.888]
Epoch [39/120    avg_loss:0.213, val_acc:0.913]
Epoch [40/120    avg_loss:0.183, val_acc:0.934]
Epoch [41/120    avg_loss:0.174, val_acc:0.933]
Epoch [42/120    avg_loss:0.186, val_acc:0.907]
Epoch [43/120    avg_loss:0.172, val_acc:0.947]
Epoch [44/120    avg_loss:0.150, val_acc:0.941]
Epoch [45/120    avg_loss:0.136, val_acc:0.942]
Epoch [46/120    avg_loss:0.131, val_acc:0.922]
Epoch [47/120    avg_loss:0.130, val_acc:0.943]
Epoch [48/120    avg_loss:0.117, val_acc:0.948]
Epoch [49/120    avg_loss:0.111, val_acc:0.944]
Epoch [50/120    avg_loss:0.109, val_acc:0.948]
Epoch [51/120    avg_loss:0.103, val_acc:0.950]
Epoch [52/120    avg_loss:0.093, val_acc:0.945]
Epoch [53/120    avg_loss:0.105, val_acc:0.951]
Epoch [54/120    avg_loss:0.104, val_acc:0.945]
Epoch [55/120    avg_loss:0.077, val_acc:0.963]
Epoch [56/120    avg_loss:0.079, val_acc:0.967]
Epoch [57/120    avg_loss:0.065, val_acc:0.956]
Epoch [58/120    avg_loss:0.068, val_acc:0.953]
Epoch [59/120    avg_loss:0.069, val_acc:0.961]
Epoch [60/120    avg_loss:0.084, val_acc:0.950]
Epoch [61/120    avg_loss:0.082, val_acc:0.958]
Epoch [62/120    avg_loss:0.074, val_acc:0.971]
Epoch [63/120    avg_loss:0.057, val_acc:0.957]
Epoch [64/120    avg_loss:0.067, val_acc:0.965]
Epoch [65/120    avg_loss:0.068, val_acc:0.971]
Epoch [66/120    avg_loss:0.050, val_acc:0.965]
Epoch [67/120    avg_loss:0.043, val_acc:0.967]
Epoch [68/120    avg_loss:0.042, val_acc:0.952]
Epoch [69/120    avg_loss:0.189, val_acc:0.906]
Epoch [70/120    avg_loss:0.140, val_acc:0.945]
Epoch [71/120    avg_loss:0.122, val_acc:0.936]
Epoch [72/120    avg_loss:0.087, val_acc:0.959]
Epoch [73/120    avg_loss:0.084, val_acc:0.956]
Epoch [74/120    avg_loss:0.064, val_acc:0.940]
Epoch [75/120    avg_loss:0.083, val_acc:0.951]
Epoch [76/120    avg_loss:0.052, val_acc:0.962]
Epoch [77/120    avg_loss:0.057, val_acc:0.963]
Epoch [78/120    avg_loss:0.051, val_acc:0.957]
Epoch [79/120    avg_loss:0.038, val_acc:0.967]
Epoch [80/120    avg_loss:0.031, val_acc:0.970]
Epoch [81/120    avg_loss:0.030, val_acc:0.968]
Epoch [82/120    avg_loss:0.027, val_acc:0.970]
Epoch [83/120    avg_loss:0.027, val_acc:0.967]
Epoch [84/120    avg_loss:0.026, val_acc:0.969]
Epoch [85/120    avg_loss:0.024, val_acc:0.970]
Epoch [86/120    avg_loss:0.027, val_acc:0.974]
Epoch [87/120    avg_loss:0.030, val_acc:0.969]
Epoch [88/120    avg_loss:0.027, val_acc:0.975]
Epoch [89/120    avg_loss:0.022, val_acc:0.971]
Epoch [90/120    avg_loss:0.027, val_acc:0.975]
Epoch [91/120    avg_loss:0.024, val_acc:0.970]
Epoch [92/120    avg_loss:0.025, val_acc:0.975]
Epoch [93/120    avg_loss:0.025, val_acc:0.976]
Epoch [94/120    avg_loss:0.025, val_acc:0.976]
Epoch [95/120    avg_loss:0.024, val_acc:0.976]
Epoch [96/120    avg_loss:0.024, val_acc:0.976]
Epoch [97/120    avg_loss:0.024, val_acc:0.976]
Epoch [98/120    avg_loss:0.027, val_acc:0.976]
Epoch [99/120    avg_loss:0.022, val_acc:0.976]
Epoch [100/120    avg_loss:0.025, val_acc:0.975]
Epoch [101/120    avg_loss:0.025, val_acc:0.975]
Epoch [102/120    avg_loss:0.023, val_acc:0.976]
Epoch [103/120    avg_loss:0.023, val_acc:0.977]
Epoch [104/120    avg_loss:0.023, val_acc:0.976]
Epoch [105/120    avg_loss:0.023, val_acc:0.976]
Epoch [106/120    avg_loss:0.022, val_acc:0.978]
Epoch [107/120    avg_loss:0.020, val_acc:0.979]
Epoch [108/120    avg_loss:0.019, val_acc:0.975]
Epoch [109/120    avg_loss:0.021, val_acc:0.976]
Epoch [110/120    avg_loss:0.023, val_acc:0.975]
Epoch [111/120    avg_loss:0.020, val_acc:0.976]
Epoch [112/120    avg_loss:0.021, val_acc:0.975]
Epoch [113/120    avg_loss:0.018, val_acc:0.977]
Epoch [114/120    avg_loss:0.024, val_acc:0.975]
Epoch [115/120    avg_loss:0.018, val_acc:0.979]
Epoch [116/120    avg_loss:0.023, val_acc:0.981]
Epoch [117/120    avg_loss:0.021, val_acc:0.979]
Epoch [118/120    avg_loss:0.018, val_acc:0.977]
Epoch [119/120    avg_loss:0.018, val_acc:0.978]
Epoch [120/120    avg_loss:0.020, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1252    0    8    0    1    0    0    0    7   17    0    0
     0    0    0]
 [   0    0    0  687    1   15    1    0    0   10    2    2   29    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    2    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    7    0    0    0    0    0    0  423    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   14    0    0    3    0    0    0    1  848    9    0    0
     0    0    0]
 [   0    0    7    0    0    0    1    0    0    1   10 2177   12    2
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0   15    4  510    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    1    0    0
  1135    1    0]
 [   0    0    0    0    0    0   24    0    0    0    0    0    0    0
     8  315    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.46341463414635

F1 scores:
[       nan 0.89655172 0.97888976 0.95615866 0.97931034 0.97291196
 0.9760479  0.96153846 0.99179367 0.65217391 0.96363636 0.98462234
 0.94009217 0.99462366 0.99300087 0.95022624 0.97674419]

Kappa:
0.9710867957341354
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa5f2092f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.818, val_acc:0.225]
Epoch [2/120    avg_loss:2.759, val_acc:0.324]
Epoch [3/120    avg_loss:2.696, val_acc:0.328]
Epoch [4/120    avg_loss:2.638, val_acc:0.322]
Epoch [5/120    avg_loss:2.558, val_acc:0.327]
Epoch [6/120    avg_loss:2.474, val_acc:0.368]
Epoch [7/120    avg_loss:2.391, val_acc:0.412]
Epoch [8/120    avg_loss:2.332, val_acc:0.456]
Epoch [9/120    avg_loss:2.273, val_acc:0.505]
Epoch [10/120    avg_loss:2.209, val_acc:0.506]
Epoch [11/120    avg_loss:2.132, val_acc:0.537]
Epoch [12/120    avg_loss:2.025, val_acc:0.590]
Epoch [13/120    avg_loss:1.923, val_acc:0.604]
Epoch [14/120    avg_loss:1.813, val_acc:0.601]
Epoch [15/120    avg_loss:1.651, val_acc:0.593]
Epoch [16/120    avg_loss:1.536, val_acc:0.635]
Epoch [17/120    avg_loss:1.417, val_acc:0.667]
Epoch [18/120    avg_loss:1.296, val_acc:0.691]
Epoch [19/120    avg_loss:1.202, val_acc:0.725]
Epoch [20/120    avg_loss:1.077, val_acc:0.757]
Epoch [21/120    avg_loss:0.967, val_acc:0.792]
Epoch [22/120    avg_loss:0.931, val_acc:0.787]
Epoch [23/120    avg_loss:0.824, val_acc:0.811]
Epoch [24/120    avg_loss:0.695, val_acc:0.831]
Epoch [25/120    avg_loss:0.597, val_acc:0.821]
Epoch [26/120    avg_loss:0.553, val_acc:0.823]
Epoch [27/120    avg_loss:0.479, val_acc:0.856]
Epoch [28/120    avg_loss:0.429, val_acc:0.888]
Epoch [29/120    avg_loss:0.411, val_acc:0.889]
Epoch [30/120    avg_loss:0.360, val_acc:0.897]
Epoch [31/120    avg_loss:0.309, val_acc:0.901]
Epoch [32/120    avg_loss:0.252, val_acc:0.920]
Epoch [33/120    avg_loss:0.243, val_acc:0.904]
Epoch [34/120    avg_loss:0.239, val_acc:0.896]
Epoch [35/120    avg_loss:0.216, val_acc:0.931]
Epoch [36/120    avg_loss:0.173, val_acc:0.920]
Epoch [37/120    avg_loss:0.157, val_acc:0.934]
Epoch [38/120    avg_loss:0.151, val_acc:0.942]
Epoch [39/120    avg_loss:0.136, val_acc:0.934]
Epoch [40/120    avg_loss:0.141, val_acc:0.934]
Epoch [41/120    avg_loss:0.104, val_acc:0.949]
Epoch [42/120    avg_loss:0.090, val_acc:0.955]
Epoch [43/120    avg_loss:0.090, val_acc:0.956]
Epoch [44/120    avg_loss:0.095, val_acc:0.940]
Epoch [45/120    avg_loss:0.124, val_acc:0.954]
Epoch [46/120    avg_loss:0.094, val_acc:0.958]
Epoch [47/120    avg_loss:0.077, val_acc:0.965]
Epoch [48/120    avg_loss:0.072, val_acc:0.954]
Epoch [49/120    avg_loss:0.062, val_acc:0.962]
Epoch [50/120    avg_loss:0.065, val_acc:0.964]
Epoch [51/120    avg_loss:0.054, val_acc:0.967]
Epoch [52/120    avg_loss:0.050, val_acc:0.964]
Epoch [53/120    avg_loss:0.058, val_acc:0.966]
Epoch [54/120    avg_loss:0.051, val_acc:0.965]
Epoch [55/120    avg_loss:0.043, val_acc:0.957]
Epoch [56/120    avg_loss:0.044, val_acc:0.964]
Epoch [57/120    avg_loss:0.040, val_acc:0.966]
Epoch [58/120    avg_loss:0.031, val_acc:0.971]
Epoch [59/120    avg_loss:0.030, val_acc:0.967]
Epoch [60/120    avg_loss:0.041, val_acc:0.966]
Epoch [61/120    avg_loss:0.040, val_acc:0.960]
Epoch [62/120    avg_loss:0.037, val_acc:0.975]
Epoch [63/120    avg_loss:0.033, val_acc:0.966]
Epoch [64/120    avg_loss:0.029, val_acc:0.968]
Epoch [65/120    avg_loss:0.027, val_acc:0.973]
Epoch [66/120    avg_loss:0.024, val_acc:0.974]
Epoch [67/120    avg_loss:0.026, val_acc:0.979]
Epoch [68/120    avg_loss:0.021, val_acc:0.975]
Epoch [69/120    avg_loss:0.023, val_acc:0.976]
Epoch [70/120    avg_loss:0.019, val_acc:0.968]
Epoch [71/120    avg_loss:0.024, val_acc:0.958]
Epoch [72/120    avg_loss:0.020, val_acc:0.980]
Epoch [73/120    avg_loss:0.016, val_acc:0.969]
Epoch [74/120    avg_loss:0.016, val_acc:0.974]
Epoch [75/120    avg_loss:0.016, val_acc:0.974]
Epoch [76/120    avg_loss:0.020, val_acc:0.979]
Epoch [77/120    avg_loss:0.016, val_acc:0.976]
Epoch [78/120    avg_loss:0.017, val_acc:0.966]
Epoch [79/120    avg_loss:0.022, val_acc:0.967]
Epoch [80/120    avg_loss:0.022, val_acc:0.962]
Epoch [81/120    avg_loss:0.015, val_acc:0.976]
Epoch [82/120    avg_loss:0.012, val_acc:0.976]
Epoch [83/120    avg_loss:0.011, val_acc:0.978]
Epoch [84/120    avg_loss:0.013, val_acc:0.981]
Epoch [85/120    avg_loss:0.183, val_acc:0.752]
Epoch [86/120    avg_loss:0.592, val_acc:0.818]
Epoch [87/120    avg_loss:0.328, val_acc:0.825]
Epoch [88/120    avg_loss:0.215, val_acc:0.933]
Epoch [89/120    avg_loss:0.123, val_acc:0.944]
Epoch [90/120    avg_loss:0.101, val_acc:0.924]
Epoch [91/120    avg_loss:0.125, val_acc:0.946]
Epoch [92/120    avg_loss:0.078, val_acc:0.958]
Epoch [93/120    avg_loss:0.078, val_acc:0.950]
Epoch [94/120    avg_loss:0.055, val_acc:0.957]
Epoch [95/120    avg_loss:0.061, val_acc:0.965]
Epoch [96/120    avg_loss:0.048, val_acc:0.968]
Epoch [97/120    avg_loss:0.042, val_acc:0.962]
Epoch [98/120    avg_loss:0.035, val_acc:0.966]
Epoch [99/120    avg_loss:0.024, val_acc:0.970]
Epoch [100/120    avg_loss:0.026, val_acc:0.972]
Epoch [101/120    avg_loss:0.023, val_acc:0.972]
Epoch [102/120    avg_loss:0.022, val_acc:0.972]
Epoch [103/120    avg_loss:0.025, val_acc:0.975]
Epoch [104/120    avg_loss:0.020, val_acc:0.978]
Epoch [105/120    avg_loss:0.026, val_acc:0.978]
Epoch [106/120    avg_loss:0.023, val_acc:0.982]
Epoch [107/120    avg_loss:0.024, val_acc:0.979]
Epoch [108/120    avg_loss:0.022, val_acc:0.978]
Epoch [109/120    avg_loss:0.018, val_acc:0.979]
Epoch [110/120    avg_loss:0.019, val_acc:0.976]
Epoch [111/120    avg_loss:0.021, val_acc:0.976]
Epoch [112/120    avg_loss:0.015, val_acc:0.976]
Epoch [113/120    avg_loss:0.015, val_acc:0.975]
Epoch [114/120    avg_loss:0.017, val_acc:0.978]
Epoch [115/120    avg_loss:0.018, val_acc:0.979]
Epoch [116/120    avg_loss:0.019, val_acc:0.978]
Epoch [117/120    avg_loss:0.020, val_acc:0.978]
Epoch [118/120    avg_loss:0.021, val_acc:0.979]
Epoch [119/120    avg_loss:0.015, val_acc:0.979]
Epoch [120/120    avg_loss:0.018, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1251    8    2    0    0    0    0    0    9   15    0    0
     0    0    0]
 [   0    0    3  701    3    0    0    0    0   17    0    7   16    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    5    0    3    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   12    0    0    1    0    0    0    0  835   24    1    0
     2    0    0]
 [   0    0   19    0    0    0    0    4    0    0    7 2179    0    0
     0    1    0]
 [   0    0    0    0    0   10    0    0    0    0    6    0  512    0
     0    4    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1133    5    0]
 [   0    0    0    0    0    1    0    0    0    4    0    0    0    0
   105  237    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.72628726287263

F1 scores:
[       nan 1.         0.97354086 0.96291209 0.98839907 0.97241379
 0.99923839 0.84745763 1.         0.59016393 0.96420323 0.98263811
 0.96331138 1.         0.95050336 0.7979798  0.98823529]

Kappa:
0.9626398501265192
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:09:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f599b7d7f98>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.817, val_acc:0.159]
Epoch [2/120    avg_loss:2.767, val_acc:0.395]
Epoch [3/120    avg_loss:2.717, val_acc:0.424]
Epoch [4/120    avg_loss:2.658, val_acc:0.401]
Epoch [5/120    avg_loss:2.580, val_acc:0.391]
Epoch [6/120    avg_loss:2.501, val_acc:0.388]
Epoch [7/120    avg_loss:2.388, val_acc:0.457]
Epoch [8/120    avg_loss:2.321, val_acc:0.481]
Epoch [9/120    avg_loss:2.235, val_acc:0.449]
Epoch [10/120    avg_loss:2.138, val_acc:0.493]
Epoch [11/120    avg_loss:2.032, val_acc:0.587]
Epoch [12/120    avg_loss:1.907, val_acc:0.627]
Epoch [13/120    avg_loss:1.800, val_acc:0.625]
Epoch [14/120    avg_loss:1.648, val_acc:0.607]
Epoch [15/120    avg_loss:1.515, val_acc:0.655]
Epoch [16/120    avg_loss:1.392, val_acc:0.651]
Epoch [17/120    avg_loss:1.252, val_acc:0.700]
Epoch [18/120    avg_loss:1.162, val_acc:0.651]
Epoch [19/120    avg_loss:1.094, val_acc:0.718]
Epoch [20/120    avg_loss:0.963, val_acc:0.767]
Epoch [21/120    avg_loss:0.866, val_acc:0.778]
Epoch [22/120    avg_loss:0.778, val_acc:0.809]
Epoch [23/120    avg_loss:0.700, val_acc:0.823]
Epoch [24/120    avg_loss:0.592, val_acc:0.876]
Epoch [25/120    avg_loss:0.535, val_acc:0.831]
Epoch [26/120    avg_loss:0.512, val_acc:0.857]
Epoch [27/120    avg_loss:0.454, val_acc:0.848]
Epoch [28/120    avg_loss:0.405, val_acc:0.875]
Epoch [29/120    avg_loss:0.373, val_acc:0.879]
Epoch [30/120    avg_loss:0.347, val_acc:0.897]
Epoch [31/120    avg_loss:0.326, val_acc:0.888]
Epoch [32/120    avg_loss:0.273, val_acc:0.908]
Epoch [33/120    avg_loss:0.274, val_acc:0.910]
Epoch [34/120    avg_loss:0.222, val_acc:0.921]
Epoch [35/120    avg_loss:0.238, val_acc:0.912]
Epoch [36/120    avg_loss:0.176, val_acc:0.949]
Epoch [37/120    avg_loss:0.166, val_acc:0.925]
Epoch [38/120    avg_loss:0.185, val_acc:0.939]
Epoch [39/120    avg_loss:0.153, val_acc:0.950]
Epoch [40/120    avg_loss:0.125, val_acc:0.951]
Epoch [41/120    avg_loss:0.195, val_acc:0.940]
Epoch [42/120    avg_loss:0.131, val_acc:0.936]
Epoch [43/120    avg_loss:0.138, val_acc:0.891]
Epoch [44/120    avg_loss:0.120, val_acc:0.962]
Epoch [45/120    avg_loss:0.095, val_acc:0.963]
Epoch [46/120    avg_loss:0.105, val_acc:0.966]
Epoch [47/120    avg_loss:0.082, val_acc:0.951]
Epoch [48/120    avg_loss:0.083, val_acc:0.956]
Epoch [49/120    avg_loss:0.071, val_acc:0.975]
Epoch [50/120    avg_loss:0.070, val_acc:0.965]
Epoch [51/120    avg_loss:0.067, val_acc:0.962]
Epoch [52/120    avg_loss:0.055, val_acc:0.969]
Epoch [53/120    avg_loss:0.049, val_acc:0.967]
Epoch [54/120    avg_loss:0.054, val_acc:0.980]
Epoch [55/120    avg_loss:0.039, val_acc:0.981]
Epoch [56/120    avg_loss:0.044, val_acc:0.973]
Epoch [57/120    avg_loss:0.058, val_acc:0.973]
Epoch [58/120    avg_loss:0.047, val_acc:0.944]
Epoch [59/120    avg_loss:0.054, val_acc:0.968]
Epoch [60/120    avg_loss:0.053, val_acc:0.970]
Epoch [61/120    avg_loss:0.048, val_acc:0.963]
Epoch [62/120    avg_loss:0.038, val_acc:0.974]
Epoch [63/120    avg_loss:0.062, val_acc:0.968]
Epoch [64/120    avg_loss:0.048, val_acc:0.969]
Epoch [65/120    avg_loss:0.051, val_acc:0.946]
Epoch [66/120    avg_loss:0.069, val_acc:0.972]
Epoch [67/120    avg_loss:0.061, val_acc:0.952]
Epoch [68/120    avg_loss:0.051, val_acc:0.953]
Epoch [69/120    avg_loss:0.043, val_acc:0.971]
Epoch [70/120    avg_loss:0.031, val_acc:0.973]
Epoch [71/120    avg_loss:0.027, val_acc:0.978]
Epoch [72/120    avg_loss:0.027, val_acc:0.978]
Epoch [73/120    avg_loss:0.029, val_acc:0.975]
Epoch [74/120    avg_loss:0.026, val_acc:0.978]
Epoch [75/120    avg_loss:0.022, val_acc:0.979]
Epoch [76/120    avg_loss:0.021, val_acc:0.976]
Epoch [77/120    avg_loss:0.020, val_acc:0.976]
Epoch [78/120    avg_loss:0.023, val_acc:0.978]
Epoch [79/120    avg_loss:0.022, val_acc:0.979]
Epoch [80/120    avg_loss:0.019, val_acc:0.979]
Epoch [81/120    avg_loss:0.021, val_acc:0.980]
Epoch [82/120    avg_loss:0.021, val_acc:0.980]
Epoch [83/120    avg_loss:0.021, val_acc:0.980]
Epoch [84/120    avg_loss:0.019, val_acc:0.980]
Epoch [85/120    avg_loss:0.022, val_acc:0.980]
Epoch [86/120    avg_loss:0.022, val_acc:0.980]
Epoch [87/120    avg_loss:0.019, val_acc:0.980]
Epoch [88/120    avg_loss:0.018, val_acc:0.980]
Epoch [89/120    avg_loss:0.016, val_acc:0.980]
Epoch [90/120    avg_loss:0.019, val_acc:0.980]
Epoch [91/120    avg_loss:0.019, val_acc:0.980]
Epoch [92/120    avg_loss:0.018, val_acc:0.980]
Epoch [93/120    avg_loss:0.018, val_acc:0.979]
Epoch [94/120    avg_loss:0.017, val_acc:0.980]
Epoch [95/120    avg_loss:0.019, val_acc:0.979]
Epoch [96/120    avg_loss:0.021, val_acc:0.979]
Epoch [97/120    avg_loss:0.017, val_acc:0.979]
Epoch [98/120    avg_loss:0.019, val_acc:0.979]
Epoch [99/120    avg_loss:0.018, val_acc:0.979]
Epoch [100/120    avg_loss:0.019, val_acc:0.979]
Epoch [101/120    avg_loss:0.020, val_acc:0.979]
Epoch [102/120    avg_loss:0.018, val_acc:0.979]
Epoch [103/120    avg_loss:0.021, val_acc:0.979]
Epoch [104/120    avg_loss:0.018, val_acc:0.979]
Epoch [105/120    avg_loss:0.024, val_acc:0.979]
Epoch [106/120    avg_loss:0.020, val_acc:0.979]
Epoch [107/120    avg_loss:0.024, val_acc:0.979]
Epoch [108/120    avg_loss:0.018, val_acc:0.979]
Epoch [109/120    avg_loss:0.019, val_acc:0.979]
Epoch [110/120    avg_loss:0.017, val_acc:0.979]
Epoch [111/120    avg_loss:0.018, val_acc:0.979]
Epoch [112/120    avg_loss:0.018, val_acc:0.979]
Epoch [113/120    avg_loss:0.019, val_acc:0.979]
Epoch [114/120    avg_loss:0.018, val_acc:0.979]
Epoch [115/120    avg_loss:0.022, val_acc:0.979]
Epoch [116/120    avg_loss:0.023, val_acc:0.979]
Epoch [117/120    avg_loss:0.018, val_acc:0.979]
Epoch [118/120    avg_loss:0.019, val_acc:0.979]
Epoch [119/120    avg_loss:0.022, val_acc:0.979]
Epoch [120/120    avg_loss:0.018, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1259    2    4    0    0    0    0    0    1   18    1    0
     0    0    0]
 [   0    0    2  714    3    9    0    0    0    5    3    0   11    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    8    0    0    1    0    0    0    0  851   13    1    0
     1    0    0]
 [   0    0    7    0    0    0    0    0    0    0   27 2171    5    0
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    7    0  518    0
     0    4    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    46  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.86449864498645

F1 scores:
[       nan 1.         0.98320968 0.97607656 0.98383372 0.98301246
 0.99848024 1.         1.         0.82926829 0.96485261 0.98413418
 0.96732026 1.         0.97582038 0.90743551 0.99408284]

Kappa:
0.975652947338785
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f93f04eaf28>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.818, val_acc:0.200]
Epoch [2/120    avg_loss:2.767, val_acc:0.349]
Epoch [3/120    avg_loss:2.712, val_acc:0.388]
Epoch [4/120    avg_loss:2.646, val_acc:0.467]
Epoch [5/120    avg_loss:2.565, val_acc:0.464]
Epoch [6/120    avg_loss:2.495, val_acc:0.513]
Epoch [7/120    avg_loss:2.393, val_acc:0.515]
Epoch [8/120    avg_loss:2.307, val_acc:0.498]
Epoch [9/120    avg_loss:2.255, val_acc:0.529]
Epoch [10/120    avg_loss:2.170, val_acc:0.504]
Epoch [11/120    avg_loss:2.084, val_acc:0.538]
Epoch [12/120    avg_loss:2.018, val_acc:0.582]
Epoch [13/120    avg_loss:1.905, val_acc:0.599]
Epoch [14/120    avg_loss:1.855, val_acc:0.623]
Epoch [15/120    avg_loss:1.768, val_acc:0.696]
Epoch [16/120    avg_loss:1.696, val_acc:0.717]
Epoch [17/120    avg_loss:1.512, val_acc:0.730]
Epoch [18/120    avg_loss:1.337, val_acc:0.726]
Epoch [19/120    avg_loss:1.254, val_acc:0.731]
Epoch [20/120    avg_loss:1.089, val_acc:0.753]
Epoch [21/120    avg_loss:1.014, val_acc:0.782]
Epoch [22/120    avg_loss:0.939, val_acc:0.746]
Epoch [23/120    avg_loss:0.867, val_acc:0.777]
Epoch [24/120    avg_loss:0.769, val_acc:0.770]
Epoch [25/120    avg_loss:0.746, val_acc:0.779]
Epoch [26/120    avg_loss:0.661, val_acc:0.787]
Epoch [27/120    avg_loss:0.606, val_acc:0.854]
Epoch [28/120    avg_loss:0.497, val_acc:0.856]
Epoch [29/120    avg_loss:0.430, val_acc:0.841]
Epoch [30/120    avg_loss:0.398, val_acc:0.871]
Epoch [31/120    avg_loss:0.360, val_acc:0.873]
Epoch [32/120    avg_loss:0.329, val_acc:0.867]
Epoch [33/120    avg_loss:0.270, val_acc:0.906]
Epoch [34/120    avg_loss:0.275, val_acc:0.896]
Epoch [35/120    avg_loss:0.271, val_acc:0.885]
Epoch [36/120    avg_loss:0.249, val_acc:0.909]
Epoch [37/120    avg_loss:0.254, val_acc:0.905]
Epoch [38/120    avg_loss:0.193, val_acc:0.924]
Epoch [39/120    avg_loss:0.163, val_acc:0.935]
Epoch [40/120    avg_loss:0.186, val_acc:0.927]
Epoch [41/120    avg_loss:0.148, val_acc:0.945]
Epoch [42/120    avg_loss:0.134, val_acc:0.927]
Epoch [43/120    avg_loss:0.125, val_acc:0.943]
Epoch [44/120    avg_loss:0.104, val_acc:0.955]
Epoch [45/120    avg_loss:0.100, val_acc:0.961]
Epoch [46/120    avg_loss:0.107, val_acc:0.950]
Epoch [47/120    avg_loss:0.097, val_acc:0.955]
Epoch [48/120    avg_loss:0.107, val_acc:0.942]
Epoch [49/120    avg_loss:0.087, val_acc:0.952]
Epoch [50/120    avg_loss:0.067, val_acc:0.957]
Epoch [51/120    avg_loss:0.081, val_acc:0.961]
Epoch [52/120    avg_loss:0.065, val_acc:0.965]
Epoch [53/120    avg_loss:0.073, val_acc:0.968]
Epoch [54/120    avg_loss:0.079, val_acc:0.955]
Epoch [55/120    avg_loss:0.070, val_acc:0.957]
Epoch [56/120    avg_loss:0.062, val_acc:0.958]
Epoch [57/120    avg_loss:0.057, val_acc:0.961]
Epoch [58/120    avg_loss:0.051, val_acc:0.961]
Epoch [59/120    avg_loss:0.047, val_acc:0.969]
Epoch [60/120    avg_loss:0.044, val_acc:0.971]
Epoch [61/120    avg_loss:0.032, val_acc:0.969]
Epoch [62/120    avg_loss:0.035, val_acc:0.961]
Epoch [63/120    avg_loss:0.040, val_acc:0.945]
Epoch [64/120    avg_loss:0.036, val_acc:0.967]
Epoch [65/120    avg_loss:0.039, val_acc:0.974]
Epoch [66/120    avg_loss:0.044, val_acc:0.950]
Epoch [67/120    avg_loss:0.036, val_acc:0.969]
Epoch [68/120    avg_loss:0.045, val_acc:0.965]
Epoch [69/120    avg_loss:0.047, val_acc:0.974]
Epoch [70/120    avg_loss:0.057, val_acc:0.976]
Epoch [71/120    avg_loss:0.044, val_acc:0.979]
Epoch [72/120    avg_loss:0.029, val_acc:0.974]
Epoch [73/120    avg_loss:0.030, val_acc:0.976]
Epoch [74/120    avg_loss:0.035, val_acc:0.967]
Epoch [75/120    avg_loss:0.041, val_acc:0.965]
Epoch [76/120    avg_loss:0.035, val_acc:0.974]
Epoch [77/120    avg_loss:0.030, val_acc:0.973]
Epoch [78/120    avg_loss:0.028, val_acc:0.973]
Epoch [79/120    avg_loss:0.026, val_acc:0.973]
Epoch [80/120    avg_loss:0.030, val_acc:0.974]
Epoch [81/120    avg_loss:0.029, val_acc:0.980]
Epoch [82/120    avg_loss:0.025, val_acc:0.977]
Epoch [83/120    avg_loss:0.341, val_acc:0.873]
Epoch [84/120    avg_loss:0.190, val_acc:0.950]
Epoch [85/120    avg_loss:0.102, val_acc:0.962]
Epoch [86/120    avg_loss:0.077, val_acc:0.967]
Epoch [87/120    avg_loss:0.057, val_acc:0.957]
Epoch [88/120    avg_loss:0.060, val_acc:0.964]
Epoch [89/120    avg_loss:0.042, val_acc:0.975]
Epoch [90/120    avg_loss:0.036, val_acc:0.975]
Epoch [91/120    avg_loss:0.031, val_acc:0.966]
Epoch [92/120    avg_loss:0.026, val_acc:0.970]
Epoch [93/120    avg_loss:0.022, val_acc:0.981]
Epoch [94/120    avg_loss:0.027, val_acc:0.965]
Epoch [95/120    avg_loss:0.030, val_acc:0.978]
Epoch [96/120    avg_loss:0.025, val_acc:0.962]
Epoch [97/120    avg_loss:0.029, val_acc:0.962]
Epoch [98/120    avg_loss:0.030, val_acc:0.981]
Epoch [99/120    avg_loss:0.019, val_acc:0.978]
Epoch [100/120    avg_loss:0.024, val_acc:0.973]
Epoch [101/120    avg_loss:0.027, val_acc:0.981]
Epoch [102/120    avg_loss:0.022, val_acc:0.979]
Epoch [103/120    avg_loss:0.014, val_acc:0.986]
Epoch [104/120    avg_loss:0.019, val_acc:0.980]
Epoch [105/120    avg_loss:0.020, val_acc:0.981]
Epoch [106/120    avg_loss:0.021, val_acc:0.980]
Epoch [107/120    avg_loss:0.028, val_acc:0.980]
Epoch [108/120    avg_loss:0.012, val_acc:0.977]
Epoch [109/120    avg_loss:0.014, val_acc:0.981]
Epoch [110/120    avg_loss:0.014, val_acc:0.970]
Epoch [111/120    avg_loss:0.022, val_acc:0.971]
Epoch [112/120    avg_loss:0.013, val_acc:0.985]
Epoch [113/120    avg_loss:0.013, val_acc:0.985]
Epoch [114/120    avg_loss:0.009, val_acc:0.984]
Epoch [115/120    avg_loss:0.012, val_acc:0.979]
Epoch [116/120    avg_loss:0.010, val_acc:0.985]
Epoch [117/120    avg_loss:0.009, val_acc:0.984]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1257    1    9    0    0    0    0    0    2   16    0    0
     0    0    0]
 [   0    0    8  686   13   12    0    0    0    4    2    3   18    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  422    0    0    0    8    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    3    0    0    6    0    0    0    0  856    7    2    0
     1    0    0]
 [   0    0    4    0    0    1    0    0    3    0   13 2181    6    1
     1    0    0]
 [   0    0    0    0    1   14    0    0    0    0    0    1  516    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1135    4    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    59  282    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.4308943089431

F1 scores:
[       nan 0.94871795 0.98318342 0.9567643  0.94877506 0.96345515
 0.99545455 1.         0.9871345  0.87179487 0.97716895 0.98732458
 0.95115207 0.99462366 0.97216274 0.88958991 0.99408284]

Kappa:
0.9707057903738745
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f742e4a0eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.824, val_acc:0.126]
Epoch [2/120    avg_loss:2.749, val_acc:0.225]
Epoch [3/120    avg_loss:2.685, val_acc:0.311]
Epoch [4/120    avg_loss:2.605, val_acc:0.374]
Epoch [5/120    avg_loss:2.507, val_acc:0.395]
Epoch [6/120    avg_loss:2.469, val_acc:0.417]
Epoch [7/120    avg_loss:2.363, val_acc:0.465]
Epoch [8/120    avg_loss:2.294, val_acc:0.450]
Epoch [9/120    avg_loss:2.234, val_acc:0.464]
Epoch [10/120    avg_loss:2.133, val_acc:0.487]
Epoch [11/120    avg_loss:2.033, val_acc:0.516]
Epoch [12/120    avg_loss:1.940, val_acc:0.548]
Epoch [13/120    avg_loss:1.829, val_acc:0.596]
Epoch [14/120    avg_loss:1.643, val_acc:0.630]
Epoch [15/120    avg_loss:1.597, val_acc:0.573]
Epoch [16/120    avg_loss:1.436, val_acc:0.663]
Epoch [17/120    avg_loss:1.274, val_acc:0.694]
Epoch [18/120    avg_loss:1.263, val_acc:0.715]
Epoch [19/120    avg_loss:1.117, val_acc:0.714]
Epoch [20/120    avg_loss:1.067, val_acc:0.731]
Epoch [21/120    avg_loss:0.960, val_acc:0.772]
Epoch [22/120    avg_loss:0.990, val_acc:0.770]
Epoch [23/120    avg_loss:0.789, val_acc:0.792]
Epoch [24/120    avg_loss:0.722, val_acc:0.799]
Epoch [25/120    avg_loss:0.702, val_acc:0.833]
Epoch [26/120    avg_loss:0.595, val_acc:0.834]
Epoch [27/120    avg_loss:0.592, val_acc:0.857]
Epoch [28/120    avg_loss:0.516, val_acc:0.877]
Epoch [29/120    avg_loss:0.466, val_acc:0.875]
Epoch [30/120    avg_loss:0.431, val_acc:0.870]
Epoch [31/120    avg_loss:0.419, val_acc:0.867]
Epoch [32/120    avg_loss:0.332, val_acc:0.898]
Epoch [33/120    avg_loss:0.336, val_acc:0.865]
Epoch [34/120    avg_loss:0.288, val_acc:0.912]
Epoch [35/120    avg_loss:0.231, val_acc:0.918]
Epoch [36/120    avg_loss:0.238, val_acc:0.920]
Epoch [37/120    avg_loss:0.222, val_acc:0.873]
Epoch [38/120    avg_loss:0.228, val_acc:0.914]
Epoch [39/120    avg_loss:0.170, val_acc:0.938]
Epoch [40/120    avg_loss:0.142, val_acc:0.933]
Epoch [41/120    avg_loss:0.145, val_acc:0.930]
Epoch [42/120    avg_loss:0.129, val_acc:0.955]
Epoch [43/120    avg_loss:0.105, val_acc:0.947]
Epoch [44/120    avg_loss:0.107, val_acc:0.948]
Epoch [45/120    avg_loss:0.091, val_acc:0.953]
Epoch [46/120    avg_loss:0.087, val_acc:0.958]
Epoch [47/120    avg_loss:0.094, val_acc:0.954]
Epoch [48/120    avg_loss:0.091, val_acc:0.961]
Epoch [49/120    avg_loss:0.084, val_acc:0.953]
Epoch [50/120    avg_loss:0.077, val_acc:0.946]
Epoch [51/120    avg_loss:0.077, val_acc:0.957]
Epoch [52/120    avg_loss:0.082, val_acc:0.958]
Epoch [53/120    avg_loss:0.065, val_acc:0.968]
Epoch [54/120    avg_loss:0.077, val_acc:0.942]
Epoch [55/120    avg_loss:0.080, val_acc:0.961]
Epoch [56/120    avg_loss:0.077, val_acc:0.967]
Epoch [57/120    avg_loss:0.077, val_acc:0.950]
Epoch [58/120    avg_loss:0.068, val_acc:0.954]
Epoch [59/120    avg_loss:0.109, val_acc:0.957]
Epoch [60/120    avg_loss:0.076, val_acc:0.954]
Epoch [61/120    avg_loss:0.056, val_acc:0.958]
Epoch [62/120    avg_loss:0.049, val_acc:0.967]
Epoch [63/120    avg_loss:0.072, val_acc:0.952]
Epoch [64/120    avg_loss:0.090, val_acc:0.956]
Epoch [65/120    avg_loss:0.065, val_acc:0.967]
Epoch [66/120    avg_loss:0.056, val_acc:0.962]
Epoch [67/120    avg_loss:0.045, val_acc:0.971]
Epoch [68/120    avg_loss:0.030, val_acc:0.973]
Epoch [69/120    avg_loss:0.030, val_acc:0.971]
Epoch [70/120    avg_loss:0.028, val_acc:0.970]
Epoch [71/120    avg_loss:0.029, val_acc:0.970]
Epoch [72/120    avg_loss:0.027, val_acc:0.971]
Epoch [73/120    avg_loss:0.026, val_acc:0.970]
Epoch [74/120    avg_loss:0.026, val_acc:0.971]
Epoch [75/120    avg_loss:0.025, val_acc:0.971]
Epoch [76/120    avg_loss:0.027, val_acc:0.974]
Epoch [77/120    avg_loss:0.026, val_acc:0.974]
Epoch [78/120    avg_loss:0.030, val_acc:0.974]
Epoch [79/120    avg_loss:0.030, val_acc:0.973]
Epoch [80/120    avg_loss:0.025, val_acc:0.975]
Epoch [81/120    avg_loss:0.024, val_acc:0.973]
Epoch [82/120    avg_loss:0.025, val_acc:0.970]
Epoch [83/120    avg_loss:0.032, val_acc:0.969]
Epoch [84/120    avg_loss:0.024, val_acc:0.970]
Epoch [85/120    avg_loss:0.029, val_acc:0.971]
Epoch [86/120    avg_loss:0.024, val_acc:0.974]
Epoch [87/120    avg_loss:0.025, val_acc:0.974]
Epoch [88/120    avg_loss:0.023, val_acc:0.975]
Epoch [89/120    avg_loss:0.026, val_acc:0.976]
Epoch [90/120    avg_loss:0.024, val_acc:0.976]
Epoch [91/120    avg_loss:0.023, val_acc:0.976]
Epoch [92/120    avg_loss:0.022, val_acc:0.975]
Epoch [93/120    avg_loss:0.022, val_acc:0.973]
Epoch [94/120    avg_loss:0.025, val_acc:0.976]
Epoch [95/120    avg_loss:0.023, val_acc:0.976]
Epoch [96/120    avg_loss:0.023, val_acc:0.975]
Epoch [97/120    avg_loss:0.020, val_acc:0.975]
Epoch [98/120    avg_loss:0.020, val_acc:0.977]
Epoch [99/120    avg_loss:0.021, val_acc:0.976]
Epoch [100/120    avg_loss:0.023, val_acc:0.976]
Epoch [101/120    avg_loss:0.024, val_acc:0.977]
Epoch [102/120    avg_loss:0.025, val_acc:0.976]
Epoch [103/120    avg_loss:0.023, val_acc:0.976]
Epoch [104/120    avg_loss:0.026, val_acc:0.977]
Epoch [105/120    avg_loss:0.019, val_acc:0.977]
Epoch [106/120    avg_loss:0.021, val_acc:0.977]
Epoch [107/120    avg_loss:0.019, val_acc:0.976]
Epoch [108/120    avg_loss:0.021, val_acc:0.976]
Epoch [109/120    avg_loss:0.018, val_acc:0.976]
Epoch [110/120    avg_loss:0.019, val_acc:0.975]
Epoch [111/120    avg_loss:0.021, val_acc:0.975]
Epoch [112/120    avg_loss:0.018, val_acc:0.976]
Epoch [113/120    avg_loss:0.019, val_acc:0.978]
Epoch [114/120    avg_loss:0.021, val_acc:0.975]
Epoch [115/120    avg_loss:0.023, val_acc:0.975]
Epoch [116/120    avg_loss:0.020, val_acc:0.975]
Epoch [117/120    avg_loss:0.022, val_acc:0.976]
Epoch [118/120    avg_loss:0.018, val_acc:0.977]
Epoch [119/120    avg_loss:0.018, val_acc:0.977]
Epoch [120/120    avg_loss:0.022, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1236    2    6    2    0    0    0    1   11   27    0    0
     0    0    0]
 [   0    0    5  698    5    1    0    0    0   16    4    1   17    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   15    0    0    2    1    0    0    0  853    4    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    1   29 2169    5    1
     0    0    0]
 [   0    0    1    1    0    8    0    0    0    0    6    3  511    0
     1    1    2]
 [   0    0    0    1    0    0    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1137    1    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    25  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.36585365853658

F1 scores:
[       nan 0.975      0.97055359 0.96143251 0.97482838 0.98297389
 0.98206278 1.         1.         0.58823529 0.95788883 0.98278206
 0.95692884 0.99459459 0.98697917 0.92283951 0.98224852]

Kappa:
0.9699724609421307
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a72e79f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.814, val_acc:0.182]
Epoch [2/120    avg_loss:2.740, val_acc:0.191]
Epoch [3/120    avg_loss:2.668, val_acc:0.196]
Epoch [4/120    avg_loss:2.590, val_acc:0.220]
Epoch [5/120    avg_loss:2.506, val_acc:0.338]
Epoch [6/120    avg_loss:2.421, val_acc:0.374]
Epoch [7/120    avg_loss:2.326, val_acc:0.391]
Epoch [8/120    avg_loss:2.249, val_acc:0.437]
Epoch [9/120    avg_loss:2.189, val_acc:0.448]
Epoch [10/120    avg_loss:2.050, val_acc:0.504]
Epoch [11/120    avg_loss:1.924, val_acc:0.558]
Epoch [12/120    avg_loss:1.842, val_acc:0.575]
Epoch [13/120    avg_loss:1.778, val_acc:0.507]
Epoch [14/120    avg_loss:1.662, val_acc:0.623]
Epoch [15/120    avg_loss:1.557, val_acc:0.641]
Epoch [16/120    avg_loss:1.420, val_acc:0.685]
Epoch [17/120    avg_loss:1.300, val_acc:0.668]
Epoch [18/120    avg_loss:1.177, val_acc:0.719]
Epoch [19/120    avg_loss:1.033, val_acc:0.725]
Epoch [20/120    avg_loss:0.965, val_acc:0.740]
Epoch [21/120    avg_loss:0.818, val_acc:0.770]
Epoch [22/120    avg_loss:0.766, val_acc:0.734]
Epoch [23/120    avg_loss:0.738, val_acc:0.784]
Epoch [24/120    avg_loss:0.679, val_acc:0.806]
Epoch [25/120    avg_loss:0.582, val_acc:0.812]
Epoch [26/120    avg_loss:0.495, val_acc:0.850]
Epoch [27/120    avg_loss:0.502, val_acc:0.819]
Epoch [28/120    avg_loss:0.431, val_acc:0.860]
Epoch [29/120    avg_loss:0.365, val_acc:0.857]
Epoch [30/120    avg_loss:0.347, val_acc:0.847]
Epoch [31/120    avg_loss:0.301, val_acc:0.884]
Epoch [32/120    avg_loss:0.289, val_acc:0.868]
Epoch [33/120    avg_loss:0.283, val_acc:0.876]
Epoch [34/120    avg_loss:0.236, val_acc:0.878]
Epoch [35/120    avg_loss:0.209, val_acc:0.887]
Epoch [36/120    avg_loss:0.251, val_acc:0.880]
Epoch [37/120    avg_loss:0.182, val_acc:0.895]
Epoch [38/120    avg_loss:0.154, val_acc:0.916]
Epoch [39/120    avg_loss:0.136, val_acc:0.913]
Epoch [40/120    avg_loss:0.144, val_acc:0.907]
Epoch [41/120    avg_loss:0.130, val_acc:0.933]
Epoch [42/120    avg_loss:0.112, val_acc:0.915]
Epoch [43/120    avg_loss:0.110, val_acc:0.915]
Epoch [44/120    avg_loss:0.105, val_acc:0.936]
Epoch [45/120    avg_loss:0.119, val_acc:0.942]
Epoch [46/120    avg_loss:0.110, val_acc:0.934]
Epoch [47/120    avg_loss:0.110, val_acc:0.918]
Epoch [48/120    avg_loss:0.091, val_acc:0.951]
Epoch [49/120    avg_loss:0.084, val_acc:0.937]
Epoch [50/120    avg_loss:0.074, val_acc:0.940]
Epoch [51/120    avg_loss:0.073, val_acc:0.941]
Epoch [52/120    avg_loss:0.071, val_acc:0.953]
Epoch [53/120    avg_loss:0.073, val_acc:0.947]
Epoch [54/120    avg_loss:0.057, val_acc:0.954]
Epoch [55/120    avg_loss:0.082, val_acc:0.951]
Epoch [56/120    avg_loss:0.063, val_acc:0.950]
Epoch [57/120    avg_loss:0.049, val_acc:0.952]
Epoch [58/120    avg_loss:0.045, val_acc:0.955]
Epoch [59/120    avg_loss:0.042, val_acc:0.958]
Epoch [60/120    avg_loss:0.045, val_acc:0.962]
Epoch [61/120    avg_loss:0.036, val_acc:0.960]
Epoch [62/120    avg_loss:0.036, val_acc:0.966]
Epoch [63/120    avg_loss:0.046, val_acc:0.952]
Epoch [64/120    avg_loss:0.045, val_acc:0.954]
Epoch [65/120    avg_loss:0.054, val_acc:0.952]
Epoch [66/120    avg_loss:0.044, val_acc:0.955]
Epoch [67/120    avg_loss:0.045, val_acc:0.960]
Epoch [68/120    avg_loss:0.031, val_acc:0.965]
Epoch [69/120    avg_loss:0.036, val_acc:0.948]
Epoch [70/120    avg_loss:0.046, val_acc:0.915]
Epoch [71/120    avg_loss:0.041, val_acc:0.956]
Epoch [72/120    avg_loss:0.031, val_acc:0.964]
Epoch [73/120    avg_loss:0.037, val_acc:0.963]
Epoch [74/120    avg_loss:0.023, val_acc:0.966]
Epoch [75/120    avg_loss:0.031, val_acc:0.953]
Epoch [76/120    avg_loss:0.028, val_acc:0.946]
Epoch [77/120    avg_loss:0.029, val_acc:0.962]
Epoch [78/120    avg_loss:0.029, val_acc:0.956]
Epoch [79/120    avg_loss:0.024, val_acc:0.967]
Epoch [80/120    avg_loss:0.019, val_acc:0.968]
Epoch [81/120    avg_loss:0.022, val_acc:0.972]
Epoch [82/120    avg_loss:0.018, val_acc:0.969]
Epoch [83/120    avg_loss:0.032, val_acc:0.964]
Epoch [84/120    avg_loss:0.022, val_acc:0.964]
Epoch [85/120    avg_loss:0.025, val_acc:0.972]
Epoch [86/120    avg_loss:0.020, val_acc:0.968]
Epoch [87/120    avg_loss:0.021, val_acc:0.970]
Epoch [88/120    avg_loss:0.019, val_acc:0.972]
Epoch [89/120    avg_loss:0.014, val_acc:0.973]
Epoch [90/120    avg_loss:0.016, val_acc:0.971]
Epoch [91/120    avg_loss:0.014, val_acc:0.972]
Epoch [92/120    avg_loss:0.015, val_acc:0.974]
Epoch [93/120    avg_loss:0.014, val_acc:0.965]
Epoch [94/120    avg_loss:0.026, val_acc:0.956]
Epoch [95/120    avg_loss:0.022, val_acc:0.974]
Epoch [96/120    avg_loss:0.021, val_acc:0.975]
Epoch [97/120    avg_loss:0.022, val_acc:0.960]
Epoch [98/120    avg_loss:0.018, val_acc:0.965]
Epoch [99/120    avg_loss:0.031, val_acc:0.972]
Epoch [100/120    avg_loss:0.016, val_acc:0.974]
Epoch [101/120    avg_loss:0.017, val_acc:0.978]
Epoch [102/120    avg_loss:0.030, val_acc:0.974]
Epoch [103/120    avg_loss:0.024, val_acc:0.963]
Epoch [104/120    avg_loss:0.015, val_acc:0.966]
Epoch [105/120    avg_loss:0.013, val_acc:0.976]
Epoch [106/120    avg_loss:0.012, val_acc:0.971]
Epoch [107/120    avg_loss:0.015, val_acc:0.966]
Epoch [108/120    avg_loss:0.018, val_acc:0.969]
Epoch [109/120    avg_loss:0.010, val_acc:0.973]
Epoch [110/120    avg_loss:0.012, val_acc:0.965]
Epoch [111/120    avg_loss:0.018, val_acc:0.971]
Epoch [112/120    avg_loss:0.013, val_acc:0.971]
Epoch [113/120    avg_loss:0.010, val_acc:0.969]
Epoch [114/120    avg_loss:0.009, val_acc:0.973]
Epoch [115/120    avg_loss:0.010, val_acc:0.976]
Epoch [116/120    avg_loss:0.008, val_acc:0.974]
Epoch [117/120    avg_loss:0.007, val_acc:0.975]
Epoch [118/120    avg_loss:0.007, val_acc:0.975]
Epoch [119/120    avg_loss:0.007, val_acc:0.978]
Epoch [120/120    avg_loss:0.008, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1252    0    0    0    0    0    0    0   13   19    1    0
     0    0    0]
 [   0    0    0  693   12    7    0    0    0   10    6    6   13    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    1    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    5    0    0   13    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    1    1    0    0    0  861    7    0    0
     0    3    0]
 [   0    0    3    0    0    0    0    0    0    0   12 2183   12    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    4    0  525    0
     0    3    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0    0
  1134    4    0]
 [   0    0    0    0    0    0   34    0    0    0    0    0    0    0
    28  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.6260162601626

F1 scores:
[       nan 0.975      0.98505114 0.96183206 0.97260274 0.98627002
 0.96893491 0.98039216 1.         0.61904762 0.97123519 0.98577557
 0.96596136 1.         0.98523023 0.88785047 0.98203593]

Kappa:
0.9729296750401963
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f10826a7f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.825, val_acc:0.134]
Epoch [2/120    avg_loss:2.772, val_acc:0.178]
Epoch [3/120    avg_loss:2.723, val_acc:0.204]
Epoch [4/120    avg_loss:2.654, val_acc:0.327]
Epoch [5/120    avg_loss:2.598, val_acc:0.410]
Epoch [6/120    avg_loss:2.531, val_acc:0.432]
Epoch [7/120    avg_loss:2.444, val_acc:0.455]
Epoch [8/120    avg_loss:2.359, val_acc:0.481]
Epoch [9/120    avg_loss:2.333, val_acc:0.482]
Epoch [10/120    avg_loss:2.226, val_acc:0.508]
Epoch [11/120    avg_loss:2.191, val_acc:0.543]
Epoch [12/120    avg_loss:2.127, val_acc:0.577]
Epoch [13/120    avg_loss:2.045, val_acc:0.590]
Epoch [14/120    avg_loss:1.978, val_acc:0.623]
Epoch [15/120    avg_loss:1.851, val_acc:0.636]
Epoch [16/120    avg_loss:1.730, val_acc:0.661]
Epoch [17/120    avg_loss:1.643, val_acc:0.652]
Epoch [18/120    avg_loss:1.566, val_acc:0.649]
Epoch [19/120    avg_loss:1.408, val_acc:0.678]
Epoch [20/120    avg_loss:1.388, val_acc:0.683]
Epoch [21/120    avg_loss:1.249, val_acc:0.705]
Epoch [22/120    avg_loss:1.164, val_acc:0.721]
Epoch [23/120    avg_loss:1.052, val_acc:0.762]
Epoch [24/120    avg_loss:0.975, val_acc:0.763]
Epoch [25/120    avg_loss:0.932, val_acc:0.794]
Epoch [26/120    avg_loss:0.836, val_acc:0.784]
Epoch [27/120    avg_loss:0.799, val_acc:0.769]
Epoch [28/120    avg_loss:0.698, val_acc:0.808]
Epoch [29/120    avg_loss:0.683, val_acc:0.826]
Epoch [30/120    avg_loss:0.606, val_acc:0.839]
Epoch [31/120    avg_loss:0.569, val_acc:0.809]
Epoch [32/120    avg_loss:0.515, val_acc:0.872]
Epoch [33/120    avg_loss:0.466, val_acc:0.868]
Epoch [34/120    avg_loss:0.404, val_acc:0.876]
Epoch [35/120    avg_loss:0.433, val_acc:0.773]
Epoch [36/120    avg_loss:0.465, val_acc:0.910]
Epoch [37/120    avg_loss:0.393, val_acc:0.895]
Epoch [38/120    avg_loss:0.307, val_acc:0.901]
Epoch [39/120    avg_loss:0.259, val_acc:0.925]
Epoch [40/120    avg_loss:0.289, val_acc:0.904]
Epoch [41/120    avg_loss:0.271, val_acc:0.929]
Epoch [42/120    avg_loss:0.206, val_acc:0.909]
Epoch [43/120    avg_loss:0.194, val_acc:0.941]
Epoch [44/120    avg_loss:0.223, val_acc:0.898]
Epoch [45/120    avg_loss:0.200, val_acc:0.932]
Epoch [46/120    avg_loss:0.175, val_acc:0.919]
Epoch [47/120    avg_loss:0.168, val_acc:0.906]
Epoch [48/120    avg_loss:0.160, val_acc:0.936]
Epoch [49/120    avg_loss:0.125, val_acc:0.955]
Epoch [50/120    avg_loss:0.116, val_acc:0.954]
Epoch [51/120    avg_loss:0.099, val_acc:0.958]
Epoch [52/120    avg_loss:0.102, val_acc:0.945]
Epoch [53/120    avg_loss:0.082, val_acc:0.957]
Epoch [54/120    avg_loss:0.093, val_acc:0.925]
Epoch [55/120    avg_loss:0.100, val_acc:0.932]
Epoch [56/120    avg_loss:0.085, val_acc:0.959]
Epoch [57/120    avg_loss:0.076, val_acc:0.956]
Epoch [58/120    avg_loss:0.095, val_acc:0.958]
Epoch [59/120    avg_loss:0.125, val_acc:0.940]
Epoch [60/120    avg_loss:0.129, val_acc:0.953]
Epoch [61/120    avg_loss:0.085, val_acc:0.961]
Epoch [62/120    avg_loss:0.097, val_acc:0.954]
Epoch [63/120    avg_loss:0.076, val_acc:0.965]
Epoch [64/120    avg_loss:0.054, val_acc:0.964]
Epoch [65/120    avg_loss:0.058, val_acc:0.939]
Epoch [66/120    avg_loss:0.067, val_acc:0.965]
Epoch [67/120    avg_loss:0.058, val_acc:0.957]
Epoch [68/120    avg_loss:0.053, val_acc:0.955]
Epoch [69/120    avg_loss:0.047, val_acc:0.965]
Epoch [70/120    avg_loss:0.047, val_acc:0.957]
Epoch [71/120    avg_loss:0.068, val_acc:0.969]
Epoch [72/120    avg_loss:0.044, val_acc:0.969]
Epoch [73/120    avg_loss:0.046, val_acc:0.962]
Epoch [74/120    avg_loss:0.055, val_acc:0.963]
Epoch [75/120    avg_loss:0.056, val_acc:0.964]
Epoch [76/120    avg_loss:0.046, val_acc:0.969]
Epoch [77/120    avg_loss:0.038, val_acc:0.969]
Epoch [78/120    avg_loss:0.050, val_acc:0.978]
Epoch [79/120    avg_loss:0.032, val_acc:0.977]
Epoch [80/120    avg_loss:0.050, val_acc:0.961]
Epoch [81/120    avg_loss:0.056, val_acc:0.966]
Epoch [82/120    avg_loss:0.030, val_acc:0.978]
Epoch [83/120    avg_loss:0.039, val_acc:0.980]
Epoch [84/120    avg_loss:0.046, val_acc:0.969]
Epoch [85/120    avg_loss:0.051, val_acc:0.961]
Epoch [86/120    avg_loss:0.035, val_acc:0.973]
Epoch [87/120    avg_loss:0.029, val_acc:0.961]
Epoch [88/120    avg_loss:0.026, val_acc:0.969]
Epoch [89/120    avg_loss:0.026, val_acc:0.970]
Epoch [90/120    avg_loss:0.028, val_acc:0.969]
Epoch [91/120    avg_loss:0.026, val_acc:0.979]
Epoch [92/120    avg_loss:0.020, val_acc:0.977]
Epoch [93/120    avg_loss:0.016, val_acc:0.977]
Epoch [94/120    avg_loss:0.019, val_acc:0.969]
Epoch [95/120    avg_loss:0.021, val_acc:0.974]
Epoch [96/120    avg_loss:0.027, val_acc:0.971]
Epoch [97/120    avg_loss:0.017, val_acc:0.976]
Epoch [98/120    avg_loss:0.018, val_acc:0.979]
Epoch [99/120    avg_loss:0.014, val_acc:0.980]
Epoch [100/120    avg_loss:0.013, val_acc:0.982]
Epoch [101/120    avg_loss:0.014, val_acc:0.980]
Epoch [102/120    avg_loss:0.015, val_acc:0.981]
Epoch [103/120    avg_loss:0.013, val_acc:0.981]
Epoch [104/120    avg_loss:0.012, val_acc:0.981]
Epoch [105/120    avg_loss:0.012, val_acc:0.981]
Epoch [106/120    avg_loss:0.011, val_acc:0.980]
Epoch [107/120    avg_loss:0.012, val_acc:0.980]
Epoch [108/120    avg_loss:0.013, val_acc:0.980]
Epoch [109/120    avg_loss:0.013, val_acc:0.981]
Epoch [110/120    avg_loss:0.014, val_acc:0.981]
Epoch [111/120    avg_loss:0.013, val_acc:0.980]
Epoch [112/120    avg_loss:0.013, val_acc:0.981]
Epoch [113/120    avg_loss:0.015, val_acc:0.979]
Epoch [114/120    avg_loss:0.011, val_acc:0.979]
Epoch [115/120    avg_loss:0.012, val_acc:0.979]
Epoch [116/120    avg_loss:0.012, val_acc:0.979]
Epoch [117/120    avg_loss:0.016, val_acc:0.980]
Epoch [118/120    avg_loss:0.012, val_acc:0.979]
Epoch [119/120    avg_loss:0.012, val_acc:0.979]
Epoch [120/120    avg_loss:0.012, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1240    2    0    0    4    0    0    1    8   27    1    0
     0    2    0]
 [   0    0    4  694    3   21    0    0    0   16    0    0    8    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    2    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   13    0    2    0    0
     0    0    0]
 [   0    0   18   64    0    6    0    0    0    0  777    1    0    0
     0    9    0]
 [   0    0    4    0    0    0   11    0    9    0    2 2180    4    0
     0    0    0]
 [   0    0    0   11    0   21    0    0    0    0    1   15  484    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0    0    0
  1131    6    0]
 [   0    0    0    0    0    0    1    0    0    5    0    0    0    0
    23  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.44444444444444

F1 scores:
[       nan 0.975      0.97216778 0.914361   0.99300699 0.94078947
 0.98498498 0.96153846 0.98964327 0.45614035 0.93221356 0.98286745
 0.9379845  1.         0.98562092 0.93255132 0.98809524]

Kappa:
0.9594704025441733
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdf79d2af28>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.807, val_acc:0.173]
Epoch [2/120    avg_loss:2.758, val_acc:0.267]
Epoch [3/120    avg_loss:2.682, val_acc:0.315]
Epoch [4/120    avg_loss:2.612, val_acc:0.381]
Epoch [5/120    avg_loss:2.536, val_acc:0.377]
Epoch [6/120    avg_loss:2.441, val_acc:0.454]
Epoch [7/120    avg_loss:2.351, val_acc:0.514]
Epoch [8/120    avg_loss:2.271, val_acc:0.426]
Epoch [9/120    avg_loss:2.221, val_acc:0.497]
Epoch [10/120    avg_loss:2.175, val_acc:0.554]
Epoch [11/120    avg_loss:2.125, val_acc:0.579]
Epoch [12/120    avg_loss:2.038, val_acc:0.614]
Epoch [13/120    avg_loss:1.969, val_acc:0.590]
Epoch [14/120    avg_loss:1.939, val_acc:0.607]
Epoch [15/120    avg_loss:1.845, val_acc:0.646]
Epoch [16/120    avg_loss:1.759, val_acc:0.678]
Epoch [17/120    avg_loss:1.686, val_acc:0.706]
Epoch [18/120    avg_loss:1.567, val_acc:0.705]
Epoch [19/120    avg_loss:1.453, val_acc:0.693]
Epoch [20/120    avg_loss:1.301, val_acc:0.660]
Epoch [21/120    avg_loss:1.240, val_acc:0.744]
Epoch [22/120    avg_loss:1.168, val_acc:0.716]
Epoch [23/120    avg_loss:1.030, val_acc:0.760]
Epoch [24/120    avg_loss:0.924, val_acc:0.775]
Epoch [25/120    avg_loss:0.851, val_acc:0.758]
Epoch [26/120    avg_loss:0.770, val_acc:0.769]
Epoch [27/120    avg_loss:0.699, val_acc:0.816]
Epoch [28/120    avg_loss:0.701, val_acc:0.834]
Epoch [29/120    avg_loss:0.591, val_acc:0.825]
Epoch [30/120    avg_loss:0.611, val_acc:0.834]
Epoch [31/120    avg_loss:0.567, val_acc:0.841]
Epoch [32/120    avg_loss:0.495, val_acc:0.836]
Epoch [33/120    avg_loss:0.492, val_acc:0.867]
Epoch [34/120    avg_loss:0.446, val_acc:0.866]
Epoch [35/120    avg_loss:0.377, val_acc:0.874]
Epoch [36/120    avg_loss:0.377, val_acc:0.878]
Epoch [37/120    avg_loss:0.336, val_acc:0.860]
Epoch [38/120    avg_loss:0.293, val_acc:0.884]
Epoch [39/120    avg_loss:0.293, val_acc:0.873]
Epoch [40/120    avg_loss:0.309, val_acc:0.887]
Epoch [41/120    avg_loss:0.287, val_acc:0.901]
Epoch [42/120    avg_loss:0.278, val_acc:0.894]
Epoch [43/120    avg_loss:0.268, val_acc:0.894]
Epoch [44/120    avg_loss:0.187, val_acc:0.930]
Epoch [45/120    avg_loss:0.165, val_acc:0.917]
Epoch [46/120    avg_loss:0.167, val_acc:0.920]
Epoch [47/120    avg_loss:0.159, val_acc:0.905]
Epoch [48/120    avg_loss:0.125, val_acc:0.944]
Epoch [49/120    avg_loss:0.117, val_acc:0.934]
Epoch [50/120    avg_loss:0.099, val_acc:0.943]
Epoch [51/120    avg_loss:0.093, val_acc:0.943]
Epoch [52/120    avg_loss:0.115, val_acc:0.941]
Epoch [53/120    avg_loss:0.120, val_acc:0.941]
Epoch [54/120    avg_loss:0.108, val_acc:0.930]
Epoch [55/120    avg_loss:0.101, val_acc:0.949]
Epoch [56/120    avg_loss:0.093, val_acc:0.949]
Epoch [57/120    avg_loss:0.085, val_acc:0.944]
Epoch [58/120    avg_loss:0.073, val_acc:0.948]
Epoch [59/120    avg_loss:0.062, val_acc:0.941]
Epoch [60/120    avg_loss:0.058, val_acc:0.945]
Epoch [61/120    avg_loss:0.091, val_acc:0.939]
Epoch [62/120    avg_loss:0.092, val_acc:0.945]
Epoch [63/120    avg_loss:0.064, val_acc:0.953]
Epoch [64/120    avg_loss:0.059, val_acc:0.961]
Epoch [65/120    avg_loss:0.050, val_acc:0.949]
Epoch [66/120    avg_loss:0.061, val_acc:0.955]
Epoch [67/120    avg_loss:0.055, val_acc:0.946]
Epoch [68/120    avg_loss:0.046, val_acc:0.959]
Epoch [69/120    avg_loss:0.044, val_acc:0.959]
Epoch [70/120    avg_loss:0.032, val_acc:0.952]
Epoch [71/120    avg_loss:0.031, val_acc:0.959]
Epoch [72/120    avg_loss:0.034, val_acc:0.953]
Epoch [73/120    avg_loss:0.043, val_acc:0.946]
Epoch [74/120    avg_loss:0.060, val_acc:0.949]
Epoch [75/120    avg_loss:0.052, val_acc:0.958]
Epoch [76/120    avg_loss:0.033, val_acc:0.961]
Epoch [77/120    avg_loss:0.029, val_acc:0.966]
Epoch [78/120    avg_loss:0.025, val_acc:0.957]
Epoch [79/120    avg_loss:0.026, val_acc:0.966]
Epoch [80/120    avg_loss:0.033, val_acc:0.961]
Epoch [81/120    avg_loss:0.028, val_acc:0.959]
Epoch [82/120    avg_loss:0.034, val_acc:0.967]
Epoch [83/120    avg_loss:0.037, val_acc:0.963]
Epoch [84/120    avg_loss:0.030, val_acc:0.971]
Epoch [85/120    avg_loss:0.035, val_acc:0.968]
Epoch [86/120    avg_loss:0.028, val_acc:0.966]
Epoch [87/120    avg_loss:0.019, val_acc:0.965]
Epoch [88/120    avg_loss:0.021, val_acc:0.965]
Epoch [89/120    avg_loss:0.017, val_acc:0.976]
Epoch [90/120    avg_loss:0.016, val_acc:0.975]
Epoch [91/120    avg_loss:0.035, val_acc:0.974]
Epoch [92/120    avg_loss:0.023, val_acc:0.975]
Epoch [93/120    avg_loss:0.020, val_acc:0.965]
Epoch [94/120    avg_loss:0.019, val_acc:0.974]
Epoch [95/120    avg_loss:0.020, val_acc:0.967]
Epoch [96/120    avg_loss:0.027, val_acc:0.967]
Epoch [97/120    avg_loss:0.021, val_acc:0.967]
Epoch [98/120    avg_loss:0.021, val_acc:0.982]
Epoch [99/120    avg_loss:0.016, val_acc:0.970]
Epoch [100/120    avg_loss:0.017, val_acc:0.971]
Epoch [101/120    avg_loss:0.013, val_acc:0.972]
Epoch [102/120    avg_loss:0.021, val_acc:0.976]
Epoch [103/120    avg_loss:0.027, val_acc:0.962]
Epoch [104/120    avg_loss:0.022, val_acc:0.975]
Epoch [105/120    avg_loss:0.022, val_acc:0.971]
Epoch [106/120    avg_loss:0.015, val_acc:0.974]
Epoch [107/120    avg_loss:0.014, val_acc:0.975]
Epoch [108/120    avg_loss:0.013, val_acc:0.972]
Epoch [109/120    avg_loss:0.016, val_acc:0.970]
Epoch [110/120    avg_loss:0.017, val_acc:0.959]
Epoch [111/120    avg_loss:0.020, val_acc:0.980]
Epoch [112/120    avg_loss:0.013, val_acc:0.977]
Epoch [113/120    avg_loss:0.010, val_acc:0.977]
Epoch [114/120    avg_loss:0.010, val_acc:0.977]
Epoch [115/120    avg_loss:0.013, val_acc:0.976]
Epoch [116/120    avg_loss:0.009, val_acc:0.976]
Epoch [117/120    avg_loss:0.010, val_acc:0.976]
Epoch [118/120    avg_loss:0.008, val_acc:0.977]
Epoch [119/120    avg_loss:0.010, val_acc:0.976]
Epoch [120/120    avg_loss:0.009, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1261    1    0    0    1    0    0    0    1   20    1    0
     0    0    0]
 [   0    0    7  728    0    0    0    0    0   10    1    1    0    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    1    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    4    5    0    4    0    0    0    0  854    5    1    0
     0    2    0]
 [   0    0    7    0    0    0    2    0    0    0   12 2184    5    0
     0    0    0]
 [   0    0    0   11    2    7    0    0    0    0    8   10  494    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1137    1    0]
 [   0    0    0    0    0    0   35    0    0    0    0    0    0    0
    58  254    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.42005420054201

F1 scores:
[       nan 0.94871795 0.98361934 0.97521768 0.99297424 0.98521047
 0.9704142  1.         0.9953271  0.73913043 0.97266515 0.98578199
 0.95       1.         0.9738758  0.8410596  0.99408284]

Kappa:
0.9705597244640691
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f208d51bf28>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.839, val_acc:0.197]
Epoch [2/120    avg_loss:2.790, val_acc:0.223]
Epoch [3/120    avg_loss:2.743, val_acc:0.251]
Epoch [4/120    avg_loss:2.705, val_acc:0.262]
Epoch [5/120    avg_loss:2.625, val_acc:0.300]
Epoch [6/120    avg_loss:2.566, val_acc:0.348]
Epoch [7/120    avg_loss:2.489, val_acc:0.370]
Epoch [8/120    avg_loss:2.391, val_acc:0.362]
Epoch [9/120    avg_loss:2.306, val_acc:0.423]
Epoch [10/120    avg_loss:2.221, val_acc:0.447]
Epoch [11/120    avg_loss:2.162, val_acc:0.486]
Epoch [12/120    avg_loss:2.072, val_acc:0.545]
Epoch [13/120    avg_loss:1.972, val_acc:0.569]
Epoch [14/120    avg_loss:1.868, val_acc:0.548]
Epoch [15/120    avg_loss:1.819, val_acc:0.567]
Epoch [16/120    avg_loss:1.662, val_acc:0.583]
Epoch [17/120    avg_loss:1.537, val_acc:0.666]
Epoch [18/120    avg_loss:1.448, val_acc:0.684]
Epoch [19/120    avg_loss:1.337, val_acc:0.692]
Epoch [20/120    avg_loss:1.199, val_acc:0.690]
Epoch [21/120    avg_loss:1.106, val_acc:0.716]
Epoch [22/120    avg_loss:0.988, val_acc:0.757]
Epoch [23/120    avg_loss:0.935, val_acc:0.773]
Epoch [24/120    avg_loss:0.842, val_acc:0.783]
Epoch [25/120    avg_loss:0.827, val_acc:0.816]
Epoch [26/120    avg_loss:0.739, val_acc:0.795]
Epoch [27/120    avg_loss:0.736, val_acc:0.806]
Epoch [28/120    avg_loss:0.737, val_acc:0.831]
Epoch [29/120    avg_loss:0.662, val_acc:0.836]
Epoch [30/120    avg_loss:0.572, val_acc:0.873]
Epoch [31/120    avg_loss:0.506, val_acc:0.852]
Epoch [32/120    avg_loss:0.486, val_acc:0.861]
Epoch [33/120    avg_loss:0.443, val_acc:0.873]
Epoch [34/120    avg_loss:0.403, val_acc:0.870]
Epoch [35/120    avg_loss:0.329, val_acc:0.906]
Epoch [36/120    avg_loss:0.292, val_acc:0.880]
Epoch [37/120    avg_loss:0.310, val_acc:0.876]
Epoch [38/120    avg_loss:0.331, val_acc:0.907]
Epoch [39/120    avg_loss:0.321, val_acc:0.882]
Epoch [40/120    avg_loss:0.307, val_acc:0.896]
Epoch [41/120    avg_loss:0.290, val_acc:0.907]
Epoch [42/120    avg_loss:0.294, val_acc:0.897]
Epoch [43/120    avg_loss:0.330, val_acc:0.922]
Epoch [44/120    avg_loss:0.228, val_acc:0.931]
Epoch [45/120    avg_loss:0.211, val_acc:0.916]
Epoch [46/120    avg_loss:0.193, val_acc:0.934]
Epoch [47/120    avg_loss:0.198, val_acc:0.936]
Epoch [48/120    avg_loss:0.154, val_acc:0.944]
Epoch [49/120    avg_loss:0.115, val_acc:0.957]
Epoch [50/120    avg_loss:0.115, val_acc:0.946]
Epoch [51/120    avg_loss:0.115, val_acc:0.955]
Epoch [52/120    avg_loss:0.123, val_acc:0.955]
Epoch [53/120    avg_loss:0.088, val_acc:0.950]
Epoch [54/120    avg_loss:0.143, val_acc:0.950]
Epoch [55/120    avg_loss:0.115, val_acc:0.964]
Epoch [56/120    avg_loss:0.088, val_acc:0.963]
Epoch [57/120    avg_loss:0.077, val_acc:0.962]
Epoch [58/120    avg_loss:0.078, val_acc:0.965]
Epoch [59/120    avg_loss:0.064, val_acc:0.968]
Epoch [60/120    avg_loss:0.059, val_acc:0.973]
Epoch [61/120    avg_loss:0.053, val_acc:0.965]
Epoch [62/120    avg_loss:0.130, val_acc:0.966]
Epoch [63/120    avg_loss:0.097, val_acc:0.963]
Epoch [64/120    avg_loss:0.078, val_acc:0.967]
Epoch [65/120    avg_loss:0.067, val_acc:0.964]
Epoch [66/120    avg_loss:0.096, val_acc:0.952]
Epoch [67/120    avg_loss:0.075, val_acc:0.966]
Epoch [68/120    avg_loss:0.058, val_acc:0.974]
Epoch [69/120    avg_loss:0.049, val_acc:0.973]
Epoch [70/120    avg_loss:0.064, val_acc:0.959]
Epoch [71/120    avg_loss:0.043, val_acc:0.977]
Epoch [72/120    avg_loss:0.035, val_acc:0.976]
Epoch [73/120    avg_loss:0.037, val_acc:0.974]
Epoch [74/120    avg_loss:0.036, val_acc:0.974]
Epoch [75/120    avg_loss:0.027, val_acc:0.980]
Epoch [76/120    avg_loss:0.025, val_acc:0.978]
Epoch [77/120    avg_loss:0.038, val_acc:0.967]
Epoch [78/120    avg_loss:0.065, val_acc:0.969]
Epoch [79/120    avg_loss:0.044, val_acc:0.968]
Epoch [80/120    avg_loss:0.042, val_acc:0.982]
Epoch [81/120    avg_loss:0.034, val_acc:0.976]
Epoch [82/120    avg_loss:0.026, val_acc:0.981]
Epoch [83/120    avg_loss:0.028, val_acc:0.971]
Epoch [84/120    avg_loss:0.027, val_acc:0.976]
Epoch [85/120    avg_loss:0.024, val_acc:0.977]
Epoch [86/120    avg_loss:0.033, val_acc:0.982]
Epoch [87/120    avg_loss:0.034, val_acc:0.970]
Epoch [88/120    avg_loss:0.040, val_acc:0.973]
Epoch [89/120    avg_loss:0.032, val_acc:0.977]
Epoch [90/120    avg_loss:0.023, val_acc:0.985]
Epoch [91/120    avg_loss:0.019, val_acc:0.980]
Epoch [92/120    avg_loss:0.016, val_acc:0.984]
Epoch [93/120    avg_loss:0.018, val_acc:0.988]
Epoch [94/120    avg_loss:0.017, val_acc:0.987]
Epoch [95/120    avg_loss:0.017, val_acc:0.986]
Epoch [96/120    avg_loss:0.017, val_acc:0.986]
Epoch [97/120    avg_loss:0.015, val_acc:0.981]
Epoch [98/120    avg_loss:0.013, val_acc:0.982]
Epoch [99/120    avg_loss:0.013, val_acc:0.984]
Epoch [100/120    avg_loss:0.018, val_acc:0.984]
Epoch [101/120    avg_loss:0.013, val_acc:0.981]
Epoch [102/120    avg_loss:0.012, val_acc:0.987]
Epoch [103/120    avg_loss:0.011, val_acc:0.984]
Epoch [104/120    avg_loss:0.010, val_acc:0.986]
Epoch [105/120    avg_loss:0.010, val_acc:0.984]
Epoch [106/120    avg_loss:0.010, val_acc:0.986]
Epoch [107/120    avg_loss:0.010, val_acc:0.987]
Epoch [108/120    avg_loss:0.009, val_acc:0.986]
Epoch [109/120    avg_loss:0.008, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.985]
Epoch [111/120    avg_loss:0.009, val_acc:0.986]
Epoch [112/120    avg_loss:0.009, val_acc:0.987]
Epoch [113/120    avg_loss:0.008, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.987]
Epoch [115/120    avg_loss:0.009, val_acc:0.987]
Epoch [116/120    avg_loss:0.010, val_acc:0.985]
Epoch [117/120    avg_loss:0.008, val_acc:0.986]
Epoch [118/120    avg_loss:0.008, val_acc:0.986]
Epoch [119/120    avg_loss:0.010, val_acc:0.985]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1263    1    0    0    1    0    0    0    2   16    0    0
     0    2    0]
 [   0    0    5  712    0   13    0    0    0   12    1    2    0    2
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6   10    0    5    0    0    0    0  844    7    1    0
     2    0    0]
 [   0    0   10    0    0    1    2    0    0    0   20 2174    3    0
     0    0    0]
 [   0    0    0   13    1   13    0    0    0    0    0   11  494    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    46  290    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.53929539295393

F1 scores:
[       nan 1.         0.98326197 0.95956873 0.99530516 0.95991091
 0.98869631 1.         1.         0.72       0.96900115 0.9834879
 0.95736434 0.99462366 0.97852234 0.90766823 0.98823529]

Kappa:
0.9719357296547458
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4d6ecf2f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.819, val_acc:0.060]
Epoch [2/120    avg_loss:2.768, val_acc:0.148]
Epoch [3/120    avg_loss:2.718, val_acc:0.271]
Epoch [4/120    avg_loss:2.671, val_acc:0.325]
Epoch [5/120    avg_loss:2.609, val_acc:0.410]
Epoch [6/120    avg_loss:2.527, val_acc:0.418]
Epoch [7/120    avg_loss:2.471, val_acc:0.412]
Epoch [8/120    avg_loss:2.418, val_acc:0.525]
Epoch [9/120    avg_loss:2.389, val_acc:0.511]
Epoch [10/120    avg_loss:2.298, val_acc:0.566]
Epoch [11/120    avg_loss:2.232, val_acc:0.571]
Epoch [12/120    avg_loss:2.179, val_acc:0.575]
Epoch [13/120    avg_loss:2.093, val_acc:0.578]
Epoch [14/120    avg_loss:2.058, val_acc:0.592]
Epoch [15/120    avg_loss:1.981, val_acc:0.619]
Epoch [16/120    avg_loss:1.945, val_acc:0.620]
Epoch [17/120    avg_loss:1.860, val_acc:0.624]
Epoch [18/120    avg_loss:1.795, val_acc:0.639]
Epoch [19/120    avg_loss:1.696, val_acc:0.657]
Epoch [20/120    avg_loss:1.569, val_acc:0.676]
Epoch [21/120    avg_loss:1.422, val_acc:0.713]
Epoch [22/120    avg_loss:1.374, val_acc:0.704]
Epoch [23/120    avg_loss:1.242, val_acc:0.740]
Epoch [24/120    avg_loss:1.136, val_acc:0.750]
Epoch [25/120    avg_loss:1.135, val_acc:0.727]
Epoch [26/120    avg_loss:1.052, val_acc:0.752]
Epoch [27/120    avg_loss:0.970, val_acc:0.783]
Epoch [28/120    avg_loss:0.851, val_acc:0.791]
Epoch [29/120    avg_loss:0.779, val_acc:0.794]
Epoch [30/120    avg_loss:0.691, val_acc:0.847]
Epoch [31/120    avg_loss:0.610, val_acc:0.851]
Epoch [32/120    avg_loss:0.609, val_acc:0.849]
Epoch [33/120    avg_loss:0.516, val_acc:0.864]
Epoch [34/120    avg_loss:0.491, val_acc:0.877]
Epoch [35/120    avg_loss:0.469, val_acc:0.893]
Epoch [36/120    avg_loss:0.544, val_acc:0.887]
Epoch [37/120    avg_loss:0.452, val_acc:0.872]
Epoch [38/120    avg_loss:0.389, val_acc:0.916]
Epoch [39/120    avg_loss:0.351, val_acc:0.893]
Epoch [40/120    avg_loss:0.297, val_acc:0.901]
Epoch [41/120    avg_loss:0.280, val_acc:0.908]
Epoch [42/120    avg_loss:0.244, val_acc:0.922]
Epoch [43/120    avg_loss:0.233, val_acc:0.902]
Epoch [44/120    avg_loss:0.207, val_acc:0.922]
Epoch [45/120    avg_loss:0.260, val_acc:0.938]
Epoch [46/120    avg_loss:0.227, val_acc:0.902]
Epoch [47/120    avg_loss:0.216, val_acc:0.923]
Epoch [48/120    avg_loss:0.183, val_acc:0.947]
Epoch [49/120    avg_loss:0.147, val_acc:0.957]
Epoch [50/120    avg_loss:0.142, val_acc:0.945]
Epoch [51/120    avg_loss:0.162, val_acc:0.925]
Epoch [52/120    avg_loss:0.186, val_acc:0.955]
Epoch [53/120    avg_loss:0.111, val_acc:0.946]
Epoch [54/120    avg_loss:0.109, val_acc:0.966]
Epoch [55/120    avg_loss:0.117, val_acc:0.959]
Epoch [56/120    avg_loss:0.103, val_acc:0.955]
Epoch [57/120    avg_loss:0.087, val_acc:0.959]
Epoch [58/120    avg_loss:0.076, val_acc:0.972]
Epoch [59/120    avg_loss:0.081, val_acc:0.966]
Epoch [60/120    avg_loss:0.078, val_acc:0.950]
Epoch [61/120    avg_loss:0.086, val_acc:0.968]
Epoch [62/120    avg_loss:0.092, val_acc:0.965]
Epoch [63/120    avg_loss:0.088, val_acc:0.957]
Epoch [64/120    avg_loss:0.086, val_acc:0.964]
Epoch [65/120    avg_loss:0.060, val_acc:0.968]
Epoch [66/120    avg_loss:0.093, val_acc:0.957]
Epoch [67/120    avg_loss:0.065, val_acc:0.971]
Epoch [68/120    avg_loss:0.078, val_acc:0.968]
Epoch [69/120    avg_loss:0.060, val_acc:0.968]
Epoch [70/120    avg_loss:0.053, val_acc:0.954]
Epoch [71/120    avg_loss:0.044, val_acc:0.975]
Epoch [72/120    avg_loss:0.049, val_acc:0.968]
Epoch [73/120    avg_loss:0.047, val_acc:0.977]
Epoch [74/120    avg_loss:0.046, val_acc:0.962]
Epoch [75/120    avg_loss:0.048, val_acc:0.972]
Epoch [76/120    avg_loss:0.043, val_acc:0.972]
Epoch [77/120    avg_loss:0.070, val_acc:0.979]
Epoch [78/120    avg_loss:0.042, val_acc:0.980]
Epoch [79/120    avg_loss:0.031, val_acc:0.982]
Epoch [80/120    avg_loss:0.026, val_acc:0.981]
Epoch [81/120    avg_loss:0.033, val_acc:0.980]
Epoch [82/120    avg_loss:0.029, val_acc:0.976]
Epoch [83/120    avg_loss:0.036, val_acc:0.979]
Epoch [84/120    avg_loss:0.027, val_acc:0.973]
Epoch [85/120    avg_loss:0.041, val_acc:0.982]
Epoch [86/120    avg_loss:0.034, val_acc:0.983]
Epoch [87/120    avg_loss:0.021, val_acc:0.980]
Epoch [88/120    avg_loss:0.027, val_acc:0.984]
Epoch [89/120    avg_loss:0.034, val_acc:0.971]
Epoch [90/120    avg_loss:0.038, val_acc:0.982]
Epoch [91/120    avg_loss:0.025, val_acc:0.981]
Epoch [92/120    avg_loss:0.024, val_acc:0.979]
Epoch [93/120    avg_loss:0.028, val_acc:0.977]
Epoch [94/120    avg_loss:0.026, val_acc:0.977]
Epoch [95/120    avg_loss:0.022, val_acc:0.986]
Epoch [96/120    avg_loss:0.028, val_acc:0.976]
Epoch [97/120    avg_loss:0.028, val_acc:0.982]
Epoch [98/120    avg_loss:0.025, val_acc:0.983]
Epoch [99/120    avg_loss:0.022, val_acc:0.983]
Epoch [100/120    avg_loss:0.021, val_acc:0.983]
Epoch [101/120    avg_loss:0.024, val_acc:0.973]
Epoch [102/120    avg_loss:0.023, val_acc:0.988]
Epoch [103/120    avg_loss:0.020, val_acc:0.984]
Epoch [104/120    avg_loss:0.018, val_acc:0.984]
Epoch [105/120    avg_loss:0.015, val_acc:0.989]
Epoch [106/120    avg_loss:0.020, val_acc:0.984]
Epoch [107/120    avg_loss:0.020, val_acc:0.985]
Epoch [108/120    avg_loss:0.015, val_acc:0.986]
Epoch [109/120    avg_loss:0.025, val_acc:0.982]
Epoch [110/120    avg_loss:0.082, val_acc:0.947]
Epoch [111/120    avg_loss:0.827, val_acc:0.740]
Epoch [112/120    avg_loss:0.569, val_acc:0.910]
Epoch [113/120    avg_loss:0.191, val_acc:0.931]
Epoch [114/120    avg_loss:0.189, val_acc:0.931]
Epoch [115/120    avg_loss:0.119, val_acc:0.948]
Epoch [116/120    avg_loss:0.082, val_acc:0.959]
Epoch [117/120    avg_loss:0.068, val_acc:0.971]
Epoch [118/120    avg_loss:0.062, val_acc:0.952]
Epoch [119/120    avg_loss:0.050, val_acc:0.970]
Epoch [120/120    avg_loss:0.032, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1221    4    7    0    0    0    0    2   12   38    1    0
     0    0    0]
 [   0    0    1  726    3    4    0    0    0    9    0    1    0    1
     0    2    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    2    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   14   34    0    7    0    0    0    0  787   29    2    0
     2    0    0]
 [   0    0    2    0    0    0    1    0    0    0   10 2190    7    0
     0    0    0]
 [   0    0    0   15    1    6    0    0    0    0    8    9  494    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   20    0    0    0    0    0    0    0    0
  1119    0    0]
 [   0    0    0    0    0    0   40    0    0    0    0    0    0    0
    79  228    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.88075880758808

F1 scores:
[       nan 0.975      0.96789536 0.95150721 0.97482838 0.95227525
 0.9697417  0.96153846 1.         0.76595745 0.92916175 0.97833371
 0.95183044 0.99730458 0.95518566 0.79029463 0.99408284]

Kappa:
0.9529732187746364
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4c53a49f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.801, val_acc:0.087]
Epoch [2/120    avg_loss:2.749, val_acc:0.145]
Epoch [3/120    avg_loss:2.695, val_acc:0.147]
Epoch [4/120    avg_loss:2.636, val_acc:0.161]
Epoch [5/120    avg_loss:2.560, val_acc:0.223]
Epoch [6/120    avg_loss:2.507, val_acc:0.322]
Epoch [7/120    avg_loss:2.427, val_acc:0.365]
Epoch [8/120    avg_loss:2.366, val_acc:0.461]
Epoch [9/120    avg_loss:2.278, val_acc:0.488]
Epoch [10/120    avg_loss:2.231, val_acc:0.526]
Epoch [11/120    avg_loss:2.137, val_acc:0.558]
Epoch [12/120    avg_loss:2.091, val_acc:0.545]
Epoch [13/120    avg_loss:2.062, val_acc:0.560]
Epoch [14/120    avg_loss:1.959, val_acc:0.582]
Epoch [15/120    avg_loss:1.912, val_acc:0.613]
Epoch [16/120    avg_loss:1.802, val_acc:0.602]
Epoch [17/120    avg_loss:1.755, val_acc:0.662]
Epoch [18/120    avg_loss:1.691, val_acc:0.698]
Epoch [19/120    avg_loss:1.621, val_acc:0.730]
Epoch [20/120    avg_loss:1.445, val_acc:0.751]
Epoch [21/120    avg_loss:1.302, val_acc:0.732]
Epoch [22/120    avg_loss:1.310, val_acc:0.747]
Epoch [23/120    avg_loss:1.179, val_acc:0.793]
Epoch [24/120    avg_loss:1.048, val_acc:0.766]
Epoch [25/120    avg_loss:0.906, val_acc:0.802]
Epoch [26/120    avg_loss:0.834, val_acc:0.799]
Epoch [27/120    avg_loss:0.750, val_acc:0.805]
Epoch [28/120    avg_loss:0.683, val_acc:0.852]
Epoch [29/120    avg_loss:0.546, val_acc:0.872]
Epoch [30/120    avg_loss:0.533, val_acc:0.841]
Epoch [31/120    avg_loss:0.500, val_acc:0.876]
Epoch [32/120    avg_loss:0.458, val_acc:0.863]
Epoch [33/120    avg_loss:0.443, val_acc:0.871]
Epoch [34/120    avg_loss:0.458, val_acc:0.871]
Epoch [35/120    avg_loss:0.310, val_acc:0.907]
Epoch [36/120    avg_loss:0.267, val_acc:0.907]
Epoch [37/120    avg_loss:0.281, val_acc:0.885]
Epoch [38/120    avg_loss:0.231, val_acc:0.906]
Epoch [39/120    avg_loss:0.206, val_acc:0.914]
Epoch [40/120    avg_loss:0.171, val_acc:0.930]
Epoch [41/120    avg_loss:0.168, val_acc:0.925]
Epoch [42/120    avg_loss:0.193, val_acc:0.931]
Epoch [43/120    avg_loss:0.180, val_acc:0.925]
Epoch [44/120    avg_loss:0.148, val_acc:0.938]
Epoch [45/120    avg_loss:0.115, val_acc:0.943]
Epoch [46/120    avg_loss:0.122, val_acc:0.931]
Epoch [47/120    avg_loss:0.141, val_acc:0.944]
Epoch [48/120    avg_loss:0.127, val_acc:0.943]
Epoch [49/120    avg_loss:0.114, val_acc:0.932]
Epoch [50/120    avg_loss:0.106, val_acc:0.956]
Epoch [51/120    avg_loss:0.115, val_acc:0.948]
Epoch [52/120    avg_loss:0.098, val_acc:0.957]
Epoch [53/120    avg_loss:0.087, val_acc:0.950]
Epoch [54/120    avg_loss:0.088, val_acc:0.948]
Epoch [55/120    avg_loss:0.087, val_acc:0.957]
Epoch [56/120    avg_loss:0.081, val_acc:0.952]
Epoch [57/120    avg_loss:0.064, val_acc:0.956]
Epoch [58/120    avg_loss:0.066, val_acc:0.955]
Epoch [59/120    avg_loss:0.062, val_acc:0.961]
Epoch [60/120    avg_loss:0.079, val_acc:0.951]
Epoch [61/120    avg_loss:0.060, val_acc:0.959]
Epoch [62/120    avg_loss:0.052, val_acc:0.961]
Epoch [63/120    avg_loss:0.046, val_acc:0.964]
Epoch [64/120    avg_loss:0.038, val_acc:0.968]
Epoch [65/120    avg_loss:0.034, val_acc:0.967]
Epoch [66/120    avg_loss:0.036, val_acc:0.969]
Epoch [67/120    avg_loss:0.038, val_acc:0.975]
Epoch [68/120    avg_loss:0.042, val_acc:0.965]
Epoch [69/120    avg_loss:0.051, val_acc:0.957]
Epoch [70/120    avg_loss:0.046, val_acc:0.956]
Epoch [71/120    avg_loss:0.039, val_acc:0.966]
Epoch [72/120    avg_loss:0.032, val_acc:0.971]
Epoch [73/120    avg_loss:0.039, val_acc:0.974]
Epoch [74/120    avg_loss:0.038, val_acc:0.969]
Epoch [75/120    avg_loss:0.028, val_acc:0.970]
Epoch [76/120    avg_loss:0.052, val_acc:0.961]
Epoch [77/120    avg_loss:0.066, val_acc:0.965]
Epoch [78/120    avg_loss:0.062, val_acc:0.942]
Epoch [79/120    avg_loss:0.033, val_acc:0.968]
Epoch [80/120    avg_loss:0.031, val_acc:0.973]
Epoch [81/120    avg_loss:0.030, val_acc:0.969]
Epoch [82/120    avg_loss:0.021, val_acc:0.968]
Epoch [83/120    avg_loss:0.022, val_acc:0.970]
Epoch [84/120    avg_loss:0.018, val_acc:0.971]
Epoch [85/120    avg_loss:0.022, val_acc:0.974]
Epoch [86/120    avg_loss:0.020, val_acc:0.975]
Epoch [87/120    avg_loss:0.018, val_acc:0.976]
Epoch [88/120    avg_loss:0.017, val_acc:0.974]
Epoch [89/120    avg_loss:0.018, val_acc:0.976]
Epoch [90/120    avg_loss:0.018, val_acc:0.976]
Epoch [91/120    avg_loss:0.018, val_acc:0.976]
Epoch [92/120    avg_loss:0.017, val_acc:0.978]
Epoch [93/120    avg_loss:0.020, val_acc:0.979]
Epoch [94/120    avg_loss:0.016, val_acc:0.978]
Epoch [95/120    avg_loss:0.018, val_acc:0.977]
Epoch [96/120    avg_loss:0.018, val_acc:0.978]
Epoch [97/120    avg_loss:0.018, val_acc:0.978]
Epoch [98/120    avg_loss:0.016, val_acc:0.977]
Epoch [99/120    avg_loss:0.019, val_acc:0.976]
Epoch [100/120    avg_loss:0.017, val_acc:0.976]
Epoch [101/120    avg_loss:0.017, val_acc:0.977]
Epoch [102/120    avg_loss:0.015, val_acc:0.977]
Epoch [103/120    avg_loss:0.014, val_acc:0.977]
Epoch [104/120    avg_loss:0.016, val_acc:0.977]
Epoch [105/120    avg_loss:0.017, val_acc:0.977]
Epoch [106/120    avg_loss:0.016, val_acc:0.977]
Epoch [107/120    avg_loss:0.014, val_acc:0.977]
Epoch [108/120    avg_loss:0.015, val_acc:0.977]
Epoch [109/120    avg_loss:0.015, val_acc:0.977]
Epoch [110/120    avg_loss:0.014, val_acc:0.977]
Epoch [111/120    avg_loss:0.015, val_acc:0.977]
Epoch [112/120    avg_loss:0.016, val_acc:0.977]
Epoch [113/120    avg_loss:0.016, val_acc:0.977]
Epoch [114/120    avg_loss:0.016, val_acc:0.977]
Epoch [115/120    avg_loss:0.015, val_acc:0.977]
Epoch [116/120    avg_loss:0.016, val_acc:0.977]
Epoch [117/120    avg_loss:0.015, val_acc:0.977]
Epoch [118/120    avg_loss:0.016, val_acc:0.977]
Epoch [119/120    avg_loss:0.016, val_acc:0.977]
Epoch [120/120    avg_loss:0.014, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1261    0    5    0    1    0    0    0    3   12    3    0
     0    0    0]
 [   0    0    0  725    1    0    0    0    0   20    0    0    1    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   23   28    0    3    3    0    0    0  807    6    5    0
     0    0    0]
 [   0    0    8    1    0    0    0    0    1    0   19 2176    5    0
     0    0    0]
 [   0    0    0   22    2    2    0    0    0    0   13    0  492    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   18    0    0    0    0    0    0    0
    71  258    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.88888888888889

F1 scores:
[       nan 0.975      0.97865735 0.95206829 0.98156682 0.98966705
 0.98353293 0.98039216 0.99883856 0.61818182 0.93891798 0.98819255
 0.94433781 1.         0.96853741 0.85289256 0.97647059]

Kappa:
0.964518545105093
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe955ccaf28>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.820, val_acc:0.202]
Epoch [2/120    avg_loss:2.768, val_acc:0.288]
Epoch [3/120    avg_loss:2.717, val_acc:0.372]
Epoch [4/120    avg_loss:2.641, val_acc:0.433]
Epoch [5/120    avg_loss:2.562, val_acc:0.469]
Epoch [6/120    avg_loss:2.487, val_acc:0.475]
Epoch [7/120    avg_loss:2.372, val_acc:0.450]
Epoch [8/120    avg_loss:2.304, val_acc:0.457]
Epoch [9/120    avg_loss:2.281, val_acc:0.488]
Epoch [10/120    avg_loss:2.209, val_acc:0.493]
Epoch [11/120    avg_loss:2.116, val_acc:0.531]
Epoch [12/120    avg_loss:2.102, val_acc:0.560]
Epoch [13/120    avg_loss:2.047, val_acc:0.587]
Epoch [14/120    avg_loss:1.976, val_acc:0.593]
Epoch [15/120    avg_loss:1.856, val_acc:0.629]
Epoch [16/120    avg_loss:1.795, val_acc:0.658]
Epoch [17/120    avg_loss:1.646, val_acc:0.644]
Epoch [18/120    avg_loss:1.516, val_acc:0.673]
Epoch [19/120    avg_loss:1.381, val_acc:0.686]
Epoch [20/120    avg_loss:1.233, val_acc:0.708]
Epoch [21/120    avg_loss:1.182, val_acc:0.714]
Epoch [22/120    avg_loss:1.131, val_acc:0.689]
Epoch [23/120    avg_loss:1.026, val_acc:0.726]
Epoch [24/120    avg_loss:0.862, val_acc:0.786]
Epoch [25/120    avg_loss:0.812, val_acc:0.775]
Epoch [26/120    avg_loss:0.656, val_acc:0.796]
Epoch [27/120    avg_loss:0.642, val_acc:0.820]
Epoch [28/120    avg_loss:0.582, val_acc:0.814]
Epoch [29/120    avg_loss:0.503, val_acc:0.839]
Epoch [30/120    avg_loss:0.468, val_acc:0.852]
Epoch [31/120    avg_loss:0.443, val_acc:0.853]
Epoch [32/120    avg_loss:0.397, val_acc:0.855]
Epoch [33/120    avg_loss:0.393, val_acc:0.861]
Epoch [34/120    avg_loss:0.393, val_acc:0.855]
Epoch [35/120    avg_loss:0.334, val_acc:0.874]
Epoch [36/120    avg_loss:0.280, val_acc:0.867]
Epoch [37/120    avg_loss:0.253, val_acc:0.871]
Epoch [38/120    avg_loss:0.275, val_acc:0.905]
Epoch [39/120    avg_loss:0.241, val_acc:0.894]
Epoch [40/120    avg_loss:0.227, val_acc:0.905]
Epoch [41/120    avg_loss:0.179, val_acc:0.914]
Epoch [42/120    avg_loss:0.167, val_acc:0.910]
Epoch [43/120    avg_loss:0.192, val_acc:0.906]
Epoch [44/120    avg_loss:0.172, val_acc:0.896]
Epoch [45/120    avg_loss:0.217, val_acc:0.851]
Epoch [46/120    avg_loss:0.229, val_acc:0.880]
Epoch [47/120    avg_loss:0.162, val_acc:0.904]
Epoch [48/120    avg_loss:0.146, val_acc:0.917]
Epoch [49/120    avg_loss:0.130, val_acc:0.936]
Epoch [50/120    avg_loss:0.124, val_acc:0.910]
Epoch [51/120    avg_loss:0.115, val_acc:0.923]
Epoch [52/120    avg_loss:0.112, val_acc:0.927]
Epoch [53/120    avg_loss:0.114, val_acc:0.925]
Epoch [54/120    avg_loss:0.134, val_acc:0.928]
Epoch [55/120    avg_loss:0.118, val_acc:0.929]
Epoch [56/120    avg_loss:0.108, val_acc:0.932]
Epoch [57/120    avg_loss:0.092, val_acc:0.933]
Epoch [58/120    avg_loss:0.098, val_acc:0.933]
Epoch [59/120    avg_loss:0.093, val_acc:0.943]
Epoch [60/120    avg_loss:0.088, val_acc:0.947]
Epoch [61/120    avg_loss:0.063, val_acc:0.941]
Epoch [62/120    avg_loss:0.067, val_acc:0.932]
Epoch [63/120    avg_loss:0.053, val_acc:0.952]
Epoch [64/120    avg_loss:0.056, val_acc:0.941]
Epoch [65/120    avg_loss:0.055, val_acc:0.955]
Epoch [66/120    avg_loss:0.058, val_acc:0.942]
Epoch [67/120    avg_loss:0.040, val_acc:0.948]
Epoch [68/120    avg_loss:0.048, val_acc:0.944]
Epoch [69/120    avg_loss:0.070, val_acc:0.947]
Epoch [70/120    avg_loss:0.049, val_acc:0.954]
Epoch [71/120    avg_loss:0.043, val_acc:0.944]
Epoch [72/120    avg_loss:0.044, val_acc:0.934]
Epoch [73/120    avg_loss:0.255, val_acc:0.893]
Epoch [74/120    avg_loss:0.242, val_acc:0.889]
Epoch [75/120    avg_loss:0.134, val_acc:0.925]
Epoch [76/120    avg_loss:0.109, val_acc:0.941]
Epoch [77/120    avg_loss:0.103, val_acc:0.927]
Epoch [78/120    avg_loss:0.071, val_acc:0.948]
Epoch [79/120    avg_loss:0.055, val_acc:0.952]
Epoch [80/120    avg_loss:0.053, val_acc:0.954]
Epoch [81/120    avg_loss:0.047, val_acc:0.957]
Epoch [82/120    avg_loss:0.045, val_acc:0.959]
Epoch [83/120    avg_loss:0.041, val_acc:0.957]
Epoch [84/120    avg_loss:0.039, val_acc:0.959]
Epoch [85/120    avg_loss:0.044, val_acc:0.955]
Epoch [86/120    avg_loss:0.040, val_acc:0.961]
Epoch [87/120    avg_loss:0.037, val_acc:0.959]
Epoch [88/120    avg_loss:0.041, val_acc:0.957]
Epoch [89/120    avg_loss:0.054, val_acc:0.957]
Epoch [90/120    avg_loss:0.040, val_acc:0.961]
Epoch [91/120    avg_loss:0.041, val_acc:0.959]
Epoch [92/120    avg_loss:0.037, val_acc:0.956]
Epoch [93/120    avg_loss:0.037, val_acc:0.961]
Epoch [94/120    avg_loss:0.036, val_acc:0.961]
Epoch [95/120    avg_loss:0.036, val_acc:0.963]
Epoch [96/120    avg_loss:0.034, val_acc:0.962]
Epoch [97/120    avg_loss:0.035, val_acc:0.963]
Epoch [98/120    avg_loss:0.037, val_acc:0.963]
Epoch [99/120    avg_loss:0.035, val_acc:0.961]
Epoch [100/120    avg_loss:0.037, val_acc:0.965]
Epoch [101/120    avg_loss:0.034, val_acc:0.964]
Epoch [102/120    avg_loss:0.040, val_acc:0.963]
Epoch [103/120    avg_loss:0.034, val_acc:0.964]
Epoch [104/120    avg_loss:0.037, val_acc:0.961]
Epoch [105/120    avg_loss:0.036, val_acc:0.964]
Epoch [106/120    avg_loss:0.030, val_acc:0.964]
Epoch [107/120    avg_loss:0.033, val_acc:0.965]
Epoch [108/120    avg_loss:0.034, val_acc:0.966]
Epoch [109/120    avg_loss:0.030, val_acc:0.969]
Epoch [110/120    avg_loss:0.031, val_acc:0.963]
Epoch [111/120    avg_loss:0.032, val_acc:0.965]
Epoch [112/120    avg_loss:0.032, val_acc:0.966]
Epoch [113/120    avg_loss:0.034, val_acc:0.966]
Epoch [114/120    avg_loss:0.029, val_acc:0.967]
Epoch [115/120    avg_loss:0.032, val_acc:0.969]
Epoch [116/120    avg_loss:0.029, val_acc:0.964]
Epoch [117/120    avg_loss:0.039, val_acc:0.969]
Epoch [118/120    avg_loss:0.031, val_acc:0.966]
Epoch [119/120    avg_loss:0.028, val_acc:0.965]
Epoch [120/120    avg_loss:0.027, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1232    5   11    0    3    0    0    0    7   26    1    0
     0    0    0]
 [   0    0    5  677    2   26    0    0    0   16    1    0   16    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    3    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0   22   19    0    8    0    0    0    0  813   10    0    0
     0    3    0]
 [   0    0   16    0    0    0   13    0    3    0   17 2155    2    4
     0    0    0]
 [   0    0    0    7    5   13    0    0    0    0    2   16  488    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    1    0    0    0    0    0    0    1    0    0    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   45    0    0    0    0    0    0    0
    24  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.9349593495935

F1 scores:
[       nan 0.95       0.9625     0.93058419 0.95945946 0.94298246
 0.95411508 0.98039216 0.99069767 0.57692308 0.94644936 0.97489256
 0.93129771 0.97612732 0.98869565 0.88535032 0.98245614]

Kappa:
0.9536633204557492
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd845766f98>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.807, val_acc:0.204]
Epoch [2/120    avg_loss:2.747, val_acc:0.395]
Epoch [3/120    avg_loss:2.694, val_acc:0.421]
Epoch [4/120    avg_loss:2.609, val_acc:0.420]
Epoch [5/120    avg_loss:2.540, val_acc:0.455]
Epoch [6/120    avg_loss:2.459, val_acc:0.471]
Epoch [7/120    avg_loss:2.434, val_acc:0.485]
Epoch [8/120    avg_loss:2.367, val_acc:0.500]
Epoch [9/120    avg_loss:2.299, val_acc:0.524]
Epoch [10/120    avg_loss:2.246, val_acc:0.532]
Epoch [11/120    avg_loss:2.187, val_acc:0.543]
Epoch [12/120    avg_loss:2.070, val_acc:0.547]
Epoch [13/120    avg_loss:2.042, val_acc:0.602]
Epoch [14/120    avg_loss:1.921, val_acc:0.625]
Epoch [15/120    avg_loss:1.790, val_acc:0.641]
Epoch [16/120    avg_loss:1.706, val_acc:0.661]
Epoch [17/120    avg_loss:1.626, val_acc:0.661]
Epoch [18/120    avg_loss:1.509, val_acc:0.662]
Epoch [19/120    avg_loss:1.410, val_acc:0.694]
Epoch [20/120    avg_loss:1.250, val_acc:0.715]
Epoch [21/120    avg_loss:1.166, val_acc:0.728]
Epoch [22/120    avg_loss:1.086, val_acc:0.750]
Epoch [23/120    avg_loss:1.025, val_acc:0.765]
Epoch [24/120    avg_loss:0.933, val_acc:0.772]
Epoch [25/120    avg_loss:0.826, val_acc:0.752]
Epoch [26/120    avg_loss:0.857, val_acc:0.798]
Epoch [27/120    avg_loss:0.792, val_acc:0.833]
Epoch [28/120    avg_loss:0.726, val_acc:0.786]
Epoch [29/120    avg_loss:0.736, val_acc:0.830]
Epoch [30/120    avg_loss:0.660, val_acc:0.838]
Epoch [31/120    avg_loss:0.613, val_acc:0.842]
Epoch [32/120    avg_loss:0.529, val_acc:0.846]
Epoch [33/120    avg_loss:0.492, val_acc:0.852]
Epoch [34/120    avg_loss:0.435, val_acc:0.866]
Epoch [35/120    avg_loss:0.380, val_acc:0.900]
Epoch [36/120    avg_loss:0.334, val_acc:0.872]
Epoch [37/120    avg_loss:0.362, val_acc:0.882]
Epoch [38/120    avg_loss:0.297, val_acc:0.882]
Epoch [39/120    avg_loss:0.288, val_acc:0.886]
Epoch [40/120    avg_loss:0.297, val_acc:0.917]
Epoch [41/120    avg_loss:0.253, val_acc:0.908]
Epoch [42/120    avg_loss:0.218, val_acc:0.936]
Epoch [43/120    avg_loss:0.174, val_acc:0.924]
Epoch [44/120    avg_loss:0.182, val_acc:0.922]
Epoch [45/120    avg_loss:0.177, val_acc:0.928]
Epoch [46/120    avg_loss:0.166, val_acc:0.929]
Epoch [47/120    avg_loss:0.184, val_acc:0.933]
Epoch [48/120    avg_loss:0.139, val_acc:0.931]
Epoch [49/120    avg_loss:0.129, val_acc:0.938]
Epoch [50/120    avg_loss:0.137, val_acc:0.930]
Epoch [51/120    avg_loss:0.120, val_acc:0.931]
Epoch [52/120    avg_loss:0.105, val_acc:0.947]
Epoch [53/120    avg_loss:0.114, val_acc:0.940]
Epoch [54/120    avg_loss:0.108, val_acc:0.935]
Epoch [55/120    avg_loss:0.101, val_acc:0.940]
Epoch [56/120    avg_loss:0.072, val_acc:0.959]
Epoch [57/120    avg_loss:0.069, val_acc:0.961]
Epoch [58/120    avg_loss:0.088, val_acc:0.955]
Epoch [59/120    avg_loss:0.071, val_acc:0.956]
Epoch [60/120    avg_loss:0.065, val_acc:0.966]
Epoch [61/120    avg_loss:0.105, val_acc:0.948]
Epoch [62/120    avg_loss:0.072, val_acc:0.963]
Epoch [63/120    avg_loss:0.069, val_acc:0.958]
Epoch [64/120    avg_loss:0.065, val_acc:0.953]
Epoch [65/120    avg_loss:0.061, val_acc:0.954]
Epoch [66/120    avg_loss:0.060, val_acc:0.958]
Epoch [67/120    avg_loss:0.048, val_acc:0.957]
Epoch [68/120    avg_loss:0.037, val_acc:0.961]
Epoch [69/120    avg_loss:0.039, val_acc:0.969]
Epoch [70/120    avg_loss:0.045, val_acc:0.967]
Epoch [71/120    avg_loss:0.040, val_acc:0.965]
Epoch [72/120    avg_loss:0.036, val_acc:0.970]
Epoch [73/120    avg_loss:0.050, val_acc:0.973]
Epoch [74/120    avg_loss:0.036, val_acc:0.962]
Epoch [75/120    avg_loss:0.050, val_acc:0.976]
Epoch [76/120    avg_loss:0.031, val_acc:0.975]
Epoch [77/120    avg_loss:0.031, val_acc:0.970]
Epoch [78/120    avg_loss:0.027, val_acc:0.976]
Epoch [79/120    avg_loss:0.033, val_acc:0.976]
Epoch [80/120    avg_loss:0.027, val_acc:0.968]
Epoch [81/120    avg_loss:0.033, val_acc:0.971]
Epoch [82/120    avg_loss:0.031, val_acc:0.971]
Epoch [83/120    avg_loss:0.032, val_acc:0.975]
Epoch [84/120    avg_loss:0.044, val_acc:0.967]
Epoch [85/120    avg_loss:0.034, val_acc:0.962]
Epoch [86/120    avg_loss:0.033, val_acc:0.970]
Epoch [87/120    avg_loss:0.022, val_acc:0.976]
Epoch [88/120    avg_loss:0.026, val_acc:0.962]
Epoch [89/120    avg_loss:0.034, val_acc:0.976]
Epoch [90/120    avg_loss:0.026, val_acc:0.973]
Epoch [91/120    avg_loss:0.031, val_acc:0.974]
Epoch [92/120    avg_loss:0.028, val_acc:0.974]
Epoch [93/120    avg_loss:0.040, val_acc:0.970]
Epoch [94/120    avg_loss:0.023, val_acc:0.975]
Epoch [95/120    avg_loss:0.019, val_acc:0.981]
Epoch [96/120    avg_loss:0.018, val_acc:0.982]
Epoch [97/120    avg_loss:0.016, val_acc:0.976]
Epoch [98/120    avg_loss:0.019, val_acc:0.974]
Epoch [99/120    avg_loss:0.018, val_acc:0.974]
Epoch [100/120    avg_loss:0.021, val_acc:0.977]
Epoch [101/120    avg_loss:0.016, val_acc:0.978]
Epoch [102/120    avg_loss:0.018, val_acc:0.979]
Epoch [103/120    avg_loss:0.015, val_acc:0.976]
Epoch [104/120    avg_loss:0.026, val_acc:0.967]
Epoch [105/120    avg_loss:0.024, val_acc:0.979]
Epoch [106/120    avg_loss:0.019, val_acc:0.978]
Epoch [107/120    avg_loss:0.016, val_acc:0.980]
Epoch [108/120    avg_loss:0.032, val_acc:0.958]
Epoch [109/120    avg_loss:0.018, val_acc:0.980]
Epoch [110/120    avg_loss:0.016, val_acc:0.981]
Epoch [111/120    avg_loss:0.013, val_acc:0.986]
Epoch [112/120    avg_loss:0.011, val_acc:0.986]
Epoch [113/120    avg_loss:0.013, val_acc:0.982]
Epoch [114/120    avg_loss:0.012, val_acc:0.985]
Epoch [115/120    avg_loss:0.010, val_acc:0.984]
Epoch [116/120    avg_loss:0.014, val_acc:0.985]
Epoch [117/120    avg_loss:0.010, val_acc:0.986]
Epoch [118/120    avg_loss:0.013, val_acc:0.987]
Epoch [119/120    avg_loss:0.011, val_acc:0.985]
Epoch [120/120    avg_loss:0.010, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1250    3    3    0    0    0    0    0    3   26    0    0
     0    0    0]
 [   0    0    5  710    2   13    0    0    0    4    0    0   11    2
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   12   32    0    2    0    0    0    0  807   11    3    0
     2    6    0]
 [   0    0    2    0    0    0    2    0    0    0   12 2190    4    0
     0    0    0]
 [   0    0    0    3    1    6    0    0    0    0    8    6  502    0
     0    4    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    62  282    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.15989159891599

F1 scores:
[       nan 0.98765432 0.9788567  0.94919786 0.9837587  0.97297297
 0.99620925 1.         1.         0.85714286 0.94607268 0.98559856
 0.95256167 0.99462366 0.97267293 0.88262911 0.97674419]

Kappa:
0.9675964127883584
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbac6e2ef28>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.810, val_acc:0.100]
Epoch [2/120    avg_loss:2.762, val_acc:0.149]
Epoch [3/120    avg_loss:2.721, val_acc:0.249]
Epoch [4/120    avg_loss:2.658, val_acc:0.254]
Epoch [5/120    avg_loss:2.585, val_acc:0.297]
Epoch [6/120    avg_loss:2.512, val_acc:0.333]
Epoch [7/120    avg_loss:2.434, val_acc:0.374]
Epoch [8/120    avg_loss:2.380, val_acc:0.418]
Epoch [9/120    avg_loss:2.271, val_acc:0.452]
Epoch [10/120    avg_loss:2.227, val_acc:0.500]
Epoch [11/120    avg_loss:2.155, val_acc:0.543]
Epoch [12/120    avg_loss:2.054, val_acc:0.555]
Epoch [13/120    avg_loss:1.933, val_acc:0.587]
Epoch [14/120    avg_loss:1.853, val_acc:0.598]
Epoch [15/120    avg_loss:1.758, val_acc:0.588]
Epoch [16/120    avg_loss:1.681, val_acc:0.606]
Epoch [17/120    avg_loss:1.715, val_acc:0.627]
Epoch [18/120    avg_loss:1.487, val_acc:0.663]
Epoch [19/120    avg_loss:1.383, val_acc:0.684]
Epoch [20/120    avg_loss:1.261, val_acc:0.711]
Epoch [21/120    avg_loss:1.164, val_acc:0.713]
Epoch [22/120    avg_loss:1.113, val_acc:0.741]
Epoch [23/120    avg_loss:0.976, val_acc:0.752]
Epoch [24/120    avg_loss:0.946, val_acc:0.759]
Epoch [25/120    avg_loss:0.826, val_acc:0.783]
Epoch [26/120    avg_loss:0.788, val_acc:0.822]
Epoch [27/120    avg_loss:0.621, val_acc:0.802]
Epoch [28/120    avg_loss:0.568, val_acc:0.815]
Epoch [29/120    avg_loss:0.520, val_acc:0.864]
Epoch [30/120    avg_loss:0.468, val_acc:0.877]
Epoch [31/120    avg_loss:0.403, val_acc:0.859]
Epoch [32/120    avg_loss:0.393, val_acc:0.852]
Epoch [33/120    avg_loss:0.441, val_acc:0.819]
Epoch [34/120    avg_loss:0.417, val_acc:0.887]
Epoch [35/120    avg_loss:0.330, val_acc:0.896]
Epoch [36/120    avg_loss:0.303, val_acc:0.908]
Epoch [37/120    avg_loss:0.230, val_acc:0.899]
Epoch [38/120    avg_loss:0.240, val_acc:0.899]
Epoch [39/120    avg_loss:0.240, val_acc:0.908]
Epoch [40/120    avg_loss:0.206, val_acc:0.909]
Epoch [41/120    avg_loss:0.194, val_acc:0.925]
Epoch [42/120    avg_loss:0.170, val_acc:0.934]
Epoch [43/120    avg_loss:0.169, val_acc:0.924]
Epoch [44/120    avg_loss:0.166, val_acc:0.919]
Epoch [45/120    avg_loss:0.159, val_acc:0.923]
Epoch [46/120    avg_loss:0.149, val_acc:0.939]
Epoch [47/120    avg_loss:0.141, val_acc:0.927]
Epoch [48/120    avg_loss:0.144, val_acc:0.931]
Epoch [49/120    avg_loss:0.131, val_acc:0.941]
Epoch [50/120    avg_loss:0.103, val_acc:0.952]
Epoch [51/120    avg_loss:0.143, val_acc:0.933]
Epoch [52/120    avg_loss:0.162, val_acc:0.932]
Epoch [53/120    avg_loss:0.128, val_acc:0.942]
Epoch [54/120    avg_loss:0.116, val_acc:0.923]
Epoch [55/120    avg_loss:0.087, val_acc:0.956]
Epoch [56/120    avg_loss:0.088, val_acc:0.944]
Epoch [57/120    avg_loss:0.089, val_acc:0.948]
Epoch [58/120    avg_loss:0.075, val_acc:0.946]
Epoch [59/120    avg_loss:0.066, val_acc:0.953]
Epoch [60/120    avg_loss:0.081, val_acc:0.936]
Epoch [61/120    avg_loss:0.081, val_acc:0.948]
Epoch [62/120    avg_loss:0.067, val_acc:0.962]
Epoch [63/120    avg_loss:0.065, val_acc:0.954]
Epoch [64/120    avg_loss:0.050, val_acc:0.962]
Epoch [65/120    avg_loss:0.052, val_acc:0.961]
Epoch [66/120    avg_loss:0.059, val_acc:0.961]
Epoch [67/120    avg_loss:0.046, val_acc:0.961]
Epoch [68/120    avg_loss:0.040, val_acc:0.966]
Epoch [69/120    avg_loss:0.057, val_acc:0.954]
Epoch [70/120    avg_loss:0.052, val_acc:0.956]
Epoch [71/120    avg_loss:0.046, val_acc:0.961]
Epoch [72/120    avg_loss:0.050, val_acc:0.951]
Epoch [73/120    avg_loss:0.046, val_acc:0.967]
Epoch [74/120    avg_loss:0.046, val_acc:0.956]
Epoch [75/120    avg_loss:0.045, val_acc:0.966]
Epoch [76/120    avg_loss:0.038, val_acc:0.957]
Epoch [77/120    avg_loss:0.038, val_acc:0.962]
Epoch [78/120    avg_loss:0.034, val_acc:0.970]
Epoch [79/120    avg_loss:0.032, val_acc:0.969]
Epoch [80/120    avg_loss:0.034, val_acc:0.963]
Epoch [81/120    avg_loss:0.037, val_acc:0.961]
Epoch [82/120    avg_loss:0.035, val_acc:0.963]
Epoch [83/120    avg_loss:0.024, val_acc:0.969]
Epoch [84/120    avg_loss:0.028, val_acc:0.966]
Epoch [85/120    avg_loss:0.026, val_acc:0.964]
Epoch [86/120    avg_loss:0.026, val_acc:0.968]
Epoch [87/120    avg_loss:0.027, val_acc:0.970]
Epoch [88/120    avg_loss:0.037, val_acc:0.959]
Epoch [89/120    avg_loss:0.029, val_acc:0.966]
Epoch [90/120    avg_loss:0.026, val_acc:0.974]
Epoch [91/120    avg_loss:0.020, val_acc:0.964]
Epoch [92/120    avg_loss:0.021, val_acc:0.966]
Epoch [93/120    avg_loss:0.029, val_acc:0.966]
Epoch [94/120    avg_loss:0.034, val_acc:0.967]
Epoch [95/120    avg_loss:0.028, val_acc:0.965]
Epoch [96/120    avg_loss:0.020, val_acc:0.971]
Epoch [97/120    avg_loss:0.022, val_acc:0.966]
Epoch [98/120    avg_loss:0.023, val_acc:0.969]
Epoch [99/120    avg_loss:0.024, val_acc:0.973]
Epoch [100/120    avg_loss:0.027, val_acc:0.974]
Epoch [101/120    avg_loss:0.018, val_acc:0.975]
Epoch [102/120    avg_loss:0.020, val_acc:0.974]
Epoch [103/120    avg_loss:0.017, val_acc:0.977]
Epoch [104/120    avg_loss:0.015, val_acc:0.976]
Epoch [105/120    avg_loss:0.015, val_acc:0.975]
Epoch [106/120    avg_loss:0.016, val_acc:0.971]
Epoch [107/120    avg_loss:0.022, val_acc:0.975]
Epoch [108/120    avg_loss:0.027, val_acc:0.970]
Epoch [109/120    avg_loss:0.017, val_acc:0.969]
Epoch [110/120    avg_loss:0.033, val_acc:0.969]
Epoch [111/120    avg_loss:0.024, val_acc:0.978]
Epoch [112/120    avg_loss:0.021, val_acc:0.977]
Epoch [113/120    avg_loss:0.019, val_acc:0.974]
Epoch [114/120    avg_loss:0.020, val_acc:0.968]
Epoch [115/120    avg_loss:0.019, val_acc:0.968]
Epoch [116/120    avg_loss:0.011, val_acc:0.978]
Epoch [117/120    avg_loss:0.012, val_acc:0.980]
Epoch [118/120    avg_loss:0.011, val_acc:0.974]
Epoch [119/120    avg_loss:0.010, val_acc:0.975]
Epoch [120/120    avg_loss:0.026, val_acc:0.951]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1195    0    0    0    0    0    0    0   25   65    0    0
     0    0    0]
 [   0    0    3  632    9   35    0    0    0   10    0    0   58    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    3    0    0    0    0
     3    0    0]
 [   0    0    7    0    0    0  647    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   12    0    0    0    0    0    0  418    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    6    0    0   10    0    0    0    0
     0    0    0]
 [   0    0   51   54    0   10    0    0    0    0  750    6    0    0
     0    4    0]
 [   0    0   35    0    0    0   10    0    0    0   18 2145    1    0
     0    1    0]
 [   0    0    0    5    0   15    0    0    0    0    0   13  495    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    0    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0   20    0    0    0    0    0    0    0
    39  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.1680216802168

F1 scores:
[       nan 0.84782609 0.927435   0.87777778 0.97931034 0.92241379
 0.96567164 0.98039216 0.98584906 0.48780488 0.89874176 0.96621622
 0.90909091 1.         0.97885196 0.9        0.95953757]

Kappa:
0.9334933185043108
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe53b02ef60>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.835, val_acc:0.060]
Epoch [2/120    avg_loss:2.778, val_acc:0.303]
Epoch [3/120    avg_loss:2.718, val_acc:0.339]
Epoch [4/120    avg_loss:2.667, val_acc:0.307]
Epoch [5/120    avg_loss:2.583, val_acc:0.346]
Epoch [6/120    avg_loss:2.500, val_acc:0.385]
Epoch [7/120    avg_loss:2.406, val_acc:0.401]
Epoch [8/120    avg_loss:2.339, val_acc:0.445]
Epoch [9/120    avg_loss:2.257, val_acc:0.427]
Epoch [10/120    avg_loss:2.198, val_acc:0.509]
Epoch [11/120    avg_loss:2.104, val_acc:0.567]
Epoch [12/120    avg_loss:1.986, val_acc:0.582]
Epoch [13/120    avg_loss:1.926, val_acc:0.643]
Epoch [14/120    avg_loss:1.803, val_acc:0.649]
Epoch [15/120    avg_loss:1.695, val_acc:0.666]
Epoch [16/120    avg_loss:1.605, val_acc:0.673]
Epoch [17/120    avg_loss:1.391, val_acc:0.702]
Epoch [18/120    avg_loss:1.274, val_acc:0.738]
Epoch [19/120    avg_loss:1.209, val_acc:0.769]
Epoch [20/120    avg_loss:1.111, val_acc:0.742]
Epoch [21/120    avg_loss:1.040, val_acc:0.746]
Epoch [22/120    avg_loss:0.960, val_acc:0.783]
Epoch [23/120    avg_loss:0.863, val_acc:0.797]
Epoch [24/120    avg_loss:0.787, val_acc:0.779]
Epoch [25/120    avg_loss:0.743, val_acc:0.807]
Epoch [26/120    avg_loss:0.696, val_acc:0.825]
Epoch [27/120    avg_loss:0.618, val_acc:0.834]
Epoch [28/120    avg_loss:0.593, val_acc:0.841]
Epoch [29/120    avg_loss:0.557, val_acc:0.876]
Epoch [30/120    avg_loss:0.520, val_acc:0.873]
Epoch [31/120    avg_loss:0.458, val_acc:0.893]
Epoch [32/120    avg_loss:0.364, val_acc:0.890]
Epoch [33/120    avg_loss:0.402, val_acc:0.916]
Epoch [34/120    avg_loss:0.351, val_acc:0.887]
Epoch [35/120    avg_loss:0.311, val_acc:0.907]
Epoch [36/120    avg_loss:0.396, val_acc:0.906]
Epoch [37/120    avg_loss:0.316, val_acc:0.900]
Epoch [38/120    avg_loss:0.257, val_acc:0.916]
Epoch [39/120    avg_loss:0.222, val_acc:0.934]
Epoch [40/120    avg_loss:0.172, val_acc:0.943]
Epoch [41/120    avg_loss:0.175, val_acc:0.935]
Epoch [42/120    avg_loss:0.177, val_acc:0.945]
Epoch [43/120    avg_loss:0.150, val_acc:0.954]
Epoch [44/120    avg_loss:0.142, val_acc:0.950]
Epoch [45/120    avg_loss:0.149, val_acc:0.950]
Epoch [46/120    avg_loss:0.157, val_acc:0.934]
Epoch [47/120    avg_loss:0.150, val_acc:0.939]
Epoch [48/120    avg_loss:0.131, val_acc:0.934]
Epoch [49/120    avg_loss:0.099, val_acc:0.935]
Epoch [50/120    avg_loss:0.087, val_acc:0.929]
Epoch [51/120    avg_loss:0.094, val_acc:0.955]
Epoch [52/120    avg_loss:0.081, val_acc:0.963]
Epoch [53/120    avg_loss:0.065, val_acc:0.961]
Epoch [54/120    avg_loss:0.060, val_acc:0.956]
Epoch [55/120    avg_loss:0.057, val_acc:0.975]
Epoch [56/120    avg_loss:0.049, val_acc:0.953]
Epoch [57/120    avg_loss:0.057, val_acc:0.966]
Epoch [58/120    avg_loss:0.049, val_acc:0.964]
Epoch [59/120    avg_loss:0.062, val_acc:0.953]
Epoch [60/120    avg_loss:0.046, val_acc:0.971]
Epoch [61/120    avg_loss:0.044, val_acc:0.965]
Epoch [62/120    avg_loss:0.045, val_acc:0.966]
Epoch [63/120    avg_loss:0.048, val_acc:0.964]
Epoch [64/120    avg_loss:0.038, val_acc:0.976]
Epoch [65/120    avg_loss:0.029, val_acc:0.976]
Epoch [66/120    avg_loss:0.027, val_acc:0.973]
Epoch [67/120    avg_loss:0.027, val_acc:0.977]
Epoch [68/120    avg_loss:0.029, val_acc:0.974]
Epoch [69/120    avg_loss:0.027, val_acc:0.964]
Epoch [70/120    avg_loss:0.043, val_acc:0.967]
Epoch [71/120    avg_loss:0.035, val_acc:0.961]
Epoch [72/120    avg_loss:0.050, val_acc:0.965]
Epoch [73/120    avg_loss:0.035, val_acc:0.975]
Epoch [74/120    avg_loss:0.028, val_acc:0.976]
Epoch [75/120    avg_loss:0.027, val_acc:0.973]
Epoch [76/120    avg_loss:0.027, val_acc:0.977]
Epoch [77/120    avg_loss:0.027, val_acc:0.974]
Epoch [78/120    avg_loss:0.021, val_acc:0.973]
Epoch [79/120    avg_loss:0.018, val_acc:0.980]
Epoch [80/120    avg_loss:0.019, val_acc:0.978]
Epoch [81/120    avg_loss:0.026, val_acc:0.970]
Epoch [82/120    avg_loss:0.021, val_acc:0.973]
Epoch [83/120    avg_loss:0.030, val_acc:0.954]
Epoch [84/120    avg_loss:0.053, val_acc:0.962]
Epoch [85/120    avg_loss:0.030, val_acc:0.974]
Epoch [86/120    avg_loss:0.018, val_acc:0.974]
Epoch [87/120    avg_loss:0.016, val_acc:0.969]
Epoch [88/120    avg_loss:0.016, val_acc:0.980]
Epoch [89/120    avg_loss:0.025, val_acc:0.965]
Epoch [90/120    avg_loss:0.044, val_acc:0.967]
Epoch [91/120    avg_loss:0.152, val_acc:0.894]
Epoch [92/120    avg_loss:0.378, val_acc:0.907]
Epoch [93/120    avg_loss:0.288, val_acc:0.933]
Epoch [94/120    avg_loss:2.313, val_acc:0.253]
Epoch [95/120    avg_loss:2.437, val_acc:0.238]
Epoch [96/120    avg_loss:2.303, val_acc:0.254]
Epoch [97/120    avg_loss:2.210, val_acc:0.355]
Epoch [98/120    avg_loss:2.122, val_acc:0.394]
Epoch [99/120    avg_loss:2.023, val_acc:0.405]
Epoch [100/120    avg_loss:1.939, val_acc:0.465]
Epoch [101/120    avg_loss:1.905, val_acc:0.482]
Epoch [102/120    avg_loss:1.875, val_acc:0.490]
Epoch [103/120    avg_loss:1.804, val_acc:0.491]
Epoch [104/120    avg_loss:1.848, val_acc:0.492]
Epoch [105/120    avg_loss:1.812, val_acc:0.492]
Epoch [106/120    avg_loss:1.861, val_acc:0.503]
Epoch [107/120    avg_loss:1.822, val_acc:0.496]
Epoch [108/120    avg_loss:1.824, val_acc:0.495]
Epoch [109/120    avg_loss:1.824, val_acc:0.513]
Epoch [110/120    avg_loss:1.812, val_acc:0.510]
Epoch [111/120    avg_loss:1.791, val_acc:0.523]
Epoch [112/120    avg_loss:1.781, val_acc:0.511]
Epoch [113/120    avg_loss:1.773, val_acc:0.519]
Epoch [114/120    avg_loss:1.812, val_acc:0.527]
Epoch [115/120    avg_loss:1.770, val_acc:0.527]
Epoch [116/120    avg_loss:1.773, val_acc:0.525]
Epoch [117/120    avg_loss:1.769, val_acc:0.526]
Epoch [118/120    avg_loss:1.759, val_acc:0.525]
Epoch [119/120    avg_loss:1.699, val_acc:0.525]
Epoch [120/120    avg_loss:1.713, val_acc:0.525]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   24    2    0    0    0    6    0    1    0    7    1    0    0
     0    0    0]
 [   0   61  419   45    0    2   35    0    5    0  473  194   13   37
     1    0    0]
 [   0    0   62   69    2   20   74    0    0    0  257   98    4  155
     6    0    0]
 [   0   20   25   38   93    0   30    0    0    0    0    0    0    7
     0    0    0]
 [   0    0    0    0    0    1  128    0    0    0   13    0    0    3
   290    0    0]
 [   0    0    0    4    0    0  360    0    0    0    0    0    0  161
   132    0    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
     0    0    0]
 [   0    7    1    0    0    0    8    0  413    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    6
     0    0    0]
 [   0    4    6    5    0   26   61    0    0    0  557   88    5   94
     0   29    0]
 [   0   39  140   23    0   19   83    0   75    0  307 1033   29  256
     2    0  204]
 [   0   16   80   40   32   15   20    0   15    0   43   35  129    5
     0    0  104]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0  166
     0    0    0]
 [   0    0    0    0    0    3   73    0    4    0    0    0    0    1
  1054    1    3]
 [   0    0    0    4    0    0  130    0    0    0    1    0    1    4
   195   12    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
     0    0   83]]

Accuracy:
47.83739837398374

F1 scores:
[       nan 0.22641509 0.41485149 0.14153846 0.54705882 0.00383877
 0.41836142 0.         0.875      0.         0.43979471 0.56463515
 0.3603352  0.30740741 0.7477829  0.06169666 0.34728033]

Kappa:
0.41681195448587965
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1ae94f2f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.825, val_acc:0.109]
Epoch [2/120    avg_loss:2.760, val_acc:0.246]
Epoch [3/120    avg_loss:2.702, val_acc:0.397]
Epoch [4/120    avg_loss:2.629, val_acc:0.444]
Epoch [5/120    avg_loss:2.569, val_acc:0.436]
Epoch [6/120    avg_loss:2.501, val_acc:0.477]
Epoch [7/120    avg_loss:2.403, val_acc:0.468]
Epoch [8/120    avg_loss:2.329, val_acc:0.525]
Epoch [9/120    avg_loss:2.242, val_acc:0.547]
Epoch [10/120    avg_loss:2.141, val_acc:0.606]
Epoch [11/120    avg_loss:2.027, val_acc:0.611]
Epoch [12/120    avg_loss:1.951, val_acc:0.635]
Epoch [13/120    avg_loss:1.891, val_acc:0.662]
Epoch [14/120    avg_loss:1.760, val_acc:0.666]
Epoch [15/120    avg_loss:1.624, val_acc:0.660]
Epoch [16/120    avg_loss:1.503, val_acc:0.719]
Epoch [17/120    avg_loss:1.395, val_acc:0.719]
Epoch [18/120    avg_loss:1.365, val_acc:0.720]
Epoch [19/120    avg_loss:1.213, val_acc:0.732]
Epoch [20/120    avg_loss:1.077, val_acc:0.768]
Epoch [21/120    avg_loss:1.031, val_acc:0.796]
Epoch [22/120    avg_loss:0.928, val_acc:0.759]
Epoch [23/120    avg_loss:0.832, val_acc:0.803]
Epoch [24/120    avg_loss:0.770, val_acc:0.811]
Epoch [25/120    avg_loss:0.711, val_acc:0.811]
Epoch [26/120    avg_loss:0.667, val_acc:0.854]
Epoch [27/120    avg_loss:0.557, val_acc:0.861]
Epoch [28/120    avg_loss:0.555, val_acc:0.861]
Epoch [29/120    avg_loss:0.555, val_acc:0.871]
Epoch [30/120    avg_loss:0.480, val_acc:0.876]
Epoch [31/120    avg_loss:0.423, val_acc:0.894]
Epoch [32/120    avg_loss:0.445, val_acc:0.873]
Epoch [33/120    avg_loss:0.399, val_acc:0.893]
Epoch [34/120    avg_loss:0.333, val_acc:0.906]
Epoch [35/120    avg_loss:0.380, val_acc:0.862]
Epoch [36/120    avg_loss:0.352, val_acc:0.865]
Epoch [37/120    avg_loss:0.318, val_acc:0.906]
Epoch [38/120    avg_loss:0.277, val_acc:0.906]
Epoch [39/120    avg_loss:0.240, val_acc:0.925]
Epoch [40/120    avg_loss:0.249, val_acc:0.906]
Epoch [41/120    avg_loss:0.214, val_acc:0.886]
Epoch [42/120    avg_loss:0.190, val_acc:0.914]
Epoch [43/120    avg_loss:0.186, val_acc:0.933]
Epoch [44/120    avg_loss:0.150, val_acc:0.924]
Epoch [45/120    avg_loss:0.194, val_acc:0.929]
Epoch [46/120    avg_loss:0.165, val_acc:0.934]
Epoch [47/120    avg_loss:0.167, val_acc:0.938]
Epoch [48/120    avg_loss:0.123, val_acc:0.942]
Epoch [49/120    avg_loss:0.137, val_acc:0.945]
Epoch [50/120    avg_loss:0.138, val_acc:0.928]
Epoch [51/120    avg_loss:0.138, val_acc:0.936]
Epoch [52/120    avg_loss:0.116, val_acc:0.956]
Epoch [53/120    avg_loss:0.084, val_acc:0.952]
Epoch [54/120    avg_loss:0.070, val_acc:0.950]
Epoch [55/120    avg_loss:0.079, val_acc:0.956]
Epoch [56/120    avg_loss:0.076, val_acc:0.951]
Epoch [57/120    avg_loss:0.077, val_acc:0.943]
Epoch [58/120    avg_loss:0.117, val_acc:0.943]
Epoch [59/120    avg_loss:0.109, val_acc:0.954]
Epoch [60/120    avg_loss:0.094, val_acc:0.952]
Epoch [61/120    avg_loss:0.111, val_acc:0.906]
Epoch [62/120    avg_loss:0.120, val_acc:0.933]
Epoch [63/120    avg_loss:0.103, val_acc:0.948]
Epoch [64/120    avg_loss:0.068, val_acc:0.966]
Epoch [65/120    avg_loss:0.048, val_acc:0.965]
Epoch [66/120    avg_loss:0.054, val_acc:0.962]
Epoch [67/120    avg_loss:0.062, val_acc:0.950]
Epoch [68/120    avg_loss:0.073, val_acc:0.957]
Epoch [69/120    avg_loss:0.049, val_acc:0.963]
Epoch [70/120    avg_loss:0.050, val_acc:0.947]
Epoch [71/120    avg_loss:0.061, val_acc:0.958]
Epoch [72/120    avg_loss:0.046, val_acc:0.966]
Epoch [73/120    avg_loss:0.040, val_acc:0.966]
Epoch [74/120    avg_loss:0.037, val_acc:0.962]
Epoch [75/120    avg_loss:0.032, val_acc:0.967]
Epoch [76/120    avg_loss:0.033, val_acc:0.964]
Epoch [77/120    avg_loss:0.030, val_acc:0.962]
Epoch [78/120    avg_loss:0.035, val_acc:0.968]
Epoch [79/120    avg_loss:0.028, val_acc:0.963]
Epoch [80/120    avg_loss:0.025, val_acc:0.969]
Epoch [81/120    avg_loss:0.025, val_acc:0.970]
Epoch [82/120    avg_loss:0.028, val_acc:0.958]
Epoch [83/120    avg_loss:0.040, val_acc:0.959]
Epoch [84/120    avg_loss:0.032, val_acc:0.963]
Epoch [85/120    avg_loss:0.025, val_acc:0.971]
Epoch [86/120    avg_loss:0.025, val_acc:0.975]
Epoch [87/120    avg_loss:0.029, val_acc:0.961]
Epoch [88/120    avg_loss:0.023, val_acc:0.977]
Epoch [89/120    avg_loss:0.020, val_acc:0.970]
Epoch [90/120    avg_loss:0.029, val_acc:0.967]
Epoch [91/120    avg_loss:0.021, val_acc:0.967]
Epoch [92/120    avg_loss:0.017, val_acc:0.974]
Epoch [93/120    avg_loss:0.033, val_acc:0.958]
Epoch [94/120    avg_loss:0.035, val_acc:0.968]
Epoch [95/120    avg_loss:0.019, val_acc:0.974]
Epoch [96/120    avg_loss:0.024, val_acc:0.977]
Epoch [97/120    avg_loss:0.018, val_acc:0.961]
Epoch [98/120    avg_loss:0.024, val_acc:0.975]
Epoch [99/120    avg_loss:0.021, val_acc:0.970]
Epoch [100/120    avg_loss:0.020, val_acc:0.974]
Epoch [101/120    avg_loss:0.022, val_acc:0.968]
Epoch [102/120    avg_loss:0.021, val_acc:0.961]
Epoch [103/120    avg_loss:0.019, val_acc:0.979]
Epoch [104/120    avg_loss:0.017, val_acc:0.976]
Epoch [105/120    avg_loss:0.014, val_acc:0.977]
Epoch [106/120    avg_loss:0.015, val_acc:0.975]
Epoch [107/120    avg_loss:0.014, val_acc:0.979]
Epoch [108/120    avg_loss:0.014, val_acc:0.976]
Epoch [109/120    avg_loss:0.013, val_acc:0.978]
Epoch [110/120    avg_loss:0.014, val_acc:0.974]
Epoch [111/120    avg_loss:0.014, val_acc:0.971]
Epoch [112/120    avg_loss:0.015, val_acc:0.973]
Epoch [113/120    avg_loss:0.010, val_acc:0.971]
Epoch [114/120    avg_loss:0.014, val_acc:0.977]
Epoch [115/120    avg_loss:0.010, val_acc:0.976]
Epoch [116/120    avg_loss:0.010, val_acc:0.979]
Epoch [117/120    avg_loss:0.011, val_acc:0.977]
Epoch [118/120    avg_loss:0.010, val_acc:0.977]
Epoch [119/120    avg_loss:0.008, val_acc:0.976]
Epoch [120/120    avg_loss:0.009, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1274    1    0    0    2    0    0    0    2    5    1    0
     0    0    0]
 [   0    0    0  716    1   18    1    0    0    8    1    0    1    0
     0    1    0]
 [   0    0    0    4  209    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0   17   29    0    4    0    0    0    0  806   15    0    0
     0    4    0]
 [   0    0   24    0    0    0    4    0    0    0    2 2172    8    0
     0    0    0]
 [   0    0    0    4    0   15    0    0    0    0    3    0  508    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1133    2    0]
 [   0    0    0    0    0    0    2    0    0    3    0    0    0    0
    72  270    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.97560975609755

F1 scores:
[       nan 0.94871795 0.98       0.95149502 0.98817967 0.95343681
 0.99014405 1.         0.99883856 0.63636364 0.9504717  0.98660005
 0.96577947 1.         0.96384517 0.86538462 0.97674419]

Kappa:
0.9655055393377509
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb93951ff28>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.827, val_acc:0.117]
Epoch [2/120    avg_loss:2.771, val_acc:0.179]
Epoch [3/120    avg_loss:2.719, val_acc:0.192]
Epoch [4/120    avg_loss:2.661, val_acc:0.252]
Epoch [5/120    avg_loss:2.613, val_acc:0.295]
Epoch [6/120    avg_loss:2.548, val_acc:0.328]
Epoch [7/120    avg_loss:2.473, val_acc:0.455]
Epoch [8/120    avg_loss:2.393, val_acc:0.509]
Epoch [9/120    avg_loss:2.315, val_acc:0.543]
Epoch [10/120    avg_loss:2.245, val_acc:0.530]
Epoch [11/120    avg_loss:2.196, val_acc:0.578]
Epoch [12/120    avg_loss:2.082, val_acc:0.595]
Epoch [13/120    avg_loss:2.014, val_acc:0.589]
Epoch [14/120    avg_loss:1.920, val_acc:0.610]
Epoch [15/120    avg_loss:1.839, val_acc:0.623]
Epoch [16/120    avg_loss:1.727, val_acc:0.637]
Epoch [17/120    avg_loss:1.639, val_acc:0.663]
Epoch [18/120    avg_loss:1.615, val_acc:0.664]
Epoch [19/120    avg_loss:1.476, val_acc:0.685]
Epoch [20/120    avg_loss:1.399, val_acc:0.686]
Epoch [21/120    avg_loss:1.312, val_acc:0.684]
Epoch [22/120    avg_loss:1.199, val_acc:0.719]
Epoch [23/120    avg_loss:1.124, val_acc:0.722]
Epoch [24/120    avg_loss:1.033, val_acc:0.750]
Epoch [25/120    avg_loss:0.940, val_acc:0.755]
Epoch [26/120    avg_loss:0.911, val_acc:0.765]
Epoch [27/120    avg_loss:0.805, val_acc:0.752]
Epoch [28/120    avg_loss:0.824, val_acc:0.764]
Epoch [29/120    avg_loss:0.780, val_acc:0.756]
Epoch [30/120    avg_loss:0.765, val_acc:0.789]
Epoch [31/120    avg_loss:0.682, val_acc:0.812]
Epoch [32/120    avg_loss:0.560, val_acc:0.804]
Epoch [33/120    avg_loss:0.608, val_acc:0.812]
Epoch [34/120    avg_loss:0.552, val_acc:0.814]
Epoch [35/120    avg_loss:0.510, val_acc:0.806]
Epoch [36/120    avg_loss:0.513, val_acc:0.806]
Epoch [37/120    avg_loss:0.449, val_acc:0.843]
Epoch [38/120    avg_loss:0.498, val_acc:0.831]
Epoch [39/120    avg_loss:0.377, val_acc:0.815]
Epoch [40/120    avg_loss:0.409, val_acc:0.832]
Epoch [41/120    avg_loss:0.339, val_acc:0.863]
Epoch [42/120    avg_loss:0.338, val_acc:0.870]
Epoch [43/120    avg_loss:0.361, val_acc:0.847]
Epoch [44/120    avg_loss:0.316, val_acc:0.872]
Epoch [45/120    avg_loss:0.264, val_acc:0.894]
Epoch [46/120    avg_loss:0.284, val_acc:0.868]
Epoch [47/120    avg_loss:0.209, val_acc:0.866]
Epoch [48/120    avg_loss:0.254, val_acc:0.854]
Epoch [49/120    avg_loss:0.236, val_acc:0.896]
Epoch [50/120    avg_loss:0.205, val_acc:0.875]
Epoch [51/120    avg_loss:0.193, val_acc:0.919]
Epoch [52/120    avg_loss:0.242, val_acc:0.898]
Epoch [53/120    avg_loss:0.230, val_acc:0.891]
Epoch [54/120    avg_loss:0.223, val_acc:0.889]
Epoch [55/120    avg_loss:0.202, val_acc:0.912]
Epoch [56/120    avg_loss:0.154, val_acc:0.925]
Epoch [57/120    avg_loss:0.126, val_acc:0.918]
Epoch [58/120    avg_loss:0.137, val_acc:0.924]
Epoch [59/120    avg_loss:0.124, val_acc:0.928]
Epoch [60/120    avg_loss:0.113, val_acc:0.928]
Epoch [61/120    avg_loss:0.114, val_acc:0.932]
Epoch [62/120    avg_loss:0.101, val_acc:0.935]
Epoch [63/120    avg_loss:0.102, val_acc:0.927]
Epoch [64/120    avg_loss:0.100, val_acc:0.934]
Epoch [65/120    avg_loss:0.084, val_acc:0.938]
Epoch [66/120    avg_loss:0.072, val_acc:0.944]
Epoch [67/120    avg_loss:0.077, val_acc:0.936]
Epoch [68/120    avg_loss:0.089, val_acc:0.941]
Epoch [69/120    avg_loss:0.094, val_acc:0.929]
Epoch [70/120    avg_loss:0.099, val_acc:0.946]
Epoch [71/120    avg_loss:0.116, val_acc:0.934]
Epoch [72/120    avg_loss:0.100, val_acc:0.944]
Epoch [73/120    avg_loss:0.077, val_acc:0.951]
Epoch [74/120    avg_loss:0.061, val_acc:0.951]
Epoch [75/120    avg_loss:0.063, val_acc:0.950]
Epoch [76/120    avg_loss:0.058, val_acc:0.959]
Epoch [77/120    avg_loss:0.052, val_acc:0.958]
Epoch [78/120    avg_loss:0.084, val_acc:0.942]
Epoch [79/120    avg_loss:0.077, val_acc:0.942]
Epoch [80/120    avg_loss:0.058, val_acc:0.954]
Epoch [81/120    avg_loss:0.055, val_acc:0.964]
Epoch [82/120    avg_loss:0.050, val_acc:0.961]
Epoch [83/120    avg_loss:0.043, val_acc:0.949]
Epoch [84/120    avg_loss:0.062, val_acc:0.957]
Epoch [85/120    avg_loss:0.048, val_acc:0.961]
Epoch [86/120    avg_loss:0.046, val_acc:0.950]
Epoch [87/120    avg_loss:0.057, val_acc:0.956]
Epoch [88/120    avg_loss:0.047, val_acc:0.961]
Epoch [89/120    avg_loss:0.046, val_acc:0.957]
Epoch [90/120    avg_loss:0.047, val_acc:0.955]
Epoch [91/120    avg_loss:0.056, val_acc:0.954]
Epoch [92/120    avg_loss:0.081, val_acc:0.949]
Epoch [93/120    avg_loss:0.051, val_acc:0.958]
Epoch [94/120    avg_loss:0.048, val_acc:0.948]
Epoch [95/120    avg_loss:0.046, val_acc:0.953]
Epoch [96/120    avg_loss:0.041, val_acc:0.951]
Epoch [97/120    avg_loss:0.039, val_acc:0.957]
Epoch [98/120    avg_loss:0.033, val_acc:0.956]
Epoch [99/120    avg_loss:0.030, val_acc:0.955]
Epoch [100/120    avg_loss:0.030, val_acc:0.957]
Epoch [101/120    avg_loss:0.033, val_acc:0.956]
Epoch [102/120    avg_loss:0.031, val_acc:0.956]
Epoch [103/120    avg_loss:0.034, val_acc:0.957]
Epoch [104/120    avg_loss:0.030, val_acc:0.959]
Epoch [105/120    avg_loss:0.036, val_acc:0.958]
Epoch [106/120    avg_loss:0.030, val_acc:0.961]
Epoch [107/120    avg_loss:0.035, val_acc:0.959]
Epoch [108/120    avg_loss:0.029, val_acc:0.962]
Epoch [109/120    avg_loss:0.027, val_acc:0.961]
Epoch [110/120    avg_loss:0.032, val_acc:0.962]
Epoch [111/120    avg_loss:0.025, val_acc:0.961]
Epoch [112/120    avg_loss:0.028, val_acc:0.962]
Epoch [113/120    avg_loss:0.030, val_acc:0.959]
Epoch [114/120    avg_loss:0.028, val_acc:0.959]
Epoch [115/120    avg_loss:0.027, val_acc:0.959]
Epoch [116/120    avg_loss:0.027, val_acc:0.962]
Epoch [117/120    avg_loss:0.030, val_acc:0.962]
Epoch [118/120    avg_loss:0.025, val_acc:0.961]
Epoch [119/120    avg_loss:0.031, val_acc:0.961]
Epoch [120/120    avg_loss:0.028, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    2 1218    0    0    0    0    0    0    0    1   44    4    0
     0   16    0]
 [   0    0    6  701    1   22    0    0    0   16    0    0    0    0
     1    0    0]
 [   0    0    0    2  210    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  424    0    0    0    3    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  426    0    0    0    3    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   58   90    0    8    0    0    0    0  710    3    6    0
     0    0    0]
 [   0    0   16    0    0    0    2    0    3    0    4 2167    9    1
     8    0    0]
 [   0    0    0    3    0   15    0    0    0    0    4    0  509    0
     0    0    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    1    0    0
  1136    0    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
   120  218    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.51490514905149

F1 scores:
[       nan 0.9        0.94308943 0.90744337 0.99056604 0.93701657
 0.99017385 1.         0.99185099 0.60377358 0.88694566 0.97899255
 0.95497186 0.99459459 0.94195688 0.75043029 0.98245614]

Kappa:
0.937403718917896
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff266d6fef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.822, val_acc:0.172]
Epoch [2/120    avg_loss:2.774, val_acc:0.192]
Epoch [3/120    avg_loss:2.734, val_acc:0.300]
Epoch [4/120    avg_loss:2.670, val_acc:0.341]
Epoch [5/120    avg_loss:2.603, val_acc:0.368]
Epoch [6/120    avg_loss:2.523, val_acc:0.374]
Epoch [7/120    avg_loss:2.437, val_acc:0.367]
Epoch [8/120    avg_loss:2.369, val_acc:0.352]
Epoch [9/120    avg_loss:2.307, val_acc:0.359]
Epoch [10/120    avg_loss:2.210, val_acc:0.352]
Epoch [11/120    avg_loss:2.146, val_acc:0.384]
Epoch [12/120    avg_loss:2.065, val_acc:0.421]
Epoch [13/120    avg_loss:2.005, val_acc:0.436]
Epoch [14/120    avg_loss:1.880, val_acc:0.490]
Epoch [15/120    avg_loss:1.803, val_acc:0.595]
Epoch [16/120    avg_loss:1.667, val_acc:0.606]
Epoch [17/120    avg_loss:1.611, val_acc:0.662]
Epoch [18/120    avg_loss:1.516, val_acc:0.650]
Epoch [19/120    avg_loss:1.440, val_acc:0.675]
Epoch [20/120    avg_loss:1.284, val_acc:0.739]
Epoch [21/120    avg_loss:1.211, val_acc:0.735]
Epoch [22/120    avg_loss:1.093, val_acc:0.748]
Epoch [23/120    avg_loss:1.031, val_acc:0.754]
Epoch [24/120    avg_loss:0.966, val_acc:0.754]
Epoch [25/120    avg_loss:0.896, val_acc:0.783]
Epoch [26/120    avg_loss:0.818, val_acc:0.787]
Epoch [27/120    avg_loss:0.794, val_acc:0.791]
Epoch [28/120    avg_loss:0.745, val_acc:0.786]
Epoch [29/120    avg_loss:0.666, val_acc:0.815]
Epoch [30/120    avg_loss:0.649, val_acc:0.820]
Epoch [31/120    avg_loss:0.594, val_acc:0.833]
Epoch [32/120    avg_loss:0.553, val_acc:0.842]
Epoch [33/120    avg_loss:0.464, val_acc:0.867]
Epoch [34/120    avg_loss:0.394, val_acc:0.893]
Epoch [35/120    avg_loss:0.387, val_acc:0.885]
Epoch [36/120    avg_loss:0.367, val_acc:0.860]
Epoch [37/120    avg_loss:0.358, val_acc:0.890]
Epoch [38/120    avg_loss:0.332, val_acc:0.895]
Epoch [39/120    avg_loss:0.286, val_acc:0.918]
Epoch [40/120    avg_loss:0.246, val_acc:0.917]
Epoch [41/120    avg_loss:0.230, val_acc:0.931]
Epoch [42/120    avg_loss:0.238, val_acc:0.914]
Epoch [43/120    avg_loss:0.228, val_acc:0.899]
Epoch [44/120    avg_loss:0.229, val_acc:0.897]
Epoch [45/120    avg_loss:0.230, val_acc:0.888]
Epoch [46/120    avg_loss:0.212, val_acc:0.924]
Epoch [47/120    avg_loss:0.176, val_acc:0.928]
Epoch [48/120    avg_loss:0.159, val_acc:0.944]
Epoch [49/120    avg_loss:0.141, val_acc:0.935]
Epoch [50/120    avg_loss:0.138, val_acc:0.938]
Epoch [51/120    avg_loss:0.114, val_acc:0.942]
Epoch [52/120    avg_loss:0.106, val_acc:0.919]
Epoch [53/120    avg_loss:0.117, val_acc:0.940]
Epoch [54/120    avg_loss:0.138, val_acc:0.942]
Epoch [55/120    avg_loss:0.131, val_acc:0.952]
Epoch [56/120    avg_loss:0.090, val_acc:0.952]
Epoch [57/120    avg_loss:0.125, val_acc:0.952]
Epoch [58/120    avg_loss:0.125, val_acc:0.930]
Epoch [59/120    avg_loss:0.096, val_acc:0.943]
Epoch [60/120    avg_loss:0.114, val_acc:0.963]
Epoch [61/120    avg_loss:0.078, val_acc:0.954]
Epoch [62/120    avg_loss:0.069, val_acc:0.963]
Epoch [63/120    avg_loss:0.065, val_acc:0.967]
Epoch [64/120    avg_loss:0.057, val_acc:0.970]
Epoch [65/120    avg_loss:0.053, val_acc:0.967]
Epoch [66/120    avg_loss:0.050, val_acc:0.968]
Epoch [67/120    avg_loss:0.048, val_acc:0.964]
Epoch [68/120    avg_loss:0.043, val_acc:0.974]
Epoch [69/120    avg_loss:0.057, val_acc:0.968]
Epoch [70/120    avg_loss:0.071, val_acc:0.962]
Epoch [71/120    avg_loss:0.045, val_acc:0.964]
Epoch [72/120    avg_loss:0.045, val_acc:0.964]
Epoch [73/120    avg_loss:0.060, val_acc:0.945]
Epoch [74/120    avg_loss:0.065, val_acc:0.957]
Epoch [75/120    avg_loss:0.057, val_acc:0.955]
Epoch [76/120    avg_loss:0.048, val_acc:0.969]
Epoch [77/120    avg_loss:0.039, val_acc:0.975]
Epoch [78/120    avg_loss:0.037, val_acc:0.959]
Epoch [79/120    avg_loss:0.033, val_acc:0.966]
Epoch [80/120    avg_loss:0.035, val_acc:0.973]
Epoch [81/120    avg_loss:0.026, val_acc:0.977]
Epoch [82/120    avg_loss:0.033, val_acc:0.974]
Epoch [83/120    avg_loss:0.069, val_acc:0.966]
Epoch [84/120    avg_loss:0.061, val_acc:0.959]
Epoch [85/120    avg_loss:0.038, val_acc:0.965]
Epoch [86/120    avg_loss:0.036, val_acc:0.980]
Epoch [87/120    avg_loss:0.029, val_acc:0.976]
Epoch [88/120    avg_loss:0.026, val_acc:0.978]
Epoch [89/120    avg_loss:0.024, val_acc:0.982]
Epoch [90/120    avg_loss:0.024, val_acc:0.979]
Epoch [91/120    avg_loss:0.025, val_acc:0.977]
Epoch [92/120    avg_loss:0.024, val_acc:0.978]
Epoch [93/120    avg_loss:0.022, val_acc:0.979]
Epoch [94/120    avg_loss:0.022, val_acc:0.954]
Epoch [95/120    avg_loss:0.035, val_acc:0.977]
Epoch [96/120    avg_loss:0.026, val_acc:0.976]
Epoch [97/120    avg_loss:0.029, val_acc:0.976]
Epoch [98/120    avg_loss:0.020, val_acc:0.976]
Epoch [99/120    avg_loss:0.016, val_acc:0.981]
Epoch [100/120    avg_loss:0.017, val_acc:0.985]
Epoch [101/120    avg_loss:0.017, val_acc:0.987]
Epoch [102/120    avg_loss:0.029, val_acc:0.979]
Epoch [103/120    avg_loss:0.028, val_acc:0.980]
Epoch [104/120    avg_loss:0.020, val_acc:0.971]
Epoch [105/120    avg_loss:0.016, val_acc:0.986]
Epoch [106/120    avg_loss:0.019, val_acc:0.984]
Epoch [107/120    avg_loss:0.018, val_acc:0.989]
Epoch [108/120    avg_loss:0.038, val_acc:0.978]
Epoch [109/120    avg_loss:0.037, val_acc:0.973]
Epoch [110/120    avg_loss:0.019, val_acc:0.977]
Epoch [111/120    avg_loss:0.019, val_acc:0.976]
Epoch [112/120    avg_loss:0.017, val_acc:0.989]
Epoch [113/120    avg_loss:0.013, val_acc:0.987]
Epoch [114/120    avg_loss:0.011, val_acc:0.986]
Epoch [115/120    avg_loss:0.012, val_acc:0.986]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.011, val_acc:0.990]
Epoch [118/120    avg_loss:0.012, val_acc:0.984]
Epoch [119/120    avg_loss:0.011, val_acc:0.982]
Epoch [120/120    avg_loss:0.024, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   31    0    0    0    0    0    0    1    0    9    0    0    0
     0    0    0]
 [   0    0 1148    1   19    0    1    0    0    3   37   67    9    0
     0    0    0]
 [   0    0    0  734    1    3    0    0    0    7    0    0    1    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  420    0    0    0    2    0    0    0    0
    13    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    0    1    0    0   15    0    0    1    0
     0    0    0]
 [   0    0    2   90    0    5    0    0    0    0  708   21   43    0
     0    6    0]
 [   0    0    2    0    0    0    1    0    0    0    0 2197    6    1
     3    0    0]
 [   0    0    0    0    1    0    0    0    0    0    0    5  527    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   39    0    0    3    0    0    0    0
    45  260    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    8    0
     0    0   76]]

Accuracy:
94.99186991869918

F1 scores:
[       nan 0.86111111 0.94214198 0.93324857 0.95302013 0.97334878
 0.96826568 1.         0.99767442 0.625      0.86924494 0.9762275
 0.93274336 0.99730458 0.97350427 0.84828711 0.94409938]

Kappa:
0.9428532896134864
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb3b2da6eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.825, val_acc:0.120]
Epoch [2/120    avg_loss:2.769, val_acc:0.253]
Epoch [3/120    avg_loss:2.715, val_acc:0.343]
Epoch [4/120    avg_loss:2.654, val_acc:0.381]
Epoch [5/120    avg_loss:2.565, val_acc:0.387]
Epoch [6/120    avg_loss:2.491, val_acc:0.449]
Epoch [7/120    avg_loss:2.427, val_acc:0.509]
Epoch [8/120    avg_loss:2.327, val_acc:0.524]
Epoch [9/120    avg_loss:2.252, val_acc:0.573]
Epoch [10/120    avg_loss:2.195, val_acc:0.547]
Epoch [11/120    avg_loss:2.140, val_acc:0.584]
Epoch [12/120    avg_loss:2.037, val_acc:0.577]
Epoch [13/120    avg_loss:1.964, val_acc:0.623]
Epoch [14/120    avg_loss:1.880, val_acc:0.643]
Epoch [15/120    avg_loss:1.813, val_acc:0.633]
Epoch [16/120    avg_loss:1.664, val_acc:0.653]
Epoch [17/120    avg_loss:1.618, val_acc:0.667]
Epoch [18/120    avg_loss:1.571, val_acc:0.677]
Epoch [19/120    avg_loss:1.459, val_acc:0.703]
Epoch [20/120    avg_loss:1.315, val_acc:0.685]
Epoch [21/120    avg_loss:1.254, val_acc:0.684]
Epoch [22/120    avg_loss:1.100, val_acc:0.749]
Epoch [23/120    avg_loss:0.999, val_acc:0.734]
Epoch [24/120    avg_loss:0.955, val_acc:0.742]
Epoch [25/120    avg_loss:0.984, val_acc:0.755]
Epoch [26/120    avg_loss:0.828, val_acc:0.785]
Epoch [27/120    avg_loss:0.734, val_acc:0.805]
Epoch [28/120    avg_loss:0.677, val_acc:0.830]
Epoch [29/120    avg_loss:0.615, val_acc:0.833]
Epoch [30/120    avg_loss:0.628, val_acc:0.837]
Epoch [31/120    avg_loss:0.522, val_acc:0.874]
Epoch [32/120    avg_loss:0.469, val_acc:0.859]
Epoch [33/120    avg_loss:0.466, val_acc:0.868]
Epoch [34/120    avg_loss:0.462, val_acc:0.880]
Epoch [35/120    avg_loss:0.383, val_acc:0.909]
Epoch [36/120    avg_loss:0.405, val_acc:0.882]
Epoch [37/120    avg_loss:0.381, val_acc:0.901]
Epoch [38/120    avg_loss:0.342, val_acc:0.919]
Epoch [39/120    avg_loss:0.304, val_acc:0.896]
Epoch [40/120    avg_loss:0.296, val_acc:0.919]
Epoch [41/120    avg_loss:0.262, val_acc:0.918]
Epoch [42/120    avg_loss:0.242, val_acc:0.914]
Epoch [43/120    avg_loss:0.271, val_acc:0.917]
Epoch [44/120    avg_loss:0.356, val_acc:0.914]
Epoch [45/120    avg_loss:0.246, val_acc:0.922]
Epoch [46/120    avg_loss:0.212, val_acc:0.938]
Epoch [47/120    avg_loss:0.189, val_acc:0.939]
Epoch [48/120    avg_loss:0.214, val_acc:0.922]
Epoch [49/120    avg_loss:0.199, val_acc:0.945]
Epoch [50/120    avg_loss:0.180, val_acc:0.909]
Epoch [51/120    avg_loss:0.227, val_acc:0.940]
Epoch [52/120    avg_loss:0.163, val_acc:0.955]
Epoch [53/120    avg_loss:0.170, val_acc:0.957]
Epoch [54/120    avg_loss:0.139, val_acc:0.945]
Epoch [55/120    avg_loss:0.159, val_acc:0.936]
Epoch [56/120    avg_loss:0.149, val_acc:0.963]
Epoch [57/120    avg_loss:0.110, val_acc:0.966]
Epoch [58/120    avg_loss:0.107, val_acc:0.950]
Epoch [59/120    avg_loss:0.116, val_acc:0.948]
Epoch [60/120    avg_loss:0.099, val_acc:0.963]
Epoch [61/120    avg_loss:0.093, val_acc:0.972]
Epoch [62/120    avg_loss:0.095, val_acc:0.965]
Epoch [63/120    avg_loss:0.098, val_acc:0.966]
Epoch [64/120    avg_loss:0.089, val_acc:0.970]
Epoch [65/120    avg_loss:0.073, val_acc:0.971]
Epoch [66/120    avg_loss:0.060, val_acc:0.980]
Epoch [67/120    avg_loss:0.115, val_acc:0.939]
Epoch [68/120    avg_loss:0.095, val_acc:0.967]
Epoch [69/120    avg_loss:0.080, val_acc:0.980]
Epoch [70/120    avg_loss:0.067, val_acc:0.966]
Epoch [71/120    avg_loss:0.063, val_acc:0.972]
Epoch [72/120    avg_loss:0.069, val_acc:0.976]
Epoch [73/120    avg_loss:0.050, val_acc:0.981]
Epoch [74/120    avg_loss:0.048, val_acc:0.982]
Epoch [75/120    avg_loss:0.052, val_acc:0.966]
Epoch [76/120    avg_loss:0.052, val_acc:0.983]
Epoch [77/120    avg_loss:0.038, val_acc:0.989]
Epoch [78/120    avg_loss:0.042, val_acc:0.989]
Epoch [79/120    avg_loss:0.038, val_acc:0.986]
Epoch [80/120    avg_loss:0.035, val_acc:0.984]
Epoch [81/120    avg_loss:0.057, val_acc:0.967]
Epoch [82/120    avg_loss:0.069, val_acc:0.954]
Epoch [83/120    avg_loss:0.056, val_acc:0.977]
Epoch [84/120    avg_loss:0.037, val_acc:0.981]
Epoch [85/120    avg_loss:0.032, val_acc:0.983]
Epoch [86/120    avg_loss:0.042, val_acc:0.984]
Epoch [87/120    avg_loss:0.033, val_acc:0.983]
Epoch [88/120    avg_loss:0.035, val_acc:0.983]
Epoch [89/120    avg_loss:0.027, val_acc:0.990]
Epoch [90/120    avg_loss:0.024, val_acc:0.986]
Epoch [91/120    avg_loss:0.022, val_acc:0.989]
Epoch [92/120    avg_loss:0.020, val_acc:0.989]
Epoch [93/120    avg_loss:0.021, val_acc:0.990]
Epoch [94/120    avg_loss:0.028, val_acc:0.983]
Epoch [95/120    avg_loss:0.044, val_acc:0.981]
Epoch [96/120    avg_loss:0.038, val_acc:0.985]
Epoch [97/120    avg_loss:0.044, val_acc:0.976]
Epoch [98/120    avg_loss:0.047, val_acc:0.982]
Epoch [99/120    avg_loss:0.042, val_acc:0.979]
Epoch [100/120    avg_loss:0.031, val_acc:0.982]
Epoch [101/120    avg_loss:0.038, val_acc:0.980]
Epoch [102/120    avg_loss:0.026, val_acc:0.983]
Epoch [103/120    avg_loss:0.021, val_acc:0.985]
Epoch [104/120    avg_loss:0.019, val_acc:0.986]
Epoch [105/120    avg_loss:0.023, val_acc:0.984]
Epoch [106/120    avg_loss:0.021, val_acc:0.993]
Epoch [107/120    avg_loss:0.018, val_acc:0.986]
Epoch [108/120    avg_loss:0.022, val_acc:0.985]
Epoch [109/120    avg_loss:0.015, val_acc:0.990]
Epoch [110/120    avg_loss:0.015, val_acc:0.992]
Epoch [111/120    avg_loss:0.015, val_acc:0.992]
Epoch [112/120    avg_loss:0.024, val_acc:0.986]
Epoch [113/120    avg_loss:0.019, val_acc:0.989]
Epoch [114/120    avg_loss:0.016, val_acc:0.990]
Epoch [115/120    avg_loss:0.018, val_acc:0.984]
Epoch [116/120    avg_loss:0.018, val_acc:0.988]
Epoch [117/120    avg_loss:0.014, val_acc:0.988]
Epoch [118/120    avg_loss:0.011, val_acc:0.990]
Epoch [119/120    avg_loss:0.014, val_acc:0.985]
Epoch [120/120    avg_loss:0.014, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1265    0    0    0    2    0    0    0    3    7    0    0
     0    8    0]
 [   0    0    2  715    0   23    0    0    0    7    0    0    0    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    2    0    0    2    0    0   12    0    0    2    0
     0    0    0]
 [   0    0   13   89    0    7    0    0    0    0  760    2    1    0
     0    3    0]
 [   0    0   19    0    0    0    0    0    1    0   15 2171    4    0
     0    0    0]
 [   0    0    0    7    3   16    0    0    0    0    0    8  499    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   12    0    0    1    0    0    2    0
    39  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.5420054200542

F1 scores:
[       nan 0.98765432 0.9787234  0.91549296 0.9882904  0.94517544
 0.98491704 1.         0.99300699 0.57142857 0.91898428 0.98636983
 0.95319962 1.         0.98228942 0.90015361 0.98809524]

Kappa:
0.9605821260926454
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6dc7114f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.820, val_acc:0.066]
Epoch [2/120    avg_loss:2.745, val_acc:0.221]
Epoch [3/120    avg_loss:2.677, val_acc:0.283]
Epoch [4/120    avg_loss:2.610, val_acc:0.361]
Epoch [5/120    avg_loss:2.543, val_acc:0.389]
Epoch [6/120    avg_loss:2.474, val_acc:0.468]
Epoch [7/120    avg_loss:2.403, val_acc:0.490]
Epoch [8/120    avg_loss:2.297, val_acc:0.507]
Epoch [9/120    avg_loss:2.235, val_acc:0.515]
Epoch [10/120    avg_loss:2.159, val_acc:0.543]
Epoch [11/120    avg_loss:2.110, val_acc:0.577]
Epoch [12/120    avg_loss:1.986, val_acc:0.609]
Epoch [13/120    avg_loss:1.910, val_acc:0.611]
Epoch [14/120    avg_loss:1.794, val_acc:0.643]
Epoch [15/120    avg_loss:1.713, val_acc:0.670]
Epoch [16/120    avg_loss:1.559, val_acc:0.662]
Epoch [17/120    avg_loss:1.514, val_acc:0.659]
Epoch [18/120    avg_loss:1.383, val_acc:0.659]
Epoch [19/120    avg_loss:1.338, val_acc:0.695]
Epoch [20/120    avg_loss:1.217, val_acc:0.706]
Epoch [21/120    avg_loss:1.091, val_acc:0.722]
Epoch [22/120    avg_loss:1.058, val_acc:0.735]
Epoch [23/120    avg_loss:0.955, val_acc:0.749]
Epoch [24/120    avg_loss:0.927, val_acc:0.774]
Epoch [25/120    avg_loss:0.888, val_acc:0.741]
Epoch [26/120    avg_loss:0.857, val_acc:0.741]
Epoch [27/120    avg_loss:0.860, val_acc:0.775]
Epoch [28/120    avg_loss:0.734, val_acc:0.761]
Epoch [29/120    avg_loss:0.675, val_acc:0.766]
Epoch [30/120    avg_loss:0.565, val_acc:0.810]
Epoch [31/120    avg_loss:0.522, val_acc:0.848]
Epoch [32/120    avg_loss:0.510, val_acc:0.836]
Epoch [33/120    avg_loss:0.478, val_acc:0.818]
Epoch [34/120    avg_loss:0.409, val_acc:0.856]
Epoch [35/120    avg_loss:0.388, val_acc:0.875]
Epoch [36/120    avg_loss:0.344, val_acc:0.869]
Epoch [37/120    avg_loss:0.344, val_acc:0.865]
Epoch [38/120    avg_loss:0.362, val_acc:0.848]
Epoch [39/120    avg_loss:0.350, val_acc:0.869]
Epoch [40/120    avg_loss:0.288, val_acc:0.878]
Epoch [41/120    avg_loss:0.259, val_acc:0.905]
Epoch [42/120    avg_loss:0.233, val_acc:0.880]
Epoch [43/120    avg_loss:0.213, val_acc:0.916]
Epoch [44/120    avg_loss:0.220, val_acc:0.896]
Epoch [45/120    avg_loss:0.224, val_acc:0.890]
Epoch [46/120    avg_loss:0.183, val_acc:0.909]
Epoch [47/120    avg_loss:0.171, val_acc:0.921]
Epoch [48/120    avg_loss:0.176, val_acc:0.892]
Epoch [49/120    avg_loss:0.174, val_acc:0.911]
Epoch [50/120    avg_loss:0.173, val_acc:0.914]
Epoch [51/120    avg_loss:0.163, val_acc:0.911]
Epoch [52/120    avg_loss:0.145, val_acc:0.919]
Epoch [53/120    avg_loss:0.133, val_acc:0.931]
Epoch [54/120    avg_loss:0.131, val_acc:0.923]
Epoch [55/120    avg_loss:0.112, val_acc:0.928]
Epoch [56/120    avg_loss:0.108, val_acc:0.910]
Epoch [57/120    avg_loss:0.133, val_acc:0.923]
Epoch [58/120    avg_loss:0.161, val_acc:0.931]
Epoch [59/120    avg_loss:0.090, val_acc:0.938]
Epoch [60/120    avg_loss:0.073, val_acc:0.945]
Epoch [61/120    avg_loss:0.081, val_acc:0.948]
Epoch [62/120    avg_loss:0.091, val_acc:0.949]
Epoch [63/120    avg_loss:0.077, val_acc:0.954]
Epoch [64/120    avg_loss:0.096, val_acc:0.936]
Epoch [65/120    avg_loss:0.079, val_acc:0.941]
Epoch [66/120    avg_loss:0.063, val_acc:0.952]
Epoch [67/120    avg_loss:0.069, val_acc:0.950]
Epoch [68/120    avg_loss:0.055, val_acc:0.947]
Epoch [69/120    avg_loss:0.057, val_acc:0.945]
Epoch [70/120    avg_loss:0.081, val_acc:0.946]
Epoch [71/120    avg_loss:0.058, val_acc:0.954]
Epoch [72/120    avg_loss:0.050, val_acc:0.954]
Epoch [73/120    avg_loss:0.042, val_acc:0.953]
Epoch [74/120    avg_loss:0.055, val_acc:0.936]
Epoch [75/120    avg_loss:0.055, val_acc:0.956]
Epoch [76/120    avg_loss:0.045, val_acc:0.955]
Epoch [77/120    avg_loss:0.043, val_acc:0.967]
Epoch [78/120    avg_loss:0.038, val_acc:0.953]
Epoch [79/120    avg_loss:0.039, val_acc:0.962]
Epoch [80/120    avg_loss:0.046, val_acc:0.959]
Epoch [81/120    avg_loss:0.054, val_acc:0.956]
Epoch [82/120    avg_loss:0.052, val_acc:0.956]
Epoch [83/120    avg_loss:0.041, val_acc:0.922]
Epoch [84/120    avg_loss:0.049, val_acc:0.952]
Epoch [85/120    avg_loss:0.034, val_acc:0.954]
Epoch [86/120    avg_loss:0.027, val_acc:0.955]
Epoch [87/120    avg_loss:0.034, val_acc:0.964]
Epoch [88/120    avg_loss:0.034, val_acc:0.964]
Epoch [89/120    avg_loss:0.031, val_acc:0.962]
Epoch [90/120    avg_loss:0.033, val_acc:0.957]
Epoch [91/120    avg_loss:0.027, val_acc:0.956]
Epoch [92/120    avg_loss:0.028, val_acc:0.963]
Epoch [93/120    avg_loss:0.022, val_acc:0.963]
Epoch [94/120    avg_loss:0.025, val_acc:0.964]
Epoch [95/120    avg_loss:0.022, val_acc:0.964]
Epoch [96/120    avg_loss:0.022, val_acc:0.964]
Epoch [97/120    avg_loss:0.020, val_acc:0.965]
Epoch [98/120    avg_loss:0.022, val_acc:0.964]
Epoch [99/120    avg_loss:0.017, val_acc:0.966]
Epoch [100/120    avg_loss:0.018, val_acc:0.965]
Epoch [101/120    avg_loss:0.022, val_acc:0.967]
Epoch [102/120    avg_loss:0.022, val_acc:0.968]
Epoch [103/120    avg_loss:0.023, val_acc:0.968]
Epoch [104/120    avg_loss:0.020, val_acc:0.968]
Epoch [105/120    avg_loss:0.021, val_acc:0.968]
Epoch [106/120    avg_loss:0.021, val_acc:0.967]
Epoch [107/120    avg_loss:0.018, val_acc:0.967]
Epoch [108/120    avg_loss:0.020, val_acc:0.967]
Epoch [109/120    avg_loss:0.019, val_acc:0.966]
Epoch [110/120    avg_loss:0.017, val_acc:0.966]
Epoch [111/120    avg_loss:0.022, val_acc:0.967]
Epoch [112/120    avg_loss:0.019, val_acc:0.967]
Epoch [113/120    avg_loss:0.021, val_acc:0.965]
Epoch [114/120    avg_loss:0.022, val_acc:0.963]
Epoch [115/120    avg_loss:0.018, val_acc:0.965]
Epoch [116/120    avg_loss:0.022, val_acc:0.965]
Epoch [117/120    avg_loss:0.025, val_acc:0.967]
Epoch [118/120    avg_loss:0.018, val_acc:0.967]
Epoch [119/120    avg_loss:0.021, val_acc:0.968]
Epoch [120/120    avg_loss:0.019, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1244    0    0    0    7    0    0    0    4   17    1    0
     0   12    0]
 [   0    0    8  707    0   20    0    0    0    9    0    0    2    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  414    0    0    0    4    0    0    0    0
    17    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   12    0    0    3    0
     0    0    0]
 [   0    0   27   89    0    4    0    0    0    0  742    7    3    0
     0    3    0]
 [   0    0   13    0    0    2    6    0    0    0    6 2168    3    3
     9    0    0]
 [   0    0    0   25    5    5    0    0    0    0    2    2  486    0
     0    4    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    1    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   45    0    0    4    0    0    2    0
    99  197    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.68834688346884

F1 scores:
[       nan 0.96202532 0.96546372 0.90006365 0.98839907 0.93984109
 0.95696572 1.         0.99883856 0.5106383  0.90875689 0.98433598
 0.93822394 0.98930481 0.94666667 0.69982238 0.95906433]

Kappa:
0.9393999543755156
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd0e60d2f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.799, val_acc:0.297]
Epoch [2/120    avg_loss:2.739, val_acc:0.376]
Epoch [3/120    avg_loss:2.673, val_acc:0.419]
Epoch [4/120    avg_loss:2.608, val_acc:0.465]
Epoch [5/120    avg_loss:2.535, val_acc:0.498]
Epoch [6/120    avg_loss:2.456, val_acc:0.517]
Epoch [7/120    avg_loss:2.389, val_acc:0.529]
Epoch [8/120    avg_loss:2.312, val_acc:0.566]
Epoch [9/120    avg_loss:2.209, val_acc:0.595]
Epoch [10/120    avg_loss:2.148, val_acc:0.606]
Epoch [11/120    avg_loss:2.099, val_acc:0.650]
Epoch [12/120    avg_loss:2.035, val_acc:0.666]
Epoch [13/120    avg_loss:1.939, val_acc:0.674]
Epoch [14/120    avg_loss:1.858, val_acc:0.670]
Epoch [15/120    avg_loss:1.719, val_acc:0.668]
Epoch [16/120    avg_loss:1.621, val_acc:0.670]
Epoch [17/120    avg_loss:1.506, val_acc:0.707]
Epoch [18/120    avg_loss:1.396, val_acc:0.716]
Epoch [19/120    avg_loss:1.255, val_acc:0.738]
Epoch [20/120    avg_loss:1.156, val_acc:0.727]
Epoch [21/120    avg_loss:1.065, val_acc:0.778]
Epoch [22/120    avg_loss:0.945, val_acc:0.779]
Epoch [23/120    avg_loss:0.932, val_acc:0.781]
Epoch [24/120    avg_loss:0.853, val_acc:0.778]
Epoch [25/120    avg_loss:0.805, val_acc:0.833]
Epoch [26/120    avg_loss:0.746, val_acc:0.811]
Epoch [27/120    avg_loss:0.715, val_acc:0.829]
Epoch [28/120    avg_loss:0.642, val_acc:0.851]
Epoch [29/120    avg_loss:0.605, val_acc:0.846]
Epoch [30/120    avg_loss:0.544, val_acc:0.877]
Epoch [31/120    avg_loss:0.457, val_acc:0.876]
Epoch [32/120    avg_loss:0.450, val_acc:0.865]
Epoch [33/120    avg_loss:0.407, val_acc:0.887]
Epoch [34/120    avg_loss:0.384, val_acc:0.878]
Epoch [35/120    avg_loss:0.347, val_acc:0.892]
Epoch [36/120    avg_loss:0.408, val_acc:0.873]
Epoch [37/120    avg_loss:0.331, val_acc:0.873]
Epoch [38/120    avg_loss:0.294, val_acc:0.892]
Epoch [39/120    avg_loss:0.264, val_acc:0.917]
Epoch [40/120    avg_loss:0.254, val_acc:0.912]
Epoch [41/120    avg_loss:0.254, val_acc:0.918]
Epoch [42/120    avg_loss:0.186, val_acc:0.932]
Epoch [43/120    avg_loss:0.198, val_acc:0.938]
Epoch [44/120    avg_loss:0.167, val_acc:0.928]
Epoch [45/120    avg_loss:0.149, val_acc:0.922]
Epoch [46/120    avg_loss:0.182, val_acc:0.928]
Epoch [47/120    avg_loss:0.238, val_acc:0.914]
Epoch [48/120    avg_loss:0.218, val_acc:0.924]
Epoch [49/120    avg_loss:0.152, val_acc:0.939]
Epoch [50/120    avg_loss:0.148, val_acc:0.936]
Epoch [51/120    avg_loss:0.139, val_acc:0.934]
Epoch [52/120    avg_loss:0.124, val_acc:0.950]
Epoch [53/120    avg_loss:0.094, val_acc:0.948]
Epoch [54/120    avg_loss:0.090, val_acc:0.949]
Epoch [55/120    avg_loss:0.121, val_acc:0.922]
Epoch [56/120    avg_loss:0.128, val_acc:0.941]
Epoch [57/120    avg_loss:0.084, val_acc:0.955]
Epoch [58/120    avg_loss:0.076, val_acc:0.953]
Epoch [59/120    avg_loss:0.072, val_acc:0.955]
Epoch [60/120    avg_loss:0.071, val_acc:0.955]
Epoch [61/120    avg_loss:0.064, val_acc:0.953]
Epoch [62/120    avg_loss:0.086, val_acc:0.953]
Epoch [63/120    avg_loss:0.074, val_acc:0.962]
Epoch [64/120    avg_loss:0.064, val_acc:0.963]
Epoch [65/120    avg_loss:0.052, val_acc:0.970]
Epoch [66/120    avg_loss:0.060, val_acc:0.949]
Epoch [67/120    avg_loss:0.087, val_acc:0.943]
Epoch [68/120    avg_loss:0.069, val_acc:0.955]
Epoch [69/120    avg_loss:0.055, val_acc:0.955]
Epoch [70/120    avg_loss:0.039, val_acc:0.961]
Epoch [71/120    avg_loss:0.048, val_acc:0.961]
Epoch [72/120    avg_loss:0.041, val_acc:0.956]
Epoch [73/120    avg_loss:0.047, val_acc:0.966]
Epoch [74/120    avg_loss:0.047, val_acc:0.961]
Epoch [75/120    avg_loss:0.044, val_acc:0.955]
Epoch [76/120    avg_loss:0.049, val_acc:0.963]
Epoch [77/120    avg_loss:0.046, val_acc:0.966]
Epoch [78/120    avg_loss:0.041, val_acc:0.969]
Epoch [79/120    avg_loss:0.031, val_acc:0.970]
Epoch [80/120    avg_loss:0.028, val_acc:0.968]
Epoch [81/120    avg_loss:0.023, val_acc:0.968]
Epoch [82/120    avg_loss:0.025, val_acc:0.966]
Epoch [83/120    avg_loss:0.023, val_acc:0.968]
Epoch [84/120    avg_loss:0.031, val_acc:0.968]
Epoch [85/120    avg_loss:0.026, val_acc:0.969]
Epoch [86/120    avg_loss:0.022, val_acc:0.966]
Epoch [87/120    avg_loss:0.026, val_acc:0.968]
Epoch [88/120    avg_loss:0.026, val_acc:0.970]
Epoch [89/120    avg_loss:0.026, val_acc:0.968]
Epoch [90/120    avg_loss:0.023, val_acc:0.970]
Epoch [91/120    avg_loss:0.023, val_acc:0.968]
Epoch [92/120    avg_loss:0.024, val_acc:0.969]
Epoch [93/120    avg_loss:0.023, val_acc:0.971]
Epoch [94/120    avg_loss:0.019, val_acc:0.971]
Epoch [95/120    avg_loss:0.020, val_acc:0.969]
Epoch [96/120    avg_loss:0.023, val_acc:0.969]
Epoch [97/120    avg_loss:0.020, val_acc:0.970]
Epoch [98/120    avg_loss:0.028, val_acc:0.971]
Epoch [99/120    avg_loss:0.021, val_acc:0.970]
Epoch [100/120    avg_loss:0.022, val_acc:0.971]
Epoch [101/120    avg_loss:0.022, val_acc:0.972]
Epoch [102/120    avg_loss:0.023, val_acc:0.972]
Epoch [103/120    avg_loss:0.018, val_acc:0.972]
Epoch [104/120    avg_loss:0.019, val_acc:0.972]
Epoch [105/120    avg_loss:0.021, val_acc:0.970]
Epoch [106/120    avg_loss:0.024, val_acc:0.975]
Epoch [107/120    avg_loss:0.018, val_acc:0.971]
Epoch [108/120    avg_loss:0.020, val_acc:0.970]
Epoch [109/120    avg_loss:0.021, val_acc:0.970]
Epoch [110/120    avg_loss:0.020, val_acc:0.971]
Epoch [111/120    avg_loss:0.022, val_acc:0.971]
Epoch [112/120    avg_loss:0.025, val_acc:0.975]
Epoch [113/120    avg_loss:0.019, val_acc:0.973]
Epoch [114/120    avg_loss:0.021, val_acc:0.972]
Epoch [115/120    avg_loss:0.023, val_acc:0.975]
Epoch [116/120    avg_loss:0.019, val_acc:0.972]
Epoch [117/120    avg_loss:0.020, val_acc:0.976]
Epoch [118/120    avg_loss:0.018, val_acc:0.977]
Epoch [119/120    avg_loss:0.019, val_acc:0.972]
Epoch [120/120    avg_loss:0.017, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1240    1    0    0    3    0    0    0    9   22    9    0
     1    0    0]
 [   0    0    6  725    2    2    0    0    0   10    0    0    1    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    0    0    2    0    0    0    0
    10    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   15   71    0    6    0    0    0    0  771    4    2    0
     3    3    0]
 [   0    0   12    0    0    0    3    0    0    0    4 2187    4    0
     0    0    0]
 [   0    0    0    5    5   15    0    0    0    0    0    0  499    0
     1    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    1    0    0
  1137    0    0]
 [   0    0    0    0    0    0    3    0    0    2    0    0    0    0
   102  240    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.14092140921409

F1 scores:
[       nan 0.975      0.96950743 0.93548387 0.98383372 0.96027242
 0.9924357  1.         1.         0.69387755 0.92779783 0.98847458
 0.94957184 0.99730458 0.95027163 0.81355932 0.93714286]

Kappa:
0.9559683511647143
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f88a9331f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.809, val_acc:0.198]
Epoch [2/120    avg_loss:2.757, val_acc:0.271]
Epoch [3/120    avg_loss:2.692, val_acc:0.282]
Epoch [4/120    avg_loss:2.632, val_acc:0.319]
Epoch [5/120    avg_loss:2.555, val_acc:0.346]
Epoch [6/120    avg_loss:2.495, val_acc:0.395]
Epoch [7/120    avg_loss:2.428, val_acc:0.440]
Epoch [8/120    avg_loss:2.372, val_acc:0.459]
Epoch [9/120    avg_loss:2.333, val_acc:0.521]
Epoch [10/120    avg_loss:2.273, val_acc:0.520]
Epoch [11/120    avg_loss:2.195, val_acc:0.528]
Epoch [12/120    avg_loss:2.137, val_acc:0.560]
Epoch [13/120    avg_loss:2.095, val_acc:0.609]
Epoch [14/120    avg_loss:1.986, val_acc:0.646]
Epoch [15/120    avg_loss:1.874, val_acc:0.671]
Epoch [16/120    avg_loss:1.822, val_acc:0.698]
Epoch [17/120    avg_loss:1.683, val_acc:0.660]
Epoch [18/120    avg_loss:1.581, val_acc:0.664]
Epoch [19/120    avg_loss:1.447, val_acc:0.709]
Epoch [20/120    avg_loss:1.313, val_acc:0.753]
Epoch [21/120    avg_loss:1.213, val_acc:0.767]
Epoch [22/120    avg_loss:1.083, val_acc:0.762]
Epoch [23/120    avg_loss:0.989, val_acc:0.805]
Epoch [24/120    avg_loss:0.915, val_acc:0.798]
Epoch [25/120    avg_loss:0.921, val_acc:0.807]
Epoch [26/120    avg_loss:0.750, val_acc:0.854]
Epoch [27/120    avg_loss:0.696, val_acc:0.873]
Epoch [28/120    avg_loss:0.647, val_acc:0.863]
Epoch [29/120    avg_loss:0.592, val_acc:0.872]
Epoch [30/120    avg_loss:0.539, val_acc:0.891]
Epoch [31/120    avg_loss:0.498, val_acc:0.845]
Epoch [32/120    avg_loss:0.598, val_acc:0.867]
Epoch [33/120    avg_loss:0.562, val_acc:0.894]
Epoch [34/120    avg_loss:0.464, val_acc:0.907]
Epoch [35/120    avg_loss:0.380, val_acc:0.927]
Epoch [36/120    avg_loss:0.353, val_acc:0.934]
Epoch [37/120    avg_loss:0.338, val_acc:0.912]
Epoch [38/120    avg_loss:0.346, val_acc:0.901]
Epoch [39/120    avg_loss:0.329, val_acc:0.930]
Epoch [40/120    avg_loss:0.256, val_acc:0.947]
Epoch [41/120    avg_loss:0.242, val_acc:0.944]
Epoch [42/120    avg_loss:0.201, val_acc:0.938]
Epoch [43/120    avg_loss:0.183, val_acc:0.945]
Epoch [44/120    avg_loss:0.180, val_acc:0.963]
Epoch [45/120    avg_loss:0.155, val_acc:0.958]
Epoch [46/120    avg_loss:0.175, val_acc:0.935]
Epoch [47/120    avg_loss:0.155, val_acc:0.945]
Epoch [48/120    avg_loss:0.147, val_acc:0.958]
Epoch [49/120    avg_loss:0.129, val_acc:0.957]
Epoch [50/120    avg_loss:0.121, val_acc:0.959]
Epoch [51/120    avg_loss:0.109, val_acc:0.940]
Epoch [52/120    avg_loss:0.120, val_acc:0.958]
Epoch [53/120    avg_loss:0.105, val_acc:0.966]
Epoch [54/120    avg_loss:0.093, val_acc:0.965]
Epoch [55/120    avg_loss:0.083, val_acc:0.971]
Epoch [56/120    avg_loss:0.093, val_acc:0.952]
Epoch [57/120    avg_loss:0.102, val_acc:0.962]
Epoch [58/120    avg_loss:0.101, val_acc:0.958]
Epoch [59/120    avg_loss:0.076, val_acc:0.970]
Epoch [60/120    avg_loss:0.076, val_acc:0.970]
Epoch [61/120    avg_loss:0.066, val_acc:0.968]
Epoch [62/120    avg_loss:0.063, val_acc:0.966]
Epoch [63/120    avg_loss:0.054, val_acc:0.966]
Epoch [64/120    avg_loss:0.056, val_acc:0.973]
Epoch [65/120    avg_loss:0.053, val_acc:0.970]
Epoch [66/120    avg_loss:0.052, val_acc:0.974]
Epoch [67/120    avg_loss:0.052, val_acc:0.968]
Epoch [68/120    avg_loss:0.041, val_acc:0.976]
Epoch [69/120    avg_loss:0.047, val_acc:0.975]
Epoch [70/120    avg_loss:0.034, val_acc:0.974]
Epoch [71/120    avg_loss:0.039, val_acc:0.972]
Epoch [72/120    avg_loss:0.045, val_acc:0.966]
Epoch [73/120    avg_loss:0.041, val_acc:0.977]
Epoch [74/120    avg_loss:0.045, val_acc:0.961]
Epoch [75/120    avg_loss:0.092, val_acc:0.956]
Epoch [76/120    avg_loss:0.070, val_acc:0.963]
Epoch [77/120    avg_loss:0.050, val_acc:0.970]
Epoch [78/120    avg_loss:0.046, val_acc:0.932]
Epoch [79/120    avg_loss:0.066, val_acc:0.972]
Epoch [80/120    avg_loss:0.065, val_acc:0.958]
Epoch [81/120    avg_loss:0.076, val_acc:0.972]
Epoch [82/120    avg_loss:0.058, val_acc:0.975]
Epoch [83/120    avg_loss:0.042, val_acc:0.979]
Epoch [84/120    avg_loss:0.034, val_acc:0.971]
Epoch [85/120    avg_loss:0.035, val_acc:0.964]
Epoch [86/120    avg_loss:0.039, val_acc:0.979]
Epoch [87/120    avg_loss:0.034, val_acc:0.975]
Epoch [88/120    avg_loss:0.028, val_acc:0.974]
Epoch [89/120    avg_loss:0.025, val_acc:0.979]
Epoch [90/120    avg_loss:0.024, val_acc:0.979]
Epoch [91/120    avg_loss:0.025, val_acc:0.983]
Epoch [92/120    avg_loss:0.026, val_acc:0.976]
Epoch [93/120    avg_loss:0.031, val_acc:0.972]
Epoch [94/120    avg_loss:0.025, val_acc:0.977]
Epoch [95/120    avg_loss:0.030, val_acc:0.981]
Epoch [96/120    avg_loss:0.020, val_acc:0.980]
Epoch [97/120    avg_loss:0.022, val_acc:0.980]
Epoch [98/120    avg_loss:0.029, val_acc:0.972]
Epoch [99/120    avg_loss:0.025, val_acc:0.980]
Epoch [100/120    avg_loss:0.021, val_acc:0.982]
Epoch [101/120    avg_loss:0.019, val_acc:0.980]
Epoch [102/120    avg_loss:0.015, val_acc:0.984]
Epoch [103/120    avg_loss:0.017, val_acc:0.980]
Epoch [104/120    avg_loss:0.017, val_acc:0.984]
Epoch [105/120    avg_loss:0.020, val_acc:0.983]
Epoch [106/120    avg_loss:0.014, val_acc:0.980]
Epoch [107/120    avg_loss:0.014, val_acc:0.986]
Epoch [108/120    avg_loss:0.017, val_acc:0.985]
Epoch [109/120    avg_loss:0.015, val_acc:0.982]
Epoch [110/120    avg_loss:0.013, val_acc:0.976]
Epoch [111/120    avg_loss:0.018, val_acc:0.979]
Epoch [112/120    avg_loss:0.016, val_acc:0.980]
Epoch [113/120    avg_loss:0.016, val_acc:0.977]
Epoch [114/120    avg_loss:0.022, val_acc:0.982]
Epoch [115/120    avg_loss:0.016, val_acc:0.983]
Epoch [116/120    avg_loss:0.014, val_acc:0.984]
Epoch [117/120    avg_loss:0.025, val_acc:0.977]
Epoch [118/120    avg_loss:0.015, val_acc:0.979]
Epoch [119/120    avg_loss:0.019, val_acc:0.982]
Epoch [120/120    avg_loss:0.101, val_acc:0.941]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1157    7    7    0    6    0    0    2    8   87    7    0
     4    0    0]
 [   0    0    8  721    3   11    2    0    0    0    0    0    1    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  278    1    0    0    1    0    0    0    0
   155    0    0]
 [   0    0    0    0    0    0  651    0    0    4    0    2    0    0
     0    0    0]
 [   0    0    0    0    0   19    0    6    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    2   85    0    6    3    0    0    3  716   46   12    0
     0    2    0]
 [   0    0    4    2    2    8   10    0    4    6    0 2160    6    5
     3    0    0]
 [   0    0    0   29    4   14    0    0    0    0    7    0  448    0
     0    0   32]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7   19    0    0    3    0    0    0    0
  1110    0    0]
 [   0    0    0    0    0    1   94    0    0   25    0    0    0   20
   121   86    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
89.97289972899729

F1 scores:
[       nan 0.98765432 0.94179894 0.90634821 0.9638009  0.71373556
 0.9022869  0.38709677 0.99303944 0.43037975 0.89165629 0.95893452
 0.88625124 0.93434343 0.87677725 0.3954023  0.84      ]

Kappa:
0.8853512773713433
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc6b18dcef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.830, val_acc:0.125]
Epoch [2/120    avg_loss:2.783, val_acc:0.346]
Epoch [3/120    avg_loss:2.737, val_acc:0.443]
Epoch [4/120    avg_loss:2.684, val_acc:0.481]
Epoch [5/120    avg_loss:2.623, val_acc:0.495]
Epoch [6/120    avg_loss:2.560, val_acc:0.506]
Epoch [7/120    avg_loss:2.469, val_acc:0.520]
Epoch [8/120    avg_loss:2.414, val_acc:0.501]
Epoch [9/120    avg_loss:2.351, val_acc:0.485]
Epoch [10/120    avg_loss:2.257, val_acc:0.532]
Epoch [11/120    avg_loss:2.195, val_acc:0.499]
Epoch [12/120    avg_loss:2.159, val_acc:0.568]
Epoch [13/120    avg_loss:2.117, val_acc:0.555]
Epoch [14/120    avg_loss:2.020, val_acc:0.598]
Epoch [15/120    avg_loss:1.962, val_acc:0.608]
Epoch [16/120    avg_loss:1.902, val_acc:0.635]
Epoch [17/120    avg_loss:1.851, val_acc:0.605]
Epoch [18/120    avg_loss:1.681, val_acc:0.650]
Epoch [19/120    avg_loss:1.608, val_acc:0.672]
Epoch [20/120    avg_loss:1.576, val_acc:0.645]
Epoch [21/120    avg_loss:1.436, val_acc:0.685]
Epoch [22/120    avg_loss:1.351, val_acc:0.685]
Epoch [23/120    avg_loss:1.276, val_acc:0.686]
Epoch [24/120    avg_loss:1.260, val_acc:0.711]
Epoch [25/120    avg_loss:1.124, val_acc:0.703]
Epoch [26/120    avg_loss:1.027, val_acc:0.759]
Epoch [27/120    avg_loss:0.957, val_acc:0.748]
Epoch [28/120    avg_loss:0.890, val_acc:0.784]
Epoch [29/120    avg_loss:0.789, val_acc:0.785]
Epoch [30/120    avg_loss:0.762, val_acc:0.780]
Epoch [31/120    avg_loss:0.733, val_acc:0.794]
Epoch [32/120    avg_loss:0.651, val_acc:0.818]
Epoch [33/120    avg_loss:0.568, val_acc:0.824]
Epoch [34/120    avg_loss:0.568, val_acc:0.848]
Epoch [35/120    avg_loss:0.546, val_acc:0.860]
Epoch [36/120    avg_loss:0.450, val_acc:0.866]
Epoch [37/120    avg_loss:0.411, val_acc:0.867]
Epoch [38/120    avg_loss:0.411, val_acc:0.858]
Epoch [39/120    avg_loss:0.398, val_acc:0.874]
Epoch [40/120    avg_loss:0.351, val_acc:0.864]
Epoch [41/120    avg_loss:0.307, val_acc:0.867]
Epoch [42/120    avg_loss:0.318, val_acc:0.869]
Epoch [43/120    avg_loss:0.297, val_acc:0.889]
Epoch [44/120    avg_loss:0.252, val_acc:0.893]
Epoch [45/120    avg_loss:0.266, val_acc:0.884]
Epoch [46/120    avg_loss:0.274, val_acc:0.893]
Epoch [47/120    avg_loss:0.244, val_acc:0.877]
Epoch [48/120    avg_loss:0.279, val_acc:0.870]
Epoch [49/120    avg_loss:0.346, val_acc:0.896]
Epoch [50/120    avg_loss:0.273, val_acc:0.904]
Epoch [51/120    avg_loss:0.191, val_acc:0.910]
Epoch [52/120    avg_loss:0.177, val_acc:0.930]
Epoch [53/120    avg_loss:0.160, val_acc:0.936]
Epoch [54/120    avg_loss:0.154, val_acc:0.912]
Epoch [55/120    avg_loss:0.140, val_acc:0.936]
Epoch [56/120    avg_loss:0.137, val_acc:0.927]
Epoch [57/120    avg_loss:0.136, val_acc:0.937]
Epoch [58/120    avg_loss:0.131, val_acc:0.938]
Epoch [59/120    avg_loss:0.119, val_acc:0.935]
Epoch [60/120    avg_loss:0.108, val_acc:0.935]
Epoch [61/120    avg_loss:0.112, val_acc:0.932]
Epoch [62/120    avg_loss:0.100, val_acc:0.949]
Epoch [63/120    avg_loss:0.090, val_acc:0.949]
Epoch [64/120    avg_loss:0.110, val_acc:0.953]
Epoch [65/120    avg_loss:0.096, val_acc:0.952]
Epoch [66/120    avg_loss:0.080, val_acc:0.957]
Epoch [67/120    avg_loss:0.071, val_acc:0.963]
Epoch [68/120    avg_loss:0.067, val_acc:0.959]
Epoch [69/120    avg_loss:0.055, val_acc:0.963]
Epoch [70/120    avg_loss:0.072, val_acc:0.964]
Epoch [71/120    avg_loss:0.066, val_acc:0.963]
Epoch [72/120    avg_loss:0.072, val_acc:0.941]
Epoch [73/120    avg_loss:0.111, val_acc:0.941]
Epoch [74/120    avg_loss:0.104, val_acc:0.940]
Epoch [75/120    avg_loss:0.066, val_acc:0.961]
Epoch [76/120    avg_loss:0.050, val_acc:0.963]
Epoch [77/120    avg_loss:0.053, val_acc:0.957]
Epoch [78/120    avg_loss:0.047, val_acc:0.974]
Epoch [79/120    avg_loss:0.051, val_acc:0.964]
Epoch [80/120    avg_loss:0.054, val_acc:0.972]
Epoch [81/120    avg_loss:0.065, val_acc:0.963]
Epoch [82/120    avg_loss:0.068, val_acc:0.966]
Epoch [83/120    avg_loss:0.047, val_acc:0.972]
Epoch [84/120    avg_loss:0.036, val_acc:0.967]
Epoch [85/120    avg_loss:0.032, val_acc:0.966]
Epoch [86/120    avg_loss:0.031, val_acc:0.971]
Epoch [87/120    avg_loss:0.037, val_acc:0.981]
Epoch [88/120    avg_loss:0.027, val_acc:0.971]
Epoch [89/120    avg_loss:0.032, val_acc:0.972]
Epoch [90/120    avg_loss:0.030, val_acc:0.970]
Epoch [91/120    avg_loss:0.033, val_acc:0.976]
Epoch [92/120    avg_loss:0.023, val_acc:0.966]
Epoch [93/120    avg_loss:0.029, val_acc:0.976]
Epoch [94/120    avg_loss:0.026, val_acc:0.980]
Epoch [95/120    avg_loss:0.024, val_acc:0.975]
Epoch [96/120    avg_loss:0.032, val_acc:0.945]
Epoch [97/120    avg_loss:0.038, val_acc:0.971]
Epoch [98/120    avg_loss:0.029, val_acc:0.979]
Epoch [99/120    avg_loss:0.047, val_acc:0.979]
Epoch [100/120    avg_loss:0.042, val_acc:0.976]
Epoch [101/120    avg_loss:0.022, val_acc:0.981]
Epoch [102/120    avg_loss:0.019, val_acc:0.982]
Epoch [103/120    avg_loss:0.020, val_acc:0.984]
Epoch [104/120    avg_loss:0.021, val_acc:0.985]
Epoch [105/120    avg_loss:0.018, val_acc:0.982]
Epoch [106/120    avg_loss:0.019, val_acc:0.981]
Epoch [107/120    avg_loss:0.016, val_acc:0.982]
Epoch [108/120    avg_loss:0.014, val_acc:0.983]
Epoch [109/120    avg_loss:0.018, val_acc:0.982]
Epoch [110/120    avg_loss:0.016, val_acc:0.982]
Epoch [111/120    avg_loss:0.016, val_acc:0.982]
Epoch [112/120    avg_loss:0.016, val_acc:0.982]
Epoch [113/120    avg_loss:0.014, val_acc:0.982]
Epoch [114/120    avg_loss:0.015, val_acc:0.982]
Epoch [115/120    avg_loss:0.014, val_acc:0.980]
Epoch [116/120    avg_loss:0.014, val_acc:0.981]
Epoch [117/120    avg_loss:0.016, val_acc:0.981]
Epoch [118/120    avg_loss:0.016, val_acc:0.981]
Epoch [119/120    avg_loss:0.015, val_acc:0.981]
Epoch [120/120    avg_loss:0.016, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1246    0    5    0    2    0    0    1    3   16    4    0
     0    8    0]
 [   0    0    8  714    0    1    0    0    0   13    0    0   11    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   13    0    0    3    0
     0    0    0]
 [   0    0    4   88    0    0    0    0    0    0  761   17    0    0
     1    4    0]
 [   0    0    5    0    0    0    1    0    0    0    4 2188    3    0
     9    0    0]
 [   0    0    1   10    7    0    0    0    0    0   15    2  499    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0    6    0    0    6    0    0    0    0
   142  193    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.56639566395664

F1 scores:
[       nan 0.98765432 0.97763829 0.91421255 0.97025172 0.99539171
 0.9924357  1.         1.         0.48148148 0.91742013 0.98691926
 0.94597156 1.         0.93744856 0.69927536 0.99401198]

Kappa:
0.9493857784235941
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb741e6cf60>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.835, val_acc:0.090]
Epoch [2/120    avg_loss:2.787, val_acc:0.220]
Epoch [3/120    avg_loss:2.734, val_acc:0.207]
Epoch [4/120    avg_loss:2.687, val_acc:0.241]
Epoch [5/120    avg_loss:2.618, val_acc:0.278]
Epoch [6/120    avg_loss:2.562, val_acc:0.314]
Epoch [7/120    avg_loss:2.521, val_acc:0.367]
Epoch [8/120    avg_loss:2.446, val_acc:0.369]
Epoch [9/120    avg_loss:2.365, val_acc:0.398]
Epoch [10/120    avg_loss:2.310, val_acc:0.396]
Epoch [11/120    avg_loss:2.234, val_acc:0.412]
Epoch [12/120    avg_loss:2.170, val_acc:0.419]
Epoch [13/120    avg_loss:2.105, val_acc:0.420]
Epoch [14/120    avg_loss:2.014, val_acc:0.433]
Epoch [15/120    avg_loss:1.956, val_acc:0.466]
Epoch [16/120    avg_loss:1.883, val_acc:0.529]
Epoch [17/120    avg_loss:1.788, val_acc:0.600]
Epoch [18/120    avg_loss:1.695, val_acc:0.603]
Epoch [19/120    avg_loss:1.600, val_acc:0.639]
Epoch [20/120    avg_loss:1.495, val_acc:0.655]
Epoch [21/120    avg_loss:1.404, val_acc:0.689]
Epoch [22/120    avg_loss:1.307, val_acc:0.713]
Epoch [23/120    avg_loss:1.176, val_acc:0.694]
Epoch [24/120    avg_loss:1.143, val_acc:0.698]
Epoch [25/120    avg_loss:1.035, val_acc:0.755]
Epoch [26/120    avg_loss:0.979, val_acc:0.741]
Epoch [27/120    avg_loss:0.910, val_acc:0.794]
Epoch [28/120    avg_loss:0.781, val_acc:0.823]
Epoch [29/120    avg_loss:0.766, val_acc:0.828]
Epoch [30/120    avg_loss:0.726, val_acc:0.837]
Epoch [31/120    avg_loss:0.757, val_acc:0.772]
Epoch [32/120    avg_loss:0.637, val_acc:0.836]
Epoch [33/120    avg_loss:0.579, val_acc:0.883]
Epoch [34/120    avg_loss:0.540, val_acc:0.839]
Epoch [35/120    avg_loss:0.530, val_acc:0.870]
Epoch [36/120    avg_loss:0.409, val_acc:0.883]
Epoch [37/120    avg_loss:0.382, val_acc:0.899]
Epoch [38/120    avg_loss:0.393, val_acc:0.900]
Epoch [39/120    avg_loss:0.476, val_acc:0.896]
Epoch [40/120    avg_loss:0.415, val_acc:0.878]
Epoch [41/120    avg_loss:0.347, val_acc:0.894]
Epoch [42/120    avg_loss:0.293, val_acc:0.926]
Epoch [43/120    avg_loss:0.242, val_acc:0.916]
Epoch [44/120    avg_loss:0.233, val_acc:0.924]
Epoch [45/120    avg_loss:0.253, val_acc:0.920]
Epoch [46/120    avg_loss:0.237, val_acc:0.922]
Epoch [47/120    avg_loss:0.177, val_acc:0.912]
Epoch [48/120    avg_loss:0.167, val_acc:0.932]
Epoch [49/120    avg_loss:0.156, val_acc:0.933]
Epoch [50/120    avg_loss:0.148, val_acc:0.101]
Epoch [51/120    avg_loss:1.570, val_acc:0.552]
Epoch [52/120    avg_loss:1.200, val_acc:0.575]
Epoch [53/120    avg_loss:0.881, val_acc:0.716]
Epoch [54/120    avg_loss:0.655, val_acc:0.752]
Epoch [55/120    avg_loss:0.527, val_acc:0.718]
Epoch [56/120    avg_loss:0.475, val_acc:0.794]
Epoch [57/120    avg_loss:0.505, val_acc:0.811]
Epoch [58/120    avg_loss:0.412, val_acc:0.818]
Epoch [59/120    avg_loss:0.376, val_acc:0.858]
Epoch [60/120    avg_loss:0.344, val_acc:0.855]
Epoch [61/120    avg_loss:0.348, val_acc:0.865]
Epoch [62/120    avg_loss:0.290, val_acc:0.891]
Epoch [63/120    avg_loss:0.244, val_acc:0.900]
Epoch [64/120    avg_loss:0.227, val_acc:0.905]
Epoch [65/120    avg_loss:0.225, val_acc:0.909]
Epoch [66/120    avg_loss:0.208, val_acc:0.916]
Epoch [67/120    avg_loss:0.209, val_acc:0.918]
Epoch [68/120    avg_loss:0.203, val_acc:0.918]
Epoch [69/120    avg_loss:0.205, val_acc:0.918]
Epoch [70/120    avg_loss:0.198, val_acc:0.920]
Epoch [71/120    avg_loss:0.188, val_acc:0.925]
Epoch [72/120    avg_loss:0.190, val_acc:0.926]
Epoch [73/120    avg_loss:0.179, val_acc:0.924]
Epoch [74/120    avg_loss:0.193, val_acc:0.925]
Epoch [75/120    avg_loss:0.182, val_acc:0.932]
Epoch [76/120    avg_loss:0.171, val_acc:0.929]
Epoch [77/120    avg_loss:0.178, val_acc:0.931]
Epoch [78/120    avg_loss:0.182, val_acc:0.931]
Epoch [79/120    avg_loss:0.179, val_acc:0.932]
Epoch [80/120    avg_loss:0.182, val_acc:0.932]
Epoch [81/120    avg_loss:0.162, val_acc:0.932]
Epoch [82/120    avg_loss:0.179, val_acc:0.932]
Epoch [83/120    avg_loss:0.174, val_acc:0.932]
Epoch [84/120    avg_loss:0.191, val_acc:0.932]
Epoch [85/120    avg_loss:0.172, val_acc:0.932]
Epoch [86/120    avg_loss:0.177, val_acc:0.932]
Epoch [87/120    avg_loss:0.166, val_acc:0.932]
Epoch [88/120    avg_loss:0.174, val_acc:0.933]
Epoch [89/120    avg_loss:0.177, val_acc:0.933]
Epoch [90/120    avg_loss:0.165, val_acc:0.933]
Epoch [91/120    avg_loss:0.169, val_acc:0.933]
Epoch [92/120    avg_loss:0.178, val_acc:0.933]
Epoch [93/120    avg_loss:0.171, val_acc:0.933]
Epoch [94/120    avg_loss:0.170, val_acc:0.933]
Epoch [95/120    avg_loss:0.181, val_acc:0.932]
Epoch [96/120    avg_loss:0.169, val_acc:0.932]
Epoch [97/120    avg_loss:0.188, val_acc:0.932]
Epoch [98/120    avg_loss:0.165, val_acc:0.932]
Epoch [99/120    avg_loss:0.168, val_acc:0.933]
Epoch [100/120    avg_loss:0.173, val_acc:0.933]
Epoch [101/120    avg_loss:0.171, val_acc:0.933]
Epoch [102/120    avg_loss:0.170, val_acc:0.933]
Epoch [103/120    avg_loss:0.166, val_acc:0.933]
Epoch [104/120    avg_loss:0.165, val_acc:0.933]
Epoch [105/120    avg_loss:0.163, val_acc:0.933]
Epoch [106/120    avg_loss:0.171, val_acc:0.933]
Epoch [107/120    avg_loss:0.178, val_acc:0.933]
Epoch [108/120    avg_loss:0.177, val_acc:0.934]
Epoch [109/120    avg_loss:0.169, val_acc:0.933]
Epoch [110/120    avg_loss:0.168, val_acc:0.933]
Epoch [111/120    avg_loss:0.171, val_acc:0.934]
Epoch [112/120    avg_loss:0.168, val_acc:0.933]
Epoch [113/120    avg_loss:0.161, val_acc:0.933]
Epoch [114/120    avg_loss:0.159, val_acc:0.934]
Epoch [115/120    avg_loss:0.170, val_acc:0.933]
Epoch [116/120    avg_loss:0.175, val_acc:0.933]
Epoch [117/120    avg_loss:0.170, val_acc:0.933]
Epoch [118/120    avg_loss:0.166, val_acc:0.933]
Epoch [119/120    avg_loss:0.173, val_acc:0.933]
Epoch [120/120    avg_loss:0.168, val_acc:0.933]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1109    9    0    0    0    0    0   11    3  141    4    0
     8    0    0]
 [   0    0    4  679    0    0    0    0    0   18    0   15   30    0
     1    0    0]
 [   0    0    0   36  174    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  404    0    0    0   13    0    0    0    0
    18    0    0]
 [   0    0    0    0    0    0  641    0    0    8    0    2    0    0
     6    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    5    0    0    0    0    0    0  425    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   16   46    0    8    0    0    0   17  702   80    0    0
     6    0    0]
 [   0    0  182    0    0   18    0    0    0   14   20 1941    8    3
    22    0    2]
 [   0    0    0   18    5    1    0    0    0    0   17    0  484    0
     1    3    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    2    0    0    0    0
  1132    3    0]
 [   0    0    0    0    0    0    7    0    0   18    0    0    0    0
   159  163    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
88.93224932249322

F1 scores:
[       nan 0.93023256 0.85406238 0.88411458 0.8877551  0.93087558
 0.98237548 1.         0.99415205 0.25       0.86827458 0.88448394
 0.90977444 0.9919571  0.90850722 0.63178295 0.95402299]

Kappa:
0.8737366086719301
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5b3d76df60>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.810, val_acc:0.176]
Epoch [2/120    avg_loss:2.752, val_acc:0.247]
Epoch [3/120    avg_loss:2.686, val_acc:0.275]
Epoch [4/120    avg_loss:2.628, val_acc:0.301]
Epoch [5/120    avg_loss:2.548, val_acc:0.351]
Epoch [6/120    avg_loss:2.478, val_acc:0.394]
Epoch [7/120    avg_loss:2.400, val_acc:0.483]
Epoch [8/120    avg_loss:2.340, val_acc:0.465]
Epoch [9/120    avg_loss:2.276, val_acc:0.554]
Epoch [10/120    avg_loss:2.230, val_acc:0.554]
Epoch [11/120    avg_loss:2.173, val_acc:0.587]
Epoch [12/120    avg_loss:2.091, val_acc:0.581]
Epoch [13/120    avg_loss:2.034, val_acc:0.604]
Epoch [14/120    avg_loss:1.961, val_acc:0.635]
Epoch [15/120    avg_loss:1.888, val_acc:0.656]
Epoch [16/120    avg_loss:1.800, val_acc:0.656]
Epoch [17/120    avg_loss:1.691, val_acc:0.649]
Epoch [18/120    avg_loss:1.581, val_acc:0.664]
Epoch [19/120    avg_loss:1.530, val_acc:0.694]
Epoch [20/120    avg_loss:1.426, val_acc:0.700]
Epoch [21/120    avg_loss:1.301, val_acc:0.716]
Epoch [22/120    avg_loss:1.127, val_acc:0.730]
Epoch [23/120    avg_loss:1.071, val_acc:0.779]
Epoch [24/120    avg_loss:0.973, val_acc:0.780]
Epoch [25/120    avg_loss:0.887, val_acc:0.774]
Epoch [26/120    avg_loss:0.896, val_acc:0.796]
Epoch [27/120    avg_loss:0.753, val_acc:0.818]
Epoch [28/120    avg_loss:0.694, val_acc:0.850]
Epoch [29/120    avg_loss:0.620, val_acc:0.839]
Epoch [30/120    avg_loss:0.570, val_acc:0.858]
Epoch [31/120    avg_loss:0.578, val_acc:0.869]
Epoch [32/120    avg_loss:0.486, val_acc:0.876]
Epoch [33/120    avg_loss:0.479, val_acc:0.760]
Epoch [34/120    avg_loss:0.431, val_acc:0.875]
Epoch [35/120    avg_loss:0.399, val_acc:0.897]
Epoch [36/120    avg_loss:0.318, val_acc:0.904]
Epoch [37/120    avg_loss:0.315, val_acc:0.877]
Epoch [38/120    avg_loss:0.302, val_acc:0.881]
Epoch [39/120    avg_loss:0.286, val_acc:0.853]
Epoch [40/120    avg_loss:0.284, val_acc:0.914]
Epoch [41/120    avg_loss:0.227, val_acc:0.904]
Epoch [42/120    avg_loss:0.214, val_acc:0.929]
Epoch [43/120    avg_loss:0.222, val_acc:0.910]
Epoch [44/120    avg_loss:0.181, val_acc:0.925]
Epoch [45/120    avg_loss:0.169, val_acc:0.919]
Epoch [46/120    avg_loss:0.174, val_acc:0.914]
Epoch [47/120    avg_loss:0.198, val_acc:0.903]
Epoch [48/120    avg_loss:0.161, val_acc:0.917]
Epoch [49/120    avg_loss:0.187, val_acc:0.942]
Epoch [50/120    avg_loss:0.161, val_acc:0.940]
Epoch [51/120    avg_loss:0.148, val_acc:0.936]
Epoch [52/120    avg_loss:0.122, val_acc:0.948]
Epoch [53/120    avg_loss:0.108, val_acc:0.942]
Epoch [54/120    avg_loss:0.108, val_acc:0.947]
Epoch [55/120    avg_loss:0.406, val_acc:0.802]
Epoch [56/120    avg_loss:0.444, val_acc:0.875]
Epoch [57/120    avg_loss:0.281, val_acc:0.906]
Epoch [58/120    avg_loss:0.195, val_acc:0.910]
Epoch [59/120    avg_loss:0.173, val_acc:0.932]
Epoch [60/120    avg_loss:0.125, val_acc:0.919]
Epoch [61/120    avg_loss:0.112, val_acc:0.929]
Epoch [62/120    avg_loss:0.159, val_acc:0.912]
Epoch [63/120    avg_loss:0.119, val_acc:0.948]
Epoch [64/120    avg_loss:0.111, val_acc:0.935]
Epoch [65/120    avg_loss:0.115, val_acc:0.934]
Epoch [66/120    avg_loss:0.079, val_acc:0.943]
Epoch [67/120    avg_loss:0.073, val_acc:0.959]
Epoch [68/120    avg_loss:0.064, val_acc:0.951]
Epoch [69/120    avg_loss:0.064, val_acc:0.955]
Epoch [70/120    avg_loss:0.062, val_acc:0.947]
Epoch [71/120    avg_loss:0.070, val_acc:0.957]
Epoch [72/120    avg_loss:0.067, val_acc:0.955]
Epoch [73/120    avg_loss:0.049, val_acc:0.954]
Epoch [74/120    avg_loss:0.060, val_acc:0.958]
Epoch [75/120    avg_loss:0.076, val_acc:0.958]
Epoch [76/120    avg_loss:0.063, val_acc:0.959]
Epoch [77/120    avg_loss:0.049, val_acc:0.953]
Epoch [78/120    avg_loss:0.056, val_acc:0.962]
Epoch [79/120    avg_loss:0.058, val_acc:0.956]
Epoch [80/120    avg_loss:0.049, val_acc:0.958]
Epoch [81/120    avg_loss:0.048, val_acc:0.971]
Epoch [82/120    avg_loss:0.063, val_acc:0.943]
Epoch [83/120    avg_loss:0.082, val_acc:0.949]
Epoch [84/120    avg_loss:0.071, val_acc:0.968]
Epoch [85/120    avg_loss:0.060, val_acc:0.965]
Epoch [86/120    avg_loss:0.035, val_acc:0.972]
Epoch [87/120    avg_loss:0.038, val_acc:0.972]
Epoch [88/120    avg_loss:0.038, val_acc:0.965]
Epoch [89/120    avg_loss:0.030, val_acc:0.961]
Epoch [90/120    avg_loss:0.055, val_acc:0.953]
Epoch [91/120    avg_loss:0.073, val_acc:0.969]
Epoch [92/120    avg_loss:0.037, val_acc:0.976]
Epoch [93/120    avg_loss:0.039, val_acc:0.973]
Epoch [94/120    avg_loss:0.036, val_acc:0.972]
Epoch [95/120    avg_loss:0.033, val_acc:0.968]
Epoch [96/120    avg_loss:0.026, val_acc:0.966]
Epoch [97/120    avg_loss:0.022, val_acc:0.969]
Epoch [98/120    avg_loss:0.028, val_acc:0.976]
Epoch [99/120    avg_loss:0.023, val_acc:0.969]
Epoch [100/120    avg_loss:0.021, val_acc:0.968]
Epoch [101/120    avg_loss:0.034, val_acc:0.966]
Epoch [102/120    avg_loss:0.025, val_acc:0.975]
Epoch [103/120    avg_loss:0.021, val_acc:0.976]
Epoch [104/120    avg_loss:0.022, val_acc:0.975]
Epoch [105/120    avg_loss:0.037, val_acc:0.968]
Epoch [106/120    avg_loss:0.023, val_acc:0.976]
Epoch [107/120    avg_loss:0.016, val_acc:0.965]
Epoch [108/120    avg_loss:0.018, val_acc:0.972]
Epoch [109/120    avg_loss:0.026, val_acc:0.973]
Epoch [110/120    avg_loss:0.023, val_acc:0.978]
Epoch [111/120    avg_loss:0.021, val_acc:0.978]
Epoch [112/120    avg_loss:0.022, val_acc:0.971]
Epoch [113/120    avg_loss:0.017, val_acc:0.975]
Epoch [114/120    avg_loss:0.014, val_acc:0.981]
Epoch [115/120    avg_loss:0.023, val_acc:0.975]
Epoch [116/120    avg_loss:0.015, val_acc:0.978]
Epoch [117/120    avg_loss:0.016, val_acc:0.977]
Epoch [118/120    avg_loss:0.011, val_acc:0.979]
Epoch [119/120    avg_loss:0.020, val_acc:0.984]
Epoch [120/120    avg_loss:0.016, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    3    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1264    0    1    0    2    0    0    0    2   14    0    0
     0    2    0]
 [   0    0    4  679    2   15    0    0    0    4    0    0   43    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    0    0    5    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    5    0    0   11    0    0    1    0
     0    0    0]
 [   0    0    7   88    0    2    2    0    0    0  773    1    0    0
     1    1    0]
 [   0    0   15    0    0    0    3    0    0    0    7 2180    2    3
     0    0    0]
 [   0    0    0    2    1    4    0    0    0    1   13    8  503    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    1    0    0    2   14    0    0    2    0    0    0    0
   122  206    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.49051490514906

F1 scores:
[       nan 0.96202532 0.98022489 0.89518787 0.99069767 0.96262741
 0.97830965 1.         1.         0.53658537 0.9257485  0.98731884
 0.9289012  0.9919571  0.94679967 0.74100719 0.98823529]

Kappa:
0.9485339056365887
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff953fd9f98>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.829, val_acc:0.125]
Epoch [2/120    avg_loss:2.772, val_acc:0.257]
Epoch [3/120    avg_loss:2.697, val_acc:0.274]
Epoch [4/120    avg_loss:2.626, val_acc:0.262]
Epoch [5/120    avg_loss:2.555, val_acc:0.273]
Epoch [6/120    avg_loss:2.490, val_acc:0.313]
Epoch [7/120    avg_loss:2.379, val_acc:0.400]
Epoch [8/120    avg_loss:2.351, val_acc:0.402]
Epoch [9/120    avg_loss:2.265, val_acc:0.399]
Epoch [10/120    avg_loss:2.192, val_acc:0.387]
Epoch [11/120    avg_loss:2.085, val_acc:0.405]
Epoch [12/120    avg_loss:2.029, val_acc:0.486]
Epoch [13/120    avg_loss:1.940, val_acc:0.444]
Epoch [14/120    avg_loss:1.846, val_acc:0.432]
Epoch [15/120    avg_loss:1.770, val_acc:0.452]
Epoch [16/120    avg_loss:1.681, val_acc:0.520]
Epoch [17/120    avg_loss:1.585, val_acc:0.547]
Epoch [18/120    avg_loss:1.429, val_acc:0.590]
Epoch [19/120    avg_loss:1.293, val_acc:0.618]
Epoch [20/120    avg_loss:1.198, val_acc:0.644]
Epoch [21/120    avg_loss:1.168, val_acc:0.649]
Epoch [22/120    avg_loss:1.066, val_acc:0.693]
Epoch [23/120    avg_loss:0.996, val_acc:0.735]
Epoch [24/120    avg_loss:0.931, val_acc:0.759]
Epoch [25/120    avg_loss:0.870, val_acc:0.758]
Epoch [26/120    avg_loss:0.804, val_acc:0.787]
Epoch [27/120    avg_loss:0.785, val_acc:0.758]
Epoch [28/120    avg_loss:0.737, val_acc:0.813]
Epoch [29/120    avg_loss:0.661, val_acc:0.794]
Epoch [30/120    avg_loss:0.651, val_acc:0.822]
Epoch [31/120    avg_loss:0.609, val_acc:0.838]
Epoch [32/120    avg_loss:0.502, val_acc:0.881]
Epoch [33/120    avg_loss:0.496, val_acc:0.857]
Epoch [34/120    avg_loss:0.452, val_acc:0.885]
Epoch [35/120    avg_loss:0.390, val_acc:0.866]
Epoch [36/120    avg_loss:0.383, val_acc:0.889]
Epoch [37/120    avg_loss:0.307, val_acc:0.891]
Epoch [38/120    avg_loss:0.347, val_acc:0.843]
Epoch [39/120    avg_loss:0.327, val_acc:0.905]
Epoch [40/120    avg_loss:0.312, val_acc:0.895]
Epoch [41/120    avg_loss:0.275, val_acc:0.913]
Epoch [42/120    avg_loss:0.245, val_acc:0.925]
Epoch [43/120    avg_loss:0.238, val_acc:0.930]
Epoch [44/120    avg_loss:0.214, val_acc:0.880]
Epoch [45/120    avg_loss:0.229, val_acc:0.937]
Epoch [46/120    avg_loss:0.160, val_acc:0.930]
Epoch [47/120    avg_loss:0.169, val_acc:0.948]
Epoch [48/120    avg_loss:0.179, val_acc:0.938]
Epoch [49/120    avg_loss:0.155, val_acc:0.950]
Epoch [50/120    avg_loss:0.207, val_acc:0.937]
Epoch [51/120    avg_loss:0.165, val_acc:0.940]
Epoch [52/120    avg_loss:0.134, val_acc:0.945]
Epoch [53/120    avg_loss:0.119, val_acc:0.948]
Epoch [54/120    avg_loss:0.127, val_acc:0.950]
Epoch [55/120    avg_loss:0.119, val_acc:0.955]
Epoch [56/120    avg_loss:0.104, val_acc:0.945]
Epoch [57/120    avg_loss:0.099, val_acc:0.952]
Epoch [58/120    avg_loss:0.100, val_acc:0.959]
Epoch [59/120    avg_loss:0.111, val_acc:0.950]
Epoch [60/120    avg_loss:0.084, val_acc:0.953]
Epoch [61/120    avg_loss:0.087, val_acc:0.928]
Epoch [62/120    avg_loss:0.123, val_acc:0.943]
Epoch [63/120    avg_loss:0.099, val_acc:0.939]
Epoch [64/120    avg_loss:0.084, val_acc:0.955]
Epoch [65/120    avg_loss:0.065, val_acc:0.966]
Epoch [66/120    avg_loss:0.068, val_acc:0.944]
Epoch [67/120    avg_loss:0.073, val_acc:0.958]
Epoch [68/120    avg_loss:0.061, val_acc:0.961]
Epoch [69/120    avg_loss:0.058, val_acc:0.963]
Epoch [70/120    avg_loss:0.055, val_acc:0.967]
Epoch [71/120    avg_loss:0.059, val_acc:0.964]
Epoch [72/120    avg_loss:0.079, val_acc:0.962]
Epoch [73/120    avg_loss:0.059, val_acc:0.961]
Epoch [74/120    avg_loss:0.047, val_acc:0.963]
Epoch [75/120    avg_loss:0.049, val_acc:0.965]
Epoch [76/120    avg_loss:0.041, val_acc:0.980]
Epoch [77/120    avg_loss:0.041, val_acc:0.975]
Epoch [78/120    avg_loss:0.039, val_acc:0.970]
Epoch [79/120    avg_loss:0.046, val_acc:0.966]
Epoch [80/120    avg_loss:0.041, val_acc:0.973]
Epoch [81/120    avg_loss:0.038, val_acc:0.976]
Epoch [82/120    avg_loss:0.049, val_acc:0.952]
Epoch [83/120    avg_loss:0.064, val_acc:0.968]
Epoch [84/120    avg_loss:0.040, val_acc:0.972]
Epoch [85/120    avg_loss:0.034, val_acc:0.977]
Epoch [86/120    avg_loss:0.031, val_acc:0.980]
Epoch [87/120    avg_loss:0.034, val_acc:0.976]
Epoch [88/120    avg_loss:0.040, val_acc:0.967]
Epoch [89/120    avg_loss:0.032, val_acc:0.975]
Epoch [90/120    avg_loss:0.060, val_acc:0.950]
Epoch [91/120    avg_loss:0.090, val_acc:0.959]
Epoch [92/120    avg_loss:0.035, val_acc:0.972]
Epoch [93/120    avg_loss:0.031, val_acc:0.972]
Epoch [94/120    avg_loss:0.025, val_acc:0.982]
Epoch [95/120    avg_loss:0.024, val_acc:0.973]
Epoch [96/120    avg_loss:0.027, val_acc:0.981]
Epoch [97/120    avg_loss:0.037, val_acc:0.968]
Epoch [98/120    avg_loss:0.033, val_acc:0.979]
Epoch [99/120    avg_loss:0.033, val_acc:0.974]
Epoch [100/120    avg_loss:0.029, val_acc:0.971]
Epoch [101/120    avg_loss:0.032, val_acc:0.982]
Epoch [102/120    avg_loss:0.020, val_acc:0.974]
Epoch [103/120    avg_loss:0.027, val_acc:0.979]
Epoch [104/120    avg_loss:0.020, val_acc:0.976]
Epoch [105/120    avg_loss:0.019, val_acc:0.977]
Epoch [106/120    avg_loss:0.018, val_acc:0.982]
Epoch [107/120    avg_loss:0.015, val_acc:0.980]
Epoch [108/120    avg_loss:0.018, val_acc:0.981]
Epoch [109/120    avg_loss:0.022, val_acc:0.980]
Epoch [110/120    avg_loss:0.017, val_acc:0.982]
Epoch [111/120    avg_loss:0.018, val_acc:0.984]
Epoch [112/120    avg_loss:0.017, val_acc:0.979]
Epoch [113/120    avg_loss:0.017, val_acc:0.973]
Epoch [114/120    avg_loss:0.020, val_acc:0.977]
Epoch [115/120    avg_loss:0.019, val_acc:0.980]
Epoch [116/120    avg_loss:0.016, val_acc:0.980]
Epoch [117/120    avg_loss:0.014, val_acc:0.982]
Epoch [118/120    avg_loss:0.012, val_acc:0.983]
Epoch [119/120    avg_loss:0.018, val_acc:0.981]
Epoch [120/120    avg_loss:0.013, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1226    1    1    0    2    0    0    0   14   35    6    0
     0    0    0]
 [   0    0    1  704    9    5    0    0    0   10    0    0   18    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   12    0    0    2    0
     0    0    0]
 [   0    0    5   81    0    3    0    0    0    0  774    4    6    0
     0    2    0]
 [   0    0    7    0    0    0    5    0    0    0   15 2180    1    2
     0    0    0]
 [   0    0    2    6    5   12    0    0    0    0    2   14  492    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    1    0    0    0    0    0    1    0    1    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0    0    0    0    4    0    0    0    0
   142  201    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.21951219512195

F1 scores:
[       nan 0.975      0.96955318 0.9125081  0.96598639 0.9740699
 0.99164768 1.         0.99883856 0.5106383  0.92088043 0.98043625
 0.92742696 0.99462366 0.94000827 0.73090909 0.98203593]

Kappa:
0.945424090590999
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0885e72ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.825, val_acc:0.026]
Epoch [2/120    avg_loss:2.776, val_acc:0.105]
Epoch [3/120    avg_loss:2.723, val_acc:0.150]
Epoch [4/120    avg_loss:2.687, val_acc:0.210]
Epoch [5/120    avg_loss:2.632, val_acc:0.250]
Epoch [6/120    avg_loss:2.573, val_acc:0.266]
Epoch [7/120    avg_loss:2.513, val_acc:0.330]
Epoch [8/120    avg_loss:2.473, val_acc:0.433]
Epoch [9/120    avg_loss:2.435, val_acc:0.454]
Epoch [10/120    avg_loss:2.351, val_acc:0.522]
Epoch [11/120    avg_loss:2.328, val_acc:0.509]
Epoch [12/120    avg_loss:2.273, val_acc:0.590]
Epoch [13/120    avg_loss:2.186, val_acc:0.582]
Epoch [14/120    avg_loss:2.151, val_acc:0.573]
Epoch [15/120    avg_loss:2.084, val_acc:0.621]
Epoch [16/120    avg_loss:1.999, val_acc:0.630]
Epoch [17/120    avg_loss:1.830, val_acc:0.635]
Epoch [18/120    avg_loss:1.725, val_acc:0.670]
Epoch [19/120    avg_loss:1.610, val_acc:0.664]
Epoch [20/120    avg_loss:1.488, val_acc:0.540]
Epoch [21/120    avg_loss:1.408, val_acc:0.658]
Epoch [22/120    avg_loss:1.275, val_acc:0.719]
Epoch [23/120    avg_loss:1.179, val_acc:0.756]
Epoch [24/120    avg_loss:1.023, val_acc:0.783]
Epoch [25/120    avg_loss:0.915, val_acc:0.826]
Epoch [26/120    avg_loss:0.880, val_acc:0.830]
Epoch [27/120    avg_loss:0.777, val_acc:0.811]
Epoch [28/120    avg_loss:0.635, val_acc:0.848]
Epoch [29/120    avg_loss:0.564, val_acc:0.856]
Epoch [30/120    avg_loss:0.567, val_acc:0.871]
Epoch [31/120    avg_loss:0.564, val_acc:0.853]
Epoch [32/120    avg_loss:0.449, val_acc:0.859]
Epoch [33/120    avg_loss:0.377, val_acc:0.906]
Epoch [34/120    avg_loss:0.348, val_acc:0.852]
Epoch [35/120    avg_loss:0.378, val_acc:0.917]
Epoch [36/120    avg_loss:0.301, val_acc:0.932]
Epoch [37/120    avg_loss:0.271, val_acc:0.908]
Epoch [38/120    avg_loss:0.289, val_acc:0.890]
Epoch [39/120    avg_loss:0.235, val_acc:0.924]
Epoch [40/120    avg_loss:0.182, val_acc:0.915]
Epoch [41/120    avg_loss:0.197, val_acc:0.922]
Epoch [42/120    avg_loss:0.173, val_acc:0.924]
Epoch [43/120    avg_loss:0.186, val_acc:0.940]
Epoch [44/120    avg_loss:0.168, val_acc:0.941]
Epoch [45/120    avg_loss:0.126, val_acc:0.952]
Epoch [46/120    avg_loss:0.117, val_acc:0.951]
Epoch [47/120    avg_loss:0.120, val_acc:0.959]
Epoch [48/120    avg_loss:0.103, val_acc:0.953]
Epoch [49/120    avg_loss:0.097, val_acc:0.955]
Epoch [50/120    avg_loss:0.086, val_acc:0.957]
Epoch [51/120    avg_loss:0.085, val_acc:0.959]
Epoch [52/120    avg_loss:0.122, val_acc:0.959]
Epoch [53/120    avg_loss:0.105, val_acc:0.948]
Epoch [54/120    avg_loss:0.094, val_acc:0.935]
Epoch [55/120    avg_loss:0.106, val_acc:0.955]
Epoch [56/120    avg_loss:0.093, val_acc:0.960]
Epoch [57/120    avg_loss:0.067, val_acc:0.965]
Epoch [58/120    avg_loss:0.086, val_acc:0.952]
Epoch [59/120    avg_loss:0.072, val_acc:0.963]
Epoch [60/120    avg_loss:0.053, val_acc:0.972]
Epoch [61/120    avg_loss:0.051, val_acc:0.964]
Epoch [62/120    avg_loss:0.063, val_acc:0.954]
Epoch [63/120    avg_loss:0.051, val_acc:0.968]
Epoch [64/120    avg_loss:0.044, val_acc:0.975]
Epoch [65/120    avg_loss:0.037, val_acc:0.976]
Epoch [66/120    avg_loss:0.044, val_acc:0.963]
Epoch [67/120    avg_loss:0.042, val_acc:0.969]
Epoch [68/120    avg_loss:0.040, val_acc:0.975]
Epoch [69/120    avg_loss:0.038, val_acc:0.975]
Epoch [70/120    avg_loss:0.032, val_acc:0.979]
Epoch [71/120    avg_loss:0.037, val_acc:0.981]
Epoch [72/120    avg_loss:0.044, val_acc:0.945]
Epoch [73/120    avg_loss:0.053, val_acc:0.970]
Epoch [74/120    avg_loss:0.029, val_acc:0.974]
Epoch [75/120    avg_loss:0.032, val_acc:0.971]
Epoch [76/120    avg_loss:0.038, val_acc:0.976]
Epoch [77/120    avg_loss:0.026, val_acc:0.975]
Epoch [78/120    avg_loss:0.027, val_acc:0.975]
Epoch [79/120    avg_loss:0.043, val_acc:0.957]
Epoch [80/120    avg_loss:0.036, val_acc:0.974]
Epoch [81/120    avg_loss:0.026, val_acc:0.979]
Epoch [82/120    avg_loss:0.021, val_acc:0.982]
Epoch [83/120    avg_loss:0.018, val_acc:0.978]
Epoch [84/120    avg_loss:0.030, val_acc:0.978]
Epoch [85/120    avg_loss:0.029, val_acc:0.968]
Epoch [86/120    avg_loss:0.027, val_acc:0.980]
Epoch [87/120    avg_loss:0.024, val_acc:0.983]
Epoch [88/120    avg_loss:0.021, val_acc:0.974]
Epoch [89/120    avg_loss:0.018, val_acc:0.981]
Epoch [90/120    avg_loss:0.018, val_acc:0.980]
Epoch [91/120    avg_loss:0.017, val_acc:0.985]
Epoch [92/120    avg_loss:0.018, val_acc:0.978]
Epoch [93/120    avg_loss:0.014, val_acc:0.976]
Epoch [94/120    avg_loss:0.012, val_acc:0.982]
Epoch [95/120    avg_loss:0.015, val_acc:0.984]
Epoch [96/120    avg_loss:0.013, val_acc:0.982]
Epoch [97/120    avg_loss:0.011, val_acc:0.986]
Epoch [98/120    avg_loss:0.012, val_acc:0.981]
Epoch [99/120    avg_loss:0.013, val_acc:0.983]
Epoch [100/120    avg_loss:0.010, val_acc:0.984]
Epoch [101/120    avg_loss:0.010, val_acc:0.984]
Epoch [102/120    avg_loss:0.009, val_acc:0.985]
Epoch [103/120    avg_loss:0.010, val_acc:0.983]
Epoch [104/120    avg_loss:0.012, val_acc:0.981]
Epoch [105/120    avg_loss:0.014, val_acc:0.980]
Epoch [106/120    avg_loss:0.015, val_acc:0.978]
Epoch [107/120    avg_loss:0.012, val_acc:0.986]
Epoch [108/120    avg_loss:0.010, val_acc:0.982]
Epoch [109/120    avg_loss:0.018, val_acc:0.979]
Epoch [110/120    avg_loss:0.018, val_acc:0.972]
Epoch [111/120    avg_loss:0.017, val_acc:0.982]
Epoch [112/120    avg_loss:0.019, val_acc:0.977]
Epoch [113/120    avg_loss:0.011, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.984]
Epoch [115/120    avg_loss:0.010, val_acc:0.984]
Epoch [116/120    avg_loss:0.012, val_acc:0.980]
Epoch [117/120    avg_loss:0.013, val_acc:0.984]
Epoch [118/120    avg_loss:0.009, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.009, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1233   12    9    0    0    0    0    0   10   21    0    0
     0    0    0]
 [   0    0    6  730    2    0    0    0    0    1    0    7    1    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    1    0    0    0   16    0    1    0    0
     0    0    0]
 [   0    0    0    1    0    0    1    0    0    0  858   12    1    0
     2    0    0]
 [   0    0   17    6    0    0    0    1    3    0   11 2158   13    0
     0    0    1]
 [   0    0    1    6    0    0    0    0    0    0    4    1  520    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1135    4    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    67  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.4959349593496

F1 scores:
[       nan 0.975      0.97010228 0.97139055 0.97247706 0.99654776
 0.99848024 0.98039216 0.99652375 0.91428571 0.975      0.97846293
 0.97287184 1.         0.96843003 0.88431062 0.98823529]

Kappa:
0.9714445593703118
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd517fabf28>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.823, val_acc:0.109]
Epoch [2/120    avg_loss:2.771, val_acc:0.172]
Epoch [3/120    avg_loss:2.719, val_acc:0.175]
Epoch [4/120    avg_loss:2.651, val_acc:0.202]
Epoch [5/120    avg_loss:2.612, val_acc:0.252]
Epoch [6/120    avg_loss:2.524, val_acc:0.367]
Epoch [7/120    avg_loss:2.452, val_acc:0.414]
Epoch [8/120    avg_loss:2.382, val_acc:0.446]
Epoch [9/120    avg_loss:2.347, val_acc:0.432]
Epoch [10/120    avg_loss:2.261, val_acc:0.461]
Epoch [11/120    avg_loss:2.219, val_acc:0.489]
Epoch [12/120    avg_loss:2.152, val_acc:0.508]
Epoch [13/120    avg_loss:2.087, val_acc:0.535]
Epoch [14/120    avg_loss:2.010, val_acc:0.547]
Epoch [15/120    avg_loss:1.931, val_acc:0.584]
Epoch [16/120    avg_loss:1.799, val_acc:0.651]
Epoch [17/120    avg_loss:1.737, val_acc:0.590]
Epoch [18/120    avg_loss:1.627, val_acc:0.661]
Epoch [19/120    avg_loss:1.473, val_acc:0.686]
Epoch [20/120    avg_loss:1.307, val_acc:0.697]
Epoch [21/120    avg_loss:1.207, val_acc:0.729]
Epoch [22/120    avg_loss:1.071, val_acc:0.753]
Epoch [23/120    avg_loss:1.048, val_acc:0.721]
Epoch [24/120    avg_loss:0.938, val_acc:0.792]
Epoch [25/120    avg_loss:0.856, val_acc:0.752]
Epoch [26/120    avg_loss:0.765, val_acc:0.805]
Epoch [27/120    avg_loss:0.679, val_acc:0.801]
Epoch [28/120    avg_loss:0.684, val_acc:0.787]
Epoch [29/120    avg_loss:0.610, val_acc:0.854]
Epoch [30/120    avg_loss:0.519, val_acc:0.855]
Epoch [31/120    avg_loss:0.471, val_acc:0.874]
Epoch [32/120    avg_loss:0.460, val_acc:0.865]
Epoch [33/120    avg_loss:0.483, val_acc:0.801]
Epoch [34/120    avg_loss:0.479, val_acc:0.844]
Epoch [35/120    avg_loss:0.379, val_acc:0.892]
Epoch [36/120    avg_loss:0.319, val_acc:0.918]
Epoch [37/120    avg_loss:0.308, val_acc:0.853]
Epoch [38/120    avg_loss:0.265, val_acc:0.912]
Epoch [39/120    avg_loss:0.230, val_acc:0.881]
Epoch [40/120    avg_loss:0.213, val_acc:0.928]
Epoch [41/120    avg_loss:0.208, val_acc:0.932]
Epoch [42/120    avg_loss:0.163, val_acc:0.938]
Epoch [43/120    avg_loss:0.180, val_acc:0.917]
Epoch [44/120    avg_loss:0.147, val_acc:0.953]
Epoch [45/120    avg_loss:0.128, val_acc:0.956]
Epoch [46/120    avg_loss:0.122, val_acc:0.941]
Epoch [47/120    avg_loss:0.115, val_acc:0.964]
Epoch [48/120    avg_loss:0.099, val_acc:0.970]
Epoch [49/120    avg_loss:0.103, val_acc:0.939]
Epoch [50/120    avg_loss:0.095, val_acc:0.952]
Epoch [51/120    avg_loss:0.095, val_acc:0.953]
Epoch [52/120    avg_loss:0.093, val_acc:0.955]
Epoch [53/120    avg_loss:0.074, val_acc:0.970]
Epoch [54/120    avg_loss:0.078, val_acc:0.968]
Epoch [55/120    avg_loss:0.077, val_acc:0.953]
Epoch [56/120    avg_loss:0.091, val_acc:0.974]
Epoch [57/120    avg_loss:0.059, val_acc:0.971]
Epoch [58/120    avg_loss:0.072, val_acc:0.970]
Epoch [59/120    avg_loss:0.058, val_acc:0.961]
Epoch [60/120    avg_loss:0.063, val_acc:0.963]
Epoch [61/120    avg_loss:0.081, val_acc:0.959]
Epoch [62/120    avg_loss:0.070, val_acc:0.976]
Epoch [63/120    avg_loss:0.047, val_acc:0.973]
Epoch [64/120    avg_loss:0.050, val_acc:0.971]
Epoch [65/120    avg_loss:0.041, val_acc:0.975]
Epoch [66/120    avg_loss:0.045, val_acc:0.974]
Epoch [67/120    avg_loss:0.035, val_acc:0.984]
Epoch [68/120    avg_loss:0.030, val_acc:0.973]
Epoch [69/120    avg_loss:0.031, val_acc:0.978]
Epoch [70/120    avg_loss:0.029, val_acc:0.978]
Epoch [71/120    avg_loss:0.024, val_acc:0.980]
Epoch [72/120    avg_loss:0.025, val_acc:0.978]
Epoch [73/120    avg_loss:0.025, val_acc:0.978]
Epoch [74/120    avg_loss:0.028, val_acc:0.982]
Epoch [75/120    avg_loss:0.028, val_acc:0.980]
Epoch [76/120    avg_loss:0.023, val_acc:0.982]
Epoch [77/120    avg_loss:0.026, val_acc:0.979]
Epoch [78/120    avg_loss:0.024, val_acc:0.979]
Epoch [79/120    avg_loss:0.029, val_acc:0.982]
Epoch [80/120    avg_loss:0.024, val_acc:0.979]
Epoch [81/120    avg_loss:0.019, val_acc:0.980]
Epoch [82/120    avg_loss:0.019, val_acc:0.980]
Epoch [83/120    avg_loss:0.019, val_acc:0.983]
Epoch [84/120    avg_loss:0.018, val_acc:0.984]
Epoch [85/120    avg_loss:0.017, val_acc:0.982]
Epoch [86/120    avg_loss:0.019, val_acc:0.985]
Epoch [87/120    avg_loss:0.018, val_acc:0.984]
Epoch [88/120    avg_loss:0.017, val_acc:0.984]
Epoch [89/120    avg_loss:0.017, val_acc:0.983]
Epoch [90/120    avg_loss:0.018, val_acc:0.983]
Epoch [91/120    avg_loss:0.016, val_acc:0.982]
Epoch [92/120    avg_loss:0.017, val_acc:0.984]
Epoch [93/120    avg_loss:0.023, val_acc:0.983]
Epoch [94/120    avg_loss:0.018, val_acc:0.983]
Epoch [95/120    avg_loss:0.015, val_acc:0.983]
Epoch [96/120    avg_loss:0.015, val_acc:0.984]
Epoch [97/120    avg_loss:0.018, val_acc:0.983]
Epoch [98/120    avg_loss:0.016, val_acc:0.983]
Epoch [99/120    avg_loss:0.017, val_acc:0.981]
Epoch [100/120    avg_loss:0.017, val_acc:0.982]
Epoch [101/120    avg_loss:0.015, val_acc:0.984]
Epoch [102/120    avg_loss:0.015, val_acc:0.984]
Epoch [103/120    avg_loss:0.015, val_acc:0.984]
Epoch [104/120    avg_loss:0.015, val_acc:0.984]
Epoch [105/120    avg_loss:0.017, val_acc:0.984]
Epoch [106/120    avg_loss:0.016, val_acc:0.985]
Epoch [107/120    avg_loss:0.016, val_acc:0.985]
Epoch [108/120    avg_loss:0.016, val_acc:0.985]
Epoch [109/120    avg_loss:0.018, val_acc:0.985]
Epoch [110/120    avg_loss:0.015, val_acc:0.985]
Epoch [111/120    avg_loss:0.015, val_acc:0.985]
Epoch [112/120    avg_loss:0.017, val_acc:0.985]
Epoch [113/120    avg_loss:0.015, val_acc:0.985]
Epoch [114/120    avg_loss:0.015, val_acc:0.985]
Epoch [115/120    avg_loss:0.018, val_acc:0.985]
Epoch [116/120    avg_loss:0.014, val_acc:0.985]
Epoch [117/120    avg_loss:0.015, val_acc:0.985]
Epoch [118/120    avg_loss:0.017, val_acc:0.985]
Epoch [119/120    avg_loss:0.016, val_acc:0.985]
Epoch [120/120    avg_loss:0.015, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0 1253    0   12    0    0    0    0    0    1   18    1    0
     0    0    0]
 [   0    0    0  726    8    0    0    0    0    1    1   10    1    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  850   16    3    0
     0    0    0]
 [   0    0   12    0    0    0    2    0    0    0   10 2169   17    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0    0    2    1  524    0
     0    2    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1127   12    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    51  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.73441734417344

F1 scores:
[       nan 0.98765432 0.98043818 0.98174442 0.95280899 0.99421965
 0.99240122 1.         1.         0.94444444 0.97701149 0.97901151
 0.97037037 0.99728997 0.97113313 0.89433384 1.        ]

Kappa:
0.9741597480222108
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:10:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb38803eef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.791, val_acc:0.229]
Epoch [2/120    avg_loss:2.735, val_acc:0.242]
Epoch [3/120    avg_loss:2.667, val_acc:0.290]
Epoch [4/120    avg_loss:2.589, val_acc:0.365]
Epoch [5/120    avg_loss:2.516, val_acc:0.389]
Epoch [6/120    avg_loss:2.476, val_acc:0.411]
Epoch [7/120    avg_loss:2.417, val_acc:0.441]
Epoch [8/120    avg_loss:2.373, val_acc:0.454]
Epoch [9/120    avg_loss:2.316, val_acc:0.467]
Epoch [10/120    avg_loss:2.249, val_acc:0.480]
Epoch [11/120    avg_loss:2.202, val_acc:0.479]
Epoch [12/120    avg_loss:2.115, val_acc:0.516]
Epoch [13/120    avg_loss:2.080, val_acc:0.509]
Epoch [14/120    avg_loss:2.048, val_acc:0.559]
Epoch [15/120    avg_loss:1.955, val_acc:0.611]
Epoch [16/120    avg_loss:1.848, val_acc:0.640]
Epoch [17/120    avg_loss:1.766, val_acc:0.668]
Epoch [18/120    avg_loss:1.669, val_acc:0.660]
Epoch [19/120    avg_loss:1.524, val_acc:0.679]
Epoch [20/120    avg_loss:1.380, val_acc:0.730]
Epoch [21/120    avg_loss:1.257, val_acc:0.752]
Epoch [22/120    avg_loss:1.127, val_acc:0.760]
Epoch [23/120    avg_loss:1.002, val_acc:0.791]
Epoch [24/120    avg_loss:0.914, val_acc:0.779]
Epoch [25/120    avg_loss:0.758, val_acc:0.814]
Epoch [26/120    avg_loss:0.723, val_acc:0.818]
Epoch [27/120    avg_loss:0.649, val_acc:0.832]
Epoch [28/120    avg_loss:0.554, val_acc:0.866]
Epoch [29/120    avg_loss:0.517, val_acc:0.867]
Epoch [30/120    avg_loss:0.507, val_acc:0.836]
Epoch [31/120    avg_loss:0.419, val_acc:0.866]
Epoch [32/120    avg_loss:0.345, val_acc:0.859]
Epoch [33/120    avg_loss:0.349, val_acc:0.886]
Epoch [34/120    avg_loss:0.363, val_acc:0.846]
Epoch [35/120    avg_loss:0.283, val_acc:0.902]
Epoch [36/120    avg_loss:0.235, val_acc:0.863]
Epoch [37/120    avg_loss:0.221, val_acc:0.893]
Epoch [38/120    avg_loss:0.196, val_acc:0.919]
Epoch [39/120    avg_loss:0.174, val_acc:0.910]
Epoch [40/120    avg_loss:0.165, val_acc:0.927]
Epoch [41/120    avg_loss:0.165, val_acc:0.942]
Epoch [42/120    avg_loss:0.174, val_acc:0.912]
Epoch [43/120    avg_loss:0.212, val_acc:0.920]
Epoch [44/120    avg_loss:0.201, val_acc:0.940]
Epoch [45/120    avg_loss:0.137, val_acc:0.932]
Epoch [46/120    avg_loss:0.117, val_acc:0.942]
Epoch [47/120    avg_loss:0.100, val_acc:0.944]
Epoch [48/120    avg_loss:0.091, val_acc:0.938]
Epoch [49/120    avg_loss:0.085, val_acc:0.950]
Epoch [50/120    avg_loss:0.090, val_acc:0.950]
Epoch [51/120    avg_loss:0.079, val_acc:0.949]
Epoch [52/120    avg_loss:0.092, val_acc:0.949]
Epoch [53/120    avg_loss:0.114, val_acc:0.951]
Epoch [54/120    avg_loss:0.089, val_acc:0.956]
Epoch [55/120    avg_loss:0.081, val_acc:0.958]
Epoch [56/120    avg_loss:0.066, val_acc:0.963]
Epoch [57/120    avg_loss:0.062, val_acc:0.963]
Epoch [58/120    avg_loss:0.052, val_acc:0.966]
Epoch [59/120    avg_loss:0.047, val_acc:0.932]
Epoch [60/120    avg_loss:0.057, val_acc:0.957]
Epoch [61/120    avg_loss:0.051, val_acc:0.960]
Epoch [62/120    avg_loss:0.054, val_acc:0.961]
Epoch [63/120    avg_loss:0.043, val_acc:0.965]
Epoch [64/120    avg_loss:0.050, val_acc:0.959]
Epoch [65/120    avg_loss:0.059, val_acc:0.967]
Epoch [66/120    avg_loss:0.043, val_acc:0.966]
Epoch [67/120    avg_loss:0.050, val_acc:0.954]
Epoch [68/120    avg_loss:0.041, val_acc:0.963]
Epoch [69/120    avg_loss:0.033, val_acc:0.971]
Epoch [70/120    avg_loss:0.027, val_acc:0.957]
Epoch [71/120    avg_loss:0.026, val_acc:0.959]
Epoch [72/120    avg_loss:0.032, val_acc:0.965]
Epoch [73/120    avg_loss:0.034, val_acc:0.966]
Epoch [74/120    avg_loss:0.027, val_acc:0.966]
Epoch [75/120    avg_loss:0.029, val_acc:0.965]
Epoch [76/120    avg_loss:0.038, val_acc:0.956]
Epoch [77/120    avg_loss:0.038, val_acc:0.965]
Epoch [78/120    avg_loss:0.025, val_acc:0.968]
Epoch [79/120    avg_loss:0.040, val_acc:0.966]
Epoch [80/120    avg_loss:0.028, val_acc:0.964]
Epoch [81/120    avg_loss:0.025, val_acc:0.968]
Epoch [82/120    avg_loss:0.028, val_acc:0.977]
Epoch [83/120    avg_loss:0.024, val_acc:0.967]
Epoch [84/120    avg_loss:0.021, val_acc:0.969]
Epoch [85/120    avg_loss:0.017, val_acc:0.973]
Epoch [86/120    avg_loss:0.016, val_acc:0.973]
Epoch [87/120    avg_loss:0.013, val_acc:0.974]
Epoch [88/120    avg_loss:0.014, val_acc:0.971]
Epoch [89/120    avg_loss:0.017, val_acc:0.975]
Epoch [90/120    avg_loss:0.014, val_acc:0.975]
Epoch [91/120    avg_loss:0.017, val_acc:0.969]
Epoch [92/120    avg_loss:0.031, val_acc:0.969]
Epoch [93/120    avg_loss:0.150, val_acc:0.928]
Epoch [94/120    avg_loss:0.100, val_acc:0.959]
Epoch [95/120    avg_loss:0.041, val_acc:0.969]
Epoch [96/120    avg_loss:0.029, val_acc:0.968]
Epoch [97/120    avg_loss:0.026, val_acc:0.969]
Epoch [98/120    avg_loss:0.022, val_acc:0.970]
Epoch [99/120    avg_loss:0.024, val_acc:0.971]
Epoch [100/120    avg_loss:0.026, val_acc:0.973]
Epoch [101/120    avg_loss:0.020, val_acc:0.974]
Epoch [102/120    avg_loss:0.018, val_acc:0.974]
Epoch [103/120    avg_loss:0.017, val_acc:0.974]
Epoch [104/120    avg_loss:0.017, val_acc:0.972]
Epoch [105/120    avg_loss:0.016, val_acc:0.973]
Epoch [106/120    avg_loss:0.016, val_acc:0.973]
Epoch [107/120    avg_loss:0.016, val_acc:0.973]
Epoch [108/120    avg_loss:0.015, val_acc:0.974]
Epoch [109/120    avg_loss:0.017, val_acc:0.973]
Epoch [110/120    avg_loss:0.016, val_acc:0.973]
Epoch [111/120    avg_loss:0.016, val_acc:0.972]
Epoch [112/120    avg_loss:0.014, val_acc:0.972]
Epoch [113/120    avg_loss:0.014, val_acc:0.971]
Epoch [114/120    avg_loss:0.016, val_acc:0.973]
Epoch [115/120    avg_loss:0.015, val_acc:0.973]
Epoch [116/120    avg_loss:0.015, val_acc:0.973]
Epoch [117/120    avg_loss:0.014, val_acc:0.973]
Epoch [118/120    avg_loss:0.017, val_acc:0.973]
Epoch [119/120    avg_loss:0.019, val_acc:0.973]
Epoch [120/120    avg_loss:0.018, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    0    0    0    0    0    0    0    0    6    0    0    0
     0    0    0]
 [   0    0 1240    4    3    1    0    0    0    0    8   29    0    0
     0    0    0]
 [   0    0    0  711    6    1    0    0    0    3    5   15    5    0
     1    0    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    3    0    0
     0    0    0]
 [   0    0    1    0    0    0    1    0    0    0  850   20    3    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0    9 2175   19    0
     1    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    6  525    0
     1    2    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1127   12    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
    86  260    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.13821138211382

F1 scores:
[       nan 0.92105263 0.97946288 0.97197539 0.97459584 0.99541284
 0.99771516 1.         1.         0.83333333 0.96976612 0.97555506
 0.96507353 1.         0.95630038 0.8373591  0.99401198]

Kappa:
0.9673320769628044
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2d46f00ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.826, val_acc:0.087]
Epoch [2/120    avg_loss:2.785, val_acc:0.086]
Epoch [3/120    avg_loss:2.744, val_acc:0.104]
Epoch [4/120    avg_loss:2.706, val_acc:0.174]
Epoch [5/120    avg_loss:2.647, val_acc:0.260]
Epoch [6/120    avg_loss:2.598, val_acc:0.323]
Epoch [7/120    avg_loss:2.543, val_acc:0.403]
Epoch [8/120    avg_loss:2.474, val_acc:0.457]
Epoch [9/120    avg_loss:2.439, val_acc:0.496]
Epoch [10/120    avg_loss:2.357, val_acc:0.519]
Epoch [11/120    avg_loss:2.296, val_acc:0.575]
Epoch [12/120    avg_loss:2.216, val_acc:0.574]
Epoch [13/120    avg_loss:2.126, val_acc:0.589]
Epoch [14/120    avg_loss:2.075, val_acc:0.596]
Epoch [15/120    avg_loss:1.986, val_acc:0.611]
Epoch [16/120    avg_loss:1.889, val_acc:0.658]
Epoch [17/120    avg_loss:1.824, val_acc:0.607]
Epoch [18/120    avg_loss:1.697, val_acc:0.597]
Epoch [19/120    avg_loss:1.557, val_acc:0.646]
Epoch [20/120    avg_loss:1.461, val_acc:0.648]
Epoch [21/120    avg_loss:1.356, val_acc:0.643]
Epoch [22/120    avg_loss:1.260, val_acc:0.700]
Epoch [23/120    avg_loss:1.073, val_acc:0.748]
Epoch [24/120    avg_loss:1.045, val_acc:0.745]
Epoch [25/120    avg_loss:0.893, val_acc:0.772]
Epoch [26/120    avg_loss:0.815, val_acc:0.775]
Epoch [27/120    avg_loss:0.749, val_acc:0.818]
Epoch [28/120    avg_loss:0.731, val_acc:0.802]
Epoch [29/120    avg_loss:0.637, val_acc:0.814]
Epoch [30/120    avg_loss:0.521, val_acc:0.815]
Epoch [31/120    avg_loss:0.488, val_acc:0.856]
Epoch [32/120    avg_loss:0.438, val_acc:0.818]
Epoch [33/120    avg_loss:0.398, val_acc:0.824]
Epoch [34/120    avg_loss:0.385, val_acc:0.851]
Epoch [35/120    avg_loss:0.354, val_acc:0.863]
Epoch [36/120    avg_loss:0.307, val_acc:0.851]
Epoch [37/120    avg_loss:0.270, val_acc:0.884]
Epoch [38/120    avg_loss:0.254, val_acc:0.902]
Epoch [39/120    avg_loss:0.239, val_acc:0.911]
Epoch [40/120    avg_loss:0.200, val_acc:0.911]
Epoch [41/120    avg_loss:0.192, val_acc:0.909]
Epoch [42/120    avg_loss:0.171, val_acc:0.907]
Epoch [43/120    avg_loss:0.162, val_acc:0.921]
Epoch [44/120    avg_loss:0.189, val_acc:0.914]
Epoch [45/120    avg_loss:0.123, val_acc:0.941]
Epoch [46/120    avg_loss:0.131, val_acc:0.930]
Epoch [47/120    avg_loss:0.121, val_acc:0.938]
Epoch [48/120    avg_loss:0.113, val_acc:0.939]
Epoch [49/120    avg_loss:0.141, val_acc:0.933]
Epoch [50/120    avg_loss:0.107, val_acc:0.933]
Epoch [51/120    avg_loss:0.111, val_acc:0.938]
Epoch [52/120    avg_loss:0.095, val_acc:0.946]
Epoch [53/120    avg_loss:0.082, val_acc:0.916]
Epoch [54/120    avg_loss:0.084, val_acc:0.949]
Epoch [55/120    avg_loss:0.073, val_acc:0.957]
Epoch [56/120    avg_loss:0.072, val_acc:0.951]
Epoch [57/120    avg_loss:0.073, val_acc:0.958]
Epoch [58/120    avg_loss:0.065, val_acc:0.955]
Epoch [59/120    avg_loss:0.062, val_acc:0.953]
Epoch [60/120    avg_loss:0.063, val_acc:0.957]
Epoch [61/120    avg_loss:0.051, val_acc:0.961]
Epoch [62/120    avg_loss:0.044, val_acc:0.957]
Epoch [63/120    avg_loss:0.045, val_acc:0.964]
Epoch [64/120    avg_loss:0.042, val_acc:0.963]
Epoch [65/120    avg_loss:0.062, val_acc:0.940]
Epoch [66/120    avg_loss:0.077, val_acc:0.931]
Epoch [67/120    avg_loss:0.068, val_acc:0.935]
Epoch [68/120    avg_loss:0.091, val_acc:0.955]
Epoch [69/120    avg_loss:0.052, val_acc:0.950]
Epoch [70/120    avg_loss:0.077, val_acc:0.952]
Epoch [71/120    avg_loss:0.059, val_acc:0.951]
Epoch [72/120    avg_loss:0.043, val_acc:0.957]
Epoch [73/120    avg_loss:0.043, val_acc:0.959]
Epoch [74/120    avg_loss:0.039, val_acc:0.954]
Epoch [75/120    avg_loss:0.036, val_acc:0.969]
Epoch [76/120    avg_loss:0.030, val_acc:0.960]
Epoch [77/120    avg_loss:0.027, val_acc:0.966]
Epoch [78/120    avg_loss:0.033, val_acc:0.957]
Epoch [79/120    avg_loss:0.029, val_acc:0.966]
Epoch [80/120    avg_loss:0.028, val_acc:0.960]
Epoch [81/120    avg_loss:0.024, val_acc:0.964]
Epoch [82/120    avg_loss:0.032, val_acc:0.960]
Epoch [83/120    avg_loss:0.031, val_acc:0.966]
Epoch [84/120    avg_loss:0.024, val_acc:0.964]
Epoch [85/120    avg_loss:0.017, val_acc:0.967]
Epoch [86/120    avg_loss:0.018, val_acc:0.964]
Epoch [87/120    avg_loss:0.020, val_acc:0.963]
Epoch [88/120    avg_loss:0.046, val_acc:0.959]
Epoch [89/120    avg_loss:0.026, val_acc:0.965]
Epoch [90/120    avg_loss:0.019, val_acc:0.965]
Epoch [91/120    avg_loss:0.019, val_acc:0.967]
Epoch [92/120    avg_loss:0.016, val_acc:0.967]
Epoch [93/120    avg_loss:0.017, val_acc:0.968]
Epoch [94/120    avg_loss:0.016, val_acc:0.968]
Epoch [95/120    avg_loss:0.015, val_acc:0.969]
Epoch [96/120    avg_loss:0.016, val_acc:0.968]
Epoch [97/120    avg_loss:0.014, val_acc:0.969]
Epoch [98/120    avg_loss:0.015, val_acc:0.970]
Epoch [99/120    avg_loss:0.015, val_acc:0.968]
Epoch [100/120    avg_loss:0.014, val_acc:0.970]
Epoch [101/120    avg_loss:0.014, val_acc:0.969]
Epoch [102/120    avg_loss:0.015, val_acc:0.966]
Epoch [103/120    avg_loss:0.016, val_acc:0.967]
Epoch [104/120    avg_loss:0.016, val_acc:0.972]
Epoch [105/120    avg_loss:0.018, val_acc:0.970]
Epoch [106/120    avg_loss:0.016, val_acc:0.970]
Epoch [107/120    avg_loss:0.016, val_acc:0.971]
Epoch [108/120    avg_loss:0.014, val_acc:0.971]
Epoch [109/120    avg_loss:0.015, val_acc:0.971]
Epoch [110/120    avg_loss:0.013, val_acc:0.969]
Epoch [111/120    avg_loss:0.014, val_acc:0.972]
Epoch [112/120    avg_loss:0.014, val_acc:0.970]
Epoch [113/120    avg_loss:0.012, val_acc:0.969]
Epoch [114/120    avg_loss:0.015, val_acc:0.972]
Epoch [115/120    avg_loss:0.014, val_acc:0.971]
Epoch [116/120    avg_loss:0.013, val_acc:0.972]
Epoch [117/120    avg_loss:0.013, val_acc:0.974]
Epoch [118/120    avg_loss:0.013, val_acc:0.973]
Epoch [119/120    avg_loss:0.015, val_acc:0.970]
Epoch [120/120    avg_loss:0.012, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1257    2    2    0    0    0    0    0    2   22    0    0
     0    0    0]
 [   0    0    0  727    1    0    0    0    0    3    1    9    6    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    1    0    0    1    0    0    0    0  844   27    0    2
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    0    8 2179   16    0
     0    0    0]
 [   0    0    0    1    2    2    0    0    0    0    4    0  522    0
     2    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    54  284    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.71273712737127

F1 scores:
[       nan 0.96202532 0.98588235 0.98376184 0.98604651 0.99078341
 0.99319728 1.         1.         0.89473684 0.97179044 0.97976619
 0.96487985 0.99462366 0.96860215 0.88198758 0.97590361]

Kappa:
0.9739012660616205
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4b66beeef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.828, val_acc:0.070]
Epoch [2/120    avg_loss:2.775, val_acc:0.093]
Epoch [3/120    avg_loss:2.720, val_acc:0.256]
Epoch [4/120    avg_loss:2.652, val_acc:0.293]
Epoch [5/120    avg_loss:2.594, val_acc:0.316]
Epoch [6/120    avg_loss:2.521, val_acc:0.366]
Epoch [7/120    avg_loss:2.483, val_acc:0.384]
Epoch [8/120    avg_loss:2.406, val_acc:0.438]
Epoch [9/120    avg_loss:2.319, val_acc:0.472]
Epoch [10/120    avg_loss:2.220, val_acc:0.497]
Epoch [11/120    avg_loss:2.193, val_acc:0.452]
Epoch [12/120    avg_loss:2.071, val_acc:0.530]
Epoch [13/120    avg_loss:1.971, val_acc:0.570]
Epoch [14/120    avg_loss:1.898, val_acc:0.535]
Epoch [15/120    avg_loss:1.820, val_acc:0.584]
Epoch [16/120    avg_loss:1.698, val_acc:0.599]
Epoch [17/120    avg_loss:1.579, val_acc:0.636]
Epoch [18/120    avg_loss:1.550, val_acc:0.617]
Epoch [19/120    avg_loss:1.448, val_acc:0.642]
Epoch [20/120    avg_loss:1.308, val_acc:0.702]
Epoch [21/120    avg_loss:1.192, val_acc:0.676]
Epoch [22/120    avg_loss:1.130, val_acc:0.710]
Epoch [23/120    avg_loss:1.075, val_acc:0.683]
Epoch [24/120    avg_loss:1.100, val_acc:0.686]
Epoch [25/120    avg_loss:0.944, val_acc:0.726]
Epoch [26/120    avg_loss:0.792, val_acc:0.762]
Epoch [27/120    avg_loss:0.689, val_acc:0.793]
Epoch [28/120    avg_loss:0.623, val_acc:0.793]
Epoch [29/120    avg_loss:0.574, val_acc:0.851]
Epoch [30/120    avg_loss:0.514, val_acc:0.846]
Epoch [31/120    avg_loss:0.472, val_acc:0.844]
Epoch [32/120    avg_loss:0.458, val_acc:0.864]
Epoch [33/120    avg_loss:0.388, val_acc:0.874]
Epoch [34/120    avg_loss:0.362, val_acc:0.892]
Epoch [35/120    avg_loss:0.325, val_acc:0.910]
Epoch [36/120    avg_loss:0.312, val_acc:0.883]
Epoch [37/120    avg_loss:0.285, val_acc:0.890]
Epoch [38/120    avg_loss:0.277, val_acc:0.901]
Epoch [39/120    avg_loss:0.213, val_acc:0.918]
Epoch [40/120    avg_loss:0.326, val_acc:0.885]
Epoch [41/120    avg_loss:0.235, val_acc:0.919]
Epoch [42/120    avg_loss:0.187, val_acc:0.941]
Epoch [43/120    avg_loss:0.191, val_acc:0.930]
Epoch [44/120    avg_loss:0.193, val_acc:0.927]
Epoch [45/120    avg_loss:0.171, val_acc:0.926]
Epoch [46/120    avg_loss:0.157, val_acc:0.941]
Epoch [47/120    avg_loss:0.189, val_acc:0.899]
Epoch [48/120    avg_loss:0.175, val_acc:0.925]
Epoch [49/120    avg_loss:0.161, val_acc:0.940]
Epoch [50/120    avg_loss:0.140, val_acc:0.945]
Epoch [51/120    avg_loss:0.110, val_acc:0.952]
Epoch [52/120    avg_loss:0.106, val_acc:0.952]
Epoch [53/120    avg_loss:0.122, val_acc:0.943]
Epoch [54/120    avg_loss:0.116, val_acc:0.953]
Epoch [55/120    avg_loss:0.084, val_acc:0.957]
Epoch [56/120    avg_loss:0.083, val_acc:0.966]
Epoch [57/120    avg_loss:0.073, val_acc:0.959]
Epoch [58/120    avg_loss:0.063, val_acc:0.958]
Epoch [59/120    avg_loss:0.068, val_acc:0.954]
Epoch [60/120    avg_loss:0.082, val_acc:0.945]
Epoch [61/120    avg_loss:0.074, val_acc:0.968]
Epoch [62/120    avg_loss:0.062, val_acc:0.969]
Epoch [63/120    avg_loss:0.064, val_acc:0.947]
Epoch [64/120    avg_loss:0.053, val_acc:0.959]
Epoch [65/120    avg_loss:0.048, val_acc:0.966]
Epoch [66/120    avg_loss:0.050, val_acc:0.973]
Epoch [67/120    avg_loss:0.045, val_acc:0.952]
Epoch [68/120    avg_loss:0.050, val_acc:0.967]
Epoch [69/120    avg_loss:0.054, val_acc:0.956]
Epoch [70/120    avg_loss:0.105, val_acc:0.953]
Epoch [71/120    avg_loss:0.135, val_acc:0.915]
Epoch [72/120    avg_loss:0.094, val_acc:0.955]
Epoch [73/120    avg_loss:0.070, val_acc:0.933]
Epoch [74/120    avg_loss:0.074, val_acc:0.958]
Epoch [75/120    avg_loss:0.065, val_acc:0.961]
Epoch [76/120    avg_loss:0.050, val_acc:0.960]
Epoch [77/120    avg_loss:0.053, val_acc:0.971]
Epoch [78/120    avg_loss:0.044, val_acc:0.961]
Epoch [79/120    avg_loss:0.034, val_acc:0.977]
Epoch [80/120    avg_loss:0.030, val_acc:0.975]
Epoch [81/120    avg_loss:0.032, val_acc:0.969]
Epoch [82/120    avg_loss:0.029, val_acc:0.971]
Epoch [83/120    avg_loss:0.043, val_acc:0.940]
Epoch [84/120    avg_loss:0.039, val_acc:0.970]
Epoch [85/120    avg_loss:0.030, val_acc:0.976]
Epoch [86/120    avg_loss:0.023, val_acc:0.975]
Epoch [87/120    avg_loss:0.019, val_acc:0.975]
Epoch [88/120    avg_loss:0.019, val_acc:0.971]
Epoch [89/120    avg_loss:0.022, val_acc:0.976]
Epoch [90/120    avg_loss:0.020, val_acc:0.981]
Epoch [91/120    avg_loss:0.017, val_acc:0.978]
Epoch [92/120    avg_loss:0.018, val_acc:0.977]
Epoch [93/120    avg_loss:0.016, val_acc:0.978]
Epoch [94/120    avg_loss:0.014, val_acc:0.980]
Epoch [95/120    avg_loss:0.015, val_acc:0.979]
Epoch [96/120    avg_loss:0.013, val_acc:0.976]
Epoch [97/120    avg_loss:0.014, val_acc:0.975]
Epoch [98/120    avg_loss:0.015, val_acc:0.978]
Epoch [99/120    avg_loss:0.014, val_acc:0.977]
Epoch [100/120    avg_loss:0.019, val_acc:0.980]
Epoch [101/120    avg_loss:0.017, val_acc:0.981]
Epoch [102/120    avg_loss:0.012, val_acc:0.977]
Epoch [103/120    avg_loss:0.014, val_acc:0.978]
Epoch [104/120    avg_loss:0.014, val_acc:0.979]
Epoch [105/120    avg_loss:0.013, val_acc:0.976]
Epoch [106/120    avg_loss:0.017, val_acc:0.975]
Epoch [107/120    avg_loss:0.014, val_acc:0.976]
Epoch [108/120    avg_loss:0.012, val_acc:0.976]
Epoch [109/120    avg_loss:0.015, val_acc:0.974]
Epoch [110/120    avg_loss:0.013, val_acc:0.976]
Epoch [111/120    avg_loss:0.015, val_acc:0.976]
Epoch [112/120    avg_loss:0.015, val_acc:0.977]
Epoch [113/120    avg_loss:0.021, val_acc:0.982]
Epoch [114/120    avg_loss:0.017, val_acc:0.972]
Epoch [115/120    avg_loss:0.013, val_acc:0.981]
Epoch [116/120    avg_loss:0.010, val_acc:0.981]
Epoch [117/120    avg_loss:0.014, val_acc:0.982]
Epoch [118/120    avg_loss:0.011, val_acc:0.980]
Epoch [119/120    avg_loss:0.009, val_acc:0.984]
Epoch [120/120    avg_loss:0.012, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1257    2    1    0    0    0    0    0    6   19    0    0
     0    0    0]
 [   0    0    1  725    3    0    0    0    0    3    4    9    1    1
     0    0    0]
 [   0    0    0    2  210    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  429    0    2    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   15    0    2    0    0
     0    0    0]
 [   0    0   10    0    0    2    0    0    0    0  830   30    2    0
     0    1    0]
 [   0    0   30    0    0    0    0    0    0    0    3 2156   21    0
     0    0    0]
 [   0    0    0    6    0    0    0    0    0    0    3   11  509    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0    0    0    0    0
  1136    2    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    56  291    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.23577235772358

F1 scores:
[       nan 1.         0.97291022 0.97840756 0.98360656 0.99076212
 0.99618612 0.96153846 0.9953271  0.83333333 0.96455549 0.97117117
 0.94785847 0.99730458 0.97260274 0.90372671 0.97619048]

Kappa:
0.9684536894730249
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa8f341bf28>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.838, val_acc:0.114]
Epoch [2/120    avg_loss:2.776, val_acc:0.189]
Epoch [3/120    avg_loss:2.734, val_acc:0.228]
Epoch [4/120    avg_loss:2.671, val_acc:0.235]
Epoch [5/120    avg_loss:2.647, val_acc:0.266]
Epoch [6/120    avg_loss:2.566, val_acc:0.289]
Epoch [7/120    avg_loss:2.525, val_acc:0.296]
Epoch [8/120    avg_loss:2.453, val_acc:0.310]
Epoch [9/120    avg_loss:2.411, val_acc:0.327]
Epoch [10/120    avg_loss:2.330, val_acc:0.327]
Epoch [11/120    avg_loss:2.279, val_acc:0.358]
Epoch [12/120    avg_loss:2.228, val_acc:0.384]
Epoch [13/120    avg_loss:2.132, val_acc:0.389]
Epoch [14/120    avg_loss:2.083, val_acc:0.431]
Epoch [15/120    avg_loss:1.948, val_acc:0.472]
Epoch [16/120    avg_loss:1.875, val_acc:0.476]
Epoch [17/120    avg_loss:1.728, val_acc:0.498]
Epoch [18/120    avg_loss:1.609, val_acc:0.506]
Epoch [19/120    avg_loss:1.526, val_acc:0.577]
Epoch [20/120    avg_loss:1.433, val_acc:0.613]
Epoch [21/120    avg_loss:1.342, val_acc:0.637]
Epoch [22/120    avg_loss:1.258, val_acc:0.680]
Epoch [23/120    avg_loss:1.181, val_acc:0.654]
Epoch [24/120    avg_loss:1.072, val_acc:0.724]
Epoch [25/120    avg_loss:0.952, val_acc:0.705]
Epoch [26/120    avg_loss:0.873, val_acc:0.750]
Epoch [27/120    avg_loss:0.876, val_acc:0.718]
Epoch [28/120    avg_loss:0.738, val_acc:0.790]
Epoch [29/120    avg_loss:0.751, val_acc:0.791]
Epoch [30/120    avg_loss:0.677, val_acc:0.798]
Epoch [31/120    avg_loss:0.654, val_acc:0.796]
Epoch [32/120    avg_loss:0.555, val_acc:0.792]
Epoch [33/120    avg_loss:0.494, val_acc:0.845]
Epoch [34/120    avg_loss:0.431, val_acc:0.844]
Epoch [35/120    avg_loss:0.396, val_acc:0.849]
Epoch [36/120    avg_loss:0.390, val_acc:0.870]
Epoch [37/120    avg_loss:0.340, val_acc:0.873]
Epoch [38/120    avg_loss:0.290, val_acc:0.880]
Epoch [39/120    avg_loss:0.278, val_acc:0.898]
Epoch [40/120    avg_loss:0.265, val_acc:0.904]
Epoch [41/120    avg_loss:0.236, val_acc:0.908]
Epoch [42/120    avg_loss:0.204, val_acc:0.919]
Epoch [43/120    avg_loss:0.192, val_acc:0.907]
Epoch [44/120    avg_loss:0.186, val_acc:0.932]
Epoch [45/120    avg_loss:0.148, val_acc:0.935]
Epoch [46/120    avg_loss:0.157, val_acc:0.934]
Epoch [47/120    avg_loss:0.164, val_acc:0.922]
Epoch [48/120    avg_loss:0.133, val_acc:0.946]
Epoch [49/120    avg_loss:0.138, val_acc:0.943]
Epoch [50/120    avg_loss:0.137, val_acc:0.928]
Epoch [51/120    avg_loss:0.115, val_acc:0.942]
Epoch [52/120    avg_loss:0.101, val_acc:0.956]
Epoch [53/120    avg_loss:0.099, val_acc:0.948]
Epoch [54/120    avg_loss:0.103, val_acc:0.938]
Epoch [55/120    avg_loss:0.103, val_acc:0.942]
Epoch [56/120    avg_loss:0.116, val_acc:0.936]
Epoch [57/120    avg_loss:0.097, val_acc:0.952]
Epoch [58/120    avg_loss:0.080, val_acc:0.949]
Epoch [59/120    avg_loss:0.102, val_acc:0.952]
Epoch [60/120    avg_loss:0.079, val_acc:0.940]
Epoch [61/120    avg_loss:0.080, val_acc:0.951]
Epoch [62/120    avg_loss:0.080, val_acc:0.966]
Epoch [63/120    avg_loss:0.069, val_acc:0.941]
Epoch [64/120    avg_loss:0.065, val_acc:0.953]
Epoch [65/120    avg_loss:0.068, val_acc:0.959]
Epoch [66/120    avg_loss:0.063, val_acc:0.959]
Epoch [67/120    avg_loss:0.053, val_acc:0.961]
Epoch [68/120    avg_loss:0.069, val_acc:0.963]
Epoch [69/120    avg_loss:0.063, val_acc:0.958]
Epoch [70/120    avg_loss:0.055, val_acc:0.961]
Epoch [71/120    avg_loss:0.053, val_acc:0.967]
Epoch [72/120    avg_loss:0.058, val_acc:0.961]
Epoch [73/120    avg_loss:0.052, val_acc:0.958]
Epoch [74/120    avg_loss:0.046, val_acc:0.970]
Epoch [75/120    avg_loss:0.047, val_acc:0.957]
Epoch [76/120    avg_loss:0.043, val_acc:0.971]
Epoch [77/120    avg_loss:0.036, val_acc:0.961]
Epoch [78/120    avg_loss:0.035, val_acc:0.974]
Epoch [79/120    avg_loss:0.025, val_acc:0.980]
Epoch [80/120    avg_loss:0.031, val_acc:0.972]
Epoch [81/120    avg_loss:0.028, val_acc:0.975]
Epoch [82/120    avg_loss:0.022, val_acc:0.978]
Epoch [83/120    avg_loss:0.023, val_acc:0.974]
Epoch [84/120    avg_loss:0.024, val_acc:0.969]
Epoch [85/120    avg_loss:0.021, val_acc:0.978]
Epoch [86/120    avg_loss:0.019, val_acc:0.979]
Epoch [87/120    avg_loss:0.021, val_acc:0.977]
Epoch [88/120    avg_loss:0.019, val_acc:0.979]
Epoch [89/120    avg_loss:0.016, val_acc:0.975]
Epoch [90/120    avg_loss:0.018, val_acc:0.978]
Epoch [91/120    avg_loss:0.017, val_acc:0.979]
Epoch [92/120    avg_loss:0.018, val_acc:0.975]
Epoch [93/120    avg_loss:0.018, val_acc:0.980]
Epoch [94/120    avg_loss:0.013, val_acc:0.981]
Epoch [95/120    avg_loss:0.014, val_acc:0.982]
Epoch [96/120    avg_loss:0.012, val_acc:0.982]
Epoch [97/120    avg_loss:0.013, val_acc:0.982]
Epoch [98/120    avg_loss:0.012, val_acc:0.982]
Epoch [99/120    avg_loss:0.015, val_acc:0.981]
Epoch [100/120    avg_loss:0.013, val_acc:0.982]
Epoch [101/120    avg_loss:0.012, val_acc:0.981]
Epoch [102/120    avg_loss:0.012, val_acc:0.983]
Epoch [103/120    avg_loss:0.012, val_acc:0.983]
Epoch [104/120    avg_loss:0.012, val_acc:0.984]
Epoch [105/120    avg_loss:0.014, val_acc:0.984]
Epoch [106/120    avg_loss:0.017, val_acc:0.980]
Epoch [107/120    avg_loss:0.013, val_acc:0.981]
Epoch [108/120    avg_loss:0.012, val_acc:0.981]
Epoch [109/120    avg_loss:0.013, val_acc:0.982]
Epoch [110/120    avg_loss:0.011, val_acc:0.982]
Epoch [111/120    avg_loss:0.013, val_acc:0.983]
Epoch [112/120    avg_loss:0.013, val_acc:0.982]
Epoch [113/120    avg_loss:0.011, val_acc:0.982]
Epoch [114/120    avg_loss:0.012, val_acc:0.983]
Epoch [115/120    avg_loss:0.012, val_acc:0.983]
Epoch [116/120    avg_loss:0.012, val_acc:0.982]
Epoch [117/120    avg_loss:0.012, val_acc:0.983]
Epoch [118/120    avg_loss:0.012, val_acc:0.983]
Epoch [119/120    avg_loss:0.013, val_acc:0.983]
Epoch [120/120    avg_loss:0.013, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    0    2    0    0
     0    0    0]
 [   0    0 1246    5    1    2    0    0    0    0    5   26    0    0
     0    0    0]
 [   0    0    2  733    3    0    0    0    0    2    2    3    2    0
     0    0    0]
 [   0    0    0    5  204    0    0    0    0    0    0    1    2    0
     0    1    0]
 [   0    0    0    0    0  429    0    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    0  842   25    0    0
     0    1    0]
 [   0    0   12    0    0    0    2    0    0    0    6 2177   12    1
     0    0    0]
 [   0    0    1    5    0    1    0    0    0    0    3    2  519    0
     2    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    0   29    0    0    0    0    0    0    0
    54  264    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.24661246612466

F1 scores:
[       nan 0.975      0.97610654 0.98060201 0.96912114 0.98961938
 0.97390007 1.         0.99883586 0.94736842 0.97172533 0.97798742
 0.96648045 0.99459459 0.96951481 0.84615385 0.97560976]

Kappa:
0.9685725440683508
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f639c6c8eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.811, val_acc:0.146]
Epoch [2/120    avg_loss:2.759, val_acc:0.131]
Epoch [3/120    avg_loss:2.702, val_acc:0.183]
Epoch [4/120    avg_loss:2.648, val_acc:0.222]
Epoch [5/120    avg_loss:2.588, val_acc:0.237]
Epoch [6/120    avg_loss:2.533, val_acc:0.318]
Epoch [7/120    avg_loss:2.493, val_acc:0.391]
Epoch [8/120    avg_loss:2.410, val_acc:0.432]
Epoch [9/120    avg_loss:2.358, val_acc:0.436]
Epoch [10/120    avg_loss:2.334, val_acc:0.465]
Epoch [11/120    avg_loss:2.246, val_acc:0.491]
Epoch [12/120    avg_loss:2.207, val_acc:0.476]
Epoch [13/120    avg_loss:2.121, val_acc:0.518]
Epoch [14/120    avg_loss:2.089, val_acc:0.528]
Epoch [15/120    avg_loss:2.020, val_acc:0.591]
Epoch [16/120    avg_loss:1.926, val_acc:0.596]
Epoch [17/120    avg_loss:1.851, val_acc:0.611]
Epoch [18/120    avg_loss:1.774, val_acc:0.626]
Epoch [19/120    avg_loss:1.671, val_acc:0.651]
Epoch [20/120    avg_loss:1.512, val_acc:0.691]
Epoch [21/120    avg_loss:1.378, val_acc:0.735]
Epoch [22/120    avg_loss:1.245, val_acc:0.693]
Epoch [23/120    avg_loss:1.179, val_acc:0.722]
Epoch [24/120    avg_loss:1.078, val_acc:0.754]
Epoch [25/120    avg_loss:0.991, val_acc:0.774]
Epoch [26/120    avg_loss:0.835, val_acc:0.807]
Epoch [27/120    avg_loss:0.736, val_acc:0.822]
Epoch [28/120    avg_loss:0.648, val_acc:0.870]
Epoch [29/120    avg_loss:0.636, val_acc:0.853]
Epoch [30/120    avg_loss:0.568, val_acc:0.820]
Epoch [31/120    avg_loss:0.503, val_acc:0.863]
Epoch [32/120    avg_loss:0.442, val_acc:0.857]
Epoch [33/120    avg_loss:0.407, val_acc:0.891]
Epoch [34/120    avg_loss:0.342, val_acc:0.896]
Epoch [35/120    avg_loss:0.330, val_acc:0.887]
Epoch [36/120    avg_loss:0.378, val_acc:0.904]
Epoch [37/120    avg_loss:0.340, val_acc:0.901]
Epoch [38/120    avg_loss:0.284, val_acc:0.918]
Epoch [39/120    avg_loss:0.228, val_acc:0.916]
Epoch [40/120    avg_loss:0.221, val_acc:0.935]
Epoch [41/120    avg_loss:0.204, val_acc:0.944]
Epoch [42/120    avg_loss:0.171, val_acc:0.933]
Epoch [43/120    avg_loss:0.192, val_acc:0.908]
Epoch [44/120    avg_loss:0.174, val_acc:0.925]
Epoch [45/120    avg_loss:0.151, val_acc:0.951]
Epoch [46/120    avg_loss:0.140, val_acc:0.941]
Epoch [47/120    avg_loss:0.125, val_acc:0.956]
Epoch [48/120    avg_loss:0.116, val_acc:0.957]
Epoch [49/120    avg_loss:0.102, val_acc:0.946]
Epoch [50/120    avg_loss:0.111, val_acc:0.960]
Epoch [51/120    avg_loss:0.105, val_acc:0.945]
Epoch [52/120    avg_loss:0.097, val_acc:0.948]
Epoch [53/120    avg_loss:0.114, val_acc:0.935]
Epoch [54/120    avg_loss:0.089, val_acc:0.958]
Epoch [55/120    avg_loss:0.080, val_acc:0.959]
Epoch [56/120    avg_loss:0.071, val_acc:0.954]
Epoch [57/120    avg_loss:0.064, val_acc:0.958]
Epoch [58/120    avg_loss:0.061, val_acc:0.967]
Epoch [59/120    avg_loss:0.065, val_acc:0.969]
Epoch [60/120    avg_loss:0.050, val_acc:0.964]
Epoch [61/120    avg_loss:0.051, val_acc:0.976]
Epoch [62/120    avg_loss:0.052, val_acc:0.959]
Epoch [63/120    avg_loss:0.058, val_acc:0.968]
Epoch [64/120    avg_loss:0.048, val_acc:0.963]
Epoch [65/120    avg_loss:0.048, val_acc:0.977]
Epoch [66/120    avg_loss:0.067, val_acc:0.945]
Epoch [67/120    avg_loss:0.064, val_acc:0.970]
Epoch [68/120    avg_loss:0.061, val_acc:0.958]
Epoch [69/120    avg_loss:0.068, val_acc:0.955]
Epoch [70/120    avg_loss:0.056, val_acc:0.966]
Epoch [71/120    avg_loss:0.042, val_acc:0.973]
Epoch [72/120    avg_loss:0.035, val_acc:0.973]
Epoch [73/120    avg_loss:0.031, val_acc:0.974]
Epoch [74/120    avg_loss:0.042, val_acc:0.972]
Epoch [75/120    avg_loss:0.040, val_acc:0.975]
Epoch [76/120    avg_loss:0.040, val_acc:0.972]
Epoch [77/120    avg_loss:0.041, val_acc:0.970]
Epoch [78/120    avg_loss:0.032, val_acc:0.978]
Epoch [79/120    avg_loss:0.026, val_acc:0.973]
Epoch [80/120    avg_loss:0.024, val_acc:0.970]
Epoch [81/120    avg_loss:0.024, val_acc:0.974]
Epoch [82/120    avg_loss:0.025, val_acc:0.971]
Epoch [83/120    avg_loss:0.028, val_acc:0.979]
Epoch [84/120    avg_loss:0.023, val_acc:0.977]
Epoch [85/120    avg_loss:0.036, val_acc:0.970]
Epoch [86/120    avg_loss:0.029, val_acc:0.975]
Epoch [87/120    avg_loss:0.029, val_acc:0.980]
Epoch [88/120    avg_loss:0.020, val_acc:0.972]
Epoch [89/120    avg_loss:0.023, val_acc:0.974]
Epoch [90/120    avg_loss:0.019, val_acc:0.985]
Epoch [91/120    avg_loss:0.016, val_acc:0.979]
Epoch [92/120    avg_loss:0.014, val_acc:0.984]
Epoch [93/120    avg_loss:0.012, val_acc:0.982]
Epoch [94/120    avg_loss:0.024, val_acc:0.984]
Epoch [95/120    avg_loss:0.024, val_acc:0.980]
Epoch [96/120    avg_loss:0.035, val_acc:0.978]
Epoch [97/120    avg_loss:0.018, val_acc:0.973]
Epoch [98/120    avg_loss:0.018, val_acc:0.983]
Epoch [99/120    avg_loss:0.018, val_acc:0.977]
Epoch [100/120    avg_loss:0.024, val_acc:0.979]
Epoch [101/120    avg_loss:0.020, val_acc:0.975]
Epoch [102/120    avg_loss:0.018, val_acc:0.980]
Epoch [103/120    avg_loss:0.012, val_acc:0.977]
Epoch [104/120    avg_loss:0.018, val_acc:0.981]
Epoch [105/120    avg_loss:0.012, val_acc:0.981]
Epoch [106/120    avg_loss:0.011, val_acc:0.981]
Epoch [107/120    avg_loss:0.012, val_acc:0.981]
Epoch [108/120    avg_loss:0.013, val_acc:0.982]
Epoch [109/120    avg_loss:0.011, val_acc:0.981]
Epoch [110/120    avg_loss:0.011, val_acc:0.983]
Epoch [111/120    avg_loss:0.012, val_acc:0.983]
Epoch [112/120    avg_loss:0.011, val_acc:0.983]
Epoch [113/120    avg_loss:0.010, val_acc:0.983]
Epoch [114/120    avg_loss:0.010, val_acc:0.982]
Epoch [115/120    avg_loss:0.012, val_acc:0.984]
Epoch [116/120    avg_loss:0.011, val_acc:0.983]
Epoch [117/120    avg_loss:0.011, val_acc:0.983]
Epoch [118/120    avg_loss:0.009, val_acc:0.983]
Epoch [119/120    avg_loss:0.011, val_acc:0.983]
Epoch [120/120    avg_loss:0.010, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0 1254    3   13    0    0    0    0    0    0   15    0    0
     0    0    0]
 [   0    0    0  723    0    1    2    0    0    3    2   12    3    1
     0    0    0]
 [   0    0    0    0  210    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  849   23    3    0
     0    0    0]
 [   0    0    1    2    0    0    5    0    0    0   11 2169   22    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    4  527    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1125   14    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    37  306    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.82113821138212

F1 scores:
[       nan 0.975      0.98701299 0.98033898 0.96330275 0.99423299
 0.98640483 1.         0.99883586 0.83333333 0.9775475  0.97768763
 0.96343693 0.99730458 0.97571552 0.91754123 0.98823529]

Kappa:
0.9751528790231674
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9942d18eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.819, val_acc:0.046]
Epoch [2/120    avg_loss:2.776, val_acc:0.163]
Epoch [3/120    avg_loss:2.742, val_acc:0.201]
Epoch [4/120    avg_loss:2.690, val_acc:0.217]
Epoch [5/120    avg_loss:2.626, val_acc:0.250]
Epoch [6/120    avg_loss:2.570, val_acc:0.292]
Epoch [7/120    avg_loss:2.505, val_acc:0.310]
Epoch [8/120    avg_loss:2.447, val_acc:0.318]
Epoch [9/120    avg_loss:2.374, val_acc:0.320]
Epoch [10/120    avg_loss:2.352, val_acc:0.323]
Epoch [11/120    avg_loss:2.259, val_acc:0.335]
Epoch [12/120    avg_loss:2.163, val_acc:0.346]
Epoch [13/120    avg_loss:2.066, val_acc:0.350]
Epoch [14/120    avg_loss:2.003, val_acc:0.371]
Epoch [15/120    avg_loss:1.870, val_acc:0.422]
Epoch [16/120    avg_loss:1.751, val_acc:0.414]
Epoch [17/120    avg_loss:1.599, val_acc:0.445]
Epoch [18/120    avg_loss:1.542, val_acc:0.520]
Epoch [19/120    avg_loss:1.446, val_acc:0.531]
Epoch [20/120    avg_loss:1.294, val_acc:0.561]
Epoch [21/120    avg_loss:1.202, val_acc:0.652]
Epoch [22/120    avg_loss:1.091, val_acc:0.671]
Epoch [23/120    avg_loss:0.974, val_acc:0.722]
Epoch [24/120    avg_loss:0.946, val_acc:0.753]
Epoch [25/120    avg_loss:0.832, val_acc:0.722]
Epoch [26/120    avg_loss:0.763, val_acc:0.771]
Epoch [27/120    avg_loss:0.668, val_acc:0.794]
Epoch [28/120    avg_loss:0.589, val_acc:0.777]
Epoch [29/120    avg_loss:0.518, val_acc:0.826]
Epoch [30/120    avg_loss:0.464, val_acc:0.829]
Epoch [31/120    avg_loss:0.446, val_acc:0.835]
Epoch [32/120    avg_loss:0.437, val_acc:0.779]
Epoch [33/120    avg_loss:0.449, val_acc:0.846]
Epoch [34/120    avg_loss:0.379, val_acc:0.865]
Epoch [35/120    avg_loss:0.337, val_acc:0.889]
Epoch [36/120    avg_loss:0.301, val_acc:0.897]
Epoch [37/120    avg_loss:0.281, val_acc:0.894]
Epoch [38/120    avg_loss:0.254, val_acc:0.911]
Epoch [39/120    avg_loss:0.265, val_acc:0.885]
Epoch [40/120    avg_loss:0.253, val_acc:0.929]
Epoch [41/120    avg_loss:0.237, val_acc:0.935]
Epoch [42/120    avg_loss:0.190, val_acc:0.929]
Epoch [43/120    avg_loss:0.180, val_acc:0.925]
Epoch [44/120    avg_loss:0.175, val_acc:0.939]
Epoch [45/120    avg_loss:0.189, val_acc:0.911]
Epoch [46/120    avg_loss:0.150, val_acc:0.945]
Epoch [47/120    avg_loss:0.157, val_acc:0.922]
Epoch [48/120    avg_loss:0.130, val_acc:0.944]
Epoch [49/120    avg_loss:0.105, val_acc:0.956]
Epoch [50/120    avg_loss:0.110, val_acc:0.951]
Epoch [51/120    avg_loss:0.106, val_acc:0.947]
Epoch [52/120    avg_loss:0.107, val_acc:0.953]
Epoch [53/120    avg_loss:0.089, val_acc:0.949]
Epoch [54/120    avg_loss:0.075, val_acc:0.945]
Epoch [55/120    avg_loss:0.085, val_acc:0.956]
Epoch [56/120    avg_loss:0.096, val_acc:0.949]
Epoch [57/120    avg_loss:0.094, val_acc:0.927]
Epoch [58/120    avg_loss:0.117, val_acc:0.944]
Epoch [59/120    avg_loss:0.089, val_acc:0.963]
Epoch [60/120    avg_loss:0.068, val_acc:0.963]
Epoch [61/120    avg_loss:0.065, val_acc:0.955]
Epoch [62/120    avg_loss:0.072, val_acc:0.957]
Epoch [63/120    avg_loss:0.067, val_acc:0.952]
Epoch [64/120    avg_loss:0.065, val_acc:0.951]
Epoch [65/120    avg_loss:0.069, val_acc:0.934]
Epoch [66/120    avg_loss:0.062, val_acc:0.967]
Epoch [67/120    avg_loss:0.055, val_acc:0.961]
Epoch [68/120    avg_loss:0.050, val_acc:0.968]
Epoch [69/120    avg_loss:0.044, val_acc:0.967]
Epoch [70/120    avg_loss:0.049, val_acc:0.965]
Epoch [71/120    avg_loss:0.043, val_acc:0.963]
Epoch [72/120    avg_loss:0.045, val_acc:0.956]
Epoch [73/120    avg_loss:0.036, val_acc:0.956]
Epoch [74/120    avg_loss:0.052, val_acc:0.971]
Epoch [75/120    avg_loss:0.051, val_acc:0.967]
Epoch [76/120    avg_loss:0.042, val_acc:0.967]
Epoch [77/120    avg_loss:0.033, val_acc:0.975]
Epoch [78/120    avg_loss:0.029, val_acc:0.966]
Epoch [79/120    avg_loss:0.021, val_acc:0.971]
Epoch [80/120    avg_loss:0.022, val_acc:0.970]
Epoch [81/120    avg_loss:0.024, val_acc:0.971]
Epoch [82/120    avg_loss:0.029, val_acc:0.979]
Epoch [83/120    avg_loss:0.027, val_acc:0.975]
Epoch [84/120    avg_loss:0.032, val_acc:0.945]
Epoch [85/120    avg_loss:0.035, val_acc:0.966]
Epoch [86/120    avg_loss:0.027, val_acc:0.972]
Epoch [87/120    avg_loss:0.018, val_acc:0.978]
Epoch [88/120    avg_loss:0.022, val_acc:0.970]
Epoch [89/120    avg_loss:0.019, val_acc:0.973]
Epoch [90/120    avg_loss:0.020, val_acc:0.975]
Epoch [91/120    avg_loss:0.025, val_acc:0.965]
Epoch [92/120    avg_loss:0.030, val_acc:0.970]
Epoch [93/120    avg_loss:0.024, val_acc:0.979]
Epoch [94/120    avg_loss:0.016, val_acc:0.979]
Epoch [95/120    avg_loss:0.015, val_acc:0.978]
Epoch [96/120    avg_loss:0.020, val_acc:0.972]
Epoch [97/120    avg_loss:0.037, val_acc:0.971]
Epoch [98/120    avg_loss:0.036, val_acc:0.955]
Epoch [99/120    avg_loss:0.035, val_acc:0.964]
Epoch [100/120    avg_loss:0.026, val_acc:0.969]
Epoch [101/120    avg_loss:0.018, val_acc:0.974]
Epoch [102/120    avg_loss:0.016, val_acc:0.970]
Epoch [103/120    avg_loss:0.016, val_acc:0.977]
Epoch [104/120    avg_loss:0.014, val_acc:0.976]
Epoch [105/120    avg_loss:0.011, val_acc:0.978]
Epoch [106/120    avg_loss:0.018, val_acc:0.976]
Epoch [107/120    avg_loss:0.015, val_acc:0.974]
Epoch [108/120    avg_loss:0.014, val_acc:0.978]
Epoch [109/120    avg_loss:0.011, val_acc:0.977]
Epoch [110/120    avg_loss:0.011, val_acc:0.977]
Epoch [111/120    avg_loss:0.011, val_acc:0.979]
Epoch [112/120    avg_loss:0.009, val_acc:0.978]
Epoch [113/120    avg_loss:0.009, val_acc:0.978]
Epoch [114/120    avg_loss:0.011, val_acc:0.978]
Epoch [115/120    avg_loss:0.010, val_acc:0.978]
Epoch [116/120    avg_loss:0.011, val_acc:0.978]
Epoch [117/120    avg_loss:0.010, val_acc:0.977]
Epoch [118/120    avg_loss:0.009, val_acc:0.978]
Epoch [119/120    avg_loss:0.010, val_acc:0.978]
Epoch [120/120    avg_loss:0.009, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1252    0    4    0    0    0    0    0    5   24    0    0
     0    0    0]
 [   0    0    2  727    2    0    0    0    0    3    5    3    5    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    5    0    0   13    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  855   14    0    0
     0    0    0]
 [   0    0    7    0    0    0    1    0    0    0   24 2156   21    0
     0    1    0]
 [   0    0    0    2    0    0    0    0    0    0    0    0  529    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    65  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.55013550135502

F1 scores:
[       nan 0.94871795 0.98119122 0.98442789 0.9837587  0.99769585
 0.99092284 1.         0.99883586 0.76470588 0.96719457 0.97777778
 0.9706422  1.         0.96743787 0.87147335 0.99408284]

Kappa:
0.9720622741203006
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa86dc9aef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.802, val_acc:0.150]
Epoch [2/120    avg_loss:2.738, val_acc:0.186]
Epoch [3/120    avg_loss:2.664, val_acc:0.194]
Epoch [4/120    avg_loss:2.616, val_acc:0.203]
Epoch [5/120    avg_loss:2.567, val_acc:0.222]
Epoch [6/120    avg_loss:2.503, val_acc:0.239]
Epoch [7/120    avg_loss:2.469, val_acc:0.279]
Epoch [8/120    avg_loss:2.439, val_acc:0.333]
Epoch [9/120    avg_loss:2.391, val_acc:0.403]
Epoch [10/120    avg_loss:2.364, val_acc:0.460]
Epoch [11/120    avg_loss:2.328, val_acc:0.491]
Epoch [12/120    avg_loss:2.255, val_acc:0.531]
Epoch [13/120    avg_loss:2.178, val_acc:0.528]
Epoch [14/120    avg_loss:2.128, val_acc:0.549]
Epoch [15/120    avg_loss:2.107, val_acc:0.553]
Epoch [16/120    avg_loss:1.999, val_acc:0.560]
Epoch [17/120    avg_loss:1.913, val_acc:0.566]
Epoch [18/120    avg_loss:1.864, val_acc:0.593]
Epoch [19/120    avg_loss:1.729, val_acc:0.559]
Epoch [20/120    avg_loss:1.638, val_acc:0.616]
Epoch [21/120    avg_loss:1.529, val_acc:0.649]
Epoch [22/120    avg_loss:1.368, val_acc:0.657]
Epoch [23/120    avg_loss:1.325, val_acc:0.634]
Epoch [24/120    avg_loss:1.213, val_acc:0.705]
Epoch [25/120    avg_loss:1.090, val_acc:0.736]
Epoch [26/120    avg_loss:0.967, val_acc:0.794]
Epoch [27/120    avg_loss:1.024, val_acc:0.805]
Epoch [28/120    avg_loss:0.892, val_acc:0.822]
Epoch [29/120    avg_loss:0.849, val_acc:0.806]
Epoch [30/120    avg_loss:0.714, val_acc:0.856]
Epoch [31/120    avg_loss:0.592, val_acc:0.885]
Epoch [32/120    avg_loss:0.528, val_acc:0.874]
Epoch [33/120    avg_loss:0.474, val_acc:0.891]
Epoch [34/120    avg_loss:0.471, val_acc:0.890]
Epoch [35/120    avg_loss:0.469, val_acc:0.896]
Epoch [36/120    avg_loss:0.360, val_acc:0.878]
Epoch [37/120    avg_loss:0.331, val_acc:0.915]
Epoch [38/120    avg_loss:0.275, val_acc:0.916]
Epoch [39/120    avg_loss:0.252, val_acc:0.952]
Epoch [40/120    avg_loss:0.201, val_acc:0.944]
Epoch [41/120    avg_loss:0.216, val_acc:0.926]
Epoch [42/120    avg_loss:0.255, val_acc:0.940]
Epoch [43/120    avg_loss:0.173, val_acc:0.935]
Epoch [44/120    avg_loss:0.175, val_acc:0.947]
Epoch [45/120    avg_loss:0.158, val_acc:0.909]
Epoch [46/120    avg_loss:0.143, val_acc:0.951]
Epoch [47/120    avg_loss:0.120, val_acc:0.940]
Epoch [48/120    avg_loss:0.112, val_acc:0.955]
Epoch [49/120    avg_loss:0.097, val_acc:0.946]
Epoch [50/120    avg_loss:0.100, val_acc:0.951]
Epoch [51/120    avg_loss:0.103, val_acc:0.955]
Epoch [52/120    avg_loss:0.083, val_acc:0.963]
Epoch [53/120    avg_loss:0.072, val_acc:0.953]
Epoch [54/120    avg_loss:0.085, val_acc:0.955]
Epoch [55/120    avg_loss:0.079, val_acc:0.952]
Epoch [56/120    avg_loss:0.072, val_acc:0.966]
Epoch [57/120    avg_loss:0.063, val_acc:0.948]
Epoch [58/120    avg_loss:0.064, val_acc:0.968]
Epoch [59/120    avg_loss:0.060, val_acc:0.966]
Epoch [60/120    avg_loss:0.051, val_acc:0.965]
Epoch [61/120    avg_loss:0.047, val_acc:0.974]
Epoch [62/120    avg_loss:0.046, val_acc:0.973]
Epoch [63/120    avg_loss:0.047, val_acc:0.968]
Epoch [64/120    avg_loss:0.044, val_acc:0.965]
Epoch [65/120    avg_loss:0.046, val_acc:0.968]
Epoch [66/120    avg_loss:0.042, val_acc:0.933]
Epoch [67/120    avg_loss:0.044, val_acc:0.969]
Epoch [68/120    avg_loss:0.042, val_acc:0.971]
Epoch [69/120    avg_loss:0.040, val_acc:0.963]
Epoch [70/120    avg_loss:0.031, val_acc:0.982]
Epoch [71/120    avg_loss:0.043, val_acc:0.974]
Epoch [72/120    avg_loss:0.038, val_acc:0.972]
Epoch [73/120    avg_loss:0.045, val_acc:0.971]
Epoch [74/120    avg_loss:0.049, val_acc:0.967]
Epoch [75/120    avg_loss:0.043, val_acc:0.965]
Epoch [76/120    avg_loss:0.036, val_acc:0.966]
Epoch [77/120    avg_loss:0.042, val_acc:0.970]
Epoch [78/120    avg_loss:0.039, val_acc:0.964]
Epoch [79/120    avg_loss:0.040, val_acc:0.968]
Epoch [80/120    avg_loss:0.048, val_acc:0.972]
Epoch [81/120    avg_loss:0.059, val_acc:0.971]
Epoch [82/120    avg_loss:0.050, val_acc:0.976]
Epoch [83/120    avg_loss:0.047, val_acc:0.949]
Epoch [84/120    avg_loss:0.029, val_acc:0.983]
Epoch [85/120    avg_loss:0.025, val_acc:0.981]
Epoch [86/120    avg_loss:0.023, val_acc:0.980]
Epoch [87/120    avg_loss:0.024, val_acc:0.981]
Epoch [88/120    avg_loss:0.023, val_acc:0.982]
Epoch [89/120    avg_loss:0.021, val_acc:0.983]
Epoch [90/120    avg_loss:0.021, val_acc:0.984]
Epoch [91/120    avg_loss:0.020, val_acc:0.983]
Epoch [92/120    avg_loss:0.022, val_acc:0.984]
Epoch [93/120    avg_loss:0.021, val_acc:0.982]
Epoch [94/120    avg_loss:0.018, val_acc:0.982]
Epoch [95/120    avg_loss:0.019, val_acc:0.983]
Epoch [96/120    avg_loss:0.017, val_acc:0.984]
Epoch [97/120    avg_loss:0.019, val_acc:0.983]
Epoch [98/120    avg_loss:0.019, val_acc:0.982]
Epoch [99/120    avg_loss:0.017, val_acc:0.983]
Epoch [100/120    avg_loss:0.020, val_acc:0.984]
Epoch [101/120    avg_loss:0.018, val_acc:0.984]
Epoch [102/120    avg_loss:0.019, val_acc:0.984]
Epoch [103/120    avg_loss:0.018, val_acc:0.984]
Epoch [104/120    avg_loss:0.017, val_acc:0.984]
Epoch [105/120    avg_loss:0.019, val_acc:0.984]
Epoch [106/120    avg_loss:0.020, val_acc:0.984]
Epoch [107/120    avg_loss:0.018, val_acc:0.984]
Epoch [108/120    avg_loss:0.017, val_acc:0.984]
Epoch [109/120    avg_loss:0.018, val_acc:0.984]
Epoch [110/120    avg_loss:0.017, val_acc:0.984]
Epoch [111/120    avg_loss:0.016, val_acc:0.984]
Epoch [112/120    avg_loss:0.018, val_acc:0.984]
Epoch [113/120    avg_loss:0.017, val_acc:0.984]
Epoch [114/120    avg_loss:0.016, val_acc:0.984]
Epoch [115/120    avg_loss:0.016, val_acc:0.984]
Epoch [116/120    avg_loss:0.018, val_acc:0.984]
Epoch [117/120    avg_loss:0.018, val_acc:0.985]
Epoch [118/120    avg_loss:0.017, val_acc:0.984]
Epoch [119/120    avg_loss:0.018, val_acc:0.985]
Epoch [120/120    avg_loss:0.016, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    2    0    0    0    0    0    0    0    1    1    0    0
     0    0    0]
 [   0    0 1261    4    7    1    0    0    0    0    0   12    0    0
     0    0    0]
 [   0    0    1  721    9    1    2    0    0    2    0    4    4    1
     0    2    0]
 [   0    0    0    0  210    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0  840   33    0    0
     0    0    0]
 [   0    0    6    0    0    1    1    0    0    0    2 2177   23    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    3    7  518    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    2    0  183
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
  1131    6    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    61  286    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.64769647696477

F1 scores:
[       nan 0.94871795 0.98631208 0.97895451 0.95671982 0.98966705
 0.99696049 1.         1.         0.94736842 0.97617664 0.97930724
 0.95748614 0.99186992 0.96832192 0.88819876 0.98823529]

Kappa:
0.9731599381696793
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff5969b8eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.810, val_acc:0.161]
Epoch [2/120    avg_loss:2.762, val_acc:0.265]
Epoch [3/120    avg_loss:2.705, val_acc:0.381]
Epoch [4/120    avg_loss:2.660, val_acc:0.441]
Epoch [5/120    avg_loss:2.596, val_acc:0.451]
Epoch [6/120    avg_loss:2.548, val_acc:0.460]
Epoch [7/120    avg_loss:2.488, val_acc:0.474]
Epoch [8/120    avg_loss:2.399, val_acc:0.484]
Epoch [9/120    avg_loss:2.354, val_acc:0.515]
Epoch [10/120    avg_loss:2.253, val_acc:0.500]
Epoch [11/120    avg_loss:2.207, val_acc:0.554]
Epoch [12/120    avg_loss:2.128, val_acc:0.544]
Epoch [13/120    avg_loss:2.052, val_acc:0.571]
Epoch [14/120    avg_loss:1.939, val_acc:0.574]
Epoch [15/120    avg_loss:1.846, val_acc:0.596]
Epoch [16/120    avg_loss:1.711, val_acc:0.569]
Epoch [17/120    avg_loss:1.574, val_acc:0.568]
Epoch [18/120    avg_loss:1.431, val_acc:0.610]
Epoch [19/120    avg_loss:1.449, val_acc:0.595]
Epoch [20/120    avg_loss:1.265, val_acc:0.604]
Epoch [21/120    avg_loss:1.122, val_acc:0.676]
Epoch [22/120    avg_loss:0.981, val_acc:0.682]
Epoch [23/120    avg_loss:0.931, val_acc:0.731]
Epoch [24/120    avg_loss:0.843, val_acc:0.786]
Epoch [25/120    avg_loss:0.784, val_acc:0.799]
Epoch [26/120    avg_loss:0.782, val_acc:0.751]
Epoch [27/120    avg_loss:0.685, val_acc:0.775]
Epoch [28/120    avg_loss:0.582, val_acc:0.832]
Epoch [29/120    avg_loss:0.509, val_acc:0.841]
Epoch [30/120    avg_loss:0.448, val_acc:0.872]
Epoch [31/120    avg_loss:0.415, val_acc:0.849]
Epoch [32/120    avg_loss:0.371, val_acc:0.847]
Epoch [33/120    avg_loss:0.343, val_acc:0.886]
Epoch [34/120    avg_loss:0.297, val_acc:0.903]
Epoch [35/120    avg_loss:0.295, val_acc:0.894]
Epoch [36/120    avg_loss:0.247, val_acc:0.897]
Epoch [37/120    avg_loss:0.261, val_acc:0.899]
Epoch [38/120    avg_loss:0.237, val_acc:0.892]
Epoch [39/120    avg_loss:0.206, val_acc:0.906]
Epoch [40/120    avg_loss:0.182, val_acc:0.904]
Epoch [41/120    avg_loss:0.177, val_acc:0.928]
Epoch [42/120    avg_loss:0.159, val_acc:0.925]
Epoch [43/120    avg_loss:0.150, val_acc:0.932]
Epoch [44/120    avg_loss:0.142, val_acc:0.933]
Epoch [45/120    avg_loss:0.123, val_acc:0.930]
Epoch [46/120    avg_loss:0.145, val_acc:0.928]
Epoch [47/120    avg_loss:0.133, val_acc:0.940]
Epoch [48/120    avg_loss:0.111, val_acc:0.925]
Epoch [49/120    avg_loss:0.097, val_acc:0.943]
Epoch [50/120    avg_loss:0.123, val_acc:0.921]
Epoch [51/120    avg_loss:0.127, val_acc:0.926]
Epoch [52/120    avg_loss:0.160, val_acc:0.938]
Epoch [53/120    avg_loss:0.106, val_acc:0.941]
Epoch [54/120    avg_loss:0.087, val_acc:0.948]
Epoch [55/120    avg_loss:0.085, val_acc:0.944]
Epoch [56/120    avg_loss:0.080, val_acc:0.947]
Epoch [57/120    avg_loss:0.076, val_acc:0.941]
Epoch [58/120    avg_loss:0.073, val_acc:0.952]
Epoch [59/120    avg_loss:0.077, val_acc:0.948]
Epoch [60/120    avg_loss:0.064, val_acc:0.963]
Epoch [61/120    avg_loss:0.054, val_acc:0.957]
Epoch [62/120    avg_loss:0.057, val_acc:0.965]
Epoch [63/120    avg_loss:0.047, val_acc:0.964]
Epoch [64/120    avg_loss:0.053, val_acc:0.944]
Epoch [65/120    avg_loss:0.049, val_acc:0.961]
Epoch [66/120    avg_loss:0.045, val_acc:0.963]
Epoch [67/120    avg_loss:0.042, val_acc:0.971]
Epoch [68/120    avg_loss:0.038, val_acc:0.966]
Epoch [69/120    avg_loss:0.037, val_acc:0.967]
Epoch [70/120    avg_loss:0.046, val_acc:0.959]
Epoch [71/120    avg_loss:0.041, val_acc:0.967]
Epoch [72/120    avg_loss:0.034, val_acc:0.972]
Epoch [73/120    avg_loss:0.049, val_acc:0.955]
Epoch [74/120    avg_loss:0.046, val_acc:0.976]
Epoch [75/120    avg_loss:0.034, val_acc:0.967]
Epoch [76/120    avg_loss:0.031, val_acc:0.970]
Epoch [77/120    avg_loss:0.025, val_acc:0.966]
Epoch [78/120    avg_loss:0.025, val_acc:0.961]
Epoch [79/120    avg_loss:0.025, val_acc:0.969]
Epoch [80/120    avg_loss:0.025, val_acc:0.970]
Epoch [81/120    avg_loss:0.026, val_acc:0.966]
Epoch [82/120    avg_loss:0.026, val_acc:0.974]
Epoch [83/120    avg_loss:0.028, val_acc:0.969]
Epoch [84/120    avg_loss:0.023, val_acc:0.976]
Epoch [85/120    avg_loss:0.025, val_acc:0.965]
Epoch [86/120    avg_loss:0.022, val_acc:0.975]
Epoch [87/120    avg_loss:0.024, val_acc:0.971]
Epoch [88/120    avg_loss:0.026, val_acc:0.976]
Epoch [89/120    avg_loss:0.025, val_acc:0.971]
Epoch [90/120    avg_loss:0.023, val_acc:0.969]
Epoch [91/120    avg_loss:0.017, val_acc:0.970]
Epoch [92/120    avg_loss:0.017, val_acc:0.965]
Epoch [93/120    avg_loss:0.015, val_acc:0.975]
Epoch [94/120    avg_loss:0.016, val_acc:0.966]
Epoch [95/120    avg_loss:0.020, val_acc:0.971]
Epoch [96/120    avg_loss:0.017, val_acc:0.970]
Epoch [97/120    avg_loss:0.016, val_acc:0.974]
Epoch [98/120    avg_loss:0.015, val_acc:0.970]
Epoch [99/120    avg_loss:0.015, val_acc:0.974]
Epoch [100/120    avg_loss:0.015, val_acc:0.973]
Epoch [101/120    avg_loss:0.013, val_acc:0.972]
Epoch [102/120    avg_loss:0.013, val_acc:0.974]
Epoch [103/120    avg_loss:0.011, val_acc:0.971]
Epoch [104/120    avg_loss:0.011, val_acc:0.972]
Epoch [105/120    avg_loss:0.013, val_acc:0.973]
Epoch [106/120    avg_loss:0.011, val_acc:0.972]
Epoch [107/120    avg_loss:0.012, val_acc:0.973]
Epoch [108/120    avg_loss:0.012, val_acc:0.973]
Epoch [109/120    avg_loss:0.014, val_acc:0.974]
Epoch [110/120    avg_loss:0.010, val_acc:0.975]
Epoch [111/120    avg_loss:0.011, val_acc:0.972]
Epoch [112/120    avg_loss:0.011, val_acc:0.972]
Epoch [113/120    avg_loss:0.011, val_acc:0.972]
Epoch [114/120    avg_loss:0.010, val_acc:0.973]
Epoch [115/120    avg_loss:0.011, val_acc:0.972]
Epoch [116/120    avg_loss:0.011, val_acc:0.971]
Epoch [117/120    avg_loss:0.010, val_acc:0.972]
Epoch [118/120    avg_loss:0.010, val_acc:0.972]
Epoch [119/120    avg_loss:0.011, val_acc:0.971]
Epoch [120/120    avg_loss:0.011, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1250    1    4    0    0    0    0    0    2   27    1    0
     0    0    0]
 [   0    0    1  721    0    0    0    0    0    8    0    6   11    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0  843   28    2    0
     0    0    0]
 [   0    0   10    0    0    0    0    0    0    1   10 2167   15    0
     7    0    0]
 [   0    0    0    2    0    0    0    0    0    0    1    0  530    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1129    9    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    38  309    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.8970189701897

F1 scores:
[       nan 0.975      0.98039216 0.97895451 0.98598131 0.99884925
 0.99923839 1.         1.         0.7826087  0.97400347 0.97612613
 0.96980787 1.         0.97622136 0.92932331 0.99408284]

Kappa:
0.9760148345765395
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa1abcf7f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.816, val_acc:0.133]
Epoch [2/120    avg_loss:2.758, val_acc:0.249]
Epoch [3/120    avg_loss:2.695, val_acc:0.336]
Epoch [4/120    avg_loss:2.639, val_acc:0.360]
Epoch [5/120    avg_loss:2.578, val_acc:0.384]
Epoch [6/120    avg_loss:2.502, val_acc:0.409]
Epoch [7/120    avg_loss:2.426, val_acc:0.423]
Epoch [8/120    avg_loss:2.360, val_acc:0.454]
Epoch [9/120    avg_loss:2.302, val_acc:0.477]
Epoch [10/120    avg_loss:2.280, val_acc:0.505]
Epoch [11/120    avg_loss:2.179, val_acc:0.528]
Epoch [12/120    avg_loss:2.136, val_acc:0.545]
Epoch [13/120    avg_loss:2.102, val_acc:0.532]
Epoch [14/120    avg_loss:2.025, val_acc:0.530]
Epoch [15/120    avg_loss:1.960, val_acc:0.524]
Epoch [16/120    avg_loss:1.878, val_acc:0.571]
Epoch [17/120    avg_loss:1.798, val_acc:0.581]
Epoch [18/120    avg_loss:1.712, val_acc:0.593]
Epoch [19/120    avg_loss:1.697, val_acc:0.580]
Epoch [20/120    avg_loss:1.556, val_acc:0.643]
Epoch [21/120    avg_loss:1.454, val_acc:0.729]
Epoch [22/120    avg_loss:1.374, val_acc:0.742]
Epoch [23/120    avg_loss:1.255, val_acc:0.784]
Epoch [24/120    avg_loss:1.189, val_acc:0.807]
Epoch [25/120    avg_loss:1.085, val_acc:0.794]
Epoch [26/120    avg_loss:0.901, val_acc:0.829]
Epoch [27/120    avg_loss:0.786, val_acc:0.826]
Epoch [28/120    avg_loss:0.685, val_acc:0.846]
Epoch [29/120    avg_loss:0.612, val_acc:0.843]
Epoch [30/120    avg_loss:0.592, val_acc:0.843]
Epoch [31/120    avg_loss:0.520, val_acc:0.865]
Epoch [32/120    avg_loss:0.450, val_acc:0.900]
Epoch [33/120    avg_loss:0.372, val_acc:0.906]
Epoch [34/120    avg_loss:0.354, val_acc:0.907]
Epoch [35/120    avg_loss:0.306, val_acc:0.915]
Epoch [36/120    avg_loss:0.299, val_acc:0.922]
Epoch [37/120    avg_loss:0.249, val_acc:0.920]
Epoch [38/120    avg_loss:0.273, val_acc:0.916]
Epoch [39/120    avg_loss:0.243, val_acc:0.927]
Epoch [40/120    avg_loss:0.178, val_acc:0.948]
Epoch [41/120    avg_loss:0.173, val_acc:0.922]
Epoch [42/120    avg_loss:0.176, val_acc:0.938]
Epoch [43/120    avg_loss:0.158, val_acc:0.934]
Epoch [44/120    avg_loss:0.184, val_acc:0.931]
Epoch [45/120    avg_loss:0.136, val_acc:0.951]
Epoch [46/120    avg_loss:0.143, val_acc:0.955]
Epoch [47/120    avg_loss:0.110, val_acc:0.956]
Epoch [48/120    avg_loss:0.103, val_acc:0.953]
Epoch [49/120    avg_loss:0.095, val_acc:0.952]
Epoch [50/120    avg_loss:0.093, val_acc:0.960]
Epoch [51/120    avg_loss:0.102, val_acc:0.956]
Epoch [52/120    avg_loss:0.088, val_acc:0.970]
Epoch [53/120    avg_loss:0.074, val_acc:0.960]
Epoch [54/120    avg_loss:0.074, val_acc:0.961]
Epoch [55/120    avg_loss:0.068, val_acc:0.969]
Epoch [56/120    avg_loss:0.073, val_acc:0.954]
Epoch [57/120    avg_loss:0.064, val_acc:0.958]
Epoch [58/120    avg_loss:0.059, val_acc:0.968]
Epoch [59/120    avg_loss:0.069, val_acc:0.958]
Epoch [60/120    avg_loss:0.074, val_acc:0.967]
Epoch [61/120    avg_loss:0.062, val_acc:0.973]
Epoch [62/120    avg_loss:0.102, val_acc:0.940]
Epoch [63/120    avg_loss:0.078, val_acc:0.966]
Epoch [64/120    avg_loss:0.055, val_acc:0.971]
Epoch [65/120    avg_loss:0.057, val_acc:0.959]
Epoch [66/120    avg_loss:0.037, val_acc:0.971]
Epoch [67/120    avg_loss:0.037, val_acc:0.971]
Epoch [68/120    avg_loss:0.047, val_acc:0.964]
Epoch [69/120    avg_loss:0.053, val_acc:0.955]
Epoch [70/120    avg_loss:0.061, val_acc:0.954]
Epoch [71/120    avg_loss:0.047, val_acc:0.973]
Epoch [72/120    avg_loss:0.041, val_acc:0.971]
Epoch [73/120    avg_loss:0.045, val_acc:0.969]
Epoch [74/120    avg_loss:0.027, val_acc:0.979]
Epoch [75/120    avg_loss:0.029, val_acc:0.976]
Epoch [76/120    avg_loss:0.037, val_acc:0.968]
Epoch [77/120    avg_loss:0.029, val_acc:0.970]
Epoch [78/120    avg_loss:0.027, val_acc:0.970]
Epoch [79/120    avg_loss:0.031, val_acc:0.980]
Epoch [80/120    avg_loss:0.030, val_acc:0.977]
Epoch [81/120    avg_loss:0.022, val_acc:0.972]
Epoch [82/120    avg_loss:0.024, val_acc:0.979]
Epoch [83/120    avg_loss:0.019, val_acc:0.971]
Epoch [84/120    avg_loss:0.019, val_acc:0.976]
Epoch [85/120    avg_loss:0.021, val_acc:0.976]
Epoch [86/120    avg_loss:0.024, val_acc:0.977]
Epoch [87/120    avg_loss:0.015, val_acc:0.980]
Epoch [88/120    avg_loss:0.019, val_acc:0.970]
Epoch [89/120    avg_loss:0.018, val_acc:0.980]
Epoch [90/120    avg_loss:0.015, val_acc:0.979]
Epoch [91/120    avg_loss:0.016, val_acc:0.974]
Epoch [92/120    avg_loss:0.018, val_acc:0.970]
Epoch [93/120    avg_loss:0.018, val_acc:0.977]
Epoch [94/120    avg_loss:0.021, val_acc:0.981]
Epoch [95/120    avg_loss:0.017, val_acc:0.970]
Epoch [96/120    avg_loss:0.017, val_acc:0.980]
Epoch [97/120    avg_loss:0.023, val_acc:0.978]
Epoch [98/120    avg_loss:0.015, val_acc:0.975]
Epoch [99/120    avg_loss:0.028, val_acc:0.970]
Epoch [100/120    avg_loss:0.026, val_acc:0.973]
Epoch [101/120    avg_loss:0.021, val_acc:0.984]
Epoch [102/120    avg_loss:0.016, val_acc:0.984]
Epoch [103/120    avg_loss:0.013, val_acc:0.981]
Epoch [104/120    avg_loss:0.013, val_acc:0.984]
Epoch [105/120    avg_loss:0.014, val_acc:0.975]
Epoch [106/120    avg_loss:0.013, val_acc:0.982]
Epoch [107/120    avg_loss:0.009, val_acc:0.981]
Epoch [108/120    avg_loss:0.012, val_acc:0.982]
Epoch [109/120    avg_loss:0.013, val_acc:0.979]
Epoch [110/120    avg_loss:0.013, val_acc:0.979]
Epoch [111/120    avg_loss:0.011, val_acc:0.979]
Epoch [112/120    avg_loss:0.012, val_acc:0.980]
Epoch [113/120    avg_loss:0.009, val_acc:0.979]
Epoch [114/120    avg_loss:0.011, val_acc:0.977]
Epoch [115/120    avg_loss:0.013, val_acc:0.978]
Epoch [116/120    avg_loss:0.011, val_acc:0.976]
Epoch [117/120    avg_loss:0.009, val_acc:0.984]
Epoch [118/120    avg_loss:0.008, val_acc:0.981]
Epoch [119/120    avg_loss:0.008, val_acc:0.982]
Epoch [120/120    avg_loss:0.009, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1252    0    6    0    1    0    0    0    8   17    1    0
     0    0    0]
 [   0    0    1  706    1    2    0    0    0    7    4    9   15    1
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    7    0    0    0    0    0    0  421    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   15    0    0    2    0
     0    0    0]
 [   0    0    7    0    0    1    3    0    0    0  849   12    3    0
     0    0    0]
 [   0    0   13    0    0    0    1    0    0    0   18 2164   13    0
     0    1    0]
 [   0    0    0    3    0    0    0    0    0    0    2    0  525    0
     1    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1135    4    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    17  324    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.84281842818429

F1 scores:
[       nan 0.90909091 0.97888976 0.96978022 0.98383372 0.99541284
 0.99018868 0.98039216 0.98942421 0.75       0.96642003 0.98073873
 0.95715588 0.99730458 0.98996947 0.95716396 0.97619048]

Kappa:
0.9754128360645864
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6994b58f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.822, val_acc:0.080]
Epoch [2/120    avg_loss:2.765, val_acc:0.102]
Epoch [3/120    avg_loss:2.697, val_acc:0.166]
Epoch [4/120    avg_loss:2.620, val_acc:0.245]
Epoch [5/120    avg_loss:2.572, val_acc:0.267]
Epoch [6/120    avg_loss:2.499, val_acc:0.284]
Epoch [7/120    avg_loss:2.429, val_acc:0.296]
Epoch [8/120    avg_loss:2.387, val_acc:0.304]
Epoch [9/120    avg_loss:2.338, val_acc:0.328]
Epoch [10/120    avg_loss:2.305, val_acc:0.335]
Epoch [11/120    avg_loss:2.264, val_acc:0.365]
Epoch [12/120    avg_loss:2.201, val_acc:0.413]
Epoch [13/120    avg_loss:2.161, val_acc:0.387]
Epoch [14/120    avg_loss:2.115, val_acc:0.470]
Epoch [15/120    avg_loss:2.044, val_acc:0.491]
Epoch [16/120    avg_loss:1.935, val_acc:0.478]
Epoch [17/120    avg_loss:1.821, val_acc:0.519]
Epoch [18/120    avg_loss:1.734, val_acc:0.540]
Epoch [19/120    avg_loss:1.585, val_acc:0.539]
Epoch [20/120    avg_loss:1.477, val_acc:0.572]
Epoch [21/120    avg_loss:1.306, val_acc:0.633]
Epoch [22/120    avg_loss:1.213, val_acc:0.661]
Epoch [23/120    avg_loss:1.191, val_acc:0.656]
Epoch [24/120    avg_loss:1.076, val_acc:0.664]
Epoch [25/120    avg_loss:0.958, val_acc:0.683]
Epoch [26/120    avg_loss:0.899, val_acc:0.742]
Epoch [27/120    avg_loss:0.863, val_acc:0.727]
Epoch [28/120    avg_loss:0.786, val_acc:0.792]
Epoch [29/120    avg_loss:0.695, val_acc:0.796]
Epoch [30/120    avg_loss:0.605, val_acc:0.816]
Epoch [31/120    avg_loss:0.559, val_acc:0.839]
Epoch [32/120    avg_loss:0.534, val_acc:0.844]
Epoch [33/120    avg_loss:0.458, val_acc:0.835]
Epoch [34/120    avg_loss:0.387, val_acc:0.875]
Epoch [35/120    avg_loss:0.353, val_acc:0.888]
Epoch [36/120    avg_loss:0.462, val_acc:0.829]
Epoch [37/120    avg_loss:0.394, val_acc:0.870]
Epoch [38/120    avg_loss:0.305, val_acc:0.875]
Epoch [39/120    avg_loss:0.255, val_acc:0.900]
Epoch [40/120    avg_loss:0.235, val_acc:0.901]
Epoch [41/120    avg_loss:0.202, val_acc:0.917]
Epoch [42/120    avg_loss:0.235, val_acc:0.897]
Epoch [43/120    avg_loss:0.220, val_acc:0.903]
Epoch [44/120    avg_loss:0.205, val_acc:0.906]
Epoch [45/120    avg_loss:0.181, val_acc:0.927]
Epoch [46/120    avg_loss:0.144, val_acc:0.946]
Epoch [47/120    avg_loss:0.133, val_acc:0.940]
Epoch [48/120    avg_loss:0.133, val_acc:0.939]
Epoch [49/120    avg_loss:0.120, val_acc:0.949]
Epoch [50/120    avg_loss:0.110, val_acc:0.938]
Epoch [51/120    avg_loss:0.106, val_acc:0.954]
Epoch [52/120    avg_loss:0.111, val_acc:0.956]
Epoch [53/120    avg_loss:0.107, val_acc:0.953]
Epoch [54/120    avg_loss:0.081, val_acc:0.956]
Epoch [55/120    avg_loss:0.091, val_acc:0.955]
Epoch [56/120    avg_loss:0.091, val_acc:0.960]
Epoch [57/120    avg_loss:0.074, val_acc:0.964]
Epoch [58/120    avg_loss:0.078, val_acc:0.967]
Epoch [59/120    avg_loss:0.147, val_acc:0.936]
Epoch [60/120    avg_loss:0.116, val_acc:0.949]
Epoch [61/120    avg_loss:0.082, val_acc:0.948]
Epoch [62/120    avg_loss:0.093, val_acc:0.952]
Epoch [63/120    avg_loss:0.068, val_acc:0.963]
Epoch [64/120    avg_loss:0.054, val_acc:0.964]
Epoch [65/120    avg_loss:0.048, val_acc:0.982]
Epoch [66/120    avg_loss:0.041, val_acc:0.980]
Epoch [67/120    avg_loss:0.037, val_acc:0.970]
Epoch [68/120    avg_loss:0.045, val_acc:0.970]
Epoch [69/120    avg_loss:0.046, val_acc:0.982]
Epoch [70/120    avg_loss:0.035, val_acc:0.977]
Epoch [71/120    avg_loss:0.044, val_acc:0.972]
Epoch [72/120    avg_loss:0.035, val_acc:0.975]
Epoch [73/120    avg_loss:0.038, val_acc:0.977]
Epoch [74/120    avg_loss:0.056, val_acc:0.971]
Epoch [75/120    avg_loss:0.038, val_acc:0.958]
Epoch [76/120    avg_loss:0.032, val_acc:0.980]
Epoch [77/120    avg_loss:0.027, val_acc:0.980]
Epoch [78/120    avg_loss:0.025, val_acc:0.984]
Epoch [79/120    avg_loss:0.025, val_acc:0.983]
Epoch [80/120    avg_loss:0.029, val_acc:0.981]
Epoch [81/120    avg_loss:0.036, val_acc:0.984]
Epoch [82/120    avg_loss:0.031, val_acc:0.989]
Epoch [83/120    avg_loss:0.031, val_acc:0.977]
Epoch [84/120    avg_loss:0.032, val_acc:0.981]
Epoch [85/120    avg_loss:0.032, val_acc:0.969]
Epoch [86/120    avg_loss:0.027, val_acc:0.977]
Epoch [87/120    avg_loss:0.029, val_acc:0.983]
Epoch [88/120    avg_loss:0.021, val_acc:0.988]
Epoch [89/120    avg_loss:0.016, val_acc:0.984]
Epoch [90/120    avg_loss:0.018, val_acc:0.984]
Epoch [91/120    avg_loss:0.018, val_acc:0.982]
Epoch [92/120    avg_loss:0.020, val_acc:0.987]
Epoch [93/120    avg_loss:0.020, val_acc:0.987]
Epoch [94/120    avg_loss:0.021, val_acc:0.986]
Epoch [95/120    avg_loss:0.017, val_acc:0.982]
Epoch [96/120    avg_loss:0.014, val_acc:0.985]
Epoch [97/120    avg_loss:0.012, val_acc:0.986]
Epoch [98/120    avg_loss:0.014, val_acc:0.987]
Epoch [99/120    avg_loss:0.012, val_acc:0.986]
Epoch [100/120    avg_loss:0.012, val_acc:0.986]
Epoch [101/120    avg_loss:0.013, val_acc:0.985]
Epoch [102/120    avg_loss:0.012, val_acc:0.986]
Epoch [103/120    avg_loss:0.012, val_acc:0.986]
Epoch [104/120    avg_loss:0.012, val_acc:0.985]
Epoch [105/120    avg_loss:0.011, val_acc:0.984]
Epoch [106/120    avg_loss:0.011, val_acc:0.984]
Epoch [107/120    avg_loss:0.012, val_acc:0.985]
Epoch [108/120    avg_loss:0.012, val_acc:0.985]
Epoch [109/120    avg_loss:0.012, val_acc:0.985]
Epoch [110/120    avg_loss:0.011, val_acc:0.985]
Epoch [111/120    avg_loss:0.011, val_acc:0.984]
Epoch [112/120    avg_loss:0.011, val_acc:0.985]
Epoch [113/120    avg_loss:0.012, val_acc:0.985]
Epoch [114/120    avg_loss:0.011, val_acc:0.985]
Epoch [115/120    avg_loss:0.011, val_acc:0.985]
Epoch [116/120    avg_loss:0.013, val_acc:0.985]
Epoch [117/120    avg_loss:0.011, val_acc:0.985]
Epoch [118/120    avg_loss:0.011, val_acc:0.985]
Epoch [119/120    avg_loss:0.012, val_acc:0.985]
Epoch [120/120    avg_loss:0.011, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1263    0    5    0    0    0    0    0    5   12    0    0
     0    0    0]
 [   0    0    2  715   12    0    0    0    0    3    2    5    8    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   11    0    0    1    2    0    0    0  840   19    1    0
     1    0    0]
 [   0    0   14    0    0    1    2    0    0    0   12 2171   10    0
     0    0    0]
 [   0    0    2    0    0    2    0    0    0    0    2    0  526    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1134    5    0]
 [   0    0    0    0    0    1   23    0    0    0    0    0    0    0
    41  282    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.64769647696477

F1 scores:
[       nan 0.96202532 0.98020955 0.97677596 0.9569161  0.98966705
 0.97834205 1.         1.         0.92307692 0.96607246 0.98302015
 0.97497683 1.         0.97716502 0.88818898 0.99408284]

Kappa:
0.9731726987091804
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f84cced6f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.820, val_acc:0.024]
Epoch [2/120    avg_loss:2.771, val_acc:0.133]
Epoch [3/120    avg_loss:2.712, val_acc:0.190]
Epoch [4/120    avg_loss:2.661, val_acc:0.261]
Epoch [5/120    avg_loss:2.601, val_acc:0.330]
Epoch [6/120    avg_loss:2.556, val_acc:0.378]
Epoch [7/120    avg_loss:2.464, val_acc:0.397]
Epoch [8/120    avg_loss:2.429, val_acc:0.434]
Epoch [9/120    avg_loss:2.367, val_acc:0.505]
Epoch [10/120    avg_loss:2.315, val_acc:0.511]
Epoch [11/120    avg_loss:2.268, val_acc:0.546]
Epoch [12/120    avg_loss:2.196, val_acc:0.576]
Epoch [13/120    avg_loss:2.130, val_acc:0.589]
Epoch [14/120    avg_loss:2.088, val_acc:0.600]
Epoch [15/120    avg_loss:1.969, val_acc:0.572]
Epoch [16/120    avg_loss:1.877, val_acc:0.576]
Epoch [17/120    avg_loss:1.858, val_acc:0.637]
Epoch [18/120    avg_loss:1.674, val_acc:0.651]
Epoch [19/120    avg_loss:1.601, val_acc:0.698]
Epoch [20/120    avg_loss:1.478, val_acc:0.661]
Epoch [21/120    avg_loss:1.327, val_acc:0.695]
Epoch [22/120    avg_loss:1.234, val_acc:0.684]
Epoch [23/120    avg_loss:1.157, val_acc:0.707]
Epoch [24/120    avg_loss:1.103, val_acc:0.733]
Epoch [25/120    avg_loss:0.971, val_acc:0.748]
Epoch [26/120    avg_loss:0.854, val_acc:0.746]
Epoch [27/120    avg_loss:0.780, val_acc:0.783]
Epoch [28/120    avg_loss:0.753, val_acc:0.798]
Epoch [29/120    avg_loss:0.641, val_acc:0.845]
Epoch [30/120    avg_loss:0.573, val_acc:0.836]
Epoch [31/120    avg_loss:0.497, val_acc:0.848]
Epoch [32/120    avg_loss:0.461, val_acc:0.867]
Epoch [33/120    avg_loss:0.406, val_acc:0.848]
Epoch [34/120    avg_loss:0.387, val_acc:0.876]
Epoch [35/120    avg_loss:0.325, val_acc:0.907]
Epoch [36/120    avg_loss:0.294, val_acc:0.903]
Epoch [37/120    avg_loss:0.261, val_acc:0.918]
Epoch [38/120    avg_loss:0.250, val_acc:0.893]
Epoch [39/120    avg_loss:0.215, val_acc:0.942]
Epoch [40/120    avg_loss:0.190, val_acc:0.933]
Epoch [41/120    avg_loss:0.162, val_acc:0.942]
Epoch [42/120    avg_loss:0.155, val_acc:0.947]
Epoch [43/120    avg_loss:0.154, val_acc:0.938]
Epoch [44/120    avg_loss:0.142, val_acc:0.953]
Epoch [45/120    avg_loss:0.146, val_acc:0.947]
Epoch [46/120    avg_loss:0.131, val_acc:0.957]
Epoch [47/120    avg_loss:0.119, val_acc:0.943]
Epoch [48/120    avg_loss:0.108, val_acc:0.939]
Epoch [49/120    avg_loss:0.109, val_acc:0.959]
Epoch [50/120    avg_loss:0.097, val_acc:0.943]
Epoch [51/120    avg_loss:0.106, val_acc:0.945]
Epoch [52/120    avg_loss:0.087, val_acc:0.955]
Epoch [53/120    avg_loss:0.090, val_acc:0.960]
Epoch [54/120    avg_loss:0.081, val_acc:0.954]
Epoch [55/120    avg_loss:0.067, val_acc:0.959]
Epoch [56/120    avg_loss:0.076, val_acc:0.946]
Epoch [57/120    avg_loss:0.081, val_acc:0.952]
Epoch [58/120    avg_loss:0.056, val_acc:0.963]
Epoch [59/120    avg_loss:0.054, val_acc:0.968]
Epoch [60/120    avg_loss:0.054, val_acc:0.963]
Epoch [61/120    avg_loss:0.061, val_acc:0.964]
Epoch [62/120    avg_loss:0.056, val_acc:0.971]
Epoch [63/120    avg_loss:0.045, val_acc:0.973]
Epoch [64/120    avg_loss:0.043, val_acc:0.966]
Epoch [65/120    avg_loss:0.048, val_acc:0.966]
Epoch [66/120    avg_loss:0.065, val_acc:0.959]
Epoch [67/120    avg_loss:0.093, val_acc:0.959]
Epoch [68/120    avg_loss:0.047, val_acc:0.961]
Epoch [69/120    avg_loss:0.052, val_acc:0.951]
Epoch [70/120    avg_loss:0.046, val_acc:0.969]
Epoch [71/120    avg_loss:0.053, val_acc:0.963]
Epoch [72/120    avg_loss:0.050, val_acc:0.955]
Epoch [73/120    avg_loss:0.044, val_acc:0.968]
Epoch [74/120    avg_loss:0.031, val_acc:0.966]
Epoch [75/120    avg_loss:0.031, val_acc:0.968]
Epoch [76/120    avg_loss:0.032, val_acc:0.975]
Epoch [77/120    avg_loss:0.029, val_acc:0.972]
Epoch [78/120    avg_loss:0.022, val_acc:0.963]
Epoch [79/120    avg_loss:0.022, val_acc:0.966]
Epoch [80/120    avg_loss:0.024, val_acc:0.972]
Epoch [81/120    avg_loss:0.024, val_acc:0.978]
Epoch [82/120    avg_loss:0.025, val_acc:0.975]
Epoch [83/120    avg_loss:0.021, val_acc:0.971]
Epoch [84/120    avg_loss:0.019, val_acc:0.975]
Epoch [85/120    avg_loss:0.023, val_acc:0.978]
Epoch [86/120    avg_loss:0.025, val_acc:0.971]
Epoch [87/120    avg_loss:0.024, val_acc:0.972]
Epoch [88/120    avg_loss:0.023, val_acc:0.963]
Epoch [89/120    avg_loss:0.019, val_acc:0.975]
Epoch [90/120    avg_loss:0.016, val_acc:0.976]
Epoch [91/120    avg_loss:0.016, val_acc:0.979]
Epoch [92/120    avg_loss:0.018, val_acc:0.969]
Epoch [93/120    avg_loss:0.019, val_acc:0.975]
Epoch [94/120    avg_loss:0.015, val_acc:0.973]
Epoch [95/120    avg_loss:0.021, val_acc:0.956]
Epoch [96/120    avg_loss:0.032, val_acc:0.970]
Epoch [97/120    avg_loss:0.017, val_acc:0.976]
Epoch [98/120    avg_loss:0.017, val_acc:0.978]
Epoch [99/120    avg_loss:0.016, val_acc:0.975]
Epoch [100/120    avg_loss:0.016, val_acc:0.978]
Epoch [101/120    avg_loss:0.016, val_acc:0.973]
Epoch [102/120    avg_loss:0.014, val_acc:0.979]
Epoch [103/120    avg_loss:0.012, val_acc:0.977]
Epoch [104/120    avg_loss:0.012, val_acc:0.977]
Epoch [105/120    avg_loss:0.013, val_acc:0.980]
Epoch [106/120    avg_loss:0.011, val_acc:0.979]
Epoch [107/120    avg_loss:0.011, val_acc:0.979]
Epoch [108/120    avg_loss:0.010, val_acc:0.977]
Epoch [109/120    avg_loss:0.009, val_acc:0.981]
Epoch [110/120    avg_loss:0.010, val_acc:0.981]
Epoch [111/120    avg_loss:0.011, val_acc:0.979]
Epoch [112/120    avg_loss:0.011, val_acc:0.979]
Epoch [113/120    avg_loss:0.010, val_acc:0.980]
Epoch [114/120    avg_loss:0.010, val_acc:0.981]
Epoch [115/120    avg_loss:0.008, val_acc:0.974]
Epoch [116/120    avg_loss:0.010, val_acc:0.977]
Epoch [117/120    avg_loss:0.009, val_acc:0.978]
Epoch [118/120    avg_loss:0.013, val_acc:0.980]
Epoch [119/120    avg_loss:0.009, val_acc:0.981]
Epoch [120/120    avg_loss:0.008, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1255    0    7    0    1    0    0    0    0   22    0    0
     0    0    0]
 [   0    0    1  715    7    0    1    0    0   11    3    2    7    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   11    0    0    1    1    0    0    0  854    7    0    0
     0    1    0]
 [   0    0    4    0    0    0    1    1    0    0   12 2165   14    0
     0    0   13]
 [   0    0    0    2    0    0    0    0    0    0    1    1  527    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1132    7    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    28  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.96205962059621

F1 scores:
[       nan 0.975      0.98200313 0.97677596 0.96818182 0.99654776
 0.98720843 0.96153846 1.         0.76595745 0.97767602 0.9823049
 0.97322253 1.         0.98392003 0.92749245 0.91208791]

Kappa:
0.9767758027620606
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fef6df8df28>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.815, val_acc:0.044]
Epoch [2/120    avg_loss:2.776, val_acc:0.105]
Epoch [3/120    avg_loss:2.720, val_acc:0.181]
Epoch [4/120    avg_loss:2.677, val_acc:0.216]
Epoch [5/120    avg_loss:2.630, val_acc:0.316]
Epoch [6/120    avg_loss:2.580, val_acc:0.336]
Epoch [7/120    avg_loss:2.479, val_acc:0.393]
Epoch [8/120    avg_loss:2.455, val_acc:0.385]
Epoch [9/120    avg_loss:2.416, val_acc:0.420]
Epoch [10/120    avg_loss:2.357, val_acc:0.411]
Epoch [11/120    avg_loss:2.316, val_acc:0.458]
Epoch [12/120    avg_loss:2.273, val_acc:0.487]
Epoch [13/120    avg_loss:2.183, val_acc:0.505]
Epoch [14/120    avg_loss:2.110, val_acc:0.552]
Epoch [15/120    avg_loss:2.076, val_acc:0.577]
Epoch [16/120    avg_loss:1.987, val_acc:0.548]
Epoch [17/120    avg_loss:1.934, val_acc:0.547]
Epoch [18/120    avg_loss:1.846, val_acc:0.559]
Epoch [19/120    avg_loss:1.696, val_acc:0.601]
Epoch [20/120    avg_loss:1.667, val_acc:0.594]
Epoch [21/120    avg_loss:1.545, val_acc:0.630]
Epoch [22/120    avg_loss:1.432, val_acc:0.681]
Epoch [23/120    avg_loss:1.316, val_acc:0.711]
Epoch [24/120    avg_loss:1.204, val_acc:0.694]
Epoch [25/120    avg_loss:1.115, val_acc:0.721]
Epoch [26/120    avg_loss:1.012, val_acc:0.769]
Epoch [27/120    avg_loss:0.925, val_acc:0.782]
Epoch [28/120    avg_loss:0.864, val_acc:0.781]
Epoch [29/120    avg_loss:0.781, val_acc:0.792]
Epoch [30/120    avg_loss:0.690, val_acc:0.814]
Epoch [31/120    avg_loss:0.660, val_acc:0.830]
Epoch [32/120    avg_loss:0.667, val_acc:0.829]
Epoch [33/120    avg_loss:0.512, val_acc:0.846]
Epoch [34/120    avg_loss:0.418, val_acc:0.880]
Epoch [35/120    avg_loss:0.401, val_acc:0.881]
Epoch [36/120    avg_loss:0.379, val_acc:0.880]
Epoch [37/120    avg_loss:0.321, val_acc:0.900]
Epoch [38/120    avg_loss:0.297, val_acc:0.916]
Epoch [39/120    avg_loss:0.254, val_acc:0.925]
Epoch [40/120    avg_loss:0.261, val_acc:0.920]
Epoch [41/120    avg_loss:0.209, val_acc:0.934]
Epoch [42/120    avg_loss:0.187, val_acc:0.943]
Epoch [43/120    avg_loss:0.189, val_acc:0.941]
Epoch [44/120    avg_loss:0.156, val_acc:0.940]
Epoch [45/120    avg_loss:0.156, val_acc:0.933]
Epoch [46/120    avg_loss:0.151, val_acc:0.943]
Epoch [47/120    avg_loss:0.121, val_acc:0.956]
Epoch [48/120    avg_loss:0.116, val_acc:0.948]
Epoch [49/120    avg_loss:0.104, val_acc:0.951]
Epoch [50/120    avg_loss:0.113, val_acc:0.941]
Epoch [51/120    avg_loss:0.113, val_acc:0.946]
Epoch [52/120    avg_loss:0.116, val_acc:0.948]
Epoch [53/120    avg_loss:0.123, val_acc:0.952]
Epoch [54/120    avg_loss:0.108, val_acc:0.943]
Epoch [55/120    avg_loss:0.082, val_acc:0.953]
Epoch [56/120    avg_loss:0.107, val_acc:0.952]
Epoch [57/120    avg_loss:0.071, val_acc:0.960]
Epoch [58/120    avg_loss:0.074, val_acc:0.957]
Epoch [59/120    avg_loss:0.065, val_acc:0.967]
Epoch [60/120    avg_loss:0.065, val_acc:0.965]
Epoch [61/120    avg_loss:0.058, val_acc:0.970]
Epoch [62/120    avg_loss:0.048, val_acc:0.966]
Epoch [63/120    avg_loss:0.046, val_acc:0.966]
Epoch [64/120    avg_loss:0.043, val_acc:0.965]
Epoch [65/120    avg_loss:0.053, val_acc:0.974]
Epoch [66/120    avg_loss:0.038, val_acc:0.965]
Epoch [67/120    avg_loss:0.056, val_acc:0.951]
Epoch [68/120    avg_loss:0.051, val_acc:0.969]
Epoch [69/120    avg_loss:0.036, val_acc:0.965]
Epoch [70/120    avg_loss:0.041, val_acc:0.967]
Epoch [71/120    avg_loss:0.037, val_acc:0.957]
Epoch [72/120    avg_loss:0.035, val_acc:0.972]
Epoch [73/120    avg_loss:0.033, val_acc:0.973]
Epoch [74/120    avg_loss:0.026, val_acc:0.976]
Epoch [75/120    avg_loss:0.025, val_acc:0.971]
Epoch [76/120    avg_loss:0.023, val_acc:0.975]
Epoch [77/120    avg_loss:0.026, val_acc:0.975]
Epoch [78/120    avg_loss:0.028, val_acc:0.974]
Epoch [79/120    avg_loss:0.023, val_acc:0.976]
Epoch [80/120    avg_loss:0.018, val_acc:0.977]
Epoch [81/120    avg_loss:0.018, val_acc:0.975]
Epoch [82/120    avg_loss:0.025, val_acc:0.971]
Epoch [83/120    avg_loss:0.027, val_acc:0.972]
Epoch [84/120    avg_loss:0.020, val_acc:0.978]
Epoch [85/120    avg_loss:0.020, val_acc:0.975]
Epoch [86/120    avg_loss:0.017, val_acc:0.976]
Epoch [87/120    avg_loss:0.024, val_acc:0.978]
Epoch [88/120    avg_loss:0.016, val_acc:0.978]
Epoch [89/120    avg_loss:0.015, val_acc:0.976]
Epoch [90/120    avg_loss:0.021, val_acc:0.974]
Epoch [91/120    avg_loss:0.018, val_acc:0.977]
Epoch [92/120    avg_loss:0.018, val_acc:0.971]
Epoch [93/120    avg_loss:0.017, val_acc:0.976]
Epoch [94/120    avg_loss:0.025, val_acc:0.974]
Epoch [95/120    avg_loss:0.016, val_acc:0.976]
Epoch [96/120    avg_loss:0.014, val_acc:0.978]
Epoch [97/120    avg_loss:0.013, val_acc:0.976]
Epoch [98/120    avg_loss:0.016, val_acc:0.976]
Epoch [99/120    avg_loss:0.041, val_acc:0.964]
Epoch [100/120    avg_loss:0.042, val_acc:0.953]
Epoch [101/120    avg_loss:0.042, val_acc:0.958]
Epoch [102/120    avg_loss:0.036, val_acc:0.964]
Epoch [103/120    avg_loss:0.036, val_acc:0.960]
Epoch [104/120    avg_loss:0.027, val_acc:0.959]
Epoch [105/120    avg_loss:0.061, val_acc:0.959]
Epoch [106/120    avg_loss:0.051, val_acc:0.958]
Epoch [107/120    avg_loss:0.034, val_acc:0.970]
Epoch [108/120    avg_loss:0.028, val_acc:0.968]
Epoch [109/120    avg_loss:0.022, val_acc:0.966]
Epoch [110/120    avg_loss:0.017, val_acc:0.970]
Epoch [111/120    avg_loss:0.019, val_acc:0.974]
Epoch [112/120    avg_loss:0.014, val_acc:0.972]
Epoch [113/120    avg_loss:0.014, val_acc:0.972]
Epoch [114/120    avg_loss:0.014, val_acc:0.973]
Epoch [115/120    avg_loss:0.015, val_acc:0.973]
Epoch [116/120    avg_loss:0.012, val_acc:0.973]
Epoch [117/120    avg_loss:0.013, val_acc:0.973]
Epoch [118/120    avg_loss:0.014, val_acc:0.974]
Epoch [119/120    avg_loss:0.012, val_acc:0.973]
Epoch [120/120    avg_loss:0.011, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1253    1   12    0    0    0    0    0    7   11    1    0
     0    0    0]
 [   0    0    0  710   16    0    0    0    0    1    3   13    4    0
     0    0    0]
 [   0    0    0    6  207    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  426    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    1    0    0    0    0  860    6    0    0
     1    1    0]
 [   0    0   10    1    0    0    0    0    0    0   13 2161   25    0
     0    0    0]
 [   0    0    0    5    2    0    0    0    0    0   12    1  512    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1132    7    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    52  283    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.37669376693766

F1 scores:
[       nan 0.95238095 0.98120595 0.96598639 0.92       0.99654776
 0.99095023 1.         0.9953271  0.97297297 0.97120271 0.98138056
 0.94902688 0.99728997 0.9733448  0.88714734 0.98203593]

Kappa:
0.9700934043085995
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f82b372ceb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.842, val_acc:0.067]
Epoch [2/120    avg_loss:2.786, val_acc:0.100]
Epoch [3/120    avg_loss:2.730, val_acc:0.124]
Epoch [4/120    avg_loss:2.672, val_acc:0.207]
Epoch [5/120    avg_loss:2.598, val_acc:0.212]
Epoch [6/120    avg_loss:2.550, val_acc:0.332]
Epoch [7/120    avg_loss:2.478, val_acc:0.366]
Epoch [8/120    avg_loss:2.435, val_acc:0.422]
Epoch [9/120    avg_loss:2.389, val_acc:0.488]
Epoch [10/120    avg_loss:2.331, val_acc:0.518]
Epoch [11/120    avg_loss:2.296, val_acc:0.527]
Epoch [12/120    avg_loss:2.208, val_acc:0.574]
Epoch [13/120    avg_loss:2.185, val_acc:0.555]
Epoch [14/120    avg_loss:2.146, val_acc:0.597]
Epoch [15/120    avg_loss:2.031, val_acc:0.612]
Epoch [16/120    avg_loss:1.998, val_acc:0.624]
Epoch [17/120    avg_loss:1.866, val_acc:0.644]
Epoch [18/120    avg_loss:1.771, val_acc:0.675]
Epoch [19/120    avg_loss:1.636, val_acc:0.685]
Epoch [20/120    avg_loss:1.512, val_acc:0.724]
Epoch [21/120    avg_loss:1.374, val_acc:0.694]
Epoch [22/120    avg_loss:1.273, val_acc:0.664]
Epoch [23/120    avg_loss:1.159, val_acc:0.739]
Epoch [24/120    avg_loss:1.105, val_acc:0.736]
Epoch [25/120    avg_loss:1.017, val_acc:0.754]
Epoch [26/120    avg_loss:0.886, val_acc:0.734]
Epoch [27/120    avg_loss:0.789, val_acc:0.777]
Epoch [28/120    avg_loss:0.740, val_acc:0.828]
Epoch [29/120    avg_loss:0.643, val_acc:0.822]
Epoch [30/120    avg_loss:0.541, val_acc:0.871]
Epoch [31/120    avg_loss:0.517, val_acc:0.869]
Epoch [32/120    avg_loss:0.474, val_acc:0.846]
Epoch [33/120    avg_loss:0.453, val_acc:0.879]
Epoch [34/120    avg_loss:0.376, val_acc:0.891]
Epoch [35/120    avg_loss:0.329, val_acc:0.880]
Epoch [36/120    avg_loss:0.302, val_acc:0.906]
Epoch [37/120    avg_loss:0.284, val_acc:0.934]
Epoch [38/120    avg_loss:0.252, val_acc:0.925]
Epoch [39/120    avg_loss:0.221, val_acc:0.938]
Epoch [40/120    avg_loss:0.207, val_acc:0.932]
Epoch [41/120    avg_loss:0.217, val_acc:0.921]
Epoch [42/120    avg_loss:0.192, val_acc:0.943]
Epoch [43/120    avg_loss:0.171, val_acc:0.936]
Epoch [44/120    avg_loss:0.144, val_acc:0.957]
Epoch [45/120    avg_loss:0.131, val_acc:0.955]
Epoch [46/120    avg_loss:0.137, val_acc:0.952]
Epoch [47/120    avg_loss:0.121, val_acc:0.943]
Epoch [48/120    avg_loss:0.117, val_acc:0.954]
Epoch [49/120    avg_loss:0.113, val_acc:0.949]
Epoch [50/120    avg_loss:0.107, val_acc:0.960]
Epoch [51/120    avg_loss:0.099, val_acc:0.960]
Epoch [52/120    avg_loss:0.086, val_acc:0.966]
Epoch [53/120    avg_loss:0.095, val_acc:0.959]
Epoch [54/120    avg_loss:0.087, val_acc:0.956]
Epoch [55/120    avg_loss:0.078, val_acc:0.957]
Epoch [56/120    avg_loss:0.073, val_acc:0.955]
Epoch [57/120    avg_loss:0.079, val_acc:0.967]
Epoch [58/120    avg_loss:0.072, val_acc:0.970]
Epoch [59/120    avg_loss:0.071, val_acc:0.968]
Epoch [60/120    avg_loss:0.055, val_acc:0.972]
Epoch [61/120    avg_loss:0.060, val_acc:0.959]
Epoch [62/120    avg_loss:0.052, val_acc:0.964]
Epoch [63/120    avg_loss:0.052, val_acc:0.941]
Epoch [64/120    avg_loss:0.054, val_acc:0.970]
Epoch [65/120    avg_loss:0.063, val_acc:0.954]
Epoch [66/120    avg_loss:0.097, val_acc:0.914]
Epoch [67/120    avg_loss:0.111, val_acc:0.954]
Epoch [68/120    avg_loss:0.075, val_acc:0.970]
Epoch [69/120    avg_loss:0.055, val_acc:0.966]
Epoch [70/120    avg_loss:0.061, val_acc:0.966]
Epoch [71/120    avg_loss:0.040, val_acc:0.970]
Epoch [72/120    avg_loss:0.034, val_acc:0.975]
Epoch [73/120    avg_loss:0.033, val_acc:0.967]
Epoch [74/120    avg_loss:0.035, val_acc:0.972]
Epoch [75/120    avg_loss:0.033, val_acc:0.968]
Epoch [76/120    avg_loss:0.025, val_acc:0.970]
Epoch [77/120    avg_loss:0.033, val_acc:0.973]
Epoch [78/120    avg_loss:0.027, val_acc:0.975]
Epoch [79/120    avg_loss:0.023, val_acc:0.973]
Epoch [80/120    avg_loss:0.023, val_acc:0.973]
Epoch [81/120    avg_loss:0.027, val_acc:0.971]
Epoch [82/120    avg_loss:0.034, val_acc:0.968]
Epoch [83/120    avg_loss:0.022, val_acc:0.972]
Epoch [84/120    avg_loss:0.029, val_acc:0.964]
Epoch [85/120    avg_loss:0.025, val_acc:0.976]
Epoch [86/120    avg_loss:0.021, val_acc:0.972]
Epoch [87/120    avg_loss:0.021, val_acc:0.974]
Epoch [88/120    avg_loss:0.018, val_acc:0.974]
Epoch [89/120    avg_loss:0.019, val_acc:0.966]
Epoch [90/120    avg_loss:0.021, val_acc:0.972]
Epoch [91/120    avg_loss:0.022, val_acc:0.971]
Epoch [92/120    avg_loss:0.032, val_acc:0.967]
Epoch [93/120    avg_loss:0.018, val_acc:0.973]
Epoch [94/120    avg_loss:0.024, val_acc:0.972]
Epoch [95/120    avg_loss:0.021, val_acc:0.975]
Epoch [96/120    avg_loss:0.023, val_acc:0.970]
Epoch [97/120    avg_loss:0.023, val_acc:0.967]
Epoch [98/120    avg_loss:0.018, val_acc:0.974]
Epoch [99/120    avg_loss:0.017, val_acc:0.976]
Epoch [100/120    avg_loss:0.014, val_acc:0.975]
Epoch [101/120    avg_loss:0.012, val_acc:0.975]
Epoch [102/120    avg_loss:0.014, val_acc:0.975]
Epoch [103/120    avg_loss:0.013, val_acc:0.974]
Epoch [104/120    avg_loss:0.012, val_acc:0.974]
Epoch [105/120    avg_loss:0.012, val_acc:0.974]
Epoch [106/120    avg_loss:0.010, val_acc:0.974]
Epoch [107/120    avg_loss:0.012, val_acc:0.975]
Epoch [108/120    avg_loss:0.014, val_acc:0.975]
Epoch [109/120    avg_loss:0.010, val_acc:0.975]
Epoch [110/120    avg_loss:0.011, val_acc:0.975]
Epoch [111/120    avg_loss:0.011, val_acc:0.975]
Epoch [112/120    avg_loss:0.011, val_acc:0.975]
Epoch [113/120    avg_loss:0.012, val_acc:0.975]
Epoch [114/120    avg_loss:0.011, val_acc:0.974]
Epoch [115/120    avg_loss:0.011, val_acc:0.974]
Epoch [116/120    avg_loss:0.011, val_acc:0.974]
Epoch [117/120    avg_loss:0.018, val_acc:0.974]
Epoch [118/120    avg_loss:0.011, val_acc:0.974]
Epoch [119/120    avg_loss:0.011, val_acc:0.974]
Epoch [120/120    avg_loss:0.011, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1245    2    6    0    0    0    0    0    4   28    0    0
     0    0    0]
 [   0    0    0  722   11    0    0    0    0    3    1    2    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    3    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    1    0    0    0    0  850   10    5    0
     1    0    0]
 [   0    0   17    0    0    0    1    0    0    0    9 2159   21    1
     0    2    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  531    0
     2    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1129    9    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    38  290    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.57181571815718

F1 scores:
[       nan 0.975      0.97379742 0.98164514 0.96162528 0.99307159
 0.98203593 1.         0.9953271  0.9        0.97757332 0.97913832
 0.96195652 0.99730458 0.97706621 0.89506173 0.98809524]

Kappa:
0.9723201923634112
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f96529cef28>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.830, val_acc:0.011]
Epoch [2/120    avg_loss:2.781, val_acc:0.061]
Epoch [3/120    avg_loss:2.723, val_acc:0.205]
Epoch [4/120    avg_loss:2.658, val_acc:0.310]
Epoch [5/120    avg_loss:2.591, val_acc:0.420]
Epoch [6/120    avg_loss:2.519, val_acc:0.441]
Epoch [7/120    avg_loss:2.443, val_acc:0.439]
Epoch [8/120    avg_loss:2.400, val_acc:0.460]
Epoch [9/120    avg_loss:2.345, val_acc:0.424]
Epoch [10/120    avg_loss:2.271, val_acc:0.492]
Epoch [11/120    avg_loss:2.211, val_acc:0.493]
Epoch [12/120    avg_loss:2.134, val_acc:0.517]
Epoch [13/120    avg_loss:2.088, val_acc:0.542]
Epoch [14/120    avg_loss:1.986, val_acc:0.507]
Epoch [15/120    avg_loss:1.868, val_acc:0.530]
Epoch [16/120    avg_loss:1.711, val_acc:0.539]
Epoch [17/120    avg_loss:1.637, val_acc:0.577]
Epoch [18/120    avg_loss:1.546, val_acc:0.595]
Epoch [19/120    avg_loss:1.386, val_acc:0.668]
Epoch [20/120    avg_loss:1.314, val_acc:0.620]
Epoch [21/120    avg_loss:1.261, val_acc:0.708]
Epoch [22/120    avg_loss:1.205, val_acc:0.728]
Epoch [23/120    avg_loss:1.073, val_acc:0.692]
Epoch [24/120    avg_loss:0.957, val_acc:0.768]
Epoch [25/120    avg_loss:0.896, val_acc:0.777]
Epoch [26/120    avg_loss:0.800, val_acc:0.780]
Epoch [27/120    avg_loss:0.740, val_acc:0.797]
Epoch [28/120    avg_loss:0.656, val_acc:0.795]
Epoch [29/120    avg_loss:0.589, val_acc:0.822]
Epoch [30/120    avg_loss:0.537, val_acc:0.823]
Epoch [31/120    avg_loss:0.494, val_acc:0.826]
Epoch [32/120    avg_loss:0.475, val_acc:0.859]
Epoch [33/120    avg_loss:0.412, val_acc:0.854]
Epoch [34/120    avg_loss:0.436, val_acc:0.876]
Epoch [35/120    avg_loss:0.385, val_acc:0.859]
Epoch [36/120    avg_loss:0.365, val_acc:0.902]
Epoch [37/120    avg_loss:0.311, val_acc:0.874]
Epoch [38/120    avg_loss:0.283, val_acc:0.899]
Epoch [39/120    avg_loss:0.290, val_acc:0.924]
Epoch [40/120    avg_loss:0.274, val_acc:0.919]
Epoch [41/120    avg_loss:0.277, val_acc:0.914]
Epoch [42/120    avg_loss:0.236, val_acc:0.914]
Epoch [43/120    avg_loss:0.165, val_acc:0.943]
Epoch [44/120    avg_loss:0.194, val_acc:0.934]
Epoch [45/120    avg_loss:0.171, val_acc:0.933]
Epoch [46/120    avg_loss:0.198, val_acc:0.928]
Epoch [47/120    avg_loss:0.198, val_acc:0.942]
Epoch [48/120    avg_loss:0.183, val_acc:0.931]
Epoch [49/120    avg_loss:0.139, val_acc:0.922]
Epoch [50/120    avg_loss:0.147, val_acc:0.954]
Epoch [51/120    avg_loss:0.126, val_acc:0.956]
Epoch [52/120    avg_loss:0.109, val_acc:0.956]
Epoch [53/120    avg_loss:0.112, val_acc:0.950]
Epoch [54/120    avg_loss:0.114, val_acc:0.941]
Epoch [55/120    avg_loss:0.093, val_acc:0.956]
Epoch [56/120    avg_loss:0.087, val_acc:0.949]
Epoch [57/120    avg_loss:0.085, val_acc:0.955]
Epoch [58/120    avg_loss:0.077, val_acc:0.959]
Epoch [59/120    avg_loss:0.073, val_acc:0.954]
Epoch [60/120    avg_loss:0.072, val_acc:0.967]
Epoch [61/120    avg_loss:0.101, val_acc:0.947]
Epoch [62/120    avg_loss:0.095, val_acc:0.949]
Epoch [63/120    avg_loss:0.076, val_acc:0.959]
Epoch [64/120    avg_loss:0.064, val_acc:0.958]
Epoch [65/120    avg_loss:0.074, val_acc:0.954]
Epoch [66/120    avg_loss:0.054, val_acc:0.952]
Epoch [67/120    avg_loss:0.052, val_acc:0.961]
Epoch [68/120    avg_loss:0.045, val_acc:0.961]
Epoch [69/120    avg_loss:0.058, val_acc:0.960]
Epoch [70/120    avg_loss:0.051, val_acc:0.968]
Epoch [71/120    avg_loss:0.036, val_acc:0.970]
Epoch [72/120    avg_loss:0.036, val_acc:0.965]
Epoch [73/120    avg_loss:0.040, val_acc:0.975]
Epoch [74/120    avg_loss:0.039, val_acc:0.960]
Epoch [75/120    avg_loss:0.035, val_acc:0.973]
Epoch [76/120    avg_loss:0.034, val_acc:0.969]
Epoch [77/120    avg_loss:0.031, val_acc:0.951]
Epoch [78/120    avg_loss:0.037, val_acc:0.970]
Epoch [79/120    avg_loss:0.032, val_acc:0.967]
Epoch [80/120    avg_loss:0.038, val_acc:0.961]
Epoch [81/120    avg_loss:0.036, val_acc:0.974]
Epoch [82/120    avg_loss:0.038, val_acc:0.971]
Epoch [83/120    avg_loss:0.029, val_acc:0.950]
Epoch [84/120    avg_loss:0.026, val_acc:0.974]
Epoch [85/120    avg_loss:0.024, val_acc:0.974]
Epoch [86/120    avg_loss:0.022, val_acc:0.973]
Epoch [87/120    avg_loss:0.026, val_acc:0.975]
Epoch [88/120    avg_loss:0.022, val_acc:0.976]
Epoch [89/120    avg_loss:0.018, val_acc:0.976]
Epoch [90/120    avg_loss:0.016, val_acc:0.974]
Epoch [91/120    avg_loss:0.015, val_acc:0.976]
Epoch [92/120    avg_loss:0.017, val_acc:0.977]
Epoch [93/120    avg_loss:0.018, val_acc:0.977]
Epoch [94/120    avg_loss:0.017, val_acc:0.976]
Epoch [95/120    avg_loss:0.019, val_acc:0.974]
Epoch [96/120    avg_loss:0.017, val_acc:0.974]
Epoch [97/120    avg_loss:0.017, val_acc:0.975]
Epoch [98/120    avg_loss:0.016, val_acc:0.977]
Epoch [99/120    avg_loss:0.019, val_acc:0.977]
Epoch [100/120    avg_loss:0.019, val_acc:0.976]
Epoch [101/120    avg_loss:0.018, val_acc:0.973]
Epoch [102/120    avg_loss:0.016, val_acc:0.975]
Epoch [103/120    avg_loss:0.019, val_acc:0.977]
Epoch [104/120    avg_loss:0.014, val_acc:0.974]
Epoch [105/120    avg_loss:0.016, val_acc:0.976]
Epoch [106/120    avg_loss:0.016, val_acc:0.975]
Epoch [107/120    avg_loss:0.015, val_acc:0.975]
Epoch [108/120    avg_loss:0.015, val_acc:0.974]
Epoch [109/120    avg_loss:0.018, val_acc:0.972]
Epoch [110/120    avg_loss:0.016, val_acc:0.976]
Epoch [111/120    avg_loss:0.014, val_acc:0.974]
Epoch [112/120    avg_loss:0.019, val_acc:0.977]
Epoch [113/120    avg_loss:0.014, val_acc:0.977]
Epoch [114/120    avg_loss:0.014, val_acc:0.975]
Epoch [115/120    avg_loss:0.014, val_acc:0.975]
Epoch [116/120    avg_loss:0.018, val_acc:0.975]
Epoch [117/120    avg_loss:0.017, val_acc:0.977]
Epoch [118/120    avg_loss:0.015, val_acc:0.977]
Epoch [119/120    avg_loss:0.015, val_acc:0.974]
Epoch [120/120    avg_loss:0.015, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1257    0    6    0    0    0    0    1    0   21    0    0
     0    0    0]
 [   0    0    0  710    6    3    0    0    0    9    4    7    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    6    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    1    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   14    0    0    2    3    0    0    0  848    5    2    0
     1    0    0]
 [   0    0   13    0    0    0    2    0    0    1    3 2186    5    0
     0    0    0]
 [   0    0    0    5    0    3    0    0    0    0    5    0  514    0
     4    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    1    0    0
  1126   11    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    13  328    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.95121951219512

F1 scores:
[       nan 0.96202532 0.97859089 0.97127223 0.97260274 0.97813579
 0.98940998 0.89285714 0.9953271  0.75       0.97527315 0.98646209
 0.96344892 1.         0.98469611 0.95626822 0.97647059]

Kappa:
0.9766405666870371
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8277853e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.844, val_acc:0.088]
Epoch [2/120    avg_loss:2.789, val_acc:0.237]
Epoch [3/120    avg_loss:2.724, val_acc:0.312]
Epoch [4/120    avg_loss:2.661, val_acc:0.403]
Epoch [5/120    avg_loss:2.603, val_acc:0.477]
Epoch [6/120    avg_loss:2.544, val_acc:0.472]
Epoch [7/120    avg_loss:2.465, val_acc:0.498]
Epoch [8/120    avg_loss:2.393, val_acc:0.501]
Epoch [9/120    avg_loss:2.329, val_acc:0.510]
Epoch [10/120    avg_loss:2.264, val_acc:0.523]
Epoch [11/120    avg_loss:2.257, val_acc:0.540]
Epoch [12/120    avg_loss:2.173, val_acc:0.532]
Epoch [13/120    avg_loss:2.114, val_acc:0.538]
Epoch [14/120    avg_loss:2.048, val_acc:0.569]
Epoch [15/120    avg_loss:1.957, val_acc:0.583]
Epoch [16/120    avg_loss:1.871, val_acc:0.609]
Epoch [17/120    avg_loss:1.783, val_acc:0.637]
Epoch [18/120    avg_loss:1.761, val_acc:0.637]
Epoch [19/120    avg_loss:1.657, val_acc:0.669]
Epoch [20/120    avg_loss:1.523, val_acc:0.671]
Epoch [21/120    avg_loss:1.374, val_acc:0.692]
Epoch [22/120    avg_loss:1.269, val_acc:0.700]
Epoch [23/120    avg_loss:1.163, val_acc:0.721]
Epoch [24/120    avg_loss:1.085, val_acc:0.763]
Epoch [25/120    avg_loss:1.005, val_acc:0.722]
Epoch [26/120    avg_loss:0.950, val_acc:0.789]
Epoch [27/120    avg_loss:0.831, val_acc:0.824]
Epoch [28/120    avg_loss:0.777, val_acc:0.804]
Epoch [29/120    avg_loss:0.706, val_acc:0.821]
Epoch [30/120    avg_loss:0.695, val_acc:0.833]
Epoch [31/120    avg_loss:0.597, val_acc:0.875]
Epoch [32/120    avg_loss:0.521, val_acc:0.885]
Epoch [33/120    avg_loss:0.477, val_acc:0.890]
Epoch [34/120    avg_loss:0.414, val_acc:0.894]
Epoch [35/120    avg_loss:0.363, val_acc:0.923]
Epoch [36/120    avg_loss:0.346, val_acc:0.915]
Epoch [37/120    avg_loss:0.289, val_acc:0.900]
Epoch [38/120    avg_loss:0.254, val_acc:0.906]
Epoch [39/120    avg_loss:0.225, val_acc:0.941]
Epoch [40/120    avg_loss:0.204, val_acc:0.925]
Epoch [41/120    avg_loss:0.242, val_acc:0.906]
Epoch [42/120    avg_loss:0.207, val_acc:0.940]
Epoch [43/120    avg_loss:0.173, val_acc:0.926]
Epoch [44/120    avg_loss:0.168, val_acc:0.918]
Epoch [45/120    avg_loss:0.173, val_acc:0.930]
Epoch [46/120    avg_loss:0.177, val_acc:0.921]
Epoch [47/120    avg_loss:0.179, val_acc:0.943]
Epoch [48/120    avg_loss:0.155, val_acc:0.939]
Epoch [49/120    avg_loss:0.138, val_acc:0.947]
Epoch [50/120    avg_loss:0.112, val_acc:0.944]
Epoch [51/120    avg_loss:0.130, val_acc:0.946]
Epoch [52/120    avg_loss:0.164, val_acc:0.916]
Epoch [53/120    avg_loss:0.138, val_acc:0.926]
Epoch [54/120    avg_loss:0.110, val_acc:0.918]
Epoch [55/120    avg_loss:0.105, val_acc:0.964]
Epoch [56/120    avg_loss:0.084, val_acc:0.959]
Epoch [57/120    avg_loss:0.084, val_acc:0.963]
Epoch [58/120    avg_loss:0.073, val_acc:0.941]
Epoch [59/120    avg_loss:0.078, val_acc:0.962]
Epoch [60/120    avg_loss:0.063, val_acc:0.971]
Epoch [61/120    avg_loss:0.068, val_acc:0.966]
Epoch [62/120    avg_loss:0.062, val_acc:0.964]
Epoch [63/120    avg_loss:0.069, val_acc:0.950]
Epoch [64/120    avg_loss:0.063, val_acc:0.966]
Epoch [65/120    avg_loss:0.055, val_acc:0.958]
Epoch [66/120    avg_loss:0.055, val_acc:0.960]
Epoch [67/120    avg_loss:0.055, val_acc:0.968]
Epoch [68/120    avg_loss:0.050, val_acc:0.960]
Epoch [69/120    avg_loss:0.042, val_acc:0.973]
Epoch [70/120    avg_loss:0.042, val_acc:0.976]
Epoch [71/120    avg_loss:0.040, val_acc:0.976]
Epoch [72/120    avg_loss:0.041, val_acc:0.961]
Epoch [73/120    avg_loss:0.040, val_acc:0.968]
Epoch [74/120    avg_loss:0.038, val_acc:0.975]
Epoch [75/120    avg_loss:0.030, val_acc:0.976]
Epoch [76/120    avg_loss:0.036, val_acc:0.969]
Epoch [77/120    avg_loss:0.031, val_acc:0.976]
Epoch [78/120    avg_loss:0.030, val_acc:0.972]
Epoch [79/120    avg_loss:0.047, val_acc:0.962]
Epoch [80/120    avg_loss:0.037, val_acc:0.975]
Epoch [81/120    avg_loss:0.026, val_acc:0.969]
Epoch [82/120    avg_loss:0.034, val_acc:0.970]
Epoch [83/120    avg_loss:0.027, val_acc:0.977]
Epoch [84/120    avg_loss:0.023, val_acc:0.975]
Epoch [85/120    avg_loss:0.027, val_acc:0.985]
Epoch [86/120    avg_loss:0.022, val_acc:0.975]
Epoch [87/120    avg_loss:0.024, val_acc:0.980]
Epoch [88/120    avg_loss:0.020, val_acc:0.966]
Epoch [89/120    avg_loss:0.024, val_acc:0.974]
Epoch [90/120    avg_loss:0.024, val_acc:0.975]
Epoch [91/120    avg_loss:0.022, val_acc:0.977]
Epoch [92/120    avg_loss:0.018, val_acc:0.977]
Epoch [93/120    avg_loss:0.017, val_acc:0.975]
Epoch [94/120    avg_loss:0.028, val_acc:0.975]
Epoch [95/120    avg_loss:0.022, val_acc:0.972]
Epoch [96/120    avg_loss:0.018, val_acc:0.976]
Epoch [97/120    avg_loss:0.020, val_acc:0.970]
Epoch [98/120    avg_loss:0.015, val_acc:0.977]
Epoch [99/120    avg_loss:0.012, val_acc:0.980]
Epoch [100/120    avg_loss:0.014, val_acc:0.980]
Epoch [101/120    avg_loss:0.013, val_acc:0.980]
Epoch [102/120    avg_loss:0.012, val_acc:0.980]
Epoch [103/120    avg_loss:0.013, val_acc:0.980]
Epoch [104/120    avg_loss:0.012, val_acc:0.977]
Epoch [105/120    avg_loss:0.012, val_acc:0.977]
Epoch [106/120    avg_loss:0.013, val_acc:0.977]
Epoch [107/120    avg_loss:0.013, val_acc:0.978]
Epoch [108/120    avg_loss:0.012, val_acc:0.978]
Epoch [109/120    avg_loss:0.011, val_acc:0.980]
Epoch [110/120    avg_loss:0.012, val_acc:0.978]
Epoch [111/120    avg_loss:0.011, val_acc:0.978]
Epoch [112/120    avg_loss:0.012, val_acc:0.980]
Epoch [113/120    avg_loss:0.013, val_acc:0.978]
Epoch [114/120    avg_loss:0.013, val_acc:0.978]
Epoch [115/120    avg_loss:0.010, val_acc:0.978]
Epoch [116/120    avg_loss:0.013, val_acc:0.978]
Epoch [117/120    avg_loss:0.010, val_acc:0.980]
Epoch [118/120    avg_loss:0.010, val_acc:0.978]
Epoch [119/120    avg_loss:0.012, val_acc:0.978]
Epoch [120/120    avg_loss:0.012, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1244    1    7    0    0    0    0    0    3   27    3    0
     0    0    0]
 [   0    0    0  711    4    4    3    0    0   15    4    4    2    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  431    2    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    3    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    2    0    0    0    0  843   26    0    0
     0    1    0]
 [   0    0    6    0    0    0    2    0    0    0    4 2186   12    0
     0    0    0]
 [   0    0    0    2    1    5    2    0    0    0    3    8  509    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    1    0    0    0    0    0    0    0
  1129    5    0]
 [   0    0    0    0    0    0   39    0    0    0    0    0    0    0
    12  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.51761517615176

F1 scores:
[       nan 1.         0.98029945 0.97264022 0.97025172 0.9784336
 0.96328928 1.         0.99649942 0.68       0.97344111 0.97939068
 0.9594722  1.         0.98818381 0.91217257 0.98823529]

Kappa:
0.9716822916458615
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd682055ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.836, val_acc:0.190]
Epoch [2/120    avg_loss:2.791, val_acc:0.219]
Epoch [3/120    avg_loss:2.741, val_acc:0.220]
Epoch [4/120    avg_loss:2.693, val_acc:0.252]
Epoch [5/120    avg_loss:2.615, val_acc:0.306]
Epoch [6/120    avg_loss:2.550, val_acc:0.404]
Epoch [7/120    avg_loss:2.490, val_acc:0.458]
Epoch [8/120    avg_loss:2.427, val_acc:0.460]
Epoch [9/120    avg_loss:2.352, val_acc:0.497]
Epoch [10/120    avg_loss:2.333, val_acc:0.539]
Epoch [11/120    avg_loss:2.261, val_acc:0.568]
Epoch [12/120    avg_loss:2.193, val_acc:0.586]
Epoch [13/120    avg_loss:2.145, val_acc:0.580]
Epoch [14/120    avg_loss:2.060, val_acc:0.586]
Epoch [15/120    avg_loss:2.050, val_acc:0.602]
Epoch [16/120    avg_loss:1.949, val_acc:0.623]
Epoch [17/120    avg_loss:1.859, val_acc:0.641]
Epoch [18/120    avg_loss:1.728, val_acc:0.681]
Epoch [19/120    avg_loss:1.633, val_acc:0.713]
Epoch [20/120    avg_loss:1.506, val_acc:0.683]
Epoch [21/120    avg_loss:1.388, val_acc:0.716]
Epoch [22/120    avg_loss:1.328, val_acc:0.765]
Epoch [23/120    avg_loss:1.161, val_acc:0.736]
Epoch [24/120    avg_loss:1.073, val_acc:0.778]
Epoch [25/120    avg_loss:0.995, val_acc:0.790]
Epoch [26/120    avg_loss:0.921, val_acc:0.782]
Epoch [27/120    avg_loss:0.818, val_acc:0.791]
Epoch [28/120    avg_loss:0.769, val_acc:0.814]
Epoch [29/120    avg_loss:0.696, val_acc:0.850]
Epoch [30/120    avg_loss:0.731, val_acc:0.827]
Epoch [31/120    avg_loss:0.599, val_acc:0.841]
Epoch [32/120    avg_loss:0.582, val_acc:0.856]
Epoch [33/120    avg_loss:0.485, val_acc:0.850]
Epoch [34/120    avg_loss:0.438, val_acc:0.876]
Epoch [35/120    avg_loss:0.386, val_acc:0.883]
Epoch [36/120    avg_loss:0.320, val_acc:0.900]
Epoch [37/120    avg_loss:0.356, val_acc:0.885]
Epoch [38/120    avg_loss:0.304, val_acc:0.892]
Epoch [39/120    avg_loss:0.289, val_acc:0.915]
Epoch [40/120    avg_loss:0.228, val_acc:0.914]
Epoch [41/120    avg_loss:0.239, val_acc:0.912]
Epoch [42/120    avg_loss:0.215, val_acc:0.925]
Epoch [43/120    avg_loss:0.251, val_acc:0.838]
Epoch [44/120    avg_loss:0.231, val_acc:0.930]
Epoch [45/120    avg_loss:0.219, val_acc:0.923]
Epoch [46/120    avg_loss:0.179, val_acc:0.907]
Epoch [47/120    avg_loss:0.161, val_acc:0.938]
Epoch [48/120    avg_loss:0.142, val_acc:0.939]
Epoch [49/120    avg_loss:0.151, val_acc:0.938]
Epoch [50/120    avg_loss:0.120, val_acc:0.939]
Epoch [51/120    avg_loss:0.135, val_acc:0.938]
Epoch [52/120    avg_loss:0.115, val_acc:0.940]
Epoch [53/120    avg_loss:0.100, val_acc:0.947]
Epoch [54/120    avg_loss:0.086, val_acc:0.959]
Epoch [55/120    avg_loss:0.124, val_acc:0.926]
Epoch [56/120    avg_loss:0.126, val_acc:0.958]
Epoch [57/120    avg_loss:0.083, val_acc:0.962]
Epoch [58/120    avg_loss:0.091, val_acc:0.955]
Epoch [59/120    avg_loss:0.085, val_acc:0.962]
Epoch [60/120    avg_loss:0.073, val_acc:0.957]
Epoch [61/120    avg_loss:0.067, val_acc:0.961]
Epoch [62/120    avg_loss:0.063, val_acc:0.962]
Epoch [63/120    avg_loss:0.071, val_acc:0.958]
Epoch [64/120    avg_loss:0.070, val_acc:0.960]
Epoch [65/120    avg_loss:0.058, val_acc:0.963]
Epoch [66/120    avg_loss:0.064, val_acc:0.966]
Epoch [67/120    avg_loss:0.076, val_acc:0.955]
Epoch [68/120    avg_loss:0.077, val_acc:0.958]
Epoch [69/120    avg_loss:0.067, val_acc:0.957]
Epoch [70/120    avg_loss:0.055, val_acc:0.957]
Epoch [71/120    avg_loss:0.041, val_acc:0.967]
Epoch [72/120    avg_loss:0.051, val_acc:0.962]
Epoch [73/120    avg_loss:0.041, val_acc:0.962]
Epoch [74/120    avg_loss:0.041, val_acc:0.966]
Epoch [75/120    avg_loss:0.036, val_acc:0.953]
Epoch [76/120    avg_loss:0.034, val_acc:0.970]
Epoch [77/120    avg_loss:0.039, val_acc:0.964]
Epoch [78/120    avg_loss:0.041, val_acc:0.971]
Epoch [79/120    avg_loss:0.039, val_acc:0.970]
Epoch [80/120    avg_loss:0.032, val_acc:0.971]
Epoch [81/120    avg_loss:0.032, val_acc:0.968]
Epoch [82/120    avg_loss:0.030, val_acc:0.971]
Epoch [83/120    avg_loss:0.033, val_acc:0.978]
Epoch [84/120    avg_loss:0.026, val_acc:0.972]
Epoch [85/120    avg_loss:0.030, val_acc:0.978]
Epoch [86/120    avg_loss:0.035, val_acc:0.976]
Epoch [87/120    avg_loss:0.032, val_acc:0.946]
Epoch [88/120    avg_loss:0.033, val_acc:0.971]
Epoch [89/120    avg_loss:0.034, val_acc:0.962]
Epoch [90/120    avg_loss:0.038, val_acc:0.967]
Epoch [91/120    avg_loss:0.037, val_acc:0.970]
Epoch [92/120    avg_loss:0.031, val_acc:0.950]
Epoch [93/120    avg_loss:0.033, val_acc:0.980]
Epoch [94/120    avg_loss:0.022, val_acc:0.976]
Epoch [95/120    avg_loss:0.020, val_acc:0.975]
Epoch [96/120    avg_loss:0.019, val_acc:0.969]
Epoch [97/120    avg_loss:0.022, val_acc:0.968]
Epoch [98/120    avg_loss:0.033, val_acc:0.969]
Epoch [99/120    avg_loss:0.049, val_acc:0.966]
Epoch [100/120    avg_loss:0.029, val_acc:0.977]
Epoch [101/120    avg_loss:0.020, val_acc:0.975]
Epoch [102/120    avg_loss:0.015, val_acc:0.975]
Epoch [103/120    avg_loss:0.016, val_acc:0.978]
Epoch [104/120    avg_loss:0.020, val_acc:0.971]
Epoch [105/120    avg_loss:0.015, val_acc:0.975]
Epoch [106/120    avg_loss:0.022, val_acc:0.958]
Epoch [107/120    avg_loss:0.025, val_acc:0.969]
Epoch [108/120    avg_loss:0.014, val_acc:0.970]
Epoch [109/120    avg_loss:0.015, val_acc:0.972]
Epoch [110/120    avg_loss:0.013, val_acc:0.973]
Epoch [111/120    avg_loss:0.013, val_acc:0.971]
Epoch [112/120    avg_loss:0.011, val_acc:0.974]
Epoch [113/120    avg_loss:0.012, val_acc:0.974]
Epoch [114/120    avg_loss:0.014, val_acc:0.974]
Epoch [115/120    avg_loss:0.014, val_acc:0.974]
Epoch [116/120    avg_loss:0.012, val_acc:0.974]
Epoch [117/120    avg_loss:0.014, val_acc:0.975]
Epoch [118/120    avg_loss:0.011, val_acc:0.976]
Epoch [119/120    avg_loss:0.013, val_acc:0.974]
Epoch [120/120    avg_loss:0.012, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1259    2    2    0    0    0    0    0    2   20    0    0
     0    0    0]
 [   0    0    7  712    8    4    1    0    0    3    3    4    1    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   16    0    0    1    0
     0    0    0]
 [   0    0    4    0    0    1    1    0    0    0  850   18    1    0
     0    0    0]
 [   0    0    6    0    0    0    1    0    0    0    4 2193    6    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    5    8  518    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1135    4    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    10  336    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.44986449864498

F1 scores:
[       nan 0.95238095 0.98282592 0.97467488 0.97706422 0.99313501
 0.99468489 1.         0.99649942 0.84210526 0.97757332 0.98473282
 0.97643732 0.98930481 0.99343545 0.97816594 0.98245614]

Kappa:
0.9823180341173778
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4e86cb4e48>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.811, val_acc:0.208]
Epoch [2/120    avg_loss:2.754, val_acc:0.214]
Epoch [3/120    avg_loss:2.695, val_acc:0.237]
Epoch [4/120    avg_loss:2.622, val_acc:0.286]
Epoch [5/120    avg_loss:2.564, val_acc:0.306]
Epoch [6/120    avg_loss:2.484, val_acc:0.329]
Epoch [7/120    avg_loss:2.419, val_acc:0.358]
Epoch [8/120    avg_loss:2.374, val_acc:0.407]
Epoch [9/120    avg_loss:2.353, val_acc:0.390]
Epoch [10/120    avg_loss:2.260, val_acc:0.414]
Epoch [11/120    avg_loss:2.206, val_acc:0.419]
Epoch [12/120    avg_loss:2.160, val_acc:0.448]
Epoch [13/120    avg_loss:2.095, val_acc:0.487]
Epoch [14/120    avg_loss:1.986, val_acc:0.501]
Epoch [15/120    avg_loss:1.972, val_acc:0.500]
Epoch [16/120    avg_loss:1.827, val_acc:0.527]
Epoch [17/120    avg_loss:1.722, val_acc:0.578]
Epoch [18/120    avg_loss:1.583, val_acc:0.574]
Epoch [19/120    avg_loss:1.487, val_acc:0.588]
Epoch [20/120    avg_loss:1.429, val_acc:0.654]
Epoch [21/120    avg_loss:1.259, val_acc:0.644]
Epoch [22/120    avg_loss:1.176, val_acc:0.692]
Epoch [23/120    avg_loss:1.090, val_acc:0.698]
Epoch [24/120    avg_loss:0.977, val_acc:0.677]
Epoch [25/120    avg_loss:0.911, val_acc:0.737]
Epoch [26/120    avg_loss:0.888, val_acc:0.642]
Epoch [27/120    avg_loss:0.840, val_acc:0.767]
Epoch [28/120    avg_loss:0.686, val_acc:0.776]
Epoch [29/120    avg_loss:0.601, val_acc:0.810]
Epoch [30/120    avg_loss:0.500, val_acc:0.803]
Epoch [31/120    avg_loss:0.473, val_acc:0.824]
Epoch [32/120    avg_loss:0.431, val_acc:0.851]
Epoch [33/120    avg_loss:0.363, val_acc:0.859]
Epoch [34/120    avg_loss:0.365, val_acc:0.845]
Epoch [35/120    avg_loss:0.299, val_acc:0.881]
Epoch [36/120    avg_loss:0.280, val_acc:0.885]
Epoch [37/120    avg_loss:0.237, val_acc:0.904]
Epoch [38/120    avg_loss:0.216, val_acc:0.897]
Epoch [39/120    avg_loss:0.187, val_acc:0.923]
Epoch [40/120    avg_loss:0.188, val_acc:0.911]
Epoch [41/120    avg_loss:0.188, val_acc:0.912]
Epoch [42/120    avg_loss:0.159, val_acc:0.934]
Epoch [43/120    avg_loss:0.152, val_acc:0.931]
Epoch [44/120    avg_loss:0.137, val_acc:0.939]
Epoch [45/120    avg_loss:0.124, val_acc:0.931]
Epoch [46/120    avg_loss:0.125, val_acc:0.932]
Epoch [47/120    avg_loss:0.108, val_acc:0.936]
Epoch [48/120    avg_loss:0.116, val_acc:0.940]
Epoch [49/120    avg_loss:0.109, val_acc:0.930]
Epoch [50/120    avg_loss:0.103, val_acc:0.950]
Epoch [51/120    avg_loss:0.101, val_acc:0.952]
Epoch [52/120    avg_loss:0.101, val_acc:0.943]
Epoch [53/120    avg_loss:0.101, val_acc:0.952]
Epoch [54/120    avg_loss:0.092, val_acc:0.929]
Epoch [55/120    avg_loss:0.102, val_acc:0.928]
Epoch [56/120    avg_loss:0.075, val_acc:0.949]
Epoch [57/120    avg_loss:0.079, val_acc:0.952]
Epoch [58/120    avg_loss:0.078, val_acc:0.952]
Epoch [59/120    avg_loss:0.080, val_acc:0.954]
Epoch [60/120    avg_loss:0.073, val_acc:0.964]
Epoch [61/120    avg_loss:0.055, val_acc:0.962]
Epoch [62/120    avg_loss:0.044, val_acc:0.969]
Epoch [63/120    avg_loss:0.054, val_acc:0.970]
Epoch [64/120    avg_loss:0.049, val_acc:0.957]
Epoch [65/120    avg_loss:0.054, val_acc:0.964]
Epoch [66/120    avg_loss:0.040, val_acc:0.962]
Epoch [67/120    avg_loss:0.042, val_acc:0.962]
Epoch [68/120    avg_loss:0.042, val_acc:0.970]
Epoch [69/120    avg_loss:0.040, val_acc:0.972]
Epoch [70/120    avg_loss:0.036, val_acc:0.968]
Epoch [71/120    avg_loss:0.035, val_acc:0.964]
Epoch [72/120    avg_loss:0.035, val_acc:0.971]
Epoch [73/120    avg_loss:0.035, val_acc:0.972]
Epoch [74/120    avg_loss:0.030, val_acc:0.962]
Epoch [75/120    avg_loss:0.038, val_acc:0.968]
Epoch [76/120    avg_loss:0.034, val_acc:0.963]
Epoch [77/120    avg_loss:0.036, val_acc:0.973]
Epoch [78/120    avg_loss:0.055, val_acc:0.974]
Epoch [79/120    avg_loss:0.026, val_acc:0.975]
Epoch [80/120    avg_loss:0.023, val_acc:0.971]
Epoch [81/120    avg_loss:0.030, val_acc:0.960]
Epoch [82/120    avg_loss:0.062, val_acc:0.956]
Epoch [83/120    avg_loss:0.056, val_acc:0.969]
Epoch [84/120    avg_loss:0.060, val_acc:0.934]
Epoch [85/120    avg_loss:0.090, val_acc:0.944]
Epoch [86/120    avg_loss:0.105, val_acc:0.921]
Epoch [87/120    avg_loss:0.079, val_acc:0.931]
Epoch [88/120    avg_loss:0.074, val_acc:0.938]
Epoch [89/120    avg_loss:0.064, val_acc:0.954]
Epoch [90/120    avg_loss:0.100, val_acc:0.954]
Epoch [91/120    avg_loss:0.053, val_acc:0.952]
Epoch [92/120    avg_loss:0.041, val_acc:0.955]
Epoch [93/120    avg_loss:0.033, val_acc:0.969]
Epoch [94/120    avg_loss:0.023, val_acc:0.968]
Epoch [95/120    avg_loss:0.024, val_acc:0.971]
Epoch [96/120    avg_loss:0.021, val_acc:0.972]
Epoch [97/120    avg_loss:0.021, val_acc:0.971]
Epoch [98/120    avg_loss:0.019, val_acc:0.974]
Epoch [99/120    avg_loss:0.018, val_acc:0.970]
Epoch [100/120    avg_loss:0.021, val_acc:0.974]
Epoch [101/120    avg_loss:0.021, val_acc:0.972]
Epoch [102/120    avg_loss:0.021, val_acc:0.972]
Epoch [103/120    avg_loss:0.023, val_acc:0.978]
Epoch [104/120    avg_loss:0.020, val_acc:0.978]
Epoch [105/120    avg_loss:0.019, val_acc:0.977]
Epoch [106/120    avg_loss:0.020, val_acc:0.980]
Epoch [107/120    avg_loss:0.018, val_acc:0.983]
Epoch [108/120    avg_loss:0.018, val_acc:0.981]
Epoch [109/120    avg_loss:0.018, val_acc:0.980]
Epoch [110/120    avg_loss:0.018, val_acc:0.982]
Epoch [111/120    avg_loss:0.017, val_acc:0.982]
Epoch [112/120    avg_loss:0.015, val_acc:0.982]
Epoch [113/120    avg_loss:0.015, val_acc:0.980]
Epoch [114/120    avg_loss:0.018, val_acc:0.982]
Epoch [115/120    avg_loss:0.017, val_acc:0.982]
Epoch [116/120    avg_loss:0.016, val_acc:0.982]
Epoch [117/120    avg_loss:0.018, val_acc:0.981]
Epoch [118/120    avg_loss:0.015, val_acc:0.982]
Epoch [119/120    avg_loss:0.016, val_acc:0.983]
Epoch [120/120    avg_loss:0.015, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1253    1    6    5    0    0    0    0    3   17    0    0
     0    0    0]
 [   0    0    0  708    4    7    2    0    0    4    2    8   12    0
     0    0    0]
 [   0    0    0    0  210    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    0  855   11    2    0
     0    0    0]
 [   0    0   12    0    0    0    1    0    0    0   17 2174    6    0
     0    0    0]
 [   0    0    1    3    0    5    0    0    0    0    2    0  521    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    54  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.48509485094851

F1 scores:
[       nan 0.92307692 0.97967162 0.97052776 0.96997691 0.97621744
 0.98871332 0.98039216 0.99883586 0.9        0.97214326 0.98371041
 0.96392229 1.         0.9711828  0.87949922 0.97590361]

Kappa:
0.9713185621650263
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f654df67f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.820, val_acc:0.077]
Epoch [2/120    avg_loss:2.775, val_acc:0.136]
Epoch [3/120    avg_loss:2.739, val_acc:0.205]
Epoch [4/120    avg_loss:2.683, val_acc:0.381]
Epoch [5/120    avg_loss:2.629, val_acc:0.440]
Epoch [6/120    avg_loss:2.558, val_acc:0.495]
Epoch [7/120    avg_loss:2.494, val_acc:0.525]
Epoch [8/120    avg_loss:2.435, val_acc:0.550]
Epoch [9/120    avg_loss:2.371, val_acc:0.570]
Epoch [10/120    avg_loss:2.327, val_acc:0.583]
Epoch [11/120    avg_loss:2.288, val_acc:0.584]
Epoch [12/120    avg_loss:2.219, val_acc:0.597]
Epoch [13/120    avg_loss:2.132, val_acc:0.593]
Epoch [14/120    avg_loss:2.081, val_acc:0.603]
Epoch [15/120    avg_loss:1.992, val_acc:0.599]
Epoch [16/120    avg_loss:1.920, val_acc:0.611]
Epoch [17/120    avg_loss:1.815, val_acc:0.677]
Epoch [18/120    avg_loss:1.686, val_acc:0.683]
Epoch [19/120    avg_loss:1.612, val_acc:0.695]
Epoch [20/120    avg_loss:1.461, val_acc:0.688]
Epoch [21/120    avg_loss:1.329, val_acc:0.745]
Epoch [22/120    avg_loss:1.260, val_acc:0.725]
Epoch [23/120    avg_loss:1.195, val_acc:0.747]
Epoch [24/120    avg_loss:1.047, val_acc:0.730]
Epoch [25/120    avg_loss:1.025, val_acc:0.745]
Epoch [26/120    avg_loss:0.908, val_acc:0.806]
Epoch [27/120    avg_loss:0.836, val_acc:0.798]
Epoch [28/120    avg_loss:0.759, val_acc:0.824]
Epoch [29/120    avg_loss:0.667, val_acc:0.835]
Epoch [30/120    avg_loss:0.563, val_acc:0.842]
Epoch [31/120    avg_loss:0.580, val_acc:0.862]
Epoch [32/120    avg_loss:0.470, val_acc:0.858]
Epoch [33/120    avg_loss:0.421, val_acc:0.874]
Epoch [34/120    avg_loss:0.366, val_acc:0.881]
Epoch [35/120    avg_loss:0.326, val_acc:0.876]
Epoch [36/120    avg_loss:0.322, val_acc:0.888]
Epoch [37/120    avg_loss:0.290, val_acc:0.909]
Epoch [38/120    avg_loss:0.302, val_acc:0.894]
Epoch [39/120    avg_loss:0.239, val_acc:0.915]
Epoch [40/120    avg_loss:0.249, val_acc:0.921]
Epoch [41/120    avg_loss:0.215, val_acc:0.895]
Epoch [42/120    avg_loss:0.217, val_acc:0.909]
Epoch [43/120    avg_loss:0.209, val_acc:0.919]
Epoch [44/120    avg_loss:0.239, val_acc:0.906]
Epoch [45/120    avg_loss:0.195, val_acc:0.921]
Epoch [46/120    avg_loss:0.170, val_acc:0.931]
Epoch [47/120    avg_loss:0.136, val_acc:0.942]
Epoch [48/120    avg_loss:0.148, val_acc:0.928]
Epoch [49/120    avg_loss:0.141, val_acc:0.913]
Epoch [50/120    avg_loss:0.131, val_acc:0.949]
Epoch [51/120    avg_loss:0.118, val_acc:0.942]
Epoch [52/120    avg_loss:0.099, val_acc:0.952]
Epoch [53/120    avg_loss:0.103, val_acc:0.950]
Epoch [54/120    avg_loss:0.090, val_acc:0.952]
Epoch [55/120    avg_loss:0.081, val_acc:0.957]
Epoch [56/120    avg_loss:0.077, val_acc:0.955]
Epoch [57/120    avg_loss:0.086, val_acc:0.942]
Epoch [58/120    avg_loss:0.087, val_acc:0.953]
Epoch [59/120    avg_loss:0.074, val_acc:0.961]
Epoch [60/120    avg_loss:0.072, val_acc:0.960]
Epoch [61/120    avg_loss:0.073, val_acc:0.947]
Epoch [62/120    avg_loss:0.062, val_acc:0.966]
Epoch [63/120    avg_loss:0.056, val_acc:0.958]
Epoch [64/120    avg_loss:0.054, val_acc:0.964]
Epoch [65/120    avg_loss:0.068, val_acc:0.952]
Epoch [66/120    avg_loss:0.061, val_acc:0.960]
Epoch [67/120    avg_loss:0.047, val_acc:0.969]
Epoch [68/120    avg_loss:0.050, val_acc:0.971]
Epoch [69/120    avg_loss:0.049, val_acc:0.968]
Epoch [70/120    avg_loss:0.046, val_acc:0.967]
Epoch [71/120    avg_loss:0.056, val_acc:0.963]
Epoch [72/120    avg_loss:0.046, val_acc:0.968]
Epoch [73/120    avg_loss:0.049, val_acc:0.964]
Epoch [74/120    avg_loss:0.042, val_acc:0.982]
Epoch [75/120    avg_loss:0.033, val_acc:0.977]
Epoch [76/120    avg_loss:0.034, val_acc:0.963]
Epoch [77/120    avg_loss:0.029, val_acc:0.974]
Epoch [78/120    avg_loss:0.026, val_acc:0.974]
Epoch [79/120    avg_loss:0.024, val_acc:0.974]
Epoch [80/120    avg_loss:0.030, val_acc:0.974]
Epoch [81/120    avg_loss:0.047, val_acc:0.972]
Epoch [82/120    avg_loss:0.027, val_acc:0.970]
Epoch [83/120    avg_loss:0.026, val_acc:0.973]
Epoch [84/120    avg_loss:0.029, val_acc:0.970]
Epoch [85/120    avg_loss:0.026, val_acc:0.968]
Epoch [86/120    avg_loss:0.027, val_acc:0.974]
Epoch [87/120    avg_loss:0.024, val_acc:0.974]
Epoch [88/120    avg_loss:0.020, val_acc:0.975]
Epoch [89/120    avg_loss:0.020, val_acc:0.978]
Epoch [90/120    avg_loss:0.019, val_acc:0.980]
Epoch [91/120    avg_loss:0.016, val_acc:0.978]
Epoch [92/120    avg_loss:0.017, val_acc:0.978]
Epoch [93/120    avg_loss:0.017, val_acc:0.980]
Epoch [94/120    avg_loss:0.016, val_acc:0.981]
Epoch [95/120    avg_loss:0.016, val_acc:0.978]
Epoch [96/120    avg_loss:0.017, val_acc:0.977]
Epoch [97/120    avg_loss:0.015, val_acc:0.978]
Epoch [98/120    avg_loss:0.016, val_acc:0.983]
Epoch [99/120    avg_loss:0.017, val_acc:0.981]
Epoch [100/120    avg_loss:0.016, val_acc:0.981]
Epoch [101/120    avg_loss:0.014, val_acc:0.981]
Epoch [102/120    avg_loss:0.017, val_acc:0.977]
Epoch [103/120    avg_loss:0.027, val_acc:0.980]
Epoch [104/120    avg_loss:0.016, val_acc:0.977]
Epoch [105/120    avg_loss:0.019, val_acc:0.977]
Epoch [106/120    avg_loss:0.014, val_acc:0.980]
Epoch [107/120    avg_loss:0.014, val_acc:0.981]
Epoch [108/120    avg_loss:0.015, val_acc:0.980]
Epoch [109/120    avg_loss:0.016, val_acc:0.980]
Epoch [110/120    avg_loss:0.015, val_acc:0.981]
Epoch [111/120    avg_loss:0.015, val_acc:0.981]
Epoch [112/120    avg_loss:0.015, val_acc:0.981]
Epoch [113/120    avg_loss:0.014, val_acc:0.980]
Epoch [114/120    avg_loss:0.016, val_acc:0.980]
Epoch [115/120    avg_loss:0.014, val_acc:0.980]
Epoch [116/120    avg_loss:0.014, val_acc:0.981]
Epoch [117/120    avg_loss:0.014, val_acc:0.981]
Epoch [118/120    avg_loss:0.014, val_acc:0.981]
Epoch [119/120    avg_loss:0.015, val_acc:0.981]
Epoch [120/120    avg_loss:0.016, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1256    1    5    0    0    0    0    0    4   19    0    0
     0    0    0]
 [   0    0    0  713    3    1    0    0    0    6    3   12    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    1    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    2    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    3    0    0    0  849   16    0    0
     0    0    0]
 [   0    0    4    0    0    0    2    1    0    0   11 2176   16    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    4    2  524    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1130    7    0]
 [   0    0    0    0    0    0   18    0    0    0    0    0    0    0
    22  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.84281842818429

F1 scores:
[       nan 0.975      0.98394046 0.9753762  0.98156682 0.99539171
 0.97670924 0.98039216 0.995338   0.82926829 0.97139588 0.9806219
 0.96412144 1.         0.98346388 0.92889561 0.97647059]

Kappa:
0.9753978986426258
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcc14ca8f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.829, val_acc:0.045]
Epoch [2/120    avg_loss:2.795, val_acc:0.163]
Epoch [3/120    avg_loss:2.745, val_acc:0.196]
Epoch [4/120    avg_loss:2.681, val_acc:0.298]
Epoch [5/120    avg_loss:2.615, val_acc:0.353]
Epoch [6/120    avg_loss:2.526, val_acc:0.404]
Epoch [7/120    avg_loss:2.470, val_acc:0.448]
Epoch [8/120    avg_loss:2.394, val_acc:0.494]
Epoch [9/120    avg_loss:2.351, val_acc:0.506]
Epoch [10/120    avg_loss:2.290, val_acc:0.509]
Epoch [11/120    avg_loss:2.249, val_acc:0.502]
Epoch [12/120    avg_loss:2.164, val_acc:0.495]
Epoch [13/120    avg_loss:2.131, val_acc:0.550]
Epoch [14/120    avg_loss:2.067, val_acc:0.570]
Epoch [15/120    avg_loss:1.982, val_acc:0.525]
Epoch [16/120    avg_loss:1.907, val_acc:0.624]
Epoch [17/120    avg_loss:1.825, val_acc:0.623]
Epoch [18/120    avg_loss:1.699, val_acc:0.638]
Epoch [19/120    avg_loss:1.599, val_acc:0.631]
Epoch [20/120    avg_loss:1.496, val_acc:0.634]
Epoch [21/120    avg_loss:1.410, val_acc:0.685]
Epoch [22/120    avg_loss:1.278, val_acc:0.696]
Epoch [23/120    avg_loss:1.172, val_acc:0.719]
Epoch [24/120    avg_loss:1.110, val_acc:0.747]
Epoch [25/120    avg_loss:1.002, val_acc:0.758]
Epoch [26/120    avg_loss:0.958, val_acc:0.741]
Epoch [27/120    avg_loss:0.878, val_acc:0.790]
Epoch [28/120    avg_loss:0.835, val_acc:0.792]
Epoch [29/120    avg_loss:0.726, val_acc:0.801]
Epoch [30/120    avg_loss:0.631, val_acc:0.849]
Epoch [31/120    avg_loss:0.578, val_acc:0.841]
Epoch [32/120    avg_loss:0.542, val_acc:0.857]
Epoch [33/120    avg_loss:0.477, val_acc:0.855]
Epoch [34/120    avg_loss:0.411, val_acc:0.856]
Epoch [35/120    avg_loss:0.449, val_acc:0.852]
Epoch [36/120    avg_loss:0.377, val_acc:0.877]
Epoch [37/120    avg_loss:0.315, val_acc:0.913]
Epoch [38/120    avg_loss:0.267, val_acc:0.914]
Epoch [39/120    avg_loss:0.536, val_acc:0.777]
Epoch [40/120    avg_loss:0.462, val_acc:0.871]
Epoch [41/120    avg_loss:0.323, val_acc:0.869]
Epoch [42/120    avg_loss:0.300, val_acc:0.898]
Epoch [43/120    avg_loss:0.245, val_acc:0.920]
Epoch [44/120    avg_loss:0.201, val_acc:0.923]
Epoch [45/120    avg_loss:0.252, val_acc:0.914]
Epoch [46/120    avg_loss:0.202, val_acc:0.928]
Epoch [47/120    avg_loss:0.156, val_acc:0.938]
Epoch [48/120    avg_loss:0.148, val_acc:0.943]
Epoch [49/120    avg_loss:0.151, val_acc:0.947]
Epoch [50/120    avg_loss:0.152, val_acc:0.929]
Epoch [51/120    avg_loss:0.168, val_acc:0.939]
Epoch [52/120    avg_loss:0.151, val_acc:0.942]
Epoch [53/120    avg_loss:0.147, val_acc:0.949]
Epoch [54/120    avg_loss:0.119, val_acc:0.935]
Epoch [55/120    avg_loss:0.101, val_acc:0.953]
Epoch [56/120    avg_loss:0.100, val_acc:0.952]
Epoch [57/120    avg_loss:0.093, val_acc:0.963]
Epoch [58/120    avg_loss:0.095, val_acc:0.942]
Epoch [59/120    avg_loss:0.087, val_acc:0.946]
Epoch [60/120    avg_loss:0.100, val_acc:0.957]
Epoch [61/120    avg_loss:0.079, val_acc:0.964]
Epoch [62/120    avg_loss:0.078, val_acc:0.960]
Epoch [63/120    avg_loss:0.067, val_acc:0.964]
Epoch [64/120    avg_loss:0.056, val_acc:0.973]
Epoch [65/120    avg_loss:0.058, val_acc:0.956]
Epoch [66/120    avg_loss:0.082, val_acc:0.969]
Epoch [67/120    avg_loss:0.052, val_acc:0.972]
Epoch [68/120    avg_loss:0.052, val_acc:0.960]
Epoch [69/120    avg_loss:0.047, val_acc:0.968]
Epoch [70/120    avg_loss:0.042, val_acc:0.969]
Epoch [71/120    avg_loss:0.042, val_acc:0.961]
Epoch [72/120    avg_loss:0.086, val_acc:0.905]
Epoch [73/120    avg_loss:0.088, val_acc:0.960]
Epoch [74/120    avg_loss:0.060, val_acc:0.963]
Epoch [75/120    avg_loss:0.055, val_acc:0.961]
Epoch [76/120    avg_loss:0.056, val_acc:0.950]
Epoch [77/120    avg_loss:0.050, val_acc:0.967]
Epoch [78/120    avg_loss:0.037, val_acc:0.970]
Epoch [79/120    avg_loss:0.040, val_acc:0.970]
Epoch [80/120    avg_loss:0.028, val_acc:0.974]
Epoch [81/120    avg_loss:0.031, val_acc:0.977]
Epoch [82/120    avg_loss:0.035, val_acc:0.980]
Epoch [83/120    avg_loss:0.025, val_acc:0.975]
Epoch [84/120    avg_loss:0.026, val_acc:0.980]
Epoch [85/120    avg_loss:0.027, val_acc:0.981]
Epoch [86/120    avg_loss:0.028, val_acc:0.976]
Epoch [87/120    avg_loss:0.027, val_acc:0.977]
Epoch [88/120    avg_loss:0.025, val_acc:0.980]
Epoch [89/120    avg_loss:0.026, val_acc:0.977]
Epoch [90/120    avg_loss:0.030, val_acc:0.975]
Epoch [91/120    avg_loss:0.024, val_acc:0.976]
Epoch [92/120    avg_loss:0.025, val_acc:0.978]
Epoch [93/120    avg_loss:0.025, val_acc:0.978]
Epoch [94/120    avg_loss:0.025, val_acc:0.975]
Epoch [95/120    avg_loss:0.024, val_acc:0.977]
Epoch [96/120    avg_loss:0.024, val_acc:0.980]
Epoch [97/120    avg_loss:0.026, val_acc:0.974]
Epoch [98/120    avg_loss:0.022, val_acc:0.980]
Epoch [99/120    avg_loss:0.025, val_acc:0.980]
Epoch [100/120    avg_loss:0.023, val_acc:0.980]
Epoch [101/120    avg_loss:0.023, val_acc:0.980]
Epoch [102/120    avg_loss:0.022, val_acc:0.978]
Epoch [103/120    avg_loss:0.024, val_acc:0.978]
Epoch [104/120    avg_loss:0.023, val_acc:0.978]
Epoch [105/120    avg_loss:0.021, val_acc:0.978]
Epoch [106/120    avg_loss:0.021, val_acc:0.978]
Epoch [107/120    avg_loss:0.025, val_acc:0.978]
Epoch [108/120    avg_loss:0.026, val_acc:0.978]
Epoch [109/120    avg_loss:0.022, val_acc:0.978]
Epoch [110/120    avg_loss:0.021, val_acc:0.978]
Epoch [111/120    avg_loss:0.023, val_acc:0.978]
Epoch [112/120    avg_loss:0.022, val_acc:0.978]
Epoch [113/120    avg_loss:0.021, val_acc:0.978]
Epoch [114/120    avg_loss:0.022, val_acc:0.978]
Epoch [115/120    avg_loss:0.023, val_acc:0.978]
Epoch [116/120    avg_loss:0.022, val_acc:0.978]
Epoch [117/120    avg_loss:0.023, val_acc:0.978]
Epoch [118/120    avg_loss:0.024, val_acc:0.978]
Epoch [119/120    avg_loss:0.021, val_acc:0.978]
Epoch [120/120    avg_loss:0.030, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1267    0    1    0    0    0    0    0    3   14    0    0
     0    0    0]
 [   0    0    1  698    1    9    0    0    0   16    6    2   13    1
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    5    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    3    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    9    0    0    5    0    0    0    0  843    6    4    0
     1    7    0]
 [   0    0   12    1    0    1    2    0    0    0   21 2161   12    0
     0    0    0]
 [   0    0    0    0    1    8    0    0    0    0    0    7  512    0
     0    4    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1127    9    0]
 [   0    0    0    0    0    0   35    0    0    0    0    0    0    0
     8  304    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.30081300813008

F1 scores:
[       nan 0.96202532 0.98445998 0.96475466 0.99065421 0.96287964
 0.96802974 0.90909091 0.99649942 0.65384615 0.96287836 0.98138056
 0.94639556 0.99730458 0.98946444 0.90611028 0.97005988]

Kappa:
0.9692419815039868
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fea7568fe48>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.829, val_acc:0.040]
Epoch [2/120    avg_loss:2.795, val_acc:0.050]
Epoch [3/120    avg_loss:2.753, val_acc:0.131]
Epoch [4/120    avg_loss:2.704, val_acc:0.170]
Epoch [5/120    avg_loss:2.639, val_acc:0.192]
Epoch [6/120    avg_loss:2.583, val_acc:0.239]
Epoch [7/120    avg_loss:2.512, val_acc:0.280]
Epoch [8/120    avg_loss:2.459, val_acc:0.310]
Epoch [9/120    avg_loss:2.425, val_acc:0.344]
Epoch [10/120    avg_loss:2.380, val_acc:0.383]
Epoch [11/120    avg_loss:2.328, val_acc:0.392]
Epoch [12/120    avg_loss:2.260, val_acc:0.448]
Epoch [13/120    avg_loss:2.205, val_acc:0.498]
Epoch [14/120    avg_loss:2.178, val_acc:0.532]
Epoch [15/120    avg_loss:2.143, val_acc:0.567]
Epoch [16/120    avg_loss:2.059, val_acc:0.566]
Epoch [17/120    avg_loss:1.987, val_acc:0.593]
Epoch [18/120    avg_loss:1.931, val_acc:0.597]
Epoch [19/120    avg_loss:1.846, val_acc:0.629]
Epoch [20/120    avg_loss:1.725, val_acc:0.664]
Epoch [21/120    avg_loss:1.659, val_acc:0.670]
Epoch [22/120    avg_loss:1.522, val_acc:0.684]
Epoch [23/120    avg_loss:1.446, val_acc:0.714]
Epoch [24/120    avg_loss:1.332, val_acc:0.738]
Epoch [25/120    avg_loss:1.258, val_acc:0.769]
Epoch [26/120    avg_loss:1.144, val_acc:0.753]
Epoch [27/120    avg_loss:1.013, val_acc:0.795]
Epoch [28/120    avg_loss:0.878, val_acc:0.798]
Epoch [29/120    avg_loss:0.844, val_acc:0.779]
Epoch [30/120    avg_loss:0.785, val_acc:0.810]
Epoch [31/120    avg_loss:0.639, val_acc:0.823]
Epoch [32/120    avg_loss:0.584, val_acc:0.846]
Epoch [33/120    avg_loss:0.574, val_acc:0.861]
Epoch [34/120    avg_loss:0.468, val_acc:0.886]
Epoch [35/120    avg_loss:0.452, val_acc:0.878]
Epoch [36/120    avg_loss:0.376, val_acc:0.897]
Epoch [37/120    avg_loss:0.344, val_acc:0.893]
Epoch [38/120    avg_loss:0.345, val_acc:0.889]
Epoch [39/120    avg_loss:0.296, val_acc:0.912]
Epoch [40/120    avg_loss:0.262, val_acc:0.914]
Epoch [41/120    avg_loss:0.267, val_acc:0.846]
Epoch [42/120    avg_loss:0.341, val_acc:0.894]
Epoch [43/120    avg_loss:0.231, val_acc:0.918]
Epoch [44/120    avg_loss:0.226, val_acc:0.917]
Epoch [45/120    avg_loss:0.256, val_acc:0.907]
Epoch [46/120    avg_loss:0.210, val_acc:0.908]
Epoch [47/120    avg_loss:0.217, val_acc:0.911]
Epoch [48/120    avg_loss:0.180, val_acc:0.930]
Epoch [49/120    avg_loss:0.155, val_acc:0.948]
Epoch [50/120    avg_loss:0.137, val_acc:0.939]
Epoch [51/120    avg_loss:0.177, val_acc:0.913]
Epoch [52/120    avg_loss:0.185, val_acc:0.925]
Epoch [53/120    avg_loss:0.147, val_acc:0.941]
Epoch [54/120    avg_loss:0.116, val_acc:0.945]
Epoch [55/120    avg_loss:0.112, val_acc:0.948]
Epoch [56/120    avg_loss:0.090, val_acc:0.956]
Epoch [57/120    avg_loss:0.085, val_acc:0.962]
Epoch [58/120    avg_loss:0.078, val_acc:0.957]
Epoch [59/120    avg_loss:0.072, val_acc:0.963]
Epoch [60/120    avg_loss:0.078, val_acc:0.956]
Epoch [61/120    avg_loss:0.080, val_acc:0.953]
Epoch [62/120    avg_loss:0.069, val_acc:0.962]
Epoch [63/120    avg_loss:0.072, val_acc:0.957]
Epoch [64/120    avg_loss:0.055, val_acc:0.962]
Epoch [65/120    avg_loss:0.045, val_acc:0.961]
Epoch [66/120    avg_loss:0.045, val_acc:0.969]
Epoch [67/120    avg_loss:0.056, val_acc:0.956]
Epoch [68/120    avg_loss:0.068, val_acc:0.955]
Epoch [69/120    avg_loss:0.069, val_acc:0.953]
Epoch [70/120    avg_loss:0.065, val_acc:0.961]
Epoch [71/120    avg_loss:0.066, val_acc:0.956]
Epoch [72/120    avg_loss:0.044, val_acc:0.967]
Epoch [73/120    avg_loss:0.037, val_acc:0.971]
Epoch [74/120    avg_loss:0.033, val_acc:0.977]
Epoch [75/120    avg_loss:0.055, val_acc:0.962]
Epoch [76/120    avg_loss:0.039, val_acc:0.977]
Epoch [77/120    avg_loss:0.047, val_acc:0.971]
Epoch [78/120    avg_loss:0.044, val_acc:0.972]
Epoch [79/120    avg_loss:0.038, val_acc:0.964]
Epoch [80/120    avg_loss:0.044, val_acc:0.974]
Epoch [81/120    avg_loss:0.030, val_acc:0.972]
Epoch [82/120    avg_loss:0.030, val_acc:0.976]
Epoch [83/120    avg_loss:0.026, val_acc:0.974]
Epoch [84/120    avg_loss:0.025, val_acc:0.968]
Epoch [85/120    avg_loss:0.034, val_acc:0.977]
Epoch [86/120    avg_loss:0.030, val_acc:0.973]
Epoch [87/120    avg_loss:0.027, val_acc:0.974]
Epoch [88/120    avg_loss:0.025, val_acc:0.973]
Epoch [89/120    avg_loss:0.028, val_acc:0.967]
Epoch [90/120    avg_loss:0.025, val_acc:0.978]
Epoch [91/120    avg_loss:0.030, val_acc:0.974]
Epoch [92/120    avg_loss:0.031, val_acc:0.970]
Epoch [93/120    avg_loss:0.028, val_acc:0.964]
Epoch [94/120    avg_loss:0.027, val_acc:0.963]
Epoch [95/120    avg_loss:0.038, val_acc:0.961]
Epoch [96/120    avg_loss:0.030, val_acc:0.964]
Epoch [97/120    avg_loss:0.028, val_acc:0.972]
Epoch [98/120    avg_loss:0.018, val_acc:0.978]
Epoch [99/120    avg_loss:0.019, val_acc:0.980]
Epoch [100/120    avg_loss:0.023, val_acc:0.987]
Epoch [101/120    avg_loss:0.018, val_acc:0.981]
Epoch [102/120    avg_loss:0.018, val_acc:0.984]
Epoch [103/120    avg_loss:0.018, val_acc:0.973]
Epoch [104/120    avg_loss:0.019, val_acc:0.968]
Epoch [105/120    avg_loss:0.016, val_acc:0.974]
Epoch [106/120    avg_loss:0.017, val_acc:0.973]
Epoch [107/120    avg_loss:0.022, val_acc:0.978]
Epoch [108/120    avg_loss:0.015, val_acc:0.984]
Epoch [109/120    avg_loss:0.011, val_acc:0.980]
Epoch [110/120    avg_loss:0.012, val_acc:0.976]
Epoch [111/120    avg_loss:0.017, val_acc:0.975]
Epoch [112/120    avg_loss:0.016, val_acc:0.983]
Epoch [113/120    avg_loss:0.015, val_acc:0.981]
Epoch [114/120    avg_loss:0.013, val_acc:0.983]
Epoch [115/120    avg_loss:0.011, val_acc:0.982]
Epoch [116/120    avg_loss:0.013, val_acc:0.983]
Epoch [117/120    avg_loss:0.010, val_acc:0.983]
Epoch [118/120    avg_loss:0.012, val_acc:0.984]
Epoch [119/120    avg_loss:0.010, val_acc:0.984]
Epoch [120/120    avg_loss:0.010, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1244    2   11    0    1    0    0    0    7   20    0    0
     0    0    0]
 [   0    0    2  711    9   16    0    0    0    4    2    1    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    4    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    4    0    0    0    0  849    8    6    0
     0    0    0]
 [   0    0    4    0    0    0    1    0    0    0    8 2190    7    0
     0    0    0]
 [   0    0    0    4    0   15    0    0    0    0    0    0  513    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   32    0    0    0    0    0    0    0
    15  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.8319783197832

F1 scores:
[       nan 0.98765432 0.978372   0.97064846 0.95515695 0.95555556
 0.97401633 0.92592593 1.         0.85       0.97474168 0.98893655
 0.96519285 1.         0.99302528 0.92735703 0.98224852]

Kappa:
0.9752842455766468
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5e1dbb8e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.824, val_acc:0.112]
Epoch [2/120    avg_loss:2.782, val_acc:0.190]
Epoch [3/120    avg_loss:2.738, val_acc:0.254]
Epoch [4/120    avg_loss:2.689, val_acc:0.364]
Epoch [5/120    avg_loss:2.646, val_acc:0.417]
Epoch [6/120    avg_loss:2.588, val_acc:0.438]
Epoch [7/120    avg_loss:2.561, val_acc:0.462]
Epoch [8/120    avg_loss:2.485, val_acc:0.514]
Epoch [9/120    avg_loss:2.415, val_acc:0.522]
Epoch [10/120    avg_loss:2.322, val_acc:0.552]
Epoch [11/120    avg_loss:2.288, val_acc:0.546]
Epoch [12/120    avg_loss:2.226, val_acc:0.583]
Epoch [13/120    avg_loss:2.168, val_acc:0.592]
Epoch [14/120    avg_loss:2.119, val_acc:0.587]
Epoch [15/120    avg_loss:2.074, val_acc:0.635]
Epoch [16/120    avg_loss:1.977, val_acc:0.643]
Epoch [17/120    avg_loss:1.880, val_acc:0.664]
Epoch [18/120    avg_loss:1.827, val_acc:0.654]
Epoch [19/120    avg_loss:1.784, val_acc:0.647]
Epoch [20/120    avg_loss:1.664, val_acc:0.670]
Epoch [21/120    avg_loss:1.570, val_acc:0.692]
Epoch [22/120    avg_loss:1.505, val_acc:0.707]
Epoch [23/120    avg_loss:1.438, val_acc:0.728]
Epoch [24/120    avg_loss:1.380, val_acc:0.721]
Epoch [25/120    avg_loss:1.280, val_acc:0.755]
Epoch [26/120    avg_loss:1.158, val_acc:0.750]
Epoch [27/120    avg_loss:1.019, val_acc:0.779]
Epoch [28/120    avg_loss:0.945, val_acc:0.800]
Epoch [29/120    avg_loss:0.888, val_acc:0.798]
Epoch [30/120    avg_loss:0.828, val_acc:0.811]
Epoch [31/120    avg_loss:0.751, val_acc:0.838]
Epoch [32/120    avg_loss:0.641, val_acc:0.853]
Epoch [33/120    avg_loss:0.577, val_acc:0.857]
Epoch [34/120    avg_loss:0.556, val_acc:0.870]
Epoch [35/120    avg_loss:0.567, val_acc:0.804]
Epoch [36/120    avg_loss:0.551, val_acc:0.871]
Epoch [37/120    avg_loss:0.445, val_acc:0.877]
Epoch [38/120    avg_loss:0.444, val_acc:0.865]
Epoch [39/120    avg_loss:0.417, val_acc:0.875]
Epoch [40/120    avg_loss:0.371, val_acc:0.909]
Epoch [41/120    avg_loss:0.358, val_acc:0.906]
Epoch [42/120    avg_loss:0.277, val_acc:0.894]
Epoch [43/120    avg_loss:0.280, val_acc:0.919]
Epoch [44/120    avg_loss:0.272, val_acc:0.922]
Epoch [45/120    avg_loss:0.211, val_acc:0.932]
Epoch [46/120    avg_loss:0.194, val_acc:0.926]
Epoch [47/120    avg_loss:0.200, val_acc:0.932]
Epoch [48/120    avg_loss:0.210, val_acc:0.913]
Epoch [49/120    avg_loss:0.178, val_acc:0.941]
Epoch [50/120    avg_loss:0.186, val_acc:0.935]
Epoch [51/120    avg_loss:0.210, val_acc:0.911]
Epoch [52/120    avg_loss:0.178, val_acc:0.936]
Epoch [53/120    avg_loss:0.164, val_acc:0.914]
Epoch [54/120    avg_loss:0.183, val_acc:0.933]
Epoch [55/120    avg_loss:0.159, val_acc:0.915]
Epoch [56/120    avg_loss:0.144, val_acc:0.947]
Epoch [57/120    avg_loss:0.129, val_acc:0.941]
Epoch [58/120    avg_loss:0.130, val_acc:0.952]
Epoch [59/120    avg_loss:0.117, val_acc:0.930]
Epoch [60/120    avg_loss:0.142, val_acc:0.947]
Epoch [61/120    avg_loss:0.100, val_acc:0.956]
Epoch [62/120    avg_loss:0.091, val_acc:0.958]
Epoch [63/120    avg_loss:0.084, val_acc:0.966]
Epoch [64/120    avg_loss:0.073, val_acc:0.967]
Epoch [65/120    avg_loss:0.061, val_acc:0.957]
Epoch [66/120    avg_loss:0.068, val_acc:0.961]
Epoch [67/120    avg_loss:0.062, val_acc:0.964]
Epoch [68/120    avg_loss:0.065, val_acc:0.968]
Epoch [69/120    avg_loss:0.064, val_acc:0.971]
Epoch [70/120    avg_loss:0.054, val_acc:0.953]
Epoch [71/120    avg_loss:0.072, val_acc:0.953]
Epoch [72/120    avg_loss:0.079, val_acc:0.970]
Epoch [73/120    avg_loss:0.051, val_acc:0.948]
Epoch [74/120    avg_loss:0.053, val_acc:0.958]
Epoch [75/120    avg_loss:0.047, val_acc:0.969]
Epoch [76/120    avg_loss:0.057, val_acc:0.969]
Epoch [77/120    avg_loss:0.058, val_acc:0.953]
Epoch [78/120    avg_loss:0.061, val_acc:0.977]
Epoch [79/120    avg_loss:0.043, val_acc:0.954]
Epoch [80/120    avg_loss:0.047, val_acc:0.960]
Epoch [81/120    avg_loss:0.039, val_acc:0.970]
Epoch [82/120    avg_loss:0.037, val_acc:0.971]
Epoch [83/120    avg_loss:0.040, val_acc:0.976]
Epoch [84/120    avg_loss:0.034, val_acc:0.975]
Epoch [85/120    avg_loss:0.032, val_acc:0.977]
Epoch [86/120    avg_loss:0.028, val_acc:0.964]
Epoch [87/120    avg_loss:0.056, val_acc:0.973]
Epoch [88/120    avg_loss:0.041, val_acc:0.980]
Epoch [89/120    avg_loss:0.047, val_acc:0.974]
Epoch [90/120    avg_loss:0.038, val_acc:0.971]
Epoch [91/120    avg_loss:0.028, val_acc:0.969]
Epoch [92/120    avg_loss:0.032, val_acc:0.977]
Epoch [93/120    avg_loss:0.035, val_acc:0.971]
Epoch [94/120    avg_loss:0.044, val_acc:0.977]
Epoch [95/120    avg_loss:0.031, val_acc:0.976]
Epoch [96/120    avg_loss:0.030, val_acc:0.976]
Epoch [97/120    avg_loss:0.029, val_acc:0.952]
Epoch [98/120    avg_loss:0.029, val_acc:0.983]
Epoch [99/120    avg_loss:0.025, val_acc:0.984]
Epoch [100/120    avg_loss:0.019, val_acc:0.972]
Epoch [101/120    avg_loss:0.029, val_acc:0.977]
Epoch [102/120    avg_loss:0.043, val_acc:0.971]
Epoch [103/120    avg_loss:0.045, val_acc:0.964]
Epoch [104/120    avg_loss:0.058, val_acc:0.963]
Epoch [105/120    avg_loss:0.062, val_acc:0.970]
Epoch [106/120    avg_loss:0.056, val_acc:0.970]
Epoch [107/120    avg_loss:0.044, val_acc:0.968]
Epoch [108/120    avg_loss:0.039, val_acc:0.975]
Epoch [109/120    avg_loss:0.032, val_acc:0.970]
Epoch [110/120    avg_loss:0.032, val_acc:0.974]
Epoch [111/120    avg_loss:0.042, val_acc:0.970]
Epoch [112/120    avg_loss:0.027, val_acc:0.974]
Epoch [113/120    avg_loss:0.020, val_acc:0.973]
Epoch [114/120    avg_loss:0.016, val_acc:0.973]
Epoch [115/120    avg_loss:0.020, val_acc:0.972]
Epoch [116/120    avg_loss:0.017, val_acc:0.971]
Epoch [117/120    avg_loss:0.020, val_acc:0.973]
Epoch [118/120    avg_loss:0.015, val_acc:0.974]
Epoch [119/120    avg_loss:0.015, val_acc:0.974]
Epoch [120/120    avg_loss:0.015, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1248    4    0    0    2    0    0    3    3   25    0    0
     0    0    0]
 [   0    0    2  694    6   13    0    0    0   15    4    0   11    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    4    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    6    4    0    0    0  826   19   10    0
     0    2    0]
 [   0    0    2    0    0    0    2    1    0    0   10 2195    0    0
     0    0    0]
 [   0    0    0    2    6    9    0    0    0    0    9    2  501    0
     3    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    1    0    0    0
  1132    2    0]
 [   0    0    0    0    0    0   22    0    0    0    0    0    0    0
    18  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.289972899729

F1 scores:
[       nan 0.975      0.98074656 0.95922598 0.97260274 0.95865922
 0.97615499 0.96153846 0.995338   0.62068966 0.95491329 0.98563089
 0.94350282 0.99462366 0.9877836  0.9331307  0.97005988]

Kappa:
0.96909400839461
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f069e259ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.803, val_acc:0.252]
Epoch [2/120    avg_loss:2.736, val_acc:0.282]
Epoch [3/120    avg_loss:2.676, val_acc:0.286]
Epoch [4/120    avg_loss:2.629, val_acc:0.292]
Epoch [5/120    avg_loss:2.560, val_acc:0.292]
Epoch [6/120    avg_loss:2.489, val_acc:0.295]
Epoch [7/120    avg_loss:2.454, val_acc:0.308]
Epoch [8/120    avg_loss:2.355, val_acc:0.319]
Epoch [9/120    avg_loss:2.315, val_acc:0.346]
Epoch [10/120    avg_loss:2.249, val_acc:0.377]
Epoch [11/120    avg_loss:2.202, val_acc:0.393]
Epoch [12/120    avg_loss:2.146, val_acc:0.431]
Epoch [13/120    avg_loss:2.113, val_acc:0.474]
Epoch [14/120    avg_loss:2.051, val_acc:0.497]
Epoch [15/120    avg_loss:1.975, val_acc:0.503]
Epoch [16/120    avg_loss:1.897, val_acc:0.512]
Epoch [17/120    avg_loss:1.781, val_acc:0.543]
Epoch [18/120    avg_loss:1.712, val_acc:0.649]
Epoch [19/120    avg_loss:1.679, val_acc:0.541]
Epoch [20/120    avg_loss:1.498, val_acc:0.637]
Epoch [21/120    avg_loss:1.411, val_acc:0.651]
Epoch [22/120    avg_loss:1.268, val_acc:0.684]
Epoch [23/120    avg_loss:1.235, val_acc:0.696]
Epoch [24/120    avg_loss:1.100, val_acc:0.732]
Epoch [25/120    avg_loss:1.018, val_acc:0.731]
Epoch [26/120    avg_loss:0.993, val_acc:0.751]
Epoch [27/120    avg_loss:0.940, val_acc:0.754]
Epoch [28/120    avg_loss:0.944, val_acc:0.751]
Epoch [29/120    avg_loss:0.823, val_acc:0.810]
Epoch [30/120    avg_loss:0.769, val_acc:0.766]
Epoch [31/120    avg_loss:0.747, val_acc:0.825]
Epoch [32/120    avg_loss:0.671, val_acc:0.828]
Epoch [33/120    avg_loss:0.618, val_acc:0.834]
Epoch [34/120    avg_loss:0.588, val_acc:0.850]
Epoch [35/120    avg_loss:0.527, val_acc:0.875]
Epoch [36/120    avg_loss:0.485, val_acc:0.888]
Epoch [37/120    avg_loss:0.439, val_acc:0.887]
Epoch [38/120    avg_loss:0.417, val_acc:0.876]
Epoch [39/120    avg_loss:0.378, val_acc:0.893]
Epoch [40/120    avg_loss:0.321, val_acc:0.897]
Epoch [41/120    avg_loss:0.301, val_acc:0.885]
Epoch [42/120    avg_loss:0.326, val_acc:0.902]
Epoch [43/120    avg_loss:0.298, val_acc:0.918]
Epoch [44/120    avg_loss:0.251, val_acc:0.933]
Epoch [45/120    avg_loss:0.232, val_acc:0.932]
Epoch [46/120    avg_loss:0.207, val_acc:0.927]
Epoch [47/120    avg_loss:0.216, val_acc:0.936]
Epoch [48/120    avg_loss:0.186, val_acc:0.920]
Epoch [49/120    avg_loss:0.168, val_acc:0.946]
Epoch [50/120    avg_loss:0.159, val_acc:0.923]
Epoch [51/120    avg_loss:0.152, val_acc:0.944]
Epoch [52/120    avg_loss:0.164, val_acc:0.941]
Epoch [53/120    avg_loss:0.126, val_acc:0.947]
Epoch [54/120    avg_loss:0.129, val_acc:0.946]
Epoch [55/120    avg_loss:0.109, val_acc:0.952]
Epoch [56/120    avg_loss:0.121, val_acc:0.943]
Epoch [57/120    avg_loss:0.135, val_acc:0.942]
Epoch [58/120    avg_loss:0.112, val_acc:0.938]
Epoch [59/120    avg_loss:0.107, val_acc:0.960]
Epoch [60/120    avg_loss:0.115, val_acc:0.945]
Epoch [61/120    avg_loss:0.118, val_acc:0.948]
Epoch [62/120    avg_loss:0.100, val_acc:0.964]
Epoch [63/120    avg_loss:0.077, val_acc:0.957]
Epoch [64/120    avg_loss:0.082, val_acc:0.960]
Epoch [65/120    avg_loss:0.085, val_acc:0.967]
Epoch [66/120    avg_loss:0.075, val_acc:0.970]
Epoch [67/120    avg_loss:0.070, val_acc:0.963]
Epoch [68/120    avg_loss:0.060, val_acc:0.962]
Epoch [69/120    avg_loss:0.066, val_acc:0.967]
Epoch [70/120    avg_loss:0.059, val_acc:0.969]
Epoch [71/120    avg_loss:0.053, val_acc:0.972]
Epoch [72/120    avg_loss:0.050, val_acc:0.961]
Epoch [73/120    avg_loss:0.056, val_acc:0.941]
Epoch [74/120    avg_loss:0.057, val_acc:0.968]
Epoch [75/120    avg_loss:0.050, val_acc:0.966]
Epoch [76/120    avg_loss:0.039, val_acc:0.973]
Epoch [77/120    avg_loss:0.035, val_acc:0.976]
Epoch [78/120    avg_loss:0.037, val_acc:0.963]
Epoch [79/120    avg_loss:0.043, val_acc:0.959]
Epoch [80/120    avg_loss:0.045, val_acc:0.970]
Epoch [81/120    avg_loss:0.038, val_acc:0.977]
Epoch [82/120    avg_loss:0.031, val_acc:0.972]
Epoch [83/120    avg_loss:0.029, val_acc:0.976]
Epoch [84/120    avg_loss:0.028, val_acc:0.978]
Epoch [85/120    avg_loss:0.032, val_acc:0.972]
Epoch [86/120    avg_loss:0.035, val_acc:0.973]
Epoch [87/120    avg_loss:0.024, val_acc:0.978]
Epoch [88/120    avg_loss:0.027, val_acc:0.975]
Epoch [89/120    avg_loss:0.023, val_acc:0.984]
Epoch [90/120    avg_loss:0.023, val_acc:0.977]
Epoch [91/120    avg_loss:0.020, val_acc:0.974]
Epoch [92/120    avg_loss:0.022, val_acc:0.976]
Epoch [93/120    avg_loss:0.021, val_acc:0.980]
Epoch [94/120    avg_loss:0.022, val_acc:0.980]
Epoch [95/120    avg_loss:0.024, val_acc:0.981]
Epoch [96/120    avg_loss:0.027, val_acc:0.978]
Epoch [97/120    avg_loss:0.027, val_acc:0.977]
Epoch [98/120    avg_loss:0.021, val_acc:0.973]
Epoch [99/120    avg_loss:0.028, val_acc:0.971]
Epoch [100/120    avg_loss:0.033, val_acc:0.972]
Epoch [101/120    avg_loss:0.030, val_acc:0.959]
Epoch [102/120    avg_loss:0.036, val_acc:0.973]
Epoch [103/120    avg_loss:0.026, val_acc:0.976]
Epoch [104/120    avg_loss:0.018, val_acc:0.976]
Epoch [105/120    avg_loss:0.015, val_acc:0.976]
Epoch [106/120    avg_loss:0.018, val_acc:0.977]
Epoch [107/120    avg_loss:0.017, val_acc:0.977]
Epoch [108/120    avg_loss:0.018, val_acc:0.977]
Epoch [109/120    avg_loss:0.017, val_acc:0.978]
Epoch [110/120    avg_loss:0.016, val_acc:0.980]
Epoch [111/120    avg_loss:0.016, val_acc:0.978]
Epoch [112/120    avg_loss:0.014, val_acc:0.978]
Epoch [113/120    avg_loss:0.015, val_acc:0.977]
Epoch [114/120    avg_loss:0.015, val_acc:0.976]
Epoch [115/120    avg_loss:0.015, val_acc:0.976]
Epoch [116/120    avg_loss:0.014, val_acc:0.976]
Epoch [117/120    avg_loss:0.013, val_acc:0.976]
Epoch [118/120    avg_loss:0.015, val_acc:0.976]
Epoch [119/120    avg_loss:0.013, val_acc:0.976]
Epoch [120/120    avg_loss:0.015, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1248    0    7    0    0    0    0    0   10   17    3    0
     0    0    0]
 [   0    0    2  719    6   10    0    0    0    1    2    1    6    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    1    0    7    0    0    0    0  844   18    0    0
     0    1    0]
 [   0    0   14    0    0    0    1    0    0    0   20 2163   12    0
     0    0    0]
 [   0    0    0    0    0   12    0    0    0    0    2    0  518    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    0   43    0    0    0    0    0    0    0
    12  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.4959349593496

F1 scores:
[       nan 0.975      0.97767333 0.97956403 0.96803653 0.96659243
 0.96683861 0.98039216 0.9953271  0.97297297 0.96182336 0.98117487
 0.96193129 1.         0.99080158 0.90123457 0.98823529]

Kappa:
0.9714610454168039
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd1a871ef98>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.817, val_acc:0.070]
Epoch [2/120    avg_loss:2.770, val_acc:0.081]
Epoch [3/120    avg_loss:2.723, val_acc:0.096]
Epoch [4/120    avg_loss:2.674, val_acc:0.114]
Epoch [5/120    avg_loss:2.632, val_acc:0.185]
Epoch [6/120    avg_loss:2.579, val_acc:0.233]
Epoch [7/120    avg_loss:2.534, val_acc:0.314]
Epoch [8/120    avg_loss:2.490, val_acc:0.412]
Epoch [9/120    avg_loss:2.434, val_acc:0.468]
Epoch [10/120    avg_loss:2.376, val_acc:0.486]
Epoch [11/120    avg_loss:2.318, val_acc:0.526]
Epoch [12/120    avg_loss:2.260, val_acc:0.536]
Epoch [13/120    avg_loss:2.227, val_acc:0.567]
Epoch [14/120    avg_loss:2.149, val_acc:0.587]
Epoch [15/120    avg_loss:2.090, val_acc:0.609]
Epoch [16/120    avg_loss:2.018, val_acc:0.616]
Epoch [17/120    avg_loss:1.925, val_acc:0.607]
Epoch [18/120    avg_loss:1.880, val_acc:0.649]
Epoch [19/120    avg_loss:1.816, val_acc:0.657]
Epoch [20/120    avg_loss:1.747, val_acc:0.669]
Epoch [21/120    avg_loss:1.703, val_acc:0.681]
Epoch [22/120    avg_loss:1.591, val_acc:0.681]
Epoch [23/120    avg_loss:1.496, val_acc:0.688]
Epoch [24/120    avg_loss:1.385, val_acc:0.724]
Epoch [25/120    avg_loss:1.252, val_acc:0.724]
Epoch [26/120    avg_loss:1.196, val_acc:0.728]
Epoch [27/120    avg_loss:1.113, val_acc:0.761]
Epoch [28/120    avg_loss:0.994, val_acc:0.762]
Epoch [29/120    avg_loss:0.938, val_acc:0.798]
Epoch [30/120    avg_loss:0.814, val_acc:0.797]
Epoch [31/120    avg_loss:0.777, val_acc:0.808]
Epoch [32/120    avg_loss:0.702, val_acc:0.839]
Epoch [33/120    avg_loss:0.653, val_acc:0.844]
Epoch [34/120    avg_loss:0.628, val_acc:0.857]
Epoch [35/120    avg_loss:0.539, val_acc:0.852]
Epoch [36/120    avg_loss:0.537, val_acc:0.887]
Epoch [37/120    avg_loss:0.439, val_acc:0.880]
Epoch [38/120    avg_loss:0.462, val_acc:0.878]
Epoch [39/120    avg_loss:0.421, val_acc:0.897]
Epoch [40/120    avg_loss:0.362, val_acc:0.907]
Epoch [41/120    avg_loss:0.369, val_acc:0.889]
Epoch [42/120    avg_loss:0.292, val_acc:0.919]
Epoch [43/120    avg_loss:0.263, val_acc:0.919]
Epoch [44/120    avg_loss:0.276, val_acc:0.913]
Epoch [45/120    avg_loss:0.235, val_acc:0.923]
Epoch [46/120    avg_loss:0.207, val_acc:0.932]
Epoch [47/120    avg_loss:0.222, val_acc:0.938]
Epoch [48/120    avg_loss:0.193, val_acc:0.898]
Epoch [49/120    avg_loss:0.246, val_acc:0.932]
Epoch [50/120    avg_loss:0.208, val_acc:0.933]
Epoch [51/120    avg_loss:0.168, val_acc:0.936]
Epoch [52/120    avg_loss:0.149, val_acc:0.940]
Epoch [53/120    avg_loss:0.151, val_acc:0.944]
Epoch [54/120    avg_loss:0.119, val_acc:0.939]
Epoch [55/120    avg_loss:0.125, val_acc:0.955]
Epoch [56/120    avg_loss:0.100, val_acc:0.954]
Epoch [57/120    avg_loss:0.102, val_acc:0.953]
Epoch [58/120    avg_loss:0.102, val_acc:0.947]
Epoch [59/120    avg_loss:0.112, val_acc:0.952]
Epoch [60/120    avg_loss:0.105, val_acc:0.941]
Epoch [61/120    avg_loss:0.105, val_acc:0.948]
Epoch [62/120    avg_loss:0.089, val_acc:0.966]
Epoch [63/120    avg_loss:0.093, val_acc:0.961]
Epoch [64/120    avg_loss:0.079, val_acc:0.962]
Epoch [65/120    avg_loss:0.065, val_acc:0.964]
Epoch [66/120    avg_loss:0.067, val_acc:0.958]
Epoch [67/120    avg_loss:0.054, val_acc:0.962]
Epoch [68/120    avg_loss:0.059, val_acc:0.968]
Epoch [69/120    avg_loss:0.059, val_acc:0.960]
Epoch [70/120    avg_loss:0.077, val_acc:0.967]
Epoch [71/120    avg_loss:0.059, val_acc:0.959]
Epoch [72/120    avg_loss:0.055, val_acc:0.959]
Epoch [73/120    avg_loss:0.089, val_acc:0.953]
Epoch [74/120    avg_loss:0.085, val_acc:0.941]
Epoch [75/120    avg_loss:0.067, val_acc:0.956]
Epoch [76/120    avg_loss:0.064, val_acc:0.964]
Epoch [77/120    avg_loss:0.077, val_acc:0.940]
Epoch [78/120    avg_loss:0.073, val_acc:0.961]
Epoch [79/120    avg_loss:0.046, val_acc:0.967]
Epoch [80/120    avg_loss:0.040, val_acc:0.977]
Epoch [81/120    avg_loss:0.040, val_acc:0.972]
Epoch [82/120    avg_loss:0.036, val_acc:0.977]
Epoch [83/120    avg_loss:0.050, val_acc:0.961]
Epoch [84/120    avg_loss:0.048, val_acc:0.966]
Epoch [85/120    avg_loss:0.055, val_acc:0.956]
Epoch [86/120    avg_loss:0.050, val_acc:0.972]
Epoch [87/120    avg_loss:0.043, val_acc:0.963]
Epoch [88/120    avg_loss:0.032, val_acc:0.974]
Epoch [89/120    avg_loss:0.030, val_acc:0.973]
Epoch [90/120    avg_loss:0.030, val_acc:0.970]
Epoch [91/120    avg_loss:0.027, val_acc:0.974]
Epoch [92/120    avg_loss:0.024, val_acc:0.977]
Epoch [93/120    avg_loss:0.022, val_acc:0.976]
Epoch [94/120    avg_loss:0.017, val_acc:0.985]
Epoch [95/120    avg_loss:0.024, val_acc:0.975]
Epoch [96/120    avg_loss:0.017, val_acc:0.977]
Epoch [97/120    avg_loss:0.021, val_acc:0.972]
Epoch [98/120    avg_loss:0.027, val_acc:0.970]
Epoch [99/120    avg_loss:0.027, val_acc:0.964]
Epoch [100/120    avg_loss:0.025, val_acc:0.977]
Epoch [101/120    avg_loss:0.021, val_acc:0.977]
Epoch [102/120    avg_loss:0.014, val_acc:0.980]
Epoch [103/120    avg_loss:0.018, val_acc:0.975]
Epoch [104/120    avg_loss:0.016, val_acc:0.981]
Epoch [105/120    avg_loss:0.015, val_acc:0.982]
Epoch [106/120    avg_loss:0.013, val_acc:0.982]
Epoch [107/120    avg_loss:0.014, val_acc:0.983]
Epoch [108/120    avg_loss:0.013, val_acc:0.984]
Epoch [109/120    avg_loss:0.013, val_acc:0.984]
Epoch [110/120    avg_loss:0.022, val_acc:0.985]
Epoch [111/120    avg_loss:0.012, val_acc:0.985]
Epoch [112/120    avg_loss:0.012, val_acc:0.983]
Epoch [113/120    avg_loss:0.013, val_acc:0.983]
Epoch [114/120    avg_loss:0.012, val_acc:0.983]
Epoch [115/120    avg_loss:0.011, val_acc:0.982]
Epoch [116/120    avg_loss:0.013, val_acc:0.982]
Epoch [117/120    avg_loss:0.011, val_acc:0.982]
Epoch [118/120    avg_loss:0.011, val_acc:0.982]
Epoch [119/120    avg_loss:0.011, val_acc:0.983]
Epoch [120/120    avg_loss:0.012, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1257    1    0    0    1    0    0    0    3   20    3    0
     0    0    0]
 [   0    0    0  722    2    5    0    0    0    8    0    2    3    5
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    5    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   21    0    0    5    5    0    0    0  828   15    0    0
     1    0    0]
 [   0    0   11    0    0    0    1    0    0    0   18 2165   15    0
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    1  524    0
     1    2    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1133    6    0]
 [   0    0    0    0    0    0   30    0    0    0    0    0    0    0
     7  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.70189701897019

F1 scores:
[       nan 0.96202532 0.97668998 0.98231293 0.9953271  0.97594502
 0.97261288 1.         0.997669   0.73469388 0.95888825 0.98119193
 0.96947271 0.98666667 0.9916849  0.93233083 0.97674419]

Kappa:
0.9738054978184306
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:11:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdd50590eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.816, val_acc:0.199]
Epoch [2/120    avg_loss:2.777, val_acc:0.238]
Epoch [3/120    avg_loss:2.745, val_acc:0.247]
Epoch [4/120    avg_loss:2.697, val_acc:0.252]
Epoch [5/120    avg_loss:2.645, val_acc:0.264]
Epoch [6/120    avg_loss:2.591, val_acc:0.320]
Epoch [7/120    avg_loss:2.527, val_acc:0.364]
Epoch [8/120    avg_loss:2.470, val_acc:0.389]
Epoch [9/120    avg_loss:2.413, val_acc:0.399]
Epoch [10/120    avg_loss:2.349, val_acc:0.416]
Epoch [11/120    avg_loss:2.309, val_acc:0.419]
Epoch [12/120    avg_loss:2.248, val_acc:0.447]
Epoch [13/120    avg_loss:2.192, val_acc:0.453]
Epoch [14/120    avg_loss:2.126, val_acc:0.449]
Epoch [15/120    avg_loss:2.001, val_acc:0.509]
Epoch [16/120    avg_loss:1.971, val_acc:0.511]
Epoch [17/120    avg_loss:1.908, val_acc:0.519]
Epoch [18/120    avg_loss:1.820, val_acc:0.597]
Epoch [19/120    avg_loss:1.725, val_acc:0.512]
Epoch [20/120    avg_loss:1.644, val_acc:0.562]
Epoch [21/120    avg_loss:1.483, val_acc:0.614]
Epoch [22/120    avg_loss:1.426, val_acc:0.562]
Epoch [23/120    avg_loss:1.331, val_acc:0.596]
Epoch [24/120    avg_loss:1.230, val_acc:0.617]
Epoch [25/120    avg_loss:1.193, val_acc:0.620]
Epoch [26/120    avg_loss:1.072, val_acc:0.666]
Epoch [27/120    avg_loss:1.004, val_acc:0.696]
Epoch [28/120    avg_loss:0.900, val_acc:0.734]
Epoch [29/120    avg_loss:0.848, val_acc:0.726]
Epoch [30/120    avg_loss:0.785, val_acc:0.772]
Epoch [31/120    avg_loss:0.753, val_acc:0.776]
Epoch [32/120    avg_loss:0.718, val_acc:0.770]
Epoch [33/120    avg_loss:0.675, val_acc:0.779]
Epoch [34/120    avg_loss:0.597, val_acc:0.788]
Epoch [35/120    avg_loss:0.532, val_acc:0.837]
Epoch [36/120    avg_loss:0.448, val_acc:0.862]
Epoch [37/120    avg_loss:0.406, val_acc:0.855]
Epoch [38/120    avg_loss:0.395, val_acc:0.855]
Epoch [39/120    avg_loss:0.401, val_acc:0.846]
Epoch [40/120    avg_loss:0.338, val_acc:0.900]
Epoch [41/120    avg_loss:0.296, val_acc:0.899]
Epoch [42/120    avg_loss:0.302, val_acc:0.900]
Epoch [43/120    avg_loss:0.264, val_acc:0.901]
Epoch [44/120    avg_loss:0.225, val_acc:0.930]
Epoch [45/120    avg_loss:0.197, val_acc:0.923]
Epoch [46/120    avg_loss:0.187, val_acc:0.921]
Epoch [47/120    avg_loss:0.171, val_acc:0.927]
Epoch [48/120    avg_loss:0.191, val_acc:0.925]
Epoch [49/120    avg_loss:0.165, val_acc:0.941]
Epoch [50/120    avg_loss:0.186, val_acc:0.940]
Epoch [51/120    avg_loss:0.150, val_acc:0.919]
Epoch [52/120    avg_loss:0.148, val_acc:0.926]
Epoch [53/120    avg_loss:0.145, val_acc:0.933]
Epoch [54/120    avg_loss:0.183, val_acc:0.919]
Epoch [55/120    avg_loss:0.166, val_acc:0.930]
Epoch [56/120    avg_loss:0.166, val_acc:0.929]
Epoch [57/120    avg_loss:0.130, val_acc:0.929]
Epoch [58/120    avg_loss:0.112, val_acc:0.939]
Epoch [59/120    avg_loss:0.110, val_acc:0.958]
Epoch [60/120    avg_loss:0.091, val_acc:0.962]
Epoch [61/120    avg_loss:0.082, val_acc:0.962]
Epoch [62/120    avg_loss:0.085, val_acc:0.943]
Epoch [63/120    avg_loss:0.072, val_acc:0.950]
Epoch [64/120    avg_loss:0.071, val_acc:0.958]
Epoch [65/120    avg_loss:0.072, val_acc:0.938]
Epoch [66/120    avg_loss:0.077, val_acc:0.959]
Epoch [67/120    avg_loss:0.075, val_acc:0.970]
Epoch [68/120    avg_loss:0.058, val_acc:0.961]
Epoch [69/120    avg_loss:0.059, val_acc:0.959]
Epoch [70/120    avg_loss:0.073, val_acc:0.969]
Epoch [71/120    avg_loss:0.058, val_acc:0.971]
Epoch [72/120    avg_loss:0.046, val_acc:0.973]
Epoch [73/120    avg_loss:0.038, val_acc:0.964]
Epoch [74/120    avg_loss:0.036, val_acc:0.976]
Epoch [75/120    avg_loss:0.035, val_acc:0.968]
Epoch [76/120    avg_loss:0.038, val_acc:0.963]
Epoch [77/120    avg_loss:0.037, val_acc:0.958]
Epoch [78/120    avg_loss:0.041, val_acc:0.975]
Epoch [79/120    avg_loss:0.035, val_acc:0.967]
Epoch [80/120    avg_loss:0.042, val_acc:0.961]
Epoch [81/120    avg_loss:0.034, val_acc:0.980]
Epoch [82/120    avg_loss:0.031, val_acc:0.981]
Epoch [83/120    avg_loss:0.027, val_acc:0.972]
Epoch [84/120    avg_loss:0.028, val_acc:0.976]
Epoch [85/120    avg_loss:0.026, val_acc:0.964]
Epoch [86/120    avg_loss:0.036, val_acc:0.966]
Epoch [87/120    avg_loss:0.032, val_acc:0.972]
Epoch [88/120    avg_loss:0.026, val_acc:0.981]
Epoch [89/120    avg_loss:0.027, val_acc:0.980]
Epoch [90/120    avg_loss:0.022, val_acc:0.980]
Epoch [91/120    avg_loss:0.019, val_acc:0.976]
Epoch [92/120    avg_loss:0.026, val_acc:0.971]
Epoch [93/120    avg_loss:0.033, val_acc:0.978]
Epoch [94/120    avg_loss:0.025, val_acc:0.978]
Epoch [95/120    avg_loss:0.024, val_acc:0.980]
Epoch [96/120    avg_loss:0.024, val_acc:0.981]
Epoch [97/120    avg_loss:0.017, val_acc:0.980]
Epoch [98/120    avg_loss:0.016, val_acc:0.980]
Epoch [99/120    avg_loss:0.016, val_acc:0.983]
Epoch [100/120    avg_loss:0.016, val_acc:0.977]
Epoch [101/120    avg_loss:0.015, val_acc:0.978]
Epoch [102/120    avg_loss:0.013, val_acc:0.976]
Epoch [103/120    avg_loss:0.013, val_acc:0.983]
Epoch [104/120    avg_loss:0.012, val_acc:0.981]
Epoch [105/120    avg_loss:0.014, val_acc:0.973]
Epoch [106/120    avg_loss:0.014, val_acc:0.980]
Epoch [107/120    avg_loss:0.017, val_acc:0.967]
Epoch [108/120    avg_loss:0.021, val_acc:0.971]
Epoch [109/120    avg_loss:0.014, val_acc:0.981]
Epoch [110/120    avg_loss:0.013, val_acc:0.983]
Epoch [111/120    avg_loss:0.013, val_acc:0.980]
Epoch [112/120    avg_loss:0.013, val_acc:0.978]
Epoch [113/120    avg_loss:0.013, val_acc:0.973]
Epoch [114/120    avg_loss:0.032, val_acc:0.969]
Epoch [115/120    avg_loss:0.016, val_acc:0.977]
Epoch [116/120    avg_loss:0.020, val_acc:0.974]
Epoch [117/120    avg_loss:0.014, val_acc:0.978]
Epoch [118/120    avg_loss:0.014, val_acc:0.978]
Epoch [119/120    avg_loss:0.012, val_acc:0.978]
Epoch [120/120    avg_loss:0.010, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    4    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1273    0    5    0    1    0    0    0    0    6    0    0
     0    0    0]
 [   0    0   11  707    1    8    0    0    0    2    2    4   10    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    2  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    7    0    0    6    1    0    0    0  841   15    3    0
     0    2    0]
 [   0    0   20    0    0    0    1    0    0    0   10 2166   13    0
     0    0    0]
 [   0    0    1    0    0   12    0    0    0    0    1    1  514    0
     0    4    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1127   12    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    14  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.73441734417344

F1 scores:
[       nan 0.94871795 0.97885429 0.97248968 0.98611111 0.96636771
 0.98277154 0.96153846 0.997669   0.91891892 0.97281666 0.98387463
 0.95539033 0.99462366 0.98643326 0.92488954 0.98203593]

Kappa:
0.9741754815691208
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f96db7f4f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.801, val_acc:0.106]
Epoch [2/120    avg_loss:2.759, val_acc:0.153]
Epoch [3/120    avg_loss:2.712, val_acc:0.220]
Epoch [4/120    avg_loss:2.643, val_acc:0.304]
Epoch [5/120    avg_loss:2.602, val_acc:0.333]
Epoch [6/120    avg_loss:2.528, val_acc:0.359]
Epoch [7/120    avg_loss:2.477, val_acc:0.460]
Epoch [8/120    avg_loss:2.427, val_acc:0.498]
Epoch [9/120    avg_loss:2.364, val_acc:0.510]
Epoch [10/120    avg_loss:2.320, val_acc:0.534]
Epoch [11/120    avg_loss:2.260, val_acc:0.541]
Epoch [12/120    avg_loss:2.215, val_acc:0.546]
Epoch [13/120    avg_loss:2.164, val_acc:0.565]
Epoch [14/120    avg_loss:2.074, val_acc:0.564]
Epoch [15/120    avg_loss:2.069, val_acc:0.592]
Epoch [16/120    avg_loss:1.950, val_acc:0.622]
Epoch [17/120    avg_loss:1.844, val_acc:0.620]
Epoch [18/120    avg_loss:1.762, val_acc:0.612]
Epoch [19/120    avg_loss:1.696, val_acc:0.640]
Epoch [20/120    avg_loss:1.648, val_acc:0.680]
Epoch [21/120    avg_loss:1.511, val_acc:0.709]
Epoch [22/120    avg_loss:1.442, val_acc:0.727]
Epoch [23/120    avg_loss:1.252, val_acc:0.728]
Epoch [24/120    avg_loss:1.204, val_acc:0.745]
Epoch [25/120    avg_loss:1.126, val_acc:0.737]
Epoch [26/120    avg_loss:1.060, val_acc:0.751]
Epoch [27/120    avg_loss:0.972, val_acc:0.781]
Epoch [28/120    avg_loss:0.886, val_acc:0.796]
Epoch [29/120    avg_loss:0.818, val_acc:0.794]
Epoch [30/120    avg_loss:0.789, val_acc:0.805]
Epoch [31/120    avg_loss:0.695, val_acc:0.809]
Epoch [32/120    avg_loss:0.627, val_acc:0.838]
Epoch [33/120    avg_loss:0.567, val_acc:0.846]
Epoch [34/120    avg_loss:0.546, val_acc:0.847]
Epoch [35/120    avg_loss:0.514, val_acc:0.867]
Epoch [36/120    avg_loss:0.473, val_acc:0.838]
Epoch [37/120    avg_loss:0.447, val_acc:0.862]
Epoch [38/120    avg_loss:0.433, val_acc:0.859]
Epoch [39/120    avg_loss:0.373, val_acc:0.894]
Epoch [40/120    avg_loss:0.307, val_acc:0.887]
Epoch [41/120    avg_loss:0.283, val_acc:0.899]
Epoch [42/120    avg_loss:0.257, val_acc:0.921]
Epoch [43/120    avg_loss:0.213, val_acc:0.912]
Epoch [44/120    avg_loss:0.282, val_acc:0.914]
Epoch [45/120    avg_loss:0.246, val_acc:0.903]
Epoch [46/120    avg_loss:0.199, val_acc:0.925]
Epoch [47/120    avg_loss:0.181, val_acc:0.929]
Epoch [48/120    avg_loss:0.207, val_acc:0.930]
Epoch [49/120    avg_loss:0.186, val_acc:0.923]
Epoch [50/120    avg_loss:0.166, val_acc:0.928]
Epoch [51/120    avg_loss:0.178, val_acc:0.934]
Epoch [52/120    avg_loss:0.152, val_acc:0.935]
Epoch [53/120    avg_loss:0.135, val_acc:0.933]
Epoch [54/120    avg_loss:0.112, val_acc:0.950]
Epoch [55/120    avg_loss:0.095, val_acc:0.947]
Epoch [56/120    avg_loss:0.127, val_acc:0.929]
Epoch [57/120    avg_loss:0.288, val_acc:0.898]
Epoch [58/120    avg_loss:0.182, val_acc:0.926]
Epoch [59/120    avg_loss:0.134, val_acc:0.915]
Epoch [60/120    avg_loss:0.149, val_acc:0.925]
Epoch [61/120    avg_loss:0.125, val_acc:0.955]
Epoch [62/120    avg_loss:0.124, val_acc:0.918]
Epoch [63/120    avg_loss:0.098, val_acc:0.954]
Epoch [64/120    avg_loss:0.088, val_acc:0.959]
Epoch [65/120    avg_loss:0.079, val_acc:0.945]
Epoch [66/120    avg_loss:0.070, val_acc:0.960]
Epoch [67/120    avg_loss:0.071, val_acc:0.955]
Epoch [68/120    avg_loss:0.066, val_acc:0.955]
Epoch [69/120    avg_loss:0.054, val_acc:0.959]
Epoch [70/120    avg_loss:0.057, val_acc:0.966]
Epoch [71/120    avg_loss:0.051, val_acc:0.956]
Epoch [72/120    avg_loss:0.057, val_acc:0.962]
Epoch [73/120    avg_loss:0.050, val_acc:0.968]
Epoch [74/120    avg_loss:0.037, val_acc:0.974]
Epoch [75/120    avg_loss:0.038, val_acc:0.975]
Epoch [76/120    avg_loss:0.049, val_acc:0.971]
Epoch [77/120    avg_loss:0.044, val_acc:0.966]
Epoch [78/120    avg_loss:0.044, val_acc:0.967]
Epoch [79/120    avg_loss:0.054, val_acc:0.972]
Epoch [80/120    avg_loss:0.052, val_acc:0.969]
Epoch [81/120    avg_loss:0.046, val_acc:0.967]
Epoch [82/120    avg_loss:0.038, val_acc:0.969]
Epoch [83/120    avg_loss:0.028, val_acc:0.976]
Epoch [84/120    avg_loss:0.028, val_acc:0.983]
Epoch [85/120    avg_loss:0.030, val_acc:0.960]
Epoch [86/120    avg_loss:0.041, val_acc:0.974]
Epoch [87/120    avg_loss:0.027, val_acc:0.974]
Epoch [88/120    avg_loss:0.027, val_acc:0.977]
Epoch [89/120    avg_loss:0.023, val_acc:0.971]
Epoch [90/120    avg_loss:0.023, val_acc:0.976]
Epoch [91/120    avg_loss:0.020, val_acc:0.976]
Epoch [92/120    avg_loss:0.032, val_acc:0.964]
Epoch [93/120    avg_loss:0.030, val_acc:0.975]
Epoch [94/120    avg_loss:0.043, val_acc:0.970]
Epoch [95/120    avg_loss:0.024, val_acc:0.976]
Epoch [96/120    avg_loss:0.021, val_acc:0.981]
Epoch [97/120    avg_loss:0.021, val_acc:0.978]
Epoch [98/120    avg_loss:0.019, val_acc:0.977]
Epoch [99/120    avg_loss:0.021, val_acc:0.978]
Epoch [100/120    avg_loss:0.023, val_acc:0.980]
Epoch [101/120    avg_loss:0.017, val_acc:0.981]
Epoch [102/120    avg_loss:0.017, val_acc:0.977]
Epoch [103/120    avg_loss:0.017, val_acc:0.977]
Epoch [104/120    avg_loss:0.015, val_acc:0.977]
Epoch [105/120    avg_loss:0.015, val_acc:0.980]
Epoch [106/120    avg_loss:0.017, val_acc:0.978]
Epoch [107/120    avg_loss:0.015, val_acc:0.976]
Epoch [108/120    avg_loss:0.016, val_acc:0.976]
Epoch [109/120    avg_loss:0.016, val_acc:0.977]
Epoch [110/120    avg_loss:0.018, val_acc:0.978]
Epoch [111/120    avg_loss:0.014, val_acc:0.978]
Epoch [112/120    avg_loss:0.018, val_acc:0.978]
Epoch [113/120    avg_loss:0.017, val_acc:0.978]
Epoch [114/120    avg_loss:0.013, val_acc:0.978]
Epoch [115/120    avg_loss:0.016, val_acc:0.978]
Epoch [116/120    avg_loss:0.017, val_acc:0.978]
Epoch [117/120    avg_loss:0.014, val_acc:0.978]
Epoch [118/120    avg_loss:0.020, val_acc:0.978]
Epoch [119/120    avg_loss:0.015, val_acc:0.978]
Epoch [120/120    avg_loss:0.017, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1264    1    3    0    1    0    0    0    2   14    0    0
     0    0    0]
 [   0    0    3  705    3   12    0    0    0   16    4    0    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    1    0    0    1    0    0   14    0    0    2    0
     0    0    0]
 [   0    0    7    0    0    7    2    0    0    0  838   19    2    0
     0    0    0]
 [   0    0    7    0    0    0    1    0    5    0    7 2186    2    2
     0    0    0]
 [   0    0    0    0    1   16    0    0    0    2    1    0  509    0
     2    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0    0    0
  1128    9    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
     9  315    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.74525745257452

F1 scores:
[       nan 0.94871795 0.98519096 0.96973865 0.98383372 0.95671476
 0.97761194 1.         0.99188876 0.51851852 0.96710906 0.98668472
 0.96492891 0.99462366 0.99034241 0.93889717 0.98245614]

Kappa:
0.9742960840349265
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fac55d35f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.816, val_acc:0.197]
Epoch [2/120    avg_loss:2.765, val_acc:0.224]
Epoch [3/120    avg_loss:2.713, val_acc:0.295]
Epoch [4/120    avg_loss:2.648, val_acc:0.304]
Epoch [5/120    avg_loss:2.583, val_acc:0.318]
Epoch [6/120    avg_loss:2.528, val_acc:0.338]
Epoch [7/120    avg_loss:2.451, val_acc:0.426]
Epoch [8/120    avg_loss:2.403, val_acc:0.488]
Epoch [9/120    avg_loss:2.356, val_acc:0.505]
Epoch [10/120    avg_loss:2.282, val_acc:0.533]
Epoch [11/120    avg_loss:2.230, val_acc:0.577]
Epoch [12/120    avg_loss:2.171, val_acc:0.577]
Epoch [13/120    avg_loss:2.103, val_acc:0.606]
Epoch [14/120    avg_loss:2.056, val_acc:0.622]
Epoch [15/120    avg_loss:1.943, val_acc:0.633]
Epoch [16/120    avg_loss:1.868, val_acc:0.683]
Epoch [17/120    avg_loss:1.729, val_acc:0.714]
Epoch [18/120    avg_loss:1.624, val_acc:0.702]
Epoch [19/120    avg_loss:1.490, val_acc:0.718]
Epoch [20/120    avg_loss:1.291, val_acc:0.723]
Epoch [21/120    avg_loss:1.234, val_acc:0.751]
Epoch [22/120    avg_loss:1.109, val_acc:0.746]
Epoch [23/120    avg_loss:1.032, val_acc:0.761]
Epoch [24/120    avg_loss:0.915, val_acc:0.768]
Epoch [25/120    avg_loss:0.862, val_acc:0.788]
Epoch [26/120    avg_loss:0.785, val_acc:0.803]
Epoch [27/120    avg_loss:0.713, val_acc:0.822]
Epoch [28/120    avg_loss:0.648, val_acc:0.822]
Epoch [29/120    avg_loss:0.627, val_acc:0.827]
Epoch [30/120    avg_loss:0.592, val_acc:0.856]
Epoch [31/120    avg_loss:0.518, val_acc:0.870]
Epoch [32/120    avg_loss:0.482, val_acc:0.888]
Epoch [33/120    avg_loss:0.444, val_acc:0.852]
Epoch [34/120    avg_loss:0.406, val_acc:0.902]
Epoch [35/120    avg_loss:0.333, val_acc:0.928]
Epoch [36/120    avg_loss:0.289, val_acc:0.934]
Epoch [37/120    avg_loss:0.247, val_acc:0.928]
Epoch [38/120    avg_loss:0.221, val_acc:0.920]
Epoch [39/120    avg_loss:0.221, val_acc:0.938]
Epoch [40/120    avg_loss:0.218, val_acc:0.957]
Epoch [41/120    avg_loss:0.188, val_acc:0.957]
Epoch [42/120    avg_loss:0.181, val_acc:0.928]
Epoch [43/120    avg_loss:0.192, val_acc:0.948]
Epoch [44/120    avg_loss:0.170, val_acc:0.962]
Epoch [45/120    avg_loss:0.144, val_acc:0.963]
Epoch [46/120    avg_loss:0.152, val_acc:0.954]
Epoch [47/120    avg_loss:0.144, val_acc:0.945]
Epoch [48/120    avg_loss:0.114, val_acc:0.963]
Epoch [49/120    avg_loss:0.107, val_acc:0.966]
Epoch [50/120    avg_loss:0.097, val_acc:0.958]
Epoch [51/120    avg_loss:0.102, val_acc:0.963]
Epoch [52/120    avg_loss:0.092, val_acc:0.970]
Epoch [53/120    avg_loss:0.083, val_acc:0.966]
Epoch [54/120    avg_loss:0.081, val_acc:0.967]
Epoch [55/120    avg_loss:0.066, val_acc:0.970]
Epoch [56/120    avg_loss:0.075, val_acc:0.967]
Epoch [57/120    avg_loss:0.065, val_acc:0.973]
Epoch [58/120    avg_loss:0.083, val_acc:0.959]
Epoch [59/120    avg_loss:0.072, val_acc:0.959]
Epoch [60/120    avg_loss:0.072, val_acc:0.963]
Epoch [61/120    avg_loss:0.059, val_acc:0.974]
Epoch [62/120    avg_loss:0.069, val_acc:0.969]
Epoch [63/120    avg_loss:0.057, val_acc:0.976]
Epoch [64/120    avg_loss:0.043, val_acc:0.980]
Epoch [65/120    avg_loss:0.057, val_acc:0.972]
Epoch [66/120    avg_loss:0.051, val_acc:0.974]
Epoch [67/120    avg_loss:0.043, val_acc:0.985]
Epoch [68/120    avg_loss:0.035, val_acc:0.978]
Epoch [69/120    avg_loss:0.044, val_acc:0.980]
Epoch [70/120    avg_loss:0.041, val_acc:0.977]
Epoch [71/120    avg_loss:0.047, val_acc:0.985]
Epoch [72/120    avg_loss:0.034, val_acc:0.987]
Epoch [73/120    avg_loss:0.033, val_acc:0.974]
Epoch [74/120    avg_loss:0.036, val_acc:0.981]
Epoch [75/120    avg_loss:0.032, val_acc:0.989]
Epoch [76/120    avg_loss:0.030, val_acc:0.980]
Epoch [77/120    avg_loss:0.039, val_acc:0.984]
Epoch [78/120    avg_loss:0.026, val_acc:0.985]
Epoch [79/120    avg_loss:0.022, val_acc:0.985]
Epoch [80/120    avg_loss:0.024, val_acc:0.984]
Epoch [81/120    avg_loss:0.026, val_acc:0.982]
Epoch [82/120    avg_loss:0.024, val_acc:0.987]
Epoch [83/120    avg_loss:0.021, val_acc:0.980]
Epoch [84/120    avg_loss:0.035, val_acc:0.981]
Epoch [85/120    avg_loss:0.035, val_acc:0.980]
Epoch [86/120    avg_loss:0.033, val_acc:0.983]
Epoch [87/120    avg_loss:0.026, val_acc:0.976]
Epoch [88/120    avg_loss:0.025, val_acc:0.983]
Epoch [89/120    avg_loss:0.018, val_acc:0.985]
Epoch [90/120    avg_loss:0.018, val_acc:0.988]
Epoch [91/120    avg_loss:0.016, val_acc:0.988]
Epoch [92/120    avg_loss:0.015, val_acc:0.987]
Epoch [93/120    avg_loss:0.015, val_acc:0.987]
Epoch [94/120    avg_loss:0.014, val_acc:0.988]
Epoch [95/120    avg_loss:0.014, val_acc:0.988]
Epoch [96/120    avg_loss:0.015, val_acc:0.988]
Epoch [97/120    avg_loss:0.015, val_acc:0.988]
Epoch [98/120    avg_loss:0.014, val_acc:0.988]
Epoch [99/120    avg_loss:0.015, val_acc:0.989]
Epoch [100/120    avg_loss:0.012, val_acc:0.989]
Epoch [101/120    avg_loss:0.013, val_acc:0.988]
Epoch [102/120    avg_loss:0.014, val_acc:0.989]
Epoch [103/120    avg_loss:0.014, val_acc:0.988]
Epoch [104/120    avg_loss:0.013, val_acc:0.988]
Epoch [105/120    avg_loss:0.013, val_acc:0.989]
Epoch [106/120    avg_loss:0.016, val_acc:0.989]
Epoch [107/120    avg_loss:0.013, val_acc:0.989]
Epoch [108/120    avg_loss:0.013, val_acc:0.988]
Epoch [109/120    avg_loss:0.013, val_acc:0.988]
Epoch [110/120    avg_loss:0.013, val_acc:0.989]
Epoch [111/120    avg_loss:0.013, val_acc:0.988]
Epoch [112/120    avg_loss:0.012, val_acc:0.988]
Epoch [113/120    avg_loss:0.012, val_acc:0.988]
Epoch [114/120    avg_loss:0.014, val_acc:0.989]
Epoch [115/120    avg_loss:0.013, val_acc:0.989]
Epoch [116/120    avg_loss:0.013, val_acc:0.989]
Epoch [117/120    avg_loss:0.013, val_acc:0.989]
Epoch [118/120    avg_loss:0.014, val_acc:0.987]
Epoch [119/120    avg_loss:0.012, val_acc:0.987]
Epoch [120/120    avg_loss:0.013, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1266    0    8    1    0    0    0    0    2    8    0    0
     0    0    0]
 [   0    0    1  715   13    0    0    0    0    4    1    0   13    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    1    0    0   14    0    0    1    0
     0    0    0]
 [   0    0    3    1    0    3    1    0    0    0  843   24    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    1    0   20 2173   10    0
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    7    6  516    0
     2    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1130    8    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
    20  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.81029810298104

F1 scores:
[       nan 0.975      0.9886763  0.97610922 0.95302013 0.99084668
 0.97910448 1.         0.99883856 0.75675676 0.96287836 0.982591
 0.96089385 0.99728997 0.98603839 0.91933029 0.99408284]

Kappa:
0.9750340228343735
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe4c38fbef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.823, val_acc:0.015]
Epoch [2/120    avg_loss:2.787, val_acc:0.108]
Epoch [3/120    avg_loss:2.743, val_acc:0.405]
Epoch [4/120    avg_loss:2.691, val_acc:0.446]
Epoch [5/120    avg_loss:2.634, val_acc:0.469]
Epoch [6/120    avg_loss:2.573, val_acc:0.508]
Epoch [7/120    avg_loss:2.505, val_acc:0.511]
Epoch [8/120    avg_loss:2.464, val_acc:0.536]
Epoch [9/120    avg_loss:2.388, val_acc:0.542]
Epoch [10/120    avg_loss:2.337, val_acc:0.543]
Epoch [11/120    avg_loss:2.268, val_acc:0.562]
Epoch [12/120    avg_loss:2.211, val_acc:0.570]
Epoch [13/120    avg_loss:2.170, val_acc:0.560]
Epoch [14/120    avg_loss:2.095, val_acc:0.572]
Epoch [15/120    avg_loss:1.995, val_acc:0.631]
Epoch [16/120    avg_loss:1.917, val_acc:0.607]
Epoch [17/120    avg_loss:1.804, val_acc:0.626]
Epoch [18/120    avg_loss:1.702, val_acc:0.679]
Epoch [19/120    avg_loss:1.558, val_acc:0.654]
Epoch [20/120    avg_loss:1.512, val_acc:0.669]
Epoch [21/120    avg_loss:1.377, val_acc:0.724]
Epoch [22/120    avg_loss:1.295, val_acc:0.732]
Epoch [23/120    avg_loss:1.235, val_acc:0.724]
Epoch [24/120    avg_loss:1.117, val_acc:0.725]
Epoch [25/120    avg_loss:1.036, val_acc:0.766]
Epoch [26/120    avg_loss:0.969, val_acc:0.774]
Epoch [27/120    avg_loss:0.947, val_acc:0.794]
Epoch [28/120    avg_loss:0.837, val_acc:0.732]
Epoch [29/120    avg_loss:0.793, val_acc:0.815]
Epoch [30/120    avg_loss:0.681, val_acc:0.837]
Epoch [31/120    avg_loss:0.654, val_acc:0.848]
Epoch [32/120    avg_loss:0.630, val_acc:0.867]
Epoch [33/120    avg_loss:0.577, val_acc:0.887]
Epoch [34/120    avg_loss:0.529, val_acc:0.870]
Epoch [35/120    avg_loss:0.473, val_acc:0.875]
Epoch [36/120    avg_loss:0.411, val_acc:0.880]
Epoch [37/120    avg_loss:0.425, val_acc:0.872]
Epoch [38/120    avg_loss:0.350, val_acc:0.905]
Epoch [39/120    avg_loss:0.354, val_acc:0.859]
Epoch [40/120    avg_loss:0.388, val_acc:0.922]
Epoch [41/120    avg_loss:0.325, val_acc:0.911]
Epoch [42/120    avg_loss:0.251, val_acc:0.953]
Epoch [43/120    avg_loss:0.239, val_acc:0.939]
Epoch [44/120    avg_loss:0.211, val_acc:0.940]
Epoch [45/120    avg_loss:0.193, val_acc:0.955]
Epoch [46/120    avg_loss:0.163, val_acc:0.966]
Epoch [47/120    avg_loss:0.166, val_acc:0.944]
Epoch [48/120    avg_loss:0.153, val_acc:0.960]
Epoch [49/120    avg_loss:0.149, val_acc:0.954]
Epoch [50/120    avg_loss:0.127, val_acc:0.971]
Epoch [51/120    avg_loss:0.133, val_acc:0.947]
Epoch [52/120    avg_loss:0.202, val_acc:0.918]
Epoch [53/120    avg_loss:0.171, val_acc:0.957]
Epoch [54/120    avg_loss:0.147, val_acc:0.964]
Epoch [55/120    avg_loss:0.118, val_acc:0.963]
Epoch [56/120    avg_loss:0.101, val_acc:0.969]
Epoch [57/120    avg_loss:0.093, val_acc:0.959]
Epoch [58/120    avg_loss:0.081, val_acc:0.971]
Epoch [59/120    avg_loss:0.087, val_acc:0.964]
Epoch [60/120    avg_loss:0.091, val_acc:0.970]
Epoch [61/120    avg_loss:0.065, val_acc:0.971]
Epoch [62/120    avg_loss:0.083, val_acc:0.957]
Epoch [63/120    avg_loss:0.097, val_acc:0.973]
Epoch [64/120    avg_loss:0.103, val_acc:0.959]
Epoch [65/120    avg_loss:0.094, val_acc:0.974]
Epoch [66/120    avg_loss:0.062, val_acc:0.974]
Epoch [67/120    avg_loss:0.051, val_acc:0.973]
Epoch [68/120    avg_loss:0.058, val_acc:0.971]
Epoch [69/120    avg_loss:0.046, val_acc:0.977]
Epoch [70/120    avg_loss:0.049, val_acc:0.981]
Epoch [71/120    avg_loss:0.037, val_acc:0.974]
Epoch [72/120    avg_loss:0.039, val_acc:0.975]
Epoch [73/120    avg_loss:0.034, val_acc:0.974]
Epoch [74/120    avg_loss:0.041, val_acc:0.967]
Epoch [75/120    avg_loss:0.034, val_acc:0.977]
Epoch [76/120    avg_loss:0.040, val_acc:0.975]
Epoch [77/120    avg_loss:0.034, val_acc:0.978]
Epoch [78/120    avg_loss:0.037, val_acc:0.974]
Epoch [79/120    avg_loss:0.046, val_acc:0.974]
Epoch [80/120    avg_loss:0.043, val_acc:0.975]
Epoch [81/120    avg_loss:0.035, val_acc:0.982]
Epoch [82/120    avg_loss:0.030, val_acc:0.977]
Epoch [83/120    avg_loss:0.036, val_acc:0.974]
Epoch [84/120    avg_loss:0.025, val_acc:0.982]
Epoch [85/120    avg_loss:0.023, val_acc:0.981]
Epoch [86/120    avg_loss:0.032, val_acc:0.950]
Epoch [87/120    avg_loss:0.034, val_acc:0.984]
Epoch [88/120    avg_loss:0.033, val_acc:0.975]
Epoch [89/120    avg_loss:0.035, val_acc:0.970]
Epoch [90/120    avg_loss:0.031, val_acc:0.977]
Epoch [91/120    avg_loss:0.028, val_acc:0.974]
Epoch [92/120    avg_loss:0.025, val_acc:0.976]
Epoch [93/120    avg_loss:0.022, val_acc:0.975]
Epoch [94/120    avg_loss:0.019, val_acc:0.977]
Epoch [95/120    avg_loss:0.022, val_acc:0.978]
Epoch [96/120    avg_loss:0.017, val_acc:0.980]
Epoch [97/120    avg_loss:0.016, val_acc:0.983]
Epoch [98/120    avg_loss:0.020, val_acc:0.978]
Epoch [99/120    avg_loss:0.021, val_acc:0.976]
Epoch [100/120    avg_loss:0.022, val_acc:0.978]
Epoch [101/120    avg_loss:0.018, val_acc:0.981]
Epoch [102/120    avg_loss:0.016, val_acc:0.980]
Epoch [103/120    avg_loss:0.013, val_acc:0.981]
Epoch [104/120    avg_loss:0.013, val_acc:0.981]
Epoch [105/120    avg_loss:0.012, val_acc:0.980]
Epoch [106/120    avg_loss:0.013, val_acc:0.980]
Epoch [107/120    avg_loss:0.014, val_acc:0.980]
Epoch [108/120    avg_loss:0.013, val_acc:0.980]
Epoch [109/120    avg_loss:0.013, val_acc:0.980]
Epoch [110/120    avg_loss:0.011, val_acc:0.980]
Epoch [111/120    avg_loss:0.013, val_acc:0.980]
Epoch [112/120    avg_loss:0.012, val_acc:0.980]
Epoch [113/120    avg_loss:0.010, val_acc:0.980]
Epoch [114/120    avg_loss:0.011, val_acc:0.980]
Epoch [115/120    avg_loss:0.011, val_acc:0.980]
Epoch [116/120    avg_loss:0.012, val_acc:0.980]
Epoch [117/120    avg_loss:0.011, val_acc:0.980]
Epoch [118/120    avg_loss:0.011, val_acc:0.980]
Epoch [119/120    avg_loss:0.010, val_acc:0.980]
Epoch [120/120    avg_loss:0.012, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    0    6    0    1    0    0    0    2   15    0    0
     0    0    0]
 [   0    0    3  691    4   22    0    0    0    9    3    1   14    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    1    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  425    0    0    0    5    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    1    0    3    0    0    0    0  861    9    0    0
     0    0    0]
 [   0    0   11    1    0    0    0    0    0    0    9 2182    7    0
     0    0    0]
 [   0    0    1    0    0    4    0    0    0    0    4    1  520    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1138    1    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    38  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.81029810298104

F1 scores:
[       nan 0.975      0.98361934 0.95972222 0.97706422 0.96428571
 0.98570354 0.98039216 0.99415205 0.7826087  0.98175599 0.98755375
 0.96296296 1.         0.98188093 0.91588785 0.98823529]

Kappa:
0.9750325329398528
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff24ae44f98>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.833, val_acc:0.077]
Epoch [2/120    avg_loss:2.780, val_acc:0.234]
Epoch [3/120    avg_loss:2.728, val_acc:0.253]
Epoch [4/120    avg_loss:2.643, val_acc:0.240]
Epoch [5/120    avg_loss:2.568, val_acc:0.239]
Epoch [6/120    avg_loss:2.508, val_acc:0.341]
Epoch [7/120    avg_loss:2.445, val_acc:0.391]
Epoch [8/120    avg_loss:2.441, val_acc:0.415]
Epoch [9/120    avg_loss:2.378, val_acc:0.445]
Epoch [10/120    avg_loss:2.320, val_acc:0.466]
Epoch [11/120    avg_loss:2.302, val_acc:0.494]
Epoch [12/120    avg_loss:2.186, val_acc:0.543]
Epoch [13/120    avg_loss:2.151, val_acc:0.540]
Epoch [14/120    avg_loss:2.096, val_acc:0.584]
Epoch [15/120    avg_loss:2.021, val_acc:0.566]
Epoch [16/120    avg_loss:1.932, val_acc:0.622]
Epoch [17/120    avg_loss:1.880, val_acc:0.624]
Epoch [18/120    avg_loss:1.815, val_acc:0.645]
Epoch [19/120    avg_loss:1.679, val_acc:0.696]
Epoch [20/120    avg_loss:1.542, val_acc:0.678]
Epoch [21/120    avg_loss:1.441, val_acc:0.710]
Epoch [22/120    avg_loss:1.381, val_acc:0.712]
Epoch [23/120    avg_loss:1.224, val_acc:0.731]
Epoch [24/120    avg_loss:1.092, val_acc:0.744]
Epoch [25/120    avg_loss:1.063, val_acc:0.708]
Epoch [26/120    avg_loss:1.018, val_acc:0.784]
Epoch [27/120    avg_loss:0.895, val_acc:0.750]
Epoch [28/120    avg_loss:0.836, val_acc:0.773]
Epoch [29/120    avg_loss:0.763, val_acc:0.811]
Epoch [30/120    avg_loss:0.688, val_acc:0.850]
Epoch [31/120    avg_loss:0.602, val_acc:0.856]
Epoch [32/120    avg_loss:0.548, val_acc:0.851]
Epoch [33/120    avg_loss:0.492, val_acc:0.849]
Epoch [34/120    avg_loss:0.504, val_acc:0.861]
Epoch [35/120    avg_loss:0.457, val_acc:0.856]
Epoch [36/120    avg_loss:0.407, val_acc:0.864]
Epoch [37/120    avg_loss:0.394, val_acc:0.888]
Epoch [38/120    avg_loss:0.367, val_acc:0.879]
Epoch [39/120    avg_loss:0.318, val_acc:0.883]
Epoch [40/120    avg_loss:0.251, val_acc:0.922]
Epoch [41/120    avg_loss:0.267, val_acc:0.894]
Epoch [42/120    avg_loss:0.223, val_acc:0.929]
Epoch [43/120    avg_loss:0.210, val_acc:0.931]
Epoch [44/120    avg_loss:0.207, val_acc:0.923]
Epoch [45/120    avg_loss:0.179, val_acc:0.936]
Epoch [46/120    avg_loss:0.173, val_acc:0.916]
Epoch [47/120    avg_loss:0.164, val_acc:0.939]
Epoch [48/120    avg_loss:0.151, val_acc:0.933]
Epoch [49/120    avg_loss:0.157, val_acc:0.928]
Epoch [50/120    avg_loss:0.153, val_acc:0.932]
Epoch [51/120    avg_loss:0.136, val_acc:0.934]
Epoch [52/120    avg_loss:0.107, val_acc:0.934]
Epoch [53/120    avg_loss:0.128, val_acc:0.927]
Epoch [54/120    avg_loss:0.131, val_acc:0.942]
Epoch [55/120    avg_loss:0.118, val_acc:0.944]
Epoch [56/120    avg_loss:0.118, val_acc:0.950]
Epoch [57/120    avg_loss:0.086, val_acc:0.955]
Epoch [58/120    avg_loss:0.080, val_acc:0.945]
Epoch [59/120    avg_loss:0.089, val_acc:0.945]
Epoch [60/120    avg_loss:0.107, val_acc:0.957]
Epoch [61/120    avg_loss:0.075, val_acc:0.950]
Epoch [62/120    avg_loss:0.074, val_acc:0.946]
Epoch [63/120    avg_loss:0.074, val_acc:0.938]
Epoch [64/120    avg_loss:0.088, val_acc:0.955]
Epoch [65/120    avg_loss:0.068, val_acc:0.966]
Epoch [66/120    avg_loss:0.073, val_acc:0.964]
Epoch [67/120    avg_loss:0.068, val_acc:0.959]
Epoch [68/120    avg_loss:0.069, val_acc:0.939]
Epoch [69/120    avg_loss:0.136, val_acc:0.931]
Epoch [70/120    avg_loss:0.110, val_acc:0.958]
Epoch [71/120    avg_loss:0.077, val_acc:0.953]
Epoch [72/120    avg_loss:0.088, val_acc:0.956]
Epoch [73/120    avg_loss:0.066, val_acc:0.961]
Epoch [74/120    avg_loss:0.055, val_acc:0.969]
Epoch [75/120    avg_loss:0.056, val_acc:0.974]
Epoch [76/120    avg_loss:0.038, val_acc:0.975]
Epoch [77/120    avg_loss:0.051, val_acc:0.966]
Epoch [78/120    avg_loss:0.050, val_acc:0.959]
Epoch [79/120    avg_loss:0.042, val_acc:0.980]
Epoch [80/120    avg_loss:0.031, val_acc:0.964]
Epoch [81/120    avg_loss:0.039, val_acc:0.964]
Epoch [82/120    avg_loss:0.042, val_acc:0.947]
Epoch [83/120    avg_loss:0.044, val_acc:0.947]
Epoch [84/120    avg_loss:0.046, val_acc:0.960]
Epoch [85/120    avg_loss:0.041, val_acc:0.952]
Epoch [86/120    avg_loss:0.056, val_acc:0.960]
Epoch [87/120    avg_loss:0.050, val_acc:0.962]
Epoch [88/120    avg_loss:0.034, val_acc:0.966]
Epoch [89/120    avg_loss:0.033, val_acc:0.975]
Epoch [90/120    avg_loss:0.031, val_acc:0.980]
Epoch [91/120    avg_loss:0.027, val_acc:0.973]
Epoch [92/120    avg_loss:0.038, val_acc:0.968]
Epoch [93/120    avg_loss:0.036, val_acc:0.962]
Epoch [94/120    avg_loss:0.042, val_acc:0.976]
Epoch [95/120    avg_loss:0.033, val_acc:0.974]
Epoch [96/120    avg_loss:0.023, val_acc:0.978]
Epoch [97/120    avg_loss:0.021, val_acc:0.964]
Epoch [98/120    avg_loss:0.024, val_acc:0.977]
Epoch [99/120    avg_loss:0.020, val_acc:0.977]
Epoch [100/120    avg_loss:0.023, val_acc:0.975]
Epoch [101/120    avg_loss:0.020, val_acc:0.975]
Epoch [102/120    avg_loss:0.019, val_acc:0.983]
Epoch [103/120    avg_loss:0.016, val_acc:0.982]
Epoch [104/120    avg_loss:0.017, val_acc:0.985]
Epoch [105/120    avg_loss:0.020, val_acc:0.981]
Epoch [106/120    avg_loss:0.019, val_acc:0.980]
Epoch [107/120    avg_loss:0.020, val_acc:0.984]
Epoch [108/120    avg_loss:0.016, val_acc:0.986]
Epoch [109/120    avg_loss:0.013, val_acc:0.983]
Epoch [110/120    avg_loss:0.014, val_acc:0.982]
Epoch [111/120    avg_loss:0.014, val_acc:0.986]
Epoch [112/120    avg_loss:0.014, val_acc:0.985]
Epoch [113/120    avg_loss:0.014, val_acc:0.967]
Epoch [114/120    avg_loss:0.017, val_acc:0.974]
Epoch [115/120    avg_loss:0.017, val_acc:0.984]
Epoch [116/120    avg_loss:0.014, val_acc:0.977]
Epoch [117/120    avg_loss:0.014, val_acc:0.986]
Epoch [118/120    avg_loss:0.011, val_acc:0.983]
Epoch [119/120    avg_loss:0.009, val_acc:0.982]
Epoch [120/120    avg_loss:0.010, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1256    1   14    0    4    0    0    0    4    6    0    0
     0    0    0]
 [   0    0    2  699    8   13    0    0    0   12    4    0    5    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   17    0    0    3    0    0    0    0  848    7    0    0
     0    0    0]
 [   0    0   10    0    0    0    1    0    0    0   12 2175   12    0
     0    0    0]
 [   0    0   11    0    0    5    0    0    0    1    0    3  510    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1137    1    0]
 [   0    0    0    0    0    0   31    0    0    0    0    0    0    0
    12  304    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.60433604336043

F1 scores:
[       nan 0.96202532 0.97326618 0.96613683 0.95089286 0.96949153
 0.97261288 0.98039216 0.997669   0.66666667 0.9708071  0.98841172
 0.95864662 0.98930481 0.99171391 0.93251534 0.98245614]

Kappa:
0.9726980252746735
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff75f797eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.832, val_acc:0.127]
Epoch [2/120    avg_loss:2.794, val_acc:0.151]
Epoch [3/120    avg_loss:2.756, val_acc:0.194]
Epoch [4/120    avg_loss:2.705, val_acc:0.254]
Epoch [5/120    avg_loss:2.653, val_acc:0.289]
Epoch [6/120    avg_loss:2.558, val_acc:0.302]
Epoch [7/120    avg_loss:2.502, val_acc:0.270]
Epoch [8/120    avg_loss:2.457, val_acc:0.276]
Epoch [9/120    avg_loss:2.382, val_acc:0.309]
Epoch [10/120    avg_loss:2.340, val_acc:0.384]
Epoch [11/120    avg_loss:2.279, val_acc:0.395]
Epoch [12/120    avg_loss:2.230, val_acc:0.436]
Epoch [13/120    avg_loss:2.194, val_acc:0.458]
Epoch [14/120    avg_loss:2.126, val_acc:0.516]
Epoch [15/120    avg_loss:2.067, val_acc:0.555]
Epoch [16/120    avg_loss:2.005, val_acc:0.580]
Epoch [17/120    avg_loss:1.916, val_acc:0.607]
Epoch [18/120    avg_loss:1.813, val_acc:0.595]
Epoch [19/120    avg_loss:1.722, val_acc:0.644]
Epoch [20/120    avg_loss:1.624, val_acc:0.662]
Epoch [21/120    avg_loss:1.520, val_acc:0.661]
Epoch [22/120    avg_loss:1.417, val_acc:0.686]
Epoch [23/120    avg_loss:1.340, val_acc:0.683]
Epoch [24/120    avg_loss:1.276, val_acc:0.719]
Epoch [25/120    avg_loss:1.270, val_acc:0.677]
Epoch [26/120    avg_loss:1.180, val_acc:0.728]
Epoch [27/120    avg_loss:1.044, val_acc:0.708]
Epoch [28/120    avg_loss:0.982, val_acc:0.766]
Epoch [29/120    avg_loss:0.880, val_acc:0.774]
Epoch [30/120    avg_loss:0.839, val_acc:0.786]
Epoch [31/120    avg_loss:0.764, val_acc:0.805]
Epoch [32/120    avg_loss:0.727, val_acc:0.808]
Epoch [33/120    avg_loss:0.643, val_acc:0.828]
Epoch [34/120    avg_loss:0.582, val_acc:0.851]
Epoch [35/120    avg_loss:0.535, val_acc:0.852]
Epoch [36/120    avg_loss:0.496, val_acc:0.861]
Epoch [37/120    avg_loss:0.494, val_acc:0.857]
Epoch [38/120    avg_loss:0.415, val_acc:0.866]
Epoch [39/120    avg_loss:0.393, val_acc:0.902]
Epoch [40/120    avg_loss:0.338, val_acc:0.913]
Epoch [41/120    avg_loss:0.337, val_acc:0.902]
Epoch [42/120    avg_loss:0.333, val_acc:0.883]
Epoch [43/120    avg_loss:0.319, val_acc:0.908]
Epoch [44/120    avg_loss:0.250, val_acc:0.925]
Epoch [45/120    avg_loss:0.212, val_acc:0.933]
Epoch [46/120    avg_loss:0.202, val_acc:0.923]
Epoch [47/120    avg_loss:0.191, val_acc:0.927]
Epoch [48/120    avg_loss:0.205, val_acc:0.929]
Epoch [49/120    avg_loss:0.204, val_acc:0.929]
Epoch [50/120    avg_loss:0.183, val_acc:0.933]
Epoch [51/120    avg_loss:0.164, val_acc:0.934]
Epoch [52/120    avg_loss:0.173, val_acc:0.940]
Epoch [53/120    avg_loss:0.149, val_acc:0.942]
Epoch [54/120    avg_loss:0.116, val_acc:0.942]
Epoch [55/120    avg_loss:0.127, val_acc:0.947]
Epoch [56/120    avg_loss:0.111, val_acc:0.946]
Epoch [57/120    avg_loss:0.088, val_acc:0.947]
Epoch [58/120    avg_loss:0.112, val_acc:0.939]
Epoch [59/120    avg_loss:0.137, val_acc:0.940]
Epoch [60/120    avg_loss:0.096, val_acc:0.940]
Epoch [61/120    avg_loss:0.086, val_acc:0.952]
Epoch [62/120    avg_loss:0.076, val_acc:0.954]
Epoch [63/120    avg_loss:0.066, val_acc:0.943]
Epoch [64/120    avg_loss:0.075, val_acc:0.955]
Epoch [65/120    avg_loss:0.066, val_acc:0.956]
Epoch [66/120    avg_loss:0.088, val_acc:0.945]
Epoch [67/120    avg_loss:0.085, val_acc:0.957]
Epoch [68/120    avg_loss:0.064, val_acc:0.960]
Epoch [69/120    avg_loss:0.061, val_acc:0.957]
Epoch [70/120    avg_loss:0.049, val_acc:0.962]
Epoch [71/120    avg_loss:0.061, val_acc:0.961]
Epoch [72/120    avg_loss:0.045, val_acc:0.964]
Epoch [73/120    avg_loss:0.048, val_acc:0.961]
Epoch [74/120    avg_loss:0.060, val_acc:0.964]
Epoch [75/120    avg_loss:0.049, val_acc:0.966]
Epoch [76/120    avg_loss:0.036, val_acc:0.968]
Epoch [77/120    avg_loss:0.044, val_acc:0.967]
Epoch [78/120    avg_loss:0.036, val_acc:0.976]
Epoch [79/120    avg_loss:0.030, val_acc:0.964]
Epoch [80/120    avg_loss:0.031, val_acc:0.962]
Epoch [81/120    avg_loss:0.031, val_acc:0.970]
Epoch [82/120    avg_loss:0.036, val_acc:0.959]
Epoch [83/120    avg_loss:0.036, val_acc:0.963]
Epoch [84/120    avg_loss:0.035, val_acc:0.967]
Epoch [85/120    avg_loss:0.028, val_acc:0.966]
Epoch [86/120    avg_loss:0.033, val_acc:0.967]
Epoch [87/120    avg_loss:0.033, val_acc:0.969]
Epoch [88/120    avg_loss:0.029, val_acc:0.974]
Epoch [89/120    avg_loss:0.028, val_acc:0.967]
Epoch [90/120    avg_loss:0.024, val_acc:0.968]
Epoch [91/120    avg_loss:0.025, val_acc:0.975]
Epoch [92/120    avg_loss:0.023, val_acc:0.972]
Epoch [93/120    avg_loss:0.019, val_acc:0.976]
Epoch [94/120    avg_loss:0.018, val_acc:0.978]
Epoch [95/120    avg_loss:0.020, val_acc:0.978]
Epoch [96/120    avg_loss:0.018, val_acc:0.978]
Epoch [97/120    avg_loss:0.017, val_acc:0.977]
Epoch [98/120    avg_loss:0.018, val_acc:0.977]
Epoch [99/120    avg_loss:0.020, val_acc:0.976]
Epoch [100/120    avg_loss:0.023, val_acc:0.980]
Epoch [101/120    avg_loss:0.017, val_acc:0.980]
Epoch [102/120    avg_loss:0.017, val_acc:0.978]
Epoch [103/120    avg_loss:0.016, val_acc:0.977]
Epoch [104/120    avg_loss:0.017, val_acc:0.977]
Epoch [105/120    avg_loss:0.018, val_acc:0.978]
Epoch [106/120    avg_loss:0.018, val_acc:0.977]
Epoch [107/120    avg_loss:0.016, val_acc:0.977]
Epoch [108/120    avg_loss:0.017, val_acc:0.975]
Epoch [109/120    avg_loss:0.015, val_acc:0.976]
Epoch [110/120    avg_loss:0.016, val_acc:0.976]
Epoch [111/120    avg_loss:0.018, val_acc:0.976]
Epoch [112/120    avg_loss:0.015, val_acc:0.976]
Epoch [113/120    avg_loss:0.021, val_acc:0.975]
Epoch [114/120    avg_loss:0.017, val_acc:0.978]
Epoch [115/120    avg_loss:0.016, val_acc:0.978]
Epoch [116/120    avg_loss:0.020, val_acc:0.977]
Epoch [117/120    avg_loss:0.015, val_acc:0.978]
Epoch [118/120    avg_loss:0.018, val_acc:0.978]
Epoch [119/120    avg_loss:0.015, val_acc:0.978]
Epoch [120/120    avg_loss:0.016, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1245    2    7    0    1    0    1    1    4   23    1    0
     0    0    0]
 [   0    0    2  708    2   11    0    0    0   20    0    2    1    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    8    0    0   10    0    0    0    0
     0    0    0]
 [   0    0    9   34    0    3    0    0    0    0  812   12    2    0
     0    3    0]
 [   0    0    7    0    0    0    2    0    0    0    5 2194    2    0
     0    0    0]
 [   0    0    0    5    3    7    0    0    0    0    8   11  499    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   28    0    0    0    0    0    0    0
    44  275    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.9430894308943

F1 scores:
[       nan 0.975      0.97723705 0.94652406 0.97260274 0.97181511
 0.96965211 1.         0.99883856 0.4        0.95193435 0.98518186
 0.95961538 0.99730458 0.97978495 0.88       0.98809524]

Kappa:
0.9651206189452056
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe497cd9e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.847, val_acc:0.006]
Epoch [2/120    avg_loss:2.806, val_acc:0.046]
Epoch [3/120    avg_loss:2.771, val_acc:0.259]
Epoch [4/120    avg_loss:2.739, val_acc:0.363]
Epoch [5/120    avg_loss:2.687, val_acc:0.410]
Epoch [6/120    avg_loss:2.645, val_acc:0.417]
Epoch [7/120    avg_loss:2.565, val_acc:0.475]
Epoch [8/120    avg_loss:2.503, val_acc:0.489]
Epoch [9/120    avg_loss:2.427, val_acc:0.507]
Epoch [10/120    avg_loss:2.393, val_acc:0.523]
Epoch [11/120    avg_loss:2.331, val_acc:0.537]
Epoch [12/120    avg_loss:2.256, val_acc:0.537]
Epoch [13/120    avg_loss:2.223, val_acc:0.533]
Epoch [14/120    avg_loss:2.159, val_acc:0.549]
Epoch [15/120    avg_loss:2.053, val_acc:0.566]
Epoch [16/120    avg_loss:1.993, val_acc:0.576]
Epoch [17/120    avg_loss:1.891, val_acc:0.590]
Epoch [18/120    avg_loss:1.770, val_acc:0.580]
Epoch [19/120    avg_loss:1.702, val_acc:0.592]
Epoch [20/120    avg_loss:1.654, val_acc:0.579]
Epoch [21/120    avg_loss:1.520, val_acc:0.623]
Epoch [22/120    avg_loss:1.450, val_acc:0.616]
Epoch [23/120    avg_loss:1.352, val_acc:0.635]
Epoch [24/120    avg_loss:1.223, val_acc:0.643]
Epoch [25/120    avg_loss:1.146, val_acc:0.654]
Epoch [26/120    avg_loss:1.102, val_acc:0.672]
Epoch [27/120    avg_loss:1.054, val_acc:0.644]
Epoch [28/120    avg_loss:0.994, val_acc:0.675]
Epoch [29/120    avg_loss:0.971, val_acc:0.694]
Epoch [30/120    avg_loss:0.899, val_acc:0.738]
Epoch [31/120    avg_loss:0.795, val_acc:0.773]
Epoch [32/120    avg_loss:0.713, val_acc:0.780]
Epoch [33/120    avg_loss:0.671, val_acc:0.786]
Epoch [34/120    avg_loss:0.636, val_acc:0.800]
Epoch [35/120    avg_loss:0.612, val_acc:0.809]
Epoch [36/120    avg_loss:0.581, val_acc:0.829]
Epoch [37/120    avg_loss:0.528, val_acc:0.827]
Epoch [38/120    avg_loss:0.473, val_acc:0.871]
Epoch [39/120    avg_loss:0.410, val_acc:0.859]
Epoch [40/120    avg_loss:0.354, val_acc:0.871]
Epoch [41/120    avg_loss:0.349, val_acc:0.887]
Epoch [42/120    avg_loss:0.302, val_acc:0.876]
Epoch [43/120    avg_loss:0.300, val_acc:0.895]
Epoch [44/120    avg_loss:0.262, val_acc:0.905]
Epoch [45/120    avg_loss:0.257, val_acc:0.902]
Epoch [46/120    avg_loss:0.234, val_acc:0.922]
Epoch [47/120    avg_loss:0.233, val_acc:0.908]
Epoch [48/120    avg_loss:0.223, val_acc:0.875]
Epoch [49/120    avg_loss:0.258, val_acc:0.915]
Epoch [50/120    avg_loss:0.218, val_acc:0.892]
Epoch [51/120    avg_loss:0.184, val_acc:0.926]
Epoch [52/120    avg_loss:0.162, val_acc:0.933]
Epoch [53/120    avg_loss:0.127, val_acc:0.941]
Epoch [54/120    avg_loss:0.168, val_acc:0.929]
Epoch [55/120    avg_loss:0.130, val_acc:0.950]
Epoch [56/120    avg_loss:0.144, val_acc:0.939]
Epoch [57/120    avg_loss:0.146, val_acc:0.922]
Epoch [58/120    avg_loss:0.123, val_acc:0.938]
Epoch [59/120    avg_loss:0.110, val_acc:0.951]
Epoch [60/120    avg_loss:0.110, val_acc:0.953]
Epoch [61/120    avg_loss:0.114, val_acc:0.943]
Epoch [62/120    avg_loss:0.085, val_acc:0.962]
Epoch [63/120    avg_loss:0.093, val_acc:0.954]
Epoch [64/120    avg_loss:0.081, val_acc:0.954]
Epoch [65/120    avg_loss:0.079, val_acc:0.958]
Epoch [66/120    avg_loss:0.072, val_acc:0.952]
Epoch [67/120    avg_loss:0.073, val_acc:0.940]
Epoch [68/120    avg_loss:0.076, val_acc:0.959]
Epoch [69/120    avg_loss:0.068, val_acc:0.951]
Epoch [70/120    avg_loss:0.090, val_acc:0.958]
Epoch [71/120    avg_loss:0.068, val_acc:0.954]
Epoch [72/120    avg_loss:0.051, val_acc:0.959]
Epoch [73/120    avg_loss:0.053, val_acc:0.961]
Epoch [74/120    avg_loss:0.050, val_acc:0.967]
Epoch [75/120    avg_loss:0.058, val_acc:0.956]
Epoch [76/120    avg_loss:0.051, val_acc:0.965]
Epoch [77/120    avg_loss:0.052, val_acc:0.968]
Epoch [78/120    avg_loss:0.046, val_acc:0.971]
Epoch [79/120    avg_loss:0.052, val_acc:0.961]
Epoch [80/120    avg_loss:0.055, val_acc:0.968]
Epoch [81/120    avg_loss:0.046, val_acc:0.967]
Epoch [82/120    avg_loss:0.040, val_acc:0.974]
Epoch [83/120    avg_loss:0.036, val_acc:0.974]
Epoch [84/120    avg_loss:0.045, val_acc:0.969]
Epoch [85/120    avg_loss:0.040, val_acc:0.962]
Epoch [86/120    avg_loss:0.040, val_acc:0.964]
Epoch [87/120    avg_loss:0.048, val_acc:0.961]
Epoch [88/120    avg_loss:0.046, val_acc:0.974]
Epoch [89/120    avg_loss:0.039, val_acc:0.970]
Epoch [90/120    avg_loss:0.032, val_acc:0.969]
Epoch [91/120    avg_loss:0.033, val_acc:0.958]
Epoch [92/120    avg_loss:0.037, val_acc:0.977]
Epoch [93/120    avg_loss:0.029, val_acc:0.978]
Epoch [94/120    avg_loss:0.032, val_acc:0.971]
Epoch [95/120    avg_loss:0.032, val_acc:0.978]
Epoch [96/120    avg_loss:0.026, val_acc:0.977]
Epoch [97/120    avg_loss:0.029, val_acc:0.970]
Epoch [98/120    avg_loss:0.040, val_acc:0.972]
Epoch [99/120    avg_loss:0.026, val_acc:0.971]
Epoch [100/120    avg_loss:0.029, val_acc:0.965]
Epoch [101/120    avg_loss:0.027, val_acc:0.968]
Epoch [102/120    avg_loss:0.030, val_acc:0.977]
Epoch [103/120    avg_loss:0.020, val_acc:0.977]
Epoch [104/120    avg_loss:0.023, val_acc:0.978]
Epoch [105/120    avg_loss:0.027, val_acc:0.963]
Epoch [106/120    avg_loss:0.020, val_acc:0.983]
Epoch [107/120    avg_loss:0.020, val_acc:0.980]
Epoch [108/120    avg_loss:0.017, val_acc:0.977]
Epoch [109/120    avg_loss:0.015, val_acc:0.982]
Epoch [110/120    avg_loss:0.018, val_acc:0.981]
Epoch [111/120    avg_loss:0.020, val_acc:0.983]
Epoch [112/120    avg_loss:0.018, val_acc:0.982]
Epoch [113/120    avg_loss:0.023, val_acc:0.980]
Epoch [114/120    avg_loss:0.021, val_acc:0.974]
Epoch [115/120    avg_loss:0.016, val_acc:0.981]
Epoch [116/120    avg_loss:0.016, val_acc:0.975]
Epoch [117/120    avg_loss:0.017, val_acc:0.978]
Epoch [118/120    avg_loss:0.019, val_acc:0.972]
Epoch [119/120    avg_loss:0.024, val_acc:0.978]
Epoch [120/120    avg_loss:0.035, val_acc:0.949]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1126    1    0    0    1    0    0    0   57  100    0    0
     0    0    0]
 [   0    0    0  709    0   22    1    0    0    6    1    2    5    1
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  375   27    4    0    1    0    0    0    0
    28    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    1   32    0    0    0    0    0    0  832    5    0    0
     0    5    0]
 [   0    0    1    0    0    0    8    0    0    0   33 2166    2    0
     0    0    0]
 [   0    0    2   17    1    0    0    0    0    0   18   17  466    0
     0    0   13]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   56    0    0    0    0    0    0    0
    65  226    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.11382113821138

F1 scores:
[       nan 0.93506494 0.93250518 0.9403183  0.99530516 0.90144231
 0.9325763  0.92592593 1.         0.80952381 0.91378364 0.96266667
 0.92460317 0.99730458 0.95989869 0.78200692 0.92222222]

Kappa:
0.9327763588256229
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc4b5895eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.845, val_acc:0.095]
Epoch [2/120    avg_loss:2.803, val_acc:0.231]
Epoch [3/120    avg_loss:2.763, val_acc:0.309]
Epoch [4/120    avg_loss:2.721, val_acc:0.348]
Epoch [5/120    avg_loss:2.663, val_acc:0.347]
Epoch [6/120    avg_loss:2.601, val_acc:0.348]
Epoch [7/120    avg_loss:2.532, val_acc:0.362]
Epoch [8/120    avg_loss:2.476, val_acc:0.383]
Epoch [9/120    avg_loss:2.415, val_acc:0.443]
Epoch [10/120    avg_loss:2.381, val_acc:0.434]
Epoch [11/120    avg_loss:2.313, val_acc:0.523]
Epoch [12/120    avg_loss:2.262, val_acc:0.489]
Epoch [13/120    avg_loss:2.215, val_acc:0.527]
Epoch [14/120    avg_loss:2.162, val_acc:0.556]
Epoch [15/120    avg_loss:2.152, val_acc:0.562]
Epoch [16/120    avg_loss:2.071, val_acc:0.569]
Epoch [17/120    avg_loss:2.059, val_acc:0.565]
Epoch [18/120    avg_loss:1.990, val_acc:0.597]
Epoch [19/120    avg_loss:1.996, val_acc:0.605]
Epoch [20/120    avg_loss:1.894, val_acc:0.631]
Epoch [21/120    avg_loss:1.799, val_acc:0.637]
Epoch [22/120    avg_loss:1.772, val_acc:0.643]
Epoch [23/120    avg_loss:1.698, val_acc:0.676]
Epoch [24/120    avg_loss:1.596, val_acc:0.693]
Epoch [25/120    avg_loss:1.532, val_acc:0.702]
Epoch [26/120    avg_loss:1.457, val_acc:0.700]
Epoch [27/120    avg_loss:1.415, val_acc:0.678]
Epoch [28/120    avg_loss:1.348, val_acc:0.736]
Epoch [29/120    avg_loss:1.213, val_acc:0.766]
Epoch [30/120    avg_loss:1.082, val_acc:0.768]
Epoch [31/120    avg_loss:1.047, val_acc:0.781]
Epoch [32/120    avg_loss:0.952, val_acc:0.810]
Epoch [33/120    avg_loss:0.868, val_acc:0.828]
Epoch [34/120    avg_loss:0.880, val_acc:0.801]
Epoch [35/120    avg_loss:0.786, val_acc:0.812]
Epoch [36/120    avg_loss:0.714, val_acc:0.848]
Epoch [37/120    avg_loss:0.591, val_acc:0.871]
Epoch [38/120    avg_loss:0.543, val_acc:0.861]
Epoch [39/120    avg_loss:0.519, val_acc:0.865]
Epoch [40/120    avg_loss:0.508, val_acc:0.859]
Epoch [41/120    avg_loss:0.479, val_acc:0.873]
Epoch [42/120    avg_loss:0.398, val_acc:0.881]
Epoch [43/120    avg_loss:0.388, val_acc:0.900]
Epoch [44/120    avg_loss:0.355, val_acc:0.892]
Epoch [45/120    avg_loss:0.326, val_acc:0.914]
Epoch [46/120    avg_loss:0.280, val_acc:0.912]
Epoch [47/120    avg_loss:0.237, val_acc:0.932]
Epoch [48/120    avg_loss:0.263, val_acc:0.923]
Epoch [49/120    avg_loss:0.234, val_acc:0.898]
Epoch [50/120    avg_loss:0.218, val_acc:0.920]
Epoch [51/120    avg_loss:0.228, val_acc:0.902]
Epoch [52/120    avg_loss:0.199, val_acc:0.923]
Epoch [53/120    avg_loss:0.178, val_acc:0.922]
Epoch [54/120    avg_loss:0.170, val_acc:0.922]
Epoch [55/120    avg_loss:0.175, val_acc:0.933]
Epoch [56/120    avg_loss:0.173, val_acc:0.935]
Epoch [57/120    avg_loss:0.164, val_acc:0.944]
Epoch [58/120    avg_loss:0.146, val_acc:0.939]
Epoch [59/120    avg_loss:0.161, val_acc:0.948]
Epoch [60/120    avg_loss:0.145, val_acc:0.955]
Epoch [61/120    avg_loss:0.118, val_acc:0.948]
Epoch [62/120    avg_loss:0.108, val_acc:0.939]
Epoch [63/120    avg_loss:0.121, val_acc:0.945]
Epoch [64/120    avg_loss:0.125, val_acc:0.939]
Epoch [65/120    avg_loss:0.114, val_acc:0.947]
Epoch [66/120    avg_loss:0.100, val_acc:0.948]
Epoch [67/120    avg_loss:0.087, val_acc:0.952]
Epoch [68/120    avg_loss:0.098, val_acc:0.947]
Epoch [69/120    avg_loss:0.096, val_acc:0.947]
Epoch [70/120    avg_loss:0.086, val_acc:0.945]
Epoch [71/120    avg_loss:0.083, val_acc:0.936]
Epoch [72/120    avg_loss:0.081, val_acc:0.953]
Epoch [73/120    avg_loss:0.073, val_acc:0.947]
Epoch [74/120    avg_loss:0.073, val_acc:0.957]
Epoch [75/120    avg_loss:0.058, val_acc:0.960]
Epoch [76/120    avg_loss:0.047, val_acc:0.960]
Epoch [77/120    avg_loss:0.049, val_acc:0.963]
Epoch [78/120    avg_loss:0.047, val_acc:0.964]
Epoch [79/120    avg_loss:0.050, val_acc:0.962]
Epoch [80/120    avg_loss:0.045, val_acc:0.964]
Epoch [81/120    avg_loss:0.045, val_acc:0.964]
Epoch [82/120    avg_loss:0.041, val_acc:0.964]
Epoch [83/120    avg_loss:0.046, val_acc:0.966]
Epoch [84/120    avg_loss:0.039, val_acc:0.964]
Epoch [85/120    avg_loss:0.044, val_acc:0.960]
Epoch [86/120    avg_loss:0.043, val_acc:0.961]
Epoch [87/120    avg_loss:0.046, val_acc:0.967]
Epoch [88/120    avg_loss:0.039, val_acc:0.964]
Epoch [89/120    avg_loss:0.049, val_acc:0.963]
Epoch [90/120    avg_loss:0.042, val_acc:0.967]
Epoch [91/120    avg_loss:0.039, val_acc:0.966]
Epoch [92/120    avg_loss:0.044, val_acc:0.964]
Epoch [93/120    avg_loss:0.040, val_acc:0.966]
Epoch [94/120    avg_loss:0.039, val_acc:0.964]
Epoch [95/120    avg_loss:0.038, val_acc:0.967]
Epoch [96/120    avg_loss:0.039, val_acc:0.966]
Epoch [97/120    avg_loss:0.038, val_acc:0.969]
Epoch [98/120    avg_loss:0.039, val_acc:0.966]
Epoch [99/120    avg_loss:0.038, val_acc:0.964]
Epoch [100/120    avg_loss:0.032, val_acc:0.964]
Epoch [101/120    avg_loss:0.040, val_acc:0.966]
Epoch [102/120    avg_loss:0.036, val_acc:0.966]
Epoch [103/120    avg_loss:0.038, val_acc:0.967]
Epoch [104/120    avg_loss:0.034, val_acc:0.966]
Epoch [105/120    avg_loss:0.035, val_acc:0.967]
Epoch [106/120    avg_loss:0.038, val_acc:0.967]
Epoch [107/120    avg_loss:0.038, val_acc:0.963]
Epoch [108/120    avg_loss:0.033, val_acc:0.964]
Epoch [109/120    avg_loss:0.037, val_acc:0.964]
Epoch [110/120    avg_loss:0.040, val_acc:0.966]
Epoch [111/120    avg_loss:0.040, val_acc:0.966]
Epoch [112/120    avg_loss:0.037, val_acc:0.964]
Epoch [113/120    avg_loss:0.041, val_acc:0.966]
Epoch [114/120    avg_loss:0.033, val_acc:0.964]
Epoch [115/120    avg_loss:0.032, val_acc:0.964]
Epoch [116/120    avg_loss:0.031, val_acc:0.964]
Epoch [117/120    avg_loss:0.034, val_acc:0.963]
Epoch [118/120    avg_loss:0.033, val_acc:0.964]
Epoch [119/120    avg_loss:0.041, val_acc:0.964]
Epoch [120/120    avg_loss:0.033, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1239    1    0    0   10    0    0    3    7   24    1    0
     0    0    0]
 [   0    0    2  695    0   22    0    0    0   15    0    0    9    3
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    2    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   16   50    0    6    1    0    0    0  779   11    0    0
     0   12    0]
 [   0    0    6    0    0    0    4    0    0    0   16 2183    0    1
     0    0    0]
 [   0    0    0    9   11    9    0    0    0    0    8    0  492    0
     1    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    1    0    0    0
  1133    0    0]
 [   0    0    1    0    0    0   10    0    0    4    0    0    0    0
    47  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.21680216802169

F1 scores:
[       nan 0.93506494 0.97214594 0.92481703 0.97482838 0.94701987
 0.98132935 1.         0.9953271  0.55172414 0.92134831 0.98599819
 0.94433781 0.98930481 0.97462366 0.88509317 0.97076023]

Kappa:
0.9568669638381836
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f844141beb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.830, val_acc:0.028]
Epoch [2/120    avg_loss:2.792, val_acc:0.078]
Epoch [3/120    avg_loss:2.748, val_acc:0.224]
Epoch [4/120    avg_loss:2.704, val_acc:0.244]
Epoch [5/120    avg_loss:2.645, val_acc:0.268]
Epoch [6/120    avg_loss:2.598, val_acc:0.305]
Epoch [7/120    avg_loss:2.538, val_acc:0.306]
Epoch [8/120    avg_loss:2.495, val_acc:0.350]
Epoch [9/120    avg_loss:2.409, val_acc:0.384]
Epoch [10/120    avg_loss:2.344, val_acc:0.414]
Epoch [11/120    avg_loss:2.301, val_acc:0.444]
Epoch [12/120    avg_loss:2.256, val_acc:0.503]
Epoch [13/120    avg_loss:2.213, val_acc:0.526]
Epoch [14/120    avg_loss:2.166, val_acc:0.559]
Epoch [15/120    avg_loss:2.088, val_acc:0.573]
Epoch [16/120    avg_loss:2.044, val_acc:0.557]
Epoch [17/120    avg_loss:1.952, val_acc:0.538]
Epoch [18/120    avg_loss:1.909, val_acc:0.592]
Epoch [19/120    avg_loss:1.785, val_acc:0.616]
Epoch [20/120    avg_loss:1.712, val_acc:0.624]
Epoch [21/120    avg_loss:1.603, val_acc:0.645]
Epoch [22/120    avg_loss:1.496, val_acc:0.669]
Epoch [23/120    avg_loss:1.466, val_acc:0.673]
Epoch [24/120    avg_loss:1.323, val_acc:0.703]
Epoch [25/120    avg_loss:1.238, val_acc:0.698]
Epoch [26/120    avg_loss:1.165, val_acc:0.700]
Epoch [27/120    avg_loss:1.096, val_acc:0.696]
Epoch [28/120    avg_loss:1.070, val_acc:0.708]
Epoch [29/120    avg_loss:0.958, val_acc:0.693]
Epoch [30/120    avg_loss:0.939, val_acc:0.765]
Epoch [31/120    avg_loss:0.888, val_acc:0.769]
Epoch [32/120    avg_loss:0.800, val_acc:0.783]
Epoch [33/120    avg_loss:0.779, val_acc:0.786]
Epoch [34/120    avg_loss:0.687, val_acc:0.799]
Epoch [35/120    avg_loss:0.580, val_acc:0.825]
Epoch [36/120    avg_loss:0.575, val_acc:0.828]
Epoch [37/120    avg_loss:0.522, val_acc:0.828]
Epoch [38/120    avg_loss:0.567, val_acc:0.848]
Epoch [39/120    avg_loss:0.527, val_acc:0.833]
Epoch [40/120    avg_loss:0.463, val_acc:0.885]
Epoch [41/120    avg_loss:0.424, val_acc:0.868]
Epoch [42/120    avg_loss:0.389, val_acc:0.879]
Epoch [43/120    avg_loss:0.331, val_acc:0.913]
Epoch [44/120    avg_loss:0.348, val_acc:0.897]
Epoch [45/120    avg_loss:0.323, val_acc:0.894]
Epoch [46/120    avg_loss:0.264, val_acc:0.915]
Epoch [47/120    avg_loss:0.226, val_acc:0.932]
Epoch [48/120    avg_loss:0.265, val_acc:0.926]
Epoch [49/120    avg_loss:0.228, val_acc:0.924]
Epoch [50/120    avg_loss:0.199, val_acc:0.942]
Epoch [51/120    avg_loss:0.173, val_acc:0.939]
Epoch [52/120    avg_loss:0.185, val_acc:0.936]
Epoch [53/120    avg_loss:0.178, val_acc:0.944]
Epoch [54/120    avg_loss:0.168, val_acc:0.930]
Epoch [55/120    avg_loss:0.191, val_acc:0.936]
Epoch [56/120    avg_loss:0.187, val_acc:0.927]
Epoch [57/120    avg_loss:0.155, val_acc:0.944]
Epoch [58/120    avg_loss:0.128, val_acc:0.922]
Epoch [59/120    avg_loss:0.147, val_acc:0.948]
Epoch [60/120    avg_loss:0.123, val_acc:0.951]
Epoch [61/120    avg_loss:0.111, val_acc:0.954]
Epoch [62/120    avg_loss:0.163, val_acc:0.952]
Epoch [63/120    avg_loss:0.124, val_acc:0.953]
Epoch [64/120    avg_loss:0.102, val_acc:0.954]
Epoch [65/120    avg_loss:0.103, val_acc:0.962]
Epoch [66/120    avg_loss:0.098, val_acc:0.959]
Epoch [67/120    avg_loss:0.079, val_acc:0.935]
Epoch [68/120    avg_loss:0.098, val_acc:0.962]
Epoch [69/120    avg_loss:0.086, val_acc:0.960]
Epoch [70/120    avg_loss:0.113, val_acc:0.950]
Epoch [71/120    avg_loss:0.305, val_acc:0.895]
Epoch [72/120    avg_loss:0.221, val_acc:0.926]
Epoch [73/120    avg_loss:0.154, val_acc:0.942]
Epoch [74/120    avg_loss:0.179, val_acc:0.925]
Epoch [75/120    avg_loss:0.157, val_acc:0.952]
Epoch [76/120    avg_loss:0.105, val_acc:0.963]
Epoch [77/120    avg_loss:0.109, val_acc:0.959]
Epoch [78/120    avg_loss:0.088, val_acc:0.963]
Epoch [79/120    avg_loss:0.079, val_acc:0.971]
Epoch [80/120    avg_loss:0.074, val_acc:0.970]
Epoch [81/120    avg_loss:0.062, val_acc:0.971]
Epoch [82/120    avg_loss:0.048, val_acc:0.979]
Epoch [83/120    avg_loss:0.051, val_acc:0.970]
Epoch [84/120    avg_loss:0.045, val_acc:0.977]
Epoch [85/120    avg_loss:0.047, val_acc:0.979]
Epoch [86/120    avg_loss:0.044, val_acc:0.968]
Epoch [87/120    avg_loss:0.045, val_acc:0.977]
Epoch [88/120    avg_loss:0.053, val_acc:0.977]
Epoch [89/120    avg_loss:0.043, val_acc:0.973]
Epoch [90/120    avg_loss:0.044, val_acc:0.983]
Epoch [91/120    avg_loss:0.036, val_acc:0.980]
Epoch [92/120    avg_loss:0.039, val_acc:0.983]
Epoch [93/120    avg_loss:0.047, val_acc:0.972]
Epoch [94/120    avg_loss:0.051, val_acc:0.971]
Epoch [95/120    avg_loss:0.033, val_acc:0.978]
Epoch [96/120    avg_loss:0.034, val_acc:0.973]
Epoch [97/120    avg_loss:0.031, val_acc:0.981]
Epoch [98/120    avg_loss:0.026, val_acc:0.982]
Epoch [99/120    avg_loss:0.022, val_acc:0.980]
Epoch [100/120    avg_loss:0.026, val_acc:0.988]
Epoch [101/120    avg_loss:0.026, val_acc:0.978]
Epoch [102/120    avg_loss:0.025, val_acc:0.983]
Epoch [103/120    avg_loss:0.025, val_acc:0.979]
Epoch [104/120    avg_loss:0.028, val_acc:0.984]
Epoch [105/120    avg_loss:0.025, val_acc:0.983]
Epoch [106/120    avg_loss:0.025, val_acc:0.969]
Epoch [107/120    avg_loss:0.029, val_acc:0.977]
Epoch [108/120    avg_loss:0.022, val_acc:0.982]
Epoch [109/120    avg_loss:0.023, val_acc:0.979]
Epoch [110/120    avg_loss:0.023, val_acc:0.980]
Epoch [111/120    avg_loss:0.024, val_acc:0.982]
Epoch [112/120    avg_loss:0.019, val_acc:0.981]
Epoch [113/120    avg_loss:0.021, val_acc:0.979]
Epoch [114/120    avg_loss:0.018, val_acc:0.984]
Epoch [115/120    avg_loss:0.017, val_acc:0.984]
Epoch [116/120    avg_loss:0.016, val_acc:0.987]
Epoch [117/120    avg_loss:0.014, val_acc:0.984]
Epoch [118/120    avg_loss:0.013, val_acc:0.984]
Epoch [119/120    avg_loss:0.014, val_acc:0.984]
Epoch [120/120    avg_loss:0.015, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1245    2   10    0    2    0    0    0    2   21    3    0
     0    0    0]
 [   0    0    2  704    1   32    0    0    0    5    0    0    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    2    0    2    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    5    0    0   12    0    0    1    0
     0    0    0]
 [   0    0   20   17    0    5    0    0    0    0  825    1    4    0
     1    2    0]
 [   0    0   12    0    0    0    7    0    0    0   14 2173    2    2
     0    0    0]
 [   0    0    0   11    5    9    0    0    0    0    7   14  480    0
     0    4    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1134    2    0]
 [   0    0    0    0    0    0    3    0    0    4    0    0    0    0
    42  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.78048780487805

F1 scores:
[       nan 0.96202532 0.97113885 0.95070898 0.9638009  0.93743139
 0.98570354 0.96153846 1.         0.58536585 0.95596756 0.98303551
 0.93476144 0.99462366 0.97758621 0.91271057 0.97674419]

Kappa:
0.9632910344455391
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0873c03f98>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.844, val_acc:0.074]
Epoch [2/120    avg_loss:2.815, val_acc:0.124]
Epoch [3/120    avg_loss:2.779, val_acc:0.162]
Epoch [4/120    avg_loss:2.743, val_acc:0.161]
Epoch [5/120    avg_loss:2.708, val_acc:0.165]
Epoch [6/120    avg_loss:2.664, val_acc:0.218]
Epoch [7/120    avg_loss:2.603, val_acc:0.249]
Epoch [8/120    avg_loss:2.560, val_acc:0.281]
Epoch [9/120    avg_loss:2.496, val_acc:0.311]
Epoch [10/120    avg_loss:2.454, val_acc:0.310]
Epoch [11/120    avg_loss:2.362, val_acc:0.321]
Epoch [12/120    avg_loss:2.297, val_acc:0.331]
Epoch [13/120    avg_loss:2.225, val_acc:0.345]
Epoch [14/120    avg_loss:2.151, val_acc:0.391]
Epoch [15/120    avg_loss:2.076, val_acc:0.373]
Epoch [16/120    avg_loss:1.984, val_acc:0.382]
Epoch [17/120    avg_loss:1.891, val_acc:0.416]
Epoch [18/120    avg_loss:1.793, val_acc:0.397]
Epoch [19/120    avg_loss:1.747, val_acc:0.451]
Epoch [20/120    avg_loss:1.621, val_acc:0.459]
Epoch [21/120    avg_loss:1.614, val_acc:0.508]
Epoch [22/120    avg_loss:1.523, val_acc:0.494]
Epoch [23/120    avg_loss:1.466, val_acc:0.473]
Epoch [24/120    avg_loss:1.380, val_acc:0.529]
Epoch [25/120    avg_loss:1.357, val_acc:0.555]
Epoch [26/120    avg_loss:1.222, val_acc:0.554]
Epoch [27/120    avg_loss:1.152, val_acc:0.614]
Epoch [28/120    avg_loss:1.015, val_acc:0.592]
Epoch [29/120    avg_loss:0.996, val_acc:0.654]
Epoch [30/120    avg_loss:0.914, val_acc:0.632]
Epoch [31/120    avg_loss:0.829, val_acc:0.746]
Epoch [32/120    avg_loss:0.813, val_acc:0.749]
Epoch [33/120    avg_loss:0.725, val_acc:0.806]
Epoch [34/120    avg_loss:0.665, val_acc:0.825]
Epoch [35/120    avg_loss:0.565, val_acc:0.838]
Epoch [36/120    avg_loss:0.562, val_acc:0.819]
Epoch [37/120    avg_loss:0.521, val_acc:0.867]
Epoch [38/120    avg_loss:0.442, val_acc:0.862]
Epoch [39/120    avg_loss:0.421, val_acc:0.875]
Epoch [40/120    avg_loss:0.377, val_acc:0.911]
Epoch [41/120    avg_loss:0.511, val_acc:0.805]
Epoch [42/120    avg_loss:0.609, val_acc:0.849]
Epoch [43/120    avg_loss:0.469, val_acc:0.890]
Epoch [44/120    avg_loss:0.382, val_acc:0.914]
Epoch [45/120    avg_loss:0.311, val_acc:0.906]
Epoch [46/120    avg_loss:0.278, val_acc:0.925]
Epoch [47/120    avg_loss:0.228, val_acc:0.944]
Epoch [48/120    avg_loss:0.186, val_acc:0.944]
Epoch [49/120    avg_loss:0.177, val_acc:0.938]
Epoch [50/120    avg_loss:0.178, val_acc:0.933]
Epoch [51/120    avg_loss:0.163, val_acc:0.942]
Epoch [52/120    avg_loss:0.158, val_acc:0.945]
Epoch [53/120    avg_loss:0.144, val_acc:0.944]
Epoch [54/120    avg_loss:0.127, val_acc:0.940]
Epoch [55/120    avg_loss:0.124, val_acc:0.934]
Epoch [56/120    avg_loss:0.142, val_acc:0.907]
Epoch [57/120    avg_loss:0.131, val_acc:0.951]
Epoch [58/120    avg_loss:0.101, val_acc:0.954]
Epoch [59/120    avg_loss:0.105, val_acc:0.952]
Epoch [60/120    avg_loss:0.089, val_acc:0.943]
Epoch [61/120    avg_loss:0.103, val_acc:0.932]
Epoch [62/120    avg_loss:0.106, val_acc:0.925]
Epoch [63/120    avg_loss:0.106, val_acc:0.948]
Epoch [64/120    avg_loss:0.093, val_acc:0.954]
Epoch [65/120    avg_loss:0.079, val_acc:0.936]
Epoch [66/120    avg_loss:0.091, val_acc:0.953]
Epoch [67/120    avg_loss:0.100, val_acc:0.945]
Epoch [68/120    avg_loss:0.084, val_acc:0.955]
Epoch [69/120    avg_loss:0.095, val_acc:0.955]
Epoch [70/120    avg_loss:0.085, val_acc:0.956]
Epoch [71/120    avg_loss:0.074, val_acc:0.959]
Epoch [72/120    avg_loss:0.062, val_acc:0.950]
Epoch [73/120    avg_loss:0.062, val_acc:0.960]
Epoch [74/120    avg_loss:0.059, val_acc:0.959]
Epoch [75/120    avg_loss:0.052, val_acc:0.961]
Epoch [76/120    avg_loss:0.057, val_acc:0.962]
Epoch [77/120    avg_loss:0.065, val_acc:0.935]
Epoch [78/120    avg_loss:0.077, val_acc:0.958]
Epoch [79/120    avg_loss:0.047, val_acc:0.967]
Epoch [80/120    avg_loss:0.039, val_acc:0.968]
Epoch [81/120    avg_loss:0.040, val_acc:0.960]
Epoch [82/120    avg_loss:0.044, val_acc:0.965]
Epoch [83/120    avg_loss:0.049, val_acc:0.960]
Epoch [84/120    avg_loss:0.048, val_acc:0.965]
Epoch [85/120    avg_loss:0.057, val_acc:0.961]
Epoch [86/120    avg_loss:0.037, val_acc:0.967]
Epoch [87/120    avg_loss:0.034, val_acc:0.956]
Epoch [88/120    avg_loss:0.032, val_acc:0.977]
Epoch [89/120    avg_loss:0.056, val_acc:0.959]
Epoch [90/120    avg_loss:0.044, val_acc:0.970]
Epoch [91/120    avg_loss:0.040, val_acc:0.971]
Epoch [92/120    avg_loss:0.033, val_acc:0.978]
Epoch [93/120    avg_loss:0.031, val_acc:0.969]
Epoch [94/120    avg_loss:0.030, val_acc:0.971]
Epoch [95/120    avg_loss:0.023, val_acc:0.972]
Epoch [96/120    avg_loss:0.039, val_acc:0.962]
Epoch [97/120    avg_loss:0.036, val_acc:0.951]
Epoch [98/120    avg_loss:0.047, val_acc:0.962]
Epoch [99/120    avg_loss:0.041, val_acc:0.971]
Epoch [100/120    avg_loss:0.027, val_acc:0.979]
Epoch [101/120    avg_loss:0.026, val_acc:0.975]
Epoch [102/120    avg_loss:0.023, val_acc:0.971]
Epoch [103/120    avg_loss:0.024, val_acc:0.972]
Epoch [104/120    avg_loss:0.022, val_acc:0.981]
Epoch [105/120    avg_loss:0.020, val_acc:0.979]
Epoch [106/120    avg_loss:0.019, val_acc:0.971]
Epoch [107/120    avg_loss:0.016, val_acc:0.978]
Epoch [108/120    avg_loss:0.019, val_acc:0.977]
Epoch [109/120    avg_loss:0.035, val_acc:0.979]
Epoch [110/120    avg_loss:0.047, val_acc:0.953]
Epoch [111/120    avg_loss:0.034, val_acc:0.977]
Epoch [112/120    avg_loss:0.025, val_acc:0.977]
Epoch [113/120    avg_loss:0.028, val_acc:0.972]
Epoch [114/120    avg_loss:0.048, val_acc:0.973]
Epoch [115/120    avg_loss:0.024, val_acc:0.974]
Epoch [116/120    avg_loss:0.024, val_acc:0.974]
Epoch [117/120    avg_loss:0.021, val_acc:0.979]
Epoch [118/120    avg_loss:0.016, val_acc:0.980]
Epoch [119/120    avg_loss:0.014, val_acc:0.980]
Epoch [120/120    avg_loss:0.015, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1256    1    4    0    0    0    0    0    0   24    0    0
     0    0    0]
 [   0    0    1  717    3    0    0    0    0   10    0    1   12    2
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    1    0    6    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3   34    0    3    0    0    0    0  824    6    0    0
     2    3    0]
 [   0    0    7    0    0    0    1    0    0    0    3 2197    2    0
     0    0    0]
 [   0    0    0   16    5    6    0    0    0    0   11    6  482    0
     0    6    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   18    0    0    7    0    3    0    0
    62  257    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.97560975609755

F1 scores:
[       nan 0.98765432 0.98394046 0.94653465 0.97260274 0.97695853
 0.98574644 0.98039216 1.         0.61016949 0.96205487 0.98808185
 0.93410853 0.99462366 0.97060077 0.83849918 0.97619048]

Kappa:
0.9654880729547763
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff001a9eeb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.810, val_acc:0.039]
Epoch [2/120    avg_loss:2.762, val_acc:0.199]
Epoch [3/120    avg_loss:2.711, val_acc:0.298]
Epoch [4/120    avg_loss:2.633, val_acc:0.424]
Epoch [5/120    avg_loss:2.610, val_acc:0.438]
Epoch [6/120    avg_loss:2.532, val_acc:0.442]
Epoch [7/120    avg_loss:2.473, val_acc:0.445]
Epoch [8/120    avg_loss:2.426, val_acc:0.453]
Epoch [9/120    avg_loss:2.322, val_acc:0.473]
Epoch [10/120    avg_loss:2.354, val_acc:0.488]
Epoch [11/120    avg_loss:2.253, val_acc:0.492]
Epoch [12/120    avg_loss:2.215, val_acc:0.513]
Epoch [13/120    avg_loss:2.172, val_acc:0.520]
Epoch [14/120    avg_loss:2.115, val_acc:0.532]
Epoch [15/120    avg_loss:2.041, val_acc:0.556]
Epoch [16/120    avg_loss:2.030, val_acc:0.577]
Epoch [17/120    avg_loss:1.960, val_acc:0.608]
Epoch [18/120    avg_loss:1.913, val_acc:0.641]
Epoch [19/120    avg_loss:1.810, val_acc:0.641]
Epoch [20/120    avg_loss:1.744, val_acc:0.667]
Epoch [21/120    avg_loss:1.683, val_acc:0.673]
Epoch [22/120    avg_loss:1.591, val_acc:0.672]
Epoch [23/120    avg_loss:1.512, val_acc:0.699]
Epoch [24/120    avg_loss:1.382, val_acc:0.706]
Epoch [25/120    avg_loss:1.368, val_acc:0.732]
Epoch [26/120    avg_loss:1.243, val_acc:0.728]
Epoch [27/120    avg_loss:1.202, val_acc:0.737]
Epoch [28/120    avg_loss:1.107, val_acc:0.742]
Epoch [29/120    avg_loss:1.034, val_acc:0.739]
Epoch [30/120    avg_loss:1.024, val_acc:0.737]
Epoch [31/120    avg_loss:0.911, val_acc:0.761]
Epoch [32/120    avg_loss:0.888, val_acc:0.773]
Epoch [33/120    avg_loss:0.784, val_acc:0.787]
Epoch [34/120    avg_loss:0.782, val_acc:0.777]
Epoch [35/120    avg_loss:0.650, val_acc:0.814]
Epoch [36/120    avg_loss:0.652, val_acc:0.812]
Epoch [37/120    avg_loss:0.632, val_acc:0.827]
Epoch [38/120    avg_loss:0.580, val_acc:0.859]
Epoch [39/120    avg_loss:0.544, val_acc:0.860]
Epoch [40/120    avg_loss:0.474, val_acc:0.883]
Epoch [41/120    avg_loss:0.455, val_acc:0.854]
Epoch [42/120    avg_loss:0.416, val_acc:0.875]
Epoch [43/120    avg_loss:0.380, val_acc:0.896]
Epoch [44/120    avg_loss:0.319, val_acc:0.891]
Epoch [45/120    avg_loss:0.321, val_acc:0.898]
Epoch [46/120    avg_loss:0.328, val_acc:0.905]
Epoch [47/120    avg_loss:0.323, val_acc:0.886]
Epoch [48/120    avg_loss:0.278, val_acc:0.905]
Epoch [49/120    avg_loss:0.258, val_acc:0.912]
Epoch [50/120    avg_loss:0.229, val_acc:0.912]
Epoch [51/120    avg_loss:0.230, val_acc:0.922]
Epoch [52/120    avg_loss:0.212, val_acc:0.906]
Epoch [53/120    avg_loss:0.361, val_acc:0.903]
Epoch [54/120    avg_loss:1.897, val_acc:0.365]
Epoch [55/120    avg_loss:1.945, val_acc:0.469]
Epoch [56/120    avg_loss:1.564, val_acc:0.632]
Epoch [57/120    avg_loss:1.236, val_acc:0.676]
Epoch [58/120    avg_loss:1.155, val_acc:0.777]
Epoch [59/120    avg_loss:0.566, val_acc:0.866]
Epoch [60/120    avg_loss:0.404, val_acc:0.819]
Epoch [61/120    avg_loss:0.373, val_acc:0.881]
Epoch [62/120    avg_loss:0.242, val_acc:0.893]
Epoch [63/120    avg_loss:0.222, val_acc:0.901]
Epoch [64/120    avg_loss:0.236, val_acc:0.904]
Epoch [65/120    avg_loss:0.198, val_acc:0.924]
Epoch [66/120    avg_loss:0.158, val_acc:0.932]
Epoch [67/120    avg_loss:0.160, val_acc:0.929]
Epoch [68/120    avg_loss:0.155, val_acc:0.931]
Epoch [69/120    avg_loss:0.135, val_acc:0.935]
Epoch [70/120    avg_loss:0.154, val_acc:0.938]
Epoch [71/120    avg_loss:0.131, val_acc:0.939]
Epoch [72/120    avg_loss:0.156, val_acc:0.938]
Epoch [73/120    avg_loss:0.134, val_acc:0.936]
Epoch [74/120    avg_loss:0.126, val_acc:0.934]
Epoch [75/120    avg_loss:0.130, val_acc:0.936]
Epoch [76/120    avg_loss:0.129, val_acc:0.936]
Epoch [77/120    avg_loss:0.130, val_acc:0.938]
Epoch [78/120    avg_loss:0.117, val_acc:0.941]
Epoch [79/120    avg_loss:0.126, val_acc:0.936]
Epoch [80/120    avg_loss:0.113, val_acc:0.942]
Epoch [81/120    avg_loss:0.129, val_acc:0.942]
Epoch [82/120    avg_loss:0.123, val_acc:0.941]
Epoch [83/120    avg_loss:0.110, val_acc:0.936]
Epoch [84/120    avg_loss:0.117, val_acc:0.939]
Epoch [85/120    avg_loss:0.123, val_acc:0.941]
Epoch [86/120    avg_loss:0.116, val_acc:0.942]
Epoch [87/120    avg_loss:0.102, val_acc:0.942]
Epoch [88/120    avg_loss:0.109, val_acc:0.942]
Epoch [89/120    avg_loss:0.112, val_acc:0.936]
Epoch [90/120    avg_loss:0.112, val_acc:0.942]
Epoch [91/120    avg_loss:0.115, val_acc:0.943]
Epoch [92/120    avg_loss:0.107, val_acc:0.944]
Epoch [93/120    avg_loss:0.104, val_acc:0.944]
Epoch [94/120    avg_loss:0.108, val_acc:0.943]
Epoch [95/120    avg_loss:0.107, val_acc:0.942]
Epoch [96/120    avg_loss:0.103, val_acc:0.943]
Epoch [97/120    avg_loss:0.109, val_acc:0.944]
Epoch [98/120    avg_loss:0.093, val_acc:0.944]
Epoch [99/120    avg_loss:0.099, val_acc:0.945]
Epoch [100/120    avg_loss:0.103, val_acc:0.944]
Epoch [101/120    avg_loss:0.106, val_acc:0.944]
Epoch [102/120    avg_loss:0.090, val_acc:0.944]
Epoch [103/120    avg_loss:0.098, val_acc:0.945]
Epoch [104/120    avg_loss:0.103, val_acc:0.944]
Epoch [105/120    avg_loss:0.097, val_acc:0.946]
Epoch [106/120    avg_loss:0.102, val_acc:0.945]
Epoch [107/120    avg_loss:0.097, val_acc:0.946]
Epoch [108/120    avg_loss:0.101, val_acc:0.943]
Epoch [109/120    avg_loss:0.088, val_acc:0.945]
Epoch [110/120    avg_loss:0.094, val_acc:0.945]
Epoch [111/120    avg_loss:0.094, val_acc:0.946]
Epoch [112/120    avg_loss:0.086, val_acc:0.946]
Epoch [113/120    avg_loss:0.088, val_acc:0.946]
Epoch [114/120    avg_loss:0.103, val_acc:0.949]
Epoch [115/120    avg_loss:0.092, val_acc:0.948]
Epoch [116/120    avg_loss:0.087, val_acc:0.943]
Epoch [117/120    avg_loss:0.082, val_acc:0.948]
Epoch [118/120    avg_loss:0.079, val_acc:0.945]
Epoch [119/120    avg_loss:0.077, val_acc:0.951]
Epoch [120/120    avg_loss:0.088, val_acc:0.950]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1206    7    0    0    0    0    0    3    8   59    1    0
     0    1    0]
 [   0    0    3  690    2   23    0    0    0   18    0    6    2    2
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    5    0    4    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  645    0    0    0    0    3    0    0
     9    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   58   14    0    8    0    0    0    0  777    9    0    0
     2    7    0]
 [   0    0   68    0    0    0   10    0    2    0   15 2101    5    4
     5    0    0]
 [   0    0    0   19    0    5    0    0    0    0    6    5  497    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    0    0    1    2    0    0
  1128    0    0]
 [   0    0    1    0    0    0    3    0    0    2    0    0    0    0
    73  268    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.57994579945799

F1 scores:
[       nan 0.975      0.92025944 0.93306288 0.9953271  0.93791574
 0.98098859 0.90909091 0.99767981 0.52459016 0.92280285 0.95608646
 0.9548511  0.98404255 0.95552732 0.86035313 0.98203593]

Kappa:
0.9381947082289087
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f528ea58e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.815, val_acc:0.037]
Epoch [2/120    avg_loss:2.784, val_acc:0.108]
Epoch [3/120    avg_loss:2.748, val_acc:0.163]
Epoch [4/120    avg_loss:2.702, val_acc:0.230]
Epoch [5/120    avg_loss:2.680, val_acc:0.291]
Epoch [6/120    avg_loss:2.623, val_acc:0.320]
Epoch [7/120    avg_loss:2.576, val_acc:0.324]
Epoch [8/120    avg_loss:2.553, val_acc:0.342]
Epoch [9/120    avg_loss:2.500, val_acc:0.368]
Epoch [10/120    avg_loss:2.467, val_acc:0.392]
Epoch [11/120    avg_loss:2.424, val_acc:0.443]
Epoch [12/120    avg_loss:2.388, val_acc:0.458]
Epoch [13/120    avg_loss:2.363, val_acc:0.484]
Epoch [14/120    avg_loss:2.313, val_acc:0.512]
Epoch [15/120    avg_loss:2.234, val_acc:0.546]
Epoch [16/120    avg_loss:2.203, val_acc:0.549]
Epoch [17/120    avg_loss:2.189, val_acc:0.562]
Epoch [18/120    avg_loss:2.175, val_acc:0.599]
Epoch [19/120    avg_loss:2.064, val_acc:0.610]
Epoch [20/120    avg_loss:2.019, val_acc:0.629]
Epoch [21/120    avg_loss:1.955, val_acc:0.636]
Epoch [22/120    avg_loss:1.906, val_acc:0.661]
Epoch [23/120    avg_loss:1.838, val_acc:0.675]
Epoch [24/120    avg_loss:1.779, val_acc:0.686]
Epoch [25/120    avg_loss:1.697, val_acc:0.704]
Epoch [26/120    avg_loss:1.649, val_acc:0.710]
Epoch [27/120    avg_loss:1.504, val_acc:0.704]
Epoch [28/120    avg_loss:1.453, val_acc:0.713]
Epoch [29/120    avg_loss:1.346, val_acc:0.732]
Epoch [30/120    avg_loss:1.252, val_acc:0.757]
Epoch [31/120    avg_loss:1.168, val_acc:0.731]
Epoch [32/120    avg_loss:1.070, val_acc:0.785]
Epoch [33/120    avg_loss:0.966, val_acc:0.792]
Epoch [34/120    avg_loss:0.941, val_acc:0.748]
Epoch [35/120    avg_loss:0.900, val_acc:0.811]
Epoch [36/120    avg_loss:0.823, val_acc:0.824]
Epoch [37/120    avg_loss:0.702, val_acc:0.853]
Epoch [38/120    avg_loss:0.643, val_acc:0.788]
Epoch [39/120    avg_loss:0.629, val_acc:0.860]
Epoch [40/120    avg_loss:0.531, val_acc:0.864]
Epoch [41/120    avg_loss:0.543, val_acc:0.867]
Epoch [42/120    avg_loss:0.525, val_acc:0.878]
Epoch [43/120    avg_loss:0.453, val_acc:0.883]
Epoch [44/120    avg_loss:0.450, val_acc:0.896]
Epoch [45/120    avg_loss:0.374, val_acc:0.902]
Epoch [46/120    avg_loss:0.349, val_acc:0.907]
Epoch [47/120    avg_loss:0.303, val_acc:0.924]
Epoch [48/120    avg_loss:0.277, val_acc:0.921]
Epoch [49/120    avg_loss:0.289, val_acc:0.935]
Epoch [50/120    avg_loss:0.267, val_acc:0.931]
Epoch [51/120    avg_loss:0.269, val_acc:0.923]
Epoch [52/120    avg_loss:0.225, val_acc:0.938]
Epoch [53/120    avg_loss:0.186, val_acc:0.944]
Epoch [54/120    avg_loss:0.171, val_acc:0.931]
Epoch [55/120    avg_loss:0.164, val_acc:0.946]
Epoch [56/120    avg_loss:0.156, val_acc:0.955]
Epoch [57/120    avg_loss:0.132, val_acc:0.958]
Epoch [58/120    avg_loss:0.124, val_acc:0.954]
Epoch [59/120    avg_loss:0.130, val_acc:0.958]
Epoch [60/120    avg_loss:0.154, val_acc:0.953]
Epoch [61/120    avg_loss:0.117, val_acc:0.953]
Epoch [62/120    avg_loss:0.112, val_acc:0.946]
Epoch [63/120    avg_loss:0.119, val_acc:0.954]
Epoch [64/120    avg_loss:0.101, val_acc:0.962]
Epoch [65/120    avg_loss:0.110, val_acc:0.953]
Epoch [66/120    avg_loss:0.093, val_acc:0.964]
Epoch [67/120    avg_loss:0.096, val_acc:0.968]
Epoch [68/120    avg_loss:0.103, val_acc:0.968]
Epoch [69/120    avg_loss:0.091, val_acc:0.968]
Epoch [70/120    avg_loss:0.074, val_acc:0.961]
Epoch [71/120    avg_loss:0.081, val_acc:0.965]
Epoch [72/120    avg_loss:0.071, val_acc:0.972]
Epoch [73/120    avg_loss:0.066, val_acc:0.975]
Epoch [74/120    avg_loss:0.058, val_acc:0.972]
Epoch [75/120    avg_loss:0.060, val_acc:0.979]
Epoch [76/120    avg_loss:0.068, val_acc:0.974]
Epoch [77/120    avg_loss:0.062, val_acc:0.968]
Epoch [78/120    avg_loss:0.072, val_acc:0.962]
Epoch [79/120    avg_loss:0.078, val_acc:0.964]
Epoch [80/120    avg_loss:0.061, val_acc:0.979]
Epoch [81/120    avg_loss:0.061, val_acc:0.977]
Epoch [82/120    avg_loss:0.075, val_acc:0.956]
Epoch [83/120    avg_loss:0.075, val_acc:0.967]
Epoch [84/120    avg_loss:0.053, val_acc:0.970]
Epoch [85/120    avg_loss:0.044, val_acc:0.974]
Epoch [86/120    avg_loss:0.049, val_acc:0.973]
Epoch [87/120    avg_loss:0.051, val_acc:0.964]
Epoch [88/120    avg_loss:0.041, val_acc:0.974]
Epoch [89/120    avg_loss:0.038, val_acc:0.973]
Epoch [90/120    avg_loss:0.039, val_acc:0.977]
Epoch [91/120    avg_loss:0.032, val_acc:0.981]
Epoch [92/120    avg_loss:0.033, val_acc:0.981]
Epoch [93/120    avg_loss:0.032, val_acc:0.985]
Epoch [94/120    avg_loss:0.026, val_acc:0.987]
Epoch [95/120    avg_loss:0.034, val_acc:0.977]
Epoch [96/120    avg_loss:0.030, val_acc:0.984]
Epoch [97/120    avg_loss:0.028, val_acc:0.983]
Epoch [98/120    avg_loss:0.033, val_acc:0.982]
Epoch [99/120    avg_loss:0.033, val_acc:0.985]
Epoch [100/120    avg_loss:0.034, val_acc:0.980]
Epoch [101/120    avg_loss:0.042, val_acc:0.982]
Epoch [102/120    avg_loss:0.035, val_acc:0.975]
Epoch [103/120    avg_loss:0.029, val_acc:0.985]
Epoch [104/120    avg_loss:0.027, val_acc:0.983]
Epoch [105/120    avg_loss:0.025, val_acc:0.984]
Epoch [106/120    avg_loss:0.028, val_acc:0.973]
Epoch [107/120    avg_loss:0.026, val_acc:0.989]
Epoch [108/120    avg_loss:0.020, val_acc:0.985]
Epoch [109/120    avg_loss:0.027, val_acc:0.979]
Epoch [110/120    avg_loss:0.030, val_acc:0.970]
Epoch [111/120    avg_loss:0.046, val_acc:0.975]
Epoch [112/120    avg_loss:0.043, val_acc:0.971]
Epoch [113/120    avg_loss:0.044, val_acc:0.981]
Epoch [114/120    avg_loss:0.030, val_acc:0.974]
Epoch [115/120    avg_loss:0.038, val_acc:0.975]
Epoch [116/120    avg_loss:0.039, val_acc:0.979]
Epoch [117/120    avg_loss:0.045, val_acc:0.972]
Epoch [118/120    avg_loss:0.033, val_acc:0.983]
Epoch [119/120    avg_loss:0.025, val_acc:0.987]
Epoch [120/120    avg_loss:0.019, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    2    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1271    2    0    0    0    0    0    0    4    8    0    0
     0    0    0]
 [   0    0    1  721    3    3    0    0    0   13    0    0    6    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0  655    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0   23   19    0    4    0    0    0    0  809   10    8    0
     0    2    0]
 [   0    0   26    0    0    0    2    0    0    0   12 2168    1    1
     0    0    0]
 [   0    0    0   11    2    7    0    0    0    0   11    7  492    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    1    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0    3    0    0    2    0    8    0    0
    29  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.31165311653116

F1 scores:
[       nan 0.94871795 0.97431966 0.96133333 0.98604651 0.97853107
 0.99468489 1.         1.         0.6        0.94399067 0.98277425
 0.94162679 0.99730458 0.98567086 0.93272171 0.97674419]

Kappa:
0.9693431404017034
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3e47043eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.814, val_acc:0.086]
Epoch [2/120    avg_loss:2.760, val_acc:0.150]
Epoch [3/120    avg_loss:2.704, val_acc:0.179]
Epoch [4/120    avg_loss:2.659, val_acc:0.203]
Epoch [5/120    avg_loss:2.600, val_acc:0.231]
Epoch [6/120    avg_loss:2.525, val_acc:0.294]
Epoch [7/120    avg_loss:2.467, val_acc:0.307]
Epoch [8/120    avg_loss:2.401, val_acc:0.328]
Epoch [9/120    avg_loss:2.344, val_acc:0.338]
Epoch [10/120    avg_loss:2.302, val_acc:0.356]
Epoch [11/120    avg_loss:2.270, val_acc:0.411]
Epoch [12/120    avg_loss:2.221, val_acc:0.538]
Epoch [13/120    avg_loss:2.174, val_acc:0.570]
Epoch [14/120    avg_loss:2.154, val_acc:0.570]
Epoch [15/120    avg_loss:2.034, val_acc:0.599]
Epoch [16/120    avg_loss:1.990, val_acc:0.607]
Epoch [17/120    avg_loss:1.938, val_acc:0.600]
Epoch [18/120    avg_loss:1.865, val_acc:0.612]
Epoch [19/120    avg_loss:1.794, val_acc:0.635]
Epoch [20/120    avg_loss:1.653, val_acc:0.645]
Epoch [21/120    avg_loss:1.506, val_acc:0.628]
Epoch [22/120    avg_loss:1.480, val_acc:0.667]
Epoch [23/120    avg_loss:1.383, val_acc:0.692]
Epoch [24/120    avg_loss:1.284, val_acc:0.718]
Epoch [25/120    avg_loss:1.176, val_acc:0.702]
Epoch [26/120    avg_loss:1.068, val_acc:0.738]
Epoch [27/120    avg_loss:0.946, val_acc:0.752]
Epoch [28/120    avg_loss:0.865, val_acc:0.766]
Epoch [29/120    avg_loss:0.838, val_acc:0.789]
Epoch [30/120    avg_loss:0.820, val_acc:0.777]
Epoch [31/120    avg_loss:0.757, val_acc:0.809]
Epoch [32/120    avg_loss:0.668, val_acc:0.840]
Epoch [33/120    avg_loss:0.616, val_acc:0.811]
Epoch [34/120    avg_loss:0.559, val_acc:0.868]
Epoch [35/120    avg_loss:0.517, val_acc:0.874]
Epoch [36/120    avg_loss:0.455, val_acc:0.883]
Epoch [37/120    avg_loss:0.425, val_acc:0.869]
Epoch [38/120    avg_loss:0.388, val_acc:0.896]
Epoch [39/120    avg_loss:0.351, val_acc:0.900]
Epoch [40/120    avg_loss:0.317, val_acc:0.935]
Epoch [41/120    avg_loss:0.291, val_acc:0.911]
Epoch [42/120    avg_loss:0.268, val_acc:0.925]
Epoch [43/120    avg_loss:0.253, val_acc:0.933]
Epoch [44/120    avg_loss:0.220, val_acc:0.944]
Epoch [45/120    avg_loss:0.204, val_acc:0.932]
Epoch [46/120    avg_loss:0.212, val_acc:0.934]
Epoch [47/120    avg_loss:0.215, val_acc:0.888]
Epoch [48/120    avg_loss:0.247, val_acc:0.941]
Epoch [49/120    avg_loss:0.171, val_acc:0.958]
Epoch [50/120    avg_loss:0.142, val_acc:0.967]
Epoch [51/120    avg_loss:0.180, val_acc:0.930]
Epoch [52/120    avg_loss:0.148, val_acc:0.963]
Epoch [53/120    avg_loss:0.144, val_acc:0.953]
Epoch [54/120    avg_loss:0.127, val_acc:0.955]
Epoch [55/120    avg_loss:0.139, val_acc:0.946]
Epoch [56/120    avg_loss:0.156, val_acc:0.944]
Epoch [57/120    avg_loss:0.137, val_acc:0.968]
Epoch [58/120    avg_loss:0.103, val_acc:0.962]
Epoch [59/120    avg_loss:0.090, val_acc:0.971]
Epoch [60/120    avg_loss:0.080, val_acc:0.967]
Epoch [61/120    avg_loss:0.090, val_acc:0.972]
Epoch [62/120    avg_loss:0.075, val_acc:0.972]
Epoch [63/120    avg_loss:0.071, val_acc:0.973]
Epoch [64/120    avg_loss:0.086, val_acc:0.960]
Epoch [65/120    avg_loss:0.146, val_acc:0.958]
Epoch [66/120    avg_loss:0.118, val_acc:0.964]
Epoch [67/120    avg_loss:0.102, val_acc:0.963]
Epoch [68/120    avg_loss:0.086, val_acc:0.969]
Epoch [69/120    avg_loss:0.073, val_acc:0.950]
Epoch [70/120    avg_loss:0.072, val_acc:0.971]
Epoch [71/120    avg_loss:0.049, val_acc:0.970]
Epoch [72/120    avg_loss:0.050, val_acc:0.969]
Epoch [73/120    avg_loss:0.065, val_acc:0.971]
Epoch [74/120    avg_loss:0.054, val_acc:0.980]
Epoch [75/120    avg_loss:0.070, val_acc:0.960]
Epoch [76/120    avg_loss:0.061, val_acc:0.968]
Epoch [77/120    avg_loss:0.068, val_acc:0.956]
Epoch [78/120    avg_loss:0.073, val_acc:0.967]
Epoch [79/120    avg_loss:0.067, val_acc:0.974]
Epoch [80/120    avg_loss:0.053, val_acc:0.972]
Epoch [81/120    avg_loss:0.038, val_acc:0.978]
Epoch [82/120    avg_loss:0.057, val_acc:0.961]
Epoch [83/120    avg_loss:0.083, val_acc:0.975]
Epoch [84/120    avg_loss:0.096, val_acc:0.968]
Epoch [85/120    avg_loss:0.050, val_acc:0.975]
Epoch [86/120    avg_loss:0.040, val_acc:0.979]
Epoch [87/120    avg_loss:0.044, val_acc:0.978]
Epoch [88/120    avg_loss:0.042, val_acc:0.984]
Epoch [89/120    avg_loss:0.030, val_acc:0.981]
Epoch [90/120    avg_loss:0.026, val_acc:0.983]
Epoch [91/120    avg_loss:0.029, val_acc:0.983]
Epoch [92/120    avg_loss:0.027, val_acc:0.983]
Epoch [93/120    avg_loss:0.024, val_acc:0.984]
Epoch [94/120    avg_loss:0.026, val_acc:0.984]
Epoch [95/120    avg_loss:0.024, val_acc:0.985]
Epoch [96/120    avg_loss:0.024, val_acc:0.987]
Epoch [97/120    avg_loss:0.024, val_acc:0.987]
Epoch [98/120    avg_loss:0.024, val_acc:0.984]
Epoch [99/120    avg_loss:0.020, val_acc:0.985]
Epoch [100/120    avg_loss:0.023, val_acc:0.985]
Epoch [101/120    avg_loss:0.023, val_acc:0.985]
Epoch [102/120    avg_loss:0.023, val_acc:0.985]
Epoch [103/120    avg_loss:0.026, val_acc:0.987]
Epoch [104/120    avg_loss:0.023, val_acc:0.987]
Epoch [105/120    avg_loss:0.021, val_acc:0.985]
Epoch [106/120    avg_loss:0.021, val_acc:0.985]
Epoch [107/120    avg_loss:0.023, val_acc:0.987]
Epoch [108/120    avg_loss:0.024, val_acc:0.987]
Epoch [109/120    avg_loss:0.022, val_acc:0.987]
Epoch [110/120    avg_loss:0.021, val_acc:0.987]
Epoch [111/120    avg_loss:0.026, val_acc:0.987]
Epoch [112/120    avg_loss:0.023, val_acc:0.987]
Epoch [113/120    avg_loss:0.019, val_acc:0.987]
Epoch [114/120    avg_loss:0.022, val_acc:0.987]
Epoch [115/120    avg_loss:0.018, val_acc:0.987]
Epoch [116/120    avg_loss:0.025, val_acc:0.988]
Epoch [117/120    avg_loss:0.018, val_acc:0.987]
Epoch [118/120    avg_loss:0.021, val_acc:0.988]
Epoch [119/120    avg_loss:0.022, val_acc:0.988]
Epoch [120/120    avg_loss:0.020, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1256    0    1    0    0    0    0    0    3   25    0    0
     0    0    0]
 [   0    0    0  714    3   16    0    0    0    5    0    0    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    1    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2   37    0    1    0    0    0    0  820    9    5    0
     0    1    0]
 [   0    0   10    0    0    0    3    0    0    0   12 2183    2    0
     0    0    0]
 [   0    0    0    2   18    2    0    0    0    0    9    0  501    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    46  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.3550135501355

F1 scores:
[       nan 0.98765432 0.98355521 0.952      0.95089286 0.96839729
 0.99093656 1.         0.997669   0.85714286 0.95404305 0.98599819
 0.95156695 1.         0.97678418 0.91419657 0.98823529]

Kappa:
0.9698365339541611
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:12:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe072173ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.827, val_acc:0.160]
Epoch [2/120    avg_loss:2.777, val_acc:0.188]
Epoch [3/120    avg_loss:2.725, val_acc:0.230]
Epoch [4/120    avg_loss:2.661, val_acc:0.269]
Epoch [5/120    avg_loss:2.620, val_acc:0.386]
Epoch [6/120    avg_loss:2.551, val_acc:0.435]
Epoch [7/120    avg_loss:2.528, val_acc:0.471]
Epoch [8/120    avg_loss:2.500, val_acc:0.483]
Epoch [9/120    avg_loss:2.423, val_acc:0.485]
Epoch [10/120    avg_loss:2.385, val_acc:0.521]
Epoch [11/120    avg_loss:2.346, val_acc:0.526]
Epoch [12/120    avg_loss:2.300, val_acc:0.540]
Epoch [13/120    avg_loss:2.234, val_acc:0.554]
Epoch [14/120    avg_loss:2.211, val_acc:0.552]
Epoch [15/120    avg_loss:2.170, val_acc:0.568]
Epoch [16/120    avg_loss:2.095, val_acc:0.603]
Epoch [17/120    avg_loss:2.020, val_acc:0.605]
Epoch [18/120    avg_loss:1.968, val_acc:0.627]
Epoch [19/120    avg_loss:1.878, val_acc:0.642]
Epoch [20/120    avg_loss:1.840, val_acc:0.627]
Epoch [21/120    avg_loss:1.748, val_acc:0.667]
Epoch [22/120    avg_loss:1.645, val_acc:0.664]
Epoch [23/120    avg_loss:1.468, val_acc:0.688]
Epoch [24/120    avg_loss:1.414, val_acc:0.699]
Epoch [25/120    avg_loss:1.314, val_acc:0.690]
Epoch [26/120    avg_loss:1.248, val_acc:0.725]
Epoch [27/120    avg_loss:1.170, val_acc:0.762]
Epoch [28/120    avg_loss:1.119, val_acc:0.751]
Epoch [29/120    avg_loss:1.097, val_acc:0.763]
Epoch [30/120    avg_loss:0.952, val_acc:0.783]
Epoch [31/120    avg_loss:0.909, val_acc:0.790]
Epoch [32/120    avg_loss:0.805, val_acc:0.836]
Epoch [33/120    avg_loss:0.766, val_acc:0.816]
Epoch [34/120    avg_loss:0.736, val_acc:0.831]
Epoch [35/120    avg_loss:0.663, val_acc:0.862]
Epoch [36/120    avg_loss:0.635, val_acc:0.834]
Epoch [37/120    avg_loss:0.534, val_acc:0.895]
Epoch [38/120    avg_loss:0.494, val_acc:0.868]
Epoch [39/120    avg_loss:0.522, val_acc:0.855]
Epoch [40/120    avg_loss:0.468, val_acc:0.874]
Epoch [41/120    avg_loss:0.388, val_acc:0.892]
Epoch [42/120    avg_loss:0.343, val_acc:0.900]
Epoch [43/120    avg_loss:0.344, val_acc:0.901]
Epoch [44/120    avg_loss:0.365, val_acc:0.906]
Epoch [45/120    avg_loss:0.314, val_acc:0.936]
Epoch [46/120    avg_loss:0.332, val_acc:0.898]
Epoch [47/120    avg_loss:0.309, val_acc:0.920]
Epoch [48/120    avg_loss:0.271, val_acc:0.933]
Epoch [49/120    avg_loss:0.204, val_acc:0.927]
Epoch [50/120    avg_loss:0.207, val_acc:0.936]
Epoch [51/120    avg_loss:0.207, val_acc:0.935]
Epoch [52/120    avg_loss:0.161, val_acc:0.954]
Epoch [53/120    avg_loss:0.138, val_acc:0.950]
Epoch [54/120    avg_loss:0.127, val_acc:0.949]
Epoch [55/120    avg_loss:0.137, val_acc:0.955]
Epoch [56/120    avg_loss:0.126, val_acc:0.960]
Epoch [57/120    avg_loss:0.123, val_acc:0.955]
Epoch [58/120    avg_loss:0.109, val_acc:0.967]
Epoch [59/120    avg_loss:0.092, val_acc:0.969]
Epoch [60/120    avg_loss:0.112, val_acc:0.959]
Epoch [61/120    avg_loss:0.092, val_acc:0.973]
Epoch [62/120    avg_loss:0.092, val_acc:0.956]
Epoch [63/120    avg_loss:0.104, val_acc:0.974]
Epoch [64/120    avg_loss:0.107, val_acc:0.964]
Epoch [65/120    avg_loss:0.088, val_acc:0.975]
Epoch [66/120    avg_loss:0.075, val_acc:0.962]
Epoch [67/120    avg_loss:0.080, val_acc:0.971]
Epoch [68/120    avg_loss:0.071, val_acc:0.977]
Epoch [69/120    avg_loss:0.073, val_acc:0.979]
Epoch [70/120    avg_loss:0.061, val_acc:0.964]
Epoch [71/120    avg_loss:0.065, val_acc:0.978]
Epoch [72/120    avg_loss:0.059, val_acc:0.969]
Epoch [73/120    avg_loss:0.071, val_acc:0.972]
Epoch [74/120    avg_loss:0.051, val_acc:0.977]
Epoch [75/120    avg_loss:0.058, val_acc:0.972]
Epoch [76/120    avg_loss:0.042, val_acc:0.969]
Epoch [77/120    avg_loss:0.048, val_acc:0.980]
Epoch [78/120    avg_loss:0.038, val_acc:0.972]
Epoch [79/120    avg_loss:0.042, val_acc:0.946]
Epoch [80/120    avg_loss:0.042, val_acc:0.982]
Epoch [81/120    avg_loss:0.054, val_acc:0.975]
Epoch [82/120    avg_loss:0.046, val_acc:0.977]
Epoch [83/120    avg_loss:0.037, val_acc:0.983]
Epoch [84/120    avg_loss:0.052, val_acc:0.977]
Epoch [85/120    avg_loss:0.036, val_acc:0.977]
Epoch [86/120    avg_loss:0.031, val_acc:0.972]
Epoch [87/120    avg_loss:0.031, val_acc:0.984]
Epoch [88/120    avg_loss:0.036, val_acc:0.978]
Epoch [89/120    avg_loss:0.035, val_acc:0.975]
Epoch [90/120    avg_loss:0.030, val_acc:0.981]
Epoch [91/120    avg_loss:0.030, val_acc:0.979]
Epoch [92/120    avg_loss:0.032, val_acc:0.977]
Epoch [93/120    avg_loss:0.031, val_acc:0.971]
Epoch [94/120    avg_loss:0.032, val_acc:0.981]
Epoch [95/120    avg_loss:0.025, val_acc:0.978]
Epoch [96/120    avg_loss:0.021, val_acc:0.990]
Epoch [97/120    avg_loss:0.021, val_acc:0.984]
Epoch [98/120    avg_loss:0.017, val_acc:0.989]
Epoch [99/120    avg_loss:0.019, val_acc:0.984]
Epoch [100/120    avg_loss:0.019, val_acc:0.987]
Epoch [101/120    avg_loss:0.019, val_acc:0.989]
Epoch [102/120    avg_loss:0.018, val_acc:0.985]
Epoch [103/120    avg_loss:0.019, val_acc:0.983]
Epoch [104/120    avg_loss:0.020, val_acc:0.982]
Epoch [105/120    avg_loss:0.017, val_acc:0.984]
Epoch [106/120    avg_loss:0.013, val_acc:0.984]
Epoch [107/120    avg_loss:0.015, val_acc:0.988]
Epoch [108/120    avg_loss:0.017, val_acc:0.985]
Epoch [109/120    avg_loss:0.017, val_acc:0.987]
Epoch [110/120    avg_loss:0.014, val_acc:0.985]
Epoch [111/120    avg_loss:0.012, val_acc:0.985]
Epoch [112/120    avg_loss:0.014, val_acc:0.984]
Epoch [113/120    avg_loss:0.011, val_acc:0.984]
Epoch [114/120    avg_loss:0.012, val_acc:0.984]
Epoch [115/120    avg_loss:0.012, val_acc:0.985]
Epoch [116/120    avg_loss:0.012, val_acc:0.985]
Epoch [117/120    avg_loss:0.014, val_acc:0.984]
Epoch [118/120    avg_loss:0.011, val_acc:0.985]
Epoch [119/120    avg_loss:0.014, val_acc:0.983]
Epoch [120/120    avg_loss:0.013, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1245    1    9    0    3    0    0    0    2   23    2    0
     0    0    0]
 [   0    0    6  713    2   12    0    0    0    9    0    1    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    2    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    8   61    0    7    0    0    0    0  787    7    0    0
     0    5    0]
 [   0    0    3    0    0    0    3    0    0    0   14 2187    3    0
     0    0    0]
 [   0    0    0   21    0    3    0    0    0    2    3    6  496    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   35    0    0    9    0    0    0    0
    89  214    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.02168021680217

F1 scores:
[       nan 0.98765432 0.97723705 0.92417369 0.97482838 0.96956032
 0.96755162 1.         0.997669   0.5862069  0.93634741 0.98624577
 0.95110259 1.         0.96239966 0.75352113 0.98809524]

Kappa:
0.9546056949356818
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f366040cf60>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.816, val_acc:0.133]
Epoch [2/120    avg_loss:2.773, val_acc:0.258]
Epoch [3/120    avg_loss:2.732, val_acc:0.343]
Epoch [4/120    avg_loss:2.689, val_acc:0.356]
Epoch [5/120    avg_loss:2.650, val_acc:0.358]
Epoch [6/120    avg_loss:2.598, val_acc:0.375]
Epoch [7/120    avg_loss:2.541, val_acc:0.382]
Epoch [8/120    avg_loss:2.475, val_acc:0.392]
Epoch [9/120    avg_loss:2.393, val_acc:0.400]
Epoch [10/120    avg_loss:2.338, val_acc:0.415]
Epoch [11/120    avg_loss:2.275, val_acc:0.438]
Epoch [12/120    avg_loss:2.207, val_acc:0.536]
Epoch [13/120    avg_loss:2.157, val_acc:0.548]
Epoch [14/120    avg_loss:2.079, val_acc:0.538]
Epoch [15/120    avg_loss:1.982, val_acc:0.562]
Epoch [16/120    avg_loss:1.921, val_acc:0.501]
Epoch [17/120    avg_loss:1.820, val_acc:0.628]
Epoch [18/120    avg_loss:1.728, val_acc:0.618]
Epoch [19/120    avg_loss:1.601, val_acc:0.628]
Epoch [20/120    avg_loss:1.516, val_acc:0.671]
Epoch [21/120    avg_loss:1.452, val_acc:0.676]
Epoch [22/120    avg_loss:1.361, val_acc:0.685]
Epoch [23/120    avg_loss:1.259, val_acc:0.693]
Epoch [24/120    avg_loss:1.210, val_acc:0.700]
Epoch [25/120    avg_loss:1.150, val_acc:0.727]
Epoch [26/120    avg_loss:1.013, val_acc:0.742]
Epoch [27/120    avg_loss:0.936, val_acc:0.754]
Epoch [28/120    avg_loss:0.850, val_acc:0.752]
Epoch [29/120    avg_loss:0.865, val_acc:0.759]
Epoch [30/120    avg_loss:0.766, val_acc:0.779]
Epoch [31/120    avg_loss:0.701, val_acc:0.821]
Epoch [32/120    avg_loss:0.641, val_acc:0.844]
Epoch [33/120    avg_loss:0.631, val_acc:0.840]
Epoch [34/120    avg_loss:0.580, val_acc:0.843]
Epoch [35/120    avg_loss:0.518, val_acc:0.867]
Epoch [36/120    avg_loss:0.495, val_acc:0.867]
Epoch [37/120    avg_loss:0.453, val_acc:0.845]
Epoch [38/120    avg_loss:0.391, val_acc:0.869]
Epoch [39/120    avg_loss:0.460, val_acc:0.865]
Epoch [40/120    avg_loss:0.435, val_acc:0.892]
Epoch [41/120    avg_loss:0.367, val_acc:0.877]
Epoch [42/120    avg_loss:0.347, val_acc:0.879]
Epoch [43/120    avg_loss:0.322, val_acc:0.863]
Epoch [44/120    avg_loss:0.265, val_acc:0.915]
Epoch [45/120    avg_loss:0.377, val_acc:0.910]
Epoch [46/120    avg_loss:0.302, val_acc:0.914]
Epoch [47/120    avg_loss:0.306, val_acc:0.893]
Epoch [48/120    avg_loss:0.310, val_acc:0.917]
Epoch [49/120    avg_loss:0.241, val_acc:0.908]
Epoch [50/120    avg_loss:0.256, val_acc:0.905]
Epoch [51/120    avg_loss:0.235, val_acc:0.926]
Epoch [52/120    avg_loss:0.168, val_acc:0.916]
Epoch [53/120    avg_loss:0.163, val_acc:0.939]
Epoch [54/120    avg_loss:0.150, val_acc:0.951]
Epoch [55/120    avg_loss:0.138, val_acc:0.946]
Epoch [56/120    avg_loss:0.125, val_acc:0.945]
Epoch [57/120    avg_loss:0.114, val_acc:0.945]
Epoch [58/120    avg_loss:0.113, val_acc:0.927]
Epoch [59/120    avg_loss:0.095, val_acc:0.946]
Epoch [60/120    avg_loss:0.117, val_acc:0.919]
Epoch [61/120    avg_loss:0.114, val_acc:0.954]
Epoch [62/120    avg_loss:0.104, val_acc:0.949]
Epoch [63/120    avg_loss:0.110, val_acc:0.948]
Epoch [64/120    avg_loss:0.092, val_acc:0.962]
Epoch [65/120    avg_loss:0.081, val_acc:0.935]
Epoch [66/120    avg_loss:0.091, val_acc:0.948]
Epoch [67/120    avg_loss:0.099, val_acc:0.955]
Epoch [68/120    avg_loss:0.073, val_acc:0.954]
Epoch [69/120    avg_loss:0.076, val_acc:0.951]
Epoch [70/120    avg_loss:0.069, val_acc:0.952]
Epoch [71/120    avg_loss:0.063, val_acc:0.958]
Epoch [72/120    avg_loss:0.062, val_acc:0.951]
Epoch [73/120    avg_loss:0.063, val_acc:0.972]
Epoch [74/120    avg_loss:0.055, val_acc:0.955]
Epoch [75/120    avg_loss:0.054, val_acc:0.967]
Epoch [76/120    avg_loss:0.054, val_acc:0.951]
Epoch [77/120    avg_loss:0.052, val_acc:0.968]
Epoch [78/120    avg_loss:0.064, val_acc:0.968]
Epoch [79/120    avg_loss:0.043, val_acc:0.959]
Epoch [80/120    avg_loss:0.051, val_acc:0.961]
Epoch [81/120    avg_loss:0.039, val_acc:0.965]
Epoch [82/120    avg_loss:0.039, val_acc:0.933]
Epoch [83/120    avg_loss:0.044, val_acc:0.963]
Epoch [84/120    avg_loss:0.052, val_acc:0.961]
Epoch [85/120    avg_loss:0.045, val_acc:0.962]
Epoch [86/120    avg_loss:0.038, val_acc:0.964]
Epoch [87/120    avg_loss:0.038, val_acc:0.961]
Epoch [88/120    avg_loss:0.034, val_acc:0.965]
Epoch [89/120    avg_loss:0.039, val_acc:0.967]
Epoch [90/120    avg_loss:0.031, val_acc:0.967]
Epoch [91/120    avg_loss:0.028, val_acc:0.969]
Epoch [92/120    avg_loss:0.026, val_acc:0.971]
Epoch [93/120    avg_loss:0.027, val_acc:0.971]
Epoch [94/120    avg_loss:0.031, val_acc:0.972]
Epoch [95/120    avg_loss:0.028, val_acc:0.971]
Epoch [96/120    avg_loss:0.024, val_acc:0.972]
Epoch [97/120    avg_loss:0.027, val_acc:0.971]
Epoch [98/120    avg_loss:0.024, val_acc:0.970]
Epoch [99/120    avg_loss:0.026, val_acc:0.971]
Epoch [100/120    avg_loss:0.026, val_acc:0.974]
Epoch [101/120    avg_loss:0.028, val_acc:0.971]
Epoch [102/120    avg_loss:0.026, val_acc:0.973]
Epoch [103/120    avg_loss:0.026, val_acc:0.972]
Epoch [104/120    avg_loss:0.022, val_acc:0.974]
Epoch [105/120    avg_loss:0.024, val_acc:0.972]
Epoch [106/120    avg_loss:0.026, val_acc:0.974]
Epoch [107/120    avg_loss:0.025, val_acc:0.974]
Epoch [108/120    avg_loss:0.025, val_acc:0.974]
Epoch [109/120    avg_loss:0.022, val_acc:0.973]
Epoch [110/120    avg_loss:0.023, val_acc:0.973]
Epoch [111/120    avg_loss:0.022, val_acc:0.972]
Epoch [112/120    avg_loss:0.022, val_acc:0.973]
Epoch [113/120    avg_loss:0.022, val_acc:0.973]
Epoch [114/120    avg_loss:0.025, val_acc:0.973]
Epoch [115/120    avg_loss:0.023, val_acc:0.972]
Epoch [116/120    avg_loss:0.024, val_acc:0.969]
Epoch [117/120    avg_loss:0.021, val_acc:0.971]
Epoch [118/120    avg_loss:0.022, val_acc:0.970]
Epoch [119/120    avg_loss:0.022, val_acc:0.971]
Epoch [120/120    avg_loss:0.028, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1220    0    0    3    2    0    0    1    6   50    2    0
     0    1    0]
 [   0    0    3  693    2   16    0    0    0   10    0    0   21    0
     2    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    1    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   28   52    0    2    0    0    0    0  784    2    2    0
     2    3    0]
 [   0    0   10    0    0    0    5    0    0    0   12 2175    7    1
     0    0    0]
 [   0    0    0   16   12    6    0    0    0    0   12    2  483    0
     0    0    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   20    0    0    2    0    0    0    0
    57  268    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.75067750677506

F1 scores:
[       nan 0.96202532 0.95836606 0.91909814 0.96818182 0.96071829
 0.97910448 1.         1.         0.66666667 0.92616657 0.97972973
 0.91737892 0.99459459 0.97098976 0.86591276 0.9704142 ]

Kappa:
0.9515249935160224
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f169f02efd0>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.830, val_acc:0.157]
Epoch [2/120    avg_loss:2.796, val_acc:0.167]
Epoch [3/120    avg_loss:2.763, val_acc:0.192]
Epoch [4/120    avg_loss:2.733, val_acc:0.285]
Epoch [5/120    avg_loss:2.685, val_acc:0.299]
Epoch [6/120    avg_loss:2.639, val_acc:0.318]
Epoch [7/120    avg_loss:2.590, val_acc:0.329]
Epoch [8/120    avg_loss:2.530, val_acc:0.345]
Epoch [9/120    avg_loss:2.487, val_acc:0.375]
Epoch [10/120    avg_loss:2.416, val_acc:0.398]
Epoch [11/120    avg_loss:2.378, val_acc:0.419]
Epoch [12/120    avg_loss:2.282, val_acc:0.471]
Epoch [13/120    avg_loss:2.269, val_acc:0.489]
Epoch [14/120    avg_loss:2.222, val_acc:0.429]
Epoch [15/120    avg_loss:2.203, val_acc:0.511]
Epoch [16/120    avg_loss:2.145, val_acc:0.525]
Epoch [17/120    avg_loss:2.056, val_acc:0.571]
Epoch [18/120    avg_loss:1.995, val_acc:0.607]
Epoch [19/120    avg_loss:1.940, val_acc:0.633]
Epoch [20/120    avg_loss:1.900, val_acc:0.617]
Epoch [21/120    avg_loss:1.882, val_acc:0.627]
Epoch [22/120    avg_loss:1.776, val_acc:0.656]
Epoch [23/120    avg_loss:1.686, val_acc:0.664]
Epoch [24/120    avg_loss:1.557, val_acc:0.673]
Epoch [25/120    avg_loss:1.488, val_acc:0.693]
Epoch [26/120    avg_loss:1.419, val_acc:0.693]
Epoch [27/120    avg_loss:1.350, val_acc:0.695]
Epoch [28/120    avg_loss:1.240, val_acc:0.686]
Epoch [29/120    avg_loss:1.217, val_acc:0.694]
Epoch [30/120    avg_loss:1.135, val_acc:0.734]
Epoch [31/120    avg_loss:0.990, val_acc:0.740]
Epoch [32/120    avg_loss:0.925, val_acc:0.746]
Epoch [33/120    avg_loss:0.878, val_acc:0.746]
Epoch [34/120    avg_loss:0.754, val_acc:0.779]
Epoch [35/120    avg_loss:0.862, val_acc:0.783]
Epoch [36/120    avg_loss:0.773, val_acc:0.780]
Epoch [37/120    avg_loss:0.766, val_acc:0.766]
Epoch [38/120    avg_loss:0.716, val_acc:0.781]
Epoch [39/120    avg_loss:0.632, val_acc:0.797]
Epoch [40/120    avg_loss:0.581, val_acc:0.808]
Epoch [41/120    avg_loss:0.530, val_acc:0.819]
Epoch [42/120    avg_loss:0.480, val_acc:0.844]
Epoch [43/120    avg_loss:0.458, val_acc:0.838]
Epoch [44/120    avg_loss:0.477, val_acc:0.857]
Epoch [45/120    avg_loss:0.447, val_acc:0.844]
Epoch [46/120    avg_loss:0.417, val_acc:0.867]
Epoch [47/120    avg_loss:0.410, val_acc:0.873]
Epoch [48/120    avg_loss:0.361, val_acc:0.891]
Epoch [49/120    avg_loss:0.336, val_acc:0.893]
Epoch [50/120    avg_loss:0.276, val_acc:0.896]
Epoch [51/120    avg_loss:0.301, val_acc:0.892]
Epoch [52/120    avg_loss:0.291, val_acc:0.892]
Epoch [53/120    avg_loss:0.268, val_acc:0.913]
Epoch [54/120    avg_loss:0.271, val_acc:0.908]
Epoch [55/120    avg_loss:0.259, val_acc:0.921]
Epoch [56/120    avg_loss:0.267, val_acc:0.902]
Epoch [57/120    avg_loss:0.221, val_acc:0.901]
Epoch [58/120    avg_loss:0.206, val_acc:0.920]
Epoch [59/120    avg_loss:0.210, val_acc:0.917]
Epoch [60/120    avg_loss:0.190, val_acc:0.927]
Epoch [61/120    avg_loss:0.210, val_acc:0.912]
Epoch [62/120    avg_loss:0.189, val_acc:0.920]
Epoch [63/120    avg_loss:0.192, val_acc:0.920]
Epoch [64/120    avg_loss:0.174, val_acc:0.930]
Epoch [65/120    avg_loss:0.179, val_acc:0.921]
Epoch [66/120    avg_loss:0.134, val_acc:0.944]
Epoch [67/120    avg_loss:0.142, val_acc:0.942]
Epoch [68/120    avg_loss:0.143, val_acc:0.945]
Epoch [69/120    avg_loss:0.125, val_acc:0.940]
Epoch [70/120    avg_loss:0.172, val_acc:0.945]
Epoch [71/120    avg_loss:0.140, val_acc:0.933]
Epoch [72/120    avg_loss:0.159, val_acc:0.936]
Epoch [73/120    avg_loss:0.149, val_acc:0.932]
Epoch [74/120    avg_loss:0.166, val_acc:0.936]
Epoch [75/120    avg_loss:0.110, val_acc:0.942]
Epoch [76/120    avg_loss:0.175, val_acc:0.922]
Epoch [77/120    avg_loss:0.152, val_acc:0.934]
Epoch [78/120    avg_loss:0.111, val_acc:0.948]
Epoch [79/120    avg_loss:0.096, val_acc:0.956]
Epoch [80/120    avg_loss:0.093, val_acc:0.953]
Epoch [81/120    avg_loss:0.079, val_acc:0.965]
Epoch [82/120    avg_loss:0.075, val_acc:0.959]
Epoch [83/120    avg_loss:0.080, val_acc:0.960]
Epoch [84/120    avg_loss:0.073, val_acc:0.970]
Epoch [85/120    avg_loss:0.056, val_acc:0.975]
Epoch [86/120    avg_loss:0.052, val_acc:0.959]
Epoch [87/120    avg_loss:0.069, val_acc:0.967]
Epoch [88/120    avg_loss:0.063, val_acc:0.969]
Epoch [89/120    avg_loss:0.053, val_acc:0.970]
Epoch [90/120    avg_loss:0.055, val_acc:0.964]
Epoch [91/120    avg_loss:0.052, val_acc:0.969]
Epoch [92/120    avg_loss:0.045, val_acc:0.972]
Epoch [93/120    avg_loss:0.044, val_acc:0.975]
Epoch [94/120    avg_loss:0.039, val_acc:0.973]
Epoch [95/120    avg_loss:0.039, val_acc:0.971]
Epoch [96/120    avg_loss:0.046, val_acc:0.967]
Epoch [97/120    avg_loss:0.051, val_acc:0.965]
Epoch [98/120    avg_loss:0.044, val_acc:0.978]
Epoch [99/120    avg_loss:0.041, val_acc:0.978]
Epoch [100/120    avg_loss:0.040, val_acc:0.974]
Epoch [101/120    avg_loss:0.040, val_acc:0.973]
Epoch [102/120    avg_loss:0.043, val_acc:0.969]
Epoch [103/120    avg_loss:0.038, val_acc:0.973]
Epoch [104/120    avg_loss:0.031, val_acc:0.978]
Epoch [105/120    avg_loss:0.029, val_acc:0.974]
Epoch [106/120    avg_loss:0.030, val_acc:0.977]
Epoch [107/120    avg_loss:0.029, val_acc:0.980]
Epoch [108/120    avg_loss:0.027, val_acc:0.984]
Epoch [109/120    avg_loss:0.030, val_acc:0.972]
Epoch [110/120    avg_loss:0.038, val_acc:0.969]
Epoch [111/120    avg_loss:0.038, val_acc:0.974]
Epoch [112/120    avg_loss:0.050, val_acc:0.975]
Epoch [113/120    avg_loss:0.034, val_acc:0.977]
Epoch [114/120    avg_loss:0.028, val_acc:0.975]
Epoch [115/120    avg_loss:0.024, val_acc:0.984]
Epoch [116/120    avg_loss:0.024, val_acc:0.979]
Epoch [117/120    avg_loss:0.022, val_acc:0.982]
Epoch [118/120    avg_loss:0.021, val_acc:0.981]
Epoch [119/120    avg_loss:0.021, val_acc:0.980]
Epoch [120/120    avg_loss:0.019, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1230    2    0    0    3    0    0    0    2   38    1    0
     0    9    0]
 [   0    0    3  700    1   21    0    0    0    8    0    0   13    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    2    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    3    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    1    0    0   11    0    1    2    0
     0    0    0]
 [   0    0   23   89    0    5    1    0    0    0  727   20    0    0
     0   10    0]
 [   0    0   12    0    0    0    8    0    0    0    1 2171    4    1
    13    0    0]
 [   0    0    0   30    6   11    0    0    0    0    8    3  473    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1138    0    0]
 [   0    0    0    0    0    3   10    0    0    0    0    0    2    0
    66  266    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
95.02439024390245

F1 scores:
[       nan 0.975      0.96357227 0.89115213 0.98383372 0.94795127
 0.97821187 0.96153846 1.         0.59459459 0.89975248 0.97660819
 0.91489362 0.99462366 0.96277496 0.84177215 0.95180723]

Kappa:
0.943226853053916
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1e5827cef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.828, val_acc:0.247]
Epoch [2/120    avg_loss:2.779, val_acc:0.317]
Epoch [3/120    avg_loss:2.732, val_acc:0.363]
Epoch [4/120    avg_loss:2.692, val_acc:0.378]
Epoch [5/120    avg_loss:2.634, val_acc:0.387]
Epoch [6/120    avg_loss:2.602, val_acc:0.394]
Epoch [7/120    avg_loss:2.539, val_acc:0.398]
Epoch [8/120    avg_loss:2.457, val_acc:0.404]
Epoch [9/120    avg_loss:2.425, val_acc:0.414]
Epoch [10/120    avg_loss:2.378, val_acc:0.442]
Epoch [11/120    avg_loss:2.348, val_acc:0.459]
Epoch [12/120    avg_loss:2.277, val_acc:0.457]
Epoch [13/120    avg_loss:2.221, val_acc:0.477]
Epoch [14/120    avg_loss:2.217, val_acc:0.498]
Epoch [15/120    avg_loss:2.145, val_acc:0.503]
Epoch [16/120    avg_loss:2.103, val_acc:0.517]
Epoch [17/120    avg_loss:2.052, val_acc:0.564]
Epoch [18/120    avg_loss:1.973, val_acc:0.609]
Epoch [19/120    avg_loss:1.913, val_acc:0.627]
Epoch [20/120    avg_loss:1.854, val_acc:0.613]
Epoch [21/120    avg_loss:1.756, val_acc:0.661]
Epoch [22/120    avg_loss:1.711, val_acc:0.670]
Epoch [23/120    avg_loss:1.582, val_acc:0.700]
Epoch [24/120    avg_loss:1.482, val_acc:0.683]
Epoch [25/120    avg_loss:1.510, val_acc:0.672]
Epoch [26/120    avg_loss:1.397, val_acc:0.718]
Epoch [27/120    avg_loss:1.287, val_acc:0.700]
Epoch [28/120    avg_loss:1.165, val_acc:0.715]
Epoch [29/120    avg_loss:1.123, val_acc:0.731]
Epoch [30/120    avg_loss:1.014, val_acc:0.747]
Epoch [31/120    avg_loss:0.942, val_acc:0.752]
Epoch [32/120    avg_loss:0.897, val_acc:0.764]
Epoch [33/120    avg_loss:0.806, val_acc:0.784]
Epoch [34/120    avg_loss:0.742, val_acc:0.799]
Epoch [35/120    avg_loss:0.694, val_acc:0.800]
Epoch [36/120    avg_loss:0.648, val_acc:0.806]
Epoch [37/120    avg_loss:0.649, val_acc:0.808]
Epoch [38/120    avg_loss:0.642, val_acc:0.831]
Epoch [39/120    avg_loss:0.536, val_acc:0.873]
Epoch [40/120    avg_loss:0.483, val_acc:0.868]
Epoch [41/120    avg_loss:0.456, val_acc:0.860]
Epoch [42/120    avg_loss:0.424, val_acc:0.897]
Epoch [43/120    avg_loss:0.399, val_acc:0.884]
Epoch [44/120    avg_loss:0.346, val_acc:0.885]
Epoch [45/120    avg_loss:0.350, val_acc:0.885]
Epoch [46/120    avg_loss:0.370, val_acc:0.900]
Epoch [47/120    avg_loss:0.362, val_acc:0.878]
Epoch [48/120    avg_loss:0.286, val_acc:0.897]
Epoch [49/120    avg_loss:0.268, val_acc:0.904]
Epoch [50/120    avg_loss:0.286, val_acc:0.896]
Epoch [51/120    avg_loss:0.270, val_acc:0.899]
Epoch [52/120    avg_loss:0.292, val_acc:0.905]
Epoch [53/120    avg_loss:0.260, val_acc:0.910]
Epoch [54/120    avg_loss:0.255, val_acc:0.883]
Epoch [55/120    avg_loss:0.298, val_acc:0.910]
Epoch [56/120    avg_loss:0.231, val_acc:0.921]
Epoch [57/120    avg_loss:0.199, val_acc:0.944]
Epoch [58/120    avg_loss:0.221, val_acc:0.885]
Epoch [59/120    avg_loss:0.207, val_acc:0.934]
Epoch [60/120    avg_loss:0.185, val_acc:0.921]
Epoch [61/120    avg_loss:0.184, val_acc:0.931]
Epoch [62/120    avg_loss:0.175, val_acc:0.931]
Epoch [63/120    avg_loss:0.177, val_acc:0.918]
Epoch [64/120    avg_loss:0.185, val_acc:0.924]
Epoch [65/120    avg_loss:2.322, val_acc:0.322]
Epoch [66/120    avg_loss:2.189, val_acc:0.368]
Epoch [67/120    avg_loss:2.155, val_acc:0.326]
Epoch [68/120    avg_loss:2.109, val_acc:0.414]
Epoch [69/120    avg_loss:2.073, val_acc:0.432]
Epoch [70/120    avg_loss:2.029, val_acc:0.449]
Epoch [71/120    avg_loss:1.999, val_acc:0.456]
Epoch [72/120    avg_loss:1.992, val_acc:0.456]
Epoch [73/120    avg_loss:1.998, val_acc:0.455]
Epoch [74/120    avg_loss:1.991, val_acc:0.456]
Epoch [75/120    avg_loss:1.988, val_acc:0.459]
Epoch [76/120    avg_loss:1.960, val_acc:0.458]
Epoch [77/120    avg_loss:1.967, val_acc:0.459]
Epoch [78/120    avg_loss:1.968, val_acc:0.457]
Epoch [79/120    avg_loss:1.963, val_acc:0.458]
Epoch [80/120    avg_loss:1.998, val_acc:0.466]
Epoch [81/120    avg_loss:1.967, val_acc:0.466]
Epoch [82/120    avg_loss:1.992, val_acc:0.469]
Epoch [83/120    avg_loss:1.984, val_acc:0.468]
Epoch [84/120    avg_loss:1.962, val_acc:0.468]
Epoch [85/120    avg_loss:1.960, val_acc:0.469]
Epoch [86/120    avg_loss:1.976, val_acc:0.469]
Epoch [87/120    avg_loss:1.953, val_acc:0.468]
Epoch [88/120    avg_loss:1.948, val_acc:0.468]
Epoch [89/120    avg_loss:1.955, val_acc:0.468]
Epoch [90/120    avg_loss:1.980, val_acc:0.468]
Epoch [91/120    avg_loss:1.922, val_acc:0.470]
Epoch [92/120    avg_loss:1.952, val_acc:0.468]
Epoch [93/120    avg_loss:1.931, val_acc:0.469]
Epoch [94/120    avg_loss:1.943, val_acc:0.469]
Epoch [95/120    avg_loss:1.987, val_acc:0.469]
Epoch [96/120    avg_loss:1.949, val_acc:0.469]
Epoch [97/120    avg_loss:1.981, val_acc:0.469]
Epoch [98/120    avg_loss:1.928, val_acc:0.469]
Epoch [99/120    avg_loss:1.951, val_acc:0.470]
Epoch [100/120    avg_loss:1.967, val_acc:0.469]
Epoch [101/120    avg_loss:1.960, val_acc:0.469]
Epoch [102/120    avg_loss:1.983, val_acc:0.469]
Epoch [103/120    avg_loss:1.958, val_acc:0.469]
Epoch [104/120    avg_loss:1.972, val_acc:0.469]
Epoch [105/120    avg_loss:1.962, val_acc:0.469]
Epoch [106/120    avg_loss:1.938, val_acc:0.469]
Epoch [107/120    avg_loss:1.952, val_acc:0.470]
Epoch [108/120    avg_loss:1.987, val_acc:0.469]
Epoch [109/120    avg_loss:1.910, val_acc:0.469]
Epoch [110/120    avg_loss:1.966, val_acc:0.469]
Epoch [111/120    avg_loss:1.967, val_acc:0.469]
Epoch [112/120    avg_loss:1.962, val_acc:0.469]
Epoch [113/120    avg_loss:1.938, val_acc:0.469]
Epoch [114/120    avg_loss:1.964, val_acc:0.469]
Epoch [115/120    avg_loss:1.944, val_acc:0.469]
Epoch [116/120    avg_loss:1.967, val_acc:0.469]
Epoch [117/120    avg_loss:1.911, val_acc:0.469]
Epoch [118/120    avg_loss:1.930, val_acc:0.469]
Epoch [119/120    avg_loss:1.950, val_acc:0.469]
Epoch [120/120    avg_loss:1.959, val_acc:0.469]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   15   10    0    0    0    6    0    0    0    0    0    0   10
     0    0    0]
 [   0    0  267   11    0  174  249    0    0    0  174  178   21  173
    38    0    0]
 [   0    0   21    8    0  162  178    0    0    0    0  109    8  261
     0    0    0]
 [   0   21    0    0    0    4   85    0    0    0    0    0    0  103
     0    0    0]
 [   0    0    0    0    0   18   81    0    0    0    3    1    9   44
   279    0    0]
 [   0    0    0    0    0    1  365    0    0    0   51   14    0  208
    18    0    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0   40    0  390    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0   11
     0    0    0]
 [   0    7   21    0    0   37   49    0    0    0  468   77   54  162
     0    0    0]
 [   0    2   19    0    0  171  196    0    8    0    2 1142  180  413
     2    0   75]
 [   0   15   40    0    0   10   95    0    0    0    0   34  151   82
     0    0  107]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0   10    0    0   24   11    0   29    0    0    0   16   30
  1019    0    0]
 [   0    0   47    0    0    0   46    0    8    0    0  100   14   52
    80    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
44.57452574525745

F1 scores:
[       nan 0.2970297  0.31046512 0.02088773 0.         0.03474903
 0.3492823  0.         0.9017341  0.         0.59504132 0.59094437
 0.30597771 0.19280875 0.79145631 0.         0.48      ]

Kappa:
0.3846075594383542
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1723e6df60>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.855, val_acc:0.132]
Epoch [2/120    avg_loss:2.819, val_acc:0.136]
Epoch [3/120    avg_loss:2.791, val_acc:0.160]
Epoch [4/120    avg_loss:2.752, val_acc:0.237]
Epoch [5/120    avg_loss:2.712, val_acc:0.375]
Epoch [6/120    avg_loss:2.673, val_acc:0.408]
Epoch [7/120    avg_loss:2.631, val_acc:0.442]
Epoch [8/120    avg_loss:2.590, val_acc:0.454]
Epoch [9/120    avg_loss:2.535, val_acc:0.492]
Epoch [10/120    avg_loss:2.480, val_acc:0.482]
Epoch [11/120    avg_loss:2.415, val_acc:0.487]
Epoch [12/120    avg_loss:2.363, val_acc:0.506]
Epoch [13/120    avg_loss:2.315, val_acc:0.531]
Epoch [14/120    avg_loss:2.278, val_acc:0.519]
Epoch [15/120    avg_loss:2.214, val_acc:0.551]
Epoch [16/120    avg_loss:2.189, val_acc:0.568]
Epoch [17/120    avg_loss:2.119, val_acc:0.558]
Epoch [18/120    avg_loss:2.076, val_acc:0.594]
Epoch [19/120    avg_loss:2.016, val_acc:0.614]
Epoch [20/120    avg_loss:1.947, val_acc:0.608]
Epoch [21/120    avg_loss:1.833, val_acc:0.636]
Epoch [22/120    avg_loss:1.771, val_acc:0.593]
Epoch [23/120    avg_loss:1.738, val_acc:0.647]
Epoch [24/120    avg_loss:1.617, val_acc:0.665]
Epoch [25/120    avg_loss:1.510, val_acc:0.701]
Epoch [26/120    avg_loss:1.393, val_acc:0.734]
Epoch [27/120    avg_loss:1.326, val_acc:0.729]
Epoch [28/120    avg_loss:1.209, val_acc:0.756]
Epoch [29/120    avg_loss:1.159, val_acc:0.746]
Epoch [30/120    avg_loss:1.038, val_acc:0.770]
Epoch [31/120    avg_loss:1.021, val_acc:0.746]
Epoch [32/120    avg_loss:0.897, val_acc:0.787]
Epoch [33/120    avg_loss:0.838, val_acc:0.789]
Epoch [34/120    avg_loss:0.827, val_acc:0.800]
Epoch [35/120    avg_loss:0.762, val_acc:0.800]
Epoch [36/120    avg_loss:0.724, val_acc:0.814]
Epoch [37/120    avg_loss:0.649, val_acc:0.816]
Epoch [38/120    avg_loss:0.577, val_acc:0.835]
Epoch [39/120    avg_loss:0.564, val_acc:0.807]
Epoch [40/120    avg_loss:0.517, val_acc:0.835]
Epoch [41/120    avg_loss:0.483, val_acc:0.850]
Epoch [42/120    avg_loss:0.441, val_acc:0.826]
Epoch [43/120    avg_loss:0.408, val_acc:0.875]
Epoch [44/120    avg_loss:0.375, val_acc:0.878]
Epoch [45/120    avg_loss:0.336, val_acc:0.887]
Epoch [46/120    avg_loss:0.301, val_acc:0.903]
Epoch [47/120    avg_loss:0.285, val_acc:0.894]
Epoch [48/120    avg_loss:0.268, val_acc:0.907]
Epoch [49/120    avg_loss:0.249, val_acc:0.875]
Epoch [50/120    avg_loss:0.260, val_acc:0.878]
Epoch [51/120    avg_loss:0.224, val_acc:0.905]
Epoch [52/120    avg_loss:0.221, val_acc:0.898]
Epoch [53/120    avg_loss:0.209, val_acc:0.914]
Epoch [54/120    avg_loss:0.162, val_acc:0.917]
Epoch [55/120    avg_loss:0.177, val_acc:0.920]
Epoch [56/120    avg_loss:0.163, val_acc:0.924]
Epoch [57/120    avg_loss:0.178, val_acc:0.921]
Epoch [58/120    avg_loss:0.166, val_acc:0.908]
Epoch [59/120    avg_loss:0.165, val_acc:0.919]
Epoch [60/120    avg_loss:0.142, val_acc:0.935]
Epoch [61/120    avg_loss:0.137, val_acc:0.926]
Epoch [62/120    avg_loss:0.145, val_acc:0.939]
Epoch [63/120    avg_loss:0.135, val_acc:0.927]
Epoch [64/120    avg_loss:0.148, val_acc:0.930]
Epoch [65/120    avg_loss:0.093, val_acc:0.942]
Epoch [66/120    avg_loss:0.088, val_acc:0.942]
Epoch [67/120    avg_loss:0.096, val_acc:0.941]
Epoch [68/120    avg_loss:0.132, val_acc:0.951]
Epoch [69/120    avg_loss:0.093, val_acc:0.952]
Epoch [70/120    avg_loss:0.071, val_acc:0.955]
Epoch [71/120    avg_loss:0.072, val_acc:0.950]
Epoch [72/120    avg_loss:0.089, val_acc:0.951]
Epoch [73/120    avg_loss:0.091, val_acc:0.944]
Epoch [74/120    avg_loss:0.087, val_acc:0.944]
Epoch [75/120    avg_loss:0.080, val_acc:0.949]
Epoch [76/120    avg_loss:0.069, val_acc:0.953]
Epoch [77/120    avg_loss:0.058, val_acc:0.958]
Epoch [78/120    avg_loss:0.053, val_acc:0.967]
Epoch [79/120    avg_loss:0.055, val_acc:0.954]
Epoch [80/120    avg_loss:0.041, val_acc:0.962]
Epoch [81/120    avg_loss:0.046, val_acc:0.956]
Epoch [82/120    avg_loss:0.054, val_acc:0.949]
Epoch [83/120    avg_loss:0.077, val_acc:0.960]
Epoch [84/120    avg_loss:0.058, val_acc:0.958]
Epoch [85/120    avg_loss:0.049, val_acc:0.965]
Epoch [86/120    avg_loss:0.043, val_acc:0.963]
Epoch [87/120    avg_loss:0.052, val_acc:0.958]
Epoch [88/120    avg_loss:0.045, val_acc:0.969]
Epoch [89/120    avg_loss:0.046, val_acc:0.967]
Epoch [90/120    avg_loss:0.041, val_acc:0.967]
Epoch [91/120    avg_loss:0.036, val_acc:0.968]
Epoch [92/120    avg_loss:0.030, val_acc:0.965]
Epoch [93/120    avg_loss:0.028, val_acc:0.971]
Epoch [94/120    avg_loss:0.036, val_acc:0.970]
Epoch [95/120    avg_loss:0.060, val_acc:0.963]
Epoch [96/120    avg_loss:0.143, val_acc:0.946]
Epoch [97/120    avg_loss:0.153, val_acc:0.938]
Epoch [98/120    avg_loss:0.114, val_acc:0.949]
Epoch [99/120    avg_loss:0.077, val_acc:0.956]
Epoch [100/120    avg_loss:0.068, val_acc:0.964]
Epoch [101/120    avg_loss:0.049, val_acc:0.959]
Epoch [102/120    avg_loss:0.044, val_acc:0.959]
Epoch [103/120    avg_loss:0.046, val_acc:0.967]
Epoch [104/120    avg_loss:0.034, val_acc:0.961]
Epoch [105/120    avg_loss:0.038, val_acc:0.962]
Epoch [106/120    avg_loss:0.036, val_acc:0.959]
Epoch [107/120    avg_loss:0.035, val_acc:0.971]
Epoch [108/120    avg_loss:0.021, val_acc:0.969]
Epoch [109/120    avg_loss:0.024, val_acc:0.972]
Epoch [110/120    avg_loss:0.021, val_acc:0.972]
Epoch [111/120    avg_loss:0.019, val_acc:0.971]
Epoch [112/120    avg_loss:0.021, val_acc:0.974]
Epoch [113/120    avg_loss:0.022, val_acc:0.973]
Epoch [114/120    avg_loss:0.021, val_acc:0.973]
Epoch [115/120    avg_loss:0.018, val_acc:0.973]
Epoch [116/120    avg_loss:0.019, val_acc:0.973]
Epoch [117/120    avg_loss:0.021, val_acc:0.974]
Epoch [118/120    avg_loss:0.018, val_acc:0.974]
Epoch [119/120    avg_loss:0.020, val_acc:0.974]
Epoch [120/120    avg_loss:0.021, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1238    1    0    0    1    0    0    2    4   19    7    1
     0   12    0]
 [   0    0    4  697    3    4    0    0    0    6    0    0   33    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  421    0    0    0    2    0    1    0    0
    11    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    2    0    0   11    0    0    2    0
     0    0    0]
 [   0    0    9   90    0    5    0    0    0    0  750    9    0    0
     1   11    0]
 [   0    0    7    0    0    0    1    0    2    0   12 2183    4    1
     0    0    0]
 [   0    0    0   17    0    9    0    0    0    0    6    9  492    0
     0    0    1]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    1    0    0    0
  1132    0    0]
 [   0    0    0    0    0   20   48    0    0    1    0    0    0    0
    66  212    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.02439024390245

F1 scores:
[       nan 0.98765432 0.97365317 0.89646302 0.99300699 0.9345172
 0.96041056 1.         0.99767981 0.55       0.90964221 0.98488608
 0.91705499 0.99191375 0.96381439 0.72852234 0.98809524]

Kappa:
0.9432401095477849
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f55b28d9f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.815, val_acc:0.044]
Epoch [2/120    avg_loss:2.749, val_acc:0.205]
Epoch [3/120    avg_loss:2.685, val_acc:0.250]
Epoch [4/120    avg_loss:2.637, val_acc:0.251]
Epoch [5/120    avg_loss:2.583, val_acc:0.277]
Epoch [6/120    avg_loss:2.532, val_acc:0.358]
Epoch [7/120    avg_loss:2.479, val_acc:0.363]
Epoch [8/120    avg_loss:2.425, val_acc:0.280]
Epoch [9/120    avg_loss:2.376, val_acc:0.385]
Epoch [10/120    avg_loss:2.326, val_acc:0.389]
Epoch [11/120    avg_loss:2.289, val_acc:0.409]
Epoch [12/120    avg_loss:2.210, val_acc:0.419]
Epoch [13/120    avg_loss:2.181, val_acc:0.402]
Epoch [14/120    avg_loss:2.152, val_acc:0.455]
Epoch [15/120    avg_loss:2.080, val_acc:0.499]
Epoch [16/120    avg_loss:2.080, val_acc:0.502]
Epoch [17/120    avg_loss:2.050, val_acc:0.561]
Epoch [18/120    avg_loss:1.940, val_acc:0.567]
Epoch [19/120    avg_loss:1.868, val_acc:0.580]
Epoch [20/120    avg_loss:1.781, val_acc:0.595]
Epoch [21/120    avg_loss:1.693, val_acc:0.640]
Epoch [22/120    avg_loss:1.672, val_acc:0.638]
Epoch [23/120    avg_loss:1.566, val_acc:0.656]
Epoch [24/120    avg_loss:1.481, val_acc:0.650]
Epoch [25/120    avg_loss:1.402, val_acc:0.672]
Epoch [26/120    avg_loss:1.297, val_acc:0.698]
Epoch [27/120    avg_loss:1.208, val_acc:0.713]
Epoch [28/120    avg_loss:1.102, val_acc:0.753]
Epoch [29/120    avg_loss:1.024, val_acc:0.745]
Epoch [30/120    avg_loss:0.956, val_acc:0.749]
Epoch [31/120    avg_loss:0.955, val_acc:0.741]
Epoch [32/120    avg_loss:0.857, val_acc:0.740]
Epoch [33/120    avg_loss:0.817, val_acc:0.763]
Epoch [34/120    avg_loss:0.748, val_acc:0.773]
Epoch [35/120    avg_loss:0.701, val_acc:0.794]
Epoch [36/120    avg_loss:0.641, val_acc:0.809]
Epoch [37/120    avg_loss:0.609, val_acc:0.836]
Epoch [38/120    avg_loss:0.589, val_acc:0.852]
Epoch [39/120    avg_loss:0.517, val_acc:0.848]
Epoch [40/120    avg_loss:0.479, val_acc:0.858]
Epoch [41/120    avg_loss:0.512, val_acc:0.859]
Epoch [42/120    avg_loss:0.488, val_acc:0.873]
Epoch [43/120    avg_loss:0.427, val_acc:0.875]
Epoch [44/120    avg_loss:0.402, val_acc:0.872]
Epoch [45/120    avg_loss:0.395, val_acc:0.878]
Epoch [46/120    avg_loss:0.360, val_acc:0.892]
Epoch [47/120    avg_loss:0.381, val_acc:0.889]
Epoch [48/120    avg_loss:0.418, val_acc:0.890]
Epoch [49/120    avg_loss:0.367, val_acc:0.890]
Epoch [50/120    avg_loss:0.298, val_acc:0.902]
Epoch [51/120    avg_loss:0.264, val_acc:0.906]
Epoch [52/120    avg_loss:0.270, val_acc:0.900]
Epoch [53/120    avg_loss:0.278, val_acc:0.918]
Epoch [54/120    avg_loss:0.203, val_acc:0.938]
Epoch [55/120    avg_loss:0.200, val_acc:0.920]
Epoch [56/120    avg_loss:0.172, val_acc:0.938]
Epoch [57/120    avg_loss:0.194, val_acc:0.926]
Epoch [58/120    avg_loss:0.176, val_acc:0.951]
Epoch [59/120    avg_loss:0.143, val_acc:0.942]
Epoch [60/120    avg_loss:0.121, val_acc:0.948]
Epoch [61/120    avg_loss:0.147, val_acc:0.938]
Epoch [62/120    avg_loss:0.137, val_acc:0.929]
Epoch [63/120    avg_loss:0.125, val_acc:0.941]
Epoch [64/120    avg_loss:0.121, val_acc:0.944]
Epoch [65/120    avg_loss:0.105, val_acc:0.949]
Epoch [66/120    avg_loss:0.109, val_acc:0.948]
Epoch [67/120    avg_loss:0.108, val_acc:0.932]
Epoch [68/120    avg_loss:0.109, val_acc:0.957]
Epoch [69/120    avg_loss:0.236, val_acc:0.918]
Epoch [70/120    avg_loss:0.318, val_acc:0.932]
Epoch [71/120    avg_loss:0.189, val_acc:0.938]
Epoch [72/120    avg_loss:0.180, val_acc:0.943]
Epoch [73/120    avg_loss:0.127, val_acc:0.953]
Epoch [74/120    avg_loss:0.097, val_acc:0.954]
Epoch [75/120    avg_loss:0.083, val_acc:0.963]
Epoch [76/120    avg_loss:0.079, val_acc:0.951]
Epoch [77/120    avg_loss:0.066, val_acc:0.956]
Epoch [78/120    avg_loss:0.060, val_acc:0.941]
Epoch [79/120    avg_loss:0.065, val_acc:0.955]
Epoch [80/120    avg_loss:0.071, val_acc:0.962]
Epoch [81/120    avg_loss:0.051, val_acc:0.969]
Epoch [82/120    avg_loss:0.043, val_acc:0.969]
Epoch [83/120    avg_loss:0.044, val_acc:0.972]
Epoch [84/120    avg_loss:0.047, val_acc:0.953]
Epoch [85/120    avg_loss:0.053, val_acc:0.958]
Epoch [86/120    avg_loss:0.041, val_acc:0.966]
Epoch [87/120    avg_loss:0.045, val_acc:0.973]
Epoch [88/120    avg_loss:0.041, val_acc:0.972]
Epoch [89/120    avg_loss:0.052, val_acc:0.973]
Epoch [90/120    avg_loss:0.047, val_acc:0.970]
Epoch [91/120    avg_loss:0.037, val_acc:0.969]
Epoch [92/120    avg_loss:0.040, val_acc:0.968]
Epoch [93/120    avg_loss:0.033, val_acc:0.970]
Epoch [94/120    avg_loss:0.038, val_acc:0.968]
Epoch [95/120    avg_loss:0.036, val_acc:0.961]
Epoch [96/120    avg_loss:0.043, val_acc:0.971]
Epoch [97/120    avg_loss:0.038, val_acc:0.975]
Epoch [98/120    avg_loss:0.036, val_acc:0.973]
Epoch [99/120    avg_loss:0.039, val_acc:0.976]
Epoch [100/120    avg_loss:0.031, val_acc:0.969]
Epoch [101/120    avg_loss:0.033, val_acc:0.968]
Epoch [102/120    avg_loss:0.031, val_acc:0.973]
Epoch [103/120    avg_loss:0.036, val_acc:0.963]
Epoch [104/120    avg_loss:0.032, val_acc:0.966]
Epoch [105/120    avg_loss:0.031, val_acc:0.969]
Epoch [106/120    avg_loss:0.030, val_acc:0.962]
Epoch [107/120    avg_loss:0.032, val_acc:0.965]
Epoch [108/120    avg_loss:0.033, val_acc:0.950]
Epoch [109/120    avg_loss:0.025, val_acc:0.981]
Epoch [110/120    avg_loss:0.019, val_acc:0.980]
Epoch [111/120    avg_loss:0.020, val_acc:0.971]
Epoch [112/120    avg_loss:0.018, val_acc:0.975]
Epoch [113/120    avg_loss:0.020, val_acc:0.968]
Epoch [114/120    avg_loss:0.018, val_acc:0.978]
Epoch [115/120    avg_loss:0.018, val_acc:0.977]
Epoch [116/120    avg_loss:0.018, val_acc:0.975]
Epoch [117/120    avg_loss:0.023, val_acc:0.971]
Epoch [118/120    avg_loss:0.022, val_acc:0.973]
Epoch [119/120    avg_loss:0.018, val_acc:0.976]
Epoch [120/120    avg_loss:0.018, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1249    0    0    0    5    0    0    0    2    6    0    0
     0   23    0]
 [   0    0    2  694    3   24    0    0    0   15    0    0    7    2
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    2    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    4    0    0   13    0    0    0    0
     0    0    0]
 [   0    0   35   90    0    4    0    0    0    0  726    5    0    0
     1   14    0]
 [   0    0   21    0    0    0   26    0    0    0    7 2132    2    2
    20    0    0]
 [   0    0    0   32    0    9    0    0    0    0    9    0  479    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    1    0    0    0
  1131    0    0]
 [   0    0    0    0    0    0   34    0    0    0    0    0    1    0
    79  233    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.5040650406504

F1 scores:
[       nan 0.98765432 0.9633629  0.88690096 0.99065421 0.94493392
 0.94934877 0.96153846 1.         0.55319149 0.8962963  0.97932935
 0.93646139 0.98930481 0.95322377 0.75526742 0.97109827]

Kappa:
0.9373887964476282
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7011b34eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.836, val_acc:0.014]
Epoch [2/120    avg_loss:2.793, val_acc:0.068]
Epoch [3/120    avg_loss:2.758, val_acc:0.103]
Epoch [4/120    avg_loss:2.719, val_acc:0.146]
Epoch [5/120    avg_loss:2.686, val_acc:0.244]
Epoch [6/120    avg_loss:2.621, val_acc:0.334]
Epoch [7/120    avg_loss:2.584, val_acc:0.420]
Epoch [8/120    avg_loss:2.528, val_acc:0.390]
Epoch [9/120    avg_loss:2.442, val_acc:0.466]
Epoch [10/120    avg_loss:2.387, val_acc:0.509]
Epoch [11/120    avg_loss:2.363, val_acc:0.493]
Epoch [12/120    avg_loss:2.268, val_acc:0.527]
Epoch [13/120    avg_loss:2.219, val_acc:0.556]
Epoch [14/120    avg_loss:2.167, val_acc:0.560]
Epoch [15/120    avg_loss:2.132, val_acc:0.562]
Epoch [16/120    avg_loss:2.081, val_acc:0.586]
Epoch [17/120    avg_loss:1.976, val_acc:0.602]
Epoch [18/120    avg_loss:1.885, val_acc:0.610]
Epoch [19/120    avg_loss:1.835, val_acc:0.603]
Epoch [20/120    avg_loss:1.742, val_acc:0.618]
Epoch [21/120    avg_loss:1.681, val_acc:0.604]
Epoch [22/120    avg_loss:1.575, val_acc:0.626]
Epoch [23/120    avg_loss:1.544, val_acc:0.645]
Epoch [24/120    avg_loss:1.449, val_acc:0.632]
Epoch [25/120    avg_loss:1.395, val_acc:0.615]
Epoch [26/120    avg_loss:1.299, val_acc:0.670]
Epoch [27/120    avg_loss:1.275, val_acc:0.662]
Epoch [28/120    avg_loss:1.190, val_acc:0.668]
Epoch [29/120    avg_loss:1.189, val_acc:0.660]
Epoch [30/120    avg_loss:1.157, val_acc:0.689]
Epoch [31/120    avg_loss:1.077, val_acc:0.722]
Epoch [32/120    avg_loss:0.973, val_acc:0.722]
Epoch [33/120    avg_loss:0.902, val_acc:0.730]
Epoch [34/120    avg_loss:0.929, val_acc:0.752]
Epoch [35/120    avg_loss:0.866, val_acc:0.766]
Epoch [36/120    avg_loss:0.815, val_acc:0.780]
Epoch [37/120    avg_loss:0.747, val_acc:0.773]
Epoch [38/120    avg_loss:0.701, val_acc:0.804]
Epoch [39/120    avg_loss:0.658, val_acc:0.799]
Epoch [40/120    avg_loss:0.627, val_acc:0.815]
Epoch [41/120    avg_loss:0.584, val_acc:0.825]
Epoch [42/120    avg_loss:0.517, val_acc:0.831]
Epoch [43/120    avg_loss:0.546, val_acc:0.822]
Epoch [44/120    avg_loss:0.503, val_acc:0.830]
Epoch [45/120    avg_loss:0.471, val_acc:0.815]
Epoch [46/120    avg_loss:0.400, val_acc:0.852]
Epoch [47/120    avg_loss:0.420, val_acc:0.855]
Epoch [48/120    avg_loss:0.399, val_acc:0.855]
Epoch [49/120    avg_loss:0.338, val_acc:0.838]
Epoch [50/120    avg_loss:0.326, val_acc:0.868]
Epoch [51/120    avg_loss:0.284, val_acc:0.851]
Epoch [52/120    avg_loss:0.312, val_acc:0.867]
Epoch [53/120    avg_loss:0.260, val_acc:0.902]
Epoch [54/120    avg_loss:0.237, val_acc:0.911]
Epoch [55/120    avg_loss:0.210, val_acc:0.912]
Epoch [56/120    avg_loss:0.195, val_acc:0.926]
Epoch [57/120    avg_loss:0.177, val_acc:0.922]
Epoch [58/120    avg_loss:0.174, val_acc:0.911]
Epoch [59/120    avg_loss:0.187, val_acc:0.914]
Epoch [60/120    avg_loss:0.161, val_acc:0.916]
Epoch [61/120    avg_loss:0.159, val_acc:0.921]
Epoch [62/120    avg_loss:0.165, val_acc:0.912]
Epoch [63/120    avg_loss:0.151, val_acc:0.933]
Epoch [64/120    avg_loss:0.142, val_acc:0.920]
Epoch [65/120    avg_loss:0.122, val_acc:0.935]
Epoch [66/120    avg_loss:0.141, val_acc:0.924]
Epoch [67/120    avg_loss:0.114, val_acc:0.927]
Epoch [68/120    avg_loss:0.142, val_acc:0.910]
Epoch [69/120    avg_loss:0.121, val_acc:0.925]
Epoch [70/120    avg_loss:0.135, val_acc:0.942]
Epoch [71/120    avg_loss:0.115, val_acc:0.932]
Epoch [72/120    avg_loss:0.104, val_acc:0.939]
Epoch [73/120    avg_loss:0.090, val_acc:0.946]
Epoch [74/120    avg_loss:0.100, val_acc:0.939]
Epoch [75/120    avg_loss:0.111, val_acc:0.929]
Epoch [76/120    avg_loss:0.131, val_acc:0.946]
Epoch [77/120    avg_loss:0.101, val_acc:0.950]
Epoch [78/120    avg_loss:0.103, val_acc:0.946]
Epoch [79/120    avg_loss:0.098, val_acc:0.948]
Epoch [80/120    avg_loss:0.094, val_acc:0.942]
Epoch [81/120    avg_loss:0.074, val_acc:0.955]
Epoch [82/120    avg_loss:0.076, val_acc:0.948]
Epoch [83/120    avg_loss:0.081, val_acc:0.955]
Epoch [84/120    avg_loss:0.065, val_acc:0.948]
Epoch [85/120    avg_loss:0.093, val_acc:0.948]
Epoch [86/120    avg_loss:0.071, val_acc:0.950]
Epoch [87/120    avg_loss:0.054, val_acc:0.953]
Epoch [88/120    avg_loss:0.054, val_acc:0.956]
Epoch [89/120    avg_loss:0.050, val_acc:0.954]
Epoch [90/120    avg_loss:0.057, val_acc:0.961]
Epoch [91/120    avg_loss:0.052, val_acc:0.963]
Epoch [92/120    avg_loss:0.046, val_acc:0.953]
Epoch [93/120    avg_loss:0.049, val_acc:0.959]
Epoch [94/120    avg_loss:0.057, val_acc:0.956]
Epoch [95/120    avg_loss:0.049, val_acc:0.961]
Epoch [96/120    avg_loss:0.056, val_acc:0.964]
Epoch [97/120    avg_loss:0.039, val_acc:0.959]
Epoch [98/120    avg_loss:0.045, val_acc:0.950]
Epoch [99/120    avg_loss:0.049, val_acc:0.969]
Epoch [100/120    avg_loss:0.042, val_acc:0.963]
Epoch [101/120    avg_loss:0.047, val_acc:0.962]
Epoch [102/120    avg_loss:0.047, val_acc:0.962]
Epoch [103/120    avg_loss:0.034, val_acc:0.964]
Epoch [104/120    avg_loss:0.033, val_acc:0.958]
Epoch [105/120    avg_loss:0.033, val_acc:0.970]
Epoch [106/120    avg_loss:0.037, val_acc:0.968]
Epoch [107/120    avg_loss:0.032, val_acc:0.968]
Epoch [108/120    avg_loss:0.028, val_acc:0.975]
Epoch [109/120    avg_loss:0.036, val_acc:0.962]
Epoch [110/120    avg_loss:0.033, val_acc:0.964]
Epoch [111/120    avg_loss:0.040, val_acc:0.964]
Epoch [112/120    avg_loss:0.201, val_acc:0.854]
Epoch [113/120    avg_loss:0.207, val_acc:0.914]
Epoch [114/120    avg_loss:0.166, val_acc:0.904]
Epoch [115/120    avg_loss:0.141, val_acc:0.932]
Epoch [116/120    avg_loss:0.105, val_acc:0.934]
Epoch [117/120    avg_loss:0.079, val_acc:0.935]
Epoch [118/120    avg_loss:0.059, val_acc:0.961]
Epoch [119/120    avg_loss:0.049, val_acc:0.942]
Epoch [120/120    avg_loss:0.066, val_acc:0.947]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1220    0    2    0    2    0    0    2   13   43    0    0
     3    0    0]
 [   0    0    2  731    1    0    0    0    0   11    0    1    0    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  388    0    5    0    9    6    2    0    0
    25    0    0]
 [   0    0    0    0    1    0  630    0    0    8    0    8    0    0
    10    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   17   90    0    0    0    0    0    0  727   38    0    0
     0    3    0]
 [   0    0   73    0    0    0    0    0    0    0   19 2101    3    0
    14    0    0]
 [   0    0    0   30   13    0    0    0    0    0   20    0  464    0
     2    0    5]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    0    4    0    0
  1130    0    0]
 [   0    0    1    0    0    0    7    0    0   16    0    0    0    0
   124  199    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
93.00813008130082

F1 scores:
[       nan 0.975      0.93918399 0.91375    0.96162528 0.93606755
 0.97222222 0.90909091 1.         0.4        0.87484958 0.9534831
 0.9261477  0.99728997 0.92320261 0.72495446 0.96511628]

Kappa:
0.9201874838441735
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1a34229fd0>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.811, val_acc:0.078]
Epoch [2/120    avg_loss:2.762, val_acc:0.137]
Epoch [3/120    avg_loss:2.718, val_acc:0.197]
Epoch [4/120    avg_loss:2.658, val_acc:0.260]
Epoch [5/120    avg_loss:2.604, val_acc:0.274]
Epoch [6/120    avg_loss:2.539, val_acc:0.358]
Epoch [7/120    avg_loss:2.468, val_acc:0.402]
Epoch [8/120    avg_loss:2.448, val_acc:0.384]
Epoch [9/120    avg_loss:2.375, val_acc:0.435]
Epoch [10/120    avg_loss:2.355, val_acc:0.469]
Epoch [11/120    avg_loss:2.325, val_acc:0.451]
Epoch [12/120    avg_loss:2.210, val_acc:0.494]
Epoch [13/120    avg_loss:2.162, val_acc:0.487]
Epoch [14/120    avg_loss:2.153, val_acc:0.466]
Epoch [15/120    avg_loss:2.084, val_acc:0.495]
Epoch [16/120    avg_loss:2.015, val_acc:0.516]
Epoch [17/120    avg_loss:1.999, val_acc:0.512]
Epoch [18/120    avg_loss:1.932, val_acc:0.527]
Epoch [19/120    avg_loss:1.825, val_acc:0.542]
Epoch [20/120    avg_loss:1.782, val_acc:0.546]
Epoch [21/120    avg_loss:1.707, val_acc:0.562]
Epoch [22/120    avg_loss:1.634, val_acc:0.558]
Epoch [23/120    avg_loss:1.543, val_acc:0.612]
Epoch [24/120    avg_loss:1.529, val_acc:0.579]
Epoch [25/120    avg_loss:1.434, val_acc:0.617]
Epoch [26/120    avg_loss:1.346, val_acc:0.655]
Epoch [27/120    avg_loss:1.215, val_acc:0.688]
Epoch [28/120    avg_loss:1.182, val_acc:0.671]
Epoch [29/120    avg_loss:1.087, val_acc:0.682]
Epoch [30/120    avg_loss:1.045, val_acc:0.683]
Epoch [31/120    avg_loss:0.991, val_acc:0.726]
Epoch [32/120    avg_loss:0.924, val_acc:0.747]
Epoch [33/120    avg_loss:0.838, val_acc:0.748]
Epoch [34/120    avg_loss:0.762, val_acc:0.791]
Epoch [35/120    avg_loss:0.745, val_acc:0.819]
Epoch [36/120    avg_loss:0.755, val_acc:0.794]
Epoch [37/120    avg_loss:0.724, val_acc:0.823]
Epoch [38/120    avg_loss:0.628, val_acc:0.797]
Epoch [39/120    avg_loss:0.585, val_acc:0.795]
Epoch [40/120    avg_loss:0.632, val_acc:0.812]
Epoch [41/120    avg_loss:0.575, val_acc:0.830]
Epoch [42/120    avg_loss:0.508, val_acc:0.808]
Epoch [43/120    avg_loss:0.482, val_acc:0.856]
Epoch [44/120    avg_loss:0.422, val_acc:0.860]
Epoch [45/120    avg_loss:0.388, val_acc:0.873]
Epoch [46/120    avg_loss:0.388, val_acc:0.866]
Epoch [47/120    avg_loss:0.377, val_acc:0.869]
Epoch [48/120    avg_loss:0.312, val_acc:0.890]
Epoch [49/120    avg_loss:0.326, val_acc:0.897]
Epoch [50/120    avg_loss:0.293, val_acc:0.896]
Epoch [51/120    avg_loss:0.282, val_acc:0.900]
Epoch [52/120    avg_loss:0.253, val_acc:0.885]
Epoch [53/120    avg_loss:0.257, val_acc:0.910]
Epoch [54/120    avg_loss:0.263, val_acc:0.909]
Epoch [55/120    avg_loss:0.237, val_acc:0.900]
Epoch [56/120    avg_loss:0.260, val_acc:0.918]
Epoch [57/120    avg_loss:0.238, val_acc:0.913]
Epoch [58/120    avg_loss:0.195, val_acc:0.926]
Epoch [59/120    avg_loss:0.212, val_acc:0.911]
Epoch [60/120    avg_loss:0.207, val_acc:0.921]
Epoch [61/120    avg_loss:0.159, val_acc:0.926]
Epoch [62/120    avg_loss:0.127, val_acc:0.942]
Epoch [63/120    avg_loss:0.116, val_acc:0.932]
Epoch [64/120    avg_loss:0.129, val_acc:0.940]
Epoch [65/120    avg_loss:0.136, val_acc:0.936]
Epoch [66/120    avg_loss:0.130, val_acc:0.933]
Epoch [67/120    avg_loss:0.115, val_acc:0.942]
Epoch [68/120    avg_loss:0.127, val_acc:0.938]
Epoch [69/120    avg_loss:0.101, val_acc:0.934]
Epoch [70/120    avg_loss:0.117, val_acc:0.946]
Epoch [71/120    avg_loss:0.098, val_acc:0.939]
Epoch [72/120    avg_loss:0.092, val_acc:0.940]
Epoch [73/120    avg_loss:0.089, val_acc:0.934]
Epoch [74/120    avg_loss:0.093, val_acc:0.939]
Epoch [75/120    avg_loss:0.090, val_acc:0.942]
Epoch [76/120    avg_loss:0.087, val_acc:0.942]
Epoch [77/120    avg_loss:0.091, val_acc:0.941]
Epoch [78/120    avg_loss:0.076, val_acc:0.948]
Epoch [79/120    avg_loss:0.070, val_acc:0.948]
Epoch [80/120    avg_loss:0.066, val_acc:0.953]
Epoch [81/120    avg_loss:0.112, val_acc:0.942]
Epoch [82/120    avg_loss:0.118, val_acc:0.946]
Epoch [83/120    avg_loss:0.088, val_acc:0.940]
Epoch [84/120    avg_loss:0.100, val_acc:0.950]
Epoch [85/120    avg_loss:0.072, val_acc:0.947]
Epoch [86/120    avg_loss:0.061, val_acc:0.954]
Epoch [87/120    avg_loss:0.063, val_acc:0.953]
Epoch [88/120    avg_loss:0.068, val_acc:0.950]
Epoch [89/120    avg_loss:0.063, val_acc:0.954]
Epoch [90/120    avg_loss:0.049, val_acc:0.949]
Epoch [91/120    avg_loss:0.054, val_acc:0.949]
Epoch [92/120    avg_loss:0.060, val_acc:0.955]
Epoch [93/120    avg_loss:0.059, val_acc:0.957]
Epoch [94/120    avg_loss:0.057, val_acc:0.948]
Epoch [95/120    avg_loss:0.049, val_acc:0.958]
Epoch [96/120    avg_loss:0.049, val_acc:0.956]
Epoch [97/120    avg_loss:0.043, val_acc:0.958]
Epoch [98/120    avg_loss:0.042, val_acc:0.959]
Epoch [99/120    avg_loss:0.050, val_acc:0.958]
Epoch [100/120    avg_loss:0.045, val_acc:0.956]
Epoch [101/120    avg_loss:0.110, val_acc:0.946]
Epoch [102/120    avg_loss:0.082, val_acc:0.947]
Epoch [103/120    avg_loss:0.088, val_acc:0.946]
Epoch [104/120    avg_loss:0.090, val_acc:0.953]
Epoch [105/120    avg_loss:0.067, val_acc:0.946]
Epoch [106/120    avg_loss:0.078, val_acc:0.949]
Epoch [107/120    avg_loss:0.055, val_acc:0.948]
Epoch [108/120    avg_loss:0.043, val_acc:0.963]
Epoch [109/120    avg_loss:0.049, val_acc:0.958]
Epoch [110/120    avg_loss:0.041, val_acc:0.955]
Epoch [111/120    avg_loss:0.044, val_acc:0.956]
Epoch [112/120    avg_loss:0.055, val_acc:0.957]
Epoch [113/120    avg_loss:0.035, val_acc:0.964]
Epoch [114/120    avg_loss:0.032, val_acc:0.954]
Epoch [115/120    avg_loss:0.034, val_acc:0.946]
Epoch [116/120    avg_loss:0.038, val_acc:0.956]
Epoch [117/120    avg_loss:0.035, val_acc:0.959]
Epoch [118/120    avg_loss:0.030, val_acc:0.956]
Epoch [119/120    avg_loss:0.028, val_acc:0.963]
Epoch [120/120    avg_loss:0.034, val_acc:0.963]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1226    0    0    0    0    0    0    0    0   58    1    0
     0    0    0]
 [   0    0    2  704    2   11    0    0    0    4    0    0   20    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    3    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    6    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   50   84    0    3    2    0    0    0  709   19    6    0
     0    2    0]
 [   0    0   34    0    0    0    1    0    0    0    1 2157   15    2
     0    0    0]
 [   0    0    0   12    2    8    0    0    0    0    5    0  505    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    1    1    2    1
  1130    0    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0   32    0
     2  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.43631436314364

F1 scores:
[       nan 0.975      0.94343978 0.91014867 0.99069767 0.96404494
 0.98412698 0.94339623 1.         0.80952381 0.89126336 0.9692204
 0.9042077  0.98143236 0.99515632 0.92615385 0.98224852]

Kappa:
0.9479516331364918
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff24fa4bef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.809, val_acc:0.057]
Epoch [2/120    avg_loss:2.762, val_acc:0.166]
Epoch [3/120    avg_loss:2.727, val_acc:0.190]
Epoch [4/120    avg_loss:2.698, val_acc:0.203]
Epoch [5/120    avg_loss:2.624, val_acc:0.219]
Epoch [6/120    avg_loss:2.565, val_acc:0.257]
Epoch [7/120    avg_loss:2.514, val_acc:0.266]
Epoch [8/120    avg_loss:2.453, val_acc:0.282]
Epoch [9/120    avg_loss:2.402, val_acc:0.317]
Epoch [10/120    avg_loss:2.374, val_acc:0.335]
Epoch [11/120    avg_loss:2.318, val_acc:0.320]
Epoch [12/120    avg_loss:2.269, val_acc:0.374]
Epoch [13/120    avg_loss:2.222, val_acc:0.333]
Epoch [14/120    avg_loss:2.125, val_acc:0.382]
Epoch [15/120    avg_loss:2.067, val_acc:0.434]
Epoch [16/120    avg_loss:1.995, val_acc:0.469]
Epoch [17/120    avg_loss:1.900, val_acc:0.490]
Epoch [18/120    avg_loss:1.807, val_acc:0.468]
Epoch [19/120    avg_loss:1.726, val_acc:0.541]
Epoch [20/120    avg_loss:1.598, val_acc:0.469]
Epoch [21/120    avg_loss:1.535, val_acc:0.548]
Epoch [22/120    avg_loss:1.481, val_acc:0.527]
Epoch [23/120    avg_loss:1.410, val_acc:0.624]
Epoch [24/120    avg_loss:1.328, val_acc:0.671]
Epoch [25/120    avg_loss:1.220, val_acc:0.669]
Epoch [26/120    avg_loss:1.148, val_acc:0.682]
Epoch [27/120    avg_loss:1.077, val_acc:0.702]
Epoch [28/120    avg_loss:1.023, val_acc:0.733]
Epoch [29/120    avg_loss:0.980, val_acc:0.771]
Epoch [30/120    avg_loss:0.905, val_acc:0.767]
Epoch [31/120    avg_loss:0.841, val_acc:0.799]
Epoch [32/120    avg_loss:0.758, val_acc:0.810]
Epoch [33/120    avg_loss:0.754, val_acc:0.789]
Epoch [34/120    avg_loss:0.725, val_acc:0.826]
Epoch [35/120    avg_loss:0.687, val_acc:0.833]
Epoch [36/120    avg_loss:0.616, val_acc:0.848]
Epoch [37/120    avg_loss:0.565, val_acc:0.844]
Epoch [38/120    avg_loss:0.558, val_acc:0.850]
Epoch [39/120    avg_loss:0.503, val_acc:0.866]
Epoch [40/120    avg_loss:0.456, val_acc:0.891]
Epoch [41/120    avg_loss:0.382, val_acc:0.886]
Epoch [42/120    avg_loss:0.391, val_acc:0.887]
Epoch [43/120    avg_loss:0.369, val_acc:0.905]
Epoch [44/120    avg_loss:0.343, val_acc:0.902]
Epoch [45/120    avg_loss:0.323, val_acc:0.919]
Epoch [46/120    avg_loss:0.272, val_acc:0.904]
Epoch [47/120    avg_loss:0.306, val_acc:0.911]
Epoch [48/120    avg_loss:0.293, val_acc:0.916]
Epoch [49/120    avg_loss:0.281, val_acc:0.911]
Epoch [50/120    avg_loss:0.241, val_acc:0.914]
Epoch [51/120    avg_loss:0.219, val_acc:0.935]
Epoch [52/120    avg_loss:0.207, val_acc:0.933]
Epoch [53/120    avg_loss:0.168, val_acc:0.945]
Epoch [54/120    avg_loss:0.148, val_acc:0.941]
Epoch [55/120    avg_loss:0.151, val_acc:0.951]
Epoch [56/120    avg_loss:0.153, val_acc:0.951]
Epoch [57/120    avg_loss:0.142, val_acc:0.955]
Epoch [58/120    avg_loss:0.156, val_acc:0.955]
Epoch [59/120    avg_loss:0.139, val_acc:0.953]
Epoch [60/120    avg_loss:0.116, val_acc:0.968]
Epoch [61/120    avg_loss:0.122, val_acc:0.960]
Epoch [62/120    avg_loss:0.135, val_acc:0.956]
Epoch [63/120    avg_loss:0.102, val_acc:0.975]
Epoch [64/120    avg_loss:0.114, val_acc:0.963]
Epoch [65/120    avg_loss:0.112, val_acc:0.964]
Epoch [66/120    avg_loss:0.097, val_acc:0.963]
Epoch [67/120    avg_loss:0.089, val_acc:0.961]
Epoch [68/120    avg_loss:0.108, val_acc:0.955]
Epoch [69/120    avg_loss:0.119, val_acc:0.965]
Epoch [70/120    avg_loss:0.108, val_acc:0.963]
Epoch [71/120    avg_loss:0.102, val_acc:0.967]
Epoch [72/120    avg_loss:0.081, val_acc:0.960]
Epoch [73/120    avg_loss:0.068, val_acc:0.977]
Epoch [74/120    avg_loss:0.057, val_acc:0.982]
Epoch [75/120    avg_loss:0.072, val_acc:0.960]
Epoch [76/120    avg_loss:0.081, val_acc:0.969]
Epoch [77/120    avg_loss:0.075, val_acc:0.972]
Epoch [78/120    avg_loss:0.061, val_acc:0.975]
Epoch [79/120    avg_loss:0.057, val_acc:0.980]
Epoch [80/120    avg_loss:0.043, val_acc:0.980]
Epoch [81/120    avg_loss:0.045, val_acc:0.978]
Epoch [82/120    avg_loss:0.052, val_acc:0.975]
Epoch [83/120    avg_loss:0.066, val_acc:0.978]
Epoch [84/120    avg_loss:0.046, val_acc:0.982]
Epoch [85/120    avg_loss:0.059, val_acc:0.977]
Epoch [86/120    avg_loss:0.078, val_acc:0.972]
Epoch [87/120    avg_loss:0.055, val_acc:0.973]
Epoch [88/120    avg_loss:0.046, val_acc:0.980]
Epoch [89/120    avg_loss:0.045, val_acc:0.982]
Epoch [90/120    avg_loss:0.041, val_acc:0.968]
Epoch [91/120    avg_loss:0.052, val_acc:0.980]
Epoch [92/120    avg_loss:0.043, val_acc:0.965]
Epoch [93/120    avg_loss:0.038, val_acc:0.973]
Epoch [94/120    avg_loss:0.034, val_acc:0.979]
Epoch [95/120    avg_loss:0.041, val_acc:0.974]
Epoch [96/120    avg_loss:0.046, val_acc:0.981]
Epoch [97/120    avg_loss:0.031, val_acc:0.984]
Epoch [98/120    avg_loss:0.028, val_acc:0.985]
Epoch [99/120    avg_loss:0.028, val_acc:0.980]
Epoch [100/120    avg_loss:0.032, val_acc:0.984]
Epoch [101/120    avg_loss:0.028, val_acc:0.984]
Epoch [102/120    avg_loss:0.028, val_acc:0.985]
Epoch [103/120    avg_loss:0.039, val_acc:0.969]
Epoch [104/120    avg_loss:0.040, val_acc:0.978]
Epoch [105/120    avg_loss:0.035, val_acc:0.982]
Epoch [106/120    avg_loss:0.046, val_acc:0.984]
Epoch [107/120    avg_loss:0.032, val_acc:0.980]
Epoch [108/120    avg_loss:0.029, val_acc:0.984]
Epoch [109/120    avg_loss:0.024, val_acc:0.980]
Epoch [110/120    avg_loss:0.021, val_acc:0.984]
Epoch [111/120    avg_loss:0.024, val_acc:0.985]
Epoch [112/120    avg_loss:0.019, val_acc:0.983]
Epoch [113/120    avg_loss:0.021, val_acc:0.981]
Epoch [114/120    avg_loss:0.023, val_acc:0.984]
Epoch [115/120    avg_loss:0.020, val_acc:0.989]
Epoch [116/120    avg_loss:0.026, val_acc:0.985]
Epoch [117/120    avg_loss:0.027, val_acc:0.983]
Epoch [118/120    avg_loss:0.018, val_acc:0.987]
Epoch [119/120    avg_loss:0.018, val_acc:0.987]
Epoch [120/120    avg_loss:0.017, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1234    0    0    0    6    0    0    0   19   18    7    0
     0    1    0]
 [   0    0    5  699    3   16    0    0    0    9    0    0   13    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   14    0    0    2    0
     0    0    0]
 [   0    0   15   83    0    6    3    0    0    0  750   14    4    0
     0    0    0]
 [   0    0    6    0    0    0   10    0    0    0    0 2185    9    0
     0    0    0]
 [   0    0    0   14    8    5    0    0    0    0    9   10  486    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    1    0    0    0    0    0
  1128    0    0]
 [   0    0    0    0    0    3   12    0    0    1    0    0    0    0
    57  274    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.73983739837398

F1 scores:
[       nan 0.975      0.9697446  0.90485437 0.97482838 0.95259096
 0.97619048 1.         0.99883856 0.62222222 0.90634441 0.98467778
 0.91958373 0.99462366 0.9707401  0.88102894 0.97619048]

Kappa:
0.9514151835014316
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0f72268eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.816, val_acc:0.151]
Epoch [2/120    avg_loss:2.781, val_acc:0.312]
Epoch [3/120    avg_loss:2.747, val_acc:0.368]
Epoch [4/120    avg_loss:2.703, val_acc:0.379]
Epoch [5/120    avg_loss:2.653, val_acc:0.403]
Epoch [6/120    avg_loss:2.603, val_acc:0.391]
Epoch [7/120    avg_loss:2.549, val_acc:0.393]
Epoch [8/120    avg_loss:2.504, val_acc:0.372]
Epoch [9/120    avg_loss:2.446, val_acc:0.450]
Epoch [10/120    avg_loss:2.369, val_acc:0.446]
Epoch [11/120    avg_loss:2.367, val_acc:0.400]
Epoch [12/120    avg_loss:2.312, val_acc:0.491]
Epoch [13/120    avg_loss:2.247, val_acc:0.480]
Epoch [14/120    avg_loss:2.217, val_acc:0.445]
Epoch [15/120    avg_loss:2.202, val_acc:0.488]
Epoch [16/120    avg_loss:2.165, val_acc:0.523]
Epoch [17/120    avg_loss:2.097, val_acc:0.529]
Epoch [18/120    avg_loss:2.074, val_acc:0.479]
Epoch [19/120    avg_loss:2.032, val_acc:0.574]
Epoch [20/120    avg_loss:2.014, val_acc:0.599]
Epoch [21/120    avg_loss:1.941, val_acc:0.631]
Epoch [22/120    avg_loss:1.907, val_acc:0.618]
Epoch [23/120    avg_loss:1.818, val_acc:0.645]
Epoch [24/120    avg_loss:1.788, val_acc:0.665]
Epoch [25/120    avg_loss:1.675, val_acc:0.671]
Epoch [26/120    avg_loss:1.649, val_acc:0.675]
Epoch [27/120    avg_loss:1.546, val_acc:0.691]
Epoch [28/120    avg_loss:1.453, val_acc:0.679]
Epoch [29/120    avg_loss:1.411, val_acc:0.675]
Epoch [30/120    avg_loss:1.262, val_acc:0.712]
Epoch [31/120    avg_loss:1.216, val_acc:0.698]
Epoch [32/120    avg_loss:1.177, val_acc:0.718]
Epoch [33/120    avg_loss:1.045, val_acc:0.724]
Epoch [34/120    avg_loss:0.994, val_acc:0.749]
Epoch [35/120    avg_loss:0.964, val_acc:0.751]
Epoch [36/120    avg_loss:0.905, val_acc:0.769]
Epoch [37/120    avg_loss:0.853, val_acc:0.772]
Epoch [38/120    avg_loss:0.804, val_acc:0.754]
Epoch [39/120    avg_loss:0.728, val_acc:0.798]
Epoch [40/120    avg_loss:0.709, val_acc:0.779]
Epoch [41/120    avg_loss:0.731, val_acc:0.798]
Epoch [42/120    avg_loss:0.683, val_acc:0.796]
Epoch [43/120    avg_loss:0.599, val_acc:0.801]
Epoch [44/120    avg_loss:0.534, val_acc:0.814]
Epoch [45/120    avg_loss:0.508, val_acc:0.835]
Epoch [46/120    avg_loss:0.519, val_acc:0.848]
Epoch [47/120    avg_loss:0.450, val_acc:0.859]
Epoch [48/120    avg_loss:0.430, val_acc:0.868]
Epoch [49/120    avg_loss:0.401, val_acc:0.859]
Epoch [50/120    avg_loss:0.347, val_acc:0.875]
Epoch [51/120    avg_loss:0.324, val_acc:0.912]
Epoch [52/120    avg_loss:0.305, val_acc:0.910]
Epoch [53/120    avg_loss:0.280, val_acc:0.881]
Epoch [54/120    avg_loss:0.277, val_acc:0.900]
Epoch [55/120    avg_loss:0.269, val_acc:0.912]
Epoch [56/120    avg_loss:0.248, val_acc:0.912]
Epoch [57/120    avg_loss:0.267, val_acc:0.891]
Epoch [58/120    avg_loss:0.254, val_acc:0.895]
Epoch [59/120    avg_loss:0.199, val_acc:0.920]
Epoch [60/120    avg_loss:0.196, val_acc:0.910]
Epoch [61/120    avg_loss:0.179, val_acc:0.921]
Epoch [62/120    avg_loss:0.157, val_acc:0.935]
Epoch [63/120    avg_loss:0.162, val_acc:0.921]
Epoch [64/120    avg_loss:0.182, val_acc:0.922]
Epoch [65/120    avg_loss:0.163, val_acc:0.934]
Epoch [66/120    avg_loss:0.137, val_acc:0.926]
Epoch [67/120    avg_loss:0.137, val_acc:0.916]
Epoch [68/120    avg_loss:0.150, val_acc:0.942]
Epoch [69/120    avg_loss:0.134, val_acc:0.948]
Epoch [70/120    avg_loss:0.145, val_acc:0.927]
Epoch [71/120    avg_loss:0.137, val_acc:0.943]
Epoch [72/120    avg_loss:0.116, val_acc:0.935]
Epoch [73/120    avg_loss:0.110, val_acc:0.936]
Epoch [74/120    avg_loss:0.100, val_acc:0.953]
Epoch [75/120    avg_loss:0.102, val_acc:0.938]
Epoch [76/120    avg_loss:0.126, val_acc:0.927]
Epoch [77/120    avg_loss:0.132, val_acc:0.934]
Epoch [78/120    avg_loss:0.116, val_acc:0.939]
Epoch [79/120    avg_loss:0.100, val_acc:0.934]
Epoch [80/120    avg_loss:0.124, val_acc:0.946]
Epoch [81/120    avg_loss:0.087, val_acc:0.943]
Epoch [82/120    avg_loss:0.081, val_acc:0.953]
Epoch [83/120    avg_loss:0.086, val_acc:0.945]
Epoch [84/120    avg_loss:0.076, val_acc:0.936]
Epoch [85/120    avg_loss:0.081, val_acc:0.946]
Epoch [86/120    avg_loss:0.082, val_acc:0.959]
Epoch [87/120    avg_loss:0.096, val_acc:0.955]
Epoch [88/120    avg_loss:0.075, val_acc:0.950]
Epoch [89/120    avg_loss:0.085, val_acc:0.949]
Epoch [90/120    avg_loss:0.074, val_acc:0.951]
Epoch [91/120    avg_loss:0.100, val_acc:0.955]
Epoch [92/120    avg_loss:0.063, val_acc:0.945]
Epoch [93/120    avg_loss:0.054, val_acc:0.956]
Epoch [94/120    avg_loss:0.064, val_acc:0.956]
Epoch [95/120    avg_loss:0.062, val_acc:0.960]
Epoch [96/120    avg_loss:0.088, val_acc:0.949]
Epoch [97/120    avg_loss:0.073, val_acc:0.945]
Epoch [98/120    avg_loss:0.054, val_acc:0.965]
Epoch [99/120    avg_loss:0.054, val_acc:0.958]
Epoch [100/120    avg_loss:0.050, val_acc:0.953]
Epoch [101/120    avg_loss:0.071, val_acc:0.944]
Epoch [102/120    avg_loss:0.066, val_acc:0.958]
Epoch [103/120    avg_loss:0.055, val_acc:0.961]
Epoch [104/120    avg_loss:0.048, val_acc:0.953]
Epoch [105/120    avg_loss:0.038, val_acc:0.964]
Epoch [106/120    avg_loss:0.041, val_acc:0.961]
Epoch [107/120    avg_loss:0.047, val_acc:0.969]
Epoch [108/120    avg_loss:0.041, val_acc:0.973]
Epoch [109/120    avg_loss:0.038, val_acc:0.967]
Epoch [110/120    avg_loss:0.034, val_acc:0.972]
Epoch [111/120    avg_loss:0.033, val_acc:0.970]
Epoch [112/120    avg_loss:0.036, val_acc:0.973]
Epoch [113/120    avg_loss:0.051, val_acc:0.953]
Epoch [114/120    avg_loss:0.075, val_acc:0.949]
Epoch [115/120    avg_loss:0.057, val_acc:0.968]
Epoch [116/120    avg_loss:0.048, val_acc:0.967]
Epoch [117/120    avg_loss:0.054, val_acc:0.955]
Epoch [118/120    avg_loss:0.051, val_acc:0.949]
Epoch [119/120    avg_loss:0.045, val_acc:0.956]
Epoch [120/120    avg_loss:0.045, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1210    1    0    0    3    0    0    0   12   44    3    0
     0   12    0]
 [   0    0    1  705    3   13    0    0    0   12    0    0   13    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  417    0    4    0    4    0    0    0    0
    10    0    0]
 [   0    0    2    0    0    0  654    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    1    0    0   14    0    0    1    0
     0    0    0]
 [   0    0   33   90    0    0    0    0    0    0  745    1    4    0
     0    2    0]
 [   0    0   41    0    0    0    0    0    0    1   21 2143    1    3
     0    0    0]
 [   0    0    2   13    8    2    0    0    0    0    6    0  500    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   33    0    0    2    0    0    1    0
   117  194    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
94.18970189701896

F1 scores:
[       nan 0.975      0.94017094 0.90500642 0.97482838 0.96193772
 0.97032641 0.92592593 0.99883856 0.54901961 0.89651023 0.97431234
 0.94073377 0.9919571  0.9463171  0.6990991  0.94545455]

Kappa:
0.9337170050708515
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efea3ee3f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.813, val_acc:0.211]
Epoch [2/120    avg_loss:2.768, val_acc:0.214]
Epoch [3/120    avg_loss:2.720, val_acc:0.216]
Epoch [4/120    avg_loss:2.677, val_acc:0.278]
Epoch [5/120    avg_loss:2.626, val_acc:0.302]
Epoch [6/120    avg_loss:2.543, val_acc:0.345]
Epoch [7/120    avg_loss:2.534, val_acc:0.392]
Epoch [8/120    avg_loss:2.489, val_acc:0.470]
Epoch [9/120    avg_loss:2.434, val_acc:0.500]
Epoch [10/120    avg_loss:2.393, val_acc:0.506]
Epoch [11/120    avg_loss:2.342, val_acc:0.503]
Epoch [12/120    avg_loss:2.303, val_acc:0.535]
Epoch [13/120    avg_loss:2.251, val_acc:0.500]
Epoch [14/120    avg_loss:2.227, val_acc:0.551]
Epoch [15/120    avg_loss:2.160, val_acc:0.519]
Epoch [16/120    avg_loss:2.102, val_acc:0.576]
Epoch [17/120    avg_loss:2.063, val_acc:0.575]
Epoch [18/120    avg_loss:2.013, val_acc:0.606]
Epoch [19/120    avg_loss:1.915, val_acc:0.627]
Epoch [20/120    avg_loss:1.881, val_acc:0.653]
Epoch [21/120    avg_loss:1.761, val_acc:0.637]
Epoch [22/120    avg_loss:1.701, val_acc:0.646]
Epoch [23/120    avg_loss:1.668, val_acc:0.668]
Epoch [24/120    avg_loss:1.501, val_acc:0.669]
Epoch [25/120    avg_loss:1.480, val_acc:0.686]
Epoch [26/120    avg_loss:1.367, val_acc:0.712]
Epoch [27/120    avg_loss:1.311, val_acc:0.700]
Epoch [28/120    avg_loss:1.258, val_acc:0.720]
Epoch [29/120    avg_loss:1.183, val_acc:0.730]
Epoch [30/120    avg_loss:1.120, val_acc:0.737]
Epoch [31/120    avg_loss:1.064, val_acc:0.750]
Epoch [32/120    avg_loss:0.966, val_acc:0.770]
Epoch [33/120    avg_loss:0.877, val_acc:0.789]
Epoch [34/120    avg_loss:0.859, val_acc:0.793]
Epoch [35/120    avg_loss:0.778, val_acc:0.823]
Epoch [36/120    avg_loss:0.732, val_acc:0.806]
Epoch [37/120    avg_loss:0.757, val_acc:0.844]
Epoch [38/120    avg_loss:0.623, val_acc:0.867]
Epoch [39/120    avg_loss:0.625, val_acc:0.830]
Epoch [40/120    avg_loss:0.576, val_acc:0.860]
Epoch [41/120    avg_loss:0.524, val_acc:0.874]
Epoch [42/120    avg_loss:0.505, val_acc:0.841]
Epoch [43/120    avg_loss:0.459, val_acc:0.868]
Epoch [44/120    avg_loss:0.444, val_acc:0.870]
Epoch [45/120    avg_loss:0.438, val_acc:0.913]
Epoch [46/120    avg_loss:0.382, val_acc:0.878]
Epoch [47/120    avg_loss:0.349, val_acc:0.917]
Epoch [48/120    avg_loss:0.308, val_acc:0.910]
Epoch [49/120    avg_loss:0.314, val_acc:0.906]
Epoch [50/120    avg_loss:0.293, val_acc:0.904]
Epoch [51/120    avg_loss:0.326, val_acc:0.891]
Epoch [52/120    avg_loss:0.276, val_acc:0.912]
Epoch [53/120    avg_loss:0.262, val_acc:0.925]
Epoch [54/120    avg_loss:0.270, val_acc:0.918]
Epoch [55/120    avg_loss:0.233, val_acc:0.922]
Epoch [56/120    avg_loss:0.256, val_acc:0.892]
Epoch [57/120    avg_loss:0.222, val_acc:0.936]
Epoch [58/120    avg_loss:0.230, val_acc:0.906]
Epoch [59/120    avg_loss:0.210, val_acc:0.924]
Epoch [60/120    avg_loss:0.171, val_acc:0.939]
Epoch [61/120    avg_loss:0.152, val_acc:0.943]
Epoch [62/120    avg_loss:0.156, val_acc:0.925]
Epoch [63/120    avg_loss:0.186, val_acc:0.942]
Epoch [64/120    avg_loss:0.143, val_acc:0.939]
Epoch [65/120    avg_loss:0.140, val_acc:0.939]
Epoch [66/120    avg_loss:0.139, val_acc:0.935]
Epoch [67/120    avg_loss:0.111, val_acc:0.946]
Epoch [68/120    avg_loss:0.105, val_acc:0.963]
Epoch [69/120    avg_loss:0.103, val_acc:0.955]
Epoch [70/120    avg_loss:0.114, val_acc:0.958]
Epoch [71/120    avg_loss:0.111, val_acc:0.948]
Epoch [72/120    avg_loss:0.098, val_acc:0.954]
Epoch [73/120    avg_loss:0.094, val_acc:0.956]
Epoch [74/120    avg_loss:0.083, val_acc:0.959]
Epoch [75/120    avg_loss:0.086, val_acc:0.958]
Epoch [76/120    avg_loss:0.085, val_acc:0.958]
Epoch [77/120    avg_loss:0.088, val_acc:0.955]
Epoch [78/120    avg_loss:0.117, val_acc:0.947]
Epoch [79/120    avg_loss:0.097, val_acc:0.958]
Epoch [80/120    avg_loss:0.095, val_acc:0.957]
Epoch [81/120    avg_loss:0.075, val_acc:0.955]
Epoch [82/120    avg_loss:0.067, val_acc:0.962]
Epoch [83/120    avg_loss:0.062, val_acc:0.964]
Epoch [84/120    avg_loss:0.053, val_acc:0.963]
Epoch [85/120    avg_loss:0.056, val_acc:0.964]
Epoch [86/120    avg_loss:0.057, val_acc:0.962]
Epoch [87/120    avg_loss:0.054, val_acc:0.964]
Epoch [88/120    avg_loss:0.048, val_acc:0.965]
Epoch [89/120    avg_loss:0.050, val_acc:0.964]
Epoch [90/120    avg_loss:0.050, val_acc:0.964]
Epoch [91/120    avg_loss:0.049, val_acc:0.964]
Epoch [92/120    avg_loss:0.050, val_acc:0.964]
Epoch [93/120    avg_loss:0.051, val_acc:0.963]
Epoch [94/120    avg_loss:0.048, val_acc:0.963]
Epoch [95/120    avg_loss:0.045, val_acc:0.963]
Epoch [96/120    avg_loss:0.044, val_acc:0.962]
Epoch [97/120    avg_loss:0.051, val_acc:0.965]
Epoch [98/120    avg_loss:0.045, val_acc:0.965]
Epoch [99/120    avg_loss:0.049, val_acc:0.969]
Epoch [100/120    avg_loss:0.051, val_acc:0.966]
Epoch [101/120    avg_loss:0.046, val_acc:0.965]
Epoch [102/120    avg_loss:0.050, val_acc:0.968]
Epoch [103/120    avg_loss:0.047, val_acc:0.966]
Epoch [104/120    avg_loss:0.047, val_acc:0.965]
Epoch [105/120    avg_loss:0.048, val_acc:0.966]
Epoch [106/120    avg_loss:0.043, val_acc:0.968]
Epoch [107/120    avg_loss:0.042, val_acc:0.968]
Epoch [108/120    avg_loss:0.045, val_acc:0.965]
Epoch [109/120    avg_loss:0.040, val_acc:0.966]
Epoch [110/120    avg_loss:0.050, val_acc:0.968]
Epoch [111/120    avg_loss:0.040, val_acc:0.966]
Epoch [112/120    avg_loss:0.040, val_acc:0.968]
Epoch [113/120    avg_loss:0.050, val_acc:0.968]
Epoch [114/120    avg_loss:0.045, val_acc:0.968]
Epoch [115/120    avg_loss:0.042, val_acc:0.968]
Epoch [116/120    avg_loss:0.042, val_acc:0.968]
Epoch [117/120    avg_loss:0.037, val_acc:0.966]
Epoch [118/120    avg_loss:0.043, val_acc:0.966]
Epoch [119/120    avg_loss:0.040, val_acc:0.968]
Epoch [120/120    avg_loss:0.044, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1191    3    0    0   10    0    0    0    5   69    5    0
     0    2    0]
 [   0    0    0  727    2    8    0    0    0    8    0    0    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  652    0    0    1    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0   10    0    0    8    0    0    0    0
     0    0    0]
 [   0    0   31   90    0    0    0    0    0    0  743    1    2    0
     1    7    0]
 [   0    0   24    5    0    4   12    0    4    0    5 2145    9    2
     0    0    0]
 [   0    0    0   23   14    5    0    0    0    0   12    0  475    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    2    0    0
  1134    0    0]
 [   0    0    0    0    0    1   49    0    0    0    0    0    1    0
   104  192    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
93.98373983739837

F1 scores:
[       nan 0.975      0.94112999 0.91159875 0.9638009  0.97742664
 0.9381295  1.         0.99303944 0.43243243 0.90334347 0.96839729
 0.91876209 0.99462366 0.95334174 0.70072993 0.94674556]

Kappa:
0.9313509295029506
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f47ed209ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.822, val_acc:0.134]
Epoch [2/120    avg_loss:2.793, val_acc:0.191]
Epoch [3/120    avg_loss:2.764, val_acc:0.249]
Epoch [4/120    avg_loss:2.731, val_acc:0.251]
Epoch [5/120    avg_loss:2.698, val_acc:0.253]
Epoch [6/120    avg_loss:2.657, val_acc:0.254]
Epoch [7/120    avg_loss:2.631, val_acc:0.262]
Epoch [8/120    avg_loss:2.563, val_acc:0.266]
Epoch [9/120    avg_loss:2.500, val_acc:0.277]
Epoch [10/120    avg_loss:2.458, val_acc:0.301]
Epoch [11/120    avg_loss:2.375, val_acc:0.404]
Epoch [12/120    avg_loss:2.354, val_acc:0.415]
Epoch [13/120    avg_loss:2.282, val_acc:0.526]
Epoch [14/120    avg_loss:2.232, val_acc:0.529]
Epoch [15/120    avg_loss:2.144, val_acc:0.573]
Epoch [16/120    avg_loss:2.130, val_acc:0.519]
Epoch [17/120    avg_loss:2.072, val_acc:0.548]
Epoch [18/120    avg_loss:1.958, val_acc:0.578]
Epoch [19/120    avg_loss:1.950, val_acc:0.577]
Epoch [20/120    avg_loss:1.928, val_acc:0.586]
Epoch [21/120    avg_loss:1.820, val_acc:0.608]
Epoch [22/120    avg_loss:1.749, val_acc:0.581]
Epoch [23/120    avg_loss:1.726, val_acc:0.613]
Epoch [24/120    avg_loss:1.601, val_acc:0.645]
Epoch [25/120    avg_loss:1.523, val_acc:0.654]
Epoch [26/120    avg_loss:1.422, val_acc:0.682]
Epoch [27/120    avg_loss:1.406, val_acc:0.681]
Epoch [28/120    avg_loss:1.306, val_acc:0.706]
Epoch [29/120    avg_loss:1.207, val_acc:0.727]
Epoch [30/120    avg_loss:1.159, val_acc:0.733]
Epoch [31/120    avg_loss:1.091, val_acc:0.753]
Epoch [32/120    avg_loss:1.024, val_acc:0.751]
Epoch [33/120    avg_loss:0.930, val_acc:0.772]
Epoch [34/120    avg_loss:0.891, val_acc:0.715]
Epoch [35/120    avg_loss:0.920, val_acc:0.777]
Epoch [36/120    avg_loss:0.773, val_acc:0.796]
Epoch [37/120    avg_loss:0.762, val_acc:0.787]
Epoch [38/120    avg_loss:0.683, val_acc:0.807]
Epoch [39/120    avg_loss:0.616, val_acc:0.812]
Epoch [40/120    avg_loss:0.572, val_acc:0.835]
Epoch [41/120    avg_loss:0.553, val_acc:0.835]
Epoch [42/120    avg_loss:0.576, val_acc:0.840]
Epoch [43/120    avg_loss:0.494, val_acc:0.826]
Epoch [44/120    avg_loss:0.484, val_acc:0.839]
Epoch [45/120    avg_loss:0.429, val_acc:0.863]
Epoch [46/120    avg_loss:0.392, val_acc:0.877]
Epoch [47/120    avg_loss:0.358, val_acc:0.892]
Epoch [48/120    avg_loss:0.343, val_acc:0.897]
Epoch [49/120    avg_loss:0.317, val_acc:0.886]
Epoch [50/120    avg_loss:0.272, val_acc:0.887]
Epoch [51/120    avg_loss:0.267, val_acc:0.891]
Epoch [52/120    avg_loss:0.247, val_acc:0.905]
Epoch [53/120    avg_loss:0.246, val_acc:0.904]
Epoch [54/120    avg_loss:0.295, val_acc:0.897]
Epoch [55/120    avg_loss:0.248, val_acc:0.906]
Epoch [56/120    avg_loss:0.218, val_acc:0.920]
Epoch [57/120    avg_loss:0.205, val_acc:0.910]
Epoch [58/120    avg_loss:0.221, val_acc:0.903]
Epoch [59/120    avg_loss:0.183, val_acc:0.914]
Epoch [60/120    avg_loss:0.168, val_acc:0.919]
Epoch [61/120    avg_loss:0.160, val_acc:0.915]
Epoch [62/120    avg_loss:0.200, val_acc:0.914]
Epoch [63/120    avg_loss:0.193, val_acc:0.896]
Epoch [64/120    avg_loss:0.200, val_acc:0.936]
Epoch [65/120    avg_loss:0.150, val_acc:0.925]
Epoch [66/120    avg_loss:0.169, val_acc:0.901]
Epoch [67/120    avg_loss:0.138, val_acc:0.943]
Epoch [68/120    avg_loss:0.115, val_acc:0.926]
Epoch [69/120    avg_loss:0.111, val_acc:0.943]
Epoch [70/120    avg_loss:0.096, val_acc:0.948]
Epoch [71/120    avg_loss:0.111, val_acc:0.930]
Epoch [72/120    avg_loss:0.108, val_acc:0.951]
Epoch [73/120    avg_loss:0.086, val_acc:0.954]
Epoch [74/120    avg_loss:0.115, val_acc:0.934]
Epoch [75/120    avg_loss:0.105, val_acc:0.934]
Epoch [76/120    avg_loss:0.091, val_acc:0.951]
Epoch [77/120    avg_loss:0.091, val_acc:0.953]
Epoch [78/120    avg_loss:0.118, val_acc:0.927]
Epoch [79/120    avg_loss:0.134, val_acc:0.950]
Epoch [80/120    avg_loss:0.091, val_acc:0.953]
Epoch [81/120    avg_loss:0.096, val_acc:0.949]
Epoch [82/120    avg_loss:0.103, val_acc:0.938]
Epoch [83/120    avg_loss:0.090, val_acc:0.948]
Epoch [84/120    avg_loss:0.077, val_acc:0.965]
Epoch [85/120    avg_loss:0.073, val_acc:0.954]
Epoch [86/120    avg_loss:0.065, val_acc:0.953]
Epoch [87/120    avg_loss:0.063, val_acc:0.959]
Epoch [88/120    avg_loss:0.047, val_acc:0.963]
Epoch [89/120    avg_loss:0.052, val_acc:0.964]
Epoch [90/120    avg_loss:0.046, val_acc:0.955]
Epoch [91/120    avg_loss:0.050, val_acc:0.954]
Epoch [92/120    avg_loss:0.059, val_acc:0.963]
Epoch [93/120    avg_loss:0.066, val_acc:0.953]
Epoch [94/120    avg_loss:0.053, val_acc:0.970]
Epoch [95/120    avg_loss:0.058, val_acc:0.971]
Epoch [96/120    avg_loss:0.044, val_acc:0.964]
Epoch [97/120    avg_loss:0.042, val_acc:0.969]
Epoch [98/120    avg_loss:0.036, val_acc:0.960]
Epoch [99/120    avg_loss:0.040, val_acc:0.968]
Epoch [100/120    avg_loss:0.038, val_acc:0.975]
Epoch [101/120    avg_loss:0.037, val_acc:0.969]
Epoch [102/120    avg_loss:0.036, val_acc:0.969]
Epoch [103/120    avg_loss:0.044, val_acc:0.959]
Epoch [104/120    avg_loss:0.059, val_acc:0.963]
Epoch [105/120    avg_loss:0.049, val_acc:0.961]
Epoch [106/120    avg_loss:0.049, val_acc:0.968]
Epoch [107/120    avg_loss:0.039, val_acc:0.967]
Epoch [108/120    avg_loss:0.033, val_acc:0.969]
Epoch [109/120    avg_loss:0.035, val_acc:0.973]
Epoch [110/120    avg_loss:0.031, val_acc:0.969]
Epoch [111/120    avg_loss:0.027, val_acc:0.971]
Epoch [112/120    avg_loss:0.028, val_acc:0.974]
Epoch [113/120    avg_loss:0.032, val_acc:0.962]
Epoch [114/120    avg_loss:0.023, val_acc:0.967]
Epoch [115/120    avg_loss:0.023, val_acc:0.973]
Epoch [116/120    avg_loss:0.019, val_acc:0.973]
Epoch [117/120    avg_loss:0.020, val_acc:0.974]
Epoch [118/120    avg_loss:0.019, val_acc:0.974]
Epoch [119/120    avg_loss:0.017, val_acc:0.974]
Epoch [120/120    avg_loss:0.020, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    2    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1243    1    0    0    2    0    0    0    0   30    0    0
     0    9    0]
 [   0    0    4  703    1   12    0    0    0   14    0    0    5    7
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    4    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    4    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0   18   90    0    5    0    0    0    0  749    6    0    0
     0    7    0]
 [   0    0   14    0    0    0    9    0    5    0   11 2162    0    3
     6    0    0]
 [   0    0    3   25   14   11    0    0    0    0   12    1  461    0
     0    3    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    2    0    0
  1136    0    0]
 [   0    0    0    0    0    1   35    0    0    0    0    0    0    0
   125  186    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.38482384823848

F1 scores:
[       nan 0.94871795 0.96769171 0.8955414  0.96598639 0.95847363
 0.96236162 1.         0.99421965 0.56       0.90787879 0.97938845
 0.922      0.97368421 0.94195688 0.67391304 0.97674419]

Kappa:
0.935923805465824
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f066abefef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.831, val_acc:0.219]
Epoch [2/120    avg_loss:2.784, val_acc:0.267]
Epoch [3/120    avg_loss:2.737, val_acc:0.310]
Epoch [4/120    avg_loss:2.685, val_acc:0.300]
Epoch [5/120    avg_loss:2.637, val_acc:0.290]
Epoch [6/120    avg_loss:2.567, val_acc:0.285]
Epoch [7/120    avg_loss:2.515, val_acc:0.306]
Epoch [8/120    avg_loss:2.477, val_acc:0.345]
Epoch [9/120    avg_loss:2.415, val_acc:0.388]
Epoch [10/120    avg_loss:2.335, val_acc:0.400]
Epoch [11/120    avg_loss:2.328, val_acc:0.384]
Epoch [12/120    avg_loss:2.303, val_acc:0.396]
Epoch [13/120    avg_loss:2.217, val_acc:0.468]
Epoch [14/120    avg_loss:2.162, val_acc:0.435]
Epoch [15/120    avg_loss:2.169, val_acc:0.494]
Epoch [16/120    avg_loss:2.075, val_acc:0.534]
Epoch [17/120    avg_loss:2.007, val_acc:0.581]
Epoch [18/120    avg_loss:1.962, val_acc:0.646]
Epoch [19/120    avg_loss:1.868, val_acc:0.662]
Epoch [20/120    avg_loss:1.789, val_acc:0.699]
Epoch [21/120    avg_loss:1.656, val_acc:0.738]
Epoch [22/120    avg_loss:1.570, val_acc:0.685]
Epoch [23/120    avg_loss:1.475, val_acc:0.748]
Epoch [24/120    avg_loss:1.326, val_acc:0.751]
Epoch [25/120    avg_loss:1.144, val_acc:0.770]
Epoch [26/120    avg_loss:1.088, val_acc:0.785]
Epoch [27/120    avg_loss:1.010, val_acc:0.771]
Epoch [28/120    avg_loss:0.960, val_acc:0.806]
Epoch [29/120    avg_loss:0.871, val_acc:0.819]
Epoch [30/120    avg_loss:0.788, val_acc:0.825]
Epoch [31/120    avg_loss:0.689, val_acc:0.849]
Epoch [32/120    avg_loss:0.612, val_acc:0.872]
Epoch [33/120    avg_loss:0.537, val_acc:0.872]
Epoch [34/120    avg_loss:0.477, val_acc:0.851]
Epoch [35/120    avg_loss:0.478, val_acc:0.850]
Epoch [36/120    avg_loss:0.410, val_acc:0.881]
Epoch [37/120    avg_loss:0.398, val_acc:0.897]
Epoch [38/120    avg_loss:0.359, val_acc:0.917]
Epoch [39/120    avg_loss:0.325, val_acc:0.916]
Epoch [40/120    avg_loss:0.330, val_acc:0.908]
Epoch [41/120    avg_loss:0.306, val_acc:0.904]
Epoch [42/120    avg_loss:0.253, val_acc:0.923]
Epoch [43/120    avg_loss:0.224, val_acc:0.920]
Epoch [44/120    avg_loss:0.226, val_acc:0.944]
Epoch [45/120    avg_loss:0.181, val_acc:0.934]
Epoch [46/120    avg_loss:0.166, val_acc:0.950]
Epoch [47/120    avg_loss:0.140, val_acc:0.949]
Epoch [48/120    avg_loss:0.143, val_acc:0.950]
Epoch [49/120    avg_loss:0.147, val_acc:0.942]
Epoch [50/120    avg_loss:0.158, val_acc:0.944]
Epoch [51/120    avg_loss:0.138, val_acc:0.944]
Epoch [52/120    avg_loss:0.132, val_acc:0.955]
Epoch [53/120    avg_loss:0.112, val_acc:0.950]
Epoch [54/120    avg_loss:0.109, val_acc:0.960]
Epoch [55/120    avg_loss:0.098, val_acc:0.951]
Epoch [56/120    avg_loss:0.091, val_acc:0.956]
Epoch [57/120    avg_loss:0.088, val_acc:0.941]
Epoch [58/120    avg_loss:0.089, val_acc:0.960]
Epoch [59/120    avg_loss:0.090, val_acc:0.952]
Epoch [60/120    avg_loss:0.071, val_acc:0.964]
Epoch [61/120    avg_loss:0.078, val_acc:0.953]
Epoch [62/120    avg_loss:0.075, val_acc:0.966]
Epoch [63/120    avg_loss:0.077, val_acc:0.948]
Epoch [64/120    avg_loss:0.073, val_acc:0.951]
Epoch [65/120    avg_loss:0.060, val_acc:0.963]
Epoch [66/120    avg_loss:0.053, val_acc:0.969]
Epoch [67/120    avg_loss:0.054, val_acc:0.964]
Epoch [68/120    avg_loss:0.065, val_acc:0.971]
Epoch [69/120    avg_loss:0.064, val_acc:0.964]
Epoch [70/120    avg_loss:0.070, val_acc:0.957]
Epoch [71/120    avg_loss:0.055, val_acc:0.967]
Epoch [72/120    avg_loss:0.049, val_acc:0.965]
Epoch [73/120    avg_loss:0.049, val_acc:0.969]
Epoch [74/120    avg_loss:0.044, val_acc:0.975]
Epoch [75/120    avg_loss:0.035, val_acc:0.976]
Epoch [76/120    avg_loss:0.034, val_acc:0.976]
Epoch [77/120    avg_loss:0.040, val_acc:0.951]
Epoch [78/120    avg_loss:0.033, val_acc:0.964]
Epoch [79/120    avg_loss:0.034, val_acc:0.972]
Epoch [80/120    avg_loss:0.035, val_acc:0.964]
Epoch [81/120    avg_loss:0.040, val_acc:0.974]
Epoch [82/120    avg_loss:0.043, val_acc:0.968]
Epoch [83/120    avg_loss:0.031, val_acc:0.967]
Epoch [84/120    avg_loss:0.046, val_acc:0.968]
Epoch [85/120    avg_loss:0.031, val_acc:0.976]
Epoch [86/120    avg_loss:0.031, val_acc:0.969]
Epoch [87/120    avg_loss:0.024, val_acc:0.973]
Epoch [88/120    avg_loss:0.030, val_acc:0.969]
Epoch [89/120    avg_loss:0.024, val_acc:0.973]
Epoch [90/120    avg_loss:0.023, val_acc:0.973]
Epoch [91/120    avg_loss:0.020, val_acc:0.977]
Epoch [92/120    avg_loss:0.034, val_acc:0.958]
Epoch [93/120    avg_loss:0.057, val_acc:0.954]
Epoch [94/120    avg_loss:0.031, val_acc:0.971]
Epoch [95/120    avg_loss:0.025, val_acc:0.971]
Epoch [96/120    avg_loss:0.017, val_acc:0.977]
Epoch [97/120    avg_loss:0.018, val_acc:0.980]
Epoch [98/120    avg_loss:0.020, val_acc:0.973]
Epoch [99/120    avg_loss:0.018, val_acc:0.977]
Epoch [100/120    avg_loss:0.016, val_acc:0.980]
Epoch [101/120    avg_loss:0.016, val_acc:0.979]
Epoch [102/120    avg_loss:0.023, val_acc:0.975]
Epoch [103/120    avg_loss:0.029, val_acc:0.975]
Epoch [104/120    avg_loss:0.020, val_acc:0.974]
Epoch [105/120    avg_loss:0.014, val_acc:0.978]
Epoch [106/120    avg_loss:0.013, val_acc:0.977]
Epoch [107/120    avg_loss:0.013, val_acc:0.978]
Epoch [108/120    avg_loss:0.012, val_acc:0.976]
Epoch [109/120    avg_loss:0.011, val_acc:0.977]
Epoch [110/120    avg_loss:0.016, val_acc:0.973]
Epoch [111/120    avg_loss:0.015, val_acc:0.973]
Epoch [112/120    avg_loss:0.015, val_acc:0.973]
Epoch [113/120    avg_loss:0.017, val_acc:0.974]
Epoch [114/120    avg_loss:0.014, val_acc:0.975]
Epoch [115/120    avg_loss:0.010, val_acc:0.977]
Epoch [116/120    avg_loss:0.010, val_acc:0.977]
Epoch [117/120    avg_loss:0.011, val_acc:0.977]
Epoch [118/120    avg_loss:0.011, val_acc:0.977]
Epoch [119/120    avg_loss:0.012, val_acc:0.976]
Epoch [120/120    avg_loss:0.013, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    1    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1254    2    3    0    1    0    0    0    1   24    0    0
     0    0    0]
 [   0    0    0  724    0    0    0    0    0    7    6    8    1    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    1    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    4    0    0    3    0    0    0    0  849   18    1    0
     0    0    0]
 [   0    0    5    0    0    0    0    4    0    0   14 2169   18    0
     0    0    0]
 [   0    0    0    7    0    0    0    0    0    0    0    2  521    0
     0    3    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    1    5    0    0    0    0    0    0    0
    46  295    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.79945799457994

F1 scores:
[       nan 0.975      0.98391526 0.97837838 0.99069767 0.9908046
 0.99470098 0.92592593 1.         0.80952381 0.9730659  0.97879061
 0.96930233 1.         0.97453604 0.89939024 0.99408284]

Kappa:
0.9749033046679243
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f51f06faeb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.783, val_acc:0.083]
Epoch [2/120    avg_loss:2.743, val_acc:0.212]
Epoch [3/120    avg_loss:2.694, val_acc:0.299]
Epoch [4/120    avg_loss:2.639, val_acc:0.360]
Epoch [5/120    avg_loss:2.561, val_acc:0.356]
Epoch [6/120    avg_loss:2.540, val_acc:0.349]
Epoch [7/120    avg_loss:2.464, val_acc:0.342]
Epoch [8/120    avg_loss:2.418, val_acc:0.342]
Epoch [9/120    avg_loss:2.397, val_acc:0.332]
Epoch [10/120    avg_loss:2.350, val_acc:0.325]
Epoch [11/120    avg_loss:2.289, val_acc:0.331]
Epoch [12/120    avg_loss:2.247, val_acc:0.342]
Epoch [13/120    avg_loss:2.214, val_acc:0.382]
Epoch [14/120    avg_loss:2.166, val_acc:0.395]
Epoch [15/120    avg_loss:2.127, val_acc:0.449]
Epoch [16/120    avg_loss:2.071, val_acc:0.404]
Epoch [17/120    avg_loss:2.029, val_acc:0.467]
Epoch [18/120    avg_loss:1.924, val_acc:0.535]
Epoch [19/120    avg_loss:1.786, val_acc:0.605]
Epoch [20/120    avg_loss:1.686, val_acc:0.624]
Epoch [21/120    avg_loss:1.599, val_acc:0.642]
Epoch [22/120    avg_loss:1.526, val_acc:0.631]
Epoch [23/120    avg_loss:1.457, val_acc:0.661]
Epoch [24/120    avg_loss:1.303, val_acc:0.727]
Epoch [25/120    avg_loss:1.204, val_acc:0.697]
Epoch [26/120    avg_loss:1.102, val_acc:0.651]
Epoch [27/120    avg_loss:1.012, val_acc:0.733]
Epoch [28/120    avg_loss:0.949, val_acc:0.774]
Epoch [29/120    avg_loss:0.891, val_acc:0.780]
Epoch [30/120    avg_loss:0.844, val_acc:0.816]
Epoch [31/120    avg_loss:0.785, val_acc:0.775]
Epoch [32/120    avg_loss:0.689, val_acc:0.841]
Epoch [33/120    avg_loss:0.594, val_acc:0.842]
Epoch [34/120    avg_loss:0.549, val_acc:0.859]
Epoch [35/120    avg_loss:0.504, val_acc:0.881]
Epoch [36/120    avg_loss:0.487, val_acc:0.870]
Epoch [37/120    avg_loss:0.425, val_acc:0.897]
Epoch [38/120    avg_loss:0.390, val_acc:0.873]
Epoch [39/120    avg_loss:0.363, val_acc:0.865]
Epoch [40/120    avg_loss:0.357, val_acc:0.853]
Epoch [41/120    avg_loss:0.322, val_acc:0.878]
Epoch [42/120    avg_loss:0.294, val_acc:0.883]
Epoch [43/120    avg_loss:0.273, val_acc:0.900]
Epoch [44/120    avg_loss:0.249, val_acc:0.903]
Epoch [45/120    avg_loss:0.209, val_acc:0.890]
Epoch [46/120    avg_loss:0.197, val_acc:0.923]
Epoch [47/120    avg_loss:0.171, val_acc:0.930]
Epoch [48/120    avg_loss:0.153, val_acc:0.927]
Epoch [49/120    avg_loss:0.144, val_acc:0.920]
Epoch [50/120    avg_loss:0.141, val_acc:0.944]
Epoch [51/120    avg_loss:0.126, val_acc:0.935]
Epoch [52/120    avg_loss:0.113, val_acc:0.950]
Epoch [53/120    avg_loss:0.102, val_acc:0.950]
Epoch [54/120    avg_loss:0.107, val_acc:0.945]
Epoch [55/120    avg_loss:0.091, val_acc:0.953]
Epoch [56/120    avg_loss:0.088, val_acc:0.954]
Epoch [57/120    avg_loss:0.082, val_acc:0.951]
Epoch [58/120    avg_loss:0.099, val_acc:0.936]
Epoch [59/120    avg_loss:0.086, val_acc:0.952]
Epoch [60/120    avg_loss:0.073, val_acc:0.958]
Epoch [61/120    avg_loss:0.067, val_acc:0.952]
Epoch [62/120    avg_loss:0.060, val_acc:0.955]
Epoch [63/120    avg_loss:0.056, val_acc:0.959]
Epoch [64/120    avg_loss:0.054, val_acc:0.952]
Epoch [65/120    avg_loss:0.062, val_acc:0.949]
Epoch [66/120    avg_loss:0.062, val_acc:0.957]
Epoch [67/120    avg_loss:0.058, val_acc:0.964]
Epoch [68/120    avg_loss:0.053, val_acc:0.958]
Epoch [69/120    avg_loss:0.043, val_acc:0.965]
Epoch [70/120    avg_loss:0.037, val_acc:0.965]
Epoch [71/120    avg_loss:0.099, val_acc:0.946]
Epoch [72/120    avg_loss:0.107, val_acc:0.934]
Epoch [73/120    avg_loss:0.101, val_acc:0.935]
Epoch [74/120    avg_loss:0.069, val_acc:0.950]
Epoch [75/120    avg_loss:0.070, val_acc:0.952]
Epoch [76/120    avg_loss:0.050, val_acc:0.960]
Epoch [77/120    avg_loss:0.037, val_acc:0.957]
Epoch [78/120    avg_loss:0.037, val_acc:0.956]
Epoch [79/120    avg_loss:0.034, val_acc:0.963]
Epoch [80/120    avg_loss:0.039, val_acc:0.963]
Epoch [81/120    avg_loss:0.036, val_acc:0.963]
Epoch [82/120    avg_loss:0.029, val_acc:0.969]
Epoch [83/120    avg_loss:0.027, val_acc:0.967]
Epoch [84/120    avg_loss:0.027, val_acc:0.968]
Epoch [85/120    avg_loss:0.027, val_acc:0.968]
Epoch [86/120    avg_loss:0.024, val_acc:0.968]
Epoch [87/120    avg_loss:0.024, val_acc:0.969]
Epoch [88/120    avg_loss:0.026, val_acc:0.969]
Epoch [89/120    avg_loss:0.033, val_acc:0.961]
Epoch [90/120    avg_loss:0.028, val_acc:0.965]
Epoch [91/120    avg_loss:0.026, val_acc:0.967]
Epoch [92/120    avg_loss:0.023, val_acc:0.974]
Epoch [93/120    avg_loss:0.017, val_acc:0.974]
Epoch [94/120    avg_loss:0.019, val_acc:0.974]
Epoch [95/120    avg_loss:0.018, val_acc:0.963]
Epoch [96/120    avg_loss:0.030, val_acc:0.952]
Epoch [97/120    avg_loss:0.033, val_acc:0.969]
Epoch [98/120    avg_loss:0.024, val_acc:0.968]
Epoch [99/120    avg_loss:0.019, val_acc:0.971]
Epoch [100/120    avg_loss:0.016, val_acc:0.973]
Epoch [101/120    avg_loss:0.015, val_acc:0.973]
Epoch [102/120    avg_loss:0.015, val_acc:0.973]
Epoch [103/120    avg_loss:0.019, val_acc:0.969]
Epoch [104/120    avg_loss:0.019, val_acc:0.967]
Epoch [105/120    avg_loss:0.015, val_acc:0.968]
Epoch [106/120    avg_loss:0.015, val_acc:0.970]
Epoch [107/120    avg_loss:0.016, val_acc:0.971]
Epoch [108/120    avg_loss:0.013, val_acc:0.971]
Epoch [109/120    avg_loss:0.013, val_acc:0.971]
Epoch [110/120    avg_loss:0.013, val_acc:0.973]
Epoch [111/120    avg_loss:0.011, val_acc:0.973]
Epoch [112/120    avg_loss:0.012, val_acc:0.973]
Epoch [113/120    avg_loss:0.012, val_acc:0.973]
Epoch [114/120    avg_loss:0.012, val_acc:0.974]
Epoch [115/120    avg_loss:0.011, val_acc:0.973]
Epoch [116/120    avg_loss:0.012, val_acc:0.973]
Epoch [117/120    avg_loss:0.012, val_acc:0.973]
Epoch [118/120    avg_loss:0.012, val_acc:0.974]
Epoch [119/120    avg_loss:0.012, val_acc:0.974]
Epoch [120/120    avg_loss:0.012, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1247    4    4    0    0    0    0    0    4   23    0    0
     0    3    0]
 [   0    0    3  719    1    0    0    0    0   16    4    2    2    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    0    0    1    0    0
     6    1    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    1    0
     0    2    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    0    0
     0    1    0]
 [   0    0   10    0    0    0    1    0    0    2  832   30    0    0
     0    0    0]
 [   0    0    8    0    0    0    0    0    0    1    3 2177   21    0
     0    0    0]
 [   0    0    0    1    6    0    0    0    0    0    3    1  516    0
     0    5    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1134    5    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
    53  277    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.25745257452574

F1 scores:
[       nan 0.975      0.97688993 0.97690217 0.97247706 0.99071926
 0.98572502 1.         0.99649942 0.62962963 0.9657574  0.97952756
 0.95910781 1.         0.97255575 0.86427457 0.97619048]

Kappa:
0.9687117187173023
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbe980cdef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.812, val_acc:0.013]
Epoch [2/120    avg_loss:2.763, val_acc:0.013]
Epoch [3/120    avg_loss:2.703, val_acc:0.111]
Epoch [4/120    avg_loss:2.664, val_acc:0.291]
Epoch [5/120    avg_loss:2.611, val_acc:0.329]
Epoch [6/120    avg_loss:2.565, val_acc:0.346]
Epoch [7/120    avg_loss:2.536, val_acc:0.370]
Epoch [8/120    avg_loss:2.477, val_acc:0.379]
Epoch [9/120    avg_loss:2.433, val_acc:0.407]
Epoch [10/120    avg_loss:2.397, val_acc:0.412]
Epoch [11/120    avg_loss:2.321, val_acc:0.454]
Epoch [12/120    avg_loss:2.260, val_acc:0.465]
Epoch [13/120    avg_loss:2.245, val_acc:0.465]
Epoch [14/120    avg_loss:2.178, val_acc:0.502]
Epoch [15/120    avg_loss:2.122, val_acc:0.492]
Epoch [16/120    avg_loss:2.051, val_acc:0.503]
Epoch [17/120    avg_loss:1.905, val_acc:0.507]
Epoch [18/120    avg_loss:1.837, val_acc:0.507]
Epoch [19/120    avg_loss:1.816, val_acc:0.569]
Epoch [20/120    avg_loss:1.638, val_acc:0.610]
Epoch [21/120    avg_loss:1.580, val_acc:0.640]
Epoch [22/120    avg_loss:1.449, val_acc:0.684]
Epoch [23/120    avg_loss:1.393, val_acc:0.666]
Epoch [24/120    avg_loss:1.266, val_acc:0.683]
Epoch [25/120    avg_loss:1.193, val_acc:0.715]
Epoch [26/120    avg_loss:1.046, val_acc:0.713]
Epoch [27/120    avg_loss:0.982, val_acc:0.724]
Epoch [28/120    avg_loss:0.937, val_acc:0.701]
Epoch [29/120    avg_loss:0.894, val_acc:0.762]
Epoch [30/120    avg_loss:0.780, val_acc:0.768]
Epoch [31/120    avg_loss:0.766, val_acc:0.781]
Epoch [32/120    avg_loss:0.693, val_acc:0.816]
Epoch [33/120    avg_loss:0.603, val_acc:0.840]
Epoch [34/120    avg_loss:0.637, val_acc:0.858]
Epoch [35/120    avg_loss:0.539, val_acc:0.845]
Epoch [36/120    avg_loss:0.473, val_acc:0.879]
Epoch [37/120    avg_loss:0.447, val_acc:0.875]
Epoch [38/120    avg_loss:0.412, val_acc:0.908]
Epoch [39/120    avg_loss:0.375, val_acc:0.906]
Epoch [40/120    avg_loss:0.343, val_acc:0.906]
Epoch [41/120    avg_loss:0.302, val_acc:0.920]
Epoch [42/120    avg_loss:0.286, val_acc:0.924]
Epoch [43/120    avg_loss:0.298, val_acc:0.882]
Epoch [44/120    avg_loss:0.282, val_acc:0.920]
Epoch [45/120    avg_loss:0.239, val_acc:0.922]
Epoch [46/120    avg_loss:0.216, val_acc:0.943]
Epoch [47/120    avg_loss:0.186, val_acc:0.943]
Epoch [48/120    avg_loss:0.196, val_acc:0.926]
Epoch [49/120    avg_loss:0.194, val_acc:0.933]
Epoch [50/120    avg_loss:0.181, val_acc:0.945]
Epoch [51/120    avg_loss:0.161, val_acc:0.942]
Epoch [52/120    avg_loss:0.198, val_acc:0.924]
Epoch [53/120    avg_loss:0.162, val_acc:0.947]
Epoch [54/120    avg_loss:0.148, val_acc:0.958]
Epoch [55/120    avg_loss:0.122, val_acc:0.955]
Epoch [56/120    avg_loss:0.111, val_acc:0.952]
Epoch [57/120    avg_loss:0.123, val_acc:0.958]
Epoch [58/120    avg_loss:0.110, val_acc:0.957]
Epoch [59/120    avg_loss:0.103, val_acc:0.956]
Epoch [60/120    avg_loss:0.079, val_acc:0.963]
Epoch [61/120    avg_loss:0.098, val_acc:0.964]
Epoch [62/120    avg_loss:0.076, val_acc:0.959]
Epoch [63/120    avg_loss:0.071, val_acc:0.966]
Epoch [64/120    avg_loss:0.073, val_acc:0.963]
Epoch [65/120    avg_loss:0.076, val_acc:0.945]
Epoch [66/120    avg_loss:0.082, val_acc:0.963]
Epoch [67/120    avg_loss:0.064, val_acc:0.963]
Epoch [68/120    avg_loss:0.063, val_acc:0.968]
Epoch [69/120    avg_loss:0.051, val_acc:0.972]
Epoch [70/120    avg_loss:0.048, val_acc:0.976]
Epoch [71/120    avg_loss:0.049, val_acc:0.979]
Epoch [72/120    avg_loss:0.053, val_acc:0.968]
Epoch [73/120    avg_loss:0.055, val_acc:0.969]
Epoch [74/120    avg_loss:0.056, val_acc:0.971]
Epoch [75/120    avg_loss:0.050, val_acc:0.965]
Epoch [76/120    avg_loss:0.044, val_acc:0.968]
Epoch [77/120    avg_loss:0.043, val_acc:0.974]
Epoch [78/120    avg_loss:0.041, val_acc:0.970]
Epoch [79/120    avg_loss:0.044, val_acc:0.978]
Epoch [80/120    avg_loss:0.040, val_acc:0.964]
Epoch [81/120    avg_loss:0.046, val_acc:0.963]
Epoch [82/120    avg_loss:0.044, val_acc:0.970]
Epoch [83/120    avg_loss:0.040, val_acc:0.976]
Epoch [84/120    avg_loss:0.033, val_acc:0.970]
Epoch [85/120    avg_loss:0.033, val_acc:0.974]
Epoch [86/120    avg_loss:0.024, val_acc:0.975]
Epoch [87/120    avg_loss:0.024, val_acc:0.978]
Epoch [88/120    avg_loss:0.027, val_acc:0.980]
Epoch [89/120    avg_loss:0.027, val_acc:0.977]
Epoch [90/120    avg_loss:0.029, val_acc:0.976]
Epoch [91/120    avg_loss:0.028, val_acc:0.978]
Epoch [92/120    avg_loss:0.025, val_acc:0.979]
Epoch [93/120    avg_loss:0.023, val_acc:0.978]
Epoch [94/120    avg_loss:0.023, val_acc:0.979]
Epoch [95/120    avg_loss:0.025, val_acc:0.977]
Epoch [96/120    avg_loss:0.024, val_acc:0.977]
Epoch [97/120    avg_loss:0.024, val_acc:0.977]
Epoch [98/120    avg_loss:0.025, val_acc:0.978]
Epoch [99/120    avg_loss:0.024, val_acc:0.976]
Epoch [100/120    avg_loss:0.024, val_acc:0.976]
Epoch [101/120    avg_loss:0.022, val_acc:0.979]
Epoch [102/120    avg_loss:0.025, val_acc:0.979]
Epoch [103/120    avg_loss:0.023, val_acc:0.978]
Epoch [104/120    avg_loss:0.024, val_acc:0.978]
Epoch [105/120    avg_loss:0.022, val_acc:0.978]
Epoch [106/120    avg_loss:0.023, val_acc:0.978]
Epoch [107/120    avg_loss:0.022, val_acc:0.978]
Epoch [108/120    avg_loss:0.023, val_acc:0.978]
Epoch [109/120    avg_loss:0.025, val_acc:0.978]
Epoch [110/120    avg_loss:0.023, val_acc:0.978]
Epoch [111/120    avg_loss:0.023, val_acc:0.978]
Epoch [112/120    avg_loss:0.023, val_acc:0.978]
Epoch [113/120    avg_loss:0.021, val_acc:0.978]
Epoch [114/120    avg_loss:0.021, val_acc:0.978]
Epoch [115/120    avg_loss:0.022, val_acc:0.978]
Epoch [116/120    avg_loss:0.021, val_acc:0.978]
Epoch [117/120    avg_loss:0.021, val_acc:0.978]
Epoch [118/120    avg_loss:0.022, val_acc:0.978]
Epoch [119/120    avg_loss:0.024, val_acc:0.978]
Epoch [120/120    avg_loss:0.023, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    1    1    0    0
     0    0    0]
 [   0    0 1261    1    3    0    0    0    0    0    3   17    0    0
     0    0    0]
 [   0    0    1  721    3    0    1    0    0   18    0    0    3    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   16    0    1    0    0
     0    0    0]
 [   0    0   13    0    0    1    3    0    0    0  839   18    0    0
     1    0    0]
 [   0    0    6    1    0    0    2    0    0    0   11 2166   24    0
     0    0    0]
 [   0    0    0    2    0    3    0    0    0    1    4    3  516    0
     0    4    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    41  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.66937669376694

F1 scores:
[       nan 0.975      0.98285269 0.97895451 0.9837587  0.99542334
 0.98942598 1.         1.         0.60377358 0.96826313 0.98075617
 0.9546716  1.         0.97706621 0.90936556 0.96969697]

Kappa:
0.9734286215641685
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa569367f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.821, val_acc:0.097]
Epoch [2/120    avg_loss:2.781, val_acc:0.160]
Epoch [3/120    avg_loss:2.720, val_acc:0.200]
Epoch [4/120    avg_loss:2.668, val_acc:0.231]
Epoch [5/120    avg_loss:2.597, val_acc:0.285]
Epoch [6/120    avg_loss:2.552, val_acc:0.332]
Epoch [7/120    avg_loss:2.501, val_acc:0.365]
Epoch [8/120    avg_loss:2.466, val_acc:0.390]
Epoch [9/120    avg_loss:2.418, val_acc:0.423]
Epoch [10/120    avg_loss:2.344, val_acc:0.446]
Epoch [11/120    avg_loss:2.322, val_acc:0.522]
Epoch [12/120    avg_loss:2.264, val_acc:0.549]
Epoch [13/120    avg_loss:2.217, val_acc:0.552]
Epoch [14/120    avg_loss:2.149, val_acc:0.591]
Epoch [15/120    avg_loss:2.087, val_acc:0.613]
Epoch [16/120    avg_loss:1.987, val_acc:0.590]
Epoch [17/120    avg_loss:1.908, val_acc:0.599]
Epoch [18/120    avg_loss:1.837, val_acc:0.597]
Epoch [19/120    avg_loss:1.656, val_acc:0.625]
Epoch [20/120    avg_loss:1.572, val_acc:0.631]
Epoch [21/120    avg_loss:1.551, val_acc:0.641]
Epoch [22/120    avg_loss:1.459, val_acc:0.680]
Epoch [23/120    avg_loss:1.312, val_acc:0.560]
Epoch [24/120    avg_loss:1.248, val_acc:0.670]
Epoch [25/120    avg_loss:1.107, val_acc:0.769]
Epoch [26/120    avg_loss:1.004, val_acc:0.785]
Epoch [27/120    avg_loss:0.879, val_acc:0.791]
Epoch [28/120    avg_loss:0.782, val_acc:0.808]
Epoch [29/120    avg_loss:0.734, val_acc:0.839]
Epoch [30/120    avg_loss:0.678, val_acc:0.825]
Epoch [31/120    avg_loss:0.625, val_acc:0.852]
Epoch [32/120    avg_loss:0.561, val_acc:0.834]
Epoch [33/120    avg_loss:0.525, val_acc:0.873]
Epoch [34/120    avg_loss:0.446, val_acc:0.897]
Epoch [35/120    avg_loss:0.422, val_acc:0.883]
Epoch [36/120    avg_loss:0.400, val_acc:0.882]
Epoch [37/120    avg_loss:0.378, val_acc:0.897]
Epoch [38/120    avg_loss:0.373, val_acc:0.886]
Epoch [39/120    avg_loss:0.356, val_acc:0.890]
Epoch [40/120    avg_loss:0.313, val_acc:0.871]
Epoch [41/120    avg_loss:0.284, val_acc:0.909]
Epoch [42/120    avg_loss:0.235, val_acc:0.930]
Epoch [43/120    avg_loss:0.208, val_acc:0.929]
Epoch [44/120    avg_loss:0.183, val_acc:0.917]
Epoch [45/120    avg_loss:0.181, val_acc:0.914]
Epoch [46/120    avg_loss:0.184, val_acc:0.930]
Epoch [47/120    avg_loss:0.154, val_acc:0.946]
Epoch [48/120    avg_loss:0.134, val_acc:0.949]
Epoch [49/120    avg_loss:0.132, val_acc:0.925]
Epoch [50/120    avg_loss:0.131, val_acc:0.944]
Epoch [51/120    avg_loss:0.113, val_acc:0.946]
Epoch [52/120    avg_loss:0.101, val_acc:0.927]
Epoch [53/120    avg_loss:0.103, val_acc:0.943]
Epoch [54/120    avg_loss:0.117, val_acc:0.949]
Epoch [55/120    avg_loss:0.095, val_acc:0.954]
Epoch [56/120    avg_loss:0.081, val_acc:0.963]
Epoch [57/120    avg_loss:0.074, val_acc:0.956]
Epoch [58/120    avg_loss:0.066, val_acc:0.966]
Epoch [59/120    avg_loss:0.063, val_acc:0.949]
Epoch [60/120    avg_loss:0.071, val_acc:0.959]
Epoch [61/120    avg_loss:0.066, val_acc:0.968]
Epoch [62/120    avg_loss:0.071, val_acc:0.963]
Epoch [63/120    avg_loss:0.073, val_acc:0.972]
Epoch [64/120    avg_loss:0.057, val_acc:0.959]
Epoch [65/120    avg_loss:0.056, val_acc:0.971]
Epoch [66/120    avg_loss:0.048, val_acc:0.967]
Epoch [67/120    avg_loss:0.050, val_acc:0.973]
Epoch [68/120    avg_loss:0.047, val_acc:0.970]
Epoch [69/120    avg_loss:0.057, val_acc:0.954]
Epoch [70/120    avg_loss:0.048, val_acc:0.975]
Epoch [71/120    avg_loss:0.057, val_acc:0.967]
Epoch [72/120    avg_loss:0.044, val_acc:0.971]
Epoch [73/120    avg_loss:0.045, val_acc:0.973]
Epoch [74/120    avg_loss:0.040, val_acc:0.969]
Epoch [75/120    avg_loss:0.032, val_acc:0.961]
Epoch [76/120    avg_loss:0.032, val_acc:0.978]
Epoch [77/120    avg_loss:0.028, val_acc:0.976]
Epoch [78/120    avg_loss:0.027, val_acc:0.976]
Epoch [79/120    avg_loss:0.028, val_acc:0.973]
Epoch [80/120    avg_loss:0.029, val_acc:0.975]
Epoch [81/120    avg_loss:0.027, val_acc:0.979]
Epoch [82/120    avg_loss:0.025, val_acc:0.968]
Epoch [83/120    avg_loss:0.023, val_acc:0.973]
Epoch [84/120    avg_loss:0.024, val_acc:0.972]
Epoch [85/120    avg_loss:0.020, val_acc:0.972]
Epoch [86/120    avg_loss:0.032, val_acc:0.971]
Epoch [87/120    avg_loss:0.028, val_acc:0.968]
Epoch [88/120    avg_loss:0.027, val_acc:0.964]
Epoch [89/120    avg_loss:0.026, val_acc:0.976]
Epoch [90/120    avg_loss:0.019, val_acc:0.979]
Epoch [91/120    avg_loss:0.017, val_acc:0.976]
Epoch [92/120    avg_loss:0.017, val_acc:0.971]
Epoch [93/120    avg_loss:0.018, val_acc:0.970]
Epoch [94/120    avg_loss:0.016, val_acc:0.982]
Epoch [95/120    avg_loss:0.018, val_acc:0.974]
Epoch [96/120    avg_loss:0.022, val_acc:0.973]
Epoch [97/120    avg_loss:0.026, val_acc:0.972]
Epoch [98/120    avg_loss:0.020, val_acc:0.966]
Epoch [99/120    avg_loss:0.030, val_acc:0.975]
Epoch [100/120    avg_loss:0.030, val_acc:0.972]
Epoch [101/120    avg_loss:0.026, val_acc:0.972]
Epoch [102/120    avg_loss:0.020, val_acc:0.977]
Epoch [103/120    avg_loss:0.024, val_acc:0.979]
Epoch [104/120    avg_loss:0.015, val_acc:0.975]
Epoch [105/120    avg_loss:0.013, val_acc:0.978]
Epoch [106/120    avg_loss:0.014, val_acc:0.970]
Epoch [107/120    avg_loss:0.016, val_acc:0.970]
Epoch [108/120    avg_loss:0.014, val_acc:0.971]
Epoch [109/120    avg_loss:0.012, val_acc:0.972]
Epoch [110/120    avg_loss:0.012, val_acc:0.974]
Epoch [111/120    avg_loss:0.013, val_acc:0.974]
Epoch [112/120    avg_loss:0.013, val_acc:0.974]
Epoch [113/120    avg_loss:0.011, val_acc:0.975]
Epoch [114/120    avg_loss:0.013, val_acc:0.976]
Epoch [115/120    avg_loss:0.012, val_acc:0.977]
Epoch [116/120    avg_loss:0.012, val_acc:0.976]
Epoch [117/120    avg_loss:0.010, val_acc:0.977]
Epoch [118/120    avg_loss:0.010, val_acc:0.976]
Epoch [119/120    avg_loss:0.012, val_acc:0.977]
Epoch [120/120    avg_loss:0.011, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1253    1    8    0    0    0    0    0    0   22    1    0
     0    0    0]
 [   0    0    1  726    2    0    0    0    0    2    1   10    5    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  842   28    0    0
     0    0    0]
 [   0    0   13    0    0    0    1    0    0    0    3 2178   14    0
     0    1    0]
 [   0    0    0    4    0    0    0    0    0    0    3    2  523    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1136    3    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
    89  251    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.47425474254743

F1 scores:
[       nan 0.98765432 0.97967162 0.98108108 0.97235023 1.
 0.99318698 1.         1.         0.91891892 0.97679814 0.97865648
 0.9703154  1.         0.96067653 0.83388704 0.99408284]

Kappa:
0.971166062966206
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa120b2ff28>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.816, val_acc:0.121]
Epoch [2/120    avg_loss:2.778, val_acc:0.145]
Epoch [3/120    avg_loss:2.741, val_acc:0.182]
Epoch [4/120    avg_loss:2.693, val_acc:0.225]
Epoch [5/120    avg_loss:2.669, val_acc:0.263]
Epoch [6/120    avg_loss:2.618, val_acc:0.290]
Epoch [7/120    avg_loss:2.548, val_acc:0.312]
Epoch [8/120    avg_loss:2.493, val_acc:0.356]
Epoch [9/120    avg_loss:2.468, val_acc:0.414]
Epoch [10/120    avg_loss:2.386, val_acc:0.443]
Epoch [11/120    avg_loss:2.343, val_acc:0.447]
Epoch [12/120    avg_loss:2.287, val_acc:0.458]
Epoch [13/120    avg_loss:2.218, val_acc:0.476]
Epoch [14/120    avg_loss:2.162, val_acc:0.503]
Epoch [15/120    avg_loss:2.034, val_acc:0.511]
Epoch [16/120    avg_loss:1.969, val_acc:0.492]
Epoch [17/120    avg_loss:1.866, val_acc:0.576]
Epoch [18/120    avg_loss:1.787, val_acc:0.553]
Epoch [19/120    avg_loss:1.731, val_acc:0.542]
Epoch [20/120    avg_loss:1.573, val_acc:0.596]
Epoch [21/120    avg_loss:1.523, val_acc:0.574]
Epoch [22/120    avg_loss:1.435, val_acc:0.613]
Epoch [23/120    avg_loss:1.362, val_acc:0.639]
Epoch [24/120    avg_loss:1.254, val_acc:0.664]
Epoch [25/120    avg_loss:1.175, val_acc:0.652]
Epoch [26/120    avg_loss:1.061, val_acc:0.668]
Epoch [27/120    avg_loss:0.973, val_acc:0.668]
Epoch [28/120    avg_loss:0.901, val_acc:0.701]
Epoch [29/120    avg_loss:0.820, val_acc:0.700]
Epoch [30/120    avg_loss:0.793, val_acc:0.745]
Epoch [31/120    avg_loss:0.711, val_acc:0.734]
Epoch [32/120    avg_loss:0.639, val_acc:0.803]
Epoch [33/120    avg_loss:0.652, val_acc:0.800]
Epoch [34/120    avg_loss:0.611, val_acc:0.821]
Epoch [35/120    avg_loss:0.556, val_acc:0.801]
Epoch [36/120    avg_loss:0.542, val_acc:0.822]
Epoch [37/120    avg_loss:0.488, val_acc:0.874]
Epoch [38/120    avg_loss:0.420, val_acc:0.866]
Epoch [39/120    avg_loss:0.392, val_acc:0.895]
Epoch [40/120    avg_loss:0.357, val_acc:0.890]
Epoch [41/120    avg_loss:0.305, val_acc:0.914]
Epoch [42/120    avg_loss:0.296, val_acc:0.926]
Epoch [43/120    avg_loss:0.273, val_acc:0.904]
Epoch [44/120    avg_loss:0.247, val_acc:0.923]
Epoch [45/120    avg_loss:0.244, val_acc:0.930]
Epoch [46/120    avg_loss:0.256, val_acc:0.927]
Epoch [47/120    avg_loss:0.207, val_acc:0.933]
Epoch [48/120    avg_loss:0.188, val_acc:0.939]
Epoch [49/120    avg_loss:0.175, val_acc:0.954]
Epoch [50/120    avg_loss:0.177, val_acc:0.928]
Epoch [51/120    avg_loss:0.161, val_acc:0.955]
Epoch [52/120    avg_loss:0.163, val_acc:0.963]
Epoch [53/120    avg_loss:0.148, val_acc:0.959]
Epoch [54/120    avg_loss:0.130, val_acc:0.969]
Epoch [55/120    avg_loss:0.140, val_acc:0.963]
Epoch [56/120    avg_loss:0.122, val_acc:0.936]
Epoch [57/120    avg_loss:0.092, val_acc:0.973]
Epoch [58/120    avg_loss:0.088, val_acc:0.956]
Epoch [59/120    avg_loss:0.094, val_acc:0.968]
Epoch [60/120    avg_loss:0.099, val_acc:0.963]
Epoch [61/120    avg_loss:0.103, val_acc:0.977]
Epoch [62/120    avg_loss:0.091, val_acc:0.969]
Epoch [63/120    avg_loss:0.081, val_acc:0.981]
Epoch [64/120    avg_loss:0.074, val_acc:0.979]
Epoch [65/120    avg_loss:0.068, val_acc:0.980]
Epoch [66/120    avg_loss:0.069, val_acc:0.979]
Epoch [67/120    avg_loss:0.067, val_acc:0.985]
Epoch [68/120    avg_loss:0.070, val_acc:0.975]
Epoch [69/120    avg_loss:0.066, val_acc:0.957]
Epoch [70/120    avg_loss:0.059, val_acc:0.977]
Epoch [71/120    avg_loss:0.047, val_acc:0.984]
Epoch [72/120    avg_loss:0.047, val_acc:0.983]
Epoch [73/120    avg_loss:0.044, val_acc:0.984]
Epoch [74/120    avg_loss:0.042, val_acc:0.982]
Epoch [75/120    avg_loss:0.043, val_acc:0.985]
Epoch [76/120    avg_loss:0.048, val_acc:0.983]
Epoch [77/120    avg_loss:0.038, val_acc:0.989]
Epoch [78/120    avg_loss:0.037, val_acc:0.984]
Epoch [79/120    avg_loss:0.038, val_acc:0.981]
Epoch [80/120    avg_loss:0.037, val_acc:0.984]
Epoch [81/120    avg_loss:0.036, val_acc:0.981]
Epoch [82/120    avg_loss:0.036, val_acc:0.979]
Epoch [83/120    avg_loss:0.047, val_acc:0.974]
Epoch [84/120    avg_loss:0.082, val_acc:0.965]
Epoch [85/120    avg_loss:0.061, val_acc:0.965]
Epoch [86/120    avg_loss:0.053, val_acc:0.969]
Epoch [87/120    avg_loss:0.046, val_acc:0.976]
Epoch [88/120    avg_loss:0.074, val_acc:0.969]
Epoch [89/120    avg_loss:0.049, val_acc:0.973]
Epoch [90/120    avg_loss:0.041, val_acc:0.978]
Epoch [91/120    avg_loss:0.032, val_acc:0.983]
Epoch [92/120    avg_loss:0.032, val_acc:0.988]
Epoch [93/120    avg_loss:0.026, val_acc:0.988]
Epoch [94/120    avg_loss:0.028, val_acc:0.986]
Epoch [95/120    avg_loss:0.023, val_acc:0.984]
Epoch [96/120    avg_loss:0.025, val_acc:0.985]
Epoch [97/120    avg_loss:0.023, val_acc:0.984]
Epoch [98/120    avg_loss:0.024, val_acc:0.985]
Epoch [99/120    avg_loss:0.026, val_acc:0.985]
Epoch [100/120    avg_loss:0.024, val_acc:0.985]
Epoch [101/120    avg_loss:0.022, val_acc:0.985]
Epoch [102/120    avg_loss:0.025, val_acc:0.985]
Epoch [103/120    avg_loss:0.024, val_acc:0.985]
Epoch [104/120    avg_loss:0.021, val_acc:0.985]
Epoch [105/120    avg_loss:0.023, val_acc:0.985]
Epoch [106/120    avg_loss:0.023, val_acc:0.985]
Epoch [107/120    avg_loss:0.022, val_acc:0.985]
Epoch [108/120    avg_loss:0.020, val_acc:0.985]
Epoch [109/120    avg_loss:0.021, val_acc:0.985]
Epoch [110/120    avg_loss:0.022, val_acc:0.985]
Epoch [111/120    avg_loss:0.023, val_acc:0.985]
Epoch [112/120    avg_loss:0.020, val_acc:0.985]
Epoch [113/120    avg_loss:0.022, val_acc:0.985]
Epoch [114/120    avg_loss:0.021, val_acc:0.985]
Epoch [115/120    avg_loss:0.022, val_acc:0.985]
Epoch [116/120    avg_loss:0.021, val_acc:0.985]
Epoch [117/120    avg_loss:0.022, val_acc:0.985]
Epoch [118/120    avg_loss:0.022, val_acc:0.985]
Epoch [119/120    avg_loss:0.023, val_acc:0.985]
Epoch [120/120    avg_loss:0.020, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    2    0    0    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0 1245    2    6    0    0    0    0    0    2   30    0    0
     0    0    0]
 [   0    0    0  726    5    0    0    0    0    2    5    4    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    0    0    2    0    0
     6    0    0]
 [   0    0    0    0    0    0  653    0    0    0    1    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   14    0    1    0    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    0  853   15    0    0
     0    0    0]
 [   0    0   19    0    0    0    0    0    0    0   19 2143   22    0
     0    7    0]
 [   0    0    0    0    1    0    0    0    0    0    0    1  527    0
     1    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  184
     1    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1127   11    0]
 [   0    0    0    0    0    1    6    0    0    0    0    0    0    0
    63  277    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.15989159891599

F1 scores:
[       nan 0.96202532 0.97341673 0.98440678 0.97260274 0.98957126
 0.99014405 1.         1.         0.8        0.97207977 0.97210252
 0.96608616 0.99728997 0.96407186 0.86024845 0.97005988]

Kappa:
0.9676131956298859
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcfd7249f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.816, val_acc:0.228]
Epoch [2/120    avg_loss:2.753, val_acc:0.333]
Epoch [3/120    avg_loss:2.705, val_acc:0.384]
Epoch [4/120    avg_loss:2.636, val_acc:0.440]
Epoch [5/120    avg_loss:2.562, val_acc:0.459]
Epoch [6/120    avg_loss:2.521, val_acc:0.467]
Epoch [7/120    avg_loss:2.454, val_acc:0.481]
Epoch [8/120    avg_loss:2.412, val_acc:0.497]
Epoch [9/120    avg_loss:2.364, val_acc:0.519]
Epoch [10/120    avg_loss:2.333, val_acc:0.527]
Epoch [11/120    avg_loss:2.247, val_acc:0.542]
Epoch [12/120    avg_loss:2.195, val_acc:0.566]
Epoch [13/120    avg_loss:2.104, val_acc:0.557]
Epoch [14/120    avg_loss:2.062, val_acc:0.580]
Epoch [15/120    avg_loss:1.978, val_acc:0.551]
Epoch [16/120    avg_loss:1.863, val_acc:0.521]
Epoch [17/120    avg_loss:1.801, val_acc:0.581]
Epoch [18/120    avg_loss:1.719, val_acc:0.613]
Epoch [19/120    avg_loss:1.614, val_acc:0.642]
Epoch [20/120    avg_loss:1.541, val_acc:0.653]
Epoch [21/120    avg_loss:1.449, val_acc:0.666]
Epoch [22/120    avg_loss:1.412, val_acc:0.649]
Epoch [23/120    avg_loss:1.273, val_acc:0.716]
Epoch [24/120    avg_loss:1.169, val_acc:0.716]
Epoch [25/120    avg_loss:1.078, val_acc:0.751]
Epoch [26/120    avg_loss:0.981, val_acc:0.768]
Epoch [27/120    avg_loss:0.907, val_acc:0.767]
Epoch [28/120    avg_loss:0.939, val_acc:0.756]
Epoch [29/120    avg_loss:0.826, val_acc:0.783]
Epoch [30/120    avg_loss:0.724, val_acc:0.785]
Epoch [31/120    avg_loss:0.767, val_acc:0.800]
Epoch [32/120    avg_loss:0.672, val_acc:0.821]
Epoch [33/120    avg_loss:0.634, val_acc:0.810]
Epoch [34/120    avg_loss:0.548, val_acc:0.810]
Epoch [35/120    avg_loss:0.469, val_acc:0.849]
Epoch [36/120    avg_loss:0.435, val_acc:0.866]
Epoch [37/120    avg_loss:0.372, val_acc:0.882]
Epoch [38/120    avg_loss:0.358, val_acc:0.878]
Epoch [39/120    avg_loss:0.295, val_acc:0.897]
Epoch [40/120    avg_loss:0.298, val_acc:0.872]
Epoch [41/120    avg_loss:0.294, val_acc:0.892]
Epoch [42/120    avg_loss:0.266, val_acc:0.918]
Epoch [43/120    avg_loss:0.227, val_acc:0.920]
Epoch [44/120    avg_loss:0.204, val_acc:0.922]
Epoch [45/120    avg_loss:0.201, val_acc:0.925]
Epoch [46/120    avg_loss:0.186, val_acc:0.922]
Epoch [47/120    avg_loss:0.174, val_acc:0.925]
Epoch [48/120    avg_loss:0.166, val_acc:0.939]
Epoch [49/120    avg_loss:0.149, val_acc:0.930]
Epoch [50/120    avg_loss:0.148, val_acc:0.929]
Epoch [51/120    avg_loss:0.144, val_acc:0.934]
Epoch [52/120    avg_loss:0.122, val_acc:0.950]
Epoch [53/120    avg_loss:0.115, val_acc:0.945]
Epoch [54/120    avg_loss:0.107, val_acc:0.958]
Epoch [55/120    avg_loss:0.114, val_acc:0.939]
Epoch [56/120    avg_loss:0.111, val_acc:0.959]
Epoch [57/120    avg_loss:0.090, val_acc:0.960]
Epoch [58/120    avg_loss:0.080, val_acc:0.951]
Epoch [59/120    avg_loss:0.098, val_acc:0.956]
Epoch [60/120    avg_loss:0.079, val_acc:0.966]
Epoch [61/120    avg_loss:0.069, val_acc:0.966]
Epoch [62/120    avg_loss:0.063, val_acc:0.973]
Epoch [63/120    avg_loss:0.058, val_acc:0.963]
Epoch [64/120    avg_loss:0.052, val_acc:0.977]
Epoch [65/120    avg_loss:0.049, val_acc:0.970]
Epoch [66/120    avg_loss:0.061, val_acc:0.973]
Epoch [67/120    avg_loss:0.058, val_acc:0.975]
Epoch [68/120    avg_loss:0.056, val_acc:0.971]
Epoch [69/120    avg_loss:0.060, val_acc:0.969]
Epoch [70/120    avg_loss:0.052, val_acc:0.968]
Epoch [71/120    avg_loss:0.062, val_acc:0.971]
Epoch [72/120    avg_loss:0.050, val_acc:0.973]
Epoch [73/120    avg_loss:0.043, val_acc:0.971]
Epoch [74/120    avg_loss:0.041, val_acc:0.970]
Epoch [75/120    avg_loss:0.032, val_acc:0.971]
Epoch [76/120    avg_loss:0.032, val_acc:0.974]
Epoch [77/120    avg_loss:0.044, val_acc:0.976]
Epoch [78/120    avg_loss:0.030, val_acc:0.976]
Epoch [79/120    avg_loss:0.028, val_acc:0.976]
Epoch [80/120    avg_loss:0.027, val_acc:0.978]
Epoch [81/120    avg_loss:0.027, val_acc:0.978]
Epoch [82/120    avg_loss:0.028, val_acc:0.979]
Epoch [83/120    avg_loss:0.023, val_acc:0.980]
Epoch [84/120    avg_loss:0.024, val_acc:0.979]
Epoch [85/120    avg_loss:0.025, val_acc:0.979]
Epoch [86/120    avg_loss:0.026, val_acc:0.976]
Epoch [87/120    avg_loss:0.023, val_acc:0.977]
Epoch [88/120    avg_loss:0.023, val_acc:0.978]
Epoch [89/120    avg_loss:0.023, val_acc:0.980]
Epoch [90/120    avg_loss:0.027, val_acc:0.979]
Epoch [91/120    avg_loss:0.028, val_acc:0.978]
Epoch [92/120    avg_loss:0.023, val_acc:0.977]
Epoch [93/120    avg_loss:0.024, val_acc:0.978]
Epoch [94/120    avg_loss:0.023, val_acc:0.978]
Epoch [95/120    avg_loss:0.023, val_acc:0.978]
Epoch [96/120    avg_loss:0.023, val_acc:0.978]
Epoch [97/120    avg_loss:0.023, val_acc:0.978]
Epoch [98/120    avg_loss:0.022, val_acc:0.978]
Epoch [99/120    avg_loss:0.026, val_acc:0.979]
Epoch [100/120    avg_loss:0.023, val_acc:0.978]
Epoch [101/120    avg_loss:0.022, val_acc:0.978]
Epoch [102/120    avg_loss:0.024, val_acc:0.978]
Epoch [103/120    avg_loss:0.022, val_acc:0.978]
Epoch [104/120    avg_loss:0.021, val_acc:0.978]
Epoch [105/120    avg_loss:0.024, val_acc:0.978]
Epoch [106/120    avg_loss:0.023, val_acc:0.978]
Epoch [107/120    avg_loss:0.024, val_acc:0.979]
Epoch [108/120    avg_loss:0.022, val_acc:0.978]
Epoch [109/120    avg_loss:0.023, val_acc:0.979]
Epoch [110/120    avg_loss:0.021, val_acc:0.977]
Epoch [111/120    avg_loss:0.022, val_acc:0.978]
Epoch [112/120    avg_loss:0.022, val_acc:0.978]
Epoch [113/120    avg_loss:0.022, val_acc:0.979]
Epoch [114/120    avg_loss:0.022, val_acc:0.979]
Epoch [115/120    avg_loss:0.022, val_acc:0.978]
Epoch [116/120    avg_loss:0.020, val_acc:0.978]
Epoch [117/120    avg_loss:0.023, val_acc:0.978]
Epoch [118/120    avg_loss:0.021, val_acc:0.978]
Epoch [119/120    avg_loss:0.023, val_acc:0.978]
Epoch [120/120    avg_loss:0.022, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    3    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1252    2    7    0    0    0    0    0    4   19    1    0
     0    0    0]
 [   0    0    1  719    0    1    0    0    0    2    4   16    4    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    1    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    0    0    0    0    0  844   18    3    0
     0    0    0]
 [   0    0    6    0    0    0    1    0    0    0    7 2172   24    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0    0   11  514    0
     0    5    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    69  277    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.37669376693766

F1 scores:
[       nan 0.96202532 0.97965571 0.97889721 0.97921478 0.99427262
 0.99467681 1.         0.99883586 0.88888889 0.97347174 0.97661871
 0.94921514 1.         0.96625374 0.86970173 0.97005988]

Kappa:
0.9700622398851587
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7c60739f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.824, val_acc:0.061]
Epoch [2/120    avg_loss:2.782, val_acc:0.123]
Epoch [3/120    avg_loss:2.746, val_acc:0.186]
Epoch [4/120    avg_loss:2.696, val_acc:0.210]
Epoch [5/120    avg_loss:2.631, val_acc:0.216]
Epoch [6/120    avg_loss:2.595, val_acc:0.292]
Epoch [7/120    avg_loss:2.551, val_acc:0.308]
Epoch [8/120    avg_loss:2.497, val_acc:0.336]
Epoch [9/120    avg_loss:2.450, val_acc:0.372]
Epoch [10/120    avg_loss:2.396, val_acc:0.384]
Epoch [11/120    avg_loss:2.369, val_acc:0.399]
Epoch [12/120    avg_loss:2.302, val_acc:0.400]
Epoch [13/120    avg_loss:2.284, val_acc:0.405]
Epoch [14/120    avg_loss:2.240, val_acc:0.402]
Epoch [15/120    avg_loss:2.208, val_acc:0.419]
Epoch [16/120    avg_loss:2.138, val_acc:0.430]
Epoch [17/120    avg_loss:2.085, val_acc:0.405]
Epoch [18/120    avg_loss:2.016, val_acc:0.404]
Epoch [19/120    avg_loss:2.024, val_acc:0.453]
Epoch [20/120    avg_loss:1.924, val_acc:0.457]
Epoch [21/120    avg_loss:1.822, val_acc:0.471]
Epoch [22/120    avg_loss:1.740, val_acc:0.496]
Epoch [23/120    avg_loss:1.592, val_acc:0.510]
Epoch [24/120    avg_loss:1.494, val_acc:0.504]
Epoch [25/120    avg_loss:1.403, val_acc:0.508]
Epoch [26/120    avg_loss:1.308, val_acc:0.547]
Epoch [27/120    avg_loss:1.169, val_acc:0.530]
Epoch [28/120    avg_loss:1.109, val_acc:0.565]
Epoch [29/120    avg_loss:1.028, val_acc:0.602]
Epoch [30/120    avg_loss:0.972, val_acc:0.582]
Epoch [31/120    avg_loss:0.932, val_acc:0.605]
Epoch [32/120    avg_loss:0.843, val_acc:0.684]
Epoch [33/120    avg_loss:0.739, val_acc:0.732]
Epoch [34/120    avg_loss:0.696, val_acc:0.704]
Epoch [35/120    avg_loss:0.649, val_acc:0.761]
Epoch [36/120    avg_loss:0.552, val_acc:0.772]
Epoch [37/120    avg_loss:0.573, val_acc:0.773]
Epoch [38/120    avg_loss:0.488, val_acc:0.809]
Epoch [39/120    avg_loss:0.439, val_acc:0.839]
Epoch [40/120    avg_loss:0.409, val_acc:0.859]
Epoch [41/120    avg_loss:0.374, val_acc:0.842]
Epoch [42/120    avg_loss:0.346, val_acc:0.875]
Epoch [43/120    avg_loss:0.323, val_acc:0.869]
Epoch [44/120    avg_loss:0.308, val_acc:0.875]
Epoch [45/120    avg_loss:0.290, val_acc:0.894]
Epoch [46/120    avg_loss:0.256, val_acc:0.908]
Epoch [47/120    avg_loss:0.251, val_acc:0.898]
Epoch [48/120    avg_loss:0.233, val_acc:0.908]
Epoch [49/120    avg_loss:0.196, val_acc:0.917]
Epoch [50/120    avg_loss:0.191, val_acc:0.933]
Epoch [51/120    avg_loss:0.178, val_acc:0.916]
Epoch [52/120    avg_loss:0.174, val_acc:0.912]
Epoch [53/120    avg_loss:0.175, val_acc:0.934]
Epoch [54/120    avg_loss:0.141, val_acc:0.938]
Epoch [55/120    avg_loss:0.136, val_acc:0.936]
Epoch [56/120    avg_loss:0.122, val_acc:0.933]
Epoch [57/120    avg_loss:0.116, val_acc:0.929]
Epoch [58/120    avg_loss:0.117, val_acc:0.941]
Epoch [59/120    avg_loss:0.100, val_acc:0.943]
Epoch [60/120    avg_loss:0.104, val_acc:0.943]
Epoch [61/120    avg_loss:0.098, val_acc:0.945]
Epoch [62/120    avg_loss:0.099, val_acc:0.924]
Epoch [63/120    avg_loss:0.111, val_acc:0.942]
Epoch [64/120    avg_loss:0.080, val_acc:0.952]
Epoch [65/120    avg_loss:0.082, val_acc:0.960]
Epoch [66/120    avg_loss:0.074, val_acc:0.957]
Epoch [67/120    avg_loss:0.065, val_acc:0.945]
Epoch [68/120    avg_loss:0.079, val_acc:0.963]
Epoch [69/120    avg_loss:0.073, val_acc:0.957]
Epoch [70/120    avg_loss:0.073, val_acc:0.952]
Epoch [71/120    avg_loss:0.069, val_acc:0.957]
Epoch [72/120    avg_loss:0.881, val_acc:0.517]
Epoch [73/120    avg_loss:1.190, val_acc:0.799]
Epoch [74/120    avg_loss:0.608, val_acc:0.825]
Epoch [75/120    avg_loss:0.497, val_acc:0.859]
Epoch [76/120    avg_loss:0.322, val_acc:0.877]
Epoch [77/120    avg_loss:0.236, val_acc:0.904]
Epoch [78/120    avg_loss:0.188, val_acc:0.917]
Epoch [79/120    avg_loss:0.157, val_acc:0.935]
Epoch [80/120    avg_loss:0.151, val_acc:0.836]
Epoch [81/120    avg_loss:0.213, val_acc:0.893]
Epoch [82/120    avg_loss:0.162, val_acc:0.934]
Epoch [83/120    avg_loss:0.121, val_acc:0.947]
Epoch [84/120    avg_loss:0.100, val_acc:0.950]
Epoch [85/120    avg_loss:0.104, val_acc:0.947]
Epoch [86/120    avg_loss:0.091, val_acc:0.948]
Epoch [87/120    avg_loss:0.098, val_acc:0.948]
Epoch [88/120    avg_loss:0.098, val_acc:0.952]
Epoch [89/120    avg_loss:0.080, val_acc:0.955]
Epoch [90/120    avg_loss:0.083, val_acc:0.946]
Epoch [91/120    avg_loss:0.091, val_acc:0.950]
Epoch [92/120    avg_loss:0.080, val_acc:0.956]
Epoch [93/120    avg_loss:0.079, val_acc:0.956]
Epoch [94/120    avg_loss:0.079, val_acc:0.952]
Epoch [95/120    avg_loss:0.078, val_acc:0.952]
Epoch [96/120    avg_loss:0.068, val_acc:0.953]
Epoch [97/120    avg_loss:0.077, val_acc:0.952]
Epoch [98/120    avg_loss:0.089, val_acc:0.954]
Epoch [99/120    avg_loss:0.073, val_acc:0.953]
Epoch [100/120    avg_loss:0.075, val_acc:0.956]
Epoch [101/120    avg_loss:0.077, val_acc:0.959]
Epoch [102/120    avg_loss:0.069, val_acc:0.955]
Epoch [103/120    avg_loss:0.070, val_acc:0.955]
Epoch [104/120    avg_loss:0.072, val_acc:0.956]
Epoch [105/120    avg_loss:0.069, val_acc:0.957]
Epoch [106/120    avg_loss:0.074, val_acc:0.957]
Epoch [107/120    avg_loss:0.077, val_acc:0.955]
Epoch [108/120    avg_loss:0.079, val_acc:0.955]
Epoch [109/120    avg_loss:0.079, val_acc:0.956]
Epoch [110/120    avg_loss:0.068, val_acc:0.956]
Epoch [111/120    avg_loss:0.070, val_acc:0.955]
Epoch [112/120    avg_loss:0.070, val_acc:0.956]
Epoch [113/120    avg_loss:0.074, val_acc:0.957]
Epoch [114/120    avg_loss:0.076, val_acc:0.956]
Epoch [115/120    avg_loss:0.071, val_acc:0.956]
Epoch [116/120    avg_loss:0.076, val_acc:0.955]
Epoch [117/120    avg_loss:0.068, val_acc:0.955]
Epoch [118/120    avg_loss:0.073, val_acc:0.955]
Epoch [119/120    avg_loss:0.075, val_acc:0.955]
Epoch [120/120    avg_loss:0.071, val_acc:0.955]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1217   20    3    0    0    0    0    0   12   32    1    0
     0    0    0]
 [   0    0    1  723    6    0    1    0    0    2    2    4    7    1
     0    0    0]
 [   0    0    0    1  208    0    0    0    0    0    0    2    2    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    0    0    1    0    0
     8    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   16    0    1    0    0
     0    0    0]
 [   0    0    8    0    0    0    3    0    0    0  821   43    0    0
     0    0    0]
 [   0    0   10    0    0    0   12    0    0    0   42 2105   39    0
     1    0    1]
 [   0    0    0    4    0    0    0    0    0    0    2    7  516    0
     3    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1120   18    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    43  289    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.9349593495935

F1 scores:
[       nan 0.88888889 0.96548988 0.96722408 0.96744186 0.98954704
 0.97393894 1.         0.9953271  0.86486486 0.93348493 0.95529839
 0.93818182 0.99730458 0.96760259 0.88379205 0.97647059]

Kappa:
0.9536691973390922
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f82a0bdcef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.853, val_acc:0.148]
Epoch [2/120    avg_loss:2.801, val_acc:0.240]
Epoch [3/120    avg_loss:2.756, val_acc:0.265]
Epoch [4/120    avg_loss:2.720, val_acc:0.282]
Epoch [5/120    avg_loss:2.674, val_acc:0.291]
Epoch [6/120    avg_loss:2.634, val_acc:0.301]
Epoch [7/120    avg_loss:2.580, val_acc:0.314]
Epoch [8/120    avg_loss:2.540, val_acc:0.342]
Epoch [9/120    avg_loss:2.483, val_acc:0.364]
Epoch [10/120    avg_loss:2.414, val_acc:0.401]
Epoch [11/120    avg_loss:2.382, val_acc:0.409]
Epoch [12/120    avg_loss:2.330, val_acc:0.484]
Epoch [13/120    avg_loss:2.303, val_acc:0.484]
Epoch [14/120    avg_loss:2.239, val_acc:0.497]
Epoch [15/120    avg_loss:2.147, val_acc:0.561]
Epoch [16/120    avg_loss:2.073, val_acc:0.570]
Epoch [17/120    avg_loss:2.017, val_acc:0.578]
Epoch [18/120    avg_loss:1.896, val_acc:0.557]
Epoch [19/120    avg_loss:1.807, val_acc:0.615]
Epoch [20/120    avg_loss:1.729, val_acc:0.616]
Epoch [21/120    avg_loss:1.634, val_acc:0.606]
Epoch [22/120    avg_loss:1.508, val_acc:0.624]
Epoch [23/120    avg_loss:1.418, val_acc:0.560]
Epoch [24/120    avg_loss:1.355, val_acc:0.614]
Epoch [25/120    avg_loss:1.276, val_acc:0.672]
Epoch [26/120    avg_loss:1.182, val_acc:0.699]
Epoch [27/120    avg_loss:1.033, val_acc:0.725]
Epoch [28/120    avg_loss:0.936, val_acc:0.706]
Epoch [29/120    avg_loss:0.854, val_acc:0.728]
Epoch [30/120    avg_loss:0.772, val_acc:0.758]
Epoch [31/120    avg_loss:0.719, val_acc:0.789]
Epoch [32/120    avg_loss:0.617, val_acc:0.805]
Epoch [33/120    avg_loss:0.524, val_acc:0.830]
Epoch [34/120    avg_loss:0.511, val_acc:0.841]
Epoch [35/120    avg_loss:0.476, val_acc:0.849]
Epoch [36/120    avg_loss:0.431, val_acc:0.861]
Epoch [37/120    avg_loss:0.393, val_acc:0.851]
Epoch [38/120    avg_loss:0.358, val_acc:0.865]
Epoch [39/120    avg_loss:0.333, val_acc:0.880]
Epoch [40/120    avg_loss:0.342, val_acc:0.874]
Epoch [41/120    avg_loss:0.296, val_acc:0.881]
Epoch [42/120    avg_loss:0.274, val_acc:0.914]
Epoch [43/120    avg_loss:0.241, val_acc:0.899]
Epoch [44/120    avg_loss:0.226, val_acc:0.920]
Epoch [45/120    avg_loss:0.202, val_acc:0.909]
Epoch [46/120    avg_loss:0.196, val_acc:0.932]
Epoch [47/120    avg_loss:0.184, val_acc:0.927]
Epoch [48/120    avg_loss:0.162, val_acc:0.934]
Epoch [49/120    avg_loss:0.168, val_acc:0.930]
Epoch [50/120    avg_loss:0.178, val_acc:0.928]
Epoch [51/120    avg_loss:0.139, val_acc:0.938]
Epoch [52/120    avg_loss:0.130, val_acc:0.924]
Epoch [53/120    avg_loss:0.131, val_acc:0.941]
Epoch [54/120    avg_loss:0.135, val_acc:0.944]
Epoch [55/120    avg_loss:0.106, val_acc:0.947]
Epoch [56/120    avg_loss:0.105, val_acc:0.952]
Epoch [57/120    avg_loss:0.102, val_acc:0.945]
Epoch [58/120    avg_loss:0.091, val_acc:0.952]
Epoch [59/120    avg_loss:0.085, val_acc:0.945]
Epoch [60/120    avg_loss:0.081, val_acc:0.953]
Epoch [61/120    avg_loss:0.080, val_acc:0.946]
Epoch [62/120    avg_loss:0.072, val_acc:0.945]
Epoch [63/120    avg_loss:0.084, val_acc:0.951]
Epoch [64/120    avg_loss:0.107, val_acc:0.940]
Epoch [65/120    avg_loss:0.097, val_acc:0.958]
Epoch [66/120    avg_loss:0.078, val_acc:0.959]
Epoch [67/120    avg_loss:0.067, val_acc:0.957]
Epoch [68/120    avg_loss:0.062, val_acc:0.957]
Epoch [69/120    avg_loss:0.067, val_acc:0.966]
Epoch [70/120    avg_loss:0.078, val_acc:0.951]
Epoch [71/120    avg_loss:0.065, val_acc:0.966]
Epoch [72/120    avg_loss:0.055, val_acc:0.966]
Epoch [73/120    avg_loss:0.063, val_acc:0.961]
Epoch [74/120    avg_loss:0.064, val_acc:0.970]
Epoch [75/120    avg_loss:0.058, val_acc:0.950]
Epoch [76/120    avg_loss:0.081, val_acc:0.958]
Epoch [77/120    avg_loss:0.059, val_acc:0.948]
Epoch [78/120    avg_loss:0.055, val_acc:0.968]
Epoch [79/120    avg_loss:0.048, val_acc:0.966]
Epoch [80/120    avg_loss:0.044, val_acc:0.967]
Epoch [81/120    avg_loss:0.034, val_acc:0.972]
Epoch [82/120    avg_loss:0.034, val_acc:0.966]
Epoch [83/120    avg_loss:0.045, val_acc:0.961]
Epoch [84/120    avg_loss:0.050, val_acc:0.947]
Epoch [85/120    avg_loss:0.051, val_acc:0.964]
Epoch [86/120    avg_loss:0.031, val_acc:0.966]
Epoch [87/120    avg_loss:0.031, val_acc:0.969]
Epoch [88/120    avg_loss:0.030, val_acc:0.951]
Epoch [89/120    avg_loss:0.027, val_acc:0.965]
Epoch [90/120    avg_loss:0.027, val_acc:0.965]
Epoch [91/120    avg_loss:0.027, val_acc:0.958]
Epoch [92/120    avg_loss:0.032, val_acc:0.976]
Epoch [93/120    avg_loss:0.032, val_acc:0.971]
Epoch [94/120    avg_loss:0.029, val_acc:0.980]
Epoch [95/120    avg_loss:0.024, val_acc:0.967]
Epoch [96/120    avg_loss:0.024, val_acc:0.975]
Epoch [97/120    avg_loss:0.021, val_acc:0.975]
Epoch [98/120    avg_loss:0.019, val_acc:0.977]
Epoch [99/120    avg_loss:0.022, val_acc:0.976]
Epoch [100/120    avg_loss:0.015, val_acc:0.978]
Epoch [101/120    avg_loss:0.016, val_acc:0.975]
Epoch [102/120    avg_loss:0.019, val_acc:0.970]
Epoch [103/120    avg_loss:0.019, val_acc:0.977]
Epoch [104/120    avg_loss:0.019, val_acc:0.972]
Epoch [105/120    avg_loss:0.016, val_acc:0.975]
Epoch [106/120    avg_loss:0.018, val_acc:0.976]
Epoch [107/120    avg_loss:0.019, val_acc:0.972]
Epoch [108/120    avg_loss:0.020, val_acc:0.976]
Epoch [109/120    avg_loss:0.015, val_acc:0.977]
Epoch [110/120    avg_loss:0.012, val_acc:0.978]
Epoch [111/120    avg_loss:0.013, val_acc:0.978]
Epoch [112/120    avg_loss:0.013, val_acc:0.979]
Epoch [113/120    avg_loss:0.012, val_acc:0.980]
Epoch [114/120    avg_loss:0.011, val_acc:0.979]
Epoch [115/120    avg_loss:0.014, val_acc:0.980]
Epoch [116/120    avg_loss:0.011, val_acc:0.979]
Epoch [117/120    avg_loss:0.013, val_acc:0.980]
Epoch [118/120    avg_loss:0.012, val_acc:0.981]
Epoch [119/120    avg_loss:0.012, val_acc:0.981]
Epoch [120/120    avg_loss:0.013, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0 1261    1    4    0    0    0    0    0    0   19    0    0
     0    0    0]
 [   0    0    2  723    2    0    0    0    0   10    0    7    1    2
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    1    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  850   21    0    0
     0    0    0]
 [   0    0    7    0    0    0    2    0    0    0    6 2182   13    0
     0    0    0]
 [   0    0    0    8    0    0    0    0    0    0    1    1  522    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1130    8    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    68  269    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.6910569105691

F1 scores:
[       nan 0.98765432 0.98554123 0.97636732 0.98139535 0.99539171
 0.98867925 1.         1.         0.75555556 0.98152425 0.98244034
 0.97206704 0.99462366 0.96498719 0.86217949 0.96969697]

Kappa:
0.973651849796563
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2a963a1e80>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.820, val_acc:0.040]
Epoch [2/120    avg_loss:2.771, val_acc:0.084]
Epoch [3/120    avg_loss:2.711, val_acc:0.182]
Epoch [4/120    avg_loss:2.672, val_acc:0.250]
Epoch [5/120    avg_loss:2.620, val_acc:0.260]
Epoch [6/120    avg_loss:2.569, val_acc:0.285]
Epoch [7/120    avg_loss:2.521, val_acc:0.328]
Epoch [8/120    avg_loss:2.500, val_acc:0.336]
Epoch [9/120    avg_loss:2.412, val_acc:0.352]
Epoch [10/120    avg_loss:2.383, val_acc:0.369]
Epoch [11/120    avg_loss:2.312, val_acc:0.396]
Epoch [12/120    avg_loss:2.250, val_acc:0.416]
Epoch [13/120    avg_loss:2.226, val_acc:0.431]
Epoch [14/120    avg_loss:2.173, val_acc:0.424]
Epoch [15/120    avg_loss:2.116, val_acc:0.438]
Epoch [16/120    avg_loss:2.050, val_acc:0.497]
Epoch [17/120    avg_loss:1.962, val_acc:0.471]
Epoch [18/120    avg_loss:1.898, val_acc:0.463]
Epoch [19/120    avg_loss:1.763, val_acc:0.525]
Epoch [20/120    avg_loss:1.676, val_acc:0.503]
Epoch [21/120    avg_loss:1.613, val_acc:0.597]
Epoch [22/120    avg_loss:1.525, val_acc:0.562]
Epoch [23/120    avg_loss:1.442, val_acc:0.582]
Epoch [24/120    avg_loss:1.341, val_acc:0.655]
Epoch [25/120    avg_loss:1.253, val_acc:0.592]
Epoch [26/120    avg_loss:1.132, val_acc:0.694]
Epoch [27/120    avg_loss:1.072, val_acc:0.637]
Epoch [28/120    avg_loss:1.017, val_acc:0.707]
Epoch [29/120    avg_loss:0.941, val_acc:0.728]
Epoch [30/120    avg_loss:0.876, val_acc:0.757]
Epoch [31/120    avg_loss:0.808, val_acc:0.733]
Epoch [32/120    avg_loss:0.710, val_acc:0.803]
Epoch [33/120    avg_loss:0.604, val_acc:0.804]
Epoch [34/120    avg_loss:0.538, val_acc:0.823]
Epoch [35/120    avg_loss:0.527, val_acc:0.823]
Epoch [36/120    avg_loss:0.441, val_acc:0.857]
Epoch [37/120    avg_loss:0.392, val_acc:0.887]
Epoch [38/120    avg_loss:0.371, val_acc:0.880]
Epoch [39/120    avg_loss:0.398, val_acc:0.872]
Epoch [40/120    avg_loss:0.333, val_acc:0.897]
Epoch [41/120    avg_loss:0.304, val_acc:0.899]
Epoch [42/120    avg_loss:0.274, val_acc:0.903]
Epoch [43/120    avg_loss:0.249, val_acc:0.908]
Epoch [44/120    avg_loss:0.248, val_acc:0.928]
Epoch [45/120    avg_loss:0.190, val_acc:0.934]
Epoch [46/120    avg_loss:0.191, val_acc:0.925]
Epoch [47/120    avg_loss:0.186, val_acc:0.938]
Epoch [48/120    avg_loss:0.189, val_acc:0.946]
Epoch [49/120    avg_loss:0.150, val_acc:0.935]
Epoch [50/120    avg_loss:0.135, val_acc:0.936]
Epoch [51/120    avg_loss:0.119, val_acc:0.939]
Epoch [52/120    avg_loss:0.125, val_acc:0.955]
Epoch [53/120    avg_loss:0.111, val_acc:0.945]
Epoch [54/120    avg_loss:0.106, val_acc:0.955]
Epoch [55/120    avg_loss:0.136, val_acc:0.921]
Epoch [56/120    avg_loss:0.144, val_acc:0.929]
Epoch [57/120    avg_loss:0.124, val_acc:0.940]
Epoch [58/120    avg_loss:0.104, val_acc:0.946]
Epoch [59/120    avg_loss:0.109, val_acc:0.942]
Epoch [60/120    avg_loss:0.090, val_acc:0.951]
Epoch [61/120    avg_loss:0.082, val_acc:0.958]
Epoch [62/120    avg_loss:0.081, val_acc:0.945]
Epoch [63/120    avg_loss:0.076, val_acc:0.947]
Epoch [64/120    avg_loss:0.071, val_acc:0.953]
Epoch [65/120    avg_loss:0.070, val_acc:0.963]
Epoch [66/120    avg_loss:0.064, val_acc:0.963]
Epoch [67/120    avg_loss:0.057, val_acc:0.968]
Epoch [68/120    avg_loss:0.052, val_acc:0.958]
Epoch [69/120    avg_loss:0.058, val_acc:0.967]
Epoch [70/120    avg_loss:0.054, val_acc:0.960]
Epoch [71/120    avg_loss:0.048, val_acc:0.959]
Epoch [72/120    avg_loss:0.049, val_acc:0.965]
Epoch [73/120    avg_loss:0.047, val_acc:0.969]
Epoch [74/120    avg_loss:0.043, val_acc:0.965]
Epoch [75/120    avg_loss:0.040, val_acc:0.969]
Epoch [76/120    avg_loss:0.035, val_acc:0.970]
Epoch [77/120    avg_loss:0.040, val_acc:0.970]
Epoch [78/120    avg_loss:0.082, val_acc:0.967]
Epoch [79/120    avg_loss:0.061, val_acc:0.957]
Epoch [80/120    avg_loss:0.044, val_acc:0.976]
Epoch [81/120    avg_loss:0.035, val_acc:0.961]
Epoch [82/120    avg_loss:0.036, val_acc:0.966]
Epoch [83/120    avg_loss:0.029, val_acc:0.969]
Epoch [84/120    avg_loss:0.033, val_acc:0.968]
Epoch [85/120    avg_loss:0.033, val_acc:0.970]
Epoch [86/120    avg_loss:0.029, val_acc:0.970]
Epoch [87/120    avg_loss:0.029, val_acc:0.972]
Epoch [88/120    avg_loss:0.026, val_acc:0.977]
Epoch [89/120    avg_loss:0.026, val_acc:0.976]
Epoch [90/120    avg_loss:0.023, val_acc:0.977]
Epoch [91/120    avg_loss:0.028, val_acc:0.973]
Epoch [92/120    avg_loss:0.029, val_acc:0.974]
Epoch [93/120    avg_loss:0.029, val_acc:0.978]
Epoch [94/120    avg_loss:0.020, val_acc:0.973]
Epoch [95/120    avg_loss:0.023, val_acc:0.974]
Epoch [96/120    avg_loss:0.023, val_acc:0.967]
Epoch [97/120    avg_loss:0.021, val_acc:0.975]
Epoch [98/120    avg_loss:0.021, val_acc:0.977]
Epoch [99/120    avg_loss:0.018, val_acc:0.978]
Epoch [100/120    avg_loss:0.017, val_acc:0.969]
Epoch [101/120    avg_loss:0.025, val_acc:0.967]
Epoch [102/120    avg_loss:0.020, val_acc:0.971]
Epoch [103/120    avg_loss:0.017, val_acc:0.978]
Epoch [104/120    avg_loss:0.018, val_acc:0.977]
Epoch [105/120    avg_loss:0.016, val_acc:0.975]
Epoch [106/120    avg_loss:0.019, val_acc:0.976]
Epoch [107/120    avg_loss:0.020, val_acc:0.971]
Epoch [108/120    avg_loss:0.017, val_acc:0.974]
Epoch [109/120    avg_loss:0.017, val_acc:0.971]
Epoch [110/120    avg_loss:0.018, val_acc:0.975]
Epoch [111/120    avg_loss:0.016, val_acc:0.978]
Epoch [112/120    avg_loss:0.016, val_acc:0.973]
Epoch [113/120    avg_loss:0.018, val_acc:0.971]
Epoch [114/120    avg_loss:0.017, val_acc:0.970]
Epoch [115/120    avg_loss:0.028, val_acc:0.974]
Epoch [116/120    avg_loss:0.015, val_acc:0.975]
Epoch [117/120    avg_loss:0.013, val_acc:0.976]
Epoch [118/120    avg_loss:0.017, val_acc:0.975]
Epoch [119/120    avg_loss:0.022, val_acc:0.971]
Epoch [120/120    avg_loss:0.026, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    0    0    0    0    0    0    0    0    6    0    0    0
     0    0    0]
 [   0    0 1263    2    3    0    0    0    0    0    7   10    0    0
     0    0    0]
 [   0    0    1  725    4    0    0    0    0    0    3    4   10    0
     0    0    0]
 [   0    0    0    4  205    0    0    0    0    0    0    1    3    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  653    0    0    0    1    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    5    0    0   13    0    0    0    0
     0    0    0]
 [   0    0   13    0    0    0    0    0    0    0  850   12    0    0
     0    0    0]
 [   0    0   39    0    0    0    0    0    0    0   38 2122   10    0
     0    1    0]
 [   0    0    0    0    0    0    0    0    0    0    9    0  521    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1128   10    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    65  268    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.83468834688347

F1 scores:
[       nan 0.92105263 0.97041875 0.98105548 0.96470588 0.99305556
 0.98195489 1.         0.997669   0.83870968 0.95025154 0.97272519
 0.96570899 1.         0.96534018 0.85350318 0.98224852]

Kappa:
0.9639148172670031
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdd1050eef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 38909==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.820, val_acc:0.117]
Epoch [2/120    avg_loss:2.762, val_acc:0.152]
Epoch [3/120    avg_loss:2.704, val_acc:0.221]
Epoch [4/120    avg_loss:2.662, val_acc:0.269]
Epoch [5/120    avg_loss:2.610, val_acc:0.335]
Epoch [6/120    avg_loss:2.552, val_acc:0.348]
Epoch [7/120    avg_loss:2.496, val_acc:0.344]
Epoch [8/120    avg_loss:2.483, val_acc:0.371]
Epoch [9/120    avg_loss:2.413, val_acc:0.382]
Epoch [10/120    avg_loss:2.387, val_acc:0.388]
Epoch [11/120    avg_loss:2.362, val_acc:0.397]
Epoch [12/120    avg_loss:2.304, val_acc:0.403]
Epoch [13/120    avg_loss:2.274, val_acc:0.416]
Epoch [14/120    avg_loss:2.200, val_acc:0.427]
Epoch [15/120    avg_loss:2.142, val_acc:0.451]
Epoch [16/120    avg_loss:2.036, val_acc:0.477]
Epoch [17/120    avg_loss:1.997, val_acc:0.491]
Epoch [18/120    avg_loss:1.913, val_acc:0.485]
Epoch [19/120    avg_loss:1.832, val_acc:0.512]
Epoch [20/120    avg_loss:1.768, val_acc:0.512]
Epoch [21/120    avg_loss:1.630, val_acc:0.554]
Epoch [22/120    avg_loss:1.541, val_acc:0.605]
Epoch [23/120    avg_loss:1.430, val_acc:0.613]
Epoch [24/120    avg_loss:1.341, val_acc:0.624]
Epoch [25/120    avg_loss:1.277, val_acc:0.613]
Epoch [26/120    avg_loss:1.140, val_acc:0.662]
Epoch [27/120    avg_loss:1.056, val_acc:0.692]
Epoch [28/120    avg_loss:0.962, val_acc:0.709]
Epoch [29/120    avg_loss:0.941, val_acc:0.702]
Epoch [30/120    avg_loss:0.834, val_acc:0.776]
Epoch [31/120    avg_loss:0.709, val_acc:0.770]
Epoch [32/120    avg_loss:0.673, val_acc:0.820]
Epoch [33/120    avg_loss:0.573, val_acc:0.828]
Epoch [34/120    avg_loss:0.488, val_acc:0.855]
Epoch [35/120    avg_loss:0.500, val_acc:0.877]
Epoch [36/120    avg_loss:0.448, val_acc:0.879]
Epoch [37/120    avg_loss:0.398, val_acc:0.874]
Epoch [38/120    avg_loss:0.353, val_acc:0.904]
Epoch [39/120    avg_loss:0.320, val_acc:0.921]
Epoch [40/120    avg_loss:0.281, val_acc:0.919]
Epoch [41/120    avg_loss:0.260, val_acc:0.919]
Epoch [42/120    avg_loss:0.246, val_acc:0.911]
Epoch [43/120    avg_loss:0.237, val_acc:0.923]
Epoch [44/120    avg_loss:0.235, val_acc:0.925]
Epoch [45/120    avg_loss:0.271, val_acc:0.931]
Epoch [46/120    avg_loss:0.215, val_acc:0.927]
Epoch [47/120    avg_loss:0.163, val_acc:0.941]
Epoch [48/120    avg_loss:0.164, val_acc:0.933]
Epoch [49/120    avg_loss:0.155, val_acc:0.929]
Epoch [50/120    avg_loss:0.129, val_acc:0.952]
Epoch [51/120    avg_loss:0.130, val_acc:0.952]
Epoch [52/120    avg_loss:0.109, val_acc:0.946]
Epoch [53/120    avg_loss:0.109, val_acc:0.939]
Epoch [54/120    avg_loss:0.110, val_acc:0.951]
Epoch [55/120    avg_loss:0.106, val_acc:0.956]
Epoch [56/120    avg_loss:0.095, val_acc:0.961]
Epoch [57/120    avg_loss:0.093, val_acc:0.941]
Epoch [58/120    avg_loss:0.111, val_acc:0.955]
Epoch [59/120    avg_loss:0.108, val_acc:0.952]
Epoch [60/120    avg_loss:0.081, val_acc:0.958]
Epoch [61/120    avg_loss:0.071, val_acc:0.960]
Epoch [62/120    avg_loss:0.065, val_acc:0.955]
Epoch [63/120    avg_loss:0.077, val_acc:0.966]
Epoch [64/120    avg_loss:0.065, val_acc:0.967]
Epoch [65/120    avg_loss:0.060, val_acc:0.963]
Epoch [66/120    avg_loss:0.065, val_acc:0.963]
Epoch [67/120    avg_loss:0.060, val_acc:0.964]
Epoch [68/120    avg_loss:0.059, val_acc:0.967]
Epoch [69/120    avg_loss:0.045, val_acc:0.971]
Epoch [70/120    avg_loss:0.048, val_acc:0.964]
Epoch [71/120    avg_loss:0.045, val_acc:0.959]
Epoch [72/120    avg_loss:0.045, val_acc:0.955]
Epoch [73/120    avg_loss:0.046, val_acc:0.968]
Epoch [74/120    avg_loss:0.038, val_acc:0.960]
Epoch [75/120    avg_loss:0.045, val_acc:0.960]
Epoch [76/120    avg_loss:0.043, val_acc:0.967]
Epoch [77/120    avg_loss:0.033, val_acc:0.966]
Epoch [78/120    avg_loss:0.034, val_acc:0.969]
Epoch [79/120    avg_loss:0.037, val_acc:0.966]
Epoch [80/120    avg_loss:0.037, val_acc:0.965]
Epoch [81/120    avg_loss:0.035, val_acc:0.970]
Epoch [82/120    avg_loss:0.031, val_acc:0.974]
Epoch [83/120    avg_loss:0.035, val_acc:0.971]
Epoch [84/120    avg_loss:0.030, val_acc:0.969]
Epoch [85/120    avg_loss:0.027, val_acc:0.968]
Epoch [86/120    avg_loss:0.031, val_acc:0.968]
Epoch [87/120    avg_loss:0.027, val_acc:0.973]
Epoch [88/120    avg_loss:0.031, val_acc:0.966]
Epoch [89/120    avg_loss:0.029, val_acc:0.968]
Epoch [90/120    avg_loss:0.026, val_acc:0.969]
Epoch [91/120    avg_loss:0.023, val_acc:0.971]
Epoch [92/120    avg_loss:0.025, val_acc:0.976]
Epoch [93/120    avg_loss:0.026, val_acc:0.969]
Epoch [94/120    avg_loss:0.028, val_acc:0.970]
Epoch [95/120    avg_loss:0.025, val_acc:0.973]
Epoch [96/120    avg_loss:0.029, val_acc:0.965]
Epoch [97/120    avg_loss:0.031, val_acc:0.976]
Epoch [98/120    avg_loss:0.031, val_acc:0.969]
Epoch [99/120    avg_loss:0.027, val_acc:0.975]
Epoch [100/120    avg_loss:0.029, val_acc:0.967]
Epoch [101/120    avg_loss:0.056, val_acc:0.954]
Epoch [102/120    avg_loss:0.075, val_acc:0.968]
Epoch [103/120    avg_loss:0.089, val_acc:0.961]
Epoch [104/120    avg_loss:0.069, val_acc:0.959]
Epoch [105/120    avg_loss:0.046, val_acc:0.973]
Epoch [106/120    avg_loss:0.035, val_acc:0.969]
Epoch [107/120    avg_loss:0.035, val_acc:0.970]
Epoch [108/120    avg_loss:0.029, val_acc:0.974]
Epoch [109/120    avg_loss:0.025, val_acc:0.973]
Epoch [110/120    avg_loss:0.028, val_acc:0.963]
Epoch [111/120    avg_loss:0.025, val_acc:0.971]
Epoch [112/120    avg_loss:0.019, val_acc:0.973]
Epoch [113/120    avg_loss:0.019, val_acc:0.971]
Epoch [114/120    avg_loss:0.017, val_acc:0.972]
Epoch [115/120    avg_loss:0.017, val_acc:0.973]
Epoch [116/120    avg_loss:0.016, val_acc:0.972]
Epoch [117/120    avg_loss:0.019, val_acc:0.974]
Epoch [118/120    avg_loss:0.018, val_acc:0.974]
Epoch [119/120    avg_loss:0.016, val_acc:0.974]
Epoch [120/120    avg_loss:0.017, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1255    5    4    0    0    0    0    0    6   15    0    0
     0    0    0]
 [   0    0    0  722   12    1    2    0    0    2    0    0    4    2
     1    1    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    0  843   22    0    0
     2    1    0]
 [   0    0   11    0    0    0    2    0    0    0   11 2180    5    0
     1    0    0]
 [   0    0    2    3    0    1    0    0    0    0    2    0  516    0
     5    1    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1132    7    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    98  247    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.21409214092141

F1 scores:
[       nan 0.975      0.98046875 0.97699594 0.96145125 0.99076212
 0.99469295 0.98039216 0.997669   0.94736842 0.96952271 0.98442086
 0.96992481 0.99462366 0.9504618  0.81788079 0.95857988]

Kappa:
0.9682062324675474
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1309890f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.826, val_acc:0.073]
Epoch [2/120    avg_loss:2.766, val_acc:0.125]
Epoch [3/120    avg_loss:2.722, val_acc:0.204]
Epoch [4/120    avg_loss:2.685, val_acc:0.242]
Epoch [5/120    avg_loss:2.635, val_acc:0.314]
Epoch [6/120    avg_loss:2.586, val_acc:0.361]
Epoch [7/120    avg_loss:2.505, val_acc:0.399]
Epoch [8/120    avg_loss:2.468, val_acc:0.421]
Epoch [9/120    avg_loss:2.420, val_acc:0.459]
Epoch [10/120    avg_loss:2.366, val_acc:0.479]
Epoch [11/120    avg_loss:2.287, val_acc:0.516]
Epoch [12/120    avg_loss:2.259, val_acc:0.517]
Epoch [13/120    avg_loss:2.188, val_acc:0.521]
Epoch [14/120    avg_loss:2.133, val_acc:0.564]
Epoch [15/120    avg_loss:2.043, val_acc:0.526]
Epoch [16/120    avg_loss:1.961, val_acc:0.543]
Epoch [17/120    avg_loss:1.876, val_acc:0.582]
Epoch [18/120    avg_loss:1.807, val_acc:0.579]
Epoch [19/120    avg_loss:1.713, val_acc:0.614]
Epoch [20/120    avg_loss:1.626, val_acc:0.636]
Epoch [21/120    avg_loss:1.525, val_acc:0.636]
Epoch [22/120    avg_loss:1.447, val_acc:0.674]
Epoch [23/120    avg_loss:1.360, val_acc:0.668]
Epoch [24/120    avg_loss:1.309, val_acc:0.675]
Epoch [25/120    avg_loss:1.188, val_acc:0.716]
Epoch [26/120    avg_loss:1.086, val_acc:0.688]
Epoch [27/120    avg_loss:1.039, val_acc:0.704]
Epoch [28/120    avg_loss:0.914, val_acc:0.742]
Epoch [29/120    avg_loss:0.870, val_acc:0.738]
Epoch [30/120    avg_loss:0.814, val_acc:0.801]
Epoch [31/120    avg_loss:0.744, val_acc:0.813]
Epoch [32/120    avg_loss:0.672, val_acc:0.823]
Epoch [33/120    avg_loss:0.592, val_acc:0.840]
Epoch [34/120    avg_loss:0.595, val_acc:0.828]
Epoch [35/120    avg_loss:0.534, val_acc:0.818]
Epoch [36/120    avg_loss:0.494, val_acc:0.863]
Epoch [37/120    avg_loss:0.477, val_acc:0.847]
Epoch [38/120    avg_loss:0.435, val_acc:0.870]
Epoch [39/120    avg_loss:0.389, val_acc:0.883]
Epoch [40/120    avg_loss:0.327, val_acc:0.899]
Epoch [41/120    avg_loss:0.322, val_acc:0.887]
Epoch [42/120    avg_loss:0.278, val_acc:0.924]
Epoch [43/120    avg_loss:0.233, val_acc:0.927]
Epoch [44/120    avg_loss:0.211, val_acc:0.925]
Epoch [45/120    avg_loss:0.207, val_acc:0.938]
Epoch [46/120    avg_loss:0.195, val_acc:0.938]
Epoch [47/120    avg_loss:0.170, val_acc:0.930]
Epoch [48/120    avg_loss:0.171, val_acc:0.938]
Epoch [49/120    avg_loss:0.149, val_acc:0.945]
Epoch [50/120    avg_loss:0.143, val_acc:0.947]
Epoch [51/120    avg_loss:0.127, val_acc:0.950]
Epoch [52/120    avg_loss:0.139, val_acc:0.942]
Epoch [53/120    avg_loss:0.120, val_acc:0.946]
Epoch [54/120    avg_loss:0.108, val_acc:0.952]
Epoch [55/120    avg_loss:0.110, val_acc:0.936]
Epoch [56/120    avg_loss:0.113, val_acc:0.957]
Epoch [57/120    avg_loss:0.094, val_acc:0.958]
Epoch [58/120    avg_loss:0.094, val_acc:0.946]
Epoch [59/120    avg_loss:0.094, val_acc:0.960]
Epoch [60/120    avg_loss:0.085, val_acc:0.961]
Epoch [61/120    avg_loss:0.082, val_acc:0.970]
Epoch [62/120    avg_loss:0.082, val_acc:0.961]
Epoch [63/120    avg_loss:0.082, val_acc:0.953]
Epoch [64/120    avg_loss:0.080, val_acc:0.970]
Epoch [65/120    avg_loss:0.068, val_acc:0.962]
Epoch [66/120    avg_loss:0.076, val_acc:0.964]
Epoch [67/120    avg_loss:0.074, val_acc:0.949]
Epoch [68/120    avg_loss:0.060, val_acc:0.962]
Epoch [69/120    avg_loss:0.064, val_acc:0.955]
Epoch [70/120    avg_loss:0.053, val_acc:0.963]
Epoch [71/120    avg_loss:0.052, val_acc:0.967]
Epoch [72/120    avg_loss:0.055, val_acc:0.966]
Epoch [73/120    avg_loss:0.048, val_acc:0.975]
Epoch [74/120    avg_loss:0.046, val_acc:0.967]
Epoch [75/120    avg_loss:0.042, val_acc:0.975]
Epoch [76/120    avg_loss:0.047, val_acc:0.966]
Epoch [77/120    avg_loss:0.044, val_acc:0.974]
Epoch [78/120    avg_loss:0.034, val_acc:0.972]
Epoch [79/120    avg_loss:0.035, val_acc:0.973]
Epoch [80/120    avg_loss:0.037, val_acc:0.972]
Epoch [81/120    avg_loss:0.029, val_acc:0.980]
Epoch [82/120    avg_loss:0.027, val_acc:0.978]
Epoch [83/120    avg_loss:0.032, val_acc:0.984]
Epoch [84/120    avg_loss:0.029, val_acc:0.978]
Epoch [85/120    avg_loss:0.027, val_acc:0.980]
Epoch [86/120    avg_loss:0.035, val_acc:0.979]
Epoch [87/120    avg_loss:0.031, val_acc:0.976]
Epoch [88/120    avg_loss:0.028, val_acc:0.978]
Epoch [89/120    avg_loss:0.025, val_acc:0.977]
Epoch [90/120    avg_loss:0.023, val_acc:0.979]
Epoch [91/120    avg_loss:0.020, val_acc:0.987]
Epoch [92/120    avg_loss:0.021, val_acc:0.979]
Epoch [93/120    avg_loss:0.023, val_acc:0.984]
Epoch [94/120    avg_loss:0.020, val_acc:0.979]
Epoch [95/120    avg_loss:0.020, val_acc:0.983]
Epoch [96/120    avg_loss:0.018, val_acc:0.978]
Epoch [97/120    avg_loss:0.018, val_acc:0.975]
Epoch [98/120    avg_loss:0.025, val_acc:0.979]
Epoch [99/120    avg_loss:0.018, val_acc:0.979]
Epoch [100/120    avg_loss:0.020, val_acc:0.973]
Epoch [101/120    avg_loss:0.026, val_acc:0.972]
Epoch [102/120    avg_loss:0.023, val_acc:0.978]
Epoch [103/120    avg_loss:0.017, val_acc:0.977]
Epoch [104/120    avg_loss:0.017, val_acc:0.977]
Epoch [105/120    avg_loss:0.015, val_acc:0.983]
Epoch [106/120    avg_loss:0.013, val_acc:0.983]
Epoch [107/120    avg_loss:0.013, val_acc:0.983]
Epoch [108/120    avg_loss:0.014, val_acc:0.984]
Epoch [109/120    avg_loss:0.014, val_acc:0.985]
Epoch [110/120    avg_loss:0.013, val_acc:0.984]
Epoch [111/120    avg_loss:0.014, val_acc:0.984]
Epoch [112/120    avg_loss:0.013, val_acc:0.984]
Epoch [113/120    avg_loss:0.013, val_acc:0.983]
Epoch [114/120    avg_loss:0.011, val_acc:0.983]
Epoch [115/120    avg_loss:0.014, val_acc:0.984]
Epoch [116/120    avg_loss:0.012, val_acc:0.984]
Epoch [117/120    avg_loss:0.013, val_acc:0.983]
Epoch [118/120    avg_loss:0.014, val_acc:0.983]
Epoch [119/120    avg_loss:0.013, val_acc:0.983]
Epoch [120/120    avg_loss:0.012, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0 1246    2    3    0    1    0    0    0    2   31    0    0
     0    0    0]
 [   0    0    4  722    0    0    0    0    0    2    3    3   12    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  856    9    0    0
     0    5    0]
 [   0    0    2    0    0    1    2    0    0    0   13 2181   11    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    3    0  528    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    18  329    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.36314363143632

F1 scores:
[       nan 0.98765432 0.98033045 0.97964722 0.99300699 0.99885189
 0.9977221  1.         0.9953271  0.91891892 0.97716895 0.98376184
 0.96880734 0.99730458 0.98819414 0.95362319 0.99408284]

Kappa:
0.9813373631061637
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4245928f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.829, val_acc:0.036]
Epoch [2/120    avg_loss:2.792, val_acc:0.093]
Epoch [3/120    avg_loss:2.758, val_acc:0.193]
Epoch [4/120    avg_loss:2.719, val_acc:0.277]
Epoch [5/120    avg_loss:2.666, val_acc:0.309]
Epoch [6/120    avg_loss:2.620, val_acc:0.325]
Epoch [7/120    avg_loss:2.566, val_acc:0.324]
Epoch [8/120    avg_loss:2.507, val_acc:0.346]
Epoch [9/120    avg_loss:2.474, val_acc:0.371]
Epoch [10/120    avg_loss:2.403, val_acc:0.343]
Epoch [11/120    avg_loss:2.375, val_acc:0.360]
Epoch [12/120    avg_loss:2.324, val_acc:0.412]
Epoch [13/120    avg_loss:2.246, val_acc:0.414]
Epoch [14/120    avg_loss:2.200, val_acc:0.420]
Epoch [15/120    avg_loss:2.112, val_acc:0.406]
Epoch [16/120    avg_loss:2.024, val_acc:0.420]
Epoch [17/120    avg_loss:1.966, val_acc:0.464]
Epoch [18/120    avg_loss:1.852, val_acc:0.438]
Epoch [19/120    avg_loss:1.772, val_acc:0.483]
Epoch [20/120    avg_loss:1.631, val_acc:0.595]
Epoch [21/120    avg_loss:1.548, val_acc:0.540]
Epoch [22/120    avg_loss:1.502, val_acc:0.504]
Epoch [23/120    avg_loss:1.393, val_acc:0.589]
Epoch [24/120    avg_loss:1.293, val_acc:0.564]
Epoch [25/120    avg_loss:1.169, val_acc:0.633]
Epoch [26/120    avg_loss:1.134, val_acc:0.665]
Epoch [27/120    avg_loss:1.007, val_acc:0.639]
Epoch [28/120    avg_loss:0.939, val_acc:0.699]
Epoch [29/120    avg_loss:0.873, val_acc:0.695]
Epoch [30/120    avg_loss:0.813, val_acc:0.696]
Epoch [31/120    avg_loss:0.734, val_acc:0.746]
Epoch [32/120    avg_loss:0.697, val_acc:0.736]
Epoch [33/120    avg_loss:0.674, val_acc:0.760]
Epoch [34/120    avg_loss:0.585, val_acc:0.790]
Epoch [35/120    avg_loss:0.536, val_acc:0.798]
Epoch [36/120    avg_loss:0.515, val_acc:0.824]
Epoch [37/120    avg_loss:0.465, val_acc:0.833]
Epoch [38/120    avg_loss:0.451, val_acc:0.815]
Epoch [39/120    avg_loss:0.417, val_acc:0.832]
Epoch [40/120    avg_loss:0.401, val_acc:0.854]
Epoch [41/120    avg_loss:0.345, val_acc:0.871]
Epoch [42/120    avg_loss:0.330, val_acc:0.860]
Epoch [43/120    avg_loss:0.300, val_acc:0.873]
Epoch [44/120    avg_loss:0.275, val_acc:0.902]
Epoch [45/120    avg_loss:0.272, val_acc:0.887]
Epoch [46/120    avg_loss:0.240, val_acc:0.890]
Epoch [47/120    avg_loss:0.229, val_acc:0.905]
Epoch [48/120    avg_loss:0.210, val_acc:0.906]
Epoch [49/120    avg_loss:0.206, val_acc:0.915]
Epoch [50/120    avg_loss:0.195, val_acc:0.914]
Epoch [51/120    avg_loss:0.173, val_acc:0.932]
Epoch [52/120    avg_loss:0.169, val_acc:0.933]
Epoch [53/120    avg_loss:0.163, val_acc:0.926]
Epoch [54/120    avg_loss:0.148, val_acc:0.941]
Epoch [55/120    avg_loss:0.130, val_acc:0.935]
Epoch [56/120    avg_loss:0.127, val_acc:0.953]
Epoch [57/120    avg_loss:0.121, val_acc:0.940]
Epoch [58/120    avg_loss:0.122, val_acc:0.945]
Epoch [59/120    avg_loss:0.138, val_acc:0.943]
Epoch [60/120    avg_loss:0.208, val_acc:0.915]
Epoch [61/120    avg_loss:0.276, val_acc:0.898]
Epoch [62/120    avg_loss:0.242, val_acc:0.932]
Epoch [63/120    avg_loss:0.165, val_acc:0.936]
Epoch [64/120    avg_loss:0.156, val_acc:0.936]
Epoch [65/120    avg_loss:0.137, val_acc:0.936]
Epoch [66/120    avg_loss:0.110, val_acc:0.933]
Epoch [67/120    avg_loss:0.110, val_acc:0.938]
Epoch [68/120    avg_loss:0.097, val_acc:0.952]
Epoch [69/120    avg_loss:0.080, val_acc:0.964]
Epoch [70/120    avg_loss:0.081, val_acc:0.955]
Epoch [71/120    avg_loss:0.076, val_acc:0.945]
Epoch [72/120    avg_loss:0.071, val_acc:0.954]
Epoch [73/120    avg_loss:0.062, val_acc:0.956]
Epoch [74/120    avg_loss:0.053, val_acc:0.969]
Epoch [75/120    avg_loss:0.045, val_acc:0.972]
Epoch [76/120    avg_loss:0.068, val_acc:0.964]
Epoch [77/120    avg_loss:0.076, val_acc:0.960]
Epoch [78/120    avg_loss:0.066, val_acc:0.954]
Epoch [79/120    avg_loss:0.053, val_acc:0.965]
Epoch [80/120    avg_loss:0.047, val_acc:0.961]
Epoch [81/120    avg_loss:0.058, val_acc:0.930]
Epoch [82/120    avg_loss:0.051, val_acc:0.960]
Epoch [83/120    avg_loss:0.046, val_acc:0.946]
Epoch [84/120    avg_loss:0.045, val_acc:0.968]
Epoch [85/120    avg_loss:0.037, val_acc:0.968]
Epoch [86/120    avg_loss:0.031, val_acc:0.970]
Epoch [87/120    avg_loss:0.032, val_acc:0.969]
Epoch [88/120    avg_loss:0.028, val_acc:0.971]
Epoch [89/120    avg_loss:0.028, val_acc:0.972]
Epoch [90/120    avg_loss:0.025, val_acc:0.973]
Epoch [91/120    avg_loss:0.028, val_acc:0.974]
Epoch [92/120    avg_loss:0.027, val_acc:0.972]
Epoch [93/120    avg_loss:0.025, val_acc:0.970]
Epoch [94/120    avg_loss:0.026, val_acc:0.973]
Epoch [95/120    avg_loss:0.024, val_acc:0.972]
Epoch [96/120    avg_loss:0.024, val_acc:0.973]
Epoch [97/120    avg_loss:0.025, val_acc:0.972]
Epoch [98/120    avg_loss:0.027, val_acc:0.972]
Epoch [99/120    avg_loss:0.024, val_acc:0.973]
Epoch [100/120    avg_loss:0.025, val_acc:0.976]
Epoch [101/120    avg_loss:0.022, val_acc:0.975]
Epoch [102/120    avg_loss:0.026, val_acc:0.974]
Epoch [103/120    avg_loss:0.024, val_acc:0.972]
Epoch [104/120    avg_loss:0.025, val_acc:0.973]
Epoch [105/120    avg_loss:0.026, val_acc:0.974]
Epoch [106/120    avg_loss:0.024, val_acc:0.971]
Epoch [107/120    avg_loss:0.024, val_acc:0.971]
Epoch [108/120    avg_loss:0.024, val_acc:0.974]
Epoch [109/120    avg_loss:0.023, val_acc:0.973]
Epoch [110/120    avg_loss:0.023, val_acc:0.973]
Epoch [111/120    avg_loss:0.022, val_acc:0.973]
Epoch [112/120    avg_loss:0.024, val_acc:0.974]
Epoch [113/120    avg_loss:0.022, val_acc:0.973]
Epoch [114/120    avg_loss:0.025, val_acc:0.973]
Epoch [115/120    avg_loss:0.025, val_acc:0.973]
Epoch [116/120    avg_loss:0.023, val_acc:0.974]
Epoch [117/120    avg_loss:0.023, val_acc:0.974]
Epoch [118/120    avg_loss:0.022, val_acc:0.974]
Epoch [119/120    avg_loss:0.021, val_acc:0.974]
Epoch [120/120    avg_loss:0.022, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1252    1    4    1    1    0    0    0    1   23    0    0
     0    2    0]
 [   0    0    2  708   10    0    0    0    0    2    1    6   18    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    1    4    0    0    0  849   16    0    0
     0    2    0]
 [   0    0   13    0    0    0    3    0    0    0   15 2177    2    0
     0    0    0]
 [   0    0    0    1    0    2    0    0    0    0    6    3  515    0
     0    3    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    14  333    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.13550135501355

F1 scores:
[       nan 0.92857143 0.98003914 0.97185999 0.96818182 0.99427262
 0.99318698 1.         0.9953271  0.94736842 0.97084048 0.98173619
 0.96351731 1.         0.99302528 0.96943231 0.97674419]

Kappa:
0.9787403709302154
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdbf77cefd0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.824, val_acc:0.010]
Epoch [2/120    avg_loss:2.784, val_acc:0.059]
Epoch [3/120    avg_loss:2.731, val_acc:0.060]
Epoch [4/120    avg_loss:2.708, val_acc:0.089]
Epoch [5/120    avg_loss:2.652, val_acc:0.241]
Epoch [6/120    avg_loss:2.615, val_acc:0.309]
Epoch [7/120    avg_loss:2.599, val_acc:0.351]
Epoch [8/120    avg_loss:2.555, val_acc:0.369]
Epoch [9/120    avg_loss:2.473, val_acc:0.385]
Epoch [10/120    avg_loss:2.435, val_acc:0.402]
Epoch [11/120    avg_loss:2.405, val_acc:0.409]
Epoch [12/120    avg_loss:2.365, val_acc:0.441]
Epoch [13/120    avg_loss:2.317, val_acc:0.470]
Epoch [14/120    avg_loss:2.264, val_acc:0.486]
Epoch [15/120    avg_loss:2.256, val_acc:0.480]
Epoch [16/120    avg_loss:2.178, val_acc:0.505]
Epoch [17/120    avg_loss:2.172, val_acc:0.521]
Epoch [18/120    avg_loss:2.128, val_acc:0.534]
Epoch [19/120    avg_loss:2.050, val_acc:0.549]
Epoch [20/120    avg_loss:2.039, val_acc:0.561]
Epoch [21/120    avg_loss:2.015, val_acc:0.566]
Epoch [22/120    avg_loss:1.948, val_acc:0.585]
Epoch [23/120    avg_loss:1.865, val_acc:0.598]
Epoch [24/120    avg_loss:1.796, val_acc:0.602]
Epoch [25/120    avg_loss:1.765, val_acc:0.628]
Epoch [26/120    avg_loss:1.669, val_acc:0.644]
Epoch [27/120    avg_loss:1.565, val_acc:0.701]
Epoch [28/120    avg_loss:1.401, val_acc:0.736]
Epoch [29/120    avg_loss:1.399, val_acc:0.707]
Epoch [30/120    avg_loss:1.243, val_acc:0.769]
Epoch [31/120    avg_loss:1.095, val_acc:0.779]
Epoch [32/120    avg_loss:0.991, val_acc:0.771]
Epoch [33/120    avg_loss:0.899, val_acc:0.783]
Epoch [34/120    avg_loss:0.892, val_acc:0.791]
Epoch [35/120    avg_loss:0.850, val_acc:0.775]
Epoch [36/120    avg_loss:0.768, val_acc:0.838]
Epoch [37/120    avg_loss:0.662, val_acc:0.845]
Epoch [38/120    avg_loss:0.603, val_acc:0.875]
Epoch [39/120    avg_loss:0.565, val_acc:0.846]
Epoch [40/120    avg_loss:0.517, val_acc:0.874]
Epoch [41/120    avg_loss:0.440, val_acc:0.891]
Epoch [42/120    avg_loss:0.404, val_acc:0.889]
Epoch [43/120    avg_loss:0.371, val_acc:0.903]
Epoch [44/120    avg_loss:0.328, val_acc:0.905]
Epoch [45/120    avg_loss:0.316, val_acc:0.893]
Epoch [46/120    avg_loss:0.277, val_acc:0.892]
Epoch [47/120    avg_loss:0.264, val_acc:0.911]
Epoch [48/120    avg_loss:0.241, val_acc:0.932]
Epoch [49/120    avg_loss:0.210, val_acc:0.929]
Epoch [50/120    avg_loss:0.180, val_acc:0.928]
Epoch [51/120    avg_loss:0.252, val_acc:0.931]
Epoch [52/120    avg_loss:0.221, val_acc:0.912]
Epoch [53/120    avg_loss:0.201, val_acc:0.924]
Epoch [54/120    avg_loss:0.171, val_acc:0.922]
Epoch [55/120    avg_loss:0.160, val_acc:0.926]
Epoch [56/120    avg_loss:0.176, val_acc:0.939]
Epoch [57/120    avg_loss:0.151, val_acc:0.939]
Epoch [58/120    avg_loss:0.128, val_acc:0.948]
Epoch [59/120    avg_loss:0.117, val_acc:0.949]
Epoch [60/120    avg_loss:0.113, val_acc:0.952]
Epoch [61/120    avg_loss:0.102, val_acc:0.954]
Epoch [62/120    avg_loss:0.093, val_acc:0.953]
Epoch [63/120    avg_loss:0.085, val_acc:0.964]
Epoch [64/120    avg_loss:0.075, val_acc:0.959]
Epoch [65/120    avg_loss:0.100, val_acc:0.957]
Epoch [66/120    avg_loss:0.133, val_acc:0.926]
Epoch [67/120    avg_loss:0.119, val_acc:0.936]
Epoch [68/120    avg_loss:0.095, val_acc:0.949]
Epoch [69/120    avg_loss:0.095, val_acc:0.958]
Epoch [70/120    avg_loss:0.096, val_acc:0.951]
Epoch [71/120    avg_loss:0.075, val_acc:0.964]
Epoch [72/120    avg_loss:0.079, val_acc:0.941]
Epoch [73/120    avg_loss:0.066, val_acc:0.968]
Epoch [74/120    avg_loss:0.054, val_acc:0.971]
Epoch [75/120    avg_loss:0.051, val_acc:0.965]
Epoch [76/120    avg_loss:0.046, val_acc:0.966]
Epoch [77/120    avg_loss:0.048, val_acc:0.967]
Epoch [78/120    avg_loss:0.048, val_acc:0.969]
Epoch [79/120    avg_loss:0.042, val_acc:0.968]
Epoch [80/120    avg_loss:0.039, val_acc:0.969]
Epoch [81/120    avg_loss:0.043, val_acc:0.975]
Epoch [82/120    avg_loss:0.044, val_acc:0.968]
Epoch [83/120    avg_loss:0.048, val_acc:0.972]
Epoch [84/120    avg_loss:0.038, val_acc:0.970]
Epoch [85/120    avg_loss:0.052, val_acc:0.969]
Epoch [86/120    avg_loss:0.039, val_acc:0.979]
Epoch [87/120    avg_loss:0.032, val_acc:0.968]
Epoch [88/120    avg_loss:0.032, val_acc:0.967]
Epoch [89/120    avg_loss:0.031, val_acc:0.975]
Epoch [90/120    avg_loss:0.029, val_acc:0.963]
Epoch [91/120    avg_loss:0.033, val_acc:0.974]
Epoch [92/120    avg_loss:0.028, val_acc:0.976]
Epoch [93/120    avg_loss:0.031, val_acc:0.981]
Epoch [94/120    avg_loss:0.030, val_acc:0.974]
Epoch [95/120    avg_loss:0.027, val_acc:0.978]
Epoch [96/120    avg_loss:0.028, val_acc:0.979]
Epoch [97/120    avg_loss:0.021, val_acc:0.980]
Epoch [98/120    avg_loss:0.025, val_acc:0.975]
Epoch [99/120    avg_loss:0.022, val_acc:0.972]
Epoch [100/120    avg_loss:0.024, val_acc:0.978]
Epoch [101/120    avg_loss:0.021, val_acc:0.976]
Epoch [102/120    avg_loss:0.025, val_acc:0.981]
Epoch [103/120    avg_loss:0.023, val_acc:0.979]
Epoch [104/120    avg_loss:0.024, val_acc:0.978]
Epoch [105/120    avg_loss:0.027, val_acc:0.981]
Epoch [106/120    avg_loss:0.028, val_acc:0.972]
Epoch [107/120    avg_loss:0.025, val_acc:0.976]
Epoch [108/120    avg_loss:0.029, val_acc:0.979]
Epoch [109/120    avg_loss:0.023, val_acc:0.978]
Epoch [110/120    avg_loss:1.442, val_acc:0.771]
Epoch [111/120    avg_loss:1.333, val_acc:0.616]
Epoch [112/120    avg_loss:0.984, val_acc:0.710]
Epoch [113/120    avg_loss:0.759, val_acc:0.733]
Epoch [114/120    avg_loss:0.618, val_acc:0.768]
Epoch [115/120    avg_loss:0.510, val_acc:0.787]
Epoch [116/120    avg_loss:0.395, val_acc:0.814]
Epoch [117/120    avg_loss:0.362, val_acc:0.834]
Epoch [118/120    avg_loss:0.320, val_acc:0.850]
Epoch [119/120    avg_loss:0.254, val_acc:0.878]
Epoch [120/120    avg_loss:0.232, val_acc:0.885]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1019   40   35    0    0    0    0    1   98   77   12    0
     3    0    0]
 [   0    0    6  638   24    0    1    0    0    2    4   68    4    0
     0    0    0]
 [   0    0    1    2  208    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  394    0    0    0    0    0    0    0    0
    41    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   17    0    0    0    0    0    0  413    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   33   24    8    1    1    0    0    7  758   41    2    0
     0    0    0]
 [   0    0   95  138   35    0    0    0    0    0   45 1820   73    1
     1    0    2]
 [   0    0    2   23    0    0    0    0    0    0    9    0  497    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1122   17    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    87  248    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
88.0650406504065

F1 scores:
[       nan 0.82828283 0.83490373 0.79156328 0.79541109 0.94939759
 0.98793363 1.         0.97983393 0.7826087  0.84740078 0.86337761
 0.88355556 0.99730458 0.93695198 0.80781759 0.97647059]

Kappa:
0.8645716803756052
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2c89247ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.817, val_acc:0.009]
Epoch [2/120    avg_loss:2.786, val_acc:0.009]
Epoch [3/120    avg_loss:2.745, val_acc:0.043]
Epoch [4/120    avg_loss:2.677, val_acc:0.082]
Epoch [5/120    avg_loss:2.644, val_acc:0.120]
Epoch [6/120    avg_loss:2.591, val_acc:0.211]
Epoch [7/120    avg_loss:2.558, val_acc:0.261]
Epoch [8/120    avg_loss:2.534, val_acc:0.341]
Epoch [9/120    avg_loss:2.488, val_acc:0.408]
Epoch [10/120    avg_loss:2.430, val_acc:0.422]
Epoch [11/120    avg_loss:2.393, val_acc:0.455]
Epoch [12/120    avg_loss:2.400, val_acc:0.476]
Epoch [13/120    avg_loss:2.326, val_acc:0.502]
Epoch [14/120    avg_loss:2.307, val_acc:0.521]
Epoch [15/120    avg_loss:2.253, val_acc:0.548]
Epoch [16/120    avg_loss:2.244, val_acc:0.524]
Epoch [17/120    avg_loss:2.184, val_acc:0.550]
Epoch [18/120    avg_loss:2.139, val_acc:0.566]
Epoch [19/120    avg_loss:2.048, val_acc:0.576]
Epoch [20/120    avg_loss:2.014, val_acc:0.589]
Epoch [21/120    avg_loss:1.924, val_acc:0.570]
Epoch [22/120    avg_loss:1.842, val_acc:0.584]
Epoch [23/120    avg_loss:1.705, val_acc:0.630]
Epoch [24/120    avg_loss:1.582, val_acc:0.660]
Epoch [25/120    avg_loss:1.511, val_acc:0.649]
Epoch [26/120    avg_loss:1.422, val_acc:0.677]
Epoch [27/120    avg_loss:1.351, val_acc:0.636]
Epoch [28/120    avg_loss:1.316, val_acc:0.691]
Epoch [29/120    avg_loss:1.184, val_acc:0.709]
Epoch [30/120    avg_loss:1.106, val_acc:0.728]
Epoch [31/120    avg_loss:0.982, val_acc:0.734]
Epoch [32/120    avg_loss:0.950, val_acc:0.750]
Epoch [33/120    avg_loss:0.862, val_acc:0.793]
Epoch [34/120    avg_loss:0.816, val_acc:0.816]
Epoch [35/120    avg_loss:0.720, val_acc:0.811]
Epoch [36/120    avg_loss:0.673, val_acc:0.816]
Epoch [37/120    avg_loss:0.617, val_acc:0.839]
Epoch [38/120    avg_loss:0.566, val_acc:0.828]
Epoch [39/120    avg_loss:0.583, val_acc:0.851]
Epoch [40/120    avg_loss:0.528, val_acc:0.840]
Epoch [41/120    avg_loss:0.452, val_acc:0.874]
Epoch [42/120    avg_loss:0.406, val_acc:0.892]
Epoch [43/120    avg_loss:0.377, val_acc:0.901]
Epoch [44/120    avg_loss:0.360, val_acc:0.887]
Epoch [45/120    avg_loss:0.330, val_acc:0.898]
Epoch [46/120    avg_loss:0.321, val_acc:0.925]
Epoch [47/120    avg_loss:0.258, val_acc:0.922]
Epoch [48/120    avg_loss:0.263, val_acc:0.923]
Epoch [49/120    avg_loss:0.249, val_acc:0.935]
Epoch [50/120    avg_loss:0.232, val_acc:0.932]
Epoch [51/120    avg_loss:0.245, val_acc:0.934]
Epoch [52/120    avg_loss:0.212, val_acc:0.927]
Epoch [53/120    avg_loss:0.242, val_acc:0.943]
Epoch [54/120    avg_loss:0.227, val_acc:0.938]
Epoch [55/120    avg_loss:0.198, val_acc:0.953]
Epoch [56/120    avg_loss:0.231, val_acc:0.914]
Epoch [57/120    avg_loss:0.207, val_acc:0.926]
Epoch [58/120    avg_loss:0.157, val_acc:0.960]
Epoch [59/120    avg_loss:0.151, val_acc:0.952]
Epoch [60/120    avg_loss:0.159, val_acc:0.946]
Epoch [61/120    avg_loss:0.132, val_acc:0.952]
Epoch [62/120    avg_loss:0.113, val_acc:0.959]
Epoch [63/120    avg_loss:0.100, val_acc:0.971]
Epoch [64/120    avg_loss:0.107, val_acc:0.930]
Epoch [65/120    avg_loss:0.118, val_acc:0.967]
Epoch [66/120    avg_loss:0.103, val_acc:0.947]
Epoch [67/120    avg_loss:0.098, val_acc:0.957]
Epoch [68/120    avg_loss:0.101, val_acc:0.963]
Epoch [69/120    avg_loss:0.084, val_acc:0.965]
Epoch [70/120    avg_loss:0.074, val_acc:0.950]
Epoch [71/120    avg_loss:0.082, val_acc:0.966]
Epoch [72/120    avg_loss:0.091, val_acc:0.974]
Epoch [73/120    avg_loss:0.080, val_acc:0.978]
Epoch [74/120    avg_loss:0.074, val_acc:0.972]
Epoch [75/120    avg_loss:0.094, val_acc:0.964]
Epoch [76/120    avg_loss:0.077, val_acc:0.959]
Epoch [77/120    avg_loss:0.084, val_acc:0.954]
Epoch [78/120    avg_loss:0.072, val_acc:0.978]
Epoch [79/120    avg_loss:0.071, val_acc:0.963]
Epoch [80/120    avg_loss:0.053, val_acc:0.976]
Epoch [81/120    avg_loss:0.046, val_acc:0.978]
Epoch [82/120    avg_loss:0.043, val_acc:0.980]
Epoch [83/120    avg_loss:0.046, val_acc:0.983]
Epoch [84/120    avg_loss:0.061, val_acc:0.973]
Epoch [85/120    avg_loss:0.039, val_acc:0.975]
Epoch [86/120    avg_loss:0.041, val_acc:0.976]
Epoch [87/120    avg_loss:0.038, val_acc:0.974]
Epoch [88/120    avg_loss:0.039, val_acc:0.973]
Epoch [89/120    avg_loss:0.034, val_acc:0.978]
Epoch [90/120    avg_loss:0.036, val_acc:0.967]
Epoch [91/120    avg_loss:0.049, val_acc:0.980]
Epoch [92/120    avg_loss:0.038, val_acc:0.982]
Epoch [93/120    avg_loss:0.035, val_acc:0.968]
Epoch [94/120    avg_loss:0.030, val_acc:0.984]
Epoch [95/120    avg_loss:0.027, val_acc:0.985]
Epoch [96/120    avg_loss:0.040, val_acc:0.951]
Epoch [97/120    avg_loss:0.066, val_acc:0.987]
Epoch [98/120    avg_loss:0.036, val_acc:0.984]
Epoch [99/120    avg_loss:0.033, val_acc:0.980]
Epoch [100/120    avg_loss:0.027, val_acc:0.984]
Epoch [101/120    avg_loss:0.024, val_acc:0.987]
Epoch [102/120    avg_loss:0.022, val_acc:0.985]
Epoch [103/120    avg_loss:0.026, val_acc:0.982]
Epoch [104/120    avg_loss:0.020, val_acc:0.983]
Epoch [105/120    avg_loss:0.023, val_acc:0.986]
Epoch [106/120    avg_loss:0.023, val_acc:0.986]
Epoch [107/120    avg_loss:0.023, val_acc:0.983]
Epoch [108/120    avg_loss:0.021, val_acc:0.985]
Epoch [109/120    avg_loss:0.018, val_acc:0.988]
Epoch [110/120    avg_loss:0.020, val_acc:0.983]
Epoch [111/120    avg_loss:0.021, val_acc:0.987]
Epoch [112/120    avg_loss:0.021, val_acc:0.987]
Epoch [113/120    avg_loss:0.020, val_acc:0.982]
Epoch [114/120    avg_loss:0.024, val_acc:0.979]
Epoch [115/120    avg_loss:0.019, val_acc:0.987]
Epoch [116/120    avg_loss:0.031, val_acc:0.974]
Epoch [117/120    avg_loss:0.024, val_acc:0.975]
Epoch [118/120    avg_loss:0.027, val_acc:0.982]
Epoch [119/120    avg_loss:0.018, val_acc:0.984]
Epoch [120/120    avg_loss:0.016, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1274    0    7    0    0    0    0    0    1    3    0    0
     0    0    0]
 [   0    0    2  740    2    0    0    0    0    0    1    0    2    0
     0    0    0]
 [   0    0    0    4  209    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    0    0    1    0    0    0    0
    10    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   34    0    1    0    0    0    0    0  827   13    0    0
     0    0    0]
 [   0    0   21    0    0    1    0    0    0    0    5 2165   18    0
     0    0    0]
 [   0    0    0    6    1    0    0    0    0    0   15    2  505    0
     1    1    3]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0  183
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1132    7    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
     6  339    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.11382113821138

F1 scores:
[       nan 0.975      0.97400612 0.98864395 0.96535797 0.9837587
 0.99848024 1.         1.         0.97297297 0.95828505 0.985659
 0.95372993 0.99456522 0.98951049 0.97694524 0.98245614]

Kappa:
0.9784997634123421
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f96b4293f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.824, val_acc:0.003]
Epoch [2/120    avg_loss:2.785, val_acc:0.113]
Epoch [3/120    avg_loss:2.749, val_acc:0.245]
Epoch [4/120    avg_loss:2.714, val_acc:0.280]
Epoch [5/120    avg_loss:2.666, val_acc:0.301]
Epoch [6/120    avg_loss:2.625, val_acc:0.353]
Epoch [7/120    avg_loss:2.578, val_acc:0.376]
Epoch [8/120    avg_loss:2.547, val_acc:0.453]
Epoch [9/120    avg_loss:2.505, val_acc:0.466]
Epoch [10/120    avg_loss:2.455, val_acc:0.475]
Epoch [11/120    avg_loss:2.426, val_acc:0.489]
Epoch [12/120    avg_loss:2.399, val_acc:0.494]
Epoch [13/120    avg_loss:2.308, val_acc:0.502]
Epoch [14/120    avg_loss:2.262, val_acc:0.526]
Epoch [15/120    avg_loss:2.214, val_acc:0.552]
Epoch [16/120    avg_loss:2.150, val_acc:0.562]
Epoch [17/120    avg_loss:2.111, val_acc:0.571]
Epoch [18/120    avg_loss:2.027, val_acc:0.592]
Epoch [19/120    avg_loss:1.996, val_acc:0.606]
Epoch [20/120    avg_loss:1.935, val_acc:0.611]
Epoch [21/120    avg_loss:1.870, val_acc:0.640]
Epoch [22/120    avg_loss:1.760, val_acc:0.657]
Epoch [23/120    avg_loss:1.643, val_acc:0.661]
Epoch [24/120    avg_loss:1.621, val_acc:0.676]
Epoch [25/120    avg_loss:1.518, val_acc:0.699]
Epoch [26/120    avg_loss:1.422, val_acc:0.702]
Epoch [27/120    avg_loss:1.284, val_acc:0.722]
Epoch [28/120    avg_loss:1.178, val_acc:0.729]
Epoch [29/120    avg_loss:1.080, val_acc:0.782]
Epoch [30/120    avg_loss:1.009, val_acc:0.784]
Epoch [31/120    avg_loss:0.926, val_acc:0.798]
Epoch [32/120    avg_loss:0.897, val_acc:0.805]
Epoch [33/120    avg_loss:0.785, val_acc:0.818]
Epoch [34/120    avg_loss:0.781, val_acc:0.836]
Epoch [35/120    avg_loss:0.652, val_acc:0.863]
Epoch [36/120    avg_loss:0.564, val_acc:0.850]
Epoch [37/120    avg_loss:0.557, val_acc:0.887]
Epoch [38/120    avg_loss:0.450, val_acc:0.876]
Epoch [39/120    avg_loss:0.410, val_acc:0.910]
Epoch [40/120    avg_loss:0.370, val_acc:0.898]
Epoch [41/120    avg_loss:0.356, val_acc:0.912]
Epoch [42/120    avg_loss:0.340, val_acc:0.932]
Epoch [43/120    avg_loss:0.281, val_acc:0.918]
Epoch [44/120    avg_loss:0.282, val_acc:0.917]
Epoch [45/120    avg_loss:0.281, val_acc:0.921]
Epoch [46/120    avg_loss:0.223, val_acc:0.924]
Epoch [47/120    avg_loss:0.216, val_acc:0.923]
Epoch [48/120    avg_loss:0.181, val_acc:0.947]
Epoch [49/120    avg_loss:0.172, val_acc:0.925]
Epoch [50/120    avg_loss:0.177, val_acc:0.926]
Epoch [51/120    avg_loss:0.181, val_acc:0.944]
Epoch [52/120    avg_loss:0.153, val_acc:0.953]
Epoch [53/120    avg_loss:0.137, val_acc:0.950]
Epoch [54/120    avg_loss:0.125, val_acc:0.954]
Epoch [55/120    avg_loss:0.121, val_acc:0.949]
Epoch [56/120    avg_loss:0.118, val_acc:0.932]
Epoch [57/120    avg_loss:0.124, val_acc:0.950]
Epoch [58/120    avg_loss:0.105, val_acc:0.953]
Epoch [59/120    avg_loss:0.123, val_acc:0.960]
Epoch [60/120    avg_loss:0.085, val_acc:0.961]
Epoch [61/120    avg_loss:0.079, val_acc:0.959]
Epoch [62/120    avg_loss:0.078, val_acc:0.969]
Epoch [63/120    avg_loss:0.072, val_acc:0.967]
Epoch [64/120    avg_loss:0.073, val_acc:0.961]
Epoch [65/120    avg_loss:0.071, val_acc:0.968]
Epoch [66/120    avg_loss:0.060, val_acc:0.961]
Epoch [67/120    avg_loss:0.070, val_acc:0.959]
Epoch [68/120    avg_loss:0.075, val_acc:0.963]
Epoch [69/120    avg_loss:0.053, val_acc:0.954]
Epoch [70/120    avg_loss:0.050, val_acc:0.971]
Epoch [71/120    avg_loss:0.052, val_acc:0.965]
Epoch [72/120    avg_loss:0.053, val_acc:0.966]
Epoch [73/120    avg_loss:0.046, val_acc:0.963]
Epoch [74/120    avg_loss:0.056, val_acc:0.974]
Epoch [75/120    avg_loss:0.052, val_acc:0.941]
Epoch [76/120    avg_loss:0.047, val_acc:0.947]
Epoch [77/120    avg_loss:0.047, val_acc:0.968]
Epoch [78/120    avg_loss:0.040, val_acc:0.970]
Epoch [79/120    avg_loss:0.040, val_acc:0.974]
Epoch [80/120    avg_loss:0.031, val_acc:0.975]
Epoch [81/120    avg_loss:0.027, val_acc:0.968]
Epoch [82/120    avg_loss:0.029, val_acc:0.969]
Epoch [83/120    avg_loss:0.029, val_acc:0.970]
Epoch [84/120    avg_loss:0.032, val_acc:0.973]
Epoch [85/120    avg_loss:0.027, val_acc:0.974]
Epoch [86/120    avg_loss:0.024, val_acc:0.972]
Epoch [87/120    avg_loss:0.028, val_acc:0.959]
Epoch [88/120    avg_loss:0.032, val_acc:0.972]
Epoch [89/120    avg_loss:0.029, val_acc:0.971]
Epoch [90/120    avg_loss:0.026, val_acc:0.976]
Epoch [91/120    avg_loss:0.020, val_acc:0.979]
Epoch [92/120    avg_loss:0.025, val_acc:0.976]
Epoch [93/120    avg_loss:0.023, val_acc:0.977]
Epoch [94/120    avg_loss:0.023, val_acc:0.969]
Epoch [95/120    avg_loss:0.033, val_acc:0.976]
Epoch [96/120    avg_loss:0.182, val_acc:0.957]
Epoch [97/120    avg_loss:0.202, val_acc:0.936]
Epoch [98/120    avg_loss:0.131, val_acc:0.945]
Epoch [99/120    avg_loss:0.093, val_acc:0.959]
Epoch [100/120    avg_loss:0.078, val_acc:0.960]
Epoch [101/120    avg_loss:0.066, val_acc:0.960]
Epoch [102/120    avg_loss:0.046, val_acc:0.957]
Epoch [103/120    avg_loss:0.043, val_acc:0.971]
Epoch [104/120    avg_loss:0.044, val_acc:0.956]
Epoch [105/120    avg_loss:0.037, val_acc:0.974]
Epoch [106/120    avg_loss:0.025, val_acc:0.975]
Epoch [107/120    avg_loss:0.023, val_acc:0.978]
Epoch [108/120    avg_loss:0.023, val_acc:0.976]
Epoch [109/120    avg_loss:0.021, val_acc:0.976]
Epoch [110/120    avg_loss:0.022, val_acc:0.976]
Epoch [111/120    avg_loss:0.022, val_acc:0.979]
Epoch [112/120    avg_loss:0.022, val_acc:0.978]
Epoch [113/120    avg_loss:0.022, val_acc:0.976]
Epoch [114/120    avg_loss:0.020, val_acc:0.978]
Epoch [115/120    avg_loss:0.023, val_acc:0.979]
Epoch [116/120    avg_loss:0.021, val_acc:0.978]
Epoch [117/120    avg_loss:0.020, val_acc:0.979]
Epoch [118/120    avg_loss:0.020, val_acc:0.979]
Epoch [119/120    avg_loss:0.021, val_acc:0.978]
Epoch [120/120    avg_loss:0.019, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1250    0   11    0    0    0    0    1    3   20    0    0
     0    0    0]
 [   0    0    1  724    2    1    2    0    0    3    4    3    5    0
     1    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    8    0    0    0    0    0    0  422    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    2    0    0    0    0  855    9    1    0
     0    0    0]
 [   0    0    2    0    0    1    2    0    0    0   13 2164   28    0
     0    0    0]
 [   0    0    0    7    2    1    0    0    0    0    4    0  515    0
     0    2    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1138    1    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    20  319    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.96205962059621

F1 scores:
[       nan 0.87356322 0.98193244 0.97903989 0.96598639 0.99313501
 0.98942598 1.         0.99061033 0.87179487 0.97324986 0.98207397
 0.9501845  1.         0.98956522 0.95223881 0.97647059]

Kappa:
0.9767752385985009
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe5deab9ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.831, val_acc:0.102]
Epoch [2/120    avg_loss:2.799, val_acc:0.130]
Epoch [3/120    avg_loss:2.763, val_acc:0.146]
Epoch [4/120    avg_loss:2.712, val_acc:0.172]
Epoch [5/120    avg_loss:2.674, val_acc:0.232]
Epoch [6/120    avg_loss:2.626, val_acc:0.277]
Epoch [7/120    avg_loss:2.555, val_acc:0.289]
Epoch [8/120    avg_loss:2.515, val_acc:0.297]
Epoch [9/120    avg_loss:2.492, val_acc:0.312]
Epoch [10/120    avg_loss:2.408, val_acc:0.340]
Epoch [11/120    avg_loss:2.388, val_acc:0.359]
Epoch [12/120    avg_loss:2.358, val_acc:0.402]
Epoch [13/120    avg_loss:2.318, val_acc:0.431]
Epoch [14/120    avg_loss:2.241, val_acc:0.442]
Epoch [15/120    avg_loss:2.185, val_acc:0.500]
Epoch [16/120    avg_loss:2.138, val_acc:0.475]
Epoch [17/120    avg_loss:2.046, val_acc:0.478]
Epoch [18/120    avg_loss:1.920, val_acc:0.505]
Epoch [19/120    avg_loss:1.835, val_acc:0.400]
Epoch [20/120    avg_loss:1.836, val_acc:0.472]
Epoch [21/120    avg_loss:1.726, val_acc:0.527]
Epoch [22/120    avg_loss:1.617, val_acc:0.542]
Epoch [23/120    avg_loss:1.492, val_acc:0.591]
Epoch [24/120    avg_loss:1.418, val_acc:0.592]
Epoch [25/120    avg_loss:1.322, val_acc:0.654]
Epoch [26/120    avg_loss:1.265, val_acc:0.691]
Epoch [27/120    avg_loss:1.190, val_acc:0.762]
Epoch [28/120    avg_loss:1.078, val_acc:0.751]
Epoch [29/120    avg_loss:0.999, val_acc:0.750]
Epoch [30/120    avg_loss:0.923, val_acc:0.830]
Epoch [31/120    avg_loss:0.833, val_acc:0.841]
Epoch [32/120    avg_loss:0.752, val_acc:0.839]
Epoch [33/120    avg_loss:0.700, val_acc:0.860]
Epoch [34/120    avg_loss:0.628, val_acc:0.875]
Epoch [35/120    avg_loss:0.549, val_acc:0.890]
Epoch [36/120    avg_loss:0.547, val_acc:0.883]
Epoch [37/120    avg_loss:0.494, val_acc:0.891]
Epoch [38/120    avg_loss:0.439, val_acc:0.893]
Epoch [39/120    avg_loss:0.410, val_acc:0.896]
Epoch [40/120    avg_loss:0.345, val_acc:0.912]
Epoch [41/120    avg_loss:0.298, val_acc:0.938]
Epoch [42/120    avg_loss:0.291, val_acc:0.929]
Epoch [43/120    avg_loss:0.246, val_acc:0.931]
Epoch [44/120    avg_loss:0.235, val_acc:0.925]
Epoch [45/120    avg_loss:0.225, val_acc:0.922]
Epoch [46/120    avg_loss:0.235, val_acc:0.931]
Epoch [47/120    avg_loss:0.209, val_acc:0.930]
Epoch [48/120    avg_loss:0.154, val_acc:0.961]
Epoch [49/120    avg_loss:0.160, val_acc:0.959]
Epoch [50/120    avg_loss:0.142, val_acc:0.963]
Epoch [51/120    avg_loss:0.142, val_acc:0.945]
Epoch [52/120    avg_loss:0.135, val_acc:0.964]
Epoch [53/120    avg_loss:0.114, val_acc:0.956]
Epoch [54/120    avg_loss:0.114, val_acc:0.969]
Epoch [55/120    avg_loss:0.104, val_acc:0.972]
Epoch [56/120    avg_loss:0.089, val_acc:0.969]
Epoch [57/120    avg_loss:0.105, val_acc:0.952]
Epoch [58/120    avg_loss:0.106, val_acc:0.955]
Epoch [59/120    avg_loss:0.097, val_acc:0.959]
Epoch [60/120    avg_loss:0.089, val_acc:0.960]
Epoch [61/120    avg_loss:0.076, val_acc:0.960]
Epoch [62/120    avg_loss:0.073, val_acc:0.967]
Epoch [63/120    avg_loss:0.070, val_acc:0.963]
Epoch [64/120    avg_loss:0.063, val_acc:0.974]
Epoch [65/120    avg_loss:0.065, val_acc:0.977]
Epoch [66/120    avg_loss:0.083, val_acc:0.963]
Epoch [67/120    avg_loss:0.064, val_acc:0.960]
Epoch [68/120    avg_loss:0.061, val_acc:0.973]
Epoch [69/120    avg_loss:0.050, val_acc:0.972]
Epoch [70/120    avg_loss:0.052, val_acc:0.974]
Epoch [71/120    avg_loss:0.047, val_acc:0.973]
Epoch [72/120    avg_loss:0.046, val_acc:0.977]
Epoch [73/120    avg_loss:0.052, val_acc:0.975]
Epoch [74/120    avg_loss:0.049, val_acc:0.975]
Epoch [75/120    avg_loss:0.035, val_acc:0.980]
Epoch [76/120    avg_loss:0.042, val_acc:0.973]
Epoch [77/120    avg_loss:0.044, val_acc:0.977]
Epoch [78/120    avg_loss:0.035, val_acc:0.977]
Epoch [79/120    avg_loss:0.034, val_acc:0.977]
Epoch [80/120    avg_loss:0.037, val_acc:0.972]
Epoch [81/120    avg_loss:0.035, val_acc:0.975]
Epoch [82/120    avg_loss:0.030, val_acc:0.976]
Epoch [83/120    avg_loss:0.045, val_acc:0.977]
Epoch [84/120    avg_loss:0.036, val_acc:0.974]
Epoch [85/120    avg_loss:0.034, val_acc:0.978]
Epoch [86/120    avg_loss:0.029, val_acc:0.976]
Epoch [87/120    avg_loss:0.026, val_acc:0.983]
Epoch [88/120    avg_loss:0.027, val_acc:0.974]
Epoch [89/120    avg_loss:0.028, val_acc:0.981]
Epoch [90/120    avg_loss:0.025, val_acc:0.976]
Epoch [91/120    avg_loss:0.023, val_acc:0.982]
Epoch [92/120    avg_loss:0.019, val_acc:0.980]
Epoch [93/120    avg_loss:0.020, val_acc:0.978]
Epoch [94/120    avg_loss:0.020, val_acc:0.982]
Epoch [95/120    avg_loss:0.024, val_acc:0.981]
Epoch [96/120    avg_loss:0.019, val_acc:0.981]
Epoch [97/120    avg_loss:0.020, val_acc:0.982]
Epoch [98/120    avg_loss:0.019, val_acc:0.984]
Epoch [99/120    avg_loss:0.016, val_acc:0.980]
Epoch [100/120    avg_loss:0.015, val_acc:0.978]
Epoch [101/120    avg_loss:0.020, val_acc:0.984]
Epoch [102/120    avg_loss:0.015, val_acc:0.983]
Epoch [103/120    avg_loss:0.016, val_acc:0.981]
Epoch [104/120    avg_loss:0.016, val_acc:0.980]
Epoch [105/120    avg_loss:0.017, val_acc:0.981]
Epoch [106/120    avg_loss:0.013, val_acc:0.984]
Epoch [107/120    avg_loss:0.016, val_acc:0.981]
Epoch [108/120    avg_loss:0.018, val_acc:0.977]
Epoch [109/120    avg_loss:0.018, val_acc:0.977]
Epoch [110/120    avg_loss:0.019, val_acc:0.981]
Epoch [111/120    avg_loss:0.014, val_acc:0.982]
Epoch [112/120    avg_loss:0.013, val_acc:0.983]
Epoch [113/120    avg_loss:0.015, val_acc:0.984]
Epoch [114/120    avg_loss:0.017, val_acc:0.985]
Epoch [115/120    avg_loss:0.012, val_acc:0.990]
Epoch [116/120    avg_loss:0.013, val_acc:0.979]
Epoch [117/120    avg_loss:0.011, val_acc:0.981]
Epoch [118/120    avg_loss:0.012, val_acc:0.989]
Epoch [119/120    avg_loss:0.013, val_acc:0.989]
Epoch [120/120    avg_loss:0.013, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1240    0    3    0    1    0    0    0   13   26    1    0
     0    1    0]
 [   0    0    1  700    5    4    1    0    0   14   10    1    9    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    4    0    0   13    0    0    1    0
     0    0    0]
 [   0    0    6    0    0    3    0    0    0    0  854    7    0    0
     0    5    0]
 [   0    0    3    0    0    0    3    1    0    0   14 2169   20    0
     0    0    0]
 [   0    0    1    0    0    2    0    0    0    3    7    4  512    0
     0    2    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    0    0    0    0
  1132    0    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    11  325    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.61517615176152

F1 scores:
[       nan 0.96202532 0.97791798 0.967519   0.98156682 0.98074745
 0.98424606 0.98039216 0.99883856 0.53061224 0.96171171 0.98167006
 0.94727105 0.99462366 0.99211218 0.95588235 0.95808383]

Kappa:
0.9728221026436401
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5f6dacef28>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.837, val_acc:0.138]
Epoch [2/120    avg_loss:2.805, val_acc:0.198]
Epoch [3/120    avg_loss:2.768, val_acc:0.221]
Epoch [4/120    avg_loss:2.716, val_acc:0.207]
Epoch [5/120    avg_loss:2.662, val_acc:0.215]
Epoch [6/120    avg_loss:2.623, val_acc:0.225]
Epoch [7/120    avg_loss:2.557, val_acc:0.233]
Epoch [8/120    avg_loss:2.501, val_acc:0.258]
Epoch [9/120    avg_loss:2.458, val_acc:0.266]
Epoch [10/120    avg_loss:2.409, val_acc:0.285]
Epoch [11/120    avg_loss:2.365, val_acc:0.342]
Epoch [12/120    avg_loss:2.319, val_acc:0.343]
Epoch [13/120    avg_loss:2.283, val_acc:0.395]
Epoch [14/120    avg_loss:2.218, val_acc:0.424]
Epoch [15/120    avg_loss:2.203, val_acc:0.429]
Epoch [16/120    avg_loss:2.134, val_acc:0.478]
Epoch [17/120    avg_loss:2.089, val_acc:0.501]
Epoch [18/120    avg_loss:2.025, val_acc:0.489]
Epoch [19/120    avg_loss:1.952, val_acc:0.557]
Epoch [20/120    avg_loss:1.894, val_acc:0.605]
Epoch [21/120    avg_loss:1.821, val_acc:0.586]
Epoch [22/120    avg_loss:1.723, val_acc:0.613]
Epoch [23/120    avg_loss:1.669, val_acc:0.575]
Epoch [24/120    avg_loss:1.600, val_acc:0.595]
Epoch [25/120    avg_loss:1.491, val_acc:0.627]
Epoch [26/120    avg_loss:1.422, val_acc:0.635]
Epoch [27/120    avg_loss:1.314, val_acc:0.629]
Epoch [28/120    avg_loss:1.203, val_acc:0.659]
Epoch [29/120    avg_loss:1.115, val_acc:0.666]
Epoch [30/120    avg_loss:1.127, val_acc:0.667]
Epoch [31/120    avg_loss:1.076, val_acc:0.668]
Epoch [32/120    avg_loss:0.997, val_acc:0.710]
Epoch [33/120    avg_loss:0.883, val_acc:0.724]
Epoch [34/120    avg_loss:0.798, val_acc:0.751]
Epoch [35/120    avg_loss:0.691, val_acc:0.761]
Epoch [36/120    avg_loss:0.652, val_acc:0.771]
Epoch [37/120    avg_loss:0.624, val_acc:0.765]
Epoch [38/120    avg_loss:0.615, val_acc:0.791]
Epoch [39/120    avg_loss:0.538, val_acc:0.812]
Epoch [40/120    avg_loss:0.514, val_acc:0.800]
Epoch [41/120    avg_loss:0.489, val_acc:0.802]
Epoch [42/120    avg_loss:0.473, val_acc:0.840]
Epoch [43/120    avg_loss:0.417, val_acc:0.836]
Epoch [44/120    avg_loss:0.366, val_acc:0.865]
Epoch [45/120    avg_loss:0.341, val_acc:0.872]
Epoch [46/120    avg_loss:0.326, val_acc:0.879]
Epoch [47/120    avg_loss:0.305, val_acc:0.883]
Epoch [48/120    avg_loss:0.287, val_acc:0.910]
Epoch [49/120    avg_loss:0.254, val_acc:0.903]
Epoch [50/120    avg_loss:0.256, val_acc:0.908]
Epoch [51/120    avg_loss:0.240, val_acc:0.912]
Epoch [52/120    avg_loss:0.208, val_acc:0.925]
Epoch [53/120    avg_loss:0.200, val_acc:0.920]
Epoch [54/120    avg_loss:0.198, val_acc:0.918]
Epoch [55/120    avg_loss:0.294, val_acc:0.917]
Epoch [56/120    avg_loss:0.212, val_acc:0.927]
Epoch [57/120    avg_loss:0.170, val_acc:0.944]
Epoch [58/120    avg_loss:0.142, val_acc:0.940]
Epoch [59/120    avg_loss:0.145, val_acc:0.934]
Epoch [60/120    avg_loss:0.139, val_acc:0.941]
Epoch [61/120    avg_loss:0.119, val_acc:0.944]
Epoch [62/120    avg_loss:0.128, val_acc:0.939]
Epoch [63/120    avg_loss:0.112, val_acc:0.947]
Epoch [64/120    avg_loss:0.108, val_acc:0.950]
Epoch [65/120    avg_loss:0.108, val_acc:0.944]
Epoch [66/120    avg_loss:0.126, val_acc:0.942]
Epoch [67/120    avg_loss:0.123, val_acc:0.954]
Epoch [68/120    avg_loss:0.094, val_acc:0.957]
Epoch [69/120    avg_loss:0.095, val_acc:0.938]
Epoch [70/120    avg_loss:0.124, val_acc:0.954]
Epoch [71/120    avg_loss:0.088, val_acc:0.954]
Epoch [72/120    avg_loss:0.085, val_acc:0.957]
Epoch [73/120    avg_loss:0.075, val_acc:0.963]
Epoch [74/120    avg_loss:0.064, val_acc:0.961]
Epoch [75/120    avg_loss:0.064, val_acc:0.965]
Epoch [76/120    avg_loss:0.076, val_acc:0.959]
Epoch [77/120    avg_loss:0.065, val_acc:0.965]
Epoch [78/120    avg_loss:0.058, val_acc:0.963]
Epoch [79/120    avg_loss:0.064, val_acc:0.968]
Epoch [80/120    avg_loss:0.055, val_acc:0.974]
Epoch [81/120    avg_loss:0.050, val_acc:0.963]
Epoch [82/120    avg_loss:0.050, val_acc:0.959]
Epoch [83/120    avg_loss:0.048, val_acc:0.974]
Epoch [84/120    avg_loss:0.045, val_acc:0.966]
Epoch [85/120    avg_loss:0.053, val_acc:0.968]
Epoch [86/120    avg_loss:0.047, val_acc:0.967]
Epoch [87/120    avg_loss:0.046, val_acc:0.973]
Epoch [88/120    avg_loss:0.035, val_acc:0.972]
Epoch [89/120    avg_loss:0.034, val_acc:0.979]
Epoch [90/120    avg_loss:0.033, val_acc:0.976]
Epoch [91/120    avg_loss:0.044, val_acc:0.968]
Epoch [92/120    avg_loss:0.037, val_acc:0.971]
Epoch [93/120    avg_loss:0.034, val_acc:0.978]
Epoch [94/120    avg_loss:0.030, val_acc:0.976]
Epoch [95/120    avg_loss:0.036, val_acc:0.969]
Epoch [96/120    avg_loss:0.036, val_acc:0.969]
Epoch [97/120    avg_loss:0.026, val_acc:0.978]
Epoch [98/120    avg_loss:0.024, val_acc:0.980]
Epoch [99/120    avg_loss:0.029, val_acc:0.972]
Epoch [100/120    avg_loss:0.026, val_acc:0.979]
Epoch [101/120    avg_loss:0.028, val_acc:0.969]
Epoch [102/120    avg_loss:0.020, val_acc:0.977]
Epoch [103/120    avg_loss:0.026, val_acc:0.977]
Epoch [104/120    avg_loss:0.027, val_acc:0.973]
Epoch [105/120    avg_loss:0.026, val_acc:0.973]
Epoch [106/120    avg_loss:0.025, val_acc:0.976]
Epoch [107/120    avg_loss:0.022, val_acc:0.978]
Epoch [108/120    avg_loss:0.024, val_acc:0.967]
Epoch [109/120    avg_loss:0.022, val_acc:0.975]
Epoch [110/120    avg_loss:0.020, val_acc:0.972]
Epoch [111/120    avg_loss:0.021, val_acc:0.970]
Epoch [112/120    avg_loss:0.019, val_acc:0.978]
Epoch [113/120    avg_loss:0.016, val_acc:0.978]
Epoch [114/120    avg_loss:0.016, val_acc:0.979]
Epoch [115/120    avg_loss:0.016, val_acc:0.978]
Epoch [116/120    avg_loss:0.015, val_acc:0.978]
Epoch [117/120    avg_loss:0.015, val_acc:0.979]
Epoch [118/120    avg_loss:0.017, val_acc:0.980]
Epoch [119/120    avg_loss:0.019, val_acc:0.980]
Epoch [120/120    avg_loss:0.016, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1247    0    5    0    0    0    0    0    0   33    0    0
     0    0    0]
 [   0    0    2  715    8    0    0    0    0    4    1    5   12    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    1    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    2    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0    6    0    0    0    1    0    0    0  852   16    0    0
     0    0    0]
 [   0    0   12    0    0    0    1    0    0    0    3 2182   12    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    8    6  516    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0   32    0    0    0    0    0    0    0
    16  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.71273712737127

F1 scores:
[       nan 0.975      0.97650744 0.97744361 0.96363636 0.99653979
 0.97253155 1.         0.997669   0.78947368 0.97987349 0.97979344
 0.95821727 1.         0.98730853 0.91019787 0.98245614]

Kappa:
0.973907591391896
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4a2bbf6ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.829, val_acc:0.163]
Epoch [2/120    avg_loss:2.794, val_acc:0.201]
Epoch [3/120    avg_loss:2.756, val_acc:0.247]
Epoch [4/120    avg_loss:2.721, val_acc:0.270]
Epoch [5/120    avg_loss:2.681, val_acc:0.277]
Epoch [6/120    avg_loss:2.623, val_acc:0.275]
Epoch [7/120    avg_loss:2.593, val_acc:0.278]
Epoch [8/120    avg_loss:2.539, val_acc:0.277]
Epoch [9/120    avg_loss:2.493, val_acc:0.296]
Epoch [10/120    avg_loss:2.462, val_acc:0.313]
Epoch [11/120    avg_loss:2.367, val_acc:0.323]
Epoch [12/120    avg_loss:2.329, val_acc:0.329]
Epoch [13/120    avg_loss:2.288, val_acc:0.338]
Epoch [14/120    avg_loss:2.200, val_acc:0.401]
Epoch [15/120    avg_loss:2.143, val_acc:0.404]
Epoch [16/120    avg_loss:2.075, val_acc:0.436]
Epoch [17/120    avg_loss:2.015, val_acc:0.521]
Epoch [18/120    avg_loss:1.891, val_acc:0.539]
Epoch [19/120    avg_loss:1.804, val_acc:0.548]
Epoch [20/120    avg_loss:1.736, val_acc:0.607]
Epoch [21/120    avg_loss:1.618, val_acc:0.555]
Epoch [22/120    avg_loss:1.536, val_acc:0.600]
Epoch [23/120    avg_loss:1.438, val_acc:0.629]
Epoch [24/120    avg_loss:1.324, val_acc:0.721]
Epoch [25/120    avg_loss:1.209, val_acc:0.736]
Epoch [26/120    avg_loss:1.123, val_acc:0.748]
Epoch [27/120    avg_loss:1.038, val_acc:0.775]
Epoch [28/120    avg_loss:0.920, val_acc:0.792]
Epoch [29/120    avg_loss:0.898, val_acc:0.754]
Epoch [30/120    avg_loss:0.812, val_acc:0.815]
Epoch [31/120    avg_loss:0.762, val_acc:0.833]
Epoch [32/120    avg_loss:0.650, val_acc:0.854]
Epoch [33/120    avg_loss:0.591, val_acc:0.833]
Epoch [34/120    avg_loss:0.572, val_acc:0.860]
Epoch [35/120    avg_loss:0.529, val_acc:0.880]
Epoch [36/120    avg_loss:0.437, val_acc:0.879]
Epoch [37/120    avg_loss:0.407, val_acc:0.887]
Epoch [38/120    avg_loss:0.410, val_acc:0.892]
Epoch [39/120    avg_loss:0.346, val_acc:0.914]
Epoch [40/120    avg_loss:0.306, val_acc:0.909]
Epoch [41/120    avg_loss:0.274, val_acc:0.913]
Epoch [42/120    avg_loss:0.303, val_acc:0.936]
Epoch [43/120    avg_loss:0.241, val_acc:0.936]
Epoch [44/120    avg_loss:0.239, val_acc:0.934]
Epoch [45/120    avg_loss:0.224, val_acc:0.949]
Epoch [46/120    avg_loss:0.201, val_acc:0.947]
Epoch [47/120    avg_loss:0.165, val_acc:0.951]
Epoch [48/120    avg_loss:0.158, val_acc:0.947]
Epoch [49/120    avg_loss:0.139, val_acc:0.958]
Epoch [50/120    avg_loss:0.135, val_acc:0.960]
Epoch [51/120    avg_loss:0.120, val_acc:0.960]
Epoch [52/120    avg_loss:0.124, val_acc:0.967]
Epoch [53/120    avg_loss:0.111, val_acc:0.957]
Epoch [54/120    avg_loss:0.108, val_acc:0.961]
Epoch [55/120    avg_loss:0.097, val_acc:0.970]
Epoch [56/120    avg_loss:0.078, val_acc:0.976]
Epoch [57/120    avg_loss:0.088, val_acc:0.961]
Epoch [58/120    avg_loss:0.090, val_acc:0.964]
Epoch [59/120    avg_loss:0.094, val_acc:0.950]
Epoch [60/120    avg_loss:0.095, val_acc:0.968]
Epoch [61/120    avg_loss:0.091, val_acc:0.966]
Epoch [62/120    avg_loss:0.081, val_acc:0.972]
Epoch [63/120    avg_loss:0.072, val_acc:0.964]
Epoch [64/120    avg_loss:0.060, val_acc:0.980]
Epoch [65/120    avg_loss:0.062, val_acc:0.976]
Epoch [66/120    avg_loss:0.063, val_acc:0.976]
Epoch [67/120    avg_loss:0.060, val_acc:0.979]
Epoch [68/120    avg_loss:0.058, val_acc:0.972]
Epoch [69/120    avg_loss:0.055, val_acc:0.977]
Epoch [70/120    avg_loss:0.042, val_acc:0.984]
Epoch [71/120    avg_loss:0.049, val_acc:0.975]
Epoch [72/120    avg_loss:0.038, val_acc:0.980]
Epoch [73/120    avg_loss:0.041, val_acc:0.982]
Epoch [74/120    avg_loss:0.047, val_acc:0.979]
Epoch [75/120    avg_loss:0.034, val_acc:0.989]
Epoch [76/120    avg_loss:0.033, val_acc:0.987]
Epoch [77/120    avg_loss:0.034, val_acc:0.982]
Epoch [78/120    avg_loss:0.031, val_acc:0.984]
Epoch [79/120    avg_loss:0.032, val_acc:0.982]
Epoch [80/120    avg_loss:0.031, val_acc:0.985]
Epoch [81/120    avg_loss:0.033, val_acc:0.984]
Epoch [82/120    avg_loss:0.032, val_acc:0.985]
Epoch [83/120    avg_loss:0.026, val_acc:0.987]
Epoch [84/120    avg_loss:0.024, val_acc:0.988]
Epoch [85/120    avg_loss:0.025, val_acc:0.984]
Epoch [86/120    avg_loss:0.028, val_acc:0.980]
Epoch [87/120    avg_loss:0.032, val_acc:0.979]
Epoch [88/120    avg_loss:0.053, val_acc:0.978]
Epoch [89/120    avg_loss:0.043, val_acc:0.986]
Epoch [90/120    avg_loss:0.029, val_acc:0.986]
Epoch [91/120    avg_loss:0.027, val_acc:0.989]
Epoch [92/120    avg_loss:0.030, val_acc:0.989]
Epoch [93/120    avg_loss:0.024, val_acc:0.992]
Epoch [94/120    avg_loss:0.025, val_acc:0.993]
Epoch [95/120    avg_loss:0.023, val_acc:0.988]
Epoch [96/120    avg_loss:0.026, val_acc:0.988]
Epoch [97/120    avg_loss:0.021, val_acc:0.989]
Epoch [98/120    avg_loss:0.021, val_acc:0.991]
Epoch [99/120    avg_loss:0.028, val_acc:0.988]
Epoch [100/120    avg_loss:0.018, val_acc:0.988]
Epoch [101/120    avg_loss:0.021, val_acc:0.988]
Epoch [102/120    avg_loss:0.020, val_acc:0.989]
Epoch [103/120    avg_loss:0.019, val_acc:0.989]
Epoch [104/120    avg_loss:0.019, val_acc:0.989]
Epoch [105/120    avg_loss:0.020, val_acc:0.989]
Epoch [106/120    avg_loss:0.018, val_acc:0.990]
Epoch [107/120    avg_loss:0.030, val_acc:0.989]
Epoch [108/120    avg_loss:0.019, val_acc:0.988]
Epoch [109/120    avg_loss:0.019, val_acc:0.988]
Epoch [110/120    avg_loss:0.019, val_acc:0.988]
Epoch [111/120    avg_loss:0.017, val_acc:0.990]
Epoch [112/120    avg_loss:0.020, val_acc:0.989]
Epoch [113/120    avg_loss:0.018, val_acc:0.990]
Epoch [114/120    avg_loss:0.019, val_acc:0.990]
Epoch [115/120    avg_loss:0.019, val_acc:0.990]
Epoch [116/120    avg_loss:0.019, val_acc:0.989]
Epoch [117/120    avg_loss:0.021, val_acc:0.989]
Epoch [118/120    avg_loss:0.018, val_acc:0.989]
Epoch [119/120    avg_loss:0.018, val_acc:0.989]
Epoch [120/120    avg_loss:0.018, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1252    0    6    0    0    0    0    0    5   22    0    0
     0    0    0]
 [   0    0    0  718    9    5    0    0    0    3    4    2    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    1    3    0    0    0  848   13    3    0
     1    0    0]
 [   0    0    7    0    0    0    0    0    0    1   10 2185    7    0
     0    0    0]
 [   0    0    0    0    1    1    0    0    0    0    1    0  530    0
     1    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
  1132    5    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
    11  319    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
98.18970189701896

F1 scores:
[       nan 0.96202532 0.98196078 0.98020478 0.9638009  0.98630137
 0.98274569 1.         1.         0.85       0.97136312 0.98601083
 0.97695853 1.         0.98951049 0.95081967 0.96932515]

Kappa:
0.9793592506892509
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd7a16d0f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.832, val_acc:0.121]
Epoch [2/120    avg_loss:2.785, val_acc:0.334]
Epoch [3/120    avg_loss:2.735, val_acc:0.353]
Epoch [4/120    avg_loss:2.687, val_acc:0.336]
Epoch [5/120    avg_loss:2.629, val_acc:0.378]
Epoch [6/120    avg_loss:2.581, val_acc:0.391]
Epoch [7/120    avg_loss:2.512, val_acc:0.415]
Epoch [8/120    avg_loss:2.451, val_acc:0.458]
Epoch [9/120    avg_loss:2.401, val_acc:0.465]
Epoch [10/120    avg_loss:2.342, val_acc:0.492]
Epoch [11/120    avg_loss:2.303, val_acc:0.472]
Epoch [12/120    avg_loss:2.240, val_acc:0.498]
Epoch [13/120    avg_loss:2.189, val_acc:0.518]
Epoch [14/120    avg_loss:2.099, val_acc:0.519]
Epoch [15/120    avg_loss:2.061, val_acc:0.557]
Epoch [16/120    avg_loss:2.012, val_acc:0.583]
Epoch [17/120    avg_loss:1.875, val_acc:0.588]
Epoch [18/120    avg_loss:1.831, val_acc:0.605]
Epoch [19/120    avg_loss:1.729, val_acc:0.637]
Epoch [20/120    avg_loss:1.604, val_acc:0.626]
Epoch [21/120    avg_loss:1.522, val_acc:0.639]
Epoch [22/120    avg_loss:1.426, val_acc:0.658]
Epoch [23/120    avg_loss:1.331, val_acc:0.694]
Epoch [24/120    avg_loss:1.249, val_acc:0.668]
Epoch [25/120    avg_loss:1.181, val_acc:0.676]
Epoch [26/120    avg_loss:1.103, val_acc:0.716]
Epoch [27/120    avg_loss:1.016, val_acc:0.708]
Epoch [28/120    avg_loss:0.959, val_acc:0.729]
Epoch [29/120    avg_loss:0.909, val_acc:0.784]
Epoch [30/120    avg_loss:0.849, val_acc:0.793]
Epoch [31/120    avg_loss:0.780, val_acc:0.824]
Epoch [32/120    avg_loss:0.721, val_acc:0.827]
Epoch [33/120    avg_loss:0.669, val_acc:0.848]
Epoch [34/120    avg_loss:0.642, val_acc:0.825]
Epoch [35/120    avg_loss:0.607, val_acc:0.851]
Epoch [36/120    avg_loss:0.542, val_acc:0.851]
Epoch [37/120    avg_loss:0.514, val_acc:0.879]
Epoch [38/120    avg_loss:0.436, val_acc:0.885]
Epoch [39/120    avg_loss:0.447, val_acc:0.867]
Epoch [40/120    avg_loss:0.393, val_acc:0.900]
Epoch [41/120    avg_loss:0.319, val_acc:0.901]
Epoch [42/120    avg_loss:0.350, val_acc:0.909]
Epoch [43/120    avg_loss:0.285, val_acc:0.928]
Epoch [44/120    avg_loss:0.238, val_acc:0.915]
Epoch [45/120    avg_loss:0.275, val_acc:0.912]
Epoch [46/120    avg_loss:0.238, val_acc:0.932]
Epoch [47/120    avg_loss:0.197, val_acc:0.935]
Epoch [48/120    avg_loss:0.178, val_acc:0.935]
Epoch [49/120    avg_loss:0.174, val_acc:0.940]
Epoch [50/120    avg_loss:0.187, val_acc:0.940]
Epoch [51/120    avg_loss:0.228, val_acc:0.933]
Epoch [52/120    avg_loss:0.228, val_acc:0.936]
Epoch [53/120    avg_loss:0.195, val_acc:0.934]
Epoch [54/120    avg_loss:0.168, val_acc:0.949]
Epoch [55/120    avg_loss:0.167, val_acc:0.934]
Epoch [56/120    avg_loss:0.177, val_acc:0.948]
Epoch [57/120    avg_loss:0.133, val_acc:0.941]
Epoch [58/120    avg_loss:0.119, val_acc:0.959]
Epoch [59/120    avg_loss:0.105, val_acc:0.965]
Epoch [60/120    avg_loss:0.089, val_acc:0.963]
Epoch [61/120    avg_loss:0.082, val_acc:0.958]
Epoch [62/120    avg_loss:0.091, val_acc:0.960]
Epoch [63/120    avg_loss:0.099, val_acc:0.939]
Epoch [64/120    avg_loss:0.095, val_acc:0.973]
Epoch [65/120    avg_loss:0.080, val_acc:0.964]
Epoch [66/120    avg_loss:0.081, val_acc:0.966]
Epoch [67/120    avg_loss:0.066, val_acc:0.967]
Epoch [68/120    avg_loss:0.058, val_acc:0.966]
Epoch [69/120    avg_loss:0.057, val_acc:0.977]
Epoch [70/120    avg_loss:0.059, val_acc:0.954]
Epoch [71/120    avg_loss:0.058, val_acc:0.972]
Epoch [72/120    avg_loss:0.048, val_acc:0.978]
Epoch [73/120    avg_loss:0.043, val_acc:0.969]
Epoch [74/120    avg_loss:0.039, val_acc:0.972]
Epoch [75/120    avg_loss:0.036, val_acc:0.970]
Epoch [76/120    avg_loss:0.040, val_acc:0.975]
Epoch [77/120    avg_loss:0.038, val_acc:0.970]
Epoch [78/120    avg_loss:0.034, val_acc:0.973]
Epoch [79/120    avg_loss:0.039, val_acc:0.973]
Epoch [80/120    avg_loss:0.040, val_acc:0.974]
Epoch [81/120    avg_loss:0.031, val_acc:0.975]
Epoch [82/120    avg_loss:0.037, val_acc:0.970]
Epoch [83/120    avg_loss:0.036, val_acc:0.973]
Epoch [84/120    avg_loss:0.033, val_acc:0.965]
Epoch [85/120    avg_loss:0.036, val_acc:0.973]
Epoch [86/120    avg_loss:0.032, val_acc:0.984]
Epoch [87/120    avg_loss:0.024, val_acc:0.978]
Epoch [88/120    avg_loss:0.026, val_acc:0.980]
Epoch [89/120    avg_loss:0.023, val_acc:0.980]
Epoch [90/120    avg_loss:0.023, val_acc:0.980]
Epoch [91/120    avg_loss:0.024, val_acc:0.978]
Epoch [92/120    avg_loss:0.025, val_acc:0.979]
Epoch [93/120    avg_loss:0.025, val_acc:0.978]
Epoch [94/120    avg_loss:0.019, val_acc:0.979]
Epoch [95/120    avg_loss:0.021, val_acc:0.979]
Epoch [96/120    avg_loss:0.022, val_acc:0.978]
Epoch [97/120    avg_loss:0.023, val_acc:0.977]
Epoch [98/120    avg_loss:0.021, val_acc:0.979]
Epoch [99/120    avg_loss:0.021, val_acc:0.978]
Epoch [100/120    avg_loss:0.021, val_acc:0.978]
Epoch [101/120    avg_loss:0.020, val_acc:0.978]
Epoch [102/120    avg_loss:0.021, val_acc:0.978]
Epoch [103/120    avg_loss:0.020, val_acc:0.978]
Epoch [104/120    avg_loss:0.021, val_acc:0.979]
Epoch [105/120    avg_loss:0.022, val_acc:0.980]
Epoch [106/120    avg_loss:0.021, val_acc:0.980]
Epoch [107/120    avg_loss:0.024, val_acc:0.979]
Epoch [108/120    avg_loss:0.019, val_acc:0.979]
Epoch [109/120    avg_loss:0.021, val_acc:0.979]
Epoch [110/120    avg_loss:0.021, val_acc:0.979]
Epoch [111/120    avg_loss:0.024, val_acc:0.979]
Epoch [112/120    avg_loss:0.022, val_acc:0.979]
Epoch [113/120    avg_loss:0.021, val_acc:0.979]
Epoch [114/120    avg_loss:0.024, val_acc:0.979]
Epoch [115/120    avg_loss:0.021, val_acc:0.979]
Epoch [116/120    avg_loss:0.020, val_acc:0.979]
Epoch [117/120    avg_loss:0.019, val_acc:0.979]
Epoch [118/120    avg_loss:0.021, val_acc:0.979]
Epoch [119/120    avg_loss:0.027, val_acc:0.979]
Epoch [120/120    avg_loss:0.020, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1243    3    4    0    2    0    0    0    4   29    0    0
     0    0    0]
 [   0    0    0  711    5    3    0    0    0   11    1    4   12    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    5    0    0    1    0    0    0    0  842   25    2    0
     0    0    0]
 [   0    0   13    0    0    0    0    0    0    0    9 2174   14    0
     0    0    0]
 [   0    0    0    1    0    7    0    0    0    0    4    0  515    0
     2    1    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1128   10    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
    15  315    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.61517615176152

F1 scores:
[       nan 0.93975904 0.97566719 0.97264022 0.97695853 0.98751419
 0.98422239 1.         0.99649942 0.72340426 0.97060519 0.97839784
 0.9537037  1.         0.98730853 0.93610698 0.96470588]

Kappa:
0.9728066131744141
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f40b5890ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39709==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.820, val_acc:0.057]
Epoch [2/120    avg_loss:2.780, val_acc:0.171]
Epoch [3/120    avg_loss:2.746, val_acc:0.212]
Epoch [4/120    avg_loss:2.703, val_acc:0.259]
Epoch [5/120    avg_loss:2.657, val_acc:0.304]
Epoch [6/120    avg_loss:2.605, val_acc:0.338]
Epoch [7/120    avg_loss:2.557, val_acc:0.358]
Epoch [8/120    avg_loss:2.510, val_acc:0.420]
Epoch [9/120    avg_loss:2.437, val_acc:0.475]
Epoch [10/120    avg_loss:2.412, val_acc:0.497]
Epoch [11/120    avg_loss:2.354, val_acc:0.512]
Epoch [12/120    avg_loss:2.324, val_acc:0.524]
Epoch [13/120    avg_loss:2.283, val_acc:0.561]
Epoch [14/120    avg_loss:2.227, val_acc:0.549]
Epoch [15/120    avg_loss:2.178, val_acc:0.569]
Epoch [16/120    avg_loss:2.074, val_acc:0.560]
Epoch [17/120    avg_loss:2.022, val_acc:0.564]
Epoch [18/120    avg_loss:1.924, val_acc:0.578]
Epoch [19/120    avg_loss:1.853, val_acc:0.555]
Epoch [20/120    avg_loss:1.711, val_acc:0.607]
Epoch [21/120    avg_loss:1.676, val_acc:0.510]
Epoch [22/120    avg_loss:1.535, val_acc:0.572]
Epoch [23/120    avg_loss:1.387, val_acc:0.553]
Epoch [24/120    avg_loss:1.303, val_acc:0.585]
Epoch [25/120    avg_loss:1.200, val_acc:0.623]
Epoch [26/120    avg_loss:1.152, val_acc:0.655]
Epoch [27/120    avg_loss:1.016, val_acc:0.657]
Epoch [28/120    avg_loss:0.939, val_acc:0.703]
Epoch [29/120    avg_loss:0.864, val_acc:0.741]
Epoch [30/120    avg_loss:0.814, val_acc:0.761]
Epoch [31/120    avg_loss:0.789, val_acc:0.734]
Epoch [32/120    avg_loss:0.657, val_acc:0.767]
Epoch [33/120    avg_loss:0.587, val_acc:0.790]
Epoch [34/120    avg_loss:0.585, val_acc:0.802]
Epoch [35/120    avg_loss:0.538, val_acc:0.836]
Epoch [36/120    avg_loss:0.490, val_acc:0.846]
Epoch [37/120    avg_loss:0.458, val_acc:0.866]
Epoch [38/120    avg_loss:0.378, val_acc:0.866]
Epoch [39/120    avg_loss:0.340, val_acc:0.873]
Epoch [40/120    avg_loss:0.333, val_acc:0.892]
Epoch [41/120    avg_loss:0.339, val_acc:0.892]
Epoch [42/120    avg_loss:0.283, val_acc:0.909]
Epoch [43/120    avg_loss:0.303, val_acc:0.899]
Epoch [44/120    avg_loss:0.271, val_acc:0.905]
Epoch [45/120    avg_loss:0.239, val_acc:0.919]
Epoch [46/120    avg_loss:0.258, val_acc:0.908]
Epoch [47/120    avg_loss:0.240, val_acc:0.924]
Epoch [48/120    avg_loss:0.210, val_acc:0.916]
Epoch [49/120    avg_loss:0.196, val_acc:0.929]
Epoch [50/120    avg_loss:0.164, val_acc:0.928]
Epoch [51/120    avg_loss:0.171, val_acc:0.935]
Epoch [52/120    avg_loss:0.163, val_acc:0.922]
Epoch [53/120    avg_loss:0.140, val_acc:0.941]
Epoch [54/120    avg_loss:0.148, val_acc:0.936]
Epoch [55/120    avg_loss:0.190, val_acc:0.935]
Epoch [56/120    avg_loss:0.168, val_acc:0.935]
Epoch [57/120    avg_loss:0.147, val_acc:0.936]
Epoch [58/120    avg_loss:0.142, val_acc:0.936]
Epoch [59/120    avg_loss:0.127, val_acc:0.951]
Epoch [60/120    avg_loss:0.121, val_acc:0.952]
Epoch [61/120    avg_loss:0.130, val_acc:0.961]
Epoch [62/120    avg_loss:0.120, val_acc:0.946]
Epoch [63/120    avg_loss:0.107, val_acc:0.948]
Epoch [64/120    avg_loss:0.095, val_acc:0.955]
Epoch [65/120    avg_loss:0.080, val_acc:0.955]
Epoch [66/120    avg_loss:0.091, val_acc:0.957]
Epoch [67/120    avg_loss:0.068, val_acc:0.955]
Epoch [68/120    avg_loss:0.060, val_acc:0.971]
Epoch [69/120    avg_loss:0.080, val_acc:0.960]
Epoch [70/120    avg_loss:0.062, val_acc:0.965]
Epoch [71/120    avg_loss:0.073, val_acc:0.945]
Epoch [72/120    avg_loss:0.090, val_acc:0.949]
Epoch [73/120    avg_loss:0.063, val_acc:0.961]
Epoch [74/120    avg_loss:0.071, val_acc:0.966]
Epoch [75/120    avg_loss:0.053, val_acc:0.968]
Epoch [76/120    avg_loss:0.050, val_acc:0.963]
Epoch [77/120    avg_loss:0.052, val_acc:0.970]
Epoch [78/120    avg_loss:0.048, val_acc:0.968]
Epoch [79/120    avg_loss:0.040, val_acc:0.969]
Epoch [80/120    avg_loss:0.040, val_acc:0.976]
Epoch [81/120    avg_loss:0.048, val_acc:0.965]
Epoch [82/120    avg_loss:0.041, val_acc:0.972]
Epoch [83/120    avg_loss:0.044, val_acc:0.964]
Epoch [84/120    avg_loss:0.039, val_acc:0.975]
Epoch [85/120    avg_loss:0.038, val_acc:0.968]
Epoch [86/120    avg_loss:0.034, val_acc:0.973]
Epoch [87/120    avg_loss:0.027, val_acc:0.973]
Epoch [88/120    avg_loss:0.027, val_acc:0.971]
Epoch [89/120    avg_loss:0.030, val_acc:0.974]
Epoch [90/120    avg_loss:0.029, val_acc:0.970]
Epoch [91/120    avg_loss:0.025, val_acc:0.976]
Epoch [92/120    avg_loss:0.022, val_acc:0.979]
Epoch [93/120    avg_loss:0.022, val_acc:0.978]
Epoch [94/120    avg_loss:0.024, val_acc:0.975]
Epoch [95/120    avg_loss:0.029, val_acc:0.972]
Epoch [96/120    avg_loss:0.029, val_acc:0.977]
Epoch [97/120    avg_loss:0.029, val_acc:0.972]
Epoch [98/120    avg_loss:0.031, val_acc:0.976]
Epoch [99/120    avg_loss:0.028, val_acc:0.966]
Epoch [100/120    avg_loss:0.031, val_acc:0.971]
Epoch [101/120    avg_loss:0.023, val_acc:0.975]
Epoch [102/120    avg_loss:0.017, val_acc:0.980]
Epoch [103/120    avg_loss:0.018, val_acc:0.980]
Epoch [104/120    avg_loss:0.020, val_acc:0.980]
Epoch [105/120    avg_loss:0.019, val_acc:0.979]
Epoch [106/120    avg_loss:0.017, val_acc:0.973]
Epoch [107/120    avg_loss:0.020, val_acc:0.977]
Epoch [108/120    avg_loss:0.020, val_acc:0.978]
Epoch [109/120    avg_loss:0.017, val_acc:0.973]
Epoch [110/120    avg_loss:0.015, val_acc:0.976]
Epoch [111/120    avg_loss:0.014, val_acc:0.978]
Epoch [112/120    avg_loss:0.022, val_acc:0.969]
Epoch [113/120    avg_loss:0.017, val_acc:0.979]
Epoch [114/120    avg_loss:0.015, val_acc:0.980]
Epoch [115/120    avg_loss:0.013, val_acc:0.983]
Epoch [116/120    avg_loss:0.013, val_acc:0.976]
Epoch [117/120    avg_loss:0.014, val_acc:0.978]
Epoch [118/120    avg_loss:0.020, val_acc:0.974]
Epoch [119/120    avg_loss:0.021, val_acc:0.973]
Epoch [120/120    avg_loss:0.017, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1247    0    7    0    0    0    0    0    3   28    0    0
     0    0    0]
 [   0    0    2  719    7    3    0    0    0   10    1    2    3    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    0  828   40    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0    6 2182   21    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    9    0  519    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1136    3    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    27  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.87533875338754

F1 scores:
[       nan 0.98765432 0.98111723 0.97756628 0.96583144 0.99541284
 0.99695586 1.         1.         0.72727273 0.96111434 0.97759857
 0.96200185 1.         0.98611111 0.95209581 0.98224852]

Kappa:
0.9757604535976443
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f40736d3ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.839, val_acc:0.067]
Epoch [2/120    avg_loss:2.810, val_acc:0.098]
Epoch [3/120    avg_loss:2.779, val_acc:0.107]
Epoch [4/120    avg_loss:2.752, val_acc:0.134]
Epoch [5/120    avg_loss:2.724, val_acc:0.170]
Epoch [6/120    avg_loss:2.682, val_acc:0.224]
Epoch [7/120    avg_loss:2.646, val_acc:0.277]
Epoch [8/120    avg_loss:2.583, val_acc:0.290]
Epoch [9/120    avg_loss:2.531, val_acc:0.290]
Epoch [10/120    avg_loss:2.500, val_acc:0.327]
Epoch [11/120    avg_loss:2.437, val_acc:0.340]
Epoch [12/120    avg_loss:2.391, val_acc:0.392]
Epoch [13/120    avg_loss:2.357, val_acc:0.409]
Epoch [14/120    avg_loss:2.289, val_acc:0.425]
Epoch [15/120    avg_loss:2.230, val_acc:0.498]
Epoch [16/120    avg_loss:2.147, val_acc:0.521]
Epoch [17/120    avg_loss:2.111, val_acc:0.577]
Epoch [18/120    avg_loss:2.063, val_acc:0.603]
Epoch [19/120    avg_loss:1.975, val_acc:0.587]
Epoch [20/120    avg_loss:1.913, val_acc:0.578]
Epoch [21/120    avg_loss:1.782, val_acc:0.617]
Epoch [22/120    avg_loss:1.700, val_acc:0.620]
Epoch [23/120    avg_loss:1.654, val_acc:0.629]
Epoch [24/120    avg_loss:1.526, val_acc:0.687]
Epoch [25/120    avg_loss:1.441, val_acc:0.689]
Epoch [26/120    avg_loss:1.338, val_acc:0.688]
Epoch [27/120    avg_loss:1.268, val_acc:0.715]
Epoch [28/120    avg_loss:1.172, val_acc:0.725]
Epoch [29/120    avg_loss:1.091, val_acc:0.763]
Epoch [30/120    avg_loss:1.049, val_acc:0.785]
Epoch [31/120    avg_loss:1.014, val_acc:0.774]
Epoch [32/120    avg_loss:0.929, val_acc:0.778]
Epoch [33/120    avg_loss:0.847, val_acc:0.800]
Epoch [34/120    avg_loss:0.743, val_acc:0.821]
Epoch [35/120    avg_loss:0.678, val_acc:0.838]
Epoch [36/120    avg_loss:0.625, val_acc:0.855]
Epoch [37/120    avg_loss:0.574, val_acc:0.885]
Epoch [38/120    avg_loss:0.549, val_acc:0.837]
Epoch [39/120    avg_loss:0.564, val_acc:0.862]
Epoch [40/120    avg_loss:0.537, val_acc:0.875]
Epoch [41/120    avg_loss:0.495, val_acc:0.860]
Epoch [42/120    avg_loss:0.417, val_acc:0.877]
Epoch [43/120    avg_loss:0.370, val_acc:0.910]
Epoch [44/120    avg_loss:0.364, val_acc:0.898]
Epoch [45/120    avg_loss:0.329, val_acc:0.923]
Epoch [46/120    avg_loss:0.293, val_acc:0.932]
Epoch [47/120    avg_loss:0.300, val_acc:0.913]
Epoch [48/120    avg_loss:0.276, val_acc:0.933]
Epoch [49/120    avg_loss:0.252, val_acc:0.930]
Epoch [50/120    avg_loss:0.236, val_acc:0.925]
Epoch [51/120    avg_loss:0.218, val_acc:0.933]
Epoch [52/120    avg_loss:0.198, val_acc:0.955]
Epoch [53/120    avg_loss:0.192, val_acc:0.940]
Epoch [54/120    avg_loss:0.189, val_acc:0.938]
Epoch [55/120    avg_loss:0.178, val_acc:0.939]
Epoch [56/120    avg_loss:0.177, val_acc:0.941]
Epoch [57/120    avg_loss:0.177, val_acc:0.924]
Epoch [58/120    avg_loss:0.178, val_acc:0.939]
Epoch [59/120    avg_loss:0.181, val_acc:0.932]
Epoch [60/120    avg_loss:0.161, val_acc:0.962]
Epoch [61/120    avg_loss:0.132, val_acc:0.961]
Epoch [62/120    avg_loss:0.139, val_acc:0.945]
Epoch [63/120    avg_loss:0.121, val_acc:0.958]
Epoch [64/120    avg_loss:0.126, val_acc:0.954]
Epoch [65/120    avg_loss:0.110, val_acc:0.957]
Epoch [66/120    avg_loss:0.107, val_acc:0.960]
Epoch [67/120    avg_loss:0.091, val_acc:0.960]
Epoch [68/120    avg_loss:0.085, val_acc:0.962]
Epoch [69/120    avg_loss:0.091, val_acc:0.957]
Epoch [70/120    avg_loss:0.078, val_acc:0.961]
Epoch [71/120    avg_loss:0.078, val_acc:0.965]
Epoch [72/120    avg_loss:0.071, val_acc:0.952]
Epoch [73/120    avg_loss:0.085, val_acc:0.961]
Epoch [74/120    avg_loss:0.111, val_acc:0.960]
Epoch [75/120    avg_loss:0.078, val_acc:0.965]
Epoch [76/120    avg_loss:0.073, val_acc:0.978]
Epoch [77/120    avg_loss:0.059, val_acc:0.964]
Epoch [78/120    avg_loss:0.068, val_acc:0.966]
Epoch [79/120    avg_loss:0.064, val_acc:0.961]
Epoch [80/120    avg_loss:0.061, val_acc:0.964]
Epoch [81/120    avg_loss:0.053, val_acc:0.970]
Epoch [82/120    avg_loss:0.054, val_acc:0.949]
Epoch [83/120    avg_loss:0.060, val_acc:0.974]
Epoch [84/120    avg_loss:0.047, val_acc:0.968]
Epoch [85/120    avg_loss:0.046, val_acc:0.970]
Epoch [86/120    avg_loss:0.042, val_acc:0.973]
Epoch [87/120    avg_loss:0.041, val_acc:0.976]
Epoch [88/120    avg_loss:0.037, val_acc:0.973]
Epoch [89/120    avg_loss:0.040, val_acc:0.977]
Epoch [90/120    avg_loss:0.037, val_acc:0.978]
Epoch [91/120    avg_loss:0.030, val_acc:0.976]
Epoch [92/120    avg_loss:0.030, val_acc:0.979]
Epoch [93/120    avg_loss:0.033, val_acc:0.979]
Epoch [94/120    avg_loss:0.027, val_acc:0.978]
Epoch [95/120    avg_loss:0.025, val_acc:0.978]
Epoch [96/120    avg_loss:0.030, val_acc:0.979]
Epoch [97/120    avg_loss:0.028, val_acc:0.978]
Epoch [98/120    avg_loss:0.027, val_acc:0.977]
Epoch [99/120    avg_loss:0.028, val_acc:0.978]
Epoch [100/120    avg_loss:0.026, val_acc:0.977]
Epoch [101/120    avg_loss:0.029, val_acc:0.979]
Epoch [102/120    avg_loss:0.025, val_acc:0.978]
Epoch [103/120    avg_loss:0.023, val_acc:0.984]
Epoch [104/120    avg_loss:0.025, val_acc:0.982]
Epoch [105/120    avg_loss:0.027, val_acc:0.980]
Epoch [106/120    avg_loss:0.025, val_acc:0.980]
Epoch [107/120    avg_loss:0.026, val_acc:0.980]
Epoch [108/120    avg_loss:0.025, val_acc:0.982]
Epoch [109/120    avg_loss:0.024, val_acc:0.983]
Epoch [110/120    avg_loss:0.028, val_acc:0.980]
Epoch [111/120    avg_loss:0.024, val_acc:0.982]
Epoch [112/120    avg_loss:0.027, val_acc:0.982]
Epoch [113/120    avg_loss:0.024, val_acc:0.983]
Epoch [114/120    avg_loss:0.029, val_acc:0.980]
Epoch [115/120    avg_loss:0.023, val_acc:0.982]
Epoch [116/120    avg_loss:0.024, val_acc:0.982]
Epoch [117/120    avg_loss:0.023, val_acc:0.983]
Epoch [118/120    avg_loss:0.022, val_acc:0.983]
Epoch [119/120    avg_loss:0.023, val_acc:0.983]
Epoch [120/120    avg_loss:0.024, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1245    1    5    0    0    0    0    0    5   28    1    0
     0    0    0]
 [   0    0    3  695    1    4    4    0    0   18    3    0   19    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    4    0    0    3    1    0    0    0  853   14    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0   10 2185    9    2
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    1    7    7  511    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1132    6    0]
 [   0    0    0    0    0    0   21    0    0    0    0    0    0    0
    16  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.64769647696477

F1 scores:
[       nan 0.93506494 0.97992916 0.96193772 0.98139535 0.98190045
 0.9798357  1.         1.         0.61818182 0.97042093 0.98312711
 0.94981413 0.99462366 0.98994316 0.93514329 0.98809524]

Kappa:
0.9731758463318126
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f68bc5a9f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.797, val_acc:0.047]
Epoch [2/120    avg_loss:2.743, val_acc:0.086]
Epoch [3/120    avg_loss:2.692, val_acc:0.145]
Epoch [4/120    avg_loss:2.657, val_acc:0.224]
Epoch [5/120    avg_loss:2.614, val_acc:0.296]
Epoch [6/120    avg_loss:2.562, val_acc:0.351]
Epoch [7/120    avg_loss:2.528, val_acc:0.373]
Epoch [8/120    avg_loss:2.496, val_acc:0.392]
Epoch [9/120    avg_loss:2.445, val_acc:0.368]
Epoch [10/120    avg_loss:2.403, val_acc:0.420]
Epoch [11/120    avg_loss:2.355, val_acc:0.424]
Epoch [12/120    avg_loss:2.325, val_acc:0.439]
Epoch [13/120    avg_loss:2.252, val_acc:0.482]
Epoch [14/120    avg_loss:2.244, val_acc:0.511]
Epoch [15/120    avg_loss:2.173, val_acc:0.568]
Epoch [16/120    avg_loss:2.143, val_acc:0.562]
Epoch [17/120    avg_loss:2.060, val_acc:0.582]
Epoch [18/120    avg_loss:1.978, val_acc:0.626]
Epoch [19/120    avg_loss:1.916, val_acc:0.628]
Epoch [20/120    avg_loss:1.873, val_acc:0.637]
Epoch [21/120    avg_loss:1.817, val_acc:0.604]
Epoch [22/120    avg_loss:1.729, val_acc:0.663]
Epoch [23/120    avg_loss:1.655, val_acc:0.690]
Epoch [24/120    avg_loss:1.555, val_acc:0.690]
Epoch [25/120    avg_loss:1.509, val_acc:0.701]
Epoch [26/120    avg_loss:1.436, val_acc:0.699]
Epoch [27/120    avg_loss:1.412, val_acc:0.723]
Epoch [28/120    avg_loss:1.285, val_acc:0.736]
Epoch [29/120    avg_loss:1.192, val_acc:0.767]
Epoch [30/120    avg_loss:1.098, val_acc:0.772]
Epoch [31/120    avg_loss:1.039, val_acc:0.774]
Epoch [32/120    avg_loss:1.090, val_acc:0.792]
Epoch [33/120    avg_loss:0.955, val_acc:0.807]
Epoch [34/120    avg_loss:0.844, val_acc:0.832]
Epoch [35/120    avg_loss:0.800, val_acc:0.809]
Epoch [36/120    avg_loss:0.737, val_acc:0.841]
Epoch [37/120    avg_loss:0.670, val_acc:0.880]
Epoch [38/120    avg_loss:0.637, val_acc:0.866]
Epoch [39/120    avg_loss:0.586, val_acc:0.885]
Epoch [40/120    avg_loss:0.531, val_acc:0.874]
Epoch [41/120    avg_loss:0.487, val_acc:0.886]
Epoch [42/120    avg_loss:0.468, val_acc:0.903]
Epoch [43/120    avg_loss:0.421, val_acc:0.896]
Epoch [44/120    avg_loss:0.458, val_acc:0.866]
Epoch [45/120    avg_loss:0.387, val_acc:0.916]
Epoch [46/120    avg_loss:0.329, val_acc:0.934]
Epoch [47/120    avg_loss:0.296, val_acc:0.923]
Epoch [48/120    avg_loss:0.275, val_acc:0.939]
Epoch [49/120    avg_loss:0.284, val_acc:0.911]
Epoch [50/120    avg_loss:0.295, val_acc:0.932]
Epoch [51/120    avg_loss:0.243, val_acc:0.948]
Epoch [52/120    avg_loss:0.220, val_acc:0.941]
Epoch [53/120    avg_loss:0.190, val_acc:0.954]
Epoch [54/120    avg_loss:0.175, val_acc:0.965]
Epoch [55/120    avg_loss:0.165, val_acc:0.959]
Epoch [56/120    avg_loss:0.190, val_acc:0.957]
Epoch [57/120    avg_loss:0.168, val_acc:0.963]
Epoch [58/120    avg_loss:0.152, val_acc:0.960]
Epoch [59/120    avg_loss:0.136, val_acc:0.957]
Epoch [60/120    avg_loss:0.149, val_acc:0.961]
Epoch [61/120    avg_loss:0.142, val_acc:0.957]
Epoch [62/120    avg_loss:0.120, val_acc:0.961]
Epoch [63/120    avg_loss:0.108, val_acc:0.968]
Epoch [64/120    avg_loss:0.112, val_acc:0.962]
Epoch [65/120    avg_loss:0.127, val_acc:0.960]
Epoch [66/120    avg_loss:0.101, val_acc:0.973]
Epoch [67/120    avg_loss:0.094, val_acc:0.975]
Epoch [68/120    avg_loss:0.078, val_acc:0.964]
Epoch [69/120    avg_loss:0.087, val_acc:0.964]
Epoch [70/120    avg_loss:0.072, val_acc:0.977]
Epoch [71/120    avg_loss:0.071, val_acc:0.977]
Epoch [72/120    avg_loss:0.074, val_acc:0.971]
Epoch [73/120    avg_loss:0.079, val_acc:0.968]
Epoch [74/120    avg_loss:0.073, val_acc:0.982]
Epoch [75/120    avg_loss:0.064, val_acc:0.976]
Epoch [76/120    avg_loss:0.069, val_acc:0.970]
Epoch [77/120    avg_loss:0.066, val_acc:0.975]
Epoch [78/120    avg_loss:0.057, val_acc:0.976]
Epoch [79/120    avg_loss:0.051, val_acc:0.955]
Epoch [80/120    avg_loss:0.050, val_acc:0.988]
Epoch [81/120    avg_loss:0.057, val_acc:0.965]
Epoch [82/120    avg_loss:0.068, val_acc:0.968]
Epoch [83/120    avg_loss:0.063, val_acc:0.983]
Epoch [84/120    avg_loss:0.054, val_acc:0.974]
Epoch [85/120    avg_loss:0.061, val_acc:0.975]
Epoch [86/120    avg_loss:0.063, val_acc:0.975]
Epoch [87/120    avg_loss:0.049, val_acc:0.972]
Epoch [88/120    avg_loss:0.052, val_acc:0.982]
Epoch [89/120    avg_loss:0.053, val_acc:0.980]
Epoch [90/120    avg_loss:0.043, val_acc:0.979]
Epoch [91/120    avg_loss:0.041, val_acc:0.982]
Epoch [92/120    avg_loss:0.036, val_acc:0.982]
Epoch [93/120    avg_loss:0.035, val_acc:0.984]
Epoch [94/120    avg_loss:0.033, val_acc:0.986]
Epoch [95/120    avg_loss:0.025, val_acc:0.987]
Epoch [96/120    avg_loss:0.025, val_acc:0.987]
Epoch [97/120    avg_loss:0.024, val_acc:0.987]
Epoch [98/120    avg_loss:0.023, val_acc:0.989]
Epoch [99/120    avg_loss:0.024, val_acc:0.986]
Epoch [100/120    avg_loss:0.027, val_acc:0.989]
Epoch [101/120    avg_loss:0.024, val_acc:0.989]
Epoch [102/120    avg_loss:0.022, val_acc:0.988]
Epoch [103/120    avg_loss:0.024, val_acc:0.988]
Epoch [104/120    avg_loss:0.022, val_acc:0.988]
Epoch [105/120    avg_loss:0.022, val_acc:0.989]
Epoch [106/120    avg_loss:0.024, val_acc:0.988]
Epoch [107/120    avg_loss:0.022, val_acc:0.989]
Epoch [108/120    avg_loss:0.022, val_acc:0.986]
Epoch [109/120    avg_loss:0.023, val_acc:0.987]
Epoch [110/120    avg_loss:0.021, val_acc:0.989]
Epoch [111/120    avg_loss:0.022, val_acc:0.986]
Epoch [112/120    avg_loss:0.021, val_acc:0.989]
Epoch [113/120    avg_loss:0.026, val_acc:0.989]
Epoch [114/120    avg_loss:0.020, val_acc:0.990]
Epoch [115/120    avg_loss:0.021, val_acc:0.988]
Epoch [116/120    avg_loss:0.022, val_acc:0.987]
Epoch [117/120    avg_loss:0.020, val_acc:0.989]
Epoch [118/120    avg_loss:0.021, val_acc:0.988]
Epoch [119/120    avg_loss:0.023, val_acc:0.987]
Epoch [120/120    avg_loss:0.023, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1257    2    3    0    1    0    0    0    2   20    0    0
     0    0    0]
 [   0    0    2  701   10    9    1    0    0   16    1    1    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    2    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   28    0    0    3    0    0    0    0  838    4    2    0
     0    0    0]
 [   0    0   11    0    0    0    2    0    0    0   14 2168   15    0
     0    0    0]
 [   0    0    0    1    7    2    0    0    0    0    2    2  517    0
     0    1    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    2
  1135    2    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    78  269    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.12737127371274

F1 scores:
[       nan 0.98765432 0.97291022 0.96423659 0.95515695 0.97727273
 0.99545455 1.         0.997669   0.57692308 0.96766744 0.98433598
 0.96007428 0.99191375 0.96513605 0.86914378 0.98224852]

Kappa:
0.9672430676335031
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f342f3daef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.802, val_acc:0.202]
Epoch [2/120    avg_loss:2.764, val_acc:0.193]
Epoch [3/120    avg_loss:2.715, val_acc:0.202]
Epoch [4/120    avg_loss:2.675, val_acc:0.268]
Epoch [5/120    avg_loss:2.616, val_acc:0.366]
Epoch [6/120    avg_loss:2.564, val_acc:0.416]
Epoch [7/120    avg_loss:2.533, val_acc:0.433]
Epoch [8/120    avg_loss:2.462, val_acc:0.425]
Epoch [9/120    avg_loss:2.449, val_acc:0.483]
Epoch [10/120    avg_loss:2.383, val_acc:0.534]
Epoch [11/120    avg_loss:2.370, val_acc:0.552]
Epoch [12/120    avg_loss:2.303, val_acc:0.573]
Epoch [13/120    avg_loss:2.274, val_acc:0.579]
Epoch [14/120    avg_loss:2.216, val_acc:0.604]
Epoch [15/120    avg_loss:2.194, val_acc:0.583]
Epoch [16/120    avg_loss:2.140, val_acc:0.608]
Epoch [17/120    avg_loss:2.118, val_acc:0.625]
Epoch [18/120    avg_loss:2.044, val_acc:0.610]
Epoch [19/120    avg_loss:2.009, val_acc:0.634]
Epoch [20/120    avg_loss:1.935, val_acc:0.627]
Epoch [21/120    avg_loss:1.895, val_acc:0.658]
Epoch [22/120    avg_loss:1.770, val_acc:0.605]
Epoch [23/120    avg_loss:1.739, val_acc:0.627]
Epoch [24/120    avg_loss:1.607, val_acc:0.648]
Epoch [25/120    avg_loss:1.560, val_acc:0.678]
Epoch [26/120    avg_loss:1.482, val_acc:0.722]
Epoch [27/120    avg_loss:1.412, val_acc:0.712]
Epoch [28/120    avg_loss:1.330, val_acc:0.701]
Epoch [29/120    avg_loss:1.270, val_acc:0.737]
Epoch [30/120    avg_loss:1.193, val_acc:0.785]
Epoch [31/120    avg_loss:1.136, val_acc:0.780]
Epoch [32/120    avg_loss:1.030, val_acc:0.763]
Epoch [33/120    avg_loss:0.999, val_acc:0.771]
Epoch [34/120    avg_loss:0.962, val_acc:0.780]
Epoch [35/120    avg_loss:0.904, val_acc:0.784]
Epoch [36/120    avg_loss:0.848, val_acc:0.814]
Epoch [37/120    avg_loss:0.775, val_acc:0.804]
Epoch [38/120    avg_loss:0.731, val_acc:0.790]
Epoch [39/120    avg_loss:0.683, val_acc:0.815]
Epoch [40/120    avg_loss:0.661, val_acc:0.797]
Epoch [41/120    avg_loss:0.582, val_acc:0.842]
Epoch [42/120    avg_loss:0.518, val_acc:0.865]
Epoch [43/120    avg_loss:0.513, val_acc:0.903]
Epoch [44/120    avg_loss:0.446, val_acc:0.886]
Epoch [45/120    avg_loss:0.360, val_acc:0.913]
Epoch [46/120    avg_loss:0.342, val_acc:0.915]
Epoch [47/120    avg_loss:0.322, val_acc:0.914]
Epoch [48/120    avg_loss:0.318, val_acc:0.930]
Epoch [49/120    avg_loss:0.258, val_acc:0.914]
Epoch [50/120    avg_loss:0.259, val_acc:0.935]
Epoch [51/120    avg_loss:0.260, val_acc:0.935]
Epoch [52/120    avg_loss:0.220, val_acc:0.928]
Epoch [53/120    avg_loss:0.212, val_acc:0.945]
Epoch [54/120    avg_loss:0.204, val_acc:0.924]
Epoch [55/120    avg_loss:0.192, val_acc:0.946]
Epoch [56/120    avg_loss:0.167, val_acc:0.948]
Epoch [57/120    avg_loss:0.165, val_acc:0.954]
Epoch [58/120    avg_loss:0.157, val_acc:0.957]
Epoch [59/120    avg_loss:0.157, val_acc:0.941]
Epoch [60/120    avg_loss:0.129, val_acc:0.950]
Epoch [61/120    avg_loss:0.138, val_acc:0.953]
Epoch [62/120    avg_loss:0.129, val_acc:0.954]
Epoch [63/120    avg_loss:0.122, val_acc:0.952]
Epoch [64/120    avg_loss:0.112, val_acc:0.949]
Epoch [65/120    avg_loss:0.125, val_acc:0.940]
Epoch [66/120    avg_loss:0.110, val_acc:0.940]
Epoch [67/120    avg_loss:0.112, val_acc:0.949]
Epoch [68/120    avg_loss:0.101, val_acc:0.936]
Epoch [69/120    avg_loss:0.119, val_acc:0.954]
Epoch [70/120    avg_loss:0.097, val_acc:0.959]
Epoch [71/120    avg_loss:0.087, val_acc:0.958]
Epoch [72/120    avg_loss:0.077, val_acc:0.973]
Epoch [73/120    avg_loss:0.087, val_acc:0.967]
Epoch [74/120    avg_loss:0.117, val_acc:0.950]
Epoch [75/120    avg_loss:0.102, val_acc:0.953]
Epoch [76/120    avg_loss:0.079, val_acc:0.943]
Epoch [77/120    avg_loss:0.071, val_acc:0.954]
Epoch [78/120    avg_loss:0.066, val_acc:0.962]
Epoch [79/120    avg_loss:0.064, val_acc:0.971]
Epoch [80/120    avg_loss:0.059, val_acc:0.963]
Epoch [81/120    avg_loss:0.067, val_acc:0.967]
Epoch [82/120    avg_loss:0.049, val_acc:0.966]
Epoch [83/120    avg_loss:0.043, val_acc:0.975]
Epoch [84/120    avg_loss:0.053, val_acc:0.950]
Epoch [85/120    avg_loss:0.058, val_acc:0.965]
Epoch [86/120    avg_loss:0.078, val_acc:0.960]
Epoch [87/120    avg_loss:0.078, val_acc:0.954]
Epoch [88/120    avg_loss:0.056, val_acc:0.968]
Epoch [89/120    avg_loss:0.057, val_acc:0.960]
Epoch [90/120    avg_loss:0.050, val_acc:0.970]
Epoch [91/120    avg_loss:0.036, val_acc:0.977]
Epoch [92/120    avg_loss:0.038, val_acc:0.973]
Epoch [93/120    avg_loss:0.045, val_acc:0.965]
Epoch [94/120    avg_loss:0.031, val_acc:0.974]
Epoch [95/120    avg_loss:0.030, val_acc:0.979]
Epoch [96/120    avg_loss:0.027, val_acc:0.976]
Epoch [97/120    avg_loss:0.026, val_acc:0.979]
Epoch [98/120    avg_loss:0.036, val_acc:0.975]
Epoch [99/120    avg_loss:0.033, val_acc:0.967]
Epoch [100/120    avg_loss:0.030, val_acc:0.967]
Epoch [101/120    avg_loss:0.031, val_acc:0.978]
Epoch [102/120    avg_loss:0.024, val_acc:0.980]
Epoch [103/120    avg_loss:0.027, val_acc:0.974]
Epoch [104/120    avg_loss:0.032, val_acc:0.971]
Epoch [105/120    avg_loss:0.032, val_acc:0.972]
Epoch [106/120    avg_loss:0.026, val_acc:0.978]
Epoch [107/120    avg_loss:0.023, val_acc:0.972]
Epoch [108/120    avg_loss:0.022, val_acc:0.977]
Epoch [109/120    avg_loss:0.018, val_acc:0.973]
Epoch [110/120    avg_loss:0.024, val_acc:0.973]
Epoch [111/120    avg_loss:0.021, val_acc:0.972]
Epoch [112/120    avg_loss:0.022, val_acc:0.979]
Epoch [113/120    avg_loss:0.018, val_acc:0.972]
Epoch [114/120    avg_loss:0.020, val_acc:0.971]
Epoch [115/120    avg_loss:0.021, val_acc:0.975]
Epoch [116/120    avg_loss:0.017, val_acc:0.977]
Epoch [117/120    avg_loss:0.018, val_acc:0.978]
Epoch [118/120    avg_loss:0.014, val_acc:0.978]
Epoch [119/120    avg_loss:0.016, val_acc:0.978]
Epoch [120/120    avg_loss:0.017, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1253    3    4    0    1    0    0    0    2   22    0    0
     0    0    0]
 [   0    0    3  700    1   12    0    0    0   15    1    8    7    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    7    0    0   11    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    9    1    0    0    0  849   15    0    0
     0    1    0]
 [   0    0    5    0    0    0    1    0    0    0    9 2184   11    0
     0    0    0]
 [   0    0    0    0    0    9    0    0    0    0    2    2  517    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    0    0    1    0    0    0
  1123    5    0]
 [   0    0    0    0    0    0   24    0    0    0    0    0    0    0
     5  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.77777777777777

F1 scores:
[       nan 0.96202532 0.98428908 0.96551724 0.98839907 0.95604396
 0.97401633 1.         1.         0.5        0.97474168 0.98334084
 0.96635514 1.         0.99073666 0.94502229 0.98224852]

Kappa:
0.9746648475327209
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8242195ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.804, val_acc:0.055]
Epoch [2/120    avg_loss:2.767, val_acc:0.132]
Epoch [3/120    avg_loss:2.722, val_acc:0.224]
Epoch [4/120    avg_loss:2.654, val_acc:0.317]
Epoch [5/120    avg_loss:2.604, val_acc:0.349]
Epoch [6/120    avg_loss:2.531, val_acc:0.389]
Epoch [7/120    avg_loss:2.508, val_acc:0.404]
Epoch [8/120    avg_loss:2.450, val_acc:0.437]
Epoch [9/120    avg_loss:2.410, val_acc:0.453]
Epoch [10/120    avg_loss:2.360, val_acc:0.475]
Epoch [11/120    avg_loss:2.315, val_acc:0.471]
Epoch [12/120    avg_loss:2.262, val_acc:0.502]
Epoch [13/120    avg_loss:2.229, val_acc:0.508]
Epoch [14/120    avg_loss:2.151, val_acc:0.537]
Epoch [15/120    avg_loss:2.087, val_acc:0.528]
Epoch [16/120    avg_loss:2.032, val_acc:0.515]
Epoch [17/120    avg_loss:1.924, val_acc:0.524]
Epoch [18/120    avg_loss:1.857, val_acc:0.576]
Epoch [19/120    avg_loss:1.773, val_acc:0.590]
Epoch [20/120    avg_loss:1.693, val_acc:0.616]
Epoch [21/120    avg_loss:1.587, val_acc:0.648]
Epoch [22/120    avg_loss:1.484, val_acc:0.639]
Epoch [23/120    avg_loss:1.403, val_acc:0.638]
Epoch [24/120    avg_loss:1.371, val_acc:0.678]
Epoch [25/120    avg_loss:1.287, val_acc:0.676]
Epoch [26/120    avg_loss:1.185, val_acc:0.680]
Epoch [27/120    avg_loss:1.105, val_acc:0.660]
Epoch [28/120    avg_loss:1.064, val_acc:0.698]
Epoch [29/120    avg_loss:1.029, val_acc:0.698]
Epoch [30/120    avg_loss:0.994, val_acc:0.726]
Epoch [31/120    avg_loss:0.950, val_acc:0.724]
Epoch [32/120    avg_loss:0.894, val_acc:0.736]
Epoch [33/120    avg_loss:0.798, val_acc:0.749]
Epoch [34/120    avg_loss:0.754, val_acc:0.760]
Epoch [35/120    avg_loss:0.702, val_acc:0.778]
Epoch [36/120    avg_loss:0.700, val_acc:0.777]
Epoch [37/120    avg_loss:0.627, val_acc:0.799]
Epoch [38/120    avg_loss:0.575, val_acc:0.824]
Epoch [39/120    avg_loss:0.546, val_acc:0.827]
Epoch [40/120    avg_loss:0.536, val_acc:0.818]
Epoch [41/120    avg_loss:0.518, val_acc:0.866]
Epoch [42/120    avg_loss:0.447, val_acc:0.863]
Epoch [43/120    avg_loss:0.421, val_acc:0.883]
Epoch [44/120    avg_loss:0.397, val_acc:0.901]
Epoch [45/120    avg_loss:0.336, val_acc:0.889]
Epoch [46/120    avg_loss:0.358, val_acc:0.898]
Epoch [47/120    avg_loss:0.319, val_acc:0.904]
Epoch [48/120    avg_loss:0.297, val_acc:0.916]
Epoch [49/120    avg_loss:0.273, val_acc:0.902]
Epoch [50/120    avg_loss:0.254, val_acc:0.898]
Epoch [51/120    avg_loss:0.243, val_acc:0.925]
Epoch [52/120    avg_loss:0.209, val_acc:0.915]
Epoch [53/120    avg_loss:0.206, val_acc:0.909]
Epoch [54/120    avg_loss:0.189, val_acc:0.924]
Epoch [55/120    avg_loss:0.176, val_acc:0.938]
Epoch [56/120    avg_loss:0.165, val_acc:0.886]
Epoch [57/120    avg_loss:0.165, val_acc:0.945]
Epoch [58/120    avg_loss:0.158, val_acc:0.938]
Epoch [59/120    avg_loss:0.143, val_acc:0.930]
Epoch [60/120    avg_loss:0.160, val_acc:0.939]
Epoch [61/120    avg_loss:0.136, val_acc:0.937]
Epoch [62/120    avg_loss:0.113, val_acc:0.950]
Epoch [63/120    avg_loss:0.110, val_acc:0.948]
Epoch [64/120    avg_loss:0.135, val_acc:0.942]
Epoch [65/120    avg_loss:0.155, val_acc:0.925]
Epoch [66/120    avg_loss:0.143, val_acc:0.938]
Epoch [67/120    avg_loss:0.121, val_acc:0.953]
Epoch [68/120    avg_loss:0.100, val_acc:0.958]
Epoch [69/120    avg_loss:0.093, val_acc:0.951]
Epoch [70/120    avg_loss:0.092, val_acc:0.957]
Epoch [71/120    avg_loss:0.079, val_acc:0.953]
Epoch [72/120    avg_loss:0.090, val_acc:0.953]
Epoch [73/120    avg_loss:0.078, val_acc:0.960]
Epoch [74/120    avg_loss:0.070, val_acc:0.965]
Epoch [75/120    avg_loss:0.058, val_acc:0.970]
Epoch [76/120    avg_loss:0.055, val_acc:0.974]
Epoch [77/120    avg_loss:0.061, val_acc:0.975]
Epoch [78/120    avg_loss:0.055, val_acc:0.968]
Epoch [79/120    avg_loss:0.057, val_acc:0.972]
Epoch [80/120    avg_loss:0.050, val_acc:0.967]
Epoch [81/120    avg_loss:0.059, val_acc:0.977]
Epoch [82/120    avg_loss:0.051, val_acc:0.972]
Epoch [83/120    avg_loss:0.059, val_acc:0.973]
Epoch [84/120    avg_loss:0.059, val_acc:0.949]
Epoch [85/120    avg_loss:0.056, val_acc:0.977]
Epoch [86/120    avg_loss:0.047, val_acc:0.975]
Epoch [87/120    avg_loss:0.040, val_acc:0.977]
Epoch [88/120    avg_loss:0.038, val_acc:0.979]
Epoch [89/120    avg_loss:0.035, val_acc:0.976]
Epoch [90/120    avg_loss:0.034, val_acc:0.978]
Epoch [91/120    avg_loss:0.029, val_acc:0.984]
Epoch [92/120    avg_loss:0.028, val_acc:0.975]
Epoch [93/120    avg_loss:0.031, val_acc:0.979]
Epoch [94/120    avg_loss:0.029, val_acc:0.986]
Epoch [95/120    avg_loss:0.031, val_acc:0.977]
Epoch [96/120    avg_loss:0.039, val_acc:0.978]
Epoch [97/120    avg_loss:0.033, val_acc:0.974]
Epoch [98/120    avg_loss:0.033, val_acc:0.979]
Epoch [99/120    avg_loss:0.025, val_acc:0.977]
Epoch [100/120    avg_loss:0.028, val_acc:0.978]
Epoch [101/120    avg_loss:0.050, val_acc:0.962]
Epoch [102/120    avg_loss:0.055, val_acc:0.976]
Epoch [103/120    avg_loss:0.039, val_acc:0.959]
Epoch [104/120    avg_loss:0.047, val_acc:0.964]
Epoch [105/120    avg_loss:0.032, val_acc:0.980]
Epoch [106/120    avg_loss:0.026, val_acc:0.976]
Epoch [107/120    avg_loss:0.025, val_acc:0.982]
Epoch [108/120    avg_loss:0.021, val_acc:0.982]
Epoch [109/120    avg_loss:0.019, val_acc:0.986]
Epoch [110/120    avg_loss:0.020, val_acc:0.988]
Epoch [111/120    avg_loss:0.018, val_acc:0.987]
Epoch [112/120    avg_loss:0.020, val_acc:0.988]
Epoch [113/120    avg_loss:0.020, val_acc:0.987]
Epoch [114/120    avg_loss:0.020, val_acc:0.983]
Epoch [115/120    avg_loss:0.019, val_acc:0.988]
Epoch [116/120    avg_loss:0.019, val_acc:0.988]
Epoch [117/120    avg_loss:0.021, val_acc:0.987]
Epoch [118/120    avg_loss:0.020, val_acc:0.988]
Epoch [119/120    avg_loss:0.018, val_acc:0.989]
Epoch [120/120    avg_loss:0.020, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1255    2    4    0    0    0    0    0    3   21    0    0
     0    0    0]
 [   0    0    3  685   42    0    0    0    0    9    1    1    5    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    5    0    0    0    0  839   20    1    0
     0    3    0]
 [   0    0    6    0    0    0    3    0    0    0   19 2169   11    0
     2    0    0]
 [   0    0    0    0    1    1    0    0    0    0    0    1  529    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1133    6    0]
 [   0    0    0    0    0    0   32    0    0    0    0    0    0    0
    10  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.55013550135502

F1 scores:
[       nan 0.98765432 0.98161908 0.9553696  0.90063425 0.99084668
 0.97333333 1.         1.         0.75555556 0.96603339 0.98078227
 0.97962963 0.99730458 0.99211909 0.92284418 0.98823529]

Kappa:
0.9720800187185806
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdef7ea2ef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.832, val_acc:0.239]
Epoch [2/120    avg_loss:2.794, val_acc:0.305]
Epoch [3/120    avg_loss:2.745, val_acc:0.329]
Epoch [4/120    avg_loss:2.704, val_acc:0.380]
Epoch [5/120    avg_loss:2.649, val_acc:0.429]
Epoch [6/120    avg_loss:2.594, val_acc:0.458]
Epoch [7/120    avg_loss:2.537, val_acc:0.474]
Epoch [8/120    avg_loss:2.500, val_acc:0.488]
Epoch [9/120    avg_loss:2.439, val_acc:0.502]
Epoch [10/120    avg_loss:2.400, val_acc:0.522]
Epoch [11/120    avg_loss:2.388, val_acc:0.512]
Epoch [12/120    avg_loss:2.329, val_acc:0.524]
Epoch [13/120    avg_loss:2.278, val_acc:0.524]
Epoch [14/120    avg_loss:2.240, val_acc:0.550]
Epoch [15/120    avg_loss:2.211, val_acc:0.552]
Epoch [16/120    avg_loss:2.151, val_acc:0.586]
Epoch [17/120    avg_loss:2.069, val_acc:0.574]
Epoch [18/120    avg_loss:2.045, val_acc:0.579]
Epoch [19/120    avg_loss:2.025, val_acc:0.613]
Epoch [20/120    avg_loss:1.995, val_acc:0.664]
Epoch [21/120    avg_loss:1.917, val_acc:0.659]
Epoch [22/120    avg_loss:1.840, val_acc:0.665]
Epoch [23/120    avg_loss:1.759, val_acc:0.705]
Epoch [24/120    avg_loss:1.697, val_acc:0.690]
Epoch [25/120    avg_loss:1.599, val_acc:0.717]
Epoch [26/120    avg_loss:1.543, val_acc:0.755]
Epoch [27/120    avg_loss:1.445, val_acc:0.746]
Epoch [28/120    avg_loss:1.376, val_acc:0.742]
Epoch [29/120    avg_loss:1.277, val_acc:0.760]
Epoch [30/120    avg_loss:1.127, val_acc:0.788]
Epoch [31/120    avg_loss:1.108, val_acc:0.810]
Epoch [32/120    avg_loss:1.061, val_acc:0.818]
Epoch [33/120    avg_loss:1.012, val_acc:0.766]
Epoch [34/120    avg_loss:0.980, val_acc:0.810]
Epoch [35/120    avg_loss:0.935, val_acc:0.842]
Epoch [36/120    avg_loss:0.797, val_acc:0.798]
Epoch [37/120    avg_loss:0.730, val_acc:0.853]
Epoch [38/120    avg_loss:0.667, val_acc:0.874]
Epoch [39/120    avg_loss:0.628, val_acc:0.875]
Epoch [40/120    avg_loss:0.566, val_acc:0.887]
Epoch [41/120    avg_loss:0.556, val_acc:0.879]
Epoch [42/120    avg_loss:0.513, val_acc:0.899]
Epoch [43/120    avg_loss:0.445, val_acc:0.911]
Epoch [44/120    avg_loss:0.429, val_acc:0.914]
Epoch [45/120    avg_loss:0.403, val_acc:0.910]
Epoch [46/120    avg_loss:0.348, val_acc:0.939]
Epoch [47/120    avg_loss:0.386, val_acc:0.912]
Epoch [48/120    avg_loss:0.357, val_acc:0.910]
Epoch [49/120    avg_loss:0.338, val_acc:0.911]
Epoch [50/120    avg_loss:0.282, val_acc:0.938]
Epoch [51/120    avg_loss:0.260, val_acc:0.940]
Epoch [52/120    avg_loss:0.238, val_acc:0.953]
Epoch [53/120    avg_loss:0.216, val_acc:0.939]
Epoch [54/120    avg_loss:0.205, val_acc:0.925]
Epoch [55/120    avg_loss:0.195, val_acc:0.953]
Epoch [56/120    avg_loss:0.241, val_acc:0.921]
Epoch [57/120    avg_loss:0.218, val_acc:0.955]
Epoch [58/120    avg_loss:0.179, val_acc:0.948]
Epoch [59/120    avg_loss:0.156, val_acc:0.955]
Epoch [60/120    avg_loss:0.144, val_acc:0.963]
Epoch [61/120    avg_loss:0.143, val_acc:0.928]
Epoch [62/120    avg_loss:0.146, val_acc:0.947]
Epoch [63/120    avg_loss:0.141, val_acc:0.951]
Epoch [64/120    avg_loss:0.124, val_acc:0.958]
Epoch [65/120    avg_loss:0.114, val_acc:0.964]
Epoch [66/120    avg_loss:0.105, val_acc:0.955]
Epoch [67/120    avg_loss:0.093, val_acc:0.954]
Epoch [68/120    avg_loss:0.087, val_acc:0.960]
Epoch [69/120    avg_loss:0.081, val_acc:0.967]
Epoch [70/120    avg_loss:0.075, val_acc:0.973]
Epoch [71/120    avg_loss:0.076, val_acc:0.971]
Epoch [72/120    avg_loss:0.077, val_acc:0.964]
Epoch [73/120    avg_loss:0.108, val_acc:0.959]
Epoch [74/120    avg_loss:0.087, val_acc:0.967]
Epoch [75/120    avg_loss:0.090, val_acc:0.973]
Epoch [76/120    avg_loss:0.076, val_acc:0.941]
Epoch [77/120    avg_loss:0.096, val_acc:0.932]
Epoch [78/120    avg_loss:0.075, val_acc:0.974]
Epoch [79/120    avg_loss:0.069, val_acc:0.975]
Epoch [80/120    avg_loss:0.067, val_acc:0.975]
Epoch [81/120    avg_loss:0.063, val_acc:0.953]
Epoch [82/120    avg_loss:0.059, val_acc:0.977]
Epoch [83/120    avg_loss:0.056, val_acc:0.971]
Epoch [84/120    avg_loss:0.056, val_acc:0.974]
Epoch [85/120    avg_loss:0.057, val_acc:0.974]
Epoch [86/120    avg_loss:0.055, val_acc:0.976]
Epoch [87/120    avg_loss:0.048, val_acc:0.983]
Epoch [88/120    avg_loss:0.040, val_acc:0.980]
Epoch [89/120    avg_loss:0.040, val_acc:0.966]
Epoch [90/120    avg_loss:0.048, val_acc:0.975]
Epoch [91/120    avg_loss:0.051, val_acc:0.978]
Epoch [92/120    avg_loss:0.038, val_acc:0.983]
Epoch [93/120    avg_loss:0.034, val_acc:0.970]
Epoch [94/120    avg_loss:0.035, val_acc:0.982]
Epoch [95/120    avg_loss:0.031, val_acc:0.982]
Epoch [96/120    avg_loss:0.030, val_acc:0.980]
Epoch [97/120    avg_loss:0.037, val_acc:0.980]
Epoch [98/120    avg_loss:0.038, val_acc:0.985]
Epoch [99/120    avg_loss:0.029, val_acc:0.985]
Epoch [100/120    avg_loss:0.028, val_acc:0.983]
Epoch [101/120    avg_loss:0.026, val_acc:0.985]
Epoch [102/120    avg_loss:0.028, val_acc:0.980]
Epoch [103/120    avg_loss:0.032, val_acc:0.980]
Epoch [104/120    avg_loss:0.029, val_acc:0.977]
Epoch [105/120    avg_loss:0.025, val_acc:0.983]
Epoch [106/120    avg_loss:0.030, val_acc:0.980]
Epoch [107/120    avg_loss:0.024, val_acc:0.978]
Epoch [108/120    avg_loss:0.020, val_acc:0.982]
Epoch [109/120    avg_loss:0.018, val_acc:0.987]
Epoch [110/120    avg_loss:0.019, val_acc:0.987]
Epoch [111/120    avg_loss:0.034, val_acc:0.980]
Epoch [112/120    avg_loss:0.021, val_acc:0.980]
Epoch [113/120    avg_loss:0.025, val_acc:0.986]
Epoch [114/120    avg_loss:0.023, val_acc:0.983]
Epoch [115/120    avg_loss:0.030, val_acc:0.978]
Epoch [116/120    avg_loss:0.031, val_acc:0.986]
Epoch [117/120    avg_loss:0.026, val_acc:0.987]
Epoch [118/120    avg_loss:0.022, val_acc:0.986]
Epoch [119/120    avg_loss:0.016, val_acc:0.986]
Epoch [120/120    avg_loss:0.024, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1262    5    0    0    0    0    0    1    3   11    3    0
     0    0    0]
 [   0    0    2  698    2    9    3    0    0   15    0    0   16    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    7    0    0    0    1    0    0
     5    0    0]
 [   0    0    1    0    0    0  654    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   42   45    0    5    2    0    0    0  763    9    4    0
     0    5    0]
 [   0    0   33    2    0    0    3    1    0    0    4 2161    6    0
     0    0    0]
 [   0    0    0    2    0    3    0    0    0    3    8    4  505    0
     0    6    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1136    3    0]
 [   0    0    0    0    0    0   36    0    0    0    0    0    0    0
     4  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.46612466124661

F1 scores:
[       nan 0.975      0.96152381 0.93004664 0.9953271  0.96567506
 0.96531365 0.86206897 1.         0.60377358 0.92205438 0.98294292
 0.94480823 0.99462366 0.99431072 0.91916168 0.97647059]

Kappa:
0.9597259902265938
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:13:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efd1274aef0>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.805, val_acc:0.068]
Epoch [2/120    avg_loss:2.772, val_acc:0.223]
Epoch [3/120    avg_loss:2.737, val_acc:0.254]
Epoch [4/120    avg_loss:2.705, val_acc:0.254]
Epoch [5/120    avg_loss:2.666, val_acc:0.329]
Epoch [6/120    avg_loss:2.631, val_acc:0.392]
Epoch [7/120    avg_loss:2.585, val_acc:0.458]
Epoch [8/120    avg_loss:2.525, val_acc:0.482]
Epoch [9/120    avg_loss:2.464, val_acc:0.504]
Epoch [10/120    avg_loss:2.434, val_acc:0.510]
Epoch [11/120    avg_loss:2.375, val_acc:0.520]
Epoch [12/120    avg_loss:2.314, val_acc:0.567]
Epoch [13/120    avg_loss:2.274, val_acc:0.551]
Epoch [14/120    avg_loss:2.205, val_acc:0.572]
Epoch [15/120    avg_loss:2.176, val_acc:0.555]
Epoch [16/120    avg_loss:2.159, val_acc:0.518]
Epoch [17/120    avg_loss:2.126, val_acc:0.566]
Epoch [18/120    avg_loss:2.014, val_acc:0.528]
Epoch [19/120    avg_loss:1.945, val_acc:0.564]
Epoch [20/120    avg_loss:1.887, val_acc:0.565]
Epoch [21/120    avg_loss:1.775, val_acc:0.611]
Epoch [22/120    avg_loss:1.733, val_acc:0.603]
Epoch [23/120    avg_loss:1.642, val_acc:0.636]
Epoch [24/120    avg_loss:1.601, val_acc:0.640]
Epoch [25/120    avg_loss:1.515, val_acc:0.667]
Epoch [26/120    avg_loss:1.469, val_acc:0.655]
Epoch [27/120    avg_loss:1.386, val_acc:0.691]
Epoch [28/120    avg_loss:1.277, val_acc:0.707]
Epoch [29/120    avg_loss:1.238, val_acc:0.699]
Epoch [30/120    avg_loss:1.234, val_acc:0.709]
Epoch [31/120    avg_loss:1.103, val_acc:0.727]
Epoch [32/120    avg_loss:1.154, val_acc:0.689]
Epoch [33/120    avg_loss:1.167, val_acc:0.730]
Epoch [34/120    avg_loss:1.019, val_acc:0.732]
Epoch [35/120    avg_loss:0.947, val_acc:0.773]
Epoch [36/120    avg_loss:0.834, val_acc:0.797]
Epoch [37/120    avg_loss:0.784, val_acc:0.783]
Epoch [38/120    avg_loss:0.755, val_acc:0.824]
Epoch [39/120    avg_loss:0.698, val_acc:0.816]
Epoch [40/120    avg_loss:0.619, val_acc:0.845]
Epoch [41/120    avg_loss:0.618, val_acc:0.857]
Epoch [42/120    avg_loss:0.560, val_acc:0.835]
Epoch [43/120    avg_loss:0.517, val_acc:0.872]
Epoch [44/120    avg_loss:0.495, val_acc:0.837]
Epoch [45/120    avg_loss:0.478, val_acc:0.868]
Epoch [46/120    avg_loss:0.452, val_acc:0.874]
Epoch [47/120    avg_loss:0.391, val_acc:0.876]
Epoch [48/120    avg_loss:0.384, val_acc:0.895]
Epoch [49/120    avg_loss:0.346, val_acc:0.890]
Epoch [50/120    avg_loss:0.331, val_acc:0.902]
Epoch [51/120    avg_loss:0.278, val_acc:0.908]
Epoch [52/120    avg_loss:0.277, val_acc:0.910]
Epoch [53/120    avg_loss:0.262, val_acc:0.922]
Epoch [54/120    avg_loss:0.242, val_acc:0.904]
Epoch [55/120    avg_loss:0.240, val_acc:0.920]
Epoch [56/120    avg_loss:0.194, val_acc:0.920]
Epoch [57/120    avg_loss:0.204, val_acc:0.922]
Epoch [58/120    avg_loss:0.205, val_acc:0.927]
Epoch [59/120    avg_loss:0.186, val_acc:0.935]
Epoch [60/120    avg_loss:0.167, val_acc:0.940]
Epoch [61/120    avg_loss:0.177, val_acc:0.915]
Epoch [62/120    avg_loss:0.174, val_acc:0.929]
Epoch [63/120    avg_loss:0.155, val_acc:0.949]
Epoch [64/120    avg_loss:0.158, val_acc:0.942]
Epoch [65/120    avg_loss:0.141, val_acc:0.948]
Epoch [66/120    avg_loss:0.134, val_acc:0.951]
Epoch [67/120    avg_loss:0.132, val_acc:0.932]
Epoch [68/120    avg_loss:0.107, val_acc:0.947]
Epoch [69/120    avg_loss:0.103, val_acc:0.950]
Epoch [70/120    avg_loss:0.099, val_acc:0.950]
Epoch [71/120    avg_loss:0.112, val_acc:0.952]
Epoch [72/120    avg_loss:0.092, val_acc:0.963]
Epoch [73/120    avg_loss:0.091, val_acc:0.950]
Epoch [74/120    avg_loss:0.087, val_acc:0.955]
Epoch [75/120    avg_loss:0.085, val_acc:0.960]
Epoch [76/120    avg_loss:0.098, val_acc:0.950]
Epoch [77/120    avg_loss:0.085, val_acc:0.957]
Epoch [78/120    avg_loss:0.069, val_acc:0.950]
Epoch [79/120    avg_loss:0.082, val_acc:0.946]
Epoch [80/120    avg_loss:0.067, val_acc:0.959]
Epoch [81/120    avg_loss:0.074, val_acc:0.957]
Epoch [82/120    avg_loss:0.070, val_acc:0.959]
Epoch [83/120    avg_loss:0.060, val_acc:0.967]
Epoch [84/120    avg_loss:0.056, val_acc:0.971]
Epoch [85/120    avg_loss:0.051, val_acc:0.959]
Epoch [86/120    avg_loss:0.046, val_acc:0.970]
Epoch [87/120    avg_loss:0.056, val_acc:0.964]
Epoch [88/120    avg_loss:0.050, val_acc:0.960]
Epoch [89/120    avg_loss:0.065, val_acc:0.963]
Epoch [90/120    avg_loss:0.053, val_acc:0.971]
Epoch [91/120    avg_loss:0.042, val_acc:0.963]
Epoch [92/120    avg_loss:0.045, val_acc:0.968]
Epoch [93/120    avg_loss:0.064, val_acc:0.952]
Epoch [94/120    avg_loss:0.056, val_acc:0.955]
Epoch [95/120    avg_loss:0.059, val_acc:0.966]
Epoch [96/120    avg_loss:0.068, val_acc:0.950]
Epoch [97/120    avg_loss:0.049, val_acc:0.959]
Epoch [98/120    avg_loss:0.048, val_acc:0.968]
Epoch [99/120    avg_loss:0.039, val_acc:0.964]
Epoch [100/120    avg_loss:0.047, val_acc:0.965]
Epoch [101/120    avg_loss:0.055, val_acc:0.892]
Epoch [102/120    avg_loss:0.092, val_acc:0.964]
Epoch [103/120    avg_loss:0.051, val_acc:0.972]
Epoch [104/120    avg_loss:0.044, val_acc:0.964]
Epoch [105/120    avg_loss:0.041, val_acc:0.963]
Epoch [106/120    avg_loss:0.061, val_acc:0.966]
Epoch [107/120    avg_loss:0.050, val_acc:0.968]
Epoch [108/120    avg_loss:0.045, val_acc:0.973]
Epoch [109/120    avg_loss:0.054, val_acc:0.971]
Epoch [110/120    avg_loss:0.043, val_acc:0.947]
Epoch [111/120    avg_loss:0.049, val_acc:0.965]
Epoch [112/120    avg_loss:0.032, val_acc:0.976]
Epoch [113/120    avg_loss:0.032, val_acc:0.979]
Epoch [114/120    avg_loss:0.033, val_acc:0.970]
Epoch [115/120    avg_loss:0.025, val_acc:0.973]
Epoch [116/120    avg_loss:0.024, val_acc:0.975]
Epoch [117/120    avg_loss:0.027, val_acc:0.963]
Epoch [118/120    avg_loss:0.032, val_acc:0.978]
Epoch [119/120    avg_loss:0.029, val_acc:0.970]
Epoch [120/120    avg_loss:0.029, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    0    0    0    0    0    0    2    0    0
     0    0    0]
 [   0    0 1256    8    8    1    0    0    0    0    1   11    0    0
     0    0    0]
 [   0    0   14  676    0   44    0    0    0    1    1    6    4    1
     0    0    0]
 [   0    0    0    5  208    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    4    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    3    0    0
     0    0    0]
 [   0    0   10    0    0   15    0    0    0    0  842    8    0    0
     0    0    0]
 [   0    0   18    0    0    0    0    0    0    0   21 2168    3    0
     0    0    0]
 [   0    0    0    1    0   19    0    0    0    0    0    6  506    0
     0    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    2    3    0    0
  1131    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
    37  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.05149051490515

F1 scores:
[       nan 0.96202532 0.97213622 0.94084899 0.96969697 0.90410959
 0.99847561 0.92592593 0.997669   0.85714286 0.96670494 0.98143957
 0.96472831 0.99459459 0.97964487 0.94045802 0.98823529]

Kappa:
0.9663804512051076
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdb3451df28>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.821, val_acc:0.135]
Epoch [2/120    avg_loss:2.781, val_acc:0.200]
Epoch [3/120    avg_loss:2.735, val_acc:0.302]
Epoch [4/120    avg_loss:2.683, val_acc:0.352]
Epoch [5/120    avg_loss:2.626, val_acc:0.367]
Epoch [6/120    avg_loss:2.552, val_acc:0.373]
Epoch [7/120    avg_loss:2.520, val_acc:0.383]
Epoch [8/120    avg_loss:2.467, val_acc:0.392]
Epoch [9/120    avg_loss:2.439, val_acc:0.414]
Epoch [10/120    avg_loss:2.391, val_acc:0.453]
Epoch [11/120    avg_loss:2.356, val_acc:0.488]
Epoch [12/120    avg_loss:2.307, val_acc:0.480]
Epoch [13/120    avg_loss:2.284, val_acc:0.492]
Epoch [14/120    avg_loss:2.242, val_acc:0.502]
Epoch [15/120    avg_loss:2.194, val_acc:0.537]
Epoch [16/120    avg_loss:2.147, val_acc:0.564]
Epoch [17/120    avg_loss:2.079, val_acc:0.529]
Epoch [18/120    avg_loss:2.028, val_acc:0.571]
Epoch [19/120    avg_loss:1.920, val_acc:0.575]
Epoch [20/120    avg_loss:1.885, val_acc:0.534]
Epoch [21/120    avg_loss:1.821, val_acc:0.584]
Epoch [22/120    avg_loss:1.702, val_acc:0.582]
Epoch [23/120    avg_loss:1.620, val_acc:0.625]
Epoch [24/120    avg_loss:1.572, val_acc:0.647]
Epoch [25/120    avg_loss:1.499, val_acc:0.683]
Epoch [26/120    avg_loss:1.426, val_acc:0.691]
Epoch [27/120    avg_loss:1.325, val_acc:0.723]
Epoch [28/120    avg_loss:1.224, val_acc:0.732]
Epoch [29/120    avg_loss:1.169, val_acc:0.749]
Epoch [30/120    avg_loss:1.101, val_acc:0.745]
Epoch [31/120    avg_loss:1.042, val_acc:0.773]
Epoch [32/120    avg_loss:0.954, val_acc:0.791]
Epoch [33/120    avg_loss:0.850, val_acc:0.824]
Epoch [34/120    avg_loss:0.826, val_acc:0.820]
Epoch [35/120    avg_loss:0.777, val_acc:0.825]
Epoch [36/120    avg_loss:0.702, val_acc:0.853]
Epoch [37/120    avg_loss:0.628, val_acc:0.851]
Epoch [38/120    avg_loss:0.568, val_acc:0.858]
Epoch [39/120    avg_loss:0.575, val_acc:0.843]
Epoch [40/120    avg_loss:0.502, val_acc:0.863]
Epoch [41/120    avg_loss:0.479, val_acc:0.848]
Epoch [42/120    avg_loss:0.433, val_acc:0.875]
Epoch [43/120    avg_loss:0.418, val_acc:0.889]
Epoch [44/120    avg_loss:0.393, val_acc:0.888]
Epoch [45/120    avg_loss:0.352, val_acc:0.874]
Epoch [46/120    avg_loss:0.329, val_acc:0.887]
Epoch [47/120    avg_loss:0.276, val_acc:0.924]
Epoch [48/120    avg_loss:0.262, val_acc:0.918]
Epoch [49/120    avg_loss:0.245, val_acc:0.900]
Epoch [50/120    avg_loss:0.229, val_acc:0.928]
Epoch [51/120    avg_loss:0.201, val_acc:0.942]
Epoch [52/120    avg_loss:0.226, val_acc:0.896]
Epoch [53/120    avg_loss:0.241, val_acc:0.922]
Epoch [54/120    avg_loss:0.197, val_acc:0.938]
Epoch [55/120    avg_loss:0.171, val_acc:0.936]
Epoch [56/120    avg_loss:0.173, val_acc:0.951]
Epoch [57/120    avg_loss:0.182, val_acc:0.943]
Epoch [58/120    avg_loss:0.149, val_acc:0.943]
Epoch [59/120    avg_loss:0.182, val_acc:0.942]
Epoch [60/120    avg_loss:0.135, val_acc:0.943]
Epoch [61/120    avg_loss:0.128, val_acc:0.957]
Epoch [62/120    avg_loss:0.142, val_acc:0.898]
Epoch [63/120    avg_loss:0.167, val_acc:0.929]
Epoch [64/120    avg_loss:0.127, val_acc:0.943]
Epoch [65/120    avg_loss:0.112, val_acc:0.960]
Epoch [66/120    avg_loss:0.108, val_acc:0.954]
Epoch [67/120    avg_loss:0.101, val_acc:0.967]
Epoch [68/120    avg_loss:0.115, val_acc:0.961]
Epoch [69/120    avg_loss:0.100, val_acc:0.967]
Epoch [70/120    avg_loss:0.082, val_acc:0.964]
Epoch [71/120    avg_loss:0.081, val_acc:0.962]
Epoch [72/120    avg_loss:0.074, val_acc:0.971]
Epoch [73/120    avg_loss:0.072, val_acc:0.970]
Epoch [74/120    avg_loss:0.078, val_acc:0.972]
Epoch [75/120    avg_loss:0.077, val_acc:0.961]
Epoch [76/120    avg_loss:0.093, val_acc:0.957]
Epoch [77/120    avg_loss:0.089, val_acc:0.971]
Epoch [78/120    avg_loss:0.101, val_acc:0.961]
Epoch [79/120    avg_loss:0.079, val_acc:0.971]
Epoch [80/120    avg_loss:0.075, val_acc:0.971]
Epoch [81/120    avg_loss:0.057, val_acc:0.972]
Epoch [82/120    avg_loss:0.063, val_acc:0.860]
Epoch [83/120    avg_loss:0.305, val_acc:0.862]
Epoch [84/120    avg_loss:0.266, val_acc:0.897]
Epoch [85/120    avg_loss:0.260, val_acc:0.930]
Epoch [86/120    avg_loss:0.159, val_acc:0.937]
Epoch [87/120    avg_loss:0.146, val_acc:0.930]
Epoch [88/120    avg_loss:0.122, val_acc:0.950]
Epoch [89/120    avg_loss:0.092, val_acc:0.961]
Epoch [90/120    avg_loss:0.074, val_acc:0.972]
Epoch [91/120    avg_loss:0.069, val_acc:0.972]
Epoch [92/120    avg_loss:0.063, val_acc:0.973]
Epoch [93/120    avg_loss:0.063, val_acc:0.972]
Epoch [94/120    avg_loss:0.054, val_acc:0.966]
Epoch [95/120    avg_loss:0.048, val_acc:0.977]
Epoch [96/120    avg_loss:0.044, val_acc:0.965]
Epoch [97/120    avg_loss:0.061, val_acc:0.976]
Epoch [98/120    avg_loss:0.044, val_acc:0.984]
Epoch [99/120    avg_loss:0.040, val_acc:0.976]
Epoch [100/120    avg_loss:0.047, val_acc:0.958]
Epoch [101/120    avg_loss:0.044, val_acc:0.973]
Epoch [102/120    avg_loss:0.041, val_acc:0.980]
Epoch [103/120    avg_loss:0.038, val_acc:0.972]
Epoch [104/120    avg_loss:0.035, val_acc:0.984]
Epoch [105/120    avg_loss:0.042, val_acc:0.980]
Epoch [106/120    avg_loss:0.035, val_acc:0.975]
Epoch [107/120    avg_loss:0.033, val_acc:0.978]
Epoch [108/120    avg_loss:0.038, val_acc:0.977]
Epoch [109/120    avg_loss:0.037, val_acc:0.979]
Epoch [110/120    avg_loss:0.025, val_acc:0.985]
Epoch [111/120    avg_loss:0.023, val_acc:0.985]
Epoch [112/120    avg_loss:0.021, val_acc:0.988]
Epoch [113/120    avg_loss:0.023, val_acc:0.979]
Epoch [114/120    avg_loss:0.028, val_acc:0.982]
Epoch [115/120    avg_loss:0.024, val_acc:0.980]
Epoch [116/120    avg_loss:0.023, val_acc:0.979]
Epoch [117/120    avg_loss:0.027, val_acc:0.982]
Epoch [118/120    avg_loss:0.028, val_acc:0.983]
Epoch [119/120    avg_loss:0.026, val_acc:0.986]
Epoch [120/120    avg_loss:0.025, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1209    0    3    0    1    0    0    0    7   65    0    0
     0    0    0]
 [   0    0    3  692   11    1    0    0    0    6    6    8   20    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  426    0    0    0    2    0
     0    0    0]
 [   0    0    0    1    0    0    3    0    0   13    0    0    1    0
     0    0    0]
 [   0    0    1    0    0    0    1    0    0    0  842   30    0    0
     0    1    0]
 [   0    0    0    0    0    4    5    0    0    0   15 2183    3    0
     0    0    0]
 [   0    0    1    0    5    1    0    0    0    0   11    8  502    0
     1    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    1    0    0
  1131    4    0]
 [   0    0    1    0    0    0   34    0    0    0    0    0    0    0
    13  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.8130081300813

F1 scores:
[       nan 0.96385542 0.9672     0.95977809 0.95259594 0.992
 0.9660767  1.         0.9953271  0.68421053 0.95681818 0.96871533
 0.94538606 1.         0.99036778 0.91858679 0.97109827]

Kappa:
0.96362002281832
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f013ebfbf60>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.813, val_acc:0.168]
Epoch [2/120    avg_loss:2.779, val_acc:0.209]
Epoch [3/120    avg_loss:2.744, val_acc:0.255]
Epoch [4/120    avg_loss:2.715, val_acc:0.299]
Epoch [5/120    avg_loss:2.662, val_acc:0.329]
Epoch [6/120    avg_loss:2.611, val_acc:0.329]
Epoch [7/120    avg_loss:2.521, val_acc:0.339]
Epoch [8/120    avg_loss:2.508, val_acc:0.355]
Epoch [9/120    avg_loss:2.467, val_acc:0.392]
Epoch [10/120    avg_loss:2.398, val_acc:0.411]
Epoch [11/120    avg_loss:2.379, val_acc:0.443]
Epoch [12/120    avg_loss:2.335, val_acc:0.475]
Epoch [13/120    avg_loss:2.267, val_acc:0.464]
Epoch [14/120    avg_loss:2.263, val_acc:0.478]
Epoch [15/120    avg_loss:2.220, val_acc:0.513]
Epoch [16/120    avg_loss:2.179, val_acc:0.546]
Epoch [17/120    avg_loss:2.131, val_acc:0.577]
Epoch [18/120    avg_loss:2.085, val_acc:0.590]
Epoch [19/120    avg_loss:2.052, val_acc:0.588]
Epoch [20/120    avg_loss:1.988, val_acc:0.600]
Epoch [21/120    avg_loss:1.953, val_acc:0.620]
Epoch [22/120    avg_loss:1.902, val_acc:0.637]
Epoch [23/120    avg_loss:1.808, val_acc:0.624]
Epoch [24/120    avg_loss:1.714, val_acc:0.634]
Epoch [25/120    avg_loss:1.613, val_acc:0.652]
Epoch [26/120    avg_loss:1.538, val_acc:0.637]
Epoch [27/120    avg_loss:1.550, val_acc:0.649]
Epoch [28/120    avg_loss:1.423, val_acc:0.648]
Epoch [29/120    avg_loss:1.345, val_acc:0.661]
Epoch [30/120    avg_loss:1.268, val_acc:0.698]
Epoch [31/120    avg_loss:1.139, val_acc:0.714]
Epoch [32/120    avg_loss:1.085, val_acc:0.729]
Epoch [33/120    avg_loss:0.990, val_acc:0.773]
Epoch [34/120    avg_loss:1.001, val_acc:0.754]
Epoch [35/120    avg_loss:0.906, val_acc:0.733]
Epoch [36/120    avg_loss:0.927, val_acc:0.764]
Epoch [37/120    avg_loss:0.888, val_acc:0.786]
Epoch [38/120    avg_loss:0.789, val_acc:0.837]
Epoch [39/120    avg_loss:0.653, val_acc:0.848]
Epoch [40/120    avg_loss:0.622, val_acc:0.822]
Epoch [41/120    avg_loss:0.564, val_acc:0.854]
Epoch [42/120    avg_loss:0.561, val_acc:0.874]
Epoch [43/120    avg_loss:0.522, val_acc:0.889]
Epoch [44/120    avg_loss:0.453, val_acc:0.899]
Epoch [45/120    avg_loss:0.419, val_acc:0.897]
Epoch [46/120    avg_loss:0.396, val_acc:0.905]
Epoch [47/120    avg_loss:0.374, val_acc:0.892]
Epoch [48/120    avg_loss:0.361, val_acc:0.902]
Epoch [49/120    avg_loss:0.352, val_acc:0.905]
Epoch [50/120    avg_loss:0.318, val_acc:0.934]
Epoch [51/120    avg_loss:0.314, val_acc:0.929]
Epoch [52/120    avg_loss:0.292, val_acc:0.945]
Epoch [53/120    avg_loss:0.274, val_acc:0.947]
Epoch [54/120    avg_loss:0.221, val_acc:0.952]
Epoch [55/120    avg_loss:0.210, val_acc:0.935]
Epoch [56/120    avg_loss:0.198, val_acc:0.954]
Epoch [57/120    avg_loss:0.198, val_acc:0.958]
Epoch [58/120    avg_loss:0.166, val_acc:0.955]
Epoch [59/120    avg_loss:0.173, val_acc:0.954]
Epoch [60/120    avg_loss:0.175, val_acc:0.954]
Epoch [61/120    avg_loss:0.174, val_acc:0.952]
Epoch [62/120    avg_loss:0.153, val_acc:0.949]
Epoch [63/120    avg_loss:0.135, val_acc:0.952]
Epoch [64/120    avg_loss:0.110, val_acc:0.966]
Epoch [65/120    avg_loss:0.106, val_acc:0.963]
Epoch [66/120    avg_loss:0.097, val_acc:0.966]
Epoch [67/120    avg_loss:0.109, val_acc:0.962]
Epoch [68/120    avg_loss:0.101, val_acc:0.962]
Epoch [69/120    avg_loss:0.092, val_acc:0.959]
Epoch [70/120    avg_loss:0.086, val_acc:0.952]
Epoch [71/120    avg_loss:0.106, val_acc:0.967]
Epoch [72/120    avg_loss:0.088, val_acc:0.977]
Epoch [73/120    avg_loss:0.072, val_acc:0.973]
Epoch [74/120    avg_loss:0.096, val_acc:0.963]
Epoch [75/120    avg_loss:0.080, val_acc:0.965]
Epoch [76/120    avg_loss:0.065, val_acc:0.974]
Epoch [77/120    avg_loss:0.057, val_acc:0.963]
Epoch [78/120    avg_loss:0.054, val_acc:0.975]
Epoch [79/120    avg_loss:0.057, val_acc:0.973]
Epoch [80/120    avg_loss:0.055, val_acc:0.982]
Epoch [81/120    avg_loss:0.057, val_acc:0.978]
Epoch [82/120    avg_loss:0.050, val_acc:0.976]
Epoch [83/120    avg_loss:0.050, val_acc:0.967]
Epoch [84/120    avg_loss:0.049, val_acc:0.980]
Epoch [85/120    avg_loss:0.048, val_acc:0.973]
Epoch [86/120    avg_loss:0.058, val_acc:0.975]
Epoch [87/120    avg_loss:0.049, val_acc:0.983]
Epoch [88/120    avg_loss:0.039, val_acc:0.974]
Epoch [89/120    avg_loss:0.042, val_acc:0.978]
Epoch [90/120    avg_loss:0.032, val_acc:0.975]
Epoch [91/120    avg_loss:0.035, val_acc:0.971]
Epoch [92/120    avg_loss:0.089, val_acc:0.943]
Epoch [93/120    avg_loss:0.125, val_acc:0.961]
Epoch [94/120    avg_loss:0.088, val_acc:0.952]
Epoch [95/120    avg_loss:0.092, val_acc:0.959]
Epoch [96/120    avg_loss:0.062, val_acc:0.978]
Epoch [97/120    avg_loss:0.059, val_acc:0.972]
Epoch [98/120    avg_loss:0.047, val_acc:0.983]
Epoch [99/120    avg_loss:0.037, val_acc:0.978]
Epoch [100/120    avg_loss:0.037, val_acc:0.974]
Epoch [101/120    avg_loss:0.041, val_acc:0.970]
Epoch [102/120    avg_loss:0.082, val_acc:0.964]
Epoch [103/120    avg_loss:0.148, val_acc:0.930]
Epoch [104/120    avg_loss:0.114, val_acc:0.962]
Epoch [105/120    avg_loss:0.074, val_acc:0.960]
Epoch [106/120    avg_loss:0.056, val_acc:0.974]
Epoch [107/120    avg_loss:0.040, val_acc:0.972]
Epoch [108/120    avg_loss:0.033, val_acc:0.964]
Epoch [109/120    avg_loss:0.041, val_acc:0.972]
Epoch [110/120    avg_loss:0.036, val_acc:0.982]
Epoch [111/120    avg_loss:0.042, val_acc:0.977]
Epoch [112/120    avg_loss:0.028, val_acc:0.982]
Epoch [113/120    avg_loss:0.027, val_acc:0.983]
Epoch [114/120    avg_loss:0.025, val_acc:0.983]
Epoch [115/120    avg_loss:0.025, val_acc:0.983]
Epoch [116/120    avg_loss:0.025, val_acc:0.980]
Epoch [117/120    avg_loss:0.027, val_acc:0.984]
Epoch [118/120    avg_loss:0.030, val_acc:0.983]
Epoch [119/120    avg_loss:0.023, val_acc:0.983]
Epoch [120/120    avg_loss:0.022, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1250    0    8    0    1    0    0    0    6   20    0    0
     0    0    0]
 [   0    0    1  707    1    0    0    0    0   15    1    6   16    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  419    1    8    0    0    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    2    0    0    0    0  854   12    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    1    0    2   14 2184    8    0
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    1    5    4  514    0
     1    0    2]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    0   40    0    0    0    0    0    0    0
    16  291    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.48509485094851

F1 scores:
[       nan 0.94871795 0.9827044  0.97248968 0.97931034 0.97103129
 0.9646539  0.84745763 1.         0.58823529 0.97100625 0.98444895
 0.95895522 0.99728997 0.98562092 0.90092879 0.98823529]

Kappa:
0.9713219995424276
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff38b89df60>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.826, val_acc:0.304]
Epoch [2/120    avg_loss:2.778, val_acc:0.351]
Epoch [3/120    avg_loss:2.738, val_acc:0.366]
Epoch [4/120    avg_loss:2.688, val_acc:0.382]
Epoch [5/120    avg_loss:2.632, val_acc:0.398]
Epoch [6/120    avg_loss:2.587, val_acc:0.404]
Epoch [7/120    avg_loss:2.543, val_acc:0.398]
Epoch [8/120    avg_loss:2.472, val_acc:0.395]
Epoch [9/120    avg_loss:2.449, val_acc:0.400]
Epoch [10/120    avg_loss:2.388, val_acc:0.405]
Epoch [11/120    avg_loss:2.362, val_acc:0.413]
Epoch [12/120    avg_loss:2.326, val_acc:0.411]
Epoch [13/120    avg_loss:2.268, val_acc:0.430]
Epoch [14/120    avg_loss:2.245, val_acc:0.446]
Epoch [15/120    avg_loss:2.220, val_acc:0.463]
Epoch [16/120    avg_loss:2.141, val_acc:0.488]
Epoch [17/120    avg_loss:2.142, val_acc:0.442]
Epoch [18/120    avg_loss:2.112, val_acc:0.480]
Epoch [19/120    avg_loss:2.065, val_acc:0.540]
Epoch [20/120    avg_loss:2.013, val_acc:0.514]
Epoch [21/120    avg_loss:1.956, val_acc:0.547]
Epoch [22/120    avg_loss:1.921, val_acc:0.453]
Epoch [23/120    avg_loss:1.921, val_acc:0.541]
Epoch [24/120    avg_loss:1.804, val_acc:0.596]
Epoch [25/120    avg_loss:1.767, val_acc:0.608]
Epoch [26/120    avg_loss:1.681, val_acc:0.609]
Epoch [27/120    avg_loss:1.574, val_acc:0.614]
Epoch [28/120    avg_loss:1.521, val_acc:0.608]
Epoch [29/120    avg_loss:1.386, val_acc:0.672]
Epoch [30/120    avg_loss:1.377, val_acc:0.693]
Epoch [31/120    avg_loss:1.250, val_acc:0.698]
Epoch [32/120    avg_loss:1.212, val_acc:0.728]
Epoch [33/120    avg_loss:1.173, val_acc:0.725]
Epoch [34/120    avg_loss:1.088, val_acc:0.735]
Epoch [35/120    avg_loss:1.070, val_acc:0.746]
Epoch [36/120    avg_loss:0.983, val_acc:0.754]
Epoch [37/120    avg_loss:0.931, val_acc:0.758]
Epoch [38/120    avg_loss:0.898, val_acc:0.734]
Epoch [39/120    avg_loss:0.843, val_acc:0.777]
Epoch [40/120    avg_loss:0.767, val_acc:0.795]
Epoch [41/120    avg_loss:0.796, val_acc:0.776]
Epoch [42/120    avg_loss:0.693, val_acc:0.802]
Epoch [43/120    avg_loss:0.646, val_acc:0.827]
Epoch [44/120    avg_loss:0.601, val_acc:0.838]
Epoch [45/120    avg_loss:0.580, val_acc:0.833]
Epoch [46/120    avg_loss:0.531, val_acc:0.837]
Epoch [47/120    avg_loss:0.491, val_acc:0.860]
Epoch [48/120    avg_loss:0.479, val_acc:0.866]
Epoch [49/120    avg_loss:0.443, val_acc:0.839]
Epoch [50/120    avg_loss:0.388, val_acc:0.874]
Epoch [51/120    avg_loss:0.419, val_acc:0.882]
Epoch [52/120    avg_loss:0.447, val_acc:0.865]
Epoch [53/120    avg_loss:0.359, val_acc:0.887]
Epoch [54/120    avg_loss:0.343, val_acc:0.889]
Epoch [55/120    avg_loss:0.336, val_acc:0.877]
Epoch [56/120    avg_loss:0.286, val_acc:0.897]
Epoch [57/120    avg_loss:0.305, val_acc:0.882]
Epoch [58/120    avg_loss:0.255, val_acc:0.934]
Epoch [59/120    avg_loss:0.234, val_acc:0.899]
Epoch [60/120    avg_loss:0.222, val_acc:0.902]
Epoch [61/120    avg_loss:0.243, val_acc:0.889]
Epoch [62/120    avg_loss:0.205, val_acc:0.913]
Epoch [63/120    avg_loss:0.182, val_acc:0.924]
Epoch [64/120    avg_loss:0.160, val_acc:0.933]
Epoch [65/120    avg_loss:0.181, val_acc:0.934]
Epoch [66/120    avg_loss:0.162, val_acc:0.926]
Epoch [67/120    avg_loss:0.153, val_acc:0.950]
Epoch [68/120    avg_loss:0.144, val_acc:0.947]
Epoch [69/120    avg_loss:0.137, val_acc:0.946]
Epoch [70/120    avg_loss:0.118, val_acc:0.949]
Epoch [71/120    avg_loss:0.113, val_acc:0.948]
Epoch [72/120    avg_loss:0.110, val_acc:0.957]
Epoch [73/120    avg_loss:0.120, val_acc:0.950]
Epoch [74/120    avg_loss:0.108, val_acc:0.952]
Epoch [75/120    avg_loss:0.095, val_acc:0.959]
Epoch [76/120    avg_loss:0.096, val_acc:0.945]
Epoch [77/120    avg_loss:0.093, val_acc:0.964]
Epoch [78/120    avg_loss:0.087, val_acc:0.960]
Epoch [79/120    avg_loss:0.090, val_acc:0.954]
Epoch [80/120    avg_loss:0.084, val_acc:0.959]
Epoch [81/120    avg_loss:0.083, val_acc:0.961]
Epoch [82/120    avg_loss:0.077, val_acc:0.966]
Epoch [83/120    avg_loss:0.076, val_acc:0.942]
Epoch [84/120    avg_loss:0.088, val_acc:0.962]
Epoch [85/120    avg_loss:0.080, val_acc:0.954]
Epoch [86/120    avg_loss:0.073, val_acc:0.953]
Epoch [87/120    avg_loss:0.078, val_acc:0.962]
Epoch [88/120    avg_loss:0.065, val_acc:0.953]
Epoch [89/120    avg_loss:0.081, val_acc:0.966]
Epoch [90/120    avg_loss:0.089, val_acc:0.963]
Epoch [91/120    avg_loss:0.073, val_acc:0.960]
Epoch [92/120    avg_loss:0.066, val_acc:0.964]
Epoch [93/120    avg_loss:0.062, val_acc:0.962]
Epoch [94/120    avg_loss:0.063, val_acc:0.967]
Epoch [95/120    avg_loss:0.062, val_acc:0.968]
Epoch [96/120    avg_loss:0.066, val_acc:0.967]
Epoch [97/120    avg_loss:0.063, val_acc:0.959]
Epoch [98/120    avg_loss:0.070, val_acc:0.961]
Epoch [99/120    avg_loss:0.060, val_acc:0.971]
Epoch [100/120    avg_loss:0.059, val_acc:0.962]
Epoch [101/120    avg_loss:0.063, val_acc:0.958]
Epoch [102/120    avg_loss:0.055, val_acc:0.971]
Epoch [103/120    avg_loss:0.040, val_acc:0.975]
Epoch [104/120    avg_loss:0.040, val_acc:0.967]
Epoch [105/120    avg_loss:0.041, val_acc:0.963]
Epoch [106/120    avg_loss:0.035, val_acc:0.977]
Epoch [107/120    avg_loss:0.055, val_acc:0.976]
Epoch [108/120    avg_loss:0.055, val_acc:0.968]
Epoch [109/120    avg_loss:0.051, val_acc:0.970]
Epoch [110/120    avg_loss:0.042, val_acc:0.968]
Epoch [111/120    avg_loss:0.036, val_acc:0.972]
Epoch [112/120    avg_loss:0.034, val_acc:0.970]
Epoch [113/120    avg_loss:0.027, val_acc:0.967]
Epoch [114/120    avg_loss:0.032, val_acc:0.968]
Epoch [115/120    avg_loss:0.031, val_acc:0.974]
Epoch [116/120    avg_loss:0.040, val_acc:0.968]
Epoch [117/120    avg_loss:0.041, val_acc:0.974]
Epoch [118/120    avg_loss:0.031, val_acc:0.974]
Epoch [119/120    avg_loss:0.029, val_acc:0.977]
Epoch [120/120    avg_loss:0.028, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1240    0    0    0    4    0    0    0   11   29    0    0
     0    1    0]
 [   0    0    7  685    4    1    0    0    0   15    1   14   20    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    5    0    3    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0   24    4    0    1    4    0    0    0  830    7    0    0
     0    5    0]
 [   0    0   22    0    0    0    0    0    0    0   16 2165    6    1
     0    0    0]
 [   0    0    1    2    2    1    0    0    0    2    9    9  503    0
     0    4    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0    0    0
  1122   15    0]
 [   0    0    0    0    0    0   21    0    0    0    0    0    0    0
     2  324    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.89972899728997

F1 scores:
[       nan 0.96202532 0.96161303 0.95205003 0.9837587  0.98493627
 0.97688292 0.90909091 1.         0.61818182 0.95020034 0.9761046
 0.94548872 0.99730458 0.99029126 0.93103448 0.98809524]

Kappa:
0.9646520153120874
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6fd3acfeb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40669==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.814, val_acc:0.214]
Epoch [2/120    avg_loss:2.781, val_acc:0.252]
Epoch [3/120    avg_loss:2.736, val_acc:0.273]
Epoch [4/120    avg_loss:2.713, val_acc:0.278]
Epoch [5/120    avg_loss:2.647, val_acc:0.262]
Epoch [6/120    avg_loss:2.590, val_acc:0.270]
Epoch [7/120    avg_loss:2.537, val_acc:0.292]
Epoch [8/120    avg_loss:2.496, val_acc:0.365]
Epoch [9/120    avg_loss:2.465, val_acc:0.384]
Epoch [10/120    avg_loss:2.433, val_acc:0.398]
Epoch [11/120    avg_loss:2.407, val_acc:0.413]
Epoch [12/120    avg_loss:2.344, val_acc:0.421]
Epoch [13/120    avg_loss:2.300, val_acc:0.447]
Epoch [14/120    avg_loss:2.313, val_acc:0.473]
Epoch [15/120    avg_loss:2.235, val_acc:0.508]
Epoch [16/120    avg_loss:2.225, val_acc:0.518]
Epoch [17/120    avg_loss:2.093, val_acc:0.585]
Epoch [18/120    avg_loss:2.059, val_acc:0.563]
Epoch [19/120    avg_loss:1.981, val_acc:0.561]
Epoch [20/120    avg_loss:1.899, val_acc:0.623]
Epoch [21/120    avg_loss:1.800, val_acc:0.661]
Epoch [22/120    avg_loss:1.684, val_acc:0.682]
Epoch [23/120    avg_loss:1.609, val_acc:0.662]
Epoch [24/120    avg_loss:1.506, val_acc:0.700]
Epoch [25/120    avg_loss:1.465, val_acc:0.679]
Epoch [26/120    avg_loss:1.397, val_acc:0.693]
Epoch [27/120    avg_loss:1.302, val_acc:0.711]
Epoch [28/120    avg_loss:1.272, val_acc:0.713]
Epoch [29/120    avg_loss:1.173, val_acc:0.734]
Epoch [30/120    avg_loss:1.119, val_acc:0.732]
Epoch [31/120    avg_loss:1.052, val_acc:0.759]
Epoch [32/120    avg_loss:0.998, val_acc:0.763]
Epoch [33/120    avg_loss:0.955, val_acc:0.790]
Epoch [34/120    avg_loss:0.909, val_acc:0.788]
Epoch [35/120    avg_loss:0.826, val_acc:0.810]
Epoch [36/120    avg_loss:0.792, val_acc:0.817]
Epoch [37/120    avg_loss:0.712, val_acc:0.836]
Epoch [38/120    avg_loss:0.675, val_acc:0.839]
Epoch [39/120    avg_loss:0.595, val_acc:0.855]
Epoch [40/120    avg_loss:0.555, val_acc:0.871]
Epoch [41/120    avg_loss:0.519, val_acc:0.866]
Epoch [42/120    avg_loss:0.476, val_acc:0.878]
Epoch [43/120    avg_loss:0.456, val_acc:0.892]
Epoch [44/120    avg_loss:0.408, val_acc:0.896]
Epoch [45/120    avg_loss:0.339, val_acc:0.921]
Epoch [46/120    avg_loss:0.308, val_acc:0.922]
Epoch [47/120    avg_loss:0.263, val_acc:0.928]
Epoch [48/120    avg_loss:0.245, val_acc:0.927]
Epoch [49/120    avg_loss:0.254, val_acc:0.942]
Epoch [50/120    avg_loss:0.239, val_acc:0.947]
Epoch [51/120    avg_loss:0.238, val_acc:0.912]
Epoch [52/120    avg_loss:0.230, val_acc:0.939]
Epoch [53/120    avg_loss:0.261, val_acc:0.928]
Epoch [54/120    avg_loss:0.212, val_acc:0.933]
Epoch [55/120    avg_loss:0.184, val_acc:0.932]
Epoch [56/120    avg_loss:0.161, val_acc:0.947]
Epoch [57/120    avg_loss:0.143, val_acc:0.948]
Epoch [58/120    avg_loss:0.139, val_acc:0.957]
Epoch [59/120    avg_loss:0.134, val_acc:0.962]
Epoch [60/120    avg_loss:0.139, val_acc:0.953]
Epoch [61/120    avg_loss:0.162, val_acc:0.954]
Epoch [62/120    avg_loss:0.158, val_acc:0.940]
Epoch [63/120    avg_loss:0.155, val_acc:0.947]
Epoch [64/120    avg_loss:0.135, val_acc:0.954]
Epoch [65/120    avg_loss:0.116, val_acc:0.963]
Epoch [66/120    avg_loss:0.106, val_acc:0.967]
Epoch [67/120    avg_loss:0.096, val_acc:0.963]
Epoch [68/120    avg_loss:0.093, val_acc:0.963]
Epoch [69/120    avg_loss:0.090, val_acc:0.966]
Epoch [70/120    avg_loss:0.097, val_acc:0.966]
Epoch [71/120    avg_loss:0.093, val_acc:0.946]
Epoch [72/120    avg_loss:0.095, val_acc:0.963]
Epoch [73/120    avg_loss:0.078, val_acc:0.970]
Epoch [74/120    avg_loss:0.070, val_acc:0.970]
Epoch [75/120    avg_loss:0.074, val_acc:0.971]
Epoch [76/120    avg_loss:0.067, val_acc:0.971]
Epoch [77/120    avg_loss:0.067, val_acc:0.970]
Epoch [78/120    avg_loss:0.064, val_acc:0.967]
Epoch [79/120    avg_loss:0.063, val_acc:0.964]
Epoch [80/120    avg_loss:0.067, val_acc:0.974]
Epoch [81/120    avg_loss:0.049, val_acc:0.972]
Epoch [82/120    avg_loss:0.046, val_acc:0.974]
Epoch [83/120    avg_loss:0.055, val_acc:0.964]
Epoch [84/120    avg_loss:0.043, val_acc:0.967]
Epoch [85/120    avg_loss:0.056, val_acc:0.967]
Epoch [86/120    avg_loss:0.060, val_acc:0.972]
Epoch [87/120    avg_loss:0.050, val_acc:0.966]
Epoch [88/120    avg_loss:0.055, val_acc:0.977]
Epoch [89/120    avg_loss:0.053, val_acc:0.968]
Epoch [90/120    avg_loss:0.048, val_acc:0.976]
Epoch [91/120    avg_loss:0.044, val_acc:0.975]
Epoch [92/120    avg_loss:0.041, val_acc:0.986]
Epoch [93/120    avg_loss:0.050, val_acc:0.962]
Epoch [94/120    avg_loss:0.049, val_acc:0.964]
Epoch [95/120    avg_loss:0.036, val_acc:0.976]
Epoch [96/120    avg_loss:0.049, val_acc:0.980]
Epoch [97/120    avg_loss:0.044, val_acc:0.982]
Epoch [98/120    avg_loss:0.035, val_acc:0.984]
Epoch [99/120    avg_loss:0.034, val_acc:0.982]
Epoch [100/120    avg_loss:0.144, val_acc:0.860]
Epoch [101/120    avg_loss:0.361, val_acc:0.920]
Epoch [102/120    avg_loss:0.231, val_acc:0.924]
Epoch [103/120    avg_loss:0.163, val_acc:0.945]
Epoch [104/120    avg_loss:0.111, val_acc:0.961]
Epoch [105/120    avg_loss:0.091, val_acc:0.967]
Epoch [106/120    avg_loss:0.060, val_acc:0.973]
Epoch [107/120    avg_loss:0.056, val_acc:0.975]
Epoch [108/120    avg_loss:0.046, val_acc:0.975]
Epoch [109/120    avg_loss:0.047, val_acc:0.975]
Epoch [110/120    avg_loss:0.049, val_acc:0.976]
Epoch [111/120    avg_loss:0.050, val_acc:0.976]
Epoch [112/120    avg_loss:0.046, val_acc:0.975]
Epoch [113/120    avg_loss:0.045, val_acc:0.976]
Epoch [114/120    avg_loss:0.042, val_acc:0.975]
Epoch [115/120    avg_loss:0.040, val_acc:0.976]
Epoch [116/120    avg_loss:0.041, val_acc:0.977]
Epoch [117/120    avg_loss:0.036, val_acc:0.977]
Epoch [118/120    avg_loss:0.037, val_acc:0.977]
Epoch [119/120    avg_loss:0.038, val_acc:0.978]
Epoch [120/120    avg_loss:0.035, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    0    0    0    0    0    0    0    0    5    1    0    0
     0    0    0]
 [   0    0 1250    0    3    0    1    0    0    0    3   22    5    0
     1    0    0]
 [   0    0    0  684    6   12    0    0    0   11    2   16   16    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  421    0    0    0    1    0    1    0    0
    12    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    4    1    0    0    0  837   23    0    0
     2    1    0]
 [   0    0    2    0    0    0    0    1    0    0   10 2193    4    0
     0    0    0]
 [   0    0    0    7    0    7    0    0    0    3    9    0  503    0
     2    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    0   31    0    0    0    0    0    0    0
    16  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.15989159891599

F1 scores:
[       nan 0.92105263 0.9827044  0.95132128 0.97931034 0.95790671
 0.97473997 0.98039216 1.         0.70588235 0.96151637 0.98208688
 0.94637817 1.         0.9813287  0.9118541  0.98224852]

Kappa:
0.9675946389188448
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f023ea32f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.818, val_acc:0.285]
Epoch [2/120    avg_loss:2.777, val_acc:0.289]
Epoch [3/120    avg_loss:2.745, val_acc:0.289]
Epoch [4/120    avg_loss:2.701, val_acc:0.291]
Epoch [5/120    avg_loss:2.665, val_acc:0.297]
Epoch [6/120    avg_loss:2.620, val_acc:0.311]
Epoch [7/120    avg_loss:2.566, val_acc:0.327]
Epoch [8/120    avg_loss:2.530, val_acc:0.329]
Epoch [9/120    avg_loss:2.519, val_acc:0.337]
Epoch [10/120    avg_loss:2.453, val_acc:0.339]
Epoch [11/120    avg_loss:2.402, val_acc:0.341]
Epoch [12/120    avg_loss:2.378, val_acc:0.364]
Epoch [13/120    avg_loss:2.342, val_acc:0.416]
Epoch [14/120    avg_loss:2.299, val_acc:0.463]
Epoch [15/120    avg_loss:2.309, val_acc:0.463]
Epoch [16/120    avg_loss:2.256, val_acc:0.514]
Epoch [17/120    avg_loss:2.232, val_acc:0.528]
Epoch [18/120    avg_loss:2.174, val_acc:0.527]
Epoch [19/120    avg_loss:2.120, val_acc:0.555]
Epoch [20/120    avg_loss:2.091, val_acc:0.570]
Epoch [21/120    avg_loss:2.028, val_acc:0.555]
Epoch [22/120    avg_loss:1.985, val_acc:0.568]
Epoch [23/120    avg_loss:1.911, val_acc:0.637]
Epoch [24/120    avg_loss:1.868, val_acc:0.670]
Epoch [25/120    avg_loss:1.810, val_acc:0.648]
Epoch [26/120    avg_loss:1.762, val_acc:0.646]
Epoch [27/120    avg_loss:1.689, val_acc:0.673]
Epoch [28/120    avg_loss:1.593, val_acc:0.703]
Epoch [29/120    avg_loss:1.495, val_acc:0.703]
Epoch [30/120    avg_loss:1.405, val_acc:0.723]
Epoch [31/120    avg_loss:1.367, val_acc:0.720]
Epoch [32/120    avg_loss:1.260, val_acc:0.726]
Epoch [33/120    avg_loss:1.195, val_acc:0.730]
Epoch [34/120    avg_loss:1.153, val_acc:0.745]
Epoch [35/120    avg_loss:1.098, val_acc:0.743]
Epoch [36/120    avg_loss:1.015, val_acc:0.748]
Epoch [37/120    avg_loss:0.963, val_acc:0.732]
Epoch [38/120    avg_loss:0.845, val_acc:0.803]
Epoch [39/120    avg_loss:0.785, val_acc:0.823]
Epoch [40/120    avg_loss:0.716, val_acc:0.838]
Epoch [41/120    avg_loss:0.672, val_acc:0.833]
Epoch [42/120    avg_loss:0.613, val_acc:0.837]
Epoch [43/120    avg_loss:0.591, val_acc:0.854]
Epoch [44/120    avg_loss:0.518, val_acc:0.862]
Epoch [45/120    avg_loss:0.487, val_acc:0.877]
Epoch [46/120    avg_loss:0.464, val_acc:0.871]
Epoch [47/120    avg_loss:0.401, val_acc:0.885]
Epoch [48/120    avg_loss:0.371, val_acc:0.909]
Epoch [49/120    avg_loss:0.365, val_acc:0.914]
Epoch [50/120    avg_loss:0.358, val_acc:0.904]
Epoch [51/120    avg_loss:0.307, val_acc:0.912]
Epoch [52/120    avg_loss:0.301, val_acc:0.899]
Epoch [53/120    avg_loss:0.295, val_acc:0.898]
Epoch [54/120    avg_loss:0.289, val_acc:0.908]
Epoch [55/120    avg_loss:0.262, val_acc:0.910]
Epoch [56/120    avg_loss:0.245, val_acc:0.924]
Epoch [57/120    avg_loss:0.211, val_acc:0.933]
Epoch [58/120    avg_loss:0.195, val_acc:0.920]
Epoch [59/120    avg_loss:0.194, val_acc:0.907]
Epoch [60/120    avg_loss:0.207, val_acc:0.903]
Epoch [61/120    avg_loss:0.192, val_acc:0.921]
Epoch [62/120    avg_loss:0.163, val_acc:0.936]
Epoch [63/120    avg_loss:0.134, val_acc:0.940]
Epoch [64/120    avg_loss:0.131, val_acc:0.935]
Epoch [65/120    avg_loss:0.124, val_acc:0.933]
Epoch [66/120    avg_loss:0.113, val_acc:0.947]
Epoch [67/120    avg_loss:0.135, val_acc:0.949]
Epoch [68/120    avg_loss:0.117, val_acc:0.952]
Epoch [69/120    avg_loss:0.100, val_acc:0.950]
Epoch [70/120    avg_loss:0.114, val_acc:0.950]
Epoch [71/120    avg_loss:0.092, val_acc:0.953]
Epoch [72/120    avg_loss:0.082, val_acc:0.945]
Epoch [73/120    avg_loss:0.083, val_acc:0.954]
Epoch [74/120    avg_loss:0.071, val_acc:0.953]
Epoch [75/120    avg_loss:0.073, val_acc:0.948]
Epoch [76/120    avg_loss:0.078, val_acc:0.959]
Epoch [77/120    avg_loss:0.074, val_acc:0.960]
Epoch [78/120    avg_loss:0.075, val_acc:0.943]
Epoch [79/120    avg_loss:0.071, val_acc:0.957]
Epoch [80/120    avg_loss:0.068, val_acc:0.954]
Epoch [81/120    avg_loss:0.072, val_acc:0.954]
Epoch [82/120    avg_loss:0.064, val_acc:0.951]
Epoch [83/120    avg_loss:0.071, val_acc:0.952]
Epoch [84/120    avg_loss:0.051, val_acc:0.959]
Epoch [85/120    avg_loss:0.049, val_acc:0.948]
Epoch [86/120    avg_loss:0.051, val_acc:0.959]
Epoch [87/120    avg_loss:0.039, val_acc:0.960]
Epoch [88/120    avg_loss:0.041, val_acc:0.961]
Epoch [89/120    avg_loss:0.041, val_acc:0.952]
Epoch [90/120    avg_loss:0.059, val_acc:0.945]
Epoch [91/120    avg_loss:0.061, val_acc:0.955]
Epoch [92/120    avg_loss:0.050, val_acc:0.951]
Epoch [93/120    avg_loss:0.042, val_acc:0.959]
Epoch [94/120    avg_loss:0.038, val_acc:0.952]
Epoch [95/120    avg_loss:0.041, val_acc:0.953]
Epoch [96/120    avg_loss:0.035, val_acc:0.962]
Epoch [97/120    avg_loss:0.041, val_acc:0.942]
Epoch [98/120    avg_loss:0.056, val_acc:0.955]
Epoch [99/120    avg_loss:0.046, val_acc:0.964]
Epoch [100/120    avg_loss:0.034, val_acc:0.966]
Epoch [101/120    avg_loss:0.026, val_acc:0.961]
Epoch [102/120    avg_loss:0.031, val_acc:0.957]
Epoch [103/120    avg_loss:0.036, val_acc:0.961]
Epoch [104/120    avg_loss:0.044, val_acc:0.966]
Epoch [105/120    avg_loss:0.033, val_acc:0.965]
Epoch [106/120    avg_loss:0.027, val_acc:0.961]
Epoch [107/120    avg_loss:0.025, val_acc:0.967]
Epoch [108/120    avg_loss:0.029, val_acc:0.961]
Epoch [109/120    avg_loss:0.029, val_acc:0.970]
Epoch [110/120    avg_loss:0.024, val_acc:0.966]
Epoch [111/120    avg_loss:0.023, val_acc:0.970]
Epoch [112/120    avg_loss:0.031, val_acc:0.960]
Epoch [113/120    avg_loss:0.029, val_acc:0.970]
Epoch [114/120    avg_loss:0.023, val_acc:0.970]
Epoch [115/120    avg_loss:0.028, val_acc:0.967]
Epoch [116/120    avg_loss:0.023, val_acc:0.971]
Epoch [117/120    avg_loss:0.027, val_acc:0.960]
Epoch [118/120    avg_loss:0.024, val_acc:0.974]
Epoch [119/120    avg_loss:0.017, val_acc:0.970]
Epoch [120/120    avg_loss:0.021, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    0    0    0    0    0    0    0    0    6    0    0    0
     0    0    0]
 [   0    0 1236   15    1    0    1    0    0    0    7   18    2    0
     0    5    0]
 [   0    0    0  718    0   18    0    0    0    6    1    0    2    1
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    6    0    2    2    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   34   45    0    6    0    0    0    0  777    8    2    0
     0    3    0]
 [   0    0   17    0    0    1   10    0    0    0   16 2154    9    1
     2    0    0]
 [   0    0    0   14    6   10    0    0    0    0    9    2  489    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0   29    0    0    3    0    0    0    0
    86  229    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
95.39295392953929

F1 scores:
[       nan 0.92105263 0.96111975 0.93307342 0.98383372 0.94854586
 0.96969697 0.89285714 0.99650757 0.71111111 0.91627358 0.98087432
 0.93499044 0.99462366 0.95983087 0.78290598 0.95808383]

Kappa:
0.9474656012495191
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f75c855cf60>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.811, val_acc:0.201]
Epoch [2/120    avg_loss:2.780, val_acc:0.217]
Epoch [3/120    avg_loss:2.740, val_acc:0.265]
Epoch [4/120    avg_loss:2.705, val_acc:0.315]
Epoch [5/120    avg_loss:2.678, val_acc:0.353]
Epoch [6/120    avg_loss:2.637, val_acc:0.374]
Epoch [7/120    avg_loss:2.586, val_acc:0.385]
Epoch [8/120    avg_loss:2.544, val_acc:0.467]
Epoch [9/120    avg_loss:2.496, val_acc:0.503]
Epoch [10/120    avg_loss:2.435, val_acc:0.526]
Epoch [11/120    avg_loss:2.378, val_acc:0.534]
Epoch [12/120    avg_loss:2.345, val_acc:0.544]
Epoch [13/120    avg_loss:2.287, val_acc:0.565]
Epoch [14/120    avg_loss:2.251, val_acc:0.603]
Epoch [15/120    avg_loss:2.227, val_acc:0.590]
Epoch [16/120    avg_loss:2.174, val_acc:0.628]
Epoch [17/120    avg_loss:2.091, val_acc:0.666]
Epoch [18/120    avg_loss:2.052, val_acc:0.670]
Epoch [19/120    avg_loss:1.996, val_acc:0.695]
Epoch [20/120    avg_loss:1.948, val_acc:0.684]
Epoch [21/120    avg_loss:1.812, val_acc:0.702]
Epoch [22/120    avg_loss:1.743, val_acc:0.697]
Epoch [23/120    avg_loss:1.674, val_acc:0.717]
Epoch [24/120    avg_loss:1.560, val_acc:0.709]
Epoch [25/120    avg_loss:1.425, val_acc:0.730]
Epoch [26/120    avg_loss:1.338, val_acc:0.757]
Epoch [27/120    avg_loss:1.281, val_acc:0.780]
Epoch [28/120    avg_loss:1.208, val_acc:0.769]
Epoch [29/120    avg_loss:1.181, val_acc:0.784]
Epoch [30/120    avg_loss:1.171, val_acc:0.790]
Epoch [31/120    avg_loss:1.064, val_acc:0.758]
Epoch [32/120    avg_loss:1.056, val_acc:0.777]
Epoch [33/120    avg_loss:0.894, val_acc:0.806]
Epoch [34/120    avg_loss:0.859, val_acc:0.799]
Epoch [35/120    avg_loss:0.815, val_acc:0.830]
Epoch [36/120    avg_loss:0.708, val_acc:0.839]
Epoch [37/120    avg_loss:0.613, val_acc:0.870]
Epoch [38/120    avg_loss:0.597, val_acc:0.872]
Epoch [39/120    avg_loss:0.562, val_acc:0.890]
Epoch [40/120    avg_loss:0.528, val_acc:0.872]
Epoch [41/120    avg_loss:0.508, val_acc:0.901]
Epoch [42/120    avg_loss:0.429, val_acc:0.910]
Epoch [43/120    avg_loss:0.384, val_acc:0.916]
Epoch [44/120    avg_loss:0.365, val_acc:0.926]
Epoch [45/120    avg_loss:0.329, val_acc:0.927]
Epoch [46/120    avg_loss:0.311, val_acc:0.928]
Epoch [47/120    avg_loss:0.292, val_acc:0.907]
Epoch [48/120    avg_loss:0.254, val_acc:0.939]
Epoch [49/120    avg_loss:0.237, val_acc:0.936]
Epoch [50/120    avg_loss:0.223, val_acc:0.935]
Epoch [51/120    avg_loss:0.230, val_acc:0.930]
Epoch [52/120    avg_loss:0.215, val_acc:0.928]
Epoch [53/120    avg_loss:0.205, val_acc:0.934]
Epoch [54/120    avg_loss:0.189, val_acc:0.941]
Epoch [55/120    avg_loss:0.164, val_acc:0.947]
Epoch [56/120    avg_loss:0.157, val_acc:0.930]
Epoch [57/120    avg_loss:0.154, val_acc:0.942]
Epoch [58/120    avg_loss:0.168, val_acc:0.950]
Epoch [59/120    avg_loss:0.160, val_acc:0.934]
Epoch [60/120    avg_loss:0.146, val_acc:0.951]
Epoch [61/120    avg_loss:0.124, val_acc:0.950]
Epoch [62/120    avg_loss:0.119, val_acc:0.956]
Epoch [63/120    avg_loss:0.120, val_acc:0.948]
Epoch [64/120    avg_loss:0.118, val_acc:0.943]
Epoch [65/120    avg_loss:0.117, val_acc:0.963]
Epoch [66/120    avg_loss:0.114, val_acc:0.959]
Epoch [67/120    avg_loss:0.106, val_acc:0.953]
Epoch [68/120    avg_loss:0.098, val_acc:0.943]
Epoch [69/120    avg_loss:0.107, val_acc:0.949]
Epoch [70/120    avg_loss:0.102, val_acc:0.956]
Epoch [71/120    avg_loss:0.087, val_acc:0.949]
Epoch [72/120    avg_loss:0.088, val_acc:0.944]
Epoch [73/120    avg_loss:0.073, val_acc:0.964]
Epoch [74/120    avg_loss:0.087, val_acc:0.956]
Epoch [75/120    avg_loss:0.081, val_acc:0.961]
Epoch [76/120    avg_loss:0.066, val_acc:0.952]
Epoch [77/120    avg_loss:0.067, val_acc:0.963]
Epoch [78/120    avg_loss:0.065, val_acc:0.963]
Epoch [79/120    avg_loss:0.053, val_acc:0.966]
Epoch [80/120    avg_loss:0.058, val_acc:0.966]
Epoch [81/120    avg_loss:0.060, val_acc:0.961]
Epoch [82/120    avg_loss:0.056, val_acc:0.970]
Epoch [83/120    avg_loss:0.060, val_acc:0.955]
Epoch [84/120    avg_loss:0.076, val_acc:0.958]
Epoch [85/120    avg_loss:0.073, val_acc:0.952]
Epoch [86/120    avg_loss:0.071, val_acc:0.961]
Epoch [87/120    avg_loss:0.066, val_acc:0.967]
Epoch [88/120    avg_loss:0.055, val_acc:0.964]
Epoch [89/120    avg_loss:0.052, val_acc:0.964]
Epoch [90/120    avg_loss:0.048, val_acc:0.975]
Epoch [91/120    avg_loss:0.041, val_acc:0.967]
Epoch [92/120    avg_loss:0.048, val_acc:0.967]
Epoch [93/120    avg_loss:0.053, val_acc:0.968]
Epoch [94/120    avg_loss:0.066, val_acc:0.958]
Epoch [95/120    avg_loss:0.049, val_acc:0.960]
Epoch [96/120    avg_loss:0.067, val_acc:0.923]
Epoch [97/120    avg_loss:0.069, val_acc:0.972]
Epoch [98/120    avg_loss:0.054, val_acc:0.958]
Epoch [99/120    avg_loss:0.070, val_acc:0.964]
Epoch [100/120    avg_loss:0.060, val_acc:0.949]
Epoch [101/120    avg_loss:0.046, val_acc:0.960]
Epoch [102/120    avg_loss:0.042, val_acc:0.968]
Epoch [103/120    avg_loss:0.039, val_acc:0.974]
Epoch [104/120    avg_loss:0.032, val_acc:0.973]
Epoch [105/120    avg_loss:0.029, val_acc:0.975]
Epoch [106/120    avg_loss:0.027, val_acc:0.975]
Epoch [107/120    avg_loss:0.026, val_acc:0.976]
Epoch [108/120    avg_loss:0.025, val_acc:0.974]
Epoch [109/120    avg_loss:0.024, val_acc:0.973]
Epoch [110/120    avg_loss:0.030, val_acc:0.976]
Epoch [111/120    avg_loss:0.027, val_acc:0.977]
Epoch [112/120    avg_loss:0.030, val_acc:0.977]
Epoch [113/120    avg_loss:0.028, val_acc:0.975]
Epoch [114/120    avg_loss:0.027, val_acc:0.975]
Epoch [115/120    avg_loss:0.029, val_acc:0.978]
Epoch [116/120    avg_loss:0.025, val_acc:0.978]
Epoch [117/120    avg_loss:0.026, val_acc:0.975]
Epoch [118/120    avg_loss:0.028, val_acc:0.973]
Epoch [119/120    avg_loss:0.022, val_acc:0.973]
Epoch [120/120    avg_loss:0.025, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    1 1241    4    1    0    2    0    0    1    5   29    1    0
     0    0    0]
 [   0    0    1  698    2   22    0    0    0   13    0    0    7    3
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    8    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   15    0    0    2    0
     0    0    0]
 [   0    0   41   53    0    4    0    0    0    0  770    6    1    0
     0    0    0]
 [   0    0   10    0    0    0    1    0    0    0    8 2185    5    1
     0    0    0]
 [   0    0    0    0    0    9    0    0    0    0    8    5  509    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1138    1    0]
 [   0    0    0    0    0    0   18    0    0    0    0    0    0    0
    56  273    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.31436314363144

F1 scores:
[       nan 0.95       0.96276183 0.92942743 0.99300699 0.95089286
 0.98277154 0.86206897 1.         0.625      0.92270821 0.98512173
 0.96037736 0.98930481 0.97514996 0.87922705 0.97647059]

Kappa:
0.9579567875398636
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcc1b153f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.810, val_acc:0.139]
Epoch [2/120    avg_loss:2.763, val_acc:0.235]
Epoch [3/120    avg_loss:2.714, val_acc:0.259]
Epoch [4/120    avg_loss:2.672, val_acc:0.273]
Epoch [5/120    avg_loss:2.627, val_acc:0.264]
Epoch [6/120    avg_loss:2.591, val_acc:0.273]
Epoch [7/120    avg_loss:2.529, val_acc:0.303]
Epoch [8/120    avg_loss:2.496, val_acc:0.312]
Epoch [9/120    avg_loss:2.436, val_acc:0.329]
Epoch [10/120    avg_loss:2.355, val_acc:0.353]
Epoch [11/120    avg_loss:2.340, val_acc:0.362]
Epoch [12/120    avg_loss:2.312, val_acc:0.398]
Epoch [13/120    avg_loss:2.293, val_acc:0.414]
Epoch [14/120    avg_loss:2.227, val_acc:0.429]
Epoch [15/120    avg_loss:2.151, val_acc:0.478]
Epoch [16/120    avg_loss:2.134, val_acc:0.472]
Epoch [17/120    avg_loss:2.105, val_acc:0.570]
Epoch [18/120    avg_loss:2.019, val_acc:0.597]
Epoch [19/120    avg_loss:1.959, val_acc:0.608]
Epoch [20/120    avg_loss:1.884, val_acc:0.623]
Epoch [21/120    avg_loss:1.828, val_acc:0.550]
Epoch [22/120    avg_loss:1.755, val_acc:0.532]
Epoch [23/120    avg_loss:1.702, val_acc:0.514]
Epoch [24/120    avg_loss:1.590, val_acc:0.491]
Epoch [25/120    avg_loss:1.576, val_acc:0.588]
Epoch [26/120    avg_loss:1.489, val_acc:0.593]
Epoch [27/120    avg_loss:1.324, val_acc:0.622]
Epoch [28/120    avg_loss:1.321, val_acc:0.596]
Epoch [29/120    avg_loss:1.235, val_acc:0.680]
Epoch [30/120    avg_loss:1.168, val_acc:0.649]
Epoch [31/120    avg_loss:1.047, val_acc:0.714]
Epoch [32/120    avg_loss:0.991, val_acc:0.743]
Epoch [33/120    avg_loss:0.965, val_acc:0.754]
Epoch [34/120    avg_loss:0.932, val_acc:0.724]
Epoch [35/120    avg_loss:0.890, val_acc:0.760]
Epoch [36/120    avg_loss:0.792, val_acc:0.768]
Epoch [37/120    avg_loss:0.771, val_acc:0.803]
Epoch [38/120    avg_loss:0.692, val_acc:0.834]
Epoch [39/120    avg_loss:0.606, val_acc:0.851]
Epoch [40/120    avg_loss:0.613, val_acc:0.828]
Epoch [41/120    avg_loss:0.561, val_acc:0.868]
Epoch [42/120    avg_loss:0.507, val_acc:0.875]
Epoch [43/120    avg_loss:0.444, val_acc:0.875]
Epoch [44/120    avg_loss:0.436, val_acc:0.893]
Epoch [45/120    avg_loss:0.390, val_acc:0.867]
Epoch [46/120    avg_loss:0.369, val_acc:0.910]
Epoch [47/120    avg_loss:0.365, val_acc:0.901]
Epoch [48/120    avg_loss:0.353, val_acc:0.915]
Epoch [49/120    avg_loss:0.335, val_acc:0.909]
Epoch [50/120    avg_loss:0.330, val_acc:0.914]
Epoch [51/120    avg_loss:0.279, val_acc:0.925]
Epoch [52/120    avg_loss:0.236, val_acc:0.935]
Epoch [53/120    avg_loss:0.225, val_acc:0.937]
Epoch [54/120    avg_loss:0.218, val_acc:0.924]
Epoch [55/120    avg_loss:0.246, val_acc:0.938]
Epoch [56/120    avg_loss:0.203, val_acc:0.939]
Epoch [57/120    avg_loss:0.177, val_acc:0.934]
Epoch [58/120    avg_loss:0.173, val_acc:0.929]
Epoch [59/120    avg_loss:0.151, val_acc:0.941]
Epoch [60/120    avg_loss:0.137, val_acc:0.945]
Epoch [61/120    avg_loss:0.159, val_acc:0.934]
Epoch [62/120    avg_loss:0.154, val_acc:0.940]
Epoch [63/120    avg_loss:0.134, val_acc:0.948]
Epoch [64/120    avg_loss:0.116, val_acc:0.942]
Epoch [65/120    avg_loss:0.123, val_acc:0.948]
Epoch [66/120    avg_loss:0.104, val_acc:0.948]
Epoch [67/120    avg_loss:0.095, val_acc:0.945]
Epoch [68/120    avg_loss:0.091, val_acc:0.947]
Epoch [69/120    avg_loss:0.092, val_acc:0.955]
Epoch [70/120    avg_loss:0.091, val_acc:0.935]
Epoch [71/120    avg_loss:0.096, val_acc:0.940]
Epoch [72/120    avg_loss:0.089, val_acc:0.957]
Epoch [73/120    avg_loss:0.083, val_acc:0.941]
Epoch [74/120    avg_loss:0.087, val_acc:0.948]
Epoch [75/120    avg_loss:0.096, val_acc:0.953]
Epoch [76/120    avg_loss:0.083, val_acc:0.955]
Epoch [77/120    avg_loss:0.072, val_acc:0.958]
Epoch [78/120    avg_loss:0.081, val_acc:0.950]
Epoch [79/120    avg_loss:0.067, val_acc:0.954]
Epoch [80/120    avg_loss:0.089, val_acc:0.953]
Epoch [81/120    avg_loss:0.072, val_acc:0.962]
Epoch [82/120    avg_loss:0.057, val_acc:0.965]
Epoch [83/120    avg_loss:0.061, val_acc:0.963]
Epoch [84/120    avg_loss:0.063, val_acc:0.955]
Epoch [85/120    avg_loss:0.058, val_acc:0.960]
Epoch [86/120    avg_loss:0.057, val_acc:0.955]
Epoch [87/120    avg_loss:0.060, val_acc:0.963]
Epoch [88/120    avg_loss:0.054, val_acc:0.960]
Epoch [89/120    avg_loss:0.048, val_acc:0.960]
Epoch [90/120    avg_loss:0.045, val_acc:0.962]
Epoch [91/120    avg_loss:0.066, val_acc:0.948]
Epoch [92/120    avg_loss:0.075, val_acc:0.966]
Epoch [93/120    avg_loss:0.058, val_acc:0.970]
Epoch [94/120    avg_loss:0.047, val_acc:0.966]
Epoch [95/120    avg_loss:0.040, val_acc:0.968]
Epoch [96/120    avg_loss:0.043, val_acc:0.955]
Epoch [97/120    avg_loss:0.044, val_acc:0.958]
Epoch [98/120    avg_loss:0.053, val_acc:0.951]
Epoch [99/120    avg_loss:0.059, val_acc:0.946]
Epoch [100/120    avg_loss:0.057, val_acc:0.965]
Epoch [101/120    avg_loss:0.042, val_acc:0.962]
Epoch [102/120    avg_loss:0.047, val_acc:0.958]
Epoch [103/120    avg_loss:0.051, val_acc:0.958]
Epoch [104/120    avg_loss:0.053, val_acc:0.960]
Epoch [105/120    avg_loss:0.043, val_acc:0.964]
Epoch [106/120    avg_loss:0.037, val_acc:0.966]
Epoch [107/120    avg_loss:0.028, val_acc:0.971]
Epoch [108/120    avg_loss:0.031, val_acc:0.971]
Epoch [109/120    avg_loss:0.026, val_acc:0.973]
Epoch [110/120    avg_loss:0.025, val_acc:0.972]
Epoch [111/120    avg_loss:0.027, val_acc:0.972]
Epoch [112/120    avg_loss:0.026, val_acc:0.972]
Epoch [113/120    avg_loss:0.025, val_acc:0.971]
Epoch [114/120    avg_loss:0.027, val_acc:0.972]
Epoch [115/120    avg_loss:0.024, val_acc:0.973]
Epoch [116/120    avg_loss:0.024, val_acc:0.975]
Epoch [117/120    avg_loss:0.023, val_acc:0.974]
Epoch [118/120    avg_loss:0.022, val_acc:0.973]
Epoch [119/120    avg_loss:0.021, val_acc:0.973]
Epoch [120/120    avg_loss:0.023, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1245    4    1    0    0    0    0    0    5   28    1    0
     0    1    0]
 [   0    0    1  698    0   18    0    0    0   13    0    0   10    7
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    1    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    3    0    0   11    0    3    0    0
     0    0    0]
 [   0    0   12   39    0    3    0    0    0    0  796   13    2    0
     0   10    0]
 [   0    0   12    0    0    0    5    0    0    1   15 2169    8    0
     0    0    0]
 [   0    0    0   21    8    6    0    0    0    0   11    1  485    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   18    0    0    3    0    0    0    0
    41  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.34688346883469

F1 scores:
[       nan 0.975      0.97455969 0.92450331 0.97931034 0.96644295
 0.97757848 1.         0.99883856 0.45833333 0.9342723  0.97989609
 0.93179635 0.98143236 0.98145752 0.88646967 0.98809524]

Kappa:
0.958344198418408
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3a6c667eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.797, val_acc:0.161]
Epoch [2/120    avg_loss:2.743, val_acc:0.180]
Epoch [3/120    avg_loss:2.706, val_acc:0.200]
Epoch [4/120    avg_loss:2.643, val_acc:0.251]
Epoch [5/120    avg_loss:2.585, val_acc:0.236]
Epoch [6/120    avg_loss:2.559, val_acc:0.252]
Epoch [7/120    avg_loss:2.534, val_acc:0.289]
Epoch [8/120    avg_loss:2.482, val_acc:0.309]
Epoch [9/120    avg_loss:2.443, val_acc:0.317]
Epoch [10/120    avg_loss:2.378, val_acc:0.330]
Epoch [11/120    avg_loss:2.387, val_acc:0.344]
Epoch [12/120    avg_loss:2.361, val_acc:0.355]
Epoch [13/120    avg_loss:2.282, val_acc:0.402]
Epoch [14/120    avg_loss:2.255, val_acc:0.431]
Epoch [15/120    avg_loss:2.201, val_acc:0.459]
Epoch [16/120    avg_loss:2.165, val_acc:0.486]
Epoch [17/120    avg_loss:2.131, val_acc:0.434]
Epoch [18/120    avg_loss:2.061, val_acc:0.477]
Epoch [19/120    avg_loss:1.980, val_acc:0.552]
Epoch [20/120    avg_loss:1.930, val_acc:0.532]
Epoch [21/120    avg_loss:1.839, val_acc:0.566]
Epoch [22/120    avg_loss:1.782, val_acc:0.578]
Epoch [23/120    avg_loss:1.718, val_acc:0.574]
Epoch [24/120    avg_loss:1.587, val_acc:0.630]
Epoch [25/120    avg_loss:1.565, val_acc:0.641]
Epoch [26/120    avg_loss:1.450, val_acc:0.652]
Epoch [27/120    avg_loss:1.369, val_acc:0.645]
Epoch [28/120    avg_loss:1.384, val_acc:0.627]
Epoch [29/120    avg_loss:1.272, val_acc:0.686]
Epoch [30/120    avg_loss:1.189, val_acc:0.686]
Epoch [31/120    avg_loss:1.085, val_acc:0.702]
Epoch [32/120    avg_loss:1.026, val_acc:0.708]
Epoch [33/120    avg_loss:0.967, val_acc:0.709]
Epoch [34/120    avg_loss:0.906, val_acc:0.748]
Epoch [35/120    avg_loss:0.882, val_acc:0.733]
Epoch [36/120    avg_loss:0.822, val_acc:0.760]
Epoch [37/120    avg_loss:0.787, val_acc:0.759]
Epoch [38/120    avg_loss:0.748, val_acc:0.753]
Epoch [39/120    avg_loss:0.677, val_acc:0.794]
Epoch [40/120    avg_loss:0.643, val_acc:0.785]
Epoch [41/120    avg_loss:0.590, val_acc:0.810]
Epoch [42/120    avg_loss:0.565, val_acc:0.807]
Epoch [43/120    avg_loss:0.466, val_acc:0.831]
Epoch [44/120    avg_loss:0.440, val_acc:0.841]
Epoch [45/120    avg_loss:0.469, val_acc:0.841]
Epoch [46/120    avg_loss:0.420, val_acc:0.876]
Epoch [47/120    avg_loss:0.437, val_acc:0.844]
Epoch [48/120    avg_loss:0.392, val_acc:0.869]
Epoch [49/120    avg_loss:0.390, val_acc:0.875]
Epoch [50/120    avg_loss:0.363, val_acc:0.875]
Epoch [51/120    avg_loss:0.296, val_acc:0.883]
Epoch [52/120    avg_loss:0.293, val_acc:0.892]
Epoch [53/120    avg_loss:0.279, val_acc:0.909]
Epoch [54/120    avg_loss:0.234, val_acc:0.907]
Epoch [55/120    avg_loss:0.223, val_acc:0.905]
Epoch [56/120    avg_loss:0.224, val_acc:0.894]
Epoch [57/120    avg_loss:0.217, val_acc:0.920]
Epoch [58/120    avg_loss:0.208, val_acc:0.905]
Epoch [59/120    avg_loss:0.209, val_acc:0.897]
Epoch [60/120    avg_loss:0.201, val_acc:0.908]
Epoch [61/120    avg_loss:0.203, val_acc:0.911]
Epoch [62/120    avg_loss:0.256, val_acc:0.912]
Epoch [63/120    avg_loss:0.187, val_acc:0.911]
Epoch [64/120    avg_loss:0.187, val_acc:0.923]
Epoch [65/120    avg_loss:0.167, val_acc:0.924]
Epoch [66/120    avg_loss:0.150, val_acc:0.926]
Epoch [67/120    avg_loss:0.169, val_acc:0.922]
Epoch [68/120    avg_loss:0.149, val_acc:0.930]
Epoch [69/120    avg_loss:0.127, val_acc:0.930]
Epoch [70/120    avg_loss:0.112, val_acc:0.924]
Epoch [71/120    avg_loss:0.126, val_acc:0.934]
Epoch [72/120    avg_loss:0.168, val_acc:0.928]
Epoch [73/120    avg_loss:0.130, val_acc:0.930]
Epoch [74/120    avg_loss:0.122, val_acc:0.924]
Epoch [75/120    avg_loss:0.122, val_acc:0.936]
Epoch [76/120    avg_loss:0.111, val_acc:0.932]
Epoch [77/120    avg_loss:0.108, val_acc:0.931]
Epoch [78/120    avg_loss:0.107, val_acc:0.947]
Epoch [79/120    avg_loss:0.107, val_acc:0.936]
Epoch [80/120    avg_loss:0.086, val_acc:0.935]
Epoch [81/120    avg_loss:0.106, val_acc:0.934]
Epoch [82/120    avg_loss:0.120, val_acc:0.925]
Epoch [83/120    avg_loss:0.125, val_acc:0.941]
Epoch [84/120    avg_loss:0.117, val_acc:0.918]
Epoch [85/120    avg_loss:0.100, val_acc:0.931]
Epoch [86/120    avg_loss:0.087, val_acc:0.947]
Epoch [87/120    avg_loss:0.086, val_acc:0.945]
Epoch [88/120    avg_loss:0.069, val_acc:0.945]
Epoch [89/120    avg_loss:0.075, val_acc:0.949]
Epoch [90/120    avg_loss:0.069, val_acc:0.934]
Epoch [91/120    avg_loss:0.063, val_acc:0.933]
Epoch [92/120    avg_loss:0.110, val_acc:0.949]
Epoch [93/120    avg_loss:0.079, val_acc:0.947]
Epoch [94/120    avg_loss:0.073, val_acc:0.932]
Epoch [95/120    avg_loss:0.072, val_acc:0.956]
Epoch [96/120    avg_loss:0.069, val_acc:0.941]
Epoch [97/120    avg_loss:0.051, val_acc:0.942]
Epoch [98/120    avg_loss:0.049, val_acc:0.953]
Epoch [99/120    avg_loss:0.048, val_acc:0.959]
Epoch [100/120    avg_loss:0.047, val_acc:0.944]
Epoch [101/120    avg_loss:0.036, val_acc:0.964]
Epoch [102/120    avg_loss:0.044, val_acc:0.957]
Epoch [103/120    avg_loss:0.045, val_acc:0.938]
Epoch [104/120    avg_loss:0.057, val_acc:0.956]
Epoch [105/120    avg_loss:0.041, val_acc:0.957]
Epoch [106/120    avg_loss:0.058, val_acc:0.953]
Epoch [107/120    avg_loss:0.047, val_acc:0.960]
Epoch [108/120    avg_loss:0.050, val_acc:0.953]
Epoch [109/120    avg_loss:0.059, val_acc:0.963]
Epoch [110/120    avg_loss:0.044, val_acc:0.958]
Epoch [111/120    avg_loss:0.047, val_acc:0.958]
Epoch [112/120    avg_loss:0.043, val_acc:0.960]
Epoch [113/120    avg_loss:0.047, val_acc:0.959]
Epoch [114/120    avg_loss:0.038, val_acc:0.953]
Epoch [115/120    avg_loss:0.034, val_acc:0.958]
Epoch [116/120    avg_loss:0.030, val_acc:0.960]
Epoch [117/120    avg_loss:0.025, val_acc:0.961]
Epoch [118/120    avg_loss:0.027, val_acc:0.961]
Epoch [119/120    avg_loss:0.025, val_acc:0.961]
Epoch [120/120    avg_loss:0.027, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1236    0    0    0    2    0    0    0    8   33    2    0
     0    4    0]
 [   0    0    3  691    6    9    0    0    0   18    0    0   19    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    7    0    2    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   20   58    0    6    0    0    0    0  783    0    0    0
     0    8    0]
 [   0    0    4    0    0    0   11    0    0    0   18 2165   10    2
     0    0    0]
 [   0    0    0   32    1   11    0    0    0    1    7    0  481    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    1    0    2    0    0    0
  1126    0    0]
 [   0    0    0    0    0    1   26    0    0    0    0    0    0    0
    43  277    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.68563685636856

F1 scores:
[       nan 0.96202532 0.97017268 0.90385873 0.98383372 0.94407159
 0.96965211 0.87719298 0.99883856 0.58181818 0.92334906 0.98185941
 0.91881566 0.9919571  0.97404844 0.87106918 0.99408284]

Kappa:
0.9508297242141541
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efd7ccb0eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.843, val_acc:0.273]
Epoch [2/120    avg_loss:2.802, val_acc:0.277]
Epoch [3/120    avg_loss:2.762, val_acc:0.319]
Epoch [4/120    avg_loss:2.715, val_acc:0.354]
Epoch [5/120    avg_loss:2.662, val_acc:0.351]
Epoch [6/120    avg_loss:2.606, val_acc:0.390]
Epoch [7/120    avg_loss:2.563, val_acc:0.411]
Epoch [8/120    avg_loss:2.477, val_acc:0.422]
Epoch [9/120    avg_loss:2.450, val_acc:0.459]
Epoch [10/120    avg_loss:2.403, val_acc:0.502]
Epoch [11/120    avg_loss:2.343, val_acc:0.518]
Epoch [12/120    avg_loss:2.320, val_acc:0.540]
Epoch [13/120    avg_loss:2.268, val_acc:0.562]
Epoch [14/120    avg_loss:2.222, val_acc:0.586]
Epoch [15/120    avg_loss:2.157, val_acc:0.589]
Epoch [16/120    avg_loss:2.148, val_acc:0.596]
Epoch [17/120    avg_loss:2.079, val_acc:0.585]
Epoch [18/120    avg_loss:2.021, val_acc:0.621]
Epoch [19/120    avg_loss:1.959, val_acc:0.630]
Epoch [20/120    avg_loss:1.830, val_acc:0.637]
Epoch [21/120    avg_loss:1.757, val_acc:0.658]
Epoch [22/120    avg_loss:1.693, val_acc:0.700]
Epoch [23/120    avg_loss:1.587, val_acc:0.687]
Epoch [24/120    avg_loss:1.599, val_acc:0.699]
Epoch [25/120    avg_loss:1.476, val_acc:0.701]
Epoch [26/120    avg_loss:1.375, val_acc:0.711]
Epoch [27/120    avg_loss:1.288, val_acc:0.738]
Epoch [28/120    avg_loss:1.206, val_acc:0.754]
Epoch [29/120    avg_loss:1.121, val_acc:0.747]
Epoch [30/120    avg_loss:1.125, val_acc:0.762]
Epoch [31/120    avg_loss:1.050, val_acc:0.785]
Epoch [32/120    avg_loss:1.036, val_acc:0.727]
Epoch [33/120    avg_loss:1.055, val_acc:0.740]
Epoch [34/120    avg_loss:0.917, val_acc:0.777]
Epoch [35/120    avg_loss:0.797, val_acc:0.782]
Epoch [36/120    avg_loss:0.784, val_acc:0.801]
Epoch [37/120    avg_loss:0.669, val_acc:0.805]
Epoch [38/120    avg_loss:0.655, val_acc:0.837]
Epoch [39/120    avg_loss:0.612, val_acc:0.829]
Epoch [40/120    avg_loss:0.567, val_acc:0.861]
Epoch [41/120    avg_loss:0.537, val_acc:0.871]
Epoch [42/120    avg_loss:0.473, val_acc:0.870]
Epoch [43/120    avg_loss:0.433, val_acc:0.884]
Epoch [44/120    avg_loss:0.408, val_acc:0.887]
Epoch [45/120    avg_loss:0.425, val_acc:0.878]
Epoch [46/120    avg_loss:0.437, val_acc:0.843]
Epoch [47/120    avg_loss:0.424, val_acc:0.858]
Epoch [48/120    avg_loss:0.481, val_acc:0.866]
Epoch [49/120    avg_loss:0.351, val_acc:0.890]
Epoch [50/120    avg_loss:0.285, val_acc:0.903]
Epoch [51/120    avg_loss:0.269, val_acc:0.898]
Epoch [52/120    avg_loss:0.264, val_acc:0.910]
Epoch [53/120    avg_loss:0.256, val_acc:0.911]
Epoch [54/120    avg_loss:0.238, val_acc:0.911]
Epoch [55/120    avg_loss:0.214, val_acc:0.911]
Epoch [56/120    avg_loss:0.186, val_acc:0.928]
Epoch [57/120    avg_loss:0.185, val_acc:0.923]
Epoch [58/120    avg_loss:0.172, val_acc:0.939]
Epoch [59/120    avg_loss:0.160, val_acc:0.938]
Epoch [60/120    avg_loss:0.169, val_acc:0.932]
Epoch [61/120    avg_loss:0.174, val_acc:0.932]
Epoch [62/120    avg_loss:0.136, val_acc:0.935]
Epoch [63/120    avg_loss:0.137, val_acc:0.930]
Epoch [64/120    avg_loss:0.122, val_acc:0.936]
Epoch [65/120    avg_loss:0.107, val_acc:0.934]
Epoch [66/120    avg_loss:0.114, val_acc:0.927]
Epoch [67/120    avg_loss:0.100, val_acc:0.947]
Epoch [68/120    avg_loss:0.100, val_acc:0.938]
Epoch [69/120    avg_loss:0.112, val_acc:0.901]
Epoch [70/120    avg_loss:0.103, val_acc:0.929]
Epoch [71/120    avg_loss:0.096, val_acc:0.935]
Epoch [72/120    avg_loss:0.107, val_acc:0.946]
Epoch [73/120    avg_loss:0.122, val_acc:0.934]
Epoch [74/120    avg_loss:0.120, val_acc:0.947]
Epoch [75/120    avg_loss:0.098, val_acc:0.955]
Epoch [76/120    avg_loss:0.087, val_acc:0.939]
Epoch [77/120    avg_loss:0.122, val_acc:0.949]
Epoch [78/120    avg_loss:0.089, val_acc:0.954]
Epoch [79/120    avg_loss:0.074, val_acc:0.951]
Epoch [80/120    avg_loss:0.072, val_acc:0.936]
Epoch [81/120    avg_loss:0.069, val_acc:0.955]
Epoch [82/120    avg_loss:0.064, val_acc:0.961]
Epoch [83/120    avg_loss:0.061, val_acc:0.953]
Epoch [84/120    avg_loss:0.064, val_acc:0.963]
Epoch [85/120    avg_loss:0.065, val_acc:0.960]
Epoch [86/120    avg_loss:0.063, val_acc:0.967]
Epoch [87/120    avg_loss:0.055, val_acc:0.966]
Epoch [88/120    avg_loss:0.050, val_acc:0.962]
Epoch [89/120    avg_loss:0.053, val_acc:0.966]
Epoch [90/120    avg_loss:0.043, val_acc:0.965]
Epoch [91/120    avg_loss:0.046, val_acc:0.959]
Epoch [92/120    avg_loss:0.049, val_acc:0.970]
Epoch [93/120    avg_loss:0.044, val_acc:0.962]
Epoch [94/120    avg_loss:0.043, val_acc:0.961]
Epoch [95/120    avg_loss:0.046, val_acc:0.967]
Epoch [96/120    avg_loss:0.045, val_acc:0.963]
Epoch [97/120    avg_loss:0.049, val_acc:0.967]
Epoch [98/120    avg_loss:0.062, val_acc:0.953]
Epoch [99/120    avg_loss:0.046, val_acc:0.967]
Epoch [100/120    avg_loss:0.060, val_acc:0.963]
Epoch [101/120    avg_loss:0.053, val_acc:0.957]
Epoch [102/120    avg_loss:0.048, val_acc:0.927]
Epoch [103/120    avg_loss:0.078, val_acc:0.966]
Epoch [104/120    avg_loss:0.052, val_acc:0.957]
Epoch [105/120    avg_loss:0.060, val_acc:0.964]
Epoch [106/120    avg_loss:0.047, val_acc:0.968]
Epoch [107/120    avg_loss:0.034, val_acc:0.967]
Epoch [108/120    avg_loss:0.033, val_acc:0.966]
Epoch [109/120    avg_loss:0.031, val_acc:0.967]
Epoch [110/120    avg_loss:0.030, val_acc:0.967]
Epoch [111/120    avg_loss:0.032, val_acc:0.968]
Epoch [112/120    avg_loss:0.031, val_acc:0.968]
Epoch [113/120    avg_loss:0.035, val_acc:0.972]
Epoch [114/120    avg_loss:0.030, val_acc:0.968]
Epoch [115/120    avg_loss:0.026, val_acc:0.970]
Epoch [116/120    avg_loss:0.027, val_acc:0.968]
Epoch [117/120    avg_loss:0.036, val_acc:0.967]
Epoch [118/120    avg_loss:0.027, val_acc:0.968]
Epoch [119/120    avg_loss:0.026, val_acc:0.968]
Epoch [120/120    avg_loss:0.026, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    2    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1239    0    0    0    7    0    0    3    3   30    3    0
     0    0    0]
 [   0    0    1  708    6   13    0    0    0   15    0    0    2    1
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    1    1    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    2    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   26   45    0    2    1    0    0    0  776    5   18    0
     2    0    0]
 [   0    0   18    0    0    0    3    0    5    0   16 2162    4    1
     1    0    0]
 [   0    0    0    6    3    9    0    0    0    0    1    4  506    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   12    0    0    1    0    3    1    0    0
  1122    0    0]
 [   0    0    1    0    0    6   17    0    0    0    0    0    0    0
    60  263    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.88075880758808

F1 scores:
[       nan 0.94871795 0.96345257 0.94023904 0.97931034 0.94585635
 0.97382199 0.98039216 0.99307159 0.61818182 0.92601432 0.97961033
 0.94667914 0.99462366 0.96226415 0.86229508 0.96511628]

Kappa:
0.9530355525221549
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8c50b60eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.832, val_acc:0.219]
Epoch [2/120    avg_loss:2.797, val_acc:0.203]
Epoch [3/120    avg_loss:2.761, val_acc:0.214]
Epoch [4/120    avg_loss:2.718, val_acc:0.218]
Epoch [5/120    avg_loss:2.679, val_acc:0.216]
Epoch [6/120    avg_loss:2.649, val_acc:0.215]
Epoch [7/120    avg_loss:2.568, val_acc:0.222]
Epoch [8/120    avg_loss:2.549, val_acc:0.252]
Epoch [9/120    avg_loss:2.505, val_acc:0.310]
Epoch [10/120    avg_loss:2.457, val_acc:0.330]
Epoch [11/120    avg_loss:2.435, val_acc:0.338]
Epoch [12/120    avg_loss:2.378, val_acc:0.409]
Epoch [13/120    avg_loss:2.376, val_acc:0.429]
Epoch [14/120    avg_loss:2.310, val_acc:0.474]
Epoch [15/120    avg_loss:2.289, val_acc:0.484]
Epoch [16/120    avg_loss:2.220, val_acc:0.480]
Epoch [17/120    avg_loss:2.160, val_acc:0.507]
Epoch [18/120    avg_loss:2.073, val_acc:0.513]
Epoch [19/120    avg_loss:1.982, val_acc:0.542]
Epoch [20/120    avg_loss:1.910, val_acc:0.558]
Epoch [21/120    avg_loss:1.873, val_acc:0.542]
Epoch [22/120    avg_loss:1.776, val_acc:0.570]
Epoch [23/120    avg_loss:1.733, val_acc:0.583]
Epoch [24/120    avg_loss:1.620, val_acc:0.596]
Epoch [25/120    avg_loss:1.565, val_acc:0.604]
Epoch [26/120    avg_loss:1.502, val_acc:0.626]
Epoch [27/120    avg_loss:1.426, val_acc:0.629]
Epoch [28/120    avg_loss:1.360, val_acc:0.636]
Epoch [29/120    avg_loss:1.306, val_acc:0.632]
Epoch [30/120    avg_loss:1.235, val_acc:0.664]
Epoch [31/120    avg_loss:1.206, val_acc:0.685]
Epoch [32/120    avg_loss:1.126, val_acc:0.711]
Epoch [33/120    avg_loss:1.079, val_acc:0.727]
Epoch [34/120    avg_loss:1.022, val_acc:0.752]
Epoch [35/120    avg_loss:0.961, val_acc:0.774]
Epoch [36/120    avg_loss:0.923, val_acc:0.774]
Epoch [37/120    avg_loss:0.889, val_acc:0.812]
Epoch [38/120    avg_loss:0.850, val_acc:0.804]
Epoch [39/120    avg_loss:0.799, val_acc:0.838]
Epoch [40/120    avg_loss:0.771, val_acc:0.836]
Epoch [41/120    avg_loss:0.693, val_acc:0.842]
Epoch [42/120    avg_loss:0.658, val_acc:0.862]
Epoch [43/120    avg_loss:0.631, val_acc:0.864]
Epoch [44/120    avg_loss:0.568, val_acc:0.857]
Epoch [45/120    avg_loss:0.566, val_acc:0.870]
Epoch [46/120    avg_loss:0.507, val_acc:0.883]
Epoch [47/120    avg_loss:0.496, val_acc:0.902]
Epoch [48/120    avg_loss:0.423, val_acc:0.911]
Epoch [49/120    avg_loss:0.382, val_acc:0.926]
Epoch [50/120    avg_loss:0.351, val_acc:0.924]
Epoch [51/120    avg_loss:0.313, val_acc:0.924]
Epoch [52/120    avg_loss:0.300, val_acc:0.932]
Epoch [53/120    avg_loss:0.265, val_acc:0.943]
Epoch [54/120    avg_loss:0.282, val_acc:0.916]
Epoch [55/120    avg_loss:0.289, val_acc:0.905]
Epoch [56/120    avg_loss:0.239, val_acc:0.943]
Epoch [57/120    avg_loss:0.205, val_acc:0.950]
Epoch [58/120    avg_loss:0.193, val_acc:0.948]
Epoch [59/120    avg_loss:0.188, val_acc:0.959]
Epoch [60/120    avg_loss:0.166, val_acc:0.942]
Epoch [61/120    avg_loss:0.170, val_acc:0.955]
Epoch [62/120    avg_loss:0.141, val_acc:0.949]
Epoch [63/120    avg_loss:0.122, val_acc:0.965]
Epoch [64/120    avg_loss:0.120, val_acc:0.958]
Epoch [65/120    avg_loss:0.119, val_acc:0.965]
Epoch [66/120    avg_loss:0.113, val_acc:0.953]
Epoch [67/120    avg_loss:0.102, val_acc:0.967]
Epoch [68/120    avg_loss:0.101, val_acc:0.963]
Epoch [69/120    avg_loss:0.094, val_acc:0.964]
Epoch [70/120    avg_loss:0.091, val_acc:0.965]
Epoch [71/120    avg_loss:0.079, val_acc:0.960]
Epoch [72/120    avg_loss:0.091, val_acc:0.963]
Epoch [73/120    avg_loss:0.085, val_acc:0.959]
Epoch [74/120    avg_loss:0.085, val_acc:0.954]
Epoch [75/120    avg_loss:0.068, val_acc:0.968]
Epoch [76/120    avg_loss:0.069, val_acc:0.961]
Epoch [77/120    avg_loss:0.069, val_acc:0.960]
Epoch [78/120    avg_loss:0.071, val_acc:0.965]
Epoch [79/120    avg_loss:0.075, val_acc:0.967]
Epoch [80/120    avg_loss:0.072, val_acc:0.962]
Epoch [81/120    avg_loss:0.076, val_acc:0.970]
Epoch [82/120    avg_loss:0.065, val_acc:0.967]
Epoch [83/120    avg_loss:0.070, val_acc:0.965]
Epoch [84/120    avg_loss:0.076, val_acc:0.968]
Epoch [85/120    avg_loss:0.060, val_acc:0.963]
Epoch [86/120    avg_loss:0.053, val_acc:0.974]
Epoch [87/120    avg_loss:0.044, val_acc:0.964]
Epoch [88/120    avg_loss:0.042, val_acc:0.979]
Epoch [89/120    avg_loss:0.053, val_acc:0.971]
Epoch [90/120    avg_loss:0.049, val_acc:0.976]
Epoch [91/120    avg_loss:0.048, val_acc:0.962]
Epoch [92/120    avg_loss:0.055, val_acc:0.974]
Epoch [93/120    avg_loss:0.037, val_acc:0.970]
Epoch [94/120    avg_loss:0.036, val_acc:0.972]
Epoch [95/120    avg_loss:0.037, val_acc:0.982]
Epoch [96/120    avg_loss:0.033, val_acc:0.974]
Epoch [97/120    avg_loss:0.045, val_acc:0.958]
Epoch [98/120    avg_loss:0.044, val_acc:0.972]
Epoch [99/120    avg_loss:0.041, val_acc:0.973]
Epoch [100/120    avg_loss:0.036, val_acc:0.977]
Epoch [101/120    avg_loss:0.046, val_acc:0.978]
Epoch [102/120    avg_loss:0.037, val_acc:0.978]
Epoch [103/120    avg_loss:0.038, val_acc:0.965]
Epoch [104/120    avg_loss:0.040, val_acc:0.976]
Epoch [105/120    avg_loss:0.031, val_acc:0.967]
Epoch [106/120    avg_loss:0.034, val_acc:0.975]
Epoch [107/120    avg_loss:0.028, val_acc:0.973]
Epoch [108/120    avg_loss:0.038, val_acc:0.972]
Epoch [109/120    avg_loss:0.030, val_acc:0.975]
Epoch [110/120    avg_loss:0.026, val_acc:0.977]
Epoch [111/120    avg_loss:0.024, val_acc:0.977]
Epoch [112/120    avg_loss:0.025, val_acc:0.977]
Epoch [113/120    avg_loss:0.023, val_acc:0.978]
Epoch [114/120    avg_loss:0.021, val_acc:0.978]
Epoch [115/120    avg_loss:0.023, val_acc:0.978]
Epoch [116/120    avg_loss:0.020, val_acc:0.979]
Epoch [117/120    avg_loss:0.019, val_acc:0.978]
Epoch [118/120    avg_loss:0.020, val_acc:0.979]
Epoch [119/120    avg_loss:0.021, val_acc:0.978]
Epoch [120/120    avg_loss:0.022, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1246    6    0    0    2    0    0    0    8   23    0    0
     0    0    0]
 [   0    0    0  724    0    8    0    0    0   13    0    0    1    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    1    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    4   37    0    5    0    0    0    0  821    7    1    0
     0    0    0]
 [   0    0    7    0    0    0    1    0    0    1   14 2181    5    1
     0    0    0]
 [   0    0    0   22    1    4    0    0    0    0   13    8  483    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    58  270    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.95392953929539

F1 scores:
[       nan 0.975      0.98033045 0.94148244 0.99765808 0.97968397
 0.98279731 1.         1.         0.625      0.94694348 0.98487243
 0.94243902 0.99730458 0.97431507 0.87378641 0.98224852]

Kappa:
0.9652523896526362
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc3ab1cff28>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.821, val_acc:0.085]
Epoch [2/120    avg_loss:2.781, val_acc:0.183]
Epoch [3/120    avg_loss:2.730, val_acc:0.195]
Epoch [4/120    avg_loss:2.707, val_acc:0.212]
Epoch [5/120    avg_loss:2.674, val_acc:0.247]
Epoch [6/120    avg_loss:2.627, val_acc:0.275]
Epoch [7/120    avg_loss:2.605, val_acc:0.315]
Epoch [8/120    avg_loss:2.591, val_acc:0.339]
Epoch [9/120    avg_loss:2.509, val_acc:0.365]
Epoch [10/120    avg_loss:2.492, val_acc:0.372]
Epoch [11/120    avg_loss:2.448, val_acc:0.384]
Epoch [12/120    avg_loss:2.426, val_acc:0.381]
Epoch [13/120    avg_loss:2.386, val_acc:0.403]
Epoch [14/120    avg_loss:2.353, val_acc:0.405]
Epoch [15/120    avg_loss:2.291, val_acc:0.460]
Epoch [16/120    avg_loss:2.288, val_acc:0.449]
Epoch [17/120    avg_loss:2.233, val_acc:0.482]
Epoch [18/120    avg_loss:2.227, val_acc:0.503]
Epoch [19/120    avg_loss:2.175, val_acc:0.519]
Epoch [20/120    avg_loss:2.127, val_acc:0.574]
Epoch [21/120    avg_loss:2.128, val_acc:0.572]
Epoch [22/120    avg_loss:2.123, val_acc:0.573]
Epoch [23/120    avg_loss:2.037, val_acc:0.608]
Epoch [24/120    avg_loss:1.994, val_acc:0.622]
Epoch [25/120    avg_loss:1.915, val_acc:0.607]
Epoch [26/120    avg_loss:1.903, val_acc:0.651]
Epoch [27/120    avg_loss:1.795, val_acc:0.665]
Epoch [28/120    avg_loss:1.749, val_acc:0.668]
Epoch [29/120    avg_loss:1.739, val_acc:0.670]
Epoch [30/120    avg_loss:1.687, val_acc:0.707]
Epoch [31/120    avg_loss:1.612, val_acc:0.691]
Epoch [32/120    avg_loss:1.573, val_acc:0.710]
Epoch [33/120    avg_loss:1.487, val_acc:0.697]
Epoch [34/120    avg_loss:1.421, val_acc:0.745]
Epoch [35/120    avg_loss:1.414, val_acc:0.738]
Epoch [36/120    avg_loss:1.325, val_acc:0.759]
Epoch [37/120    avg_loss:1.195, val_acc:0.772]
Epoch [38/120    avg_loss:1.096, val_acc:0.795]
Epoch [39/120    avg_loss:1.038, val_acc:0.803]
Epoch [40/120    avg_loss:0.942, val_acc:0.801]
Epoch [41/120    avg_loss:0.861, val_acc:0.819]
Epoch [42/120    avg_loss:0.808, val_acc:0.825]
Epoch [43/120    avg_loss:0.784, val_acc:0.820]
Epoch [44/120    avg_loss:0.756, val_acc:0.818]
Epoch [45/120    avg_loss:0.810, val_acc:0.815]
Epoch [46/120    avg_loss:0.698, val_acc:0.831]
Epoch [47/120    avg_loss:0.643, val_acc:0.823]
Epoch [48/120    avg_loss:0.572, val_acc:0.859]
Epoch [49/120    avg_loss:0.522, val_acc:0.855]
Epoch [50/120    avg_loss:0.492, val_acc:0.848]
Epoch [51/120    avg_loss:0.468, val_acc:0.841]
Epoch [52/120    avg_loss:0.415, val_acc:0.874]
Epoch [53/120    avg_loss:0.357, val_acc:0.895]
Epoch [54/120    avg_loss:0.302, val_acc:0.908]
Epoch [55/120    avg_loss:0.308, val_acc:0.912]
Epoch [56/120    avg_loss:0.287, val_acc:0.906]
Epoch [57/120    avg_loss:0.266, val_acc:0.894]
Epoch [58/120    avg_loss:0.268, val_acc:0.900]
Epoch [59/120    avg_loss:0.222, val_acc:0.918]
Epoch [60/120    avg_loss:0.213, val_acc:0.915]
Epoch [61/120    avg_loss:0.208, val_acc:0.910]
Epoch [62/120    avg_loss:0.236, val_acc:0.910]
Epoch [63/120    avg_loss:0.189, val_acc:0.919]
Epoch [64/120    avg_loss:0.166, val_acc:0.925]
Epoch [65/120    avg_loss:0.164, val_acc:0.934]
Epoch [66/120    avg_loss:0.190, val_acc:0.919]
Epoch [67/120    avg_loss:0.172, val_acc:0.902]
Epoch [68/120    avg_loss:0.169, val_acc:0.928]
Epoch [69/120    avg_loss:0.146, val_acc:0.938]
Epoch [70/120    avg_loss:0.128, val_acc:0.934]
Epoch [71/120    avg_loss:0.132, val_acc:0.927]
Epoch [72/120    avg_loss:0.144, val_acc:0.934]
Epoch [73/120    avg_loss:0.133, val_acc:0.935]
Epoch [74/120    avg_loss:0.113, val_acc:0.945]
Epoch [75/120    avg_loss:0.105, val_acc:0.942]
Epoch [76/120    avg_loss:0.097, val_acc:0.944]
Epoch [77/120    avg_loss:0.104, val_acc:0.949]
Epoch [78/120    avg_loss:0.087, val_acc:0.949]
Epoch [79/120    avg_loss:0.088, val_acc:0.945]
Epoch [80/120    avg_loss:0.090, val_acc:0.951]
Epoch [81/120    avg_loss:0.083, val_acc:0.957]
Epoch [82/120    avg_loss:0.119, val_acc:0.944]
Epoch [83/120    avg_loss:0.094, val_acc:0.953]
Epoch [84/120    avg_loss:0.076, val_acc:0.951]
Epoch [85/120    avg_loss:0.061, val_acc:0.959]
Epoch [86/120    avg_loss:0.077, val_acc:0.951]
Epoch [87/120    avg_loss:0.069, val_acc:0.949]
Epoch [88/120    avg_loss:0.071, val_acc:0.950]
Epoch [89/120    avg_loss:0.074, val_acc:0.944]
Epoch [90/120    avg_loss:0.065, val_acc:0.959]
Epoch [91/120    avg_loss:0.061, val_acc:0.952]
Epoch [92/120    avg_loss:0.058, val_acc:0.958]
Epoch [93/120    avg_loss:0.057, val_acc:0.958]
Epoch [94/120    avg_loss:0.067, val_acc:0.953]
Epoch [95/120    avg_loss:0.054, val_acc:0.959]
Epoch [96/120    avg_loss:0.056, val_acc:0.961]
Epoch [97/120    avg_loss:0.047, val_acc:0.966]
Epoch [98/120    avg_loss:0.058, val_acc:0.959]
Epoch [99/120    avg_loss:0.052, val_acc:0.963]
Epoch [100/120    avg_loss:0.042, val_acc:0.966]
Epoch [101/120    avg_loss:0.042, val_acc:0.963]
Epoch [102/120    avg_loss:0.037, val_acc:0.967]
Epoch [103/120    avg_loss:0.034, val_acc:0.973]
Epoch [104/120    avg_loss:0.038, val_acc:0.963]
Epoch [105/120    avg_loss:0.035, val_acc:0.973]
Epoch [106/120    avg_loss:0.031, val_acc:0.969]
Epoch [107/120    avg_loss:0.030, val_acc:0.970]
Epoch [108/120    avg_loss:0.028, val_acc:0.970]
Epoch [109/120    avg_loss:0.032, val_acc:0.970]
Epoch [110/120    avg_loss:0.027, val_acc:0.974]
Epoch [111/120    avg_loss:0.031, val_acc:0.966]
Epoch [112/120    avg_loss:0.028, val_acc:0.973]
Epoch [113/120    avg_loss:0.030, val_acc:0.970]
Epoch [114/120    avg_loss:0.029, val_acc:0.968]
Epoch [115/120    avg_loss:0.027, val_acc:0.973]
Epoch [116/120    avg_loss:0.037, val_acc:0.970]
Epoch [117/120    avg_loss:0.036, val_acc:0.969]
Epoch [118/120    avg_loss:0.038, val_acc:0.965]
Epoch [119/120    avg_loss:0.028, val_acc:0.972]
Epoch [120/120    avg_loss:0.039, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1247    8    4    4    1    0    0    0    1   13    4    0
     0    3    0]
 [   0    0    0  710    3   12    0    0    0   21    0    1    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0   36   44    0    6    0    0    0    0  783    5    0    0
     0    1    0]
 [   0    0   55    2    0    3    1    1    0    1   11 2128    5    3
     0    0    0]
 [   0    0    0   31   14   12    0    0    0    0    2    0  468    0
     0    1    6]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    1    0    0    0
  1133    0    0]
 [   0    0    0    0    0    0   26    0    0    0    0    0    0    0
    38  283    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
95.64227642276423

F1 scores:
[       nan 0.94871795 0.95081967 0.91849935 0.95302013 0.94945055
 0.97761194 0.98039216 1.         0.49122807 0.93381038 0.97659477
 0.92307692 0.98924731 0.98052791 0.89133858 0.94736842]

Kappa:
0.9503617862759178
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff3bc036f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.834, val_acc:0.250]
Epoch [2/120    avg_loss:2.807, val_acc:0.275]
Epoch [3/120    avg_loss:2.783, val_acc:0.299]
Epoch [4/120    avg_loss:2.755, val_acc:0.332]
Epoch [5/120    avg_loss:2.710, val_acc:0.330]
Epoch [6/120    avg_loss:2.678, val_acc:0.336]
Epoch [7/120    avg_loss:2.622, val_acc:0.332]
Epoch [8/120    avg_loss:2.574, val_acc:0.350]
Epoch [9/120    avg_loss:2.484, val_acc:0.377]
Epoch [10/120    avg_loss:2.470, val_acc:0.385]
Epoch [11/120    avg_loss:2.401, val_acc:0.409]
Epoch [12/120    avg_loss:2.394, val_acc:0.434]
Epoch [13/120    avg_loss:2.358, val_acc:0.451]
Epoch [14/120    avg_loss:2.294, val_acc:0.487]
Epoch [15/120    avg_loss:2.254, val_acc:0.503]
Epoch [16/120    avg_loss:2.197, val_acc:0.475]
Epoch [17/120    avg_loss:2.168, val_acc:0.540]
Epoch [18/120    avg_loss:2.115, val_acc:0.535]
Epoch [19/120    avg_loss:2.037, val_acc:0.528]
Epoch [20/120    avg_loss:1.934, val_acc:0.553]
Epoch [21/120    avg_loss:1.876, val_acc:0.579]
Epoch [22/120    avg_loss:1.775, val_acc:0.583]
Epoch [23/120    avg_loss:1.744, val_acc:0.590]
Epoch [24/120    avg_loss:1.658, val_acc:0.590]
Epoch [25/120    avg_loss:1.654, val_acc:0.595]
Epoch [26/120    avg_loss:1.645, val_acc:0.615]
Epoch [27/120    avg_loss:1.481, val_acc:0.635]
Epoch [28/120    avg_loss:1.416, val_acc:0.635]
Epoch [29/120    avg_loss:1.344, val_acc:0.630]
Epoch [30/120    avg_loss:1.292, val_acc:0.642]
Epoch [31/120    avg_loss:1.237, val_acc:0.641]
Epoch [32/120    avg_loss:1.168, val_acc:0.679]
Epoch [33/120    avg_loss:1.116, val_acc:0.716]
Epoch [34/120    avg_loss:1.072, val_acc:0.723]
Epoch [35/120    avg_loss:1.036, val_acc:0.739]
Epoch [36/120    avg_loss:0.965, val_acc:0.747]
Epoch [37/120    avg_loss:0.928, val_acc:0.758]
Epoch [38/120    avg_loss:0.885, val_acc:0.765]
Epoch [39/120    avg_loss:0.868, val_acc:0.774]
Epoch [40/120    avg_loss:0.809, val_acc:0.790]
Epoch [41/120    avg_loss:0.737, val_acc:0.801]
Epoch [42/120    avg_loss:0.677, val_acc:0.803]
Epoch [43/120    avg_loss:0.666, val_acc:0.804]
Epoch [44/120    avg_loss:0.617, val_acc:0.833]
Epoch [45/120    avg_loss:0.547, val_acc:0.858]
Epoch [46/120    avg_loss:0.535, val_acc:0.862]
Epoch [47/120    avg_loss:0.512, val_acc:0.874]
Epoch [48/120    avg_loss:0.444, val_acc:0.884]
Epoch [49/120    avg_loss:0.434, val_acc:0.893]
Epoch [50/120    avg_loss:0.383, val_acc:0.895]
Epoch [51/120    avg_loss:0.345, val_acc:0.930]
Epoch [52/120    avg_loss:0.481, val_acc:0.868]
Epoch [53/120    avg_loss:0.390, val_acc:0.886]
Epoch [54/120    avg_loss:0.357, val_acc:0.907]
Epoch [55/120    avg_loss:0.317, val_acc:0.891]
Epoch [56/120    avg_loss:0.280, val_acc:0.915]
Epoch [57/120    avg_loss:0.267, val_acc:0.908]
Epoch [58/120    avg_loss:0.254, val_acc:0.917]
Epoch [59/120    avg_loss:0.252, val_acc:0.903]
Epoch [60/120    avg_loss:0.242, val_acc:0.895]
Epoch [61/120    avg_loss:0.230, val_acc:0.922]
Epoch [62/120    avg_loss:0.269, val_acc:0.922]
Epoch [63/120    avg_loss:0.250, val_acc:0.911]
Epoch [64/120    avg_loss:0.219, val_acc:0.920]
Epoch [65/120    avg_loss:0.185, val_acc:0.939]
Epoch [66/120    avg_loss:0.167, val_acc:0.943]
Epoch [67/120    avg_loss:0.153, val_acc:0.949]
Epoch [68/120    avg_loss:0.148, val_acc:0.949]
Epoch [69/120    avg_loss:0.145, val_acc:0.951]
Epoch [70/120    avg_loss:0.140, val_acc:0.946]
Epoch [71/120    avg_loss:0.144, val_acc:0.950]
Epoch [72/120    avg_loss:0.127, val_acc:0.951]
Epoch [73/120    avg_loss:0.128, val_acc:0.948]
Epoch [74/120    avg_loss:0.130, val_acc:0.952]
Epoch [75/120    avg_loss:0.122, val_acc:0.951]
Epoch [76/120    avg_loss:0.130, val_acc:0.948]
Epoch [77/120    avg_loss:0.126, val_acc:0.953]
Epoch [78/120    avg_loss:0.126, val_acc:0.950]
Epoch [79/120    avg_loss:0.126, val_acc:0.951]
Epoch [80/120    avg_loss:0.122, val_acc:0.950]
Epoch [81/120    avg_loss:0.125, val_acc:0.951]
Epoch [82/120    avg_loss:0.119, val_acc:0.952]
Epoch [83/120    avg_loss:0.112, val_acc:0.955]
Epoch [84/120    avg_loss:0.106, val_acc:0.953]
Epoch [85/120    avg_loss:0.106, val_acc:0.951]
Epoch [86/120    avg_loss:0.113, val_acc:0.952]
Epoch [87/120    avg_loss:0.118, val_acc:0.950]
Epoch [88/120    avg_loss:0.111, val_acc:0.953]
Epoch [89/120    avg_loss:0.103, val_acc:0.955]
Epoch [90/120    avg_loss:0.110, val_acc:0.951]
Epoch [91/120    avg_loss:0.104, val_acc:0.954]
Epoch [92/120    avg_loss:0.112, val_acc:0.957]
Epoch [93/120    avg_loss:0.103, val_acc:0.954]
Epoch [94/120    avg_loss:0.111, val_acc:0.954]
Epoch [95/120    avg_loss:0.103, val_acc:0.952]
Epoch [96/120    avg_loss:0.104, val_acc:0.953]
Epoch [97/120    avg_loss:0.099, val_acc:0.952]
Epoch [98/120    avg_loss:0.104, val_acc:0.959]
Epoch [99/120    avg_loss:0.107, val_acc:0.953]
Epoch [100/120    avg_loss:0.101, val_acc:0.952]
Epoch [101/120    avg_loss:0.104, val_acc:0.953]
Epoch [102/120    avg_loss:0.102, val_acc:0.954]
Epoch [103/120    avg_loss:0.100, val_acc:0.954]
Epoch [104/120    avg_loss:0.101, val_acc:0.957]
Epoch [105/120    avg_loss:0.099, val_acc:0.954]
Epoch [106/120    avg_loss:0.092, val_acc:0.954]
Epoch [107/120    avg_loss:0.099, val_acc:0.955]
Epoch [108/120    avg_loss:0.096, val_acc:0.958]
Epoch [109/120    avg_loss:0.094, val_acc:0.957]
Epoch [110/120    avg_loss:0.091, val_acc:0.952]
Epoch [111/120    avg_loss:0.093, val_acc:0.952]
Epoch [112/120    avg_loss:0.092, val_acc:0.953]
Epoch [113/120    avg_loss:0.094, val_acc:0.953]
Epoch [114/120    avg_loss:0.099, val_acc:0.955]
Epoch [115/120    avg_loss:0.100, val_acc:0.955]
Epoch [116/120    avg_loss:0.099, val_acc:0.957]
Epoch [117/120    avg_loss:0.093, val_acc:0.954]
Epoch [118/120    avg_loss:0.090, val_acc:0.955]
Epoch [119/120    avg_loss:0.098, val_acc:0.954]
Epoch [120/120    avg_loss:0.094, val_acc:0.954]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1187    8    0    8    3    0    0    3    8   66    0    0
     0    2    0]
 [   0    0    5  689    4   17    0    0    0    8    0    3   18    2
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    5    0    1    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0  651    0    0    0    0    5    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   25   46    0    7    0    0    0    0  753   34    0    0
     3    7    0]
 [   0    0   27    0    0    7    9    0    0    0   18 2122    5    7
    15    0    0]
 [   0    0    0    5   14    6    0    0    0    0   14   14  469    0
     1    0   11]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    0    0    1    1    0    0
  1127    0    0]
 [   0    0    0    0    0    0   28    0    0    0    1    0    0    0
    68  250    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
93.94037940379404

F1 scores:
[       nan 0.975      0.93833992 0.92173913 0.95945946 0.9336235
 0.96587537 0.90909091 1.         0.72340426 0.9007177  0.95263749
 0.90979631 0.9762533  0.95751912 0.82508251 0.91428571]

Kappa:
0.9308734290747059
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd244763f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.806, val_acc:0.161]
Epoch [2/120    avg_loss:2.768, val_acc:0.200]
Epoch [3/120    avg_loss:2.737, val_acc:0.182]
Epoch [4/120    avg_loss:2.706, val_acc:0.216]
Epoch [5/120    avg_loss:2.672, val_acc:0.222]
Epoch [6/120    avg_loss:2.623, val_acc:0.234]
Epoch [7/120    avg_loss:2.554, val_acc:0.245]
Epoch [8/120    avg_loss:2.521, val_acc:0.249]
Epoch [9/120    avg_loss:2.490, val_acc:0.261]
Epoch [10/120    avg_loss:2.446, val_acc:0.295]
Epoch [11/120    avg_loss:2.412, val_acc:0.325]
Epoch [12/120    avg_loss:2.359, val_acc:0.375]
Epoch [13/120    avg_loss:2.340, val_acc:0.374]
Epoch [14/120    avg_loss:2.312, val_acc:0.418]
Epoch [15/120    avg_loss:2.248, val_acc:0.443]
Epoch [16/120    avg_loss:2.242, val_acc:0.521]
Epoch [17/120    avg_loss:2.169, val_acc:0.528]
Epoch [18/120    avg_loss:2.111, val_acc:0.576]
Epoch [19/120    avg_loss:2.104, val_acc:0.614]
Epoch [20/120    avg_loss:2.078, val_acc:0.618]
Epoch [21/120    avg_loss:1.998, val_acc:0.638]
Epoch [22/120    avg_loss:1.956, val_acc:0.625]
Epoch [23/120    avg_loss:1.883, val_acc:0.638]
Epoch [24/120    avg_loss:1.759, val_acc:0.647]
Epoch [25/120    avg_loss:1.689, val_acc:0.652]
Epoch [26/120    avg_loss:1.558, val_acc:0.697]
Epoch [27/120    avg_loss:1.501, val_acc:0.688]
Epoch [28/120    avg_loss:1.370, val_acc:0.683]
Epoch [29/120    avg_loss:1.343, val_acc:0.718]
Epoch [30/120    avg_loss:1.292, val_acc:0.734]
Epoch [31/120    avg_loss:1.196, val_acc:0.751]
Epoch [32/120    avg_loss:1.079, val_acc:0.770]
Epoch [33/120    avg_loss:1.079, val_acc:0.799]
Epoch [34/120    avg_loss:1.012, val_acc:0.801]
Epoch [35/120    avg_loss:0.933, val_acc:0.821]
Epoch [36/120    avg_loss:0.858, val_acc:0.847]
Epoch [37/120    avg_loss:0.752, val_acc:0.841]
Epoch [38/120    avg_loss:0.704, val_acc:0.842]
Epoch [39/120    avg_loss:0.671, val_acc:0.860]
Epoch [40/120    avg_loss:0.636, val_acc:0.873]
Epoch [41/120    avg_loss:0.586, val_acc:0.872]
Epoch [42/120    avg_loss:0.492, val_acc:0.863]
Epoch [43/120    avg_loss:0.460, val_acc:0.892]
Epoch [44/120    avg_loss:0.444, val_acc:0.905]
Epoch [45/120    avg_loss:0.424, val_acc:0.899]
Epoch [46/120    avg_loss:0.405, val_acc:0.890]
Epoch [47/120    avg_loss:0.341, val_acc:0.937]
Epoch [48/120    avg_loss:0.367, val_acc:0.902]
Epoch [49/120    avg_loss:0.363, val_acc:0.905]
Epoch [50/120    avg_loss:0.297, val_acc:0.925]
Epoch [51/120    avg_loss:0.270, val_acc:0.913]
Epoch [52/120    avg_loss:0.231, val_acc:0.942]
Epoch [53/120    avg_loss:0.299, val_acc:0.946]
Epoch [54/120    avg_loss:0.221, val_acc:0.935]
Epoch [55/120    avg_loss:0.192, val_acc:0.937]
Epoch [56/120    avg_loss:0.193, val_acc:0.948]
Epoch [57/120    avg_loss:0.167, val_acc:0.949]
Epoch [58/120    avg_loss:0.166, val_acc:0.955]
Epoch [59/120    avg_loss:0.142, val_acc:0.928]
Epoch [60/120    avg_loss:0.143, val_acc:0.958]
Epoch [61/120    avg_loss:0.138, val_acc:0.961]
Epoch [62/120    avg_loss:0.148, val_acc:0.933]
Epoch [63/120    avg_loss:0.130, val_acc:0.966]
Epoch [64/120    avg_loss:0.133, val_acc:0.959]
Epoch [65/120    avg_loss:0.111, val_acc:0.917]
Epoch [66/120    avg_loss:0.160, val_acc:0.962]
Epoch [67/120    avg_loss:0.125, val_acc:0.967]
Epoch [68/120    avg_loss:0.109, val_acc:0.968]
Epoch [69/120    avg_loss:0.104, val_acc:0.970]
Epoch [70/120    avg_loss:0.094, val_acc:0.967]
Epoch [71/120    avg_loss:0.086, val_acc:0.972]
Epoch [72/120    avg_loss:0.078, val_acc:0.976]
Epoch [73/120    avg_loss:0.077, val_acc:0.971]
Epoch [74/120    avg_loss:0.064, val_acc:0.973]
Epoch [75/120    avg_loss:0.072, val_acc:0.966]
Epoch [76/120    avg_loss:0.069, val_acc:0.963]
Epoch [77/120    avg_loss:0.094, val_acc:0.966]
Epoch [78/120    avg_loss:0.080, val_acc:0.975]
Epoch [79/120    avg_loss:0.065, val_acc:0.970]
Epoch [80/120    avg_loss:0.062, val_acc:0.975]
Epoch [81/120    avg_loss:0.050, val_acc:0.972]
Epoch [82/120    avg_loss:0.048, val_acc:0.978]
Epoch [83/120    avg_loss:0.047, val_acc:0.975]
Epoch [84/120    avg_loss:0.052, val_acc:0.976]
Epoch [85/120    avg_loss:0.050, val_acc:0.975]
Epoch [86/120    avg_loss:0.055, val_acc:0.968]
Epoch [87/120    avg_loss:0.047, val_acc:0.975]
Epoch [88/120    avg_loss:0.040, val_acc:0.979]
Epoch [89/120    avg_loss:0.044, val_acc:0.975]
Epoch [90/120    avg_loss:0.055, val_acc:0.977]
Epoch [91/120    avg_loss:0.048, val_acc:0.978]
Epoch [92/120    avg_loss:0.040, val_acc:0.978]
Epoch [93/120    avg_loss:0.040, val_acc:0.978]
Epoch [94/120    avg_loss:0.041, val_acc:0.980]
Epoch [95/120    avg_loss:0.031, val_acc:0.978]
Epoch [96/120    avg_loss:0.033, val_acc:0.982]
Epoch [97/120    avg_loss:0.031, val_acc:0.978]
Epoch [98/120    avg_loss:0.031, val_acc:0.980]
Epoch [99/120    avg_loss:0.032, val_acc:0.983]
Epoch [100/120    avg_loss:0.035, val_acc:0.986]
Epoch [101/120    avg_loss:0.034, val_acc:0.974]
Epoch [102/120    avg_loss:0.036, val_acc:0.985]
Epoch [103/120    avg_loss:0.036, val_acc:0.982]
Epoch [104/120    avg_loss:0.039, val_acc:0.965]
Epoch [105/120    avg_loss:0.036, val_acc:0.979]
Epoch [106/120    avg_loss:0.027, val_acc:0.977]
Epoch [107/120    avg_loss:0.023, val_acc:0.978]
Epoch [108/120    avg_loss:0.023, val_acc:0.984]
Epoch [109/120    avg_loss:0.023, val_acc:0.985]
Epoch [110/120    avg_loss:0.024, val_acc:0.984]
Epoch [111/120    avg_loss:0.020, val_acc:0.985]
Epoch [112/120    avg_loss:0.019, val_acc:0.984]
Epoch [113/120    avg_loss:0.022, val_acc:0.983]
Epoch [114/120    avg_loss:0.019, val_acc:0.986]
Epoch [115/120    avg_loss:0.025, val_acc:0.987]
Epoch [116/120    avg_loss:0.018, val_acc:0.986]
Epoch [117/120    avg_loss:0.019, val_acc:0.985]
Epoch [118/120    avg_loss:0.017, val_acc:0.984]
Epoch [119/120    avg_loss:0.018, val_acc:0.983]
Epoch [120/120    avg_loss:0.015, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1254    0    7    1    2    0    0    0    5   11    1    0
     0    4    0]
 [   0    0    1  704   21    7    0    0    0   10    0    1    2    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    1    2    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    7   35    0    5    0    0    0    0  800   17    6    0
     1    4    0]
 [   0    0    6    0    0    0    2    0    0    0   12 2187    3    0
     0    0    0]
 [   0    0    0    0    2    3    0    0    0    2    1   13  507    0
     0    3    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
    55  267    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.84552845528455

F1 scores:
[       nan 0.96202532 0.98237368 0.94751009 0.93421053 0.975
 0.97767857 0.96153846 0.997669   0.69387755 0.94339623 0.98535706
 0.96022727 1.         0.97517123 0.8544     0.98245614]

Kappa:
0.9640204305812811
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5844cfff60>
supervision:full
center_pixel:True
Network :
Number of parameter: 41789==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.816, val_acc:0.205]
Epoch [2/120    avg_loss:2.787, val_acc:0.307]
Epoch [3/120    avg_loss:2.747, val_acc:0.315]
Epoch [4/120    avg_loss:2.723, val_acc:0.318]
Epoch [5/120    avg_loss:2.681, val_acc:0.324]
Epoch [6/120    avg_loss:2.656, val_acc:0.325]
Epoch [7/120    avg_loss:2.632, val_acc:0.332]
Epoch [8/120    avg_loss:2.569, val_acc:0.335]
Epoch [9/120    avg_loss:2.519, val_acc:0.338]
Epoch [10/120    avg_loss:2.489, val_acc:0.353]
Epoch [11/120    avg_loss:2.489, val_acc:0.385]
Epoch [12/120    avg_loss:2.427, val_acc:0.401]
Epoch [13/120    avg_loss:2.378, val_acc:0.416]
Epoch [14/120    avg_loss:2.364, val_acc:0.442]
Epoch [15/120    avg_loss:2.287, val_acc:0.463]
Epoch [16/120    avg_loss:2.278, val_acc:0.483]
Epoch [17/120    avg_loss:2.254, val_acc:0.506]
Epoch [18/120    avg_loss:2.222, val_acc:0.510]
Epoch [19/120    avg_loss:2.132, val_acc:0.534]
Epoch [20/120    avg_loss:2.099, val_acc:0.567]
Epoch [21/120    avg_loss:2.078, val_acc:0.564]
Epoch [22/120    avg_loss:2.035, val_acc:0.565]
Epoch [23/120    avg_loss:1.956, val_acc:0.611]
Epoch [24/120    avg_loss:1.890, val_acc:0.631]
Epoch [25/120    avg_loss:1.827, val_acc:0.675]
Epoch [26/120    avg_loss:1.774, val_acc:0.648]
Epoch [27/120    avg_loss:1.665, val_acc:0.660]
Epoch [28/120    avg_loss:1.585, val_acc:0.632]
Epoch [29/120    avg_loss:1.580, val_acc:0.659]
Epoch [30/120    avg_loss:1.534, val_acc:0.659]
Epoch [31/120    avg_loss:1.393, val_acc:0.672]
Epoch [32/120    avg_loss:1.316, val_acc:0.680]
Epoch [33/120    avg_loss:1.217, val_acc:0.706]
Epoch [34/120    avg_loss:1.180, val_acc:0.690]
Epoch [35/120    avg_loss:1.116, val_acc:0.709]
Epoch [36/120    avg_loss:1.101, val_acc:0.726]
Epoch [37/120    avg_loss:0.989, val_acc:0.732]
Epoch [38/120    avg_loss:0.960, val_acc:0.739]
Epoch [39/120    avg_loss:0.908, val_acc:0.759]
Epoch [40/120    avg_loss:0.876, val_acc:0.761]
Epoch [41/120    avg_loss:0.812, val_acc:0.781]
Epoch [42/120    avg_loss:0.777, val_acc:0.811]
Epoch [43/120    avg_loss:0.739, val_acc:0.809]
Epoch [44/120    avg_loss:0.676, val_acc:0.807]
Epoch [45/120    avg_loss:0.649, val_acc:0.834]
Epoch [46/120    avg_loss:0.642, val_acc:0.822]
Epoch [47/120    avg_loss:0.643, val_acc:0.835]
Epoch [48/120    avg_loss:0.541, val_acc:0.832]
Epoch [49/120    avg_loss:0.525, val_acc:0.842]
Epoch [50/120    avg_loss:0.455, val_acc:0.848]
Epoch [51/120    avg_loss:0.454, val_acc:0.843]
Epoch [52/120    avg_loss:0.428, val_acc:0.858]
Epoch [53/120    avg_loss:0.384, val_acc:0.890]
Epoch [54/120    avg_loss:0.337, val_acc:0.886]
Epoch [55/120    avg_loss:0.320, val_acc:0.899]
Epoch [56/120    avg_loss:0.347, val_acc:0.899]
Epoch [57/120    avg_loss:0.304, val_acc:0.892]
Epoch [58/120    avg_loss:0.253, val_acc:0.902]
Epoch [59/120    avg_loss:0.274, val_acc:0.887]
Epoch [60/120    avg_loss:0.275, val_acc:0.910]
Epoch [61/120    avg_loss:0.232, val_acc:0.916]
Epoch [62/120    avg_loss:0.241, val_acc:0.926]
Epoch [63/120    avg_loss:0.220, val_acc:0.897]
Epoch [64/120    avg_loss:0.233, val_acc:0.916]
Epoch [65/120    avg_loss:0.197, val_acc:0.928]
Epoch [66/120    avg_loss:0.188, val_acc:0.899]
Epoch [67/120    avg_loss:0.210, val_acc:0.925]
Epoch [68/120    avg_loss:0.159, val_acc:0.927]
Epoch [69/120    avg_loss:0.140, val_acc:0.935]
Epoch [70/120    avg_loss:0.122, val_acc:0.947]
Epoch [71/120    avg_loss:0.129, val_acc:0.944]
Epoch [72/120    avg_loss:0.121, val_acc:0.939]
Epoch [73/120    avg_loss:0.112, val_acc:0.944]
Epoch [74/120    avg_loss:0.107, val_acc:0.956]
Epoch [75/120    avg_loss:0.103, val_acc:0.945]
Epoch [76/120    avg_loss:0.116, val_acc:0.952]
Epoch [77/120    avg_loss:0.104, val_acc:0.957]
Epoch [78/120    avg_loss:0.090, val_acc:0.943]
Epoch [79/120    avg_loss:0.093, val_acc:0.949]
Epoch [80/120    avg_loss:0.078, val_acc:0.958]
Epoch [81/120    avg_loss:0.076, val_acc:0.961]
Epoch [82/120    avg_loss:0.084, val_acc:0.943]
Epoch [83/120    avg_loss:0.077, val_acc:0.955]
Epoch [84/120    avg_loss:0.079, val_acc:0.957]
Epoch [85/120    avg_loss:0.097, val_acc:0.947]
Epoch [86/120    avg_loss:0.089, val_acc:0.959]
Epoch [87/120    avg_loss:0.085, val_acc:0.956]
Epoch [88/120    avg_loss:0.088, val_acc:0.969]
Epoch [89/120    avg_loss:0.068, val_acc:0.961]
Epoch [90/120    avg_loss:0.073, val_acc:0.964]
Epoch [91/120    avg_loss:0.067, val_acc:0.959]
Epoch [92/120    avg_loss:0.059, val_acc:0.961]
Epoch [93/120    avg_loss:0.054, val_acc:0.972]
Epoch [94/120    avg_loss:0.061, val_acc:0.961]
Epoch [95/120    avg_loss:0.058, val_acc:0.973]
Epoch [96/120    avg_loss:0.049, val_acc:0.972]
Epoch [97/120    avg_loss:0.050, val_acc:0.965]
Epoch [98/120    avg_loss:0.059, val_acc:0.953]
Epoch [99/120    avg_loss:0.049, val_acc:0.973]
Epoch [100/120    avg_loss:0.047, val_acc:0.970]
Epoch [101/120    avg_loss:0.051, val_acc:0.965]
Epoch [102/120    avg_loss:0.040, val_acc:0.970]
Epoch [103/120    avg_loss:0.035, val_acc:0.975]
Epoch [104/120    avg_loss:0.038, val_acc:0.975]
Epoch [105/120    avg_loss:0.036, val_acc:0.982]
Epoch [106/120    avg_loss:0.044, val_acc:0.973]
Epoch [107/120    avg_loss:0.034, val_acc:0.969]
Epoch [108/120    avg_loss:0.035, val_acc:0.970]
Epoch [109/120    avg_loss:0.036, val_acc:0.977]
Epoch [110/120    avg_loss:0.045, val_acc:0.966]
Epoch [111/120    avg_loss:0.037, val_acc:0.972]
Epoch [112/120    avg_loss:0.032, val_acc:0.969]
Epoch [113/120    avg_loss:0.039, val_acc:0.973]
Epoch [114/120    avg_loss:0.038, val_acc:0.969]
Epoch [115/120    avg_loss:0.038, val_acc:0.970]
Epoch [116/120    avg_loss:0.034, val_acc:0.970]
Epoch [117/120    avg_loss:0.031, val_acc:0.974]
Epoch [118/120    avg_loss:0.032, val_acc:0.977]
Epoch [119/120    avg_loss:0.024, val_acc:0.978]
Epoch [120/120    avg_loss:0.021, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1240    4    0    0    0    0    0    0    5   34    0    0
     0    2    0]
 [   0    0    0  733    2    0    0    0    0    9    0    0    3    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0    7   33    0    4    0    0    0    0  814   11    0    0
     0    6    0]
 [   0    0    8    0    0    0    7    0    0    0   16 2173    6    0
     0    0    0]
 [   0    0    0   12   11   14    0    0    0    0    2    4  487    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    47  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.95392953929539

F1 scores:
[       nan 0.98765432 0.97637795 0.95754409 0.96803653 0.9740113
 0.9798357  0.98039216 1.         0.69565217 0.94982497 0.98037446
 0.94471387 1.         0.97890659 0.8822606  0.98245614]

Kappa:
0.9652608805385308
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1254132eb8>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.818, val_acc:0.020]
Epoch [2/120    avg_loss:2.777, val_acc:0.128]
Epoch [3/120    avg_loss:2.736, val_acc:0.149]
Epoch [4/120    avg_loss:2.698, val_acc:0.145]
Epoch [5/120    avg_loss:2.650, val_acc:0.193]
Epoch [6/120    avg_loss:2.611, val_acc:0.241]
Epoch [7/120    avg_loss:2.566, val_acc:0.273]
Epoch [8/120    avg_loss:2.517, val_acc:0.299]
Epoch [9/120    avg_loss:2.471, val_acc:0.316]
Epoch [10/120    avg_loss:2.425, val_acc:0.348]
Epoch [11/120    avg_loss:2.378, val_acc:0.382]
Epoch [12/120    avg_loss:2.330, val_acc:0.427]
Epoch [13/120    avg_loss:2.288, val_acc:0.458]
Epoch [14/120    avg_loss:2.253, val_acc:0.499]
Epoch [15/120    avg_loss:2.191, val_acc:0.527]
Epoch [16/120    avg_loss:2.136, val_acc:0.517]
Epoch [17/120    avg_loss:2.048, val_acc:0.467]
Epoch [18/120    avg_loss:1.977, val_acc:0.537]
Epoch [19/120    avg_loss:1.884, val_acc:0.474]
Epoch [20/120    avg_loss:1.876, val_acc:0.535]
Epoch [21/120    avg_loss:1.772, val_acc:0.494]
Epoch [22/120    avg_loss:1.712, val_acc:0.508]
Epoch [23/120    avg_loss:1.691, val_acc:0.592]
Epoch [24/120    avg_loss:1.610, val_acc:0.560]
Epoch [25/120    avg_loss:1.554, val_acc:0.643]
Epoch [26/120    avg_loss:1.485, val_acc:0.603]
Epoch [27/120    avg_loss:1.422, val_acc:0.607]
Epoch [28/120    avg_loss:1.377, val_acc:0.670]
Epoch [29/120    avg_loss:1.281, val_acc:0.653]
Epoch [30/120    avg_loss:1.257, val_acc:0.642]
Epoch [31/120    avg_loss:1.180, val_acc:0.726]
Epoch [32/120    avg_loss:1.111, val_acc:0.733]
Epoch [33/120    avg_loss:1.064, val_acc:0.757]
Epoch [34/120    avg_loss:1.021, val_acc:0.759]
Epoch [35/120    avg_loss:0.977, val_acc:0.782]
Epoch [36/120    avg_loss:0.921, val_acc:0.765]
Epoch [37/120    avg_loss:0.846, val_acc:0.784]
Epoch [38/120    avg_loss:0.825, val_acc:0.798]
Epoch [39/120    avg_loss:0.770, val_acc:0.803]
Epoch [40/120    avg_loss:0.749, val_acc:0.808]
Epoch [41/120    avg_loss:0.713, val_acc:0.794]
Epoch [42/120    avg_loss:0.652, val_acc:0.839]
Epoch [43/120    avg_loss:0.588, val_acc:0.803]
Epoch [44/120    avg_loss:0.547, val_acc:0.858]
Epoch [45/120    avg_loss:0.568, val_acc:0.840]
Epoch [46/120    avg_loss:0.494, val_acc:0.844]
Epoch [47/120    avg_loss:0.468, val_acc:0.887]
Epoch [48/120    avg_loss:0.410, val_acc:0.881]
Epoch [49/120    avg_loss:0.398, val_acc:0.868]
Epoch [50/120    avg_loss:0.390, val_acc:0.893]
Epoch [51/120    avg_loss:0.358, val_acc:0.892]
Epoch [52/120    avg_loss:0.311, val_acc:0.893]
Epoch [53/120    avg_loss:0.295, val_acc:0.912]
Epoch [54/120    avg_loss:0.275, val_acc:0.911]
Epoch [55/120    avg_loss:0.260, val_acc:0.919]
Epoch [56/120    avg_loss:0.271, val_acc:0.925]
Epoch [57/120    avg_loss:0.255, val_acc:0.902]
Epoch [58/120    avg_loss:0.221, val_acc:0.933]
Epoch [59/120    avg_loss:0.197, val_acc:0.941]
Epoch [60/120    avg_loss:0.170, val_acc:0.935]
Epoch [61/120    avg_loss:0.154, val_acc:0.944]
Epoch [62/120    avg_loss:0.166, val_acc:0.943]
Epoch [63/120    avg_loss:0.169, val_acc:0.951]
Epoch [64/120    avg_loss:0.159, val_acc:0.958]
Epoch [65/120    avg_loss:0.158, val_acc:0.952]
Epoch [66/120    avg_loss:0.134, val_acc:0.953]
Epoch [67/120    avg_loss:0.131, val_acc:0.951]
Epoch [68/120    avg_loss:0.131, val_acc:0.947]
Epoch [69/120    avg_loss:0.125, val_acc:0.950]
Epoch [70/120    avg_loss:0.098, val_acc:0.959]
Epoch [71/120    avg_loss:0.098, val_acc:0.950]
Epoch [72/120    avg_loss:0.095, val_acc:0.958]
Epoch [73/120    avg_loss:0.084, val_acc:0.955]
Epoch [74/120    avg_loss:0.084, val_acc:0.959]
Epoch [75/120    avg_loss:0.093, val_acc:0.963]
Epoch [76/120    avg_loss:0.081, val_acc:0.963]
Epoch [77/120    avg_loss:0.073, val_acc:0.963]
Epoch [78/120    avg_loss:0.065, val_acc:0.964]
Epoch [79/120    avg_loss:0.079, val_acc:0.967]
Epoch [80/120    avg_loss:0.095, val_acc:0.940]
Epoch [81/120    avg_loss:0.113, val_acc:0.943]
Epoch [82/120    avg_loss:0.076, val_acc:0.961]
Epoch [83/120    avg_loss:0.069, val_acc:0.963]
Epoch [84/120    avg_loss:0.058, val_acc:0.955]
Epoch [85/120    avg_loss:0.055, val_acc:0.974]
Epoch [86/120    avg_loss:0.054, val_acc:0.961]
Epoch [87/120    avg_loss:0.053, val_acc:0.960]
Epoch [88/120    avg_loss:0.054, val_acc:0.957]
Epoch [89/120    avg_loss:0.051, val_acc:0.964]
Epoch [90/120    avg_loss:0.054, val_acc:0.966]
Epoch [91/120    avg_loss:0.051, val_acc:0.968]
Epoch [92/120    avg_loss:0.043, val_acc:0.968]
Epoch [93/120    avg_loss:0.049, val_acc:0.969]
Epoch [94/120    avg_loss:0.038, val_acc:0.966]
Epoch [95/120    avg_loss:0.035, val_acc:0.970]
Epoch [96/120    avg_loss:0.040, val_acc:0.976]
Epoch [97/120    avg_loss:0.038, val_acc:0.967]
Epoch [98/120    avg_loss:0.035, val_acc:0.968]
Epoch [99/120    avg_loss:0.037, val_acc:0.977]
Epoch [100/120    avg_loss:0.040, val_acc:0.973]
Epoch [101/120    avg_loss:0.039, val_acc:0.967]
Epoch [102/120    avg_loss:0.040, val_acc:0.970]
Epoch [103/120    avg_loss:0.035, val_acc:0.968]
Epoch [104/120    avg_loss:0.031, val_acc:0.969]
Epoch [105/120    avg_loss:0.040, val_acc:0.977]
Epoch [106/120    avg_loss:0.035, val_acc:0.976]
Epoch [107/120    avg_loss:0.037, val_acc:0.967]
Epoch [108/120    avg_loss:0.032, val_acc:0.977]
Epoch [109/120    avg_loss:0.025, val_acc:0.975]
Epoch [110/120    avg_loss:0.026, val_acc:0.972]
Epoch [111/120    avg_loss:0.026, val_acc:0.970]
Epoch [112/120    avg_loss:0.028, val_acc:0.977]
Epoch [113/120    avg_loss:0.028, val_acc:0.970]
Epoch [114/120    avg_loss:0.027, val_acc:0.977]
Epoch [115/120    avg_loss:0.026, val_acc:0.978]
Epoch [116/120    avg_loss:0.023, val_acc:0.973]
Epoch [117/120    avg_loss:0.023, val_acc:0.983]
Epoch [118/120    avg_loss:0.022, val_acc:0.980]
Epoch [119/120    avg_loss:0.020, val_acc:0.974]
Epoch [120/120    avg_loss:0.020, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1237    2    0    0    2    0    0    0    0   30    4    0
     0   10    0]
 [   0    0    1  725    2   12    0    0    0    5    0    0    1    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    1    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   15   89    0    2    1    0    0    0  758    8    0    0
     2    0    0]
 [   0    0    7    0    0    0    3    0    0    0    2 2188    5    0
     5    0    0]
 [   0    0    0   10    3    8    0    0    0    0    3    3  503    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   46    0    0    0    0    0    0    0
   116  185    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
95.53387533875339

F1 scores:
[       nan 0.975      0.97210216 0.92121982 0.98839907 0.97187852
 0.96122897 1.         1.         0.76190476 0.92439024 0.98580761
 0.95809524 1.         0.94837635 0.68265683 0.95857988]

Kappa:
0.9490162006063003
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f01694d2fd0>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.845, val_acc:0.061]
Epoch [2/120    avg_loss:2.809, val_acc:0.096]
Epoch [3/120    avg_loss:2.777, val_acc:0.163]
Epoch [4/120    avg_loss:2.749, val_acc:0.217]
Epoch [5/120    avg_loss:2.722, val_acc:0.305]
Epoch [6/120    avg_loss:2.686, val_acc:0.326]
Epoch [7/120    avg_loss:2.642, val_acc:0.334]
Epoch [8/120    avg_loss:2.583, val_acc:0.351]
Epoch [9/120    avg_loss:2.540, val_acc:0.378]
Epoch [10/120    avg_loss:2.514, val_acc:0.463]
Epoch [11/120    avg_loss:2.472, val_acc:0.453]
Epoch [12/120    avg_loss:2.444, val_acc:0.463]
Epoch [13/120    avg_loss:2.405, val_acc:0.474]
Epoch [14/120    avg_loss:2.365, val_acc:0.493]
Epoch [15/120    avg_loss:2.322, val_acc:0.520]
Epoch [16/120    avg_loss:2.272, val_acc:0.530]
Epoch [17/120    avg_loss:2.250, val_acc:0.528]
Epoch [18/120    avg_loss:2.202, val_acc:0.536]
Epoch [19/120    avg_loss:2.139, val_acc:0.552]
Epoch [20/120    avg_loss:2.090, val_acc:0.559]
Epoch [21/120    avg_loss:2.070, val_acc:0.564]
Epoch [22/120    avg_loss:1.989, val_acc:0.553]
Epoch [23/120    avg_loss:1.947, val_acc:0.570]
Epoch [24/120    avg_loss:1.855, val_acc:0.552]
Epoch [25/120    avg_loss:1.766, val_acc:0.578]
Epoch [26/120    avg_loss:1.679, val_acc:0.556]
Epoch [27/120    avg_loss:1.634, val_acc:0.533]
Epoch [28/120    avg_loss:1.613, val_acc:0.599]
Epoch [29/120    avg_loss:1.505, val_acc:0.639]
Epoch [30/120    avg_loss:1.473, val_acc:0.644]
Epoch [31/120    avg_loss:1.398, val_acc:0.634]
Epoch [32/120    avg_loss:1.327, val_acc:0.666]
Epoch [33/120    avg_loss:1.243, val_acc:0.675]
Epoch [34/120    avg_loss:1.207, val_acc:0.653]
Epoch [35/120    avg_loss:1.204, val_acc:0.665]
Epoch [36/120    avg_loss:1.128, val_acc:0.713]
Epoch [37/120    avg_loss:1.069, val_acc:0.715]
Epoch [38/120    avg_loss:1.038, val_acc:0.714]
Epoch [39/120    avg_loss:0.978, val_acc:0.731]
Epoch [40/120    avg_loss:0.940, val_acc:0.743]
Epoch [41/120    avg_loss:0.930, val_acc:0.722]
Epoch [42/120    avg_loss:0.835, val_acc:0.760]
Epoch [43/120    avg_loss:0.786, val_acc:0.769]
Epoch [44/120    avg_loss:0.727, val_acc:0.781]
Epoch [45/120    avg_loss:0.681, val_acc:0.794]
Epoch [46/120    avg_loss:0.646, val_acc:0.777]
Epoch [47/120    avg_loss:0.617, val_acc:0.793]
Epoch [48/120    avg_loss:0.587, val_acc:0.789]
Epoch [49/120    avg_loss:0.569, val_acc:0.816]
Epoch [50/120    avg_loss:0.558, val_acc:0.823]
Epoch [51/120    avg_loss:0.552, val_acc:0.818]
Epoch [52/120    avg_loss:0.494, val_acc:0.830]
Epoch [53/120    avg_loss:0.440, val_acc:0.840]
Epoch [54/120    avg_loss:0.456, val_acc:0.836]
Epoch [55/120    avg_loss:0.422, val_acc:0.844]
Epoch [56/120    avg_loss:0.413, val_acc:0.851]
Epoch [57/120    avg_loss:0.408, val_acc:0.858]
Epoch [58/120    avg_loss:0.378, val_acc:0.849]
Epoch [59/120    avg_loss:0.355, val_acc:0.848]
Epoch [60/120    avg_loss:0.347, val_acc:0.875]
Epoch [61/120    avg_loss:0.336, val_acc:0.868]
Epoch [62/120    avg_loss:0.302, val_acc:0.889]
Epoch [63/120    avg_loss:0.299, val_acc:0.886]
Epoch [64/120    avg_loss:0.276, val_acc:0.911]
Epoch [65/120    avg_loss:0.259, val_acc:0.906]
Epoch [66/120    avg_loss:0.251, val_acc:0.916]
Epoch [67/120    avg_loss:0.237, val_acc:0.901]
Epoch [68/120    avg_loss:0.212, val_acc:0.917]
Epoch [69/120    avg_loss:0.202, val_acc:0.917]
Epoch [70/120    avg_loss:0.201, val_acc:0.917]
Epoch [71/120    avg_loss:0.237, val_acc:0.918]
Epoch [72/120    avg_loss:0.198, val_acc:0.917]
Epoch [73/120    avg_loss:0.195, val_acc:0.926]
Epoch [74/120    avg_loss:0.248, val_acc:0.910]
Epoch [75/120    avg_loss:0.217, val_acc:0.908]
Epoch [76/120    avg_loss:0.194, val_acc:0.907]
Epoch [77/120    avg_loss:0.186, val_acc:0.935]
Epoch [78/120    avg_loss:0.176, val_acc:0.925]
Epoch [79/120    avg_loss:0.198, val_acc:0.895]
Epoch [80/120    avg_loss:0.173, val_acc:0.930]
Epoch [81/120    avg_loss:0.166, val_acc:0.925]
Epoch [82/120    avg_loss:0.187, val_acc:0.911]
Epoch [83/120    avg_loss:0.147, val_acc:0.932]
Epoch [84/120    avg_loss:0.136, val_acc:0.935]
Epoch [85/120    avg_loss:0.180, val_acc:0.923]
Epoch [86/120    avg_loss:0.149, val_acc:0.922]
Epoch [87/120    avg_loss:0.156, val_acc:0.924]
Epoch [88/120    avg_loss:0.141, val_acc:0.947]
Epoch [89/120    avg_loss:0.130, val_acc:0.936]
Epoch [90/120    avg_loss:0.119, val_acc:0.942]
Epoch [91/120    avg_loss:0.111, val_acc:0.949]
Epoch [92/120    avg_loss:0.099, val_acc:0.953]
Epoch [93/120    avg_loss:0.082, val_acc:0.949]
Epoch [94/120    avg_loss:0.087, val_acc:0.960]
Epoch [95/120    avg_loss:0.090, val_acc:0.952]
Epoch [96/120    avg_loss:0.077, val_acc:0.956]
Epoch [97/120    avg_loss:0.074, val_acc:0.953]
Epoch [98/120    avg_loss:0.073, val_acc:0.961]
Epoch [99/120    avg_loss:0.066, val_acc:0.957]
Epoch [100/120    avg_loss:0.068, val_acc:0.950]
Epoch [101/120    avg_loss:0.066, val_acc:0.947]
Epoch [102/120    avg_loss:0.056, val_acc:0.961]
Epoch [103/120    avg_loss:0.060, val_acc:0.961]
Epoch [104/120    avg_loss:0.072, val_acc:0.955]
Epoch [105/120    avg_loss:0.062, val_acc:0.948]
Epoch [106/120    avg_loss:0.056, val_acc:0.961]
Epoch [107/120    avg_loss:0.049, val_acc:0.961]
Epoch [108/120    avg_loss:0.057, val_acc:0.955]
Epoch [109/120    avg_loss:0.051, val_acc:0.965]
Epoch [110/120    avg_loss:0.059, val_acc:0.964]
Epoch [111/120    avg_loss:0.047, val_acc:0.967]
Epoch [112/120    avg_loss:0.140, val_acc:0.880]
Epoch [113/120    avg_loss:0.220, val_acc:0.900]
Epoch [114/120    avg_loss:0.123, val_acc:0.948]
Epoch [115/120    avg_loss:0.131, val_acc:0.934]
Epoch [116/120    avg_loss:0.111, val_acc:0.934]
Epoch [117/120    avg_loss:0.106, val_acc:0.939]
Epoch [118/120    avg_loss:0.115, val_acc:0.951]
Epoch [119/120    avg_loss:0.083, val_acc:0.958]
Epoch [120/120    avg_loss:0.084, val_acc:0.947]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    2    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1215    7    0    0    7    0    0    0    5   49    0    0
     2    0    0]
 [   0    0    1  716    2    4    0    0    0    6    0    0   17    1
     0    0    0]
 [   0    0    0    5  208    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  396    0    6    0    5    0    0    0    0
    28    0    0]
 [   0    0    0    0    1    0  652    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    7    0    0    1    0    0    9    0    0    1    0
     0    0    0]
 [   0    0   56   89    0   11    1    0    0    0  684   22   10    0
     2    0    0]
 [   0    0   70    0    0    1    2    0    0    0    6 2112   10    6
     3    0    0]
 [   0    0    7   10   10    3    0    0    0    0   11   11  465    0
     0    0   17]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    4    0    0
  1132    0    0]
 [   0    0    1    0    0    0   26    0    0    0    0    0    0    0
   109  211    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
92.76964769647697

F1 scores:
[       nan 0.96202532 0.92150171 0.90575585 0.95852535 0.93176471
 0.96879643 0.89285714 0.99883856 0.47368421 0.86363636 0.95738894
 0.89337176 0.98143236 0.93747412 0.7562724  0.8839779 ]

Kappa:
0.9174573951940024
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4547e8af28>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.847, val_acc:0.023]
Epoch [2/120    avg_loss:2.810, val_acc:0.092]
Epoch [3/120    avg_loss:2.778, val_acc:0.158]
Epoch [4/120    avg_loss:2.746, val_acc:0.216]
Epoch [5/120    avg_loss:2.708, val_acc:0.305]
Epoch [6/120    avg_loss:2.650, val_acc:0.433]
Epoch [7/120    avg_loss:2.607, val_acc:0.455]
Epoch [8/120    avg_loss:2.542, val_acc:0.495]
Epoch [9/120    avg_loss:2.517, val_acc:0.491]
Epoch [10/120    avg_loss:2.460, val_acc:0.502]
Epoch [11/120    avg_loss:2.434, val_acc:0.549]
Epoch [12/120    avg_loss:2.372, val_acc:0.553]
Epoch [13/120    avg_loss:2.377, val_acc:0.548]
Epoch [14/120    avg_loss:2.291, val_acc:0.540]
Epoch [15/120    avg_loss:2.246, val_acc:0.561]
Epoch [16/120    avg_loss:2.233, val_acc:0.561]
Epoch [17/120    avg_loss:2.157, val_acc:0.580]
Epoch [18/120    avg_loss:2.055, val_acc:0.588]
Epoch [19/120    avg_loss:1.999, val_acc:0.608]
Epoch [20/120    avg_loss:1.925, val_acc:0.590]
Epoch [21/120    avg_loss:1.864, val_acc:0.590]
Epoch [22/120    avg_loss:1.718, val_acc:0.589]
Epoch [23/120    avg_loss:1.605, val_acc:0.620]
Epoch [24/120    avg_loss:1.544, val_acc:0.615]
Epoch [25/120    avg_loss:1.481, val_acc:0.639]
Epoch [26/120    avg_loss:1.462, val_acc:0.635]
Epoch [27/120    avg_loss:1.368, val_acc:0.644]
Epoch [28/120    avg_loss:1.398, val_acc:0.670]
Epoch [29/120    avg_loss:1.340, val_acc:0.667]
Epoch [30/120    avg_loss:1.221, val_acc:0.691]
Epoch [31/120    avg_loss:1.085, val_acc:0.702]
Epoch [32/120    avg_loss:1.060, val_acc:0.703]
Epoch [33/120    avg_loss:1.010, val_acc:0.720]
Epoch [34/120    avg_loss:0.942, val_acc:0.740]
Epoch [35/120    avg_loss:0.896, val_acc:0.760]
Epoch [36/120    avg_loss:0.862, val_acc:0.775]
Epoch [37/120    avg_loss:0.781, val_acc:0.774]
Epoch [38/120    avg_loss:0.788, val_acc:0.747]
Epoch [39/120    avg_loss:0.739, val_acc:0.780]
Epoch [40/120    avg_loss:0.685, val_acc:0.811]
Epoch [41/120    avg_loss:0.614, val_acc:0.834]
Epoch [42/120    avg_loss:0.601, val_acc:0.814]
Epoch [43/120    avg_loss:0.556, val_acc:0.832]
Epoch [44/120    avg_loss:0.538, val_acc:0.834]
Epoch [45/120    avg_loss:0.471, val_acc:0.847]
Epoch [46/120    avg_loss:0.510, val_acc:0.863]
Epoch [47/120    avg_loss:0.439, val_acc:0.841]
Epoch [48/120    avg_loss:0.425, val_acc:0.881]
Epoch [49/120    avg_loss:0.458, val_acc:0.855]
Epoch [50/120    avg_loss:0.427, val_acc:0.874]
Epoch [51/120    avg_loss:0.363, val_acc:0.892]
Epoch [52/120    avg_loss:0.332, val_acc:0.886]
Epoch [53/120    avg_loss:0.337, val_acc:0.891]
Epoch [54/120    avg_loss:0.315, val_acc:0.894]
Epoch [55/120    avg_loss:0.338, val_acc:0.887]
Epoch [56/120    avg_loss:0.297, val_acc:0.887]
Epoch [57/120    avg_loss:0.299, val_acc:0.898]
Epoch [58/120    avg_loss:0.285, val_acc:0.883]
Epoch [59/120    avg_loss:0.252, val_acc:0.903]
Epoch [60/120    avg_loss:0.251, val_acc:0.900]
Epoch [61/120    avg_loss:0.244, val_acc:0.905]
Epoch [62/120    avg_loss:0.208, val_acc:0.923]
Epoch [63/120    avg_loss:0.205, val_acc:0.915]
Epoch [64/120    avg_loss:0.184, val_acc:0.924]
Epoch [65/120    avg_loss:0.186, val_acc:0.923]
Epoch [66/120    avg_loss:0.169, val_acc:0.928]
Epoch [67/120    avg_loss:0.151, val_acc:0.939]
Epoch [68/120    avg_loss:0.146, val_acc:0.932]
Epoch [69/120    avg_loss:0.155, val_acc:0.935]
Epoch [70/120    avg_loss:0.151, val_acc:0.932]
Epoch [71/120    avg_loss:0.158, val_acc:0.931]
Epoch [72/120    avg_loss:0.130, val_acc:0.944]
Epoch [73/120    avg_loss:0.139, val_acc:0.927]
Epoch [74/120    avg_loss:0.142, val_acc:0.933]
Epoch [75/120    avg_loss:0.128, val_acc:0.941]
Epoch [76/120    avg_loss:0.114, val_acc:0.941]
Epoch [77/120    avg_loss:0.117, val_acc:0.943]
Epoch [78/120    avg_loss:0.128, val_acc:0.944]
Epoch [79/120    avg_loss:0.124, val_acc:0.941]
Epoch [80/120    avg_loss:0.094, val_acc:0.947]
Epoch [81/120    avg_loss:0.094, val_acc:0.948]
Epoch [82/120    avg_loss:0.088, val_acc:0.955]
Epoch [83/120    avg_loss:0.075, val_acc:0.947]
Epoch [84/120    avg_loss:0.081, val_acc:0.948]
Epoch [85/120    avg_loss:0.088, val_acc:0.955]
Epoch [86/120    avg_loss:0.080, val_acc:0.955]
Epoch [87/120    avg_loss:0.081, val_acc:0.943]
Epoch [88/120    avg_loss:0.084, val_acc:0.941]
Epoch [89/120    avg_loss:0.116, val_acc:0.924]
Epoch [90/120    avg_loss:0.108, val_acc:0.947]
Epoch [91/120    avg_loss:0.089, val_acc:0.953]
Epoch [92/120    avg_loss:0.138, val_acc:0.924]
Epoch [93/120    avg_loss:0.139, val_acc:0.949]
Epoch [94/120    avg_loss:0.144, val_acc:0.947]
Epoch [95/120    avg_loss:0.121, val_acc:0.938]
Epoch [96/120    avg_loss:0.073, val_acc:0.953]
Epoch [97/120    avg_loss:0.071, val_acc:0.949]
Epoch [98/120    avg_loss:0.081, val_acc:0.951]
Epoch [99/120    avg_loss:0.067, val_acc:0.963]
Epoch [100/120    avg_loss:0.059, val_acc:0.958]
Epoch [101/120    avg_loss:0.046, val_acc:0.968]
Epoch [102/120    avg_loss:0.046, val_acc:0.959]
Epoch [103/120    avg_loss:0.042, val_acc:0.970]
Epoch [104/120    avg_loss:0.041, val_acc:0.970]
Epoch [105/120    avg_loss:0.045, val_acc:0.966]
Epoch [106/120    avg_loss:0.039, val_acc:0.975]
Epoch [107/120    avg_loss:0.035, val_acc:0.969]
Epoch [108/120    avg_loss:0.037, val_acc:0.968]
Epoch [109/120    avg_loss:0.031, val_acc:0.972]
Epoch [110/120    avg_loss:0.036, val_acc:0.966]
Epoch [111/120    avg_loss:0.031, val_acc:0.972]
Epoch [112/120    avg_loss:0.034, val_acc:0.969]
Epoch [113/120    avg_loss:0.034, val_acc:0.965]
Epoch [114/120    avg_loss:0.033, val_acc:0.973]
Epoch [115/120    avg_loss:0.028, val_acc:0.974]
Epoch [116/120    avg_loss:0.031, val_acc:0.972]
Epoch [117/120    avg_loss:0.033, val_acc:0.966]
Epoch [118/120    avg_loss:0.032, val_acc:0.963]
Epoch [119/120    avg_loss:0.034, val_acc:0.965]
Epoch [120/120    avg_loss:0.031, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    0    0    0    0    0    0    0    0    6    0    0    0
     0    0    0]
 [   0    0 1233    3    0    0    4    0    0    0   10   32    3    0
     0    0    0]
 [   0    0   11  708    1   16    0    0    0    4    0    0    5    2
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    2    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  648    0    0    0    0    4    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    7    0    0   10    0    0    1    0
     0    0    0]
 [   0    0   27   79    0    3    0    0    0    0  763    1    0    0
     0    2    0]
 [   0    0   14    0    0    2   20    0    2    0    4 2156    2    4
     6    0    0]
 [   0    0    0   32   15    6    0    0    0    0   15    9  446    0
     0    0   11]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    2    0    0
  1135    0    0]
 [   0    0    0    0    0    0   22    0    0    0    0    0    0    0
    90  235    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.66666666666667

F1 scores:
[       nan 0.92105263 0.95953307 0.90248566 0.96145125 0.96179775
 0.95434462 0.98039216 0.99767981 0.58823529 0.91104478 0.97689171
 0.89919355 0.98404255 0.95418243 0.80479452 0.93854749]

Kappa:
0.9391487999474867
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc5740edf28>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.843, val_acc:0.158]
Epoch [2/120    avg_loss:2.810, val_acc:0.188]
Epoch [3/120    avg_loss:2.778, val_acc:0.201]
Epoch [4/120    avg_loss:2.752, val_acc:0.208]
Epoch [5/120    avg_loss:2.720, val_acc:0.208]
Epoch [6/120    avg_loss:2.685, val_acc:0.255]
Epoch [7/120    avg_loss:2.651, val_acc:0.283]
Epoch [8/120    avg_loss:2.615, val_acc:0.367]
Epoch [9/120    avg_loss:2.557, val_acc:0.392]
Epoch [10/120    avg_loss:2.526, val_acc:0.424]
Epoch [11/120    avg_loss:2.482, val_acc:0.465]
Epoch [12/120    avg_loss:2.394, val_acc:0.473]
Epoch [13/120    avg_loss:2.398, val_acc:0.528]
Epoch [14/120    avg_loss:2.361, val_acc:0.523]
Epoch [15/120    avg_loss:2.324, val_acc:0.541]
Epoch [16/120    avg_loss:2.260, val_acc:0.556]
Epoch [17/120    avg_loss:2.207, val_acc:0.550]
Epoch [18/120    avg_loss:2.186, val_acc:0.589]
Epoch [19/120    avg_loss:2.173, val_acc:0.577]
Epoch [20/120    avg_loss:2.092, val_acc:0.592]
Epoch [21/120    avg_loss:2.038, val_acc:0.608]
Epoch [22/120    avg_loss:1.992, val_acc:0.599]
Epoch [23/120    avg_loss:1.939, val_acc:0.609]
Epoch [24/120    avg_loss:1.884, val_acc:0.634]
Epoch [25/120    avg_loss:1.849, val_acc:0.624]
Epoch [26/120    avg_loss:1.744, val_acc:0.634]
Epoch [27/120    avg_loss:1.652, val_acc:0.624]
Epoch [28/120    avg_loss:1.518, val_acc:0.657]
Epoch [29/120    avg_loss:1.457, val_acc:0.701]
Epoch [30/120    avg_loss:1.429, val_acc:0.683]
Epoch [31/120    avg_loss:1.350, val_acc:0.719]
Epoch [32/120    avg_loss:1.252, val_acc:0.731]
Epoch [33/120    avg_loss:1.141, val_acc:0.733]
Epoch [34/120    avg_loss:1.107, val_acc:0.753]
Epoch [35/120    avg_loss:1.044, val_acc:0.751]
Epoch [36/120    avg_loss:0.941, val_acc:0.770]
Epoch [37/120    avg_loss:0.898, val_acc:0.774]
Epoch [38/120    avg_loss:0.908, val_acc:0.762]
Epoch [39/120    avg_loss:0.911, val_acc:0.759]
Epoch [40/120    avg_loss:0.825, val_acc:0.773]
Epoch [41/120    avg_loss:0.716, val_acc:0.805]
Epoch [42/120    avg_loss:0.683, val_acc:0.827]
Epoch [43/120    avg_loss:0.616, val_acc:0.828]
Epoch [44/120    avg_loss:0.545, val_acc:0.833]
Epoch [45/120    avg_loss:0.555, val_acc:0.825]
Epoch [46/120    avg_loss:0.567, val_acc:0.857]
Epoch [47/120    avg_loss:0.525, val_acc:0.857]
Epoch [48/120    avg_loss:0.492, val_acc:0.858]
Epoch [49/120    avg_loss:0.476, val_acc:0.883]
Epoch [50/120    avg_loss:0.411, val_acc:0.899]
Epoch [51/120    avg_loss:0.431, val_acc:0.868]
Epoch [52/120    avg_loss:0.425, val_acc:0.890]
Epoch [53/120    avg_loss:0.344, val_acc:0.893]
Epoch [54/120    avg_loss:0.323, val_acc:0.908]
Epoch [55/120    avg_loss:0.307, val_acc:0.906]
Epoch [56/120    avg_loss:0.282, val_acc:0.915]
Epoch [57/120    avg_loss:0.271, val_acc:0.923]
Epoch [58/120    avg_loss:0.245, val_acc:0.938]
Epoch [59/120    avg_loss:0.244, val_acc:0.923]
Epoch [60/120    avg_loss:0.241, val_acc:0.925]
Epoch [61/120    avg_loss:0.206, val_acc:0.932]
Epoch [62/120    avg_loss:0.205, val_acc:0.927]
Epoch [63/120    avg_loss:0.246, val_acc:0.932]
Epoch [64/120    avg_loss:0.193, val_acc:0.936]
Epoch [65/120    avg_loss:0.194, val_acc:0.936]
Epoch [66/120    avg_loss:0.177, val_acc:0.951]
Epoch [67/120    avg_loss:0.151, val_acc:0.959]
Epoch [68/120    avg_loss:0.149, val_acc:0.947]
Epoch [69/120    avg_loss:0.168, val_acc:0.945]
Epoch [70/120    avg_loss:0.153, val_acc:0.944]
Epoch [71/120    avg_loss:0.155, val_acc:0.951]
Epoch [72/120    avg_loss:0.159, val_acc:0.945]
Epoch [73/120    avg_loss:0.162, val_acc:0.950]
Epoch [74/120    avg_loss:0.149, val_acc:0.941]
Epoch [75/120    avg_loss:0.120, val_acc:0.949]
Epoch [76/120    avg_loss:0.117, val_acc:0.959]
Epoch [77/120    avg_loss:0.135, val_acc:0.943]
Epoch [78/120    avg_loss:0.127, val_acc:0.951]
Epoch [79/120    avg_loss:0.119, val_acc:0.949]
Epoch [80/120    avg_loss:0.098, val_acc:0.950]
Epoch [81/120    avg_loss:0.086, val_acc:0.952]
Epoch [82/120    avg_loss:0.089, val_acc:0.965]
Epoch [83/120    avg_loss:0.086, val_acc:0.955]
Epoch [84/120    avg_loss:0.088, val_acc:0.966]
Epoch [85/120    avg_loss:0.082, val_acc:0.964]
Epoch [86/120    avg_loss:0.068, val_acc:0.967]
Epoch [87/120    avg_loss:0.072, val_acc:0.959]
Epoch [88/120    avg_loss:0.091, val_acc:0.957]
Epoch [89/120    avg_loss:0.079, val_acc:0.968]
Epoch [90/120    avg_loss:0.069, val_acc:0.958]
Epoch [91/120    avg_loss:0.106, val_acc:0.949]
Epoch [92/120    avg_loss:0.104, val_acc:0.956]
Epoch [93/120    avg_loss:0.088, val_acc:0.972]
Epoch [94/120    avg_loss:0.086, val_acc:0.956]
Epoch [95/120    avg_loss:0.058, val_acc:0.972]
Epoch [96/120    avg_loss:0.056, val_acc:0.961]
Epoch [97/120    avg_loss:0.048, val_acc:0.970]
Epoch [98/120    avg_loss:0.056, val_acc:0.972]
Epoch [99/120    avg_loss:0.048, val_acc:0.968]
Epoch [100/120    avg_loss:0.049, val_acc:0.974]
Epoch [101/120    avg_loss:0.051, val_acc:0.975]
Epoch [102/120    avg_loss:0.048, val_acc:0.976]
Epoch [103/120    avg_loss:0.041, val_acc:0.961]
Epoch [104/120    avg_loss:0.051, val_acc:0.967]
Epoch [105/120    avg_loss:0.051, val_acc:0.969]
Epoch [106/120    avg_loss:0.040, val_acc:0.980]
Epoch [107/120    avg_loss:0.037, val_acc:0.975]
Epoch [108/120    avg_loss:0.037, val_acc:0.975]
Epoch [109/120    avg_loss:0.036, val_acc:0.978]
Epoch [110/120    avg_loss:0.042, val_acc:0.977]
Epoch [111/120    avg_loss:0.040, val_acc:0.976]
Epoch [112/120    avg_loss:0.045, val_acc:0.975]
Epoch [113/120    avg_loss:0.044, val_acc:0.969]
Epoch [114/120    avg_loss:0.036, val_acc:0.970]
Epoch [115/120    avg_loss:0.034, val_acc:0.975]
Epoch [116/120    avg_loss:0.035, val_acc:0.973]
Epoch [117/120    avg_loss:0.033, val_acc:0.975]
Epoch [118/120    avg_loss:0.039, val_acc:0.976]
Epoch [119/120    avg_loss:0.036, val_acc:0.975]
Epoch [120/120    avg_loss:0.029, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    3    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1239    2    0    0    2    0    0    0    2   26    2    0
     0   12    0]
 [   0    0    2  693    1   11    0    0    0   15    0    0   25    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  430    0    2    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    1    0    0   15    0    0    1    0
     0    0    0]
 [   0    0   50   90    0    4    0    0    0    0  712    7    0    0
     3    9    0]
 [   0    0    6    0    0    1    5    0    0    0    0 2186    7    1
     4    0    0]
 [   0    0    0   13    3   10    0    0    0    0    9    0  492    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    2    0    0
  1137    0    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
   110  223    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.92682926829268

F1 scores:
[       nan 0.96202532 0.95860735 0.89650712 0.98834499 0.96520763
 0.98353293 0.96153846 1.         0.58823529 0.89111389 0.98668472
 0.92568203 0.99730458 0.95027163 0.75465313 0.95402299]

Kappa:
0.9421091940503757
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa37bbcdf98>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.831, val_acc:0.334]
Epoch [2/120    avg_loss:2.804, val_acc:0.267]
Epoch [3/120    avg_loss:2.774, val_acc:0.243]
Epoch [4/120    avg_loss:2.744, val_acc:0.311]
Epoch [5/120    avg_loss:2.717, val_acc:0.339]
Epoch [6/120    avg_loss:2.657, val_acc:0.436]
Epoch [7/120    avg_loss:2.626, val_acc:0.428]
Epoch [8/120    avg_loss:2.572, val_acc:0.475]
Epoch [9/120    avg_loss:2.511, val_acc:0.501]
Epoch [10/120    avg_loss:2.456, val_acc:0.476]
Epoch [11/120    avg_loss:2.430, val_acc:0.511]
Epoch [12/120    avg_loss:2.395, val_acc:0.506]
Epoch [13/120    avg_loss:2.388, val_acc:0.542]
Epoch [14/120    avg_loss:2.321, val_acc:0.550]
Epoch [15/120    avg_loss:2.262, val_acc:0.565]
Epoch [16/120    avg_loss:2.239, val_acc:0.600]
Epoch [17/120    avg_loss:2.237, val_acc:0.623]
Epoch [18/120    avg_loss:2.181, val_acc:0.622]
Epoch [19/120    avg_loss:2.136, val_acc:0.626]
Epoch [20/120    avg_loss:2.117, val_acc:0.637]
Epoch [21/120    avg_loss:2.049, val_acc:0.655]
Epoch [22/120    avg_loss:1.994, val_acc:0.653]
Epoch [23/120    avg_loss:1.906, val_acc:0.665]
Epoch [24/120    avg_loss:1.852, val_acc:0.678]
Epoch [25/120    avg_loss:1.803, val_acc:0.690]
Epoch [26/120    avg_loss:1.645, val_acc:0.703]
Epoch [27/120    avg_loss:1.627, val_acc:0.686]
Epoch [28/120    avg_loss:1.521, val_acc:0.715]
Epoch [29/120    avg_loss:1.453, val_acc:0.714]
Epoch [30/120    avg_loss:1.359, val_acc:0.723]
Epoch [31/120    avg_loss:1.309, val_acc:0.727]
Epoch [32/120    avg_loss:1.257, val_acc:0.694]
Epoch [33/120    avg_loss:1.167, val_acc:0.711]
Epoch [34/120    avg_loss:1.081, val_acc:0.760]
Epoch [35/120    avg_loss:1.050, val_acc:0.792]
Epoch [36/120    avg_loss:1.009, val_acc:0.780]
Epoch [37/120    avg_loss:0.916, val_acc:0.751]
Epoch [38/120    avg_loss:0.873, val_acc:0.800]
Epoch [39/120    avg_loss:0.927, val_acc:0.761]
Epoch [40/120    avg_loss:0.894, val_acc:0.800]
Epoch [41/120    avg_loss:0.770, val_acc:0.838]
Epoch [42/120    avg_loss:0.723, val_acc:0.824]
Epoch [43/120    avg_loss:0.661, val_acc:0.852]
Epoch [44/120    avg_loss:0.580, val_acc:0.857]
Epoch [45/120    avg_loss:0.556, val_acc:0.883]
Epoch [46/120    avg_loss:0.486, val_acc:0.865]
Epoch [47/120    avg_loss:0.481, val_acc:0.907]
Epoch [48/120    avg_loss:0.460, val_acc:0.891]
Epoch [49/120    avg_loss:0.518, val_acc:0.881]
Epoch [50/120    avg_loss:0.472, val_acc:0.878]
Epoch [51/120    avg_loss:0.392, val_acc:0.926]
Epoch [52/120    avg_loss:0.343, val_acc:0.931]
Epoch [53/120    avg_loss:0.316, val_acc:0.923]
Epoch [54/120    avg_loss:0.287, val_acc:0.938]
Epoch [55/120    avg_loss:0.274, val_acc:0.943]
Epoch [56/120    avg_loss:0.267, val_acc:0.952]
Epoch [57/120    avg_loss:0.255, val_acc:0.952]
Epoch [58/120    avg_loss:0.224, val_acc:0.956]
Epoch [59/120    avg_loss:0.220, val_acc:0.947]
Epoch [60/120    avg_loss:0.233, val_acc:0.943]
Epoch [61/120    avg_loss:0.188, val_acc:0.958]
Epoch [62/120    avg_loss:0.181, val_acc:0.950]
Epoch [63/120    avg_loss:0.172, val_acc:0.944]
Epoch [64/120    avg_loss:0.177, val_acc:0.957]
Epoch [65/120    avg_loss:0.175, val_acc:0.945]
Epoch [66/120    avg_loss:0.178, val_acc:0.950]
Epoch [67/120    avg_loss:0.167, val_acc:0.961]
Epoch [68/120    avg_loss:0.136, val_acc:0.961]
Epoch [69/120    avg_loss:0.139, val_acc:0.965]
Epoch [70/120    avg_loss:0.229, val_acc:0.955]
Epoch [71/120    avg_loss:0.150, val_acc:0.952]
Epoch [72/120    avg_loss:0.131, val_acc:0.959]
Epoch [73/120    avg_loss:0.105, val_acc:0.970]
Epoch [74/120    avg_loss:0.096, val_acc:0.967]
Epoch [75/120    avg_loss:0.092, val_acc:0.965]
Epoch [76/120    avg_loss:0.096, val_acc:0.970]
Epoch [77/120    avg_loss:0.082, val_acc:0.967]
Epoch [78/120    avg_loss:0.082, val_acc:0.964]
Epoch [79/120    avg_loss:0.074, val_acc:0.965]
Epoch [80/120    avg_loss:0.100, val_acc:0.951]
Epoch [81/120    avg_loss:0.121, val_acc:0.959]
Epoch [82/120    avg_loss:0.099, val_acc:0.974]
Epoch [83/120    avg_loss:0.072, val_acc:0.978]
Epoch [84/120    avg_loss:0.063, val_acc:0.972]
Epoch [85/120    avg_loss:0.054, val_acc:0.975]
Epoch [86/120    avg_loss:0.061, val_acc:0.969]
Epoch [87/120    avg_loss:0.055, val_acc:0.975]
Epoch [88/120    avg_loss:0.058, val_acc:0.973]
Epoch [89/120    avg_loss:0.055, val_acc:0.973]
Epoch [90/120    avg_loss:0.050, val_acc:0.967]
Epoch [91/120    avg_loss:0.048, val_acc:0.977]
Epoch [92/120    avg_loss:0.059, val_acc:0.976]
Epoch [93/120    avg_loss:0.052, val_acc:0.964]
Epoch [94/120    avg_loss:0.047, val_acc:0.973]
Epoch [95/120    avg_loss:0.050, val_acc:0.977]
Epoch [96/120    avg_loss:0.047, val_acc:0.968]
Epoch [97/120    avg_loss:0.039, val_acc:0.980]
Epoch [98/120    avg_loss:0.033, val_acc:0.977]
Epoch [99/120    avg_loss:0.035, val_acc:0.978]
Epoch [100/120    avg_loss:0.033, val_acc:0.977]
Epoch [101/120    avg_loss:0.030, val_acc:0.980]
Epoch [102/120    avg_loss:0.034, val_acc:0.980]
Epoch [103/120    avg_loss:0.033, val_acc:0.980]
Epoch [104/120    avg_loss:0.031, val_acc:0.980]
Epoch [105/120    avg_loss:0.029, val_acc:0.981]
Epoch [106/120    avg_loss:0.034, val_acc:0.980]
Epoch [107/120    avg_loss:0.029, val_acc:0.980]
Epoch [108/120    avg_loss:0.029, val_acc:0.980]
Epoch [109/120    avg_loss:0.030, val_acc:0.980]
Epoch [110/120    avg_loss:0.030, val_acc:0.980]
Epoch [111/120    avg_loss:0.032, val_acc:0.980]
Epoch [112/120    avg_loss:0.028, val_acc:0.980]
Epoch [113/120    avg_loss:0.030, val_acc:0.981]
Epoch [114/120    avg_loss:0.031, val_acc:0.980]
Epoch [115/120    avg_loss:0.028, val_acc:0.980]
Epoch [116/120    avg_loss:0.030, val_acc:0.980]
Epoch [117/120    avg_loss:0.029, val_acc:0.980]
Epoch [118/120    avg_loss:0.028, val_acc:0.980]
Epoch [119/120    avg_loss:0.029, val_acc:0.981]
Epoch [120/120    avg_loss:0.027, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1248    4    0    0    1    0    0    0    0   32    0    0
     0    0    0]
 [   0    0    4  713    1   21    0    0    0    4    0    0    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   14    0    0    1    0
     0    0    0]
 [   0    0   20   90    0    0    0    0    0    0  749   13    3    0
     0    0    0]
 [   0    0   19    0    0    0    2    0    0    0    2 2179    7    0
     1    0    0]
 [   0    0    0    4    5    5    0    0    0    0   12    5  498    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    1    0    0
  1136    0    0]
 [   0    0    0    0    0    1   15    0    0    7    0    0    0    0
    98  226    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.6639566395664

F1 scores:
[       nan 0.975      0.9689441  0.91527599 0.98611111 0.96651786
 0.98274569 1.         1.         0.60869565 0.91285801 0.98131052
 0.94947569 1.         0.95703454 0.78883072 0.95906433]

Kappa:
0.950506032892081
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faca8a6def0>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.818, val_acc:0.002]
Epoch [2/120    avg_loss:2.794, val_acc:0.062]
Epoch [3/120    avg_loss:2.776, val_acc:0.094]
Epoch [4/120    avg_loss:2.751, val_acc:0.185]
Epoch [5/120    avg_loss:2.726, val_acc:0.260]
Epoch [6/120    avg_loss:2.698, val_acc:0.315]
Epoch [7/120    avg_loss:2.653, val_acc:0.338]
Epoch [8/120    avg_loss:2.616, val_acc:0.412]
Epoch [9/120    avg_loss:2.558, val_acc:0.456]
Epoch [10/120    avg_loss:2.505, val_acc:0.448]
Epoch [11/120    avg_loss:2.469, val_acc:0.458]
Epoch [12/120    avg_loss:2.438, val_acc:0.474]
Epoch [13/120    avg_loss:2.389, val_acc:0.458]
Epoch [14/120    avg_loss:2.357, val_acc:0.495]
Epoch [15/120    avg_loss:2.334, val_acc:0.516]
Epoch [16/120    avg_loss:2.284, val_acc:0.514]
Epoch [17/120    avg_loss:2.267, val_acc:0.528]
Epoch [18/120    avg_loss:2.167, val_acc:0.527]
Epoch [19/120    avg_loss:2.163, val_acc:0.572]
Epoch [20/120    avg_loss:2.079, val_acc:0.574]
Epoch [21/120    avg_loss:2.002, val_acc:0.560]
Epoch [22/120    avg_loss:1.983, val_acc:0.598]
Epoch [23/120    avg_loss:1.927, val_acc:0.598]
Epoch [24/120    avg_loss:1.819, val_acc:0.593]
Epoch [25/120    avg_loss:1.768, val_acc:0.617]
Epoch [26/120    avg_loss:1.805, val_acc:0.602]
Epoch [27/120    avg_loss:1.668, val_acc:0.645]
Epoch [28/120    avg_loss:1.591, val_acc:0.615]
Epoch [29/120    avg_loss:1.464, val_acc:0.640]
Epoch [30/120    avg_loss:1.453, val_acc:0.665]
Epoch [31/120    avg_loss:1.381, val_acc:0.673]
Epoch [32/120    avg_loss:1.320, val_acc:0.667]
Epoch [33/120    avg_loss:1.274, val_acc:0.660]
Epoch [34/120    avg_loss:1.173, val_acc:0.668]
Epoch [35/120    avg_loss:1.098, val_acc:0.700]
Epoch [36/120    avg_loss:1.051, val_acc:0.705]
Epoch [37/120    avg_loss:1.016, val_acc:0.722]
Epoch [38/120    avg_loss:0.954, val_acc:0.738]
Epoch [39/120    avg_loss:0.910, val_acc:0.747]
Epoch [40/120    avg_loss:0.856, val_acc:0.751]
Epoch [41/120    avg_loss:0.869, val_acc:0.743]
Epoch [42/120    avg_loss:0.836, val_acc:0.758]
Epoch [43/120    avg_loss:0.750, val_acc:0.770]
Epoch [44/120    avg_loss:0.694, val_acc:0.799]
Epoch [45/120    avg_loss:0.646, val_acc:0.787]
Epoch [46/120    avg_loss:0.589, val_acc:0.818]
Epoch [47/120    avg_loss:0.585, val_acc:0.825]
Epoch [48/120    avg_loss:0.644, val_acc:0.793]
Epoch [49/120    avg_loss:0.630, val_acc:0.801]
Epoch [50/120    avg_loss:0.566, val_acc:0.833]
Epoch [51/120    avg_loss:0.546, val_acc:0.816]
Epoch [52/120    avg_loss:0.689, val_acc:0.832]
Epoch [53/120    avg_loss:0.558, val_acc:0.831]
Epoch [54/120    avg_loss:0.540, val_acc:0.807]
Epoch [55/120    avg_loss:0.481, val_acc:0.856]
Epoch [56/120    avg_loss:0.439, val_acc:0.830]
Epoch [57/120    avg_loss:0.441, val_acc:0.867]
Epoch [58/120    avg_loss:0.363, val_acc:0.865]
Epoch [59/120    avg_loss:0.354, val_acc:0.898]
Epoch [60/120    avg_loss:0.326, val_acc:0.886]
Epoch [61/120    avg_loss:0.312, val_acc:0.899]
Epoch [62/120    avg_loss:0.282, val_acc:0.910]
Epoch [63/120    avg_loss:0.263, val_acc:0.914]
Epoch [64/120    avg_loss:0.258, val_acc:0.922]
Epoch [65/120    avg_loss:0.241, val_acc:0.911]
Epoch [66/120    avg_loss:0.238, val_acc:0.914]
Epoch [67/120    avg_loss:0.231, val_acc:0.911]
Epoch [68/120    avg_loss:0.208, val_acc:0.933]
Epoch [69/120    avg_loss:0.189, val_acc:0.914]
Epoch [70/120    avg_loss:0.220, val_acc:0.916]
Epoch [71/120    avg_loss:0.222, val_acc:0.919]
Epoch [72/120    avg_loss:0.214, val_acc:0.914]
Epoch [73/120    avg_loss:0.174, val_acc:0.939]
Epoch [74/120    avg_loss:0.154, val_acc:0.936]
Epoch [75/120    avg_loss:0.146, val_acc:0.931]
Epoch [76/120    avg_loss:0.147, val_acc:0.935]
Epoch [77/120    avg_loss:0.133, val_acc:0.952]
Epoch [78/120    avg_loss:0.115, val_acc:0.941]
Epoch [79/120    avg_loss:0.104, val_acc:0.944]
Epoch [80/120    avg_loss:0.119, val_acc:0.920]
Epoch [81/120    avg_loss:0.155, val_acc:0.935]
Epoch [82/120    avg_loss:0.118, val_acc:0.926]
Epoch [83/120    avg_loss:0.113, val_acc:0.942]
Epoch [84/120    avg_loss:0.098, val_acc:0.938]
Epoch [85/120    avg_loss:0.134, val_acc:0.926]
Epoch [86/120    avg_loss:0.208, val_acc:0.910]
Epoch [87/120    avg_loss:0.197, val_acc:0.922]
Epoch [88/120    avg_loss:0.169, val_acc:0.930]
Epoch [89/120    avg_loss:0.132, val_acc:0.942]
Epoch [90/120    avg_loss:0.123, val_acc:0.938]
Epoch [91/120    avg_loss:0.122, val_acc:0.945]
Epoch [92/120    avg_loss:0.088, val_acc:0.948]
Epoch [93/120    avg_loss:0.085, val_acc:0.949]
Epoch [94/120    avg_loss:0.083, val_acc:0.949]
Epoch [95/120    avg_loss:0.072, val_acc:0.948]
Epoch [96/120    avg_loss:0.072, val_acc:0.948]
Epoch [97/120    avg_loss:0.073, val_acc:0.950]
Epoch [98/120    avg_loss:0.073, val_acc:0.950]
Epoch [99/120    avg_loss:0.075, val_acc:0.950]
Epoch [100/120    avg_loss:0.070, val_acc:0.951]
Epoch [101/120    avg_loss:0.071, val_acc:0.953]
Epoch [102/120    avg_loss:0.076, val_acc:0.951]
Epoch [103/120    avg_loss:0.065, val_acc:0.950]
Epoch [104/120    avg_loss:0.068, val_acc:0.951]
Epoch [105/120    avg_loss:0.069, val_acc:0.950]
Epoch [106/120    avg_loss:0.065, val_acc:0.951]
Epoch [107/120    avg_loss:0.070, val_acc:0.952]
Epoch [108/120    avg_loss:0.061, val_acc:0.952]
Epoch [109/120    avg_loss:0.066, val_acc:0.952]
Epoch [110/120    avg_loss:0.060, val_acc:0.952]
Epoch [111/120    avg_loss:0.068, val_acc:0.952]
Epoch [112/120    avg_loss:0.066, val_acc:0.952]
Epoch [113/120    avg_loss:0.060, val_acc:0.955]
Epoch [114/120    avg_loss:0.064, val_acc:0.955]
Epoch [115/120    avg_loss:0.059, val_acc:0.955]
Epoch [116/120    avg_loss:0.063, val_acc:0.955]
Epoch [117/120    avg_loss:0.061, val_acc:0.955]
Epoch [118/120    avg_loss:0.065, val_acc:0.957]
Epoch [119/120    avg_loss:0.062, val_acc:0.953]
Epoch [120/120    avg_loss:0.062, val_acc:0.956]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    1    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    2 1176    9    0    0    5    0    0    0   10   67    5    0
     8    3    0]
 [   0    0    2  654    1   20    0    0    0    7    0    0   62    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  419    0    8    0    3    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    7    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   24   87    0   10    0    0    0    0  737   12    0    0
     1    4    0]
 [   0    0   29    0    0    0   20    0    0    0    9 2123    1    3
    25    0    0]
 [   0    0    0   12   14   14    0    0    0    0    5    0  480    0
     0    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    4    1    0    0
  1129    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   133  214    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
92.92140921409214

F1 scores:
[       nan 0.91139241 0.93444577 0.8667992  0.96598639 0.92801772
 0.97597598 0.86206897 1.         0.75555556 0.89659367 0.96063348
 0.88642659 0.98930481 0.92540984 0.75352113 0.94915254]

Kappa:
0.9192649009879036
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f83ce50ff28>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.830, val_acc:0.142]
Epoch [2/120    avg_loss:2.801, val_acc:0.170]
Epoch [3/120    avg_loss:2.771, val_acc:0.230]
Epoch [4/120    avg_loss:2.750, val_acc:0.241]
Epoch [5/120    avg_loss:2.714, val_acc:0.275]
Epoch [6/120    avg_loss:2.684, val_acc:0.308]
Epoch [7/120    avg_loss:2.635, val_acc:0.345]
Epoch [8/120    avg_loss:2.597, val_acc:0.377]
Epoch [9/120    avg_loss:2.545, val_acc:0.394]
Epoch [10/120    avg_loss:2.500, val_acc:0.407]
Epoch [11/120    avg_loss:2.458, val_acc:0.392]
Epoch [12/120    avg_loss:2.405, val_acc:0.402]
Epoch [13/120    avg_loss:2.386, val_acc:0.406]
Epoch [14/120    avg_loss:2.344, val_acc:0.448]
Epoch [15/120    avg_loss:2.311, val_acc:0.466]
Epoch [16/120    avg_loss:2.242, val_acc:0.441]
Epoch [17/120    avg_loss:2.199, val_acc:0.499]
Epoch [18/120    avg_loss:2.168, val_acc:0.503]
Epoch [19/120    avg_loss:2.140, val_acc:0.515]
Epoch [20/120    avg_loss:2.100, val_acc:0.531]
Epoch [21/120    avg_loss:2.047, val_acc:0.580]
Epoch [22/120    avg_loss:2.002, val_acc:0.583]
Epoch [23/120    avg_loss:1.942, val_acc:0.598]
Epoch [24/120    avg_loss:1.913, val_acc:0.620]
Epoch [25/120    avg_loss:1.831, val_acc:0.674]
Epoch [26/120    avg_loss:1.723, val_acc:0.667]
Epoch [27/120    avg_loss:1.662, val_acc:0.697]
Epoch [28/120    avg_loss:1.575, val_acc:0.713]
Epoch [29/120    avg_loss:1.509, val_acc:0.714]
Epoch [30/120    avg_loss:1.339, val_acc:0.726]
Epoch [31/120    avg_loss:1.261, val_acc:0.762]
Epoch [32/120    avg_loss:1.208, val_acc:0.758]
Epoch [33/120    avg_loss:1.185, val_acc:0.736]
Epoch [34/120    avg_loss:1.067, val_acc:0.756]
Epoch [35/120    avg_loss:0.968, val_acc:0.761]
Epoch [36/120    avg_loss:0.940, val_acc:0.780]
Epoch [37/120    avg_loss:0.864, val_acc:0.768]
Epoch [38/120    avg_loss:0.819, val_acc:0.798]
Epoch [39/120    avg_loss:0.786, val_acc:0.803]
Epoch [40/120    avg_loss:0.726, val_acc:0.817]
Epoch [41/120    avg_loss:0.668, val_acc:0.823]
Epoch [42/120    avg_loss:0.602, val_acc:0.830]
Epoch [43/120    avg_loss:0.577, val_acc:0.852]
Epoch [44/120    avg_loss:0.554, val_acc:0.861]
Epoch [45/120    avg_loss:0.527, val_acc:0.882]
Epoch [46/120    avg_loss:0.448, val_acc:0.869]
Epoch [47/120    avg_loss:0.432, val_acc:0.876]
Epoch [48/120    avg_loss:0.375, val_acc:0.876]
Epoch [49/120    avg_loss:0.338, val_acc:0.887]
Epoch [50/120    avg_loss:0.351, val_acc:0.891]
Epoch [51/120    avg_loss:0.333, val_acc:0.889]
Epoch [52/120    avg_loss:0.280, val_acc:0.907]
Epoch [53/120    avg_loss:0.287, val_acc:0.911]
Epoch [54/120    avg_loss:0.281, val_acc:0.910]
Epoch [55/120    avg_loss:0.230, val_acc:0.933]
Epoch [56/120    avg_loss:0.271, val_acc:0.923]
Epoch [57/120    avg_loss:0.226, val_acc:0.918]
Epoch [58/120    avg_loss:0.234, val_acc:0.910]
Epoch [59/120    avg_loss:0.216, val_acc:0.931]
Epoch [60/120    avg_loss:0.179, val_acc:0.924]
Epoch [61/120    avg_loss:0.191, val_acc:0.930]
Epoch [62/120    avg_loss:0.181, val_acc:0.922]
Epoch [63/120    avg_loss:0.171, val_acc:0.925]
Epoch [64/120    avg_loss:0.144, val_acc:0.922]
Epoch [65/120    avg_loss:0.142, val_acc:0.938]
Epoch [66/120    avg_loss:0.121, val_acc:0.945]
Epoch [67/120    avg_loss:0.148, val_acc:0.948]
Epoch [68/120    avg_loss:0.116, val_acc:0.939]
Epoch [69/120    avg_loss:0.138, val_acc:0.934]
Epoch [70/120    avg_loss:0.146, val_acc:0.951]
Epoch [71/120    avg_loss:0.128, val_acc:0.934]
Epoch [72/120    avg_loss:0.124, val_acc:0.936]
Epoch [73/120    avg_loss:0.133, val_acc:0.941]
Epoch [74/120    avg_loss:0.129, val_acc:0.941]
Epoch [75/120    avg_loss:0.132, val_acc:0.949]
Epoch [76/120    avg_loss:0.114, val_acc:0.953]
Epoch [77/120    avg_loss:0.103, val_acc:0.953]
Epoch [78/120    avg_loss:0.116, val_acc:0.935]
Epoch [79/120    avg_loss:0.086, val_acc:0.956]
Epoch [80/120    avg_loss:0.079, val_acc:0.968]
Epoch [81/120    avg_loss:0.079, val_acc:0.957]
Epoch [82/120    avg_loss:0.072, val_acc:0.960]
Epoch [83/120    avg_loss:0.065, val_acc:0.967]
Epoch [84/120    avg_loss:0.074, val_acc:0.959]
Epoch [85/120    avg_loss:0.067, val_acc:0.961]
Epoch [86/120    avg_loss:0.061, val_acc:0.965]
Epoch [87/120    avg_loss:0.056, val_acc:0.970]
Epoch [88/120    avg_loss:0.057, val_acc:0.958]
Epoch [89/120    avg_loss:0.075, val_acc:0.952]
Epoch [90/120    avg_loss:0.076, val_acc:0.963]
Epoch [91/120    avg_loss:0.068, val_acc:0.960]
Epoch [92/120    avg_loss:0.083, val_acc:0.959]
Epoch [93/120    avg_loss:0.065, val_acc:0.957]
Epoch [94/120    avg_loss:0.057, val_acc:0.969]
Epoch [95/120    avg_loss:0.059, val_acc:0.967]
Epoch [96/120    avg_loss:0.052, val_acc:0.965]
Epoch [97/120    avg_loss:0.048, val_acc:0.965]
Epoch [98/120    avg_loss:0.051, val_acc:0.966]
Epoch [99/120    avg_loss:0.053, val_acc:0.975]
Epoch [100/120    avg_loss:0.046, val_acc:0.978]
Epoch [101/120    avg_loss:0.042, val_acc:0.961]
Epoch [102/120    avg_loss:0.039, val_acc:0.968]
Epoch [103/120    avg_loss:0.045, val_acc:0.974]
Epoch [104/120    avg_loss:0.064, val_acc:0.964]
Epoch [105/120    avg_loss:0.056, val_acc:0.970]
Epoch [106/120    avg_loss:0.058, val_acc:0.961]
Epoch [107/120    avg_loss:0.050, val_acc:0.973]
Epoch [108/120    avg_loss:0.040, val_acc:0.977]
Epoch [109/120    avg_loss:0.046, val_acc:0.978]
Epoch [110/120    avg_loss:0.047, val_acc:0.966]
Epoch [111/120    avg_loss:0.047, val_acc:0.969]
Epoch [112/120    avg_loss:0.037, val_acc:0.970]
Epoch [113/120    avg_loss:0.034, val_acc:0.974]
Epoch [114/120    avg_loss:0.030, val_acc:0.978]
Epoch [115/120    avg_loss:0.025, val_acc:0.978]
Epoch [116/120    avg_loss:0.032, val_acc:0.982]
Epoch [117/120    avg_loss:0.027, val_acc:0.980]
Epoch [118/120    avg_loss:0.033, val_acc:0.960]
Epoch [119/120    avg_loss:0.035, val_acc:0.981]
Epoch [120/120    avg_loss:0.032, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1226    0    5    0    4    0    0    2    8   17    3    0
     0   20    0]
 [   0    0    2  579    0    8    0    0    0   28    0    0  130    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    2    0    0    0    0
     6    0    0]
 [   0    0    2    1    1    0  643    0    0    3    0    4    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   25   90    0    0    0    0    0    0  748    7    0    0
     0    5    0]
 [   0    0    9   13    0    6    2    0    0    1   10 2157    7    4
     1    0    0]
 [   0    0    1    9    4    1    0    0    0    2   18    0  492    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    2    0    0
  1136    0    0]
 [   0    0    0    0    0    0   52    0    0    0    0    0    0    0
    98  197    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
93.18157181571816

F1 scores:
[       nan 0.94871795 0.96156863 0.8047255  0.97706422 0.97377423
 0.94698085 1.         1.         0.46575342 0.89903846 0.98112349
 0.84318766 0.98930481 0.95342006 0.69244288 0.96      ]

Kappa:
0.9222927375112078
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0909367f60>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.814, val_acc:0.163]
Epoch [2/120    avg_loss:2.782, val_acc:0.173]
Epoch [3/120    avg_loss:2.747, val_acc:0.241]
Epoch [4/120    avg_loss:2.707, val_acc:0.265]
Epoch [5/120    avg_loss:2.667, val_acc:0.301]
Epoch [6/120    avg_loss:2.641, val_acc:0.353]
Epoch [7/120    avg_loss:2.588, val_acc:0.411]
Epoch [8/120    avg_loss:2.552, val_acc:0.424]
Epoch [9/120    avg_loss:2.496, val_acc:0.457]
Epoch [10/120    avg_loss:2.435, val_acc:0.452]
Epoch [11/120    avg_loss:2.388, val_acc:0.410]
Epoch [12/120    avg_loss:2.342, val_acc:0.472]
Epoch [13/120    avg_loss:2.295, val_acc:0.502]
Epoch [14/120    avg_loss:2.242, val_acc:0.531]
Epoch [15/120    avg_loss:2.183, val_acc:0.549]
Epoch [16/120    avg_loss:2.114, val_acc:0.534]
Epoch [17/120    avg_loss:2.063, val_acc:0.534]
Epoch [18/120    avg_loss:2.001, val_acc:0.469]
Epoch [19/120    avg_loss:1.947, val_acc:0.524]
Epoch [20/120    avg_loss:1.865, val_acc:0.565]
Epoch [21/120    avg_loss:1.822, val_acc:0.589]
Epoch [22/120    avg_loss:1.775, val_acc:0.611]
Epoch [23/120    avg_loss:1.654, val_acc:0.615]
Epoch [24/120    avg_loss:1.576, val_acc:0.660]
Epoch [25/120    avg_loss:1.492, val_acc:0.625]
Epoch [26/120    avg_loss:1.459, val_acc:0.628]
Epoch [27/120    avg_loss:1.382, val_acc:0.661]
Epoch [28/120    avg_loss:1.307, val_acc:0.688]
Epoch [29/120    avg_loss:1.191, val_acc:0.723]
Epoch [30/120    avg_loss:1.136, val_acc:0.709]
Epoch [31/120    avg_loss:1.134, val_acc:0.694]
Epoch [32/120    avg_loss:1.058, val_acc:0.710]
Epoch [33/120    avg_loss:0.981, val_acc:0.745]
Epoch [34/120    avg_loss:0.952, val_acc:0.790]
Epoch [35/120    avg_loss:0.869, val_acc:0.750]
Epoch [36/120    avg_loss:0.768, val_acc:0.769]
Epoch [37/120    avg_loss:0.739, val_acc:0.784]
Epoch [38/120    avg_loss:0.718, val_acc:0.817]
Epoch [39/120    avg_loss:0.648, val_acc:0.830]
Epoch [40/120    avg_loss:0.635, val_acc:0.836]
Epoch [41/120    avg_loss:0.604, val_acc:0.787]
Epoch [42/120    avg_loss:0.565, val_acc:0.839]
Epoch [43/120    avg_loss:0.532, val_acc:0.825]
Epoch [44/120    avg_loss:0.534, val_acc:0.840]
Epoch [45/120    avg_loss:0.494, val_acc:0.874]
Epoch [46/120    avg_loss:0.468, val_acc:0.850]
Epoch [47/120    avg_loss:0.437, val_acc:0.884]
Epoch [48/120    avg_loss:0.426, val_acc:0.861]
Epoch [49/120    avg_loss:0.400, val_acc:0.885]
Epoch [50/120    avg_loss:0.347, val_acc:0.872]
Epoch [51/120    avg_loss:0.335, val_acc:0.897]
Epoch [52/120    avg_loss:0.297, val_acc:0.906]
Epoch [53/120    avg_loss:0.304, val_acc:0.892]
Epoch [54/120    avg_loss:0.261, val_acc:0.894]
Epoch [55/120    avg_loss:0.263, val_acc:0.910]
Epoch [56/120    avg_loss:0.239, val_acc:0.933]
Epoch [57/120    avg_loss:0.243, val_acc:0.911]
Epoch [58/120    avg_loss:0.249, val_acc:0.899]
Epoch [59/120    avg_loss:0.222, val_acc:0.918]
Epoch [60/120    avg_loss:0.205, val_acc:0.916]
Epoch [61/120    avg_loss:0.176, val_acc:0.926]
Epoch [62/120    avg_loss:0.187, val_acc:0.926]
Epoch [63/120    avg_loss:0.195, val_acc:0.916]
Epoch [64/120    avg_loss:0.184, val_acc:0.942]
Epoch [65/120    avg_loss:0.189, val_acc:0.923]
Epoch [66/120    avg_loss:0.184, val_acc:0.915]
Epoch [67/120    avg_loss:0.178, val_acc:0.914]
Epoch [68/120    avg_loss:0.150, val_acc:0.940]
Epoch [69/120    avg_loss:0.123, val_acc:0.936]
Epoch [70/120    avg_loss:0.120, val_acc:0.945]
Epoch [71/120    avg_loss:0.136, val_acc:0.925]
Epoch [72/120    avg_loss:0.122, val_acc:0.941]
Epoch [73/120    avg_loss:0.114, val_acc:0.944]
Epoch [74/120    avg_loss:0.140, val_acc:0.940]
Epoch [75/120    avg_loss:0.107, val_acc:0.953]
Epoch [76/120    avg_loss:0.107, val_acc:0.940]
Epoch [77/120    avg_loss:0.127, val_acc:0.936]
Epoch [78/120    avg_loss:0.138, val_acc:0.933]
Epoch [79/120    avg_loss:0.111, val_acc:0.942]
Epoch [80/120    avg_loss:0.095, val_acc:0.945]
Epoch [81/120    avg_loss:0.097, val_acc:0.952]
Epoch [82/120    avg_loss:0.119, val_acc:0.932]
Epoch [83/120    avg_loss:0.106, val_acc:0.939]
Epoch [84/120    avg_loss:0.113, val_acc:0.865]
Epoch [85/120    avg_loss:0.347, val_acc:0.883]
Epoch [86/120    avg_loss:0.228, val_acc:0.932]
Epoch [87/120    avg_loss:0.165, val_acc:0.928]
Epoch [88/120    avg_loss:0.146, val_acc:0.927]
Epoch [89/120    avg_loss:0.136, val_acc:0.934]
Epoch [90/120    avg_loss:0.102, val_acc:0.940]
Epoch [91/120    avg_loss:0.082, val_acc:0.948]
Epoch [92/120    avg_loss:0.089, val_acc:0.951]
Epoch [93/120    avg_loss:0.080, val_acc:0.945]
Epoch [94/120    avg_loss:0.072, val_acc:0.947]
Epoch [95/120    avg_loss:0.084, val_acc:0.944]
Epoch [96/120    avg_loss:0.078, val_acc:0.949]
Epoch [97/120    avg_loss:0.075, val_acc:0.951]
Epoch [98/120    avg_loss:0.071, val_acc:0.949]
Epoch [99/120    avg_loss:0.064, val_acc:0.950]
Epoch [100/120    avg_loss:0.063, val_acc:0.950]
Epoch [101/120    avg_loss:0.071, val_acc:0.947]
Epoch [102/120    avg_loss:0.068, val_acc:0.947]
Epoch [103/120    avg_loss:0.067, val_acc:0.949]
Epoch [104/120    avg_loss:0.065, val_acc:0.948]
Epoch [105/120    avg_loss:0.067, val_acc:0.949]
Epoch [106/120    avg_loss:0.061, val_acc:0.948]
Epoch [107/120    avg_loss:0.061, val_acc:0.948]
Epoch [108/120    avg_loss:0.067, val_acc:0.949]
Epoch [109/120    avg_loss:0.063, val_acc:0.949]
Epoch [110/120    avg_loss:0.061, val_acc:0.950]
Epoch [111/120    avg_loss:0.058, val_acc:0.951]
Epoch [112/120    avg_loss:0.073, val_acc:0.951]
Epoch [113/120    avg_loss:0.057, val_acc:0.951]
Epoch [114/120    avg_loss:0.057, val_acc:0.951]
Epoch [115/120    avg_loss:0.069, val_acc:0.951]
Epoch [116/120    avg_loss:0.066, val_acc:0.951]
Epoch [117/120    avg_loss:0.058, val_acc:0.951]
Epoch [118/120    avg_loss:0.061, val_acc:0.951]
Epoch [119/120    avg_loss:0.067, val_acc:0.951]
Epoch [120/120    avg_loss:0.068, val_acc:0.951]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1207    0    0    0   12    0    0    0   11   28    3    2
     0   22    0]
 [   0    0    6  699    1    5    0    0    0   19    0    0   11    5
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  404    0   11    0    8    0    0    0    0
    12    0    0]
 [   0    0    0    0    1    0  651    0    0    1    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   15    0    0    2    0
     0    0    0]
 [   0    0   38   89    0    6    1    0    0    0  731    6    0    0
     3    1    0]
 [   0    0   21    0    0    5   24    0    0    0    6 2131    6    6
    11    0    0]
 [   0    0    0   32   12    8    0    0    0    0   15    1  441    0
     0    0   25]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    1    4    0
  1133    0    0]
 [   0    0    0    0    0    0   27    0    0   16    0    0    9    0
   102  193    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
92.97560975609755

F1 scores:
[       nan 0.975      0.94407509 0.89215061 0.96818182 0.93626883
 0.94828842 0.81967213 0.9953271  0.38961039 0.89092017 0.97283725
 0.86982249 0.96605744 0.94377343 0.68561279 0.87046632]

Kappa:
0.9199536631630382
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc17bda9f28>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.824, val_acc:0.009]
Epoch [2/120    avg_loss:2.791, val_acc:0.051]
Epoch [3/120    avg_loss:2.760, val_acc:0.084]
Epoch [4/120    avg_loss:2.724, val_acc:0.113]
Epoch [5/120    avg_loss:2.695, val_acc:0.163]
Epoch [6/120    avg_loss:2.621, val_acc:0.207]
Epoch [7/120    avg_loss:2.598, val_acc:0.284]
Epoch [8/120    avg_loss:2.541, val_acc:0.278]
Epoch [9/120    avg_loss:2.521, val_acc:0.395]
Epoch [10/120    avg_loss:2.464, val_acc:0.455]
Epoch [11/120    avg_loss:2.408, val_acc:0.503]
Epoch [12/120    avg_loss:2.395, val_acc:0.531]
Epoch [13/120    avg_loss:2.333, val_acc:0.539]
Epoch [14/120    avg_loss:2.286, val_acc:0.548]
Epoch [15/120    avg_loss:2.218, val_acc:0.537]
Epoch [16/120    avg_loss:2.170, val_acc:0.550]
Epoch [17/120    avg_loss:2.122, val_acc:0.552]
Epoch [18/120    avg_loss:2.026, val_acc:0.534]
Epoch [19/120    avg_loss:1.981, val_acc:0.575]
Epoch [20/120    avg_loss:1.904, val_acc:0.577]
Epoch [21/120    avg_loss:1.835, val_acc:0.565]
Epoch [22/120    avg_loss:1.747, val_acc:0.586]
Epoch [23/120    avg_loss:1.714, val_acc:0.602]
Epoch [24/120    avg_loss:1.752, val_acc:0.565]
Epoch [25/120    avg_loss:1.593, val_acc:0.616]
Epoch [26/120    avg_loss:1.486, val_acc:0.641]
Epoch [27/120    avg_loss:1.432, val_acc:0.665]
Epoch [28/120    avg_loss:1.358, val_acc:0.665]
Epoch [29/120    avg_loss:1.328, val_acc:0.660]
Epoch [30/120    avg_loss:1.272, val_acc:0.598]
Epoch [31/120    avg_loss:1.260, val_acc:0.681]
Epoch [32/120    avg_loss:1.145, val_acc:0.703]
Epoch [33/120    avg_loss:1.054, val_acc:0.739]
Epoch [34/120    avg_loss:1.047, val_acc:0.709]
Epoch [35/120    avg_loss:0.967, val_acc:0.744]
Epoch [36/120    avg_loss:0.906, val_acc:0.752]
Epoch [37/120    avg_loss:0.881, val_acc:0.751]
Epoch [38/120    avg_loss:0.804, val_acc:0.782]
Epoch [39/120    avg_loss:0.775, val_acc:0.790]
Epoch [40/120    avg_loss:0.687, val_acc:0.799]
Epoch [41/120    avg_loss:0.690, val_acc:0.824]
Epoch [42/120    avg_loss:0.656, val_acc:0.849]
Epoch [43/120    avg_loss:0.623, val_acc:0.783]
Epoch [44/120    avg_loss:0.609, val_acc:0.839]
Epoch [45/120    avg_loss:0.540, val_acc:0.840]
Epoch [46/120    avg_loss:0.500, val_acc:0.841]
Epoch [47/120    avg_loss:0.438, val_acc:0.864]
Epoch [48/120    avg_loss:0.488, val_acc:0.857]
Epoch [49/120    avg_loss:0.468, val_acc:0.866]
Epoch [50/120    avg_loss:0.422, val_acc:0.856]
Epoch [51/120    avg_loss:0.408, val_acc:0.860]
Epoch [52/120    avg_loss:0.414, val_acc:0.889]
Epoch [53/120    avg_loss:0.421, val_acc:0.867]
Epoch [54/120    avg_loss:0.361, val_acc:0.884]
Epoch [55/120    avg_loss:0.360, val_acc:0.863]
Epoch [56/120    avg_loss:0.330, val_acc:0.881]
Epoch [57/120    avg_loss:0.294, val_acc:0.903]
Epoch [58/120    avg_loss:0.281, val_acc:0.881]
Epoch [59/120    avg_loss:0.278, val_acc:0.899]
Epoch [60/120    avg_loss:0.262, val_acc:0.915]
Epoch [61/120    avg_loss:0.249, val_acc:0.861]
Epoch [62/120    avg_loss:0.305, val_acc:0.919]
Epoch [63/120    avg_loss:0.261, val_acc:0.908]
Epoch [64/120    avg_loss:0.209, val_acc:0.926]
Epoch [65/120    avg_loss:0.275, val_acc:0.927]
Epoch [66/120    avg_loss:0.264, val_acc:0.923]
Epoch [67/120    avg_loss:0.196, val_acc:0.922]
Epoch [68/120    avg_loss:0.178, val_acc:0.927]
Epoch [69/120    avg_loss:0.191, val_acc:0.927]
Epoch [70/120    avg_loss:0.146, val_acc:0.942]
Epoch [71/120    avg_loss:0.124, val_acc:0.944]
Epoch [72/120    avg_loss:0.135, val_acc:0.943]
Epoch [73/120    avg_loss:0.134, val_acc:0.952]
Epoch [74/120    avg_loss:0.246, val_acc:0.903]
Epoch [75/120    avg_loss:0.262, val_acc:0.918]
Epoch [76/120    avg_loss:0.191, val_acc:0.917]
Epoch [77/120    avg_loss:0.166, val_acc:0.941]
Epoch [78/120    avg_loss:0.161, val_acc:0.944]
Epoch [79/120    avg_loss:0.130, val_acc:0.934]
Epoch [80/120    avg_loss:0.138, val_acc:0.928]
Epoch [81/120    avg_loss:0.108, val_acc:0.939]
Epoch [82/120    avg_loss:0.107, val_acc:0.952]
Epoch [83/120    avg_loss:0.102, val_acc:0.947]
Epoch [84/120    avg_loss:0.105, val_acc:0.941]
Epoch [85/120    avg_loss:0.104, val_acc:0.945]
Epoch [86/120    avg_loss:0.110, val_acc:0.948]
Epoch [87/120    avg_loss:0.082, val_acc:0.956]
Epoch [88/120    avg_loss:0.087, val_acc:0.950]
Epoch [89/120    avg_loss:0.072, val_acc:0.949]
Epoch [90/120    avg_loss:0.090, val_acc:0.945]
Epoch [91/120    avg_loss:0.078, val_acc:0.956]
Epoch [92/120    avg_loss:0.072, val_acc:0.957]
Epoch [93/120    avg_loss:0.086, val_acc:0.957]
Epoch [94/120    avg_loss:0.060, val_acc:0.953]
Epoch [95/120    avg_loss:0.078, val_acc:0.952]
Epoch [96/120    avg_loss:0.056, val_acc:0.960]
Epoch [97/120    avg_loss:0.056, val_acc:0.953]
Epoch [98/120    avg_loss:0.054, val_acc:0.967]
Epoch [99/120    avg_loss:0.053, val_acc:0.958]
Epoch [100/120    avg_loss:0.044, val_acc:0.959]
Epoch [101/120    avg_loss:0.054, val_acc:0.953]
Epoch [102/120    avg_loss:0.057, val_acc:0.952]
Epoch [103/120    avg_loss:0.048, val_acc:0.965]
Epoch [104/120    avg_loss:0.043, val_acc:0.960]
Epoch [105/120    avg_loss:0.041, val_acc:0.970]
Epoch [106/120    avg_loss:0.043, val_acc:0.965]
Epoch [107/120    avg_loss:0.040, val_acc:0.967]
Epoch [108/120    avg_loss:0.048, val_acc:0.961]
Epoch [109/120    avg_loss:0.059, val_acc:0.958]
Epoch [110/120    avg_loss:0.046, val_acc:0.951]
Epoch [111/120    avg_loss:0.055, val_acc:0.959]
Epoch [112/120    avg_loss:0.049, val_acc:0.964]
Epoch [113/120    avg_loss:0.049, val_acc:0.972]
Epoch [114/120    avg_loss:0.044, val_acc:0.969]
Epoch [115/120    avg_loss:0.034, val_acc:0.972]
Epoch [116/120    avg_loss:0.033, val_acc:0.968]
Epoch [117/120    avg_loss:0.032, val_acc:0.973]
Epoch [118/120    avg_loss:0.041, val_acc:0.964]
Epoch [119/120    avg_loss:0.064, val_acc:0.969]
Epoch [120/120    avg_loss:0.052, val_acc:0.950]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1133   30    0    0   14    0    0    0   17   63    1    0
     0   27    0]
 [   0    0    0  662   25   15    0    0    0   27    0    0   18    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  421    0    8    0    6    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0  655    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    7    0    0   11    0    0    0    0
     0    0    0]
 [   0    0   26   90    0    5    0    0    0    0  735    2    0    0
     2   15    0]
 [   0    0   13    0    0    1   30    0    0    0    8 2129    6    3
    20    0    0]
 [   0    0    0   24   38   11    0    0    0    1   16    0  412    0
     1    4   27]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   12    3    0    1    0    0    0    0    0
  1123    0    0]
 [   0    0    0    0    0    3   49    0    0    0    0    0    0   10
    83  202    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
91.69647696476964

F1 scores:
[       nan 0.975      0.92188771 0.85254346 0.87116564 0.9324474
 0.92579505 0.86206897 0.99883856 0.34920635 0.8892922  0.96662883
 0.84860968 0.96605744 0.94847973 0.6789916  0.86153846]

Kappa:
0.905461073116726
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:10
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa370889f98>
supervision:full
center_pixel:True
Network :
Number of parameter: 43069==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.825, val_acc:0.002]
Epoch [2/120    avg_loss:2.797, val_acc:0.015]
Epoch [3/120    avg_loss:2.769, val_acc:0.133]
Epoch [4/120    avg_loss:2.733, val_acc:0.157]
Epoch [5/120    avg_loss:2.704, val_acc:0.262]
Epoch [6/120    avg_loss:2.655, val_acc:0.329]
Epoch [7/120    avg_loss:2.613, val_acc:0.410]
Epoch [8/120    avg_loss:2.547, val_acc:0.429]
Epoch [9/120    avg_loss:2.502, val_acc:0.445]
Epoch [10/120    avg_loss:2.444, val_acc:0.471]
Epoch [11/120    avg_loss:2.398, val_acc:0.475]
Epoch [12/120    avg_loss:2.357, val_acc:0.468]
Epoch [13/120    avg_loss:2.326, val_acc:0.490]
Epoch [14/120    avg_loss:2.248, val_acc:0.473]
Epoch [15/120    avg_loss:2.218, val_acc:0.461]
Epoch [16/120    avg_loss:2.116, val_acc:0.487]
Epoch [17/120    avg_loss:2.086, val_acc:0.527]
Epoch [18/120    avg_loss:2.008, val_acc:0.486]
Epoch [19/120    avg_loss:1.913, val_acc:0.529]
Epoch [20/120    avg_loss:1.847, val_acc:0.581]
Epoch [21/120    avg_loss:1.800, val_acc:0.537]
Epoch [22/120    avg_loss:1.734, val_acc:0.569]
Epoch [23/120    avg_loss:1.651, val_acc:0.593]
Epoch [24/120    avg_loss:1.515, val_acc:0.640]
Epoch [25/120    avg_loss:1.445, val_acc:0.555]
Epoch [26/120    avg_loss:1.367, val_acc:0.633]
Epoch [27/120    avg_loss:1.312, val_acc:0.614]
Epoch [28/120    avg_loss:1.210, val_acc:0.638]
Epoch [29/120    avg_loss:1.140, val_acc:0.681]
Epoch [30/120    avg_loss:1.132, val_acc:0.683]
Epoch [31/120    avg_loss:1.014, val_acc:0.689]
Epoch [32/120    avg_loss:1.007, val_acc:0.720]
Epoch [33/120    avg_loss:0.928, val_acc:0.708]
Epoch [34/120    avg_loss:0.898, val_acc:0.713]
Epoch [35/120    avg_loss:0.886, val_acc:0.711]
Epoch [36/120    avg_loss:0.815, val_acc:0.751]
Epoch [37/120    avg_loss:0.779, val_acc:0.767]
Epoch [38/120    avg_loss:0.747, val_acc:0.776]
Epoch [39/120    avg_loss:0.742, val_acc:0.780]
Epoch [40/120    avg_loss:0.666, val_acc:0.808]
Epoch [41/120    avg_loss:0.619, val_acc:0.825]
Epoch [42/120    avg_loss:0.588, val_acc:0.813]
Epoch [43/120    avg_loss:0.527, val_acc:0.832]
Epoch [44/120    avg_loss:0.510, val_acc:0.802]
Epoch [45/120    avg_loss:0.482, val_acc:0.838]
Epoch [46/120    avg_loss:0.446, val_acc:0.849]
Epoch [47/120    avg_loss:0.409, val_acc:0.840]
Epoch [48/120    avg_loss:0.406, val_acc:0.849]
Epoch [49/120    avg_loss:0.398, val_acc:0.858]
Epoch [50/120    avg_loss:0.385, val_acc:0.856]
Epoch [51/120    avg_loss:0.345, val_acc:0.876]
Epoch [52/120    avg_loss:0.309, val_acc:0.865]
Epoch [53/120    avg_loss:0.289, val_acc:0.875]
Epoch [54/120    avg_loss:0.255, val_acc:0.885]
Epoch [55/120    avg_loss:0.252, val_acc:0.880]
Epoch [56/120    avg_loss:0.289, val_acc:0.875]
Epoch [57/120    avg_loss:0.250, val_acc:0.890]
Epoch [58/120    avg_loss:0.228, val_acc:0.887]
Epoch [59/120    avg_loss:0.205, val_acc:0.908]
Epoch [60/120    avg_loss:0.220, val_acc:0.917]
Epoch [61/120    avg_loss:0.224, val_acc:0.898]
Epoch [62/120    avg_loss:0.185, val_acc:0.918]
Epoch [63/120    avg_loss:0.193, val_acc:0.914]
Epoch [64/120    avg_loss:0.185, val_acc:0.925]
Epoch [65/120    avg_loss:0.194, val_acc:0.908]
Epoch [66/120    avg_loss:0.178, val_acc:0.926]
Epoch [67/120    avg_loss:0.138, val_acc:0.935]
Epoch [68/120    avg_loss:0.133, val_acc:0.936]
Epoch [69/120    avg_loss:0.137, val_acc:0.930]
Epoch [70/120    avg_loss:0.128, val_acc:0.933]
Epoch [71/120    avg_loss:0.116, val_acc:0.935]
Epoch [72/120    avg_loss:0.129, val_acc:0.936]
Epoch [73/120    avg_loss:0.111, val_acc:0.931]
Epoch [74/120    avg_loss:0.112, val_acc:0.949]
Epoch [75/120    avg_loss:0.103, val_acc:0.942]
Epoch [76/120    avg_loss:0.101, val_acc:0.938]
Epoch [77/120    avg_loss:0.096, val_acc:0.940]
Epoch [78/120    avg_loss:0.098, val_acc:0.935]
Epoch [79/120    avg_loss:0.107, val_acc:0.940]
Epoch [80/120    avg_loss:0.115, val_acc:0.944]
Epoch [81/120    avg_loss:0.112, val_acc:0.948]
Epoch [82/120    avg_loss:0.093, val_acc:0.945]
Epoch [83/120    avg_loss:0.078, val_acc:0.951]
Epoch [84/120    avg_loss:0.073, val_acc:0.940]
Epoch [85/120    avg_loss:0.074, val_acc:0.939]
Epoch [86/120    avg_loss:0.083, val_acc:0.951]
Epoch [87/120    avg_loss:0.071, val_acc:0.952]
Epoch [88/120    avg_loss:0.072, val_acc:0.961]
Epoch [89/120    avg_loss:0.075, val_acc:0.956]
Epoch [90/120    avg_loss:0.076, val_acc:0.950]
Epoch [91/120    avg_loss:0.062, val_acc:0.958]
Epoch [92/120    avg_loss:0.067, val_acc:0.956]
Epoch [93/120    avg_loss:0.054, val_acc:0.957]
Epoch [94/120    avg_loss:0.051, val_acc:0.962]
Epoch [95/120    avg_loss:0.060, val_acc:0.961]
Epoch [96/120    avg_loss:0.064, val_acc:0.956]
Epoch [97/120    avg_loss:0.063, val_acc:0.960]
Epoch [98/120    avg_loss:0.126, val_acc:0.945]
Epoch [99/120    avg_loss:0.105, val_acc:0.946]
Epoch [100/120    avg_loss:0.197, val_acc:0.910]
Epoch [101/120    avg_loss:0.163, val_acc:0.923]
Epoch [102/120    avg_loss:0.126, val_acc:0.951]
Epoch [103/120    avg_loss:0.082, val_acc:0.956]
Epoch [104/120    avg_loss:0.076, val_acc:0.946]
Epoch [105/120    avg_loss:0.076, val_acc:0.957]
Epoch [106/120    avg_loss:0.059, val_acc:0.964]
Epoch [107/120    avg_loss:0.044, val_acc:0.967]
Epoch [108/120    avg_loss:0.040, val_acc:0.963]
Epoch [109/120    avg_loss:0.048, val_acc:0.961]
Epoch [110/120    avg_loss:0.049, val_acc:0.955]
Epoch [111/120    avg_loss:0.043, val_acc:0.961]
Epoch [112/120    avg_loss:0.039, val_acc:0.969]
Epoch [113/120    avg_loss:0.043, val_acc:0.951]
Epoch [114/120    avg_loss:0.038, val_acc:0.962]
Epoch [115/120    avg_loss:0.040, val_acc:0.964]
Epoch [116/120    avg_loss:0.032, val_acc:0.969]
Epoch [117/120    avg_loss:0.040, val_acc:0.964]
Epoch [118/120    avg_loss:0.048, val_acc:0.963]
Epoch [119/120    avg_loss:0.036, val_acc:0.970]
Epoch [120/120    avg_loss:0.035, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1109    4    0    0    0    0    0    0   60  107    5    0
     0    0    0]
 [   0    0    2  725    0    0    0    0    0   11    0    0    9    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  425    0    7    0    2    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  649    0    0    0    0    5    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   10    0    4    0    0
     0    0    0]
 [   0    0    3   90    0    3    0    0    0    0  721   54    3    0
     0    1    0]
 [   0    0    2    0    0    0    1    0    0    0   13 2190    4    0
     0    0    0]
 [   0    0    0    0    1    1    0    0    0    0    7    2  520    0
     0    0    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    2    0    0
  1135    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
   174  172    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
93.55013550135502

F1 scores:
[       nan 0.98765432 0.92339717 0.92356688 0.99530516 0.98152425
 0.993114   0.87719298 0.99883856 0.48780488 0.85986881 0.95737705
 0.96564531 0.99728997 0.92615259 0.66153846 0.97647059]

Kappa:
0.9262132801214427
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3b017415f8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.644, val_acc:0.404]
Epoch [2/120    avg_loss:2.282, val_acc:0.490]
Epoch [3/120    avg_loss:1.984, val_acc:0.566]
Epoch [4/120    avg_loss:1.735, val_acc:0.588]
Epoch [5/120    avg_loss:1.486, val_acc:0.627]
Epoch [6/120    avg_loss:1.339, val_acc:0.619]
Epoch [7/120    avg_loss:1.181, val_acc:0.637]
Epoch [8/120    avg_loss:1.016, val_acc:0.741]
Epoch [9/120    avg_loss:0.980, val_acc:0.706]
Epoch [10/120    avg_loss:0.799, val_acc:0.751]
Epoch [11/120    avg_loss:1.217, val_acc:0.640]
Epoch [12/120    avg_loss:0.848, val_acc:0.754]
Epoch [13/120    avg_loss:0.643, val_acc:0.793]
Epoch [14/120    avg_loss:0.557, val_acc:0.799]
Epoch [15/120    avg_loss:0.466, val_acc:0.795]
Epoch [16/120    avg_loss:0.459, val_acc:0.834]
Epoch [17/120    avg_loss:0.406, val_acc:0.857]
Epoch [18/120    avg_loss:0.355, val_acc:0.824]
Epoch [19/120    avg_loss:0.326, val_acc:0.760]
Epoch [20/120    avg_loss:0.427, val_acc:0.866]
Epoch [21/120    avg_loss:0.331, val_acc:0.870]
Epoch [22/120    avg_loss:0.277, val_acc:0.864]
Epoch [23/120    avg_loss:0.238, val_acc:0.900]
Epoch [24/120    avg_loss:0.281, val_acc:0.853]
Epoch [25/120    avg_loss:0.255, val_acc:0.907]
Epoch [26/120    avg_loss:0.204, val_acc:0.909]
Epoch [27/120    avg_loss:0.191, val_acc:0.924]
Epoch [28/120    avg_loss:0.142, val_acc:0.939]
Epoch [29/120    avg_loss:0.192, val_acc:0.914]
Epoch [30/120    avg_loss:0.166, val_acc:0.804]
Epoch [31/120    avg_loss:0.249, val_acc:0.907]
Epoch [32/120    avg_loss:0.215, val_acc:0.930]
Epoch [33/120    avg_loss:0.134, val_acc:0.939]
Epoch [34/120    avg_loss:0.120, val_acc:0.946]
Epoch [35/120    avg_loss:0.149, val_acc:0.936]
Epoch [36/120    avg_loss:0.108, val_acc:0.947]
Epoch [37/120    avg_loss:0.124, val_acc:0.936]
Epoch [38/120    avg_loss:0.130, val_acc:0.944]
Epoch [39/120    avg_loss:0.114, val_acc:0.945]
Epoch [40/120    avg_loss:0.094, val_acc:0.955]
Epoch [41/120    avg_loss:0.108, val_acc:0.931]
Epoch [42/120    avg_loss:0.068, val_acc:0.960]
Epoch [43/120    avg_loss:0.060, val_acc:0.951]
Epoch [44/120    avg_loss:0.104, val_acc:0.824]
Epoch [45/120    avg_loss:0.152, val_acc:0.939]
Epoch [46/120    avg_loss:0.078, val_acc:0.958]
Epoch [47/120    avg_loss:0.065, val_acc:0.958]
Epoch [48/120    avg_loss:0.055, val_acc:0.957]
Epoch [49/120    avg_loss:0.070, val_acc:0.962]
Epoch [50/120    avg_loss:0.073, val_acc:0.966]
Epoch [51/120    avg_loss:0.039, val_acc:0.960]
Epoch [52/120    avg_loss:0.037, val_acc:0.967]
Epoch [53/120    avg_loss:0.053, val_acc:0.949]
Epoch [54/120    avg_loss:0.042, val_acc:0.956]
Epoch [55/120    avg_loss:0.044, val_acc:0.946]
Epoch [56/120    avg_loss:0.033, val_acc:0.960]
Epoch [57/120    avg_loss:0.028, val_acc:0.963]
Epoch [58/120    avg_loss:0.037, val_acc:0.958]
Epoch [59/120    avg_loss:0.047, val_acc:0.954]
Epoch [60/120    avg_loss:0.036, val_acc:0.964]
Epoch [61/120    avg_loss:0.030, val_acc:0.962]
Epoch [62/120    avg_loss:0.022, val_acc:0.967]
Epoch [63/120    avg_loss:0.023, val_acc:0.966]
Epoch [64/120    avg_loss:0.018, val_acc:0.968]
Epoch [65/120    avg_loss:0.019, val_acc:0.966]
Epoch [66/120    avg_loss:0.023, val_acc:0.964]
Epoch [67/120    avg_loss:0.036, val_acc:0.962]
Epoch [68/120    avg_loss:0.116, val_acc:0.964]
Epoch [69/120    avg_loss:0.059, val_acc:0.969]
Epoch [70/120    avg_loss:0.027, val_acc:0.968]
Epoch [71/120    avg_loss:0.029, val_acc:0.963]
Epoch [72/120    avg_loss:0.027, val_acc:0.959]
Epoch [73/120    avg_loss:0.030, val_acc:0.969]
Epoch [74/120    avg_loss:0.055, val_acc:0.962]
Epoch [75/120    avg_loss:0.034, val_acc:0.963]
Epoch [76/120    avg_loss:0.028, val_acc:0.961]
Epoch [77/120    avg_loss:0.028, val_acc:0.955]
Epoch [78/120    avg_loss:0.016, val_acc:0.966]
Epoch [79/120    avg_loss:0.015, val_acc:0.971]
Epoch [80/120    avg_loss:0.020, val_acc:0.947]
Epoch [81/120    avg_loss:0.027, val_acc:0.957]
Epoch [82/120    avg_loss:0.012, val_acc:0.971]
Epoch [83/120    avg_loss:0.012, val_acc:0.968]
Epoch [84/120    avg_loss:0.012, val_acc:0.972]
Epoch [85/120    avg_loss:0.030, val_acc:0.945]
Epoch [86/120    avg_loss:0.026, val_acc:0.966]
Epoch [87/120    avg_loss:0.020, val_acc:0.954]
Epoch [88/120    avg_loss:0.042, val_acc:0.965]
Epoch [89/120    avg_loss:0.023, val_acc:0.962]
Epoch [90/120    avg_loss:0.015, val_acc:0.968]
Epoch [91/120    avg_loss:0.013, val_acc:0.971]
Epoch [92/120    avg_loss:0.016, val_acc:0.960]
Epoch [93/120    avg_loss:0.015, val_acc:0.974]
Epoch [94/120    avg_loss:0.025, val_acc:0.968]
Epoch [95/120    avg_loss:0.022, val_acc:0.963]
Epoch [96/120    avg_loss:0.014, val_acc:0.975]
Epoch [97/120    avg_loss:0.014, val_acc:0.961]
Epoch [98/120    avg_loss:0.010, val_acc:0.973]
Epoch [99/120    avg_loss:0.007, val_acc:0.970]
Epoch [100/120    avg_loss:0.235, val_acc:0.820]
Epoch [101/120    avg_loss:0.210, val_acc:0.955]
Epoch [102/120    avg_loss:0.048, val_acc:0.962]
Epoch [103/120    avg_loss:0.029, val_acc:0.959]
Epoch [104/120    avg_loss:0.025, val_acc:0.960]
Epoch [105/120    avg_loss:0.016, val_acc:0.972]
Epoch [106/120    avg_loss:0.023, val_acc:0.971]
Epoch [107/120    avg_loss:0.018, val_acc:0.968]
Epoch [108/120    avg_loss:0.022, val_acc:0.952]
Epoch [109/120    avg_loss:0.024, val_acc:0.967]
Epoch [110/120    avg_loss:0.025, val_acc:0.973]
Epoch [111/120    avg_loss:0.012, val_acc:0.973]
Epoch [112/120    avg_loss:0.012, val_acc:0.973]
Epoch [113/120    avg_loss:0.014, val_acc:0.977]
Epoch [114/120    avg_loss:0.011, val_acc:0.975]
Epoch [115/120    avg_loss:0.009, val_acc:0.976]
Epoch [116/120    avg_loss:0.009, val_acc:0.974]
Epoch [117/120    avg_loss:0.011, val_acc:0.973]
Epoch [118/120    avg_loss:0.010, val_acc:0.972]
Epoch [119/120    avg_loss:0.009, val_acc:0.973]
Epoch [120/120    avg_loss:0.013, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1258    8    2    2    0    0    0    1    1   13    0    0
     0    0    0]
 [   0    0    3  734    1    0    0    0    0    0    0    2    7    0
     0    0    0]
 [   0    0    0    0  210    1    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    4  431    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    0  836   32    0    0
     0    0    0]
 [   0    0   17    8    0    0    0    0    0    0   19 2162    1    1
     2    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0    4  519    0
     0    7    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1133    6    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    73  274    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.50677506775068

F1 scores:
[       nan 0.98765432 0.97860755 0.97931955 0.97674419 0.99194476
 0.99923839 1.         1.         0.97297297 0.96591566 0.977617
 0.97648166 0.99730458 0.96507666 0.86435331 0.98823529]

Kappa:
0.971555402093332
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f7e95ca58>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.604, val_acc:0.448]
Epoch [2/120    avg_loss:2.213, val_acc:0.549]
Epoch [3/120    avg_loss:1.952, val_acc:0.545]
Epoch [4/120    avg_loss:1.726, val_acc:0.589]
Epoch [5/120    avg_loss:1.520, val_acc:0.644]
Epoch [6/120    avg_loss:1.312, val_acc:0.654]
Epoch [7/120    avg_loss:1.151, val_acc:0.715]
Epoch [8/120    avg_loss:1.036, val_acc:0.700]
Epoch [9/120    avg_loss:0.909, val_acc:0.697]
Epoch [10/120    avg_loss:0.830, val_acc:0.782]
Epoch [11/120    avg_loss:0.683, val_acc:0.798]
Epoch [12/120    avg_loss:0.584, val_acc:0.807]
Epoch [13/120    avg_loss:0.630, val_acc:0.814]
Epoch [14/120    avg_loss:0.555, val_acc:0.785]
Epoch [15/120    avg_loss:0.552, val_acc:0.825]
Epoch [16/120    avg_loss:0.435, val_acc:0.804]
Epoch [17/120    avg_loss:0.431, val_acc:0.864]
Epoch [18/120    avg_loss:0.375, val_acc:0.825]
Epoch [19/120    avg_loss:0.417, val_acc:0.816]
Epoch [20/120    avg_loss:0.439, val_acc:0.841]
Epoch [21/120    avg_loss:0.316, val_acc:0.876]
Epoch [22/120    avg_loss:0.308, val_acc:0.889]
Epoch [23/120    avg_loss:0.295, val_acc:0.809]
Epoch [24/120    avg_loss:0.278, val_acc:0.820]
Epoch [25/120    avg_loss:0.244, val_acc:0.912]
Epoch [26/120    avg_loss:0.239, val_acc:0.913]
Epoch [27/120    avg_loss:0.198, val_acc:0.887]
Epoch [28/120    avg_loss:0.167, val_acc:0.888]
Epoch [29/120    avg_loss:0.183, val_acc:0.914]
Epoch [30/120    avg_loss:0.164, val_acc:0.889]
Epoch [31/120    avg_loss:0.157, val_acc:0.896]
Epoch [32/120    avg_loss:0.142, val_acc:0.945]
Epoch [33/120    avg_loss:0.135, val_acc:0.941]
Epoch [34/120    avg_loss:0.142, val_acc:0.907]
Epoch [35/120    avg_loss:0.207, val_acc:0.940]
Epoch [36/120    avg_loss:0.138, val_acc:0.947]
Epoch [37/120    avg_loss:0.098, val_acc:0.950]
Epoch [38/120    avg_loss:0.091, val_acc:0.941]
Epoch [39/120    avg_loss:0.089, val_acc:0.943]
Epoch [40/120    avg_loss:0.088, val_acc:0.943]
Epoch [41/120    avg_loss:0.075, val_acc:0.950]
Epoch [42/120    avg_loss:0.077, val_acc:0.954]
Epoch [43/120    avg_loss:0.059, val_acc:0.947]
Epoch [44/120    avg_loss:0.144, val_acc:0.939]
Epoch [45/120    avg_loss:0.074, val_acc:0.950]
Epoch [46/120    avg_loss:0.077, val_acc:0.910]
Epoch [47/120    avg_loss:0.103, val_acc:0.911]
Epoch [48/120    avg_loss:0.110, val_acc:0.942]
Epoch [49/120    avg_loss:0.073, val_acc:0.947]
Epoch [50/120    avg_loss:0.082, val_acc:0.948]
Epoch [51/120    avg_loss:0.111, val_acc:0.926]
Epoch [52/120    avg_loss:0.144, val_acc:0.925]
Epoch [53/120    avg_loss:0.119, val_acc:0.917]
Epoch [54/120    avg_loss:0.075, val_acc:0.936]
Epoch [55/120    avg_loss:0.074, val_acc:0.944]
Epoch [56/120    avg_loss:0.053, val_acc:0.963]
Epoch [57/120    avg_loss:0.043, val_acc:0.967]
Epoch [58/120    avg_loss:0.030, val_acc:0.969]
Epoch [59/120    avg_loss:0.033, val_acc:0.969]
Epoch [60/120    avg_loss:0.032, val_acc:0.970]
Epoch [61/120    avg_loss:0.030, val_acc:0.969]
Epoch [62/120    avg_loss:0.032, val_acc:0.969]
Epoch [63/120    avg_loss:0.036, val_acc:0.969]
Epoch [64/120    avg_loss:0.031, val_acc:0.968]
Epoch [65/120    avg_loss:0.037, val_acc:0.968]
Epoch [66/120    avg_loss:0.038, val_acc:0.969]
Epoch [67/120    avg_loss:0.031, val_acc:0.970]
Epoch [68/120    avg_loss:0.044, val_acc:0.969]
Epoch [69/120    avg_loss:0.034, val_acc:0.968]
Epoch [70/120    avg_loss:0.032, val_acc:0.970]
Epoch [71/120    avg_loss:0.029, val_acc:0.969]
Epoch [72/120    avg_loss:0.024, val_acc:0.971]
Epoch [73/120    avg_loss:0.024, val_acc:0.975]
Epoch [74/120    avg_loss:0.025, val_acc:0.973]
Epoch [75/120    avg_loss:0.023, val_acc:0.973]
Epoch [76/120    avg_loss:0.028, val_acc:0.976]
Epoch [77/120    avg_loss:0.021, val_acc:0.975]
Epoch [78/120    avg_loss:0.024, val_acc:0.974]
Epoch [79/120    avg_loss:0.024, val_acc:0.977]
Epoch [80/120    avg_loss:0.023, val_acc:0.972]
Epoch [81/120    avg_loss:0.028, val_acc:0.974]
Epoch [82/120    avg_loss:0.042, val_acc:0.974]
Epoch [83/120    avg_loss:0.025, val_acc:0.975]
Epoch [84/120    avg_loss:0.019, val_acc:0.973]
Epoch [85/120    avg_loss:0.021, val_acc:0.976]
Epoch [86/120    avg_loss:0.029, val_acc:0.976]
Epoch [87/120    avg_loss:0.022, val_acc:0.975]
Epoch [88/120    avg_loss:0.026, val_acc:0.974]
Epoch [89/120    avg_loss:0.018, val_acc:0.975]
Epoch [90/120    avg_loss:0.023, val_acc:0.975]
Epoch [91/120    avg_loss:0.020, val_acc:0.975]
Epoch [92/120    avg_loss:0.019, val_acc:0.975]
Epoch [93/120    avg_loss:0.020, val_acc:0.975]
Epoch [94/120    avg_loss:0.019, val_acc:0.975]
Epoch [95/120    avg_loss:0.018, val_acc:0.975]
Epoch [96/120    avg_loss:0.025, val_acc:0.975]
Epoch [97/120    avg_loss:0.018, val_acc:0.976]
Epoch [98/120    avg_loss:0.027, val_acc:0.975]
Epoch [99/120    avg_loss:0.019, val_acc:0.975]
Epoch [100/120    avg_loss:0.016, val_acc:0.975]
Epoch [101/120    avg_loss:0.020, val_acc:0.975]
Epoch [102/120    avg_loss:0.020, val_acc:0.975]
Epoch [103/120    avg_loss:0.021, val_acc:0.975]
Epoch [104/120    avg_loss:0.017, val_acc:0.976]
Epoch [105/120    avg_loss:0.021, val_acc:0.975]
Epoch [106/120    avg_loss:0.019, val_acc:0.975]
Epoch [107/120    avg_loss:0.017, val_acc:0.975]
Epoch [108/120    avg_loss:0.021, val_acc:0.975]
Epoch [109/120    avg_loss:0.019, val_acc:0.975]
Epoch [110/120    avg_loss:0.023, val_acc:0.975]
Epoch [111/120    avg_loss:0.022, val_acc:0.975]
Epoch [112/120    avg_loss:0.024, val_acc:0.975]
Epoch [113/120    avg_loss:0.023, val_acc:0.975]
Epoch [114/120    avg_loss:0.023, val_acc:0.975]
Epoch [115/120    avg_loss:0.019, val_acc:0.975]
Epoch [116/120    avg_loss:0.016, val_acc:0.975]
Epoch [117/120    avg_loss:0.025, val_acc:0.975]
Epoch [118/120    avg_loss:0.018, val_acc:0.975]
Epoch [119/120    avg_loss:0.021, val_acc:0.975]
Epoch [120/120    avg_loss:0.018, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0 1231    7   15    0    0    0    0    0    4   28    0    0
     0    0    0]
 [   0    0    1  724    3    0    0    0    0    0    1   12    5    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    1    0    0  433    0    0    0    0    0    0    0    0
     0    1    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    0    0    0    0    0  853   14    0    0
     0    0    0]
 [   0   10   13    0    0    1    0    0    0    0   15 2156    9    1
     1    3    1]
 [   0    0    0    2    0    0    0    0    0    0    4    2  525    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
  1120   17    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    61  286    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.30081300813008

F1 scores:
[       nan 0.87912088 0.9696731  0.97837838 0.95945946 0.99425947
 0.99771167 1.         1.         1.         0.97374429 0.97468354
 0.97674419 0.99730458 0.96468562 0.87328244 0.98224852]

Kappa:
0.969225352267409
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5fcf26fa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.680, val_acc:0.351]
Epoch [2/120    avg_loss:2.274, val_acc:0.450]
Epoch [3/120    avg_loss:1.981, val_acc:0.410]
Epoch [4/120    avg_loss:1.721, val_acc:0.533]
Epoch [5/120    avg_loss:1.585, val_acc:0.633]
Epoch [6/120    avg_loss:1.395, val_acc:0.657]
Epoch [7/120    avg_loss:1.183, val_acc:0.625]
Epoch [8/120    avg_loss:1.047, val_acc:0.695]
Epoch [9/120    avg_loss:0.940, val_acc:0.722]
Epoch [10/120    avg_loss:0.869, val_acc:0.739]
Epoch [11/120    avg_loss:0.684, val_acc:0.744]
Epoch [12/120    avg_loss:0.689, val_acc:0.737]
Epoch [13/120    avg_loss:0.572, val_acc:0.765]
Epoch [14/120    avg_loss:0.589, val_acc:0.778]
Epoch [15/120    avg_loss:0.513, val_acc:0.780]
Epoch [16/120    avg_loss:0.434, val_acc:0.833]
Epoch [17/120    avg_loss:0.418, val_acc:0.819]
Epoch [18/120    avg_loss:0.390, val_acc:0.836]
Epoch [19/120    avg_loss:0.344, val_acc:0.785]
Epoch [20/120    avg_loss:0.347, val_acc:0.839]
Epoch [21/120    avg_loss:0.335, val_acc:0.840]
Epoch [22/120    avg_loss:0.324, val_acc:0.876]
Epoch [23/120    avg_loss:0.247, val_acc:0.887]
Epoch [24/120    avg_loss:0.246, val_acc:0.876]
Epoch [25/120    avg_loss:0.257, val_acc:0.882]
Epoch [26/120    avg_loss:0.222, val_acc:0.896]
Epoch [27/120    avg_loss:0.190, val_acc:0.899]
Epoch [28/120    avg_loss:0.197, val_acc:0.847]
Epoch [29/120    avg_loss:0.271, val_acc:0.914]
Epoch [30/120    avg_loss:0.171, val_acc:0.879]
Epoch [31/120    avg_loss:0.194, val_acc:0.895]
Epoch [32/120    avg_loss:0.166, val_acc:0.915]
Epoch [33/120    avg_loss:0.179, val_acc:0.903]
Epoch [34/120    avg_loss:0.175, val_acc:0.882]
Epoch [35/120    avg_loss:0.134, val_acc:0.931]
Epoch [36/120    avg_loss:0.096, val_acc:0.941]
Epoch [37/120    avg_loss:0.098, val_acc:0.908]
Epoch [38/120    avg_loss:0.105, val_acc:0.940]
Epoch [39/120    avg_loss:0.081, val_acc:0.928]
Epoch [40/120    avg_loss:0.113, val_acc:0.918]
Epoch [41/120    avg_loss:0.117, val_acc:0.954]
Epoch [42/120    avg_loss:0.111, val_acc:0.916]
Epoch [43/120    avg_loss:0.111, val_acc:0.933]
Epoch [44/120    avg_loss:0.069, val_acc:0.950]
Epoch [45/120    avg_loss:0.081, val_acc:0.943]
Epoch [46/120    avg_loss:0.065, val_acc:0.931]
Epoch [47/120    avg_loss:0.064, val_acc:0.949]
Epoch [48/120    avg_loss:0.079, val_acc:0.939]
Epoch [49/120    avg_loss:0.053, val_acc:0.912]
Epoch [50/120    avg_loss:0.180, val_acc:0.906]
Epoch [51/120    avg_loss:0.121, val_acc:0.949]
Epoch [52/120    avg_loss:0.060, val_acc:0.961]
Epoch [53/120    avg_loss:0.069, val_acc:0.960]
Epoch [54/120    avg_loss:0.063, val_acc:0.960]
Epoch [55/120    avg_loss:0.052, val_acc:0.952]
Epoch [56/120    avg_loss:0.043, val_acc:0.959]
Epoch [57/120    avg_loss:0.060, val_acc:0.931]
Epoch [58/120    avg_loss:0.123, val_acc:0.948]
Epoch [59/120    avg_loss:0.050, val_acc:0.952]
Epoch [60/120    avg_loss:0.041, val_acc:0.966]
Epoch [61/120    avg_loss:0.040, val_acc:0.945]
Epoch [62/120    avg_loss:0.059, val_acc:0.954]
Epoch [63/120    avg_loss:0.041, val_acc:0.959]
Epoch [64/120    avg_loss:0.049, val_acc:0.942]
Epoch [65/120    avg_loss:0.054, val_acc:0.964]
Epoch [66/120    avg_loss:0.033, val_acc:0.965]
Epoch [67/120    avg_loss:0.030, val_acc:0.959]
Epoch [68/120    avg_loss:0.040, val_acc:0.955]
Epoch [69/120    avg_loss:0.044, val_acc:0.955]
Epoch [70/120    avg_loss:0.027, val_acc:0.960]
Epoch [71/120    avg_loss:0.022, val_acc:0.963]
Epoch [72/120    avg_loss:0.030, val_acc:0.961]
Epoch [73/120    avg_loss:0.028, val_acc:0.964]
Epoch [74/120    avg_loss:0.019, val_acc:0.967]
Epoch [75/120    avg_loss:0.018, val_acc:0.968]
Epoch [76/120    avg_loss:0.016, val_acc:0.971]
Epoch [77/120    avg_loss:0.019, val_acc:0.969]
Epoch [78/120    avg_loss:0.012, val_acc:0.968]
Epoch [79/120    avg_loss:0.014, val_acc:0.968]
Epoch [80/120    avg_loss:0.015, val_acc:0.968]
Epoch [81/120    avg_loss:0.014, val_acc:0.968]
Epoch [82/120    avg_loss:0.017, val_acc:0.968]
Epoch [83/120    avg_loss:0.021, val_acc:0.967]
Epoch [84/120    avg_loss:0.015, val_acc:0.970]
Epoch [85/120    avg_loss:0.014, val_acc:0.970]
Epoch [86/120    avg_loss:0.024, val_acc:0.967]
Epoch [87/120    avg_loss:0.016, val_acc:0.969]
Epoch [88/120    avg_loss:0.017, val_acc:0.969]
Epoch [89/120    avg_loss:0.012, val_acc:0.969]
Epoch [90/120    avg_loss:0.016, val_acc:0.969]
Epoch [91/120    avg_loss:0.010, val_acc:0.969]
Epoch [92/120    avg_loss:0.015, val_acc:0.969]
Epoch [93/120    avg_loss:0.014, val_acc:0.969]
Epoch [94/120    avg_loss:0.012, val_acc:0.969]
Epoch [95/120    avg_loss:0.013, val_acc:0.969]
Epoch [96/120    avg_loss:0.015, val_acc:0.969]
Epoch [97/120    avg_loss:0.016, val_acc:0.969]
Epoch [98/120    avg_loss:0.017, val_acc:0.969]
Epoch [99/120    avg_loss:0.012, val_acc:0.969]
Epoch [100/120    avg_loss:0.016, val_acc:0.969]
Epoch [101/120    avg_loss:0.012, val_acc:0.969]
Epoch [102/120    avg_loss:0.013, val_acc:0.969]
Epoch [103/120    avg_loss:0.012, val_acc:0.969]
Epoch [104/120    avg_loss:0.013, val_acc:0.969]
Epoch [105/120    avg_loss:0.010, val_acc:0.969]
Epoch [106/120    avg_loss:0.013, val_acc:0.969]
Epoch [107/120    avg_loss:0.011, val_acc:0.969]
Epoch [108/120    avg_loss:0.011, val_acc:0.969]
Epoch [109/120    avg_loss:0.017, val_acc:0.969]
Epoch [110/120    avg_loss:0.013, val_acc:0.969]
Epoch [111/120    avg_loss:0.014, val_acc:0.969]
Epoch [112/120    avg_loss:0.016, val_acc:0.969]
Epoch [113/120    avg_loss:0.011, val_acc:0.969]
Epoch [114/120    avg_loss:0.012, val_acc:0.969]
Epoch [115/120    avg_loss:0.015, val_acc:0.969]
Epoch [116/120    avg_loss:0.013, val_acc:0.969]
Epoch [117/120    avg_loss:0.011, val_acc:0.969]
Epoch [118/120    avg_loss:0.010, val_acc:0.969]
Epoch [119/120    avg_loss:0.011, val_acc:0.969]
Epoch [120/120    avg_loss:0.015, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    1    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1240    7   10    0    0    0    0    0    7   20    1    0
     0    0    0]
 [   0    0    0  727    7    0    0    0    0    0    0    8    5    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    1    0    1
     2    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    3    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    9    0    0    2    0    0    0    0  851   13    0    0
     0    0    0]
 [   0    0    7    4    0    8    1    0    0    0   35 2148    4    0
     0    3    0]
 [   0    0    0    2    0    0    0    0    0    0    0    7  522    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1124   14    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    1    0
    91  252    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.88888888888889

F1 scores:
[       nan 0.975      0.97560976 0.97715054 0.95927602 0.98289624
 0.99238965 1.         1.         0.97142857 0.96266968 0.97392881
 0.97752809 0.99730458 0.9533503  0.81685575 0.98224852]

Kappa:
0.9645179492880371
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe91a7a4a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.670, val_acc:0.409]
Epoch [2/120    avg_loss:2.331, val_acc:0.502]
Epoch [3/120    avg_loss:2.019, val_acc:0.541]
Epoch [4/120    avg_loss:1.776, val_acc:0.616]
Epoch [5/120    avg_loss:1.592, val_acc:0.664]
Epoch [6/120    avg_loss:1.368, val_acc:0.679]
Epoch [7/120    avg_loss:1.244, val_acc:0.704]
Epoch [8/120    avg_loss:1.119, val_acc:0.763]
Epoch [9/120    avg_loss:0.923, val_acc:0.767]
Epoch [10/120    avg_loss:0.858, val_acc:0.776]
Epoch [11/120    avg_loss:0.722, val_acc:0.804]
Epoch [12/120    avg_loss:0.652, val_acc:0.802]
Epoch [13/120    avg_loss:0.561, val_acc:0.793]
Epoch [14/120    avg_loss:0.591, val_acc:0.846]
Epoch [15/120    avg_loss:0.469, val_acc:0.814]
Epoch [16/120    avg_loss:0.457, val_acc:0.832]
Epoch [17/120    avg_loss:0.389, val_acc:0.842]
Epoch [18/120    avg_loss:0.448, val_acc:0.860]
Epoch [19/120    avg_loss:0.303, val_acc:0.860]
Epoch [20/120    avg_loss:0.289, val_acc:0.889]
Epoch [21/120    avg_loss:0.302, val_acc:0.879]
Epoch [22/120    avg_loss:0.271, val_acc:0.898]
Epoch [23/120    avg_loss:0.196, val_acc:0.914]
Epoch [24/120    avg_loss:0.220, val_acc:0.900]
Epoch [25/120    avg_loss:0.236, val_acc:0.929]
Epoch [26/120    avg_loss:0.171, val_acc:0.920]
Epoch [27/120    avg_loss:0.247, val_acc:0.896]
Epoch [28/120    avg_loss:0.208, val_acc:0.887]
Epoch [29/120    avg_loss:0.167, val_acc:0.911]
Epoch [30/120    avg_loss:0.161, val_acc:0.931]
Epoch [31/120    avg_loss:0.127, val_acc:0.939]
Epoch [32/120    avg_loss:0.164, val_acc:0.933]
Epoch [33/120    avg_loss:0.128, val_acc:0.894]
Epoch [34/120    avg_loss:0.193, val_acc:0.938]
Epoch [35/120    avg_loss:0.123, val_acc:0.915]
Epoch [36/120    avg_loss:0.212, val_acc:0.925]
Epoch [37/120    avg_loss:0.147, val_acc:0.925]
Epoch [38/120    avg_loss:0.109, val_acc:0.954]
Epoch [39/120    avg_loss:0.092, val_acc:0.956]
Epoch [40/120    avg_loss:0.085, val_acc:0.927]
Epoch [41/120    avg_loss:0.084, val_acc:0.954]
Epoch [42/120    avg_loss:0.100, val_acc:0.945]
Epoch [43/120    avg_loss:0.123, val_acc:0.952]
Epoch [44/120    avg_loss:0.063, val_acc:0.933]
Epoch [45/120    avg_loss:0.059, val_acc:0.948]
Epoch [46/120    avg_loss:0.136, val_acc:0.888]
Epoch [47/120    avg_loss:0.163, val_acc:0.942]
Epoch [48/120    avg_loss:0.101, val_acc:0.954]
Epoch [49/120    avg_loss:0.059, val_acc:0.957]
Epoch [50/120    avg_loss:0.083, val_acc:0.890]
Epoch [51/120    avg_loss:0.111, val_acc:0.951]
Epoch [52/120    avg_loss:0.067, val_acc:0.959]
Epoch [53/120    avg_loss:0.059, val_acc:0.961]
Epoch [54/120    avg_loss:0.030, val_acc:0.966]
Epoch [55/120    avg_loss:0.034, val_acc:0.964]
Epoch [56/120    avg_loss:0.382, val_acc:0.926]
Epoch [57/120    avg_loss:0.151, val_acc:0.946]
Epoch [58/120    avg_loss:0.085, val_acc:0.952]
Epoch [59/120    avg_loss:0.060, val_acc:0.951]
Epoch [60/120    avg_loss:0.054, val_acc:0.951]
Epoch [61/120    avg_loss:0.044, val_acc:0.955]
Epoch [62/120    avg_loss:0.035, val_acc:0.966]
Epoch [63/120    avg_loss:0.034, val_acc:0.962]
Epoch [64/120    avg_loss:0.057, val_acc:0.963]
Epoch [65/120    avg_loss:0.061, val_acc:0.940]
Epoch [66/120    avg_loss:0.041, val_acc:0.951]
Epoch [67/120    avg_loss:0.028, val_acc:0.970]
Epoch [68/120    avg_loss:0.036, val_acc:0.954]
Epoch [69/120    avg_loss:0.046, val_acc:0.962]
Epoch [70/120    avg_loss:0.020, val_acc:0.970]
Epoch [71/120    avg_loss:0.027, val_acc:0.970]
Epoch [72/120    avg_loss:0.018, val_acc:0.969]
Epoch [73/120    avg_loss:0.014, val_acc:0.966]
Epoch [74/120    avg_loss:0.023, val_acc:0.970]
Epoch [75/120    avg_loss:0.024, val_acc:0.967]
Epoch [76/120    avg_loss:0.011, val_acc:0.975]
Epoch [77/120    avg_loss:0.027, val_acc:0.964]
Epoch [78/120    avg_loss:0.025, val_acc:0.965]
Epoch [79/120    avg_loss:0.026, val_acc:0.954]
Epoch [80/120    avg_loss:0.042, val_acc:0.961]
Epoch [81/120    avg_loss:0.028, val_acc:0.967]
Epoch [82/120    avg_loss:0.013, val_acc:0.973]
Epoch [83/120    avg_loss:0.012, val_acc:0.977]
Epoch [84/120    avg_loss:0.018, val_acc:0.964]
Epoch [85/120    avg_loss:0.012, val_acc:0.970]
Epoch [86/120    avg_loss:0.011, val_acc:0.965]
Epoch [87/120    avg_loss:0.023, val_acc:0.968]
Epoch [88/120    avg_loss:0.019, val_acc:0.970]
Epoch [89/120    avg_loss:0.013, val_acc:0.972]
Epoch [90/120    avg_loss:0.013, val_acc:0.965]
Epoch [91/120    avg_loss:0.016, val_acc:0.969]
Epoch [92/120    avg_loss:0.016, val_acc:0.965]
Epoch [93/120    avg_loss:0.015, val_acc:0.970]
Epoch [94/120    avg_loss:0.026, val_acc:0.973]
Epoch [95/120    avg_loss:0.033, val_acc:0.927]
Epoch [96/120    avg_loss:0.202, val_acc:0.944]
Epoch [97/120    avg_loss:0.062, val_acc:0.959]
Epoch [98/120    avg_loss:0.055, val_acc:0.963]
Epoch [99/120    avg_loss:0.050, val_acc:0.962]
Epoch [100/120    avg_loss:0.033, val_acc:0.966]
Epoch [101/120    avg_loss:0.032, val_acc:0.962]
Epoch [102/120    avg_loss:0.026, val_acc:0.966]
Epoch [103/120    avg_loss:0.027, val_acc:0.967]
Epoch [104/120    avg_loss:0.021, val_acc:0.964]
Epoch [105/120    avg_loss:0.017, val_acc:0.967]
Epoch [106/120    avg_loss:0.018, val_acc:0.966]
Epoch [107/120    avg_loss:0.021, val_acc:0.965]
Epoch [108/120    avg_loss:0.023, val_acc:0.969]
Epoch [109/120    avg_loss:0.021, val_acc:0.968]
Epoch [110/120    avg_loss:0.020, val_acc:0.968]
Epoch [111/120    avg_loss:0.025, val_acc:0.968]
Epoch [112/120    avg_loss:0.016, val_acc:0.967]
Epoch [113/120    avg_loss:0.018, val_acc:0.967]
Epoch [114/120    avg_loss:0.018, val_acc:0.967]
Epoch [115/120    avg_loss:0.017, val_acc:0.967]
Epoch [116/120    avg_loss:0.018, val_acc:0.968]
Epoch [117/120    avg_loss:0.013, val_acc:0.966]
Epoch [118/120    avg_loss:0.017, val_acc:0.967]
Epoch [119/120    avg_loss:0.019, val_acc:0.968]
Epoch [120/120    avg_loss:0.015, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    2    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1242    8    5    0    0    0    0    0    2   27    1    0
     0    0    0]
 [   0    0    2  727    2    0    0    0    0    0    0    5   10    0
     0    1    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    1    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    0    0    0    0    0  827   37    0    0
     2    0    0]
 [   0    0   13   10    0    3    0    0    0    1   13 2140   29    0
     0    1    0]
 [   0    0    3    2    0    0    0    0    0    0    0    4  520    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    0    0    0    0
  1116   18    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
   103  241    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.36856368563686

F1 scores:
[       nan 0.96202532 0.97183099 0.97257525 0.97911833 0.98630137
 0.99847561 1.         1.         0.97297297 0.96274738 0.967669
 0.94804011 1.         0.94416244 0.79276316 0.95402299]

Kappa:
0.958571829761177
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:14:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5dfef76a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.657, val_acc:0.456]
Epoch [2/120    avg_loss:2.248, val_acc:0.521]
Epoch [3/120    avg_loss:1.894, val_acc:0.550]
Epoch [4/120    avg_loss:1.650, val_acc:0.591]
Epoch [5/120    avg_loss:1.485, val_acc:0.578]
Epoch [6/120    avg_loss:1.254, val_acc:0.700]
Epoch [7/120    avg_loss:1.208, val_acc:0.668]
Epoch [8/120    avg_loss:1.019, val_acc:0.672]
Epoch [9/120    avg_loss:0.905, val_acc:0.654]
Epoch [10/120    avg_loss:0.801, val_acc:0.773]
Epoch [11/120    avg_loss:0.829, val_acc:0.690]
Epoch [12/120    avg_loss:0.631, val_acc:0.771]
Epoch [13/120    avg_loss:0.742, val_acc:0.796]
Epoch [14/120    avg_loss:0.563, val_acc:0.840]
Epoch [15/120    avg_loss:0.595, val_acc:0.759]
Epoch [16/120    avg_loss:0.448, val_acc:0.828]
Epoch [17/120    avg_loss:0.434, val_acc:0.841]
Epoch [18/120    avg_loss:0.411, val_acc:0.844]
Epoch [19/120    avg_loss:0.366, val_acc:0.887]
Epoch [20/120    avg_loss:0.333, val_acc:0.863]
Epoch [21/120    avg_loss:0.307, val_acc:0.818]
Epoch [22/120    avg_loss:0.258, val_acc:0.887]
Epoch [23/120    avg_loss:0.222, val_acc:0.883]
Epoch [24/120    avg_loss:0.214, val_acc:0.904]
Epoch [25/120    avg_loss:0.189, val_acc:0.892]
Epoch [26/120    avg_loss:0.237, val_acc:0.853]
Epoch [27/120    avg_loss:0.253, val_acc:0.885]
Epoch [28/120    avg_loss:0.207, val_acc:0.935]
Epoch [29/120    avg_loss:0.186, val_acc:0.856]
Epoch [30/120    avg_loss:0.660, val_acc:0.844]
Epoch [31/120    avg_loss:0.340, val_acc:0.884]
Epoch [32/120    avg_loss:0.240, val_acc:0.911]
Epoch [33/120    avg_loss:0.182, val_acc:0.912]
Epoch [34/120    avg_loss:0.153, val_acc:0.926]
Epoch [35/120    avg_loss:0.138, val_acc:0.916]
Epoch [36/120    avg_loss:0.127, val_acc:0.930]
Epoch [37/120    avg_loss:0.120, val_acc:0.943]
Epoch [38/120    avg_loss:0.101, val_acc:0.940]
Epoch [39/120    avg_loss:0.099, val_acc:0.927]
Epoch [40/120    avg_loss:0.081, val_acc:0.957]
Epoch [41/120    avg_loss:0.080, val_acc:0.955]
Epoch [42/120    avg_loss:0.079, val_acc:0.939]
Epoch [43/120    avg_loss:0.091, val_acc:0.951]
Epoch [44/120    avg_loss:0.077, val_acc:0.922]
Epoch [45/120    avg_loss:0.096, val_acc:0.943]
Epoch [46/120    avg_loss:0.127, val_acc:0.941]
Epoch [47/120    avg_loss:0.073, val_acc:0.941]
Epoch [48/120    avg_loss:0.112, val_acc:0.947]
Epoch [49/120    avg_loss:0.089, val_acc:0.953]
Epoch [50/120    avg_loss:0.047, val_acc:0.954]
Epoch [51/120    avg_loss:0.049, val_acc:0.956]
Epoch [52/120    avg_loss:0.066, val_acc:0.969]
Epoch [53/120    avg_loss:0.044, val_acc:0.959]
Epoch [54/120    avg_loss:0.041, val_acc:0.956]
Epoch [55/120    avg_loss:0.035, val_acc:0.958]
Epoch [56/120    avg_loss:0.034, val_acc:0.952]
Epoch [57/120    avg_loss:0.050, val_acc:0.950]
Epoch [58/120    avg_loss:0.035, val_acc:0.967]
Epoch [59/120    avg_loss:0.045, val_acc:0.960]
Epoch [60/120    avg_loss:0.075, val_acc:0.936]
Epoch [61/120    avg_loss:0.088, val_acc:0.945]
Epoch [62/120    avg_loss:0.052, val_acc:0.955]
Epoch [63/120    avg_loss:0.087, val_acc:0.932]
Epoch [64/120    avg_loss:0.072, val_acc:0.944]
Epoch [65/120    avg_loss:0.055, val_acc:0.960]
Epoch [66/120    avg_loss:0.038, val_acc:0.966]
Epoch [67/120    avg_loss:0.027, val_acc:0.968]
Epoch [68/120    avg_loss:0.028, val_acc:0.970]
Epoch [69/120    avg_loss:0.024, val_acc:0.970]
Epoch [70/120    avg_loss:0.027, val_acc:0.971]
Epoch [71/120    avg_loss:0.032, val_acc:0.971]
Epoch [72/120    avg_loss:0.018, val_acc:0.971]
Epoch [73/120    avg_loss:0.022, val_acc:0.971]
Epoch [74/120    avg_loss:0.021, val_acc:0.971]
Epoch [75/120    avg_loss:0.022, val_acc:0.973]
Epoch [76/120    avg_loss:0.021, val_acc:0.973]
Epoch [77/120    avg_loss:0.019, val_acc:0.971]
Epoch [78/120    avg_loss:0.019, val_acc:0.973]
Epoch [79/120    avg_loss:0.017, val_acc:0.974]
Epoch [80/120    avg_loss:0.017, val_acc:0.973]
Epoch [81/120    avg_loss:0.024, val_acc:0.974]
Epoch [82/120    avg_loss:0.017, val_acc:0.973]
Epoch [83/120    avg_loss:0.020, val_acc:0.972]
Epoch [84/120    avg_loss:0.017, val_acc:0.972]
Epoch [85/120    avg_loss:0.018, val_acc:0.975]
Epoch [86/120    avg_loss:0.025, val_acc:0.974]
Epoch [87/120    avg_loss:0.017, val_acc:0.974]
Epoch [88/120    avg_loss:0.016, val_acc:0.975]
Epoch [89/120    avg_loss:0.018, val_acc:0.975]
Epoch [90/120    avg_loss:0.013, val_acc:0.975]
Epoch [91/120    avg_loss:0.015, val_acc:0.974]
Epoch [92/120    avg_loss:0.016, val_acc:0.973]
Epoch [93/120    avg_loss:0.023, val_acc:0.976]
Epoch [94/120    avg_loss:0.011, val_acc:0.975]
Epoch [95/120    avg_loss:0.017, val_acc:0.974]
Epoch [96/120    avg_loss:0.017, val_acc:0.974]
Epoch [97/120    avg_loss:0.014, val_acc:0.975]
Epoch [98/120    avg_loss:0.014, val_acc:0.973]
Epoch [99/120    avg_loss:0.018, val_acc:0.974]
Epoch [100/120    avg_loss:0.015, val_acc:0.973]
Epoch [101/120    avg_loss:0.031, val_acc:0.971]
Epoch [102/120    avg_loss:0.022, val_acc:0.975]
Epoch [103/120    avg_loss:0.015, val_acc:0.975]
Epoch [104/120    avg_loss:0.015, val_acc:0.975]
Epoch [105/120    avg_loss:0.015, val_acc:0.973]
Epoch [106/120    avg_loss:0.012, val_acc:0.974]
Epoch [107/120    avg_loss:0.013, val_acc:0.975]
Epoch [108/120    avg_loss:0.021, val_acc:0.975]
Epoch [109/120    avg_loss:0.015, val_acc:0.975]
Epoch [110/120    avg_loss:0.013, val_acc:0.975]
Epoch [111/120    avg_loss:0.014, val_acc:0.975]
Epoch [112/120    avg_loss:0.016, val_acc:0.975]
Epoch [113/120    avg_loss:0.015, val_acc:0.974]
Epoch [114/120    avg_loss:0.017, val_acc:0.974]
Epoch [115/120    avg_loss:0.012, val_acc:0.974]
Epoch [116/120    avg_loss:0.013, val_acc:0.974]
Epoch [117/120    avg_loss:0.014, val_acc:0.974]
Epoch [118/120    avg_loss:0.013, val_acc:0.974]
Epoch [119/120    avg_loss:0.019, val_acc:0.974]
Epoch [120/120    avg_loss:0.011, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1251    4    5    0    0    0    0    0    5   20    0    0
     0    0    0]
 [   0    0    0  739    0    1    0    0    0    1    0    2    3    0
     0    1    0]
 [   0    0    0    5  208    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    0    0    0
     5    1    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    2  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    1    0    0
     0    1    0]
 [   0    0   10    8    0    0    0    0    0    0  826   30    0    0
     0    1    0]
 [   0    0    9    2    0    0    0    2    0    2   11 2165   13    1
     1    4    0]
 [   0    0    0    5    0    1    0    0    0    0    3    0  518    0
     0    4    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1114   24    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    77  269    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.06233062330624

F1 scores:
[       nan 1.         0.97925636 0.97880795 0.97652582 0.98961938
 0.99847793 0.92592593 0.997669   0.86486486 0.96046512 0.97786811
 0.97003745 0.99730458 0.95335901 0.82515337 0.98245614]

Kappa:
0.9664921345128542
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd8d4978a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.670, val_acc:0.430]
Epoch [2/120    avg_loss:2.298, val_acc:0.514]
Epoch [3/120    avg_loss:1.972, val_acc:0.538]
Epoch [4/120    avg_loss:1.727, val_acc:0.600]
Epoch [5/120    avg_loss:1.504, val_acc:0.613]
Epoch [6/120    avg_loss:1.306, val_acc:0.654]
Epoch [7/120    avg_loss:1.129, val_acc:0.679]
Epoch [8/120    avg_loss:0.983, val_acc:0.751]
Epoch [9/120    avg_loss:0.881, val_acc:0.762]
Epoch [10/120    avg_loss:0.922, val_acc:0.741]
Epoch [11/120    avg_loss:0.736, val_acc:0.781]
Epoch [12/120    avg_loss:0.616, val_acc:0.786]
Epoch [13/120    avg_loss:0.595, val_acc:0.817]
Epoch [14/120    avg_loss:0.572, val_acc:0.831]
Epoch [15/120    avg_loss:0.568, val_acc:0.658]
Epoch [16/120    avg_loss:0.616, val_acc:0.832]
Epoch [17/120    avg_loss:0.400, val_acc:0.806]
Epoch [18/120    avg_loss:0.347, val_acc:0.865]
Epoch [19/120    avg_loss:0.304, val_acc:0.849]
Epoch [20/120    avg_loss:0.276, val_acc:0.870]
Epoch [21/120    avg_loss:0.246, val_acc:0.898]
Epoch [22/120    avg_loss:0.282, val_acc:0.906]
Epoch [23/120    avg_loss:0.223, val_acc:0.910]
Epoch [24/120    avg_loss:0.309, val_acc:0.906]
Epoch [25/120    avg_loss:0.283, val_acc:0.908]
Epoch [26/120    avg_loss:0.181, val_acc:0.923]
Epoch [27/120    avg_loss:0.184, val_acc:0.877]
Epoch [28/120    avg_loss:0.172, val_acc:0.915]
Epoch [29/120    avg_loss:0.134, val_acc:0.926]
Epoch [30/120    avg_loss:0.140, val_acc:0.902]
Epoch [31/120    avg_loss:0.244, val_acc:0.904]
Epoch [32/120    avg_loss:0.188, val_acc:0.915]
Epoch [33/120    avg_loss:0.188, val_acc:0.923]
Epoch [34/120    avg_loss:0.148, val_acc:0.881]
Epoch [35/120    avg_loss:0.125, val_acc:0.931]
Epoch [36/120    avg_loss:0.158, val_acc:0.930]
Epoch [37/120    avg_loss:0.116, val_acc:0.948]
Epoch [38/120    avg_loss:0.126, val_acc:0.922]
Epoch [39/120    avg_loss:0.144, val_acc:0.931]
Epoch [40/120    avg_loss:0.085, val_acc:0.942]
Epoch [41/120    avg_loss:0.171, val_acc:0.929]
Epoch [42/120    avg_loss:0.083, val_acc:0.940]
Epoch [43/120    avg_loss:0.074, val_acc:0.920]
Epoch [44/120    avg_loss:0.085, val_acc:0.954]
Epoch [45/120    avg_loss:0.129, val_acc:0.931]
Epoch [46/120    avg_loss:0.116, val_acc:0.953]
Epoch [47/120    avg_loss:0.063, val_acc:0.954]
Epoch [48/120    avg_loss:0.082, val_acc:0.954]
Epoch [49/120    avg_loss:0.058, val_acc:0.948]
Epoch [50/120    avg_loss:0.048, val_acc:0.950]
Epoch [51/120    avg_loss:0.067, val_acc:0.947]
Epoch [52/120    avg_loss:0.055, val_acc:0.934]
Epoch [53/120    avg_loss:0.076, val_acc:0.921]
Epoch [54/120    avg_loss:0.073, val_acc:0.965]
Epoch [55/120    avg_loss:0.047, val_acc:0.958]
Epoch [56/120    avg_loss:0.040, val_acc:0.966]
Epoch [57/120    avg_loss:0.056, val_acc:0.943]
Epoch [58/120    avg_loss:0.047, val_acc:0.961]
Epoch [59/120    avg_loss:0.040, val_acc:0.928]
Epoch [60/120    avg_loss:0.043, val_acc:0.957]
Epoch [61/120    avg_loss:0.043, val_acc:0.950]
Epoch [62/120    avg_loss:0.044, val_acc:0.960]
Epoch [63/120    avg_loss:0.039, val_acc:0.965]
Epoch [64/120    avg_loss:0.031, val_acc:0.967]
Epoch [65/120    avg_loss:0.027, val_acc:0.966]
Epoch [66/120    avg_loss:0.044, val_acc:0.961]
Epoch [67/120    avg_loss:0.041, val_acc:0.962]
Epoch [68/120    avg_loss:0.029, val_acc:0.962]
Epoch [69/120    avg_loss:0.025, val_acc:0.967]
Epoch [70/120    avg_loss:0.028, val_acc:0.962]
Epoch [71/120    avg_loss:0.016, val_acc:0.964]
Epoch [72/120    avg_loss:0.028, val_acc:0.959]
Epoch [73/120    avg_loss:0.021, val_acc:0.960]
Epoch [74/120    avg_loss:0.021, val_acc:0.961]
Epoch [75/120    avg_loss:0.025, val_acc:0.965]
Epoch [76/120    avg_loss:0.045, val_acc:0.947]
Epoch [77/120    avg_loss:0.031, val_acc:0.960]
Epoch [78/120    avg_loss:0.026, val_acc:0.969]
Epoch [79/120    avg_loss:0.022, val_acc:0.964]
Epoch [80/120    avg_loss:0.022, val_acc:0.951]
Epoch [81/120    avg_loss:0.019, val_acc:0.970]
Epoch [82/120    avg_loss:0.018, val_acc:0.960]
Epoch [83/120    avg_loss:0.014, val_acc:0.970]
Epoch [84/120    avg_loss:0.015, val_acc:0.966]
Epoch [85/120    avg_loss:0.012, val_acc:0.973]
Epoch [86/120    avg_loss:0.020, val_acc:0.969]
Epoch [87/120    avg_loss:0.034, val_acc:0.971]
Epoch [88/120    avg_loss:0.017, val_acc:0.967]
Epoch [89/120    avg_loss:0.016, val_acc:0.962]
Epoch [90/120    avg_loss:0.015, val_acc:0.963]
Epoch [91/120    avg_loss:0.011, val_acc:0.971]
Epoch [92/120    avg_loss:0.019, val_acc:0.969]
Epoch [93/120    avg_loss:0.011, val_acc:0.967]
Epoch [94/120    avg_loss:0.008, val_acc:0.971]
Epoch [95/120    avg_loss:0.009, val_acc:0.966]
Epoch [96/120    avg_loss:0.020, val_acc:0.960]
Epoch [97/120    avg_loss:0.014, val_acc:0.969]
Epoch [98/120    avg_loss:0.028, val_acc:0.969]
Epoch [99/120    avg_loss:0.013, val_acc:0.969]
Epoch [100/120    avg_loss:0.017, val_acc:0.971]
Epoch [101/120    avg_loss:0.015, val_acc:0.970]
Epoch [102/120    avg_loss:0.009, val_acc:0.972]
Epoch [103/120    avg_loss:0.017, val_acc:0.973]
Epoch [104/120    avg_loss:0.013, val_acc:0.974]
Epoch [105/120    avg_loss:0.009, val_acc:0.973]
Epoch [106/120    avg_loss:0.009, val_acc:0.971]
Epoch [107/120    avg_loss:0.013, val_acc:0.970]
Epoch [108/120    avg_loss:0.007, val_acc:0.971]
Epoch [109/120    avg_loss:0.011, val_acc:0.974]
Epoch [110/120    avg_loss:0.008, val_acc:0.975]
Epoch [111/120    avg_loss:0.011, val_acc:0.974]
Epoch [112/120    avg_loss:0.006, val_acc:0.974]
Epoch [113/120    avg_loss:0.012, val_acc:0.974]
Epoch [114/120    avg_loss:0.007, val_acc:0.971]
Epoch [115/120    avg_loss:0.010, val_acc:0.970]
Epoch [116/120    avg_loss:0.006, val_acc:0.972]
Epoch [117/120    avg_loss:0.011, val_acc:0.973]
Epoch [118/120    avg_loss:0.011, val_acc:0.975]
Epoch [119/120    avg_loss:0.009, val_acc:0.977]
Epoch [120/120    avg_loss:0.008, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1256    2    8    2    1    0    0    1    7    8    0    0
     0    0    0]
 [   0    0    0  732    3    0    0    0    0    1    1    5    4    0
     1    0    0]
 [   0    0    0    1  208    3    0    0    0    0    0    0    0    0
     0    1    0]
 [   0    0    0    0    0  433    0    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    0    0    0    0    1  847   17    1    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    1   29 2156   17    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0    5  522    0
     0    2    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1121   18    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    90  255    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.19241192411924

F1 scores:
[       nan 0.975      0.9820172  0.98718813 0.96296296 0.99198167
 0.99696049 1.         1.         0.9        0.9625     0.97955475
 0.96756256 1.         0.95282618 0.81861958 0.97076023]

Kappa:
0.9679840266050658
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f74c23cda58>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.672, val_acc:0.398]
Epoch [2/120    avg_loss:2.305, val_acc:0.528]
Epoch [3/120    avg_loss:1.972, val_acc:0.589]
Epoch [4/120    avg_loss:1.699, val_acc:0.626]
Epoch [5/120    avg_loss:1.445, val_acc:0.669]
Epoch [6/120    avg_loss:1.219, val_acc:0.679]
Epoch [7/120    avg_loss:1.109, val_acc:0.709]
Epoch [8/120    avg_loss:0.946, val_acc:0.629]
Epoch [9/120    avg_loss:0.817, val_acc:0.747]
Epoch [10/120    avg_loss:0.730, val_acc:0.751]
Epoch [11/120    avg_loss:0.653, val_acc:0.799]
Epoch [12/120    avg_loss:0.606, val_acc:0.798]
Epoch [13/120    avg_loss:0.549, val_acc:0.841]
Epoch [14/120    avg_loss:0.501, val_acc:0.823]
Epoch [15/120    avg_loss:0.441, val_acc:0.810]
Epoch [16/120    avg_loss:0.423, val_acc:0.839]
Epoch [17/120    avg_loss:0.422, val_acc:0.874]
Epoch [18/120    avg_loss:0.356, val_acc:0.835]
Epoch [19/120    avg_loss:0.384, val_acc:0.852]
Epoch [20/120    avg_loss:0.354, val_acc:0.877]
Epoch [21/120    avg_loss:0.351, val_acc:0.881]
Epoch [22/120    avg_loss:0.328, val_acc:0.866]
Epoch [23/120    avg_loss:0.221, val_acc:0.893]
Epoch [24/120    avg_loss:0.304, val_acc:0.895]
Epoch [25/120    avg_loss:0.267, val_acc:0.851]
Epoch [26/120    avg_loss:0.234, val_acc:0.888]
Epoch [27/120    avg_loss:0.179, val_acc:0.905]
Epoch [28/120    avg_loss:0.195, val_acc:0.926]
Epoch [29/120    avg_loss:0.141, val_acc:0.895]
Epoch [30/120    avg_loss:0.146, val_acc:0.917]
Epoch [31/120    avg_loss:0.139, val_acc:0.928]
Epoch [32/120    avg_loss:0.172, val_acc:0.922]
Epoch [33/120    avg_loss:0.119, val_acc:0.930]
Epoch [34/120    avg_loss:0.112, val_acc:0.908]
Epoch [35/120    avg_loss:0.180, val_acc:0.941]
Epoch [36/120    avg_loss:0.174, val_acc:0.904]
Epoch [37/120    avg_loss:0.121, val_acc:0.923]
Epoch [38/120    avg_loss:0.171, val_acc:0.941]
Epoch [39/120    avg_loss:0.129, val_acc:0.942]
Epoch [40/120    avg_loss:0.158, val_acc:0.956]
Epoch [41/120    avg_loss:0.105, val_acc:0.948]
Epoch [42/120    avg_loss:0.106, val_acc:0.949]
Epoch [43/120    avg_loss:0.065, val_acc:0.963]
Epoch [44/120    avg_loss:0.061, val_acc:0.952]
Epoch [45/120    avg_loss:0.048, val_acc:0.929]
Epoch [46/120    avg_loss:0.075, val_acc:0.941]
Epoch [47/120    avg_loss:0.062, val_acc:0.965]
Epoch [48/120    avg_loss:0.062, val_acc:0.948]
Epoch [49/120    avg_loss:0.075, val_acc:0.939]
Epoch [50/120    avg_loss:0.079, val_acc:0.939]
Epoch [51/120    avg_loss:0.079, val_acc:0.947]
Epoch [52/120    avg_loss:0.055, val_acc:0.962]
Epoch [53/120    avg_loss:0.077, val_acc:0.930]
Epoch [54/120    avg_loss:0.085, val_acc:0.930]
Epoch [55/120    avg_loss:0.067, val_acc:0.946]
Epoch [56/120    avg_loss:0.056, val_acc:0.936]
Epoch [57/120    avg_loss:0.034, val_acc:0.914]
Epoch [58/120    avg_loss:0.074, val_acc:0.970]
Epoch [59/120    avg_loss:0.048, val_acc:0.956]
Epoch [60/120    avg_loss:0.075, val_acc:0.963]
Epoch [61/120    avg_loss:0.134, val_acc:0.938]
Epoch [62/120    avg_loss:0.064, val_acc:0.964]
Epoch [63/120    avg_loss:0.046, val_acc:0.968]
Epoch [64/120    avg_loss:0.048, val_acc:0.948]
Epoch [65/120    avg_loss:0.040, val_acc:0.957]
Epoch [66/120    avg_loss:0.051, val_acc:0.965]
Epoch [67/120    avg_loss:0.048, val_acc:0.954]
Epoch [68/120    avg_loss:0.035, val_acc:0.971]
Epoch [69/120    avg_loss:0.023, val_acc:0.969]
Epoch [70/120    avg_loss:0.028, val_acc:0.967]
Epoch [71/120    avg_loss:0.023, val_acc:0.971]
Epoch [72/120    avg_loss:0.017, val_acc:0.973]
Epoch [73/120    avg_loss:0.024, val_acc:0.976]
Epoch [74/120    avg_loss:0.021, val_acc:0.972]
Epoch [75/120    avg_loss:0.030, val_acc:0.935]
Epoch [76/120    avg_loss:0.035, val_acc:0.959]
Epoch [77/120    avg_loss:0.041, val_acc:0.970]
Epoch [78/120    avg_loss:0.057, val_acc:0.940]
Epoch [79/120    avg_loss:0.056, val_acc:0.971]
Epoch [80/120    avg_loss:0.087, val_acc:0.952]
Epoch [81/120    avg_loss:0.043, val_acc:0.963]
Epoch [82/120    avg_loss:0.072, val_acc:0.927]
Epoch [83/120    avg_loss:0.108, val_acc:0.927]
Epoch [84/120    avg_loss:0.169, val_acc:0.925]
Epoch [85/120    avg_loss:0.147, val_acc:0.939]
Epoch [86/120    avg_loss:0.055, val_acc:0.948]
Epoch [87/120    avg_loss:0.046, val_acc:0.965]
Epoch [88/120    avg_loss:0.031, val_acc:0.969]
Epoch [89/120    avg_loss:0.026, val_acc:0.969]
Epoch [90/120    avg_loss:0.035, val_acc:0.968]
Epoch [91/120    avg_loss:0.026, val_acc:0.966]
Epoch [92/120    avg_loss:0.028, val_acc:0.970]
Epoch [93/120    avg_loss:0.020, val_acc:0.969]
Epoch [94/120    avg_loss:0.021, val_acc:0.969]
Epoch [95/120    avg_loss:0.024, val_acc:0.966]
Epoch [96/120    avg_loss:0.037, val_acc:0.970]
Epoch [97/120    avg_loss:0.032, val_acc:0.970]
Epoch [98/120    avg_loss:0.020, val_acc:0.971]
Epoch [99/120    avg_loss:0.024, val_acc:0.970]
Epoch [100/120    avg_loss:0.025, val_acc:0.969]
Epoch [101/120    avg_loss:0.021, val_acc:0.970]
Epoch [102/120    avg_loss:0.027, val_acc:0.970]
Epoch [103/120    avg_loss:0.021, val_acc:0.972]
Epoch [104/120    avg_loss:0.022, val_acc:0.970]
Epoch [105/120    avg_loss:0.021, val_acc:0.971]
Epoch [106/120    avg_loss:0.022, val_acc:0.970]
Epoch [107/120    avg_loss:0.018, val_acc:0.972]
Epoch [108/120    avg_loss:0.026, val_acc:0.970]
Epoch [109/120    avg_loss:0.020, val_acc:0.970]
Epoch [110/120    avg_loss:0.022, val_acc:0.969]
Epoch [111/120    avg_loss:0.016, val_acc:0.970]
Epoch [112/120    avg_loss:0.018, val_acc:0.969]
Epoch [113/120    avg_loss:0.017, val_acc:0.969]
Epoch [114/120    avg_loss:0.026, val_acc:0.969]
Epoch [115/120    avg_loss:0.022, val_acc:0.969]
Epoch [116/120    avg_loss:0.022, val_acc:0.969]
Epoch [117/120    avg_loss:0.021, val_acc:0.970]
Epoch [118/120    avg_loss:0.018, val_acc:0.970]
Epoch [119/120    avg_loss:0.018, val_acc:0.970]
Epoch [120/120    avg_loss:0.022, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1256    2    3    0    1    0    0    0    3   20    0    0
     0    0    0]
 [   0    0    2  726    3    0    0    0    0    0    0   11    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    4    0    0    0    0  844   15    4    0
     0    2    0]
 [   0    0   14    0    0    1    0    2    0    0   24 2154   14    0
     0    1    0]
 [   0    0    0    3    0    0    0    0    0    0    2    3  523    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    0    0    0    0    0    0    1    0
  1113   16    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    61  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.37669376693766

F1 scores:
[       nan 0.98765432 0.98010144 0.98240866 0.98611111 0.98301246
 0.99771863 0.96153846 1.         1.         0.96512293 0.9759855
 0.96494465 1.         0.96155508 0.87557604 0.97005988]

Kappa:
0.9700886415661003
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0b2068c9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.626, val_acc:0.418]
Epoch [2/120    avg_loss:2.192, val_acc:0.466]
Epoch [3/120    avg_loss:1.896, val_acc:0.527]
Epoch [4/120    avg_loss:1.670, val_acc:0.535]
Epoch [5/120    avg_loss:1.513, val_acc:0.549]
Epoch [6/120    avg_loss:1.389, val_acc:0.610]
Epoch [7/120    avg_loss:1.292, val_acc:0.640]
Epoch [8/120    avg_loss:1.107, val_acc:0.640]
Epoch [9/120    avg_loss:1.042, val_acc:0.652]
Epoch [10/120    avg_loss:0.936, val_acc:0.695]
Epoch [11/120    avg_loss:0.862, val_acc:0.702]
Epoch [12/120    avg_loss:0.748, val_acc:0.728]
Epoch [13/120    avg_loss:0.692, val_acc:0.744]
Epoch [14/120    avg_loss:0.589, val_acc:0.795]
Epoch [15/120    avg_loss:0.607, val_acc:0.777]
Epoch [16/120    avg_loss:0.486, val_acc:0.793]
Epoch [17/120    avg_loss:0.493, val_acc:0.774]
Epoch [18/120    avg_loss:0.451, val_acc:0.805]
Epoch [19/120    avg_loss:0.363, val_acc:0.814]
Epoch [20/120    avg_loss:0.308, val_acc:0.848]
Epoch [21/120    avg_loss:0.310, val_acc:0.824]
Epoch [22/120    avg_loss:0.317, val_acc:0.861]
Epoch [23/120    avg_loss:0.260, val_acc:0.841]
Epoch [24/120    avg_loss:0.240, val_acc:0.895]
Epoch [25/120    avg_loss:0.288, val_acc:0.896]
Epoch [26/120    avg_loss:0.248, val_acc:0.894]
Epoch [27/120    avg_loss:0.227, val_acc:0.873]
Epoch [28/120    avg_loss:0.179, val_acc:0.899]
Epoch [29/120    avg_loss:0.154, val_acc:0.897]
Epoch [30/120    avg_loss:0.200, val_acc:0.887]
Epoch [31/120    avg_loss:0.180, val_acc:0.907]
Epoch [32/120    avg_loss:0.208, val_acc:0.883]
Epoch [33/120    avg_loss:0.253, val_acc:0.889]
Epoch [34/120    avg_loss:0.168, val_acc:0.861]
Epoch [35/120    avg_loss:0.209, val_acc:0.911]
Epoch [36/120    avg_loss:0.144, val_acc:0.921]
Epoch [37/120    avg_loss:0.166, val_acc:0.887]
Epoch [38/120    avg_loss:0.115, val_acc:0.923]
Epoch [39/120    avg_loss:0.134, val_acc:0.930]
Epoch [40/120    avg_loss:0.115, val_acc:0.882]
Epoch [41/120    avg_loss:0.199, val_acc:0.912]
Epoch [42/120    avg_loss:0.159, val_acc:0.860]
Epoch [43/120    avg_loss:0.147, val_acc:0.943]
Epoch [44/120    avg_loss:0.102, val_acc:0.870]
Epoch [45/120    avg_loss:0.092, val_acc:0.938]
Epoch [46/120    avg_loss:0.089, val_acc:0.944]
Epoch [47/120    avg_loss:0.116, val_acc:0.917]
Epoch [48/120    avg_loss:0.067, val_acc:0.959]
Epoch [49/120    avg_loss:0.063, val_acc:0.913]
Epoch [50/120    avg_loss:0.049, val_acc:0.961]
Epoch [51/120    avg_loss:0.061, val_acc:0.947]
Epoch [52/120    avg_loss:0.048, val_acc:0.941]
Epoch [53/120    avg_loss:0.075, val_acc:0.950]
Epoch [54/120    avg_loss:0.073, val_acc:0.950]
Epoch [55/120    avg_loss:0.053, val_acc:0.952]
Epoch [56/120    avg_loss:0.035, val_acc:0.963]
Epoch [57/120    avg_loss:0.054, val_acc:0.940]
Epoch [58/120    avg_loss:0.094, val_acc:0.950]
Epoch [59/120    avg_loss:0.067, val_acc:0.965]
Epoch [60/120    avg_loss:0.048, val_acc:0.948]
Epoch [61/120    avg_loss:0.039, val_acc:0.971]
Epoch [62/120    avg_loss:0.034, val_acc:0.961]
Epoch [63/120    avg_loss:0.041, val_acc:0.949]
Epoch [64/120    avg_loss:0.027, val_acc:0.958]
Epoch [65/120    avg_loss:0.049, val_acc:0.955]
Epoch [66/120    avg_loss:0.034, val_acc:0.946]
Epoch [67/120    avg_loss:0.062, val_acc:0.951]
Epoch [68/120    avg_loss:0.038, val_acc:0.962]
Epoch [69/120    avg_loss:0.019, val_acc:0.963]
Epoch [70/120    avg_loss:0.048, val_acc:0.927]
Epoch [71/120    avg_loss:0.036, val_acc:0.964]
Epoch [72/120    avg_loss:0.026, val_acc:0.958]
Epoch [73/120    avg_loss:0.023, val_acc:0.965]
Epoch [74/120    avg_loss:0.021, val_acc:0.960]
Epoch [75/120    avg_loss:0.019, val_acc:0.965]
Epoch [76/120    avg_loss:0.014, val_acc:0.964]
Epoch [77/120    avg_loss:0.014, val_acc:0.968]
Epoch [78/120    avg_loss:0.014, val_acc:0.966]
Epoch [79/120    avg_loss:0.012, val_acc:0.963]
Epoch [80/120    avg_loss:0.011, val_acc:0.966]
Epoch [81/120    avg_loss:0.013, val_acc:0.966]
Epoch [82/120    avg_loss:0.015, val_acc:0.966]
Epoch [83/120    avg_loss:0.009, val_acc:0.965]
Epoch [84/120    avg_loss:0.011, val_acc:0.966]
Epoch [85/120    avg_loss:0.014, val_acc:0.966]
Epoch [86/120    avg_loss:0.010, val_acc:0.966]
Epoch [87/120    avg_loss:0.012, val_acc:0.965]
Epoch [88/120    avg_loss:0.019, val_acc:0.965]
Epoch [89/120    avg_loss:0.013, val_acc:0.966]
Epoch [90/120    avg_loss:0.009, val_acc:0.966]
Epoch [91/120    avg_loss:0.011, val_acc:0.966]
Epoch [92/120    avg_loss:0.011, val_acc:0.966]
Epoch [93/120    avg_loss:0.009, val_acc:0.966]
Epoch [94/120    avg_loss:0.011, val_acc:0.966]
Epoch [95/120    avg_loss:0.009, val_acc:0.966]
Epoch [96/120    avg_loss:0.009, val_acc:0.966]
Epoch [97/120    avg_loss:0.010, val_acc:0.966]
Epoch [98/120    avg_loss:0.009, val_acc:0.966]
Epoch [99/120    avg_loss:0.009, val_acc:0.966]
Epoch [100/120    avg_loss:0.025, val_acc:0.966]
Epoch [101/120    avg_loss:0.011, val_acc:0.966]
Epoch [102/120    avg_loss:0.008, val_acc:0.966]
Epoch [103/120    avg_loss:0.010, val_acc:0.966]
Epoch [104/120    avg_loss:0.013, val_acc:0.966]
Epoch [105/120    avg_loss:0.009, val_acc:0.966]
Epoch [106/120    avg_loss:0.015, val_acc:0.966]
Epoch [107/120    avg_loss:0.010, val_acc:0.966]
Epoch [108/120    avg_loss:0.013, val_acc:0.966]
Epoch [109/120    avg_loss:0.017, val_acc:0.966]
Epoch [110/120    avg_loss:0.012, val_acc:0.966]
Epoch [111/120    avg_loss:0.010, val_acc:0.966]
Epoch [112/120    avg_loss:0.009, val_acc:0.966]
Epoch [113/120    avg_loss:0.012, val_acc:0.966]
Epoch [114/120    avg_loss:0.010, val_acc:0.966]
Epoch [115/120    avg_loss:0.011, val_acc:0.966]
Epoch [116/120    avg_loss:0.010, val_acc:0.966]
Epoch [117/120    avg_loss:0.011, val_acc:0.966]
Epoch [118/120    avg_loss:0.012, val_acc:0.966]
Epoch [119/120    avg_loss:0.012, val_acc:0.966]
Epoch [120/120    avg_loss:0.012, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    0    0    0    0    0    6    0    0    0    0    0    0
     0    0    0]
 [   0    1 1249    3    4    0    0    0    0    0    2   26    0    0
     0    0    0]
 [   0    0    4  739    0    0    0    0    0    0    0    0    4    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    1    0    0  428    2    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    1  652    0    0    0    0    1    0    1
     1    1    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    0    0
     0    1    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    1    0    0    0    0  849   18    1    0
     1    0    0]
 [   0    0    5    0    0    2    0    3    0    0   10 2163    1    0
     3    1   22]
 [   0    0    3    8    0    1    0    0    0    0    4    1  514    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    0    0    0    0
  1117   18    0]
 [   0    0    0    0    0    6    0    0    0    0    0    0    0    0
    98  243    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.92140921409214

F1 scores:
[       nan 0.90909091 0.97884013 0.98730795 0.98834499 0.9738339
 0.99466056 0.82758621 0.99883586 1.         0.97586207 0.97873303
 0.97533207 0.99730458 0.94540838 0.79282219 0.87958115]

Kappa:
0.9648844006640157
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb09c3d4a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.666, val_acc:0.438]
Epoch [2/120    avg_loss:2.274, val_acc:0.494]
Epoch [3/120    avg_loss:1.984, val_acc:0.518]
Epoch [4/120    avg_loss:1.741, val_acc:0.553]
Epoch [5/120    avg_loss:1.566, val_acc:0.624]
Epoch [6/120    avg_loss:1.334, val_acc:0.664]
Epoch [7/120    avg_loss:1.186, val_acc:0.626]
Epoch [8/120    avg_loss:1.071, val_acc:0.670]
Epoch [9/120    avg_loss:0.945, val_acc:0.723]
Epoch [10/120    avg_loss:0.820, val_acc:0.706]
Epoch [11/120    avg_loss:0.769, val_acc:0.761]
Epoch [12/120    avg_loss:0.704, val_acc:0.755]
Epoch [13/120    avg_loss:0.601, val_acc:0.781]
Epoch [14/120    avg_loss:0.506, val_acc:0.788]
Epoch [15/120    avg_loss:0.499, val_acc:0.785]
Epoch [16/120    avg_loss:0.487, val_acc:0.836]
Epoch [17/120    avg_loss:0.390, val_acc:0.811]
Epoch [18/120    avg_loss:0.407, val_acc:0.841]
Epoch [19/120    avg_loss:0.359, val_acc:0.838]
Epoch [20/120    avg_loss:0.318, val_acc:0.844]
Epoch [21/120    avg_loss:0.316, val_acc:0.797]
Epoch [22/120    avg_loss:0.361, val_acc:0.818]
Epoch [23/120    avg_loss:0.249, val_acc:0.895]
Epoch [24/120    avg_loss:0.285, val_acc:0.821]
Epoch [25/120    avg_loss:0.284, val_acc:0.892]
Epoch [26/120    avg_loss:0.255, val_acc:0.894]
Epoch [27/120    avg_loss:0.205, val_acc:0.887]
Epoch [28/120    avg_loss:0.203, val_acc:0.899]
Epoch [29/120    avg_loss:0.198, val_acc:0.885]
Epoch [30/120    avg_loss:0.211, val_acc:0.895]
Epoch [31/120    avg_loss:0.186, val_acc:0.902]
Epoch [32/120    avg_loss:0.153, val_acc:0.932]
Epoch [33/120    avg_loss:0.142, val_acc:0.913]
Epoch [34/120    avg_loss:0.164, val_acc:0.895]
Epoch [35/120    avg_loss:0.127, val_acc:0.912]
Epoch [36/120    avg_loss:0.108, val_acc:0.936]
Epoch [37/120    avg_loss:0.122, val_acc:0.942]
Epoch [38/120    avg_loss:0.113, val_acc:0.945]
Epoch [39/120    avg_loss:0.127, val_acc:0.927]
Epoch [40/120    avg_loss:0.079, val_acc:0.936]
Epoch [41/120    avg_loss:0.094, val_acc:0.915]
Epoch [42/120    avg_loss:0.102, val_acc:0.929]
Epoch [43/120    avg_loss:0.093, val_acc:0.921]
Epoch [44/120    avg_loss:0.084, val_acc:0.931]
Epoch [45/120    avg_loss:0.071, val_acc:0.947]
Epoch [46/120    avg_loss:0.106, val_acc:0.926]
Epoch [47/120    avg_loss:0.063, val_acc:0.956]
Epoch [48/120    avg_loss:0.063, val_acc:0.954]
Epoch [49/120    avg_loss:0.047, val_acc:0.961]
Epoch [50/120    avg_loss:0.062, val_acc:0.931]
Epoch [51/120    avg_loss:0.127, val_acc:0.922]
Epoch [52/120    avg_loss:0.094, val_acc:0.948]
Epoch [53/120    avg_loss:0.072, val_acc:0.941]
Epoch [54/120    avg_loss:0.064, val_acc:0.950]
Epoch [55/120    avg_loss:0.075, val_acc:0.938]
Epoch [56/120    avg_loss:0.068, val_acc:0.964]
Epoch [57/120    avg_loss:0.063, val_acc:0.957]
Epoch [58/120    avg_loss:0.047, val_acc:0.958]
Epoch [59/120    avg_loss:0.044, val_acc:0.964]
Epoch [60/120    avg_loss:0.037, val_acc:0.970]
Epoch [61/120    avg_loss:0.049, val_acc:0.961]
Epoch [62/120    avg_loss:0.045, val_acc:0.970]
Epoch [63/120    avg_loss:0.031, val_acc:0.969]
Epoch [64/120    avg_loss:0.030, val_acc:0.974]
Epoch [65/120    avg_loss:0.171, val_acc:0.930]
Epoch [66/120    avg_loss:0.056, val_acc:0.960]
Epoch [67/120    avg_loss:0.070, val_acc:0.947]
Epoch [68/120    avg_loss:0.056, val_acc:0.965]
Epoch [69/120    avg_loss:0.043, val_acc:0.957]
Epoch [70/120    avg_loss:0.033, val_acc:0.969]
Epoch [71/120    avg_loss:0.065, val_acc:0.940]
Epoch [72/120    avg_loss:0.033, val_acc:0.966]
Epoch [73/120    avg_loss:0.038, val_acc:0.965]
Epoch [74/120    avg_loss:0.040, val_acc:0.974]
Epoch [75/120    avg_loss:0.048, val_acc:0.932]
Epoch [76/120    avg_loss:0.049, val_acc:0.965]
Epoch [77/120    avg_loss:0.029, val_acc:0.962]
Epoch [78/120    avg_loss:0.049, val_acc:0.973]
Epoch [79/120    avg_loss:0.040, val_acc:0.968]
Epoch [80/120    avg_loss:0.022, val_acc:0.963]
Epoch [81/120    avg_loss:0.037, val_acc:0.931]
Epoch [82/120    avg_loss:0.042, val_acc:0.950]
Epoch [83/120    avg_loss:0.019, val_acc:0.971]
Epoch [84/120    avg_loss:0.018, val_acc:0.970]
Epoch [85/120    avg_loss:0.019, val_acc:0.976]
Epoch [86/120    avg_loss:0.017, val_acc:0.971]
Epoch [87/120    avg_loss:0.012, val_acc:0.973]
Epoch [88/120    avg_loss:0.020, val_acc:0.956]
Epoch [89/120    avg_loss:0.015, val_acc:0.973]
Epoch [90/120    avg_loss:0.021, val_acc:0.975]
Epoch [91/120    avg_loss:0.011, val_acc:0.977]
Epoch [92/120    avg_loss:0.018, val_acc:0.968]
Epoch [93/120    avg_loss:0.017, val_acc:0.975]
Epoch [94/120    avg_loss:0.025, val_acc:0.965]
Epoch [95/120    avg_loss:0.019, val_acc:0.978]
Epoch [96/120    avg_loss:0.022, val_acc:0.969]
Epoch [97/120    avg_loss:0.011, val_acc:0.976]
Epoch [98/120    avg_loss:0.010, val_acc:0.978]
Epoch [99/120    avg_loss:0.014, val_acc:0.973]
Epoch [100/120    avg_loss:0.042, val_acc:0.961]
Epoch [101/120    avg_loss:0.025, val_acc:0.969]
Epoch [102/120    avg_loss:0.014, val_acc:0.969]
Epoch [103/120    avg_loss:0.012, val_acc:0.977]
Epoch [104/120    avg_loss:0.030, val_acc:0.961]
Epoch [105/120    avg_loss:0.014, val_acc:0.953]
Epoch [106/120    avg_loss:0.020, val_acc:0.968]
Epoch [107/120    avg_loss:0.010, val_acc:0.973]
Epoch [108/120    avg_loss:0.008, val_acc:0.975]
Epoch [109/120    avg_loss:0.009, val_acc:0.980]
Epoch [110/120    avg_loss:0.007, val_acc:0.975]
Epoch [111/120    avg_loss:0.013, val_acc:0.969]
Epoch [112/120    avg_loss:0.029, val_acc:0.967]
Epoch [113/120    avg_loss:0.014, val_acc:0.971]
Epoch [114/120    avg_loss:0.036, val_acc:0.960]
Epoch [115/120    avg_loss:0.009, val_acc:0.970]
Epoch [116/120    avg_loss:0.009, val_acc:0.968]
Epoch [117/120    avg_loss:0.008, val_acc:0.968]
Epoch [118/120    avg_loss:0.009, val_acc:0.971]
Epoch [119/120    avg_loss:0.018, val_acc:0.959]
Epoch [120/120    avg_loss:0.015, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    1    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1242    0   11    3    0    0    0    0    3   26    0    0
     0    0    0]
 [   0    0    0  734    2    0    1    0    0    1    0    2    7    0
     0    0    0]
 [   0    0    0    1  210    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    0    0
     0    2    0]
 [   0    0   11    0    0    0    0    0    0    0  844   15    4    0
     0    1    0]
 [   0    0   10    0    0    0    0    2    0    0    5 2192    1    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0    1  524    0
     0    5    2]
 [   0    0    0    1    0    0    0    0    0    0    0    1    0  183
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
   106  240    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.32249322493224

F1 scores:
[       nan 0.96202532 0.9744998  0.98855219 0.96330275 0.99427262
 0.99848024 0.96153846 1.         0.91428571 0.97685185 0.98583315
 0.97670084 0.99456522 0.94940978 0.78947368 0.98224852]

Kappa:
0.9694367402795132
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9f4a3e6a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.636, val_acc:0.451]
Epoch [2/120    avg_loss:2.308, val_acc:0.444]
Epoch [3/120    avg_loss:2.024, val_acc:0.535]
Epoch [4/120    avg_loss:1.789, val_acc:0.568]
Epoch [5/120    avg_loss:1.609, val_acc:0.595]
Epoch [6/120    avg_loss:1.412, val_acc:0.631]
Epoch [7/120    avg_loss:1.298, val_acc:0.624]
Epoch [8/120    avg_loss:1.207, val_acc:0.674]
Epoch [9/120    avg_loss:1.024, val_acc:0.680]
Epoch [10/120    avg_loss:0.906, val_acc:0.736]
Epoch [11/120    avg_loss:0.882, val_acc:0.602]
Epoch [12/120    avg_loss:0.847, val_acc:0.733]
Epoch [13/120    avg_loss:0.734, val_acc:0.793]
Epoch [14/120    avg_loss:0.758, val_acc:0.764]
Epoch [15/120    avg_loss:0.646, val_acc:0.793]
Epoch [16/120    avg_loss:0.524, val_acc:0.806]
Epoch [17/120    avg_loss:0.447, val_acc:0.783]
Epoch [18/120    avg_loss:0.470, val_acc:0.818]
Epoch [19/120    avg_loss:0.360, val_acc:0.823]
Epoch [20/120    avg_loss:0.341, val_acc:0.866]
Epoch [21/120    avg_loss:0.317, val_acc:0.805]
Epoch [22/120    avg_loss:0.385, val_acc:0.845]
Epoch [23/120    avg_loss:0.275, val_acc:0.883]
Epoch [24/120    avg_loss:0.274, val_acc:0.844]
Epoch [25/120    avg_loss:0.213, val_acc:0.895]
Epoch [26/120    avg_loss:0.212, val_acc:0.880]
Epoch [27/120    avg_loss:0.198, val_acc:0.849]
Epoch [28/120    avg_loss:0.200, val_acc:0.883]
Epoch [29/120    avg_loss:0.183, val_acc:0.875]
Epoch [30/120    avg_loss:0.173, val_acc:0.892]
Epoch [31/120    avg_loss:0.203, val_acc:0.851]
Epoch [32/120    avg_loss:0.212, val_acc:0.895]
Epoch [33/120    avg_loss:0.159, val_acc:0.872]
Epoch [34/120    avg_loss:0.197, val_acc:0.907]
Epoch [35/120    avg_loss:0.135, val_acc:0.923]
Epoch [36/120    avg_loss:0.154, val_acc:0.866]
Epoch [37/120    avg_loss:0.197, val_acc:0.912]
Epoch [38/120    avg_loss:0.126, val_acc:0.934]
Epoch [39/120    avg_loss:0.107, val_acc:0.943]
Epoch [40/120    avg_loss:0.091, val_acc:0.930]
Epoch [41/120    avg_loss:0.102, val_acc:0.942]
Epoch [42/120    avg_loss:0.099, val_acc:0.927]
Epoch [43/120    avg_loss:0.155, val_acc:0.930]
Epoch [44/120    avg_loss:0.073, val_acc:0.935]
Epoch [45/120    avg_loss:0.064, val_acc:0.944]
Epoch [46/120    avg_loss:0.074, val_acc:0.909]
Epoch [47/120    avg_loss:0.097, val_acc:0.932]
Epoch [48/120    avg_loss:0.098, val_acc:0.940]
Epoch [49/120    avg_loss:0.088, val_acc:0.940]
Epoch [50/120    avg_loss:0.076, val_acc:0.929]
Epoch [51/120    avg_loss:0.064, val_acc:0.921]
Epoch [52/120    avg_loss:0.068, val_acc:0.946]
Epoch [53/120    avg_loss:0.056, val_acc:0.944]
Epoch [54/120    avg_loss:0.061, val_acc:0.946]
Epoch [55/120    avg_loss:0.066, val_acc:0.952]
Epoch [56/120    avg_loss:0.073, val_acc:0.945]
Epoch [57/120    avg_loss:0.054, val_acc:0.959]
Epoch [58/120    avg_loss:0.099, val_acc:0.945]
Epoch [59/120    avg_loss:0.043, val_acc:0.951]
Epoch [60/120    avg_loss:0.039, val_acc:0.952]
Epoch [61/120    avg_loss:0.047, val_acc:0.964]
Epoch [62/120    avg_loss:0.048, val_acc:0.941]
Epoch [63/120    avg_loss:0.041, val_acc:0.942]
Epoch [64/120    avg_loss:0.043, val_acc:0.961]
Epoch [65/120    avg_loss:0.048, val_acc:0.961]
Epoch [66/120    avg_loss:0.026, val_acc:0.959]
Epoch [67/120    avg_loss:0.039, val_acc:0.956]
Epoch [68/120    avg_loss:0.027, val_acc:0.963]
Epoch [69/120    avg_loss:0.043, val_acc:0.946]
Epoch [70/120    avg_loss:0.028, val_acc:0.968]
Epoch [71/120    avg_loss:0.027, val_acc:0.959]
Epoch [72/120    avg_loss:0.033, val_acc:0.964]
Epoch [73/120    avg_loss:0.077, val_acc:0.966]
Epoch [74/120    avg_loss:0.035, val_acc:0.962]
Epoch [75/120    avg_loss:0.032, val_acc:0.971]
Epoch [76/120    avg_loss:0.017, val_acc:0.954]
Epoch [77/120    avg_loss:0.054, val_acc:0.950]
Epoch [78/120    avg_loss:0.083, val_acc:0.954]
Epoch [79/120    avg_loss:0.035, val_acc:0.962]
Epoch [80/120    avg_loss:0.031, val_acc:0.966]
Epoch [81/120    avg_loss:0.027, val_acc:0.962]
Epoch [82/120    avg_loss:0.036, val_acc:0.959]
Epoch [83/120    avg_loss:0.056, val_acc:0.955]
Epoch [84/120    avg_loss:0.154, val_acc:0.868]
Epoch [85/120    avg_loss:0.125, val_acc:0.889]
Epoch [86/120    avg_loss:0.073, val_acc:0.957]
Epoch [87/120    avg_loss:0.035, val_acc:0.957]
Epoch [88/120    avg_loss:0.031, val_acc:0.957]
Epoch [89/120    avg_loss:0.021, val_acc:0.961]
Epoch [90/120    avg_loss:0.050, val_acc:0.960]
Epoch [91/120    avg_loss:0.018, val_acc:0.964]
Epoch [92/120    avg_loss:0.015, val_acc:0.963]
Epoch [93/120    avg_loss:0.038, val_acc:0.965]
Epoch [94/120    avg_loss:0.018, val_acc:0.967]
Epoch [95/120    avg_loss:0.020, val_acc:0.966]
Epoch [96/120    avg_loss:0.014, val_acc:0.967]
Epoch [97/120    avg_loss:0.018, val_acc:0.967]
Epoch [98/120    avg_loss:0.016, val_acc:0.966]
Epoch [99/120    avg_loss:0.014, val_acc:0.970]
Epoch [100/120    avg_loss:0.013, val_acc:0.969]
Epoch [101/120    avg_loss:0.021, val_acc:0.966]
Epoch [102/120    avg_loss:0.014, val_acc:0.966]
Epoch [103/120    avg_loss:0.013, val_acc:0.967]
Epoch [104/120    avg_loss:0.013, val_acc:0.967]
Epoch [105/120    avg_loss:0.014, val_acc:0.967]
Epoch [106/120    avg_loss:0.014, val_acc:0.968]
Epoch [107/120    avg_loss:0.011, val_acc:0.968]
Epoch [108/120    avg_loss:0.016, val_acc:0.968]
Epoch [109/120    avg_loss:0.017, val_acc:0.968]
Epoch [110/120    avg_loss:0.015, val_acc:0.969]
Epoch [111/120    avg_loss:0.013, val_acc:0.969]
Epoch [112/120    avg_loss:0.010, val_acc:0.969]
Epoch [113/120    avg_loss:0.017, val_acc:0.969]
Epoch [114/120    avg_loss:0.010, val_acc:0.970]
Epoch [115/120    avg_loss:0.012, val_acc:0.970]
Epoch [116/120    avg_loss:0.014, val_acc:0.970]
Epoch [117/120    avg_loss:0.017, val_acc:0.970]
Epoch [118/120    avg_loss:0.013, val_acc:0.970]
Epoch [119/120    avg_loss:0.017, val_acc:0.970]
Epoch [120/120    avg_loss:0.015, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1265    3    8    0    0    0    0    1    1    7    0    0
     0    0    0]
 [   0    0    0  729   13    0    0    0    0    1    1    1    2    0
     0    0    0]
 [   0    0    0    0  210    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    0    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   16    0    1    0    0
     0    0    0]
 [   0    0   10    0    0    1    0    0    0    0  848   15    0    0
     1    0    0]
 [   0    0   21    0    0    1    2    0    0    0   35 2137    8    0
     5    1    0]
 [   0    0    0    3    3    1    0    0    0    0    3    1  520    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1127   12    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    77  270    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.21409214092141

F1 scores:
[       nan 0.975      0.97948122 0.98380567 0.93959732 0.98845266
 0.99696049 1.         1.         0.88888889 0.9619966  0.97758463
 0.97378277 1.         0.95630038 0.85578447 0.98224852]

Kappa:
0.9682444151284486
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6398ddca90>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.639, val_acc:0.430]
Epoch [2/120    avg_loss:2.267, val_acc:0.528]
Epoch [3/120    avg_loss:1.998, val_acc:0.564]
Epoch [4/120    avg_loss:1.752, val_acc:0.593]
Epoch [5/120    avg_loss:1.567, val_acc:0.625]
Epoch [6/120    avg_loss:1.424, val_acc:0.650]
Epoch [7/120    avg_loss:1.157, val_acc:0.690]
Epoch [8/120    avg_loss:1.037, val_acc:0.673]
Epoch [9/120    avg_loss:0.953, val_acc:0.731]
Epoch [10/120    avg_loss:0.833, val_acc:0.832]
Epoch [11/120    avg_loss:0.691, val_acc:0.802]
Epoch [12/120    avg_loss:0.600, val_acc:0.848]
Epoch [13/120    avg_loss:0.511, val_acc:0.801]
Epoch [14/120    avg_loss:0.471, val_acc:0.853]
Epoch [15/120    avg_loss:0.409, val_acc:0.865]
Epoch [16/120    avg_loss:0.348, val_acc:0.883]
Epoch [17/120    avg_loss:0.365, val_acc:0.847]
Epoch [18/120    avg_loss:0.352, val_acc:0.896]
Epoch [19/120    avg_loss:0.334, val_acc:0.882]
Epoch [20/120    avg_loss:0.302, val_acc:0.891]
Epoch [21/120    avg_loss:0.303, val_acc:0.928]
Epoch [22/120    avg_loss:0.227, val_acc:0.909]
Epoch [23/120    avg_loss:0.226, val_acc:0.942]
Epoch [24/120    avg_loss:0.210, val_acc:0.912]
Epoch [25/120    avg_loss:0.182, val_acc:0.905]
Epoch [26/120    avg_loss:0.290, val_acc:0.879]
Epoch [27/120    avg_loss:0.257, val_acc:0.886]
Epoch [28/120    avg_loss:0.286, val_acc:0.915]
Epoch [29/120    avg_loss:0.191, val_acc:0.916]
Epoch [30/120    avg_loss:0.139, val_acc:0.949]
Epoch [31/120    avg_loss:0.127, val_acc:0.943]
Epoch [32/120    avg_loss:0.146, val_acc:0.952]
Epoch [33/120    avg_loss:0.105, val_acc:0.958]
Epoch [34/120    avg_loss:0.087, val_acc:0.966]
Epoch [35/120    avg_loss:0.125, val_acc:0.909]
Epoch [36/120    avg_loss:0.104, val_acc:0.931]
Epoch [37/120    avg_loss:0.110, val_acc:0.938]
Epoch [38/120    avg_loss:0.107, val_acc:0.906]
Epoch [39/120    avg_loss:0.140, val_acc:0.930]
Epoch [40/120    avg_loss:0.073, val_acc:0.958]
Epoch [41/120    avg_loss:0.089, val_acc:0.968]
Epoch [42/120    avg_loss:0.063, val_acc:0.958]
Epoch [43/120    avg_loss:0.073, val_acc:0.971]
Epoch [44/120    avg_loss:0.063, val_acc:0.965]
Epoch [45/120    avg_loss:0.056, val_acc:0.951]
Epoch [46/120    avg_loss:0.085, val_acc:0.964]
Epoch [47/120    avg_loss:0.071, val_acc:0.961]
Epoch [48/120    avg_loss:0.054, val_acc:0.974]
Epoch [49/120    avg_loss:0.045, val_acc:0.973]
Epoch [50/120    avg_loss:0.058, val_acc:0.968]
Epoch [51/120    avg_loss:0.045, val_acc:0.978]
Epoch [52/120    avg_loss:0.039, val_acc:0.963]
Epoch [53/120    avg_loss:0.054, val_acc:0.953]
Epoch [54/120    avg_loss:0.057, val_acc:0.964]
Epoch [55/120    avg_loss:0.035, val_acc:0.976]
Epoch [56/120    avg_loss:0.031, val_acc:0.963]
Epoch [57/120    avg_loss:0.160, val_acc:0.941]
Epoch [58/120    avg_loss:0.093, val_acc:0.966]
Epoch [59/120    avg_loss:0.048, val_acc:0.966]
Epoch [60/120    avg_loss:0.033, val_acc:0.961]
Epoch [61/120    avg_loss:0.072, val_acc:0.968]
Epoch [62/120    avg_loss:0.046, val_acc:0.963]
Epoch [63/120    avg_loss:0.093, val_acc:0.964]
Epoch [64/120    avg_loss:0.078, val_acc:0.942]
Epoch [65/120    avg_loss:0.046, val_acc:0.973]
Epoch [66/120    avg_loss:0.033, val_acc:0.975]
Epoch [67/120    avg_loss:0.026, val_acc:0.978]
Epoch [68/120    avg_loss:0.029, val_acc:0.978]
Epoch [69/120    avg_loss:0.028, val_acc:0.977]
Epoch [70/120    avg_loss:0.027, val_acc:0.978]
Epoch [71/120    avg_loss:0.033, val_acc:0.978]
Epoch [72/120    avg_loss:0.023, val_acc:0.979]
Epoch [73/120    avg_loss:0.025, val_acc:0.979]
Epoch [74/120    avg_loss:0.019, val_acc:0.979]
Epoch [75/120    avg_loss:0.022, val_acc:0.980]
Epoch [76/120    avg_loss:0.019, val_acc:0.981]
Epoch [77/120    avg_loss:0.025, val_acc:0.981]
Epoch [78/120    avg_loss:0.020, val_acc:0.981]
Epoch [79/120    avg_loss:0.024, val_acc:0.979]
Epoch [80/120    avg_loss:0.019, val_acc:0.980]
Epoch [81/120    avg_loss:0.019, val_acc:0.982]
Epoch [82/120    avg_loss:0.016, val_acc:0.981]
Epoch [83/120    avg_loss:0.017, val_acc:0.980]
Epoch [84/120    avg_loss:0.018, val_acc:0.981]
Epoch [85/120    avg_loss:0.016, val_acc:0.983]
Epoch [86/120    avg_loss:0.014, val_acc:0.982]
Epoch [87/120    avg_loss:0.016, val_acc:0.981]
Epoch [88/120    avg_loss:0.019, val_acc:0.983]
Epoch [89/120    avg_loss:0.016, val_acc:0.982]
Epoch [90/120    avg_loss:0.015, val_acc:0.983]
Epoch [91/120    avg_loss:0.015, val_acc:0.981]
Epoch [92/120    avg_loss:0.014, val_acc:0.982]
Epoch [93/120    avg_loss:0.015, val_acc:0.982]
Epoch [94/120    avg_loss:0.015, val_acc:0.981]
Epoch [95/120    avg_loss:0.015, val_acc:0.983]
Epoch [96/120    avg_loss:0.015, val_acc:0.983]
Epoch [97/120    avg_loss:0.013, val_acc:0.982]
Epoch [98/120    avg_loss:0.015, val_acc:0.981]
Epoch [99/120    avg_loss:0.015, val_acc:0.981]
Epoch [100/120    avg_loss:0.013, val_acc:0.981]
Epoch [101/120    avg_loss:0.014, val_acc:0.984]
Epoch [102/120    avg_loss:0.016, val_acc:0.981]
Epoch [103/120    avg_loss:0.015, val_acc:0.981]
Epoch [104/120    avg_loss:0.017, val_acc:0.981]
Epoch [105/120    avg_loss:0.012, val_acc:0.980]
Epoch [106/120    avg_loss:0.012, val_acc:0.982]
Epoch [107/120    avg_loss:0.011, val_acc:0.982]
Epoch [108/120    avg_loss:0.013, val_acc:0.981]
Epoch [109/120    avg_loss:0.013, val_acc:0.981]
Epoch [110/120    avg_loss:0.013, val_acc:0.982]
Epoch [111/120    avg_loss:0.010, val_acc:0.982]
Epoch [112/120    avg_loss:0.014, val_acc:0.982]
Epoch [113/120    avg_loss:0.012, val_acc:0.983]
Epoch [114/120    avg_loss:0.009, val_acc:0.984]
Epoch [115/120    avg_loss:0.014, val_acc:0.984]
Epoch [116/120    avg_loss:0.013, val_acc:0.981]
Epoch [117/120    avg_loss:0.012, val_acc:0.980]
Epoch [118/120    avg_loss:0.015, val_acc:0.981]
Epoch [119/120    avg_loss:0.012, val_acc:0.981]
Epoch [120/120    avg_loss:0.015, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1251    1    8    1    2    0    0    1    4   17    0    0
     0    0    0]
 [   0    0    0  725    1    1    0    0    0    4    0    0   16    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    6    0    0    3    1    0    0    0  846   18    0    0
     0    1    0]
 [   0    0    9    1    0    0    1    0    0    0   30 2167    2    0
     0    0    0]
 [   0    0    1    2    0    0    0    0    0    0    2    0  527    0
     0    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1132    7    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    81  266    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.46341463414635

F1 scores:
[       nan 0.98765432 0.98040752 0.98238482 0.97695853 0.992
 0.99467681 1.         0.99649942 0.85       0.96245734 0.98209835
 0.97232472 0.99728997 0.96135881 0.85668277 0.98823529]

Kappa:
0.9710693334049547
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdb0e9eba90>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.656, val_acc:0.480]
Epoch [2/120    avg_loss:2.293, val_acc:0.562]
Epoch [3/120    avg_loss:1.979, val_acc:0.582]
Epoch [4/120    avg_loss:1.693, val_acc:0.601]
Epoch [5/120    avg_loss:1.436, val_acc:0.658]
Epoch [6/120    avg_loss:1.248, val_acc:0.700]
Epoch [7/120    avg_loss:1.107, val_acc:0.743]
Epoch [8/120    avg_loss:0.971, val_acc:0.729]
Epoch [9/120    avg_loss:0.944, val_acc:0.750]
Epoch [10/120    avg_loss:0.808, val_acc:0.753]
Epoch [11/120    avg_loss:0.788, val_acc:0.743]
Epoch [12/120    avg_loss:0.682, val_acc:0.821]
Epoch [13/120    avg_loss:0.577, val_acc:0.769]
Epoch [14/120    avg_loss:0.543, val_acc:0.826]
Epoch [15/120    avg_loss:0.502, val_acc:0.853]
Epoch [16/120    avg_loss:0.403, val_acc:0.820]
Epoch [17/120    avg_loss:0.445, val_acc:0.869]
Epoch [18/120    avg_loss:0.368, val_acc:0.884]
Epoch [19/120    avg_loss:0.337, val_acc:0.869]
Epoch [20/120    avg_loss:0.320, val_acc:0.879]
Epoch [21/120    avg_loss:0.273, val_acc:0.891]
Epoch [22/120    avg_loss:0.236, val_acc:0.933]
Epoch [23/120    avg_loss:0.191, val_acc:0.912]
Epoch [24/120    avg_loss:0.239, val_acc:0.912]
Epoch [25/120    avg_loss:0.167, val_acc:0.939]
Epoch [26/120    avg_loss:0.144, val_acc:0.936]
Epoch [27/120    avg_loss:0.175, val_acc:0.921]
Epoch [28/120    avg_loss:0.203, val_acc:0.902]
Epoch [29/120    avg_loss:0.152, val_acc:0.921]
Epoch [30/120    avg_loss:0.108, val_acc:0.941]
Epoch [31/120    avg_loss:0.109, val_acc:0.939]
Epoch [32/120    avg_loss:0.116, val_acc:0.947]
Epoch [33/120    avg_loss:0.092, val_acc:0.939]
Epoch [34/120    avg_loss:0.135, val_acc:0.926]
Epoch [35/120    avg_loss:0.156, val_acc:0.933]
Epoch [36/120    avg_loss:0.082, val_acc:0.961]
Epoch [37/120    avg_loss:0.088, val_acc:0.955]
Epoch [38/120    avg_loss:0.107, val_acc:0.943]
Epoch [39/120    avg_loss:0.188, val_acc:0.942]
Epoch [40/120    avg_loss:0.120, val_acc:0.949]
Epoch [41/120    avg_loss:0.117, val_acc:0.928]
Epoch [42/120    avg_loss:0.095, val_acc:0.956]
Epoch [43/120    avg_loss:0.062, val_acc:0.953]
Epoch [44/120    avg_loss:0.099, val_acc:0.964]
Epoch [45/120    avg_loss:0.075, val_acc:0.963]
Epoch [46/120    avg_loss:0.052, val_acc:0.970]
Epoch [47/120    avg_loss:0.044, val_acc:0.970]
Epoch [48/120    avg_loss:0.048, val_acc:0.968]
Epoch [49/120    avg_loss:0.055, val_acc:0.970]
Epoch [50/120    avg_loss:0.084, val_acc:0.958]
Epoch [51/120    avg_loss:0.038, val_acc:0.973]
Epoch [52/120    avg_loss:0.031, val_acc:0.965]
Epoch [53/120    avg_loss:0.051, val_acc:0.961]
Epoch [54/120    avg_loss:0.043, val_acc:0.955]
Epoch [55/120    avg_loss:0.036, val_acc:0.965]
Epoch [56/120    avg_loss:0.273, val_acc:0.907]
Epoch [57/120    avg_loss:0.212, val_acc:0.916]
Epoch [58/120    avg_loss:0.116, val_acc:0.954]
Epoch [59/120    avg_loss:0.053, val_acc:0.966]
Epoch [60/120    avg_loss:0.053, val_acc:0.953]
Epoch [61/120    avg_loss:0.061, val_acc:0.976]
Epoch [62/120    avg_loss:0.048, val_acc:0.974]
Epoch [63/120    avg_loss:0.048, val_acc:0.968]
Epoch [64/120    avg_loss:0.043, val_acc:0.974]
Epoch [65/120    avg_loss:0.040, val_acc:0.964]
Epoch [66/120    avg_loss:0.030, val_acc:0.967]
Epoch [67/120    avg_loss:0.029, val_acc:0.967]
Epoch [68/120    avg_loss:0.037, val_acc:0.974]
Epoch [69/120    avg_loss:0.036, val_acc:0.975]
Epoch [70/120    avg_loss:0.035, val_acc:0.970]
Epoch [71/120    avg_loss:0.045, val_acc:0.976]
Epoch [72/120    avg_loss:0.026, val_acc:0.974]
Epoch [73/120    avg_loss:0.035, val_acc:0.979]
Epoch [74/120    avg_loss:0.042, val_acc:0.962]
Epoch [75/120    avg_loss:0.026, val_acc:0.979]
Epoch [76/120    avg_loss:0.023, val_acc:0.979]
Epoch [77/120    avg_loss:0.034, val_acc:0.975]
Epoch [78/120    avg_loss:0.014, val_acc:0.980]
Epoch [79/120    avg_loss:0.016, val_acc:0.975]
Epoch [80/120    avg_loss:0.031, val_acc:0.981]
Epoch [81/120    avg_loss:0.018, val_acc:0.973]
Epoch [82/120    avg_loss:0.024, val_acc:0.983]
Epoch [83/120    avg_loss:0.024, val_acc:0.973]
Epoch [84/120    avg_loss:0.017, val_acc:0.978]
Epoch [85/120    avg_loss:0.026, val_acc:0.976]
Epoch [86/120    avg_loss:0.016, val_acc:0.979]
Epoch [87/120    avg_loss:0.028, val_acc:0.976]
Epoch [88/120    avg_loss:0.033, val_acc:0.959]
Epoch [89/120    avg_loss:0.038, val_acc:0.974]
Epoch [90/120    avg_loss:0.015, val_acc:0.982]
Epoch [91/120    avg_loss:0.018, val_acc:0.980]
Epoch [92/120    avg_loss:0.013, val_acc:0.983]
Epoch [93/120    avg_loss:0.010, val_acc:0.979]
Epoch [94/120    avg_loss:0.011, val_acc:0.982]
Epoch [95/120    avg_loss:0.011, val_acc:0.980]
Epoch [96/120    avg_loss:0.017, val_acc:0.980]
Epoch [97/120    avg_loss:0.013, val_acc:0.980]
Epoch [98/120    avg_loss:0.010, val_acc:0.982]
Epoch [99/120    avg_loss:0.012, val_acc:0.958]
Epoch [100/120    avg_loss:0.021, val_acc:0.982]
Epoch [101/120    avg_loss:0.010, val_acc:0.982]
Epoch [102/120    avg_loss:0.009, val_acc:0.976]
Epoch [103/120    avg_loss:0.010, val_acc:0.980]
Epoch [104/120    avg_loss:0.007, val_acc:0.983]
Epoch [105/120    avg_loss:0.012, val_acc:0.982]
Epoch [106/120    avg_loss:0.020, val_acc:0.981]
Epoch [107/120    avg_loss:0.009, val_acc:0.980]
Epoch [108/120    avg_loss:0.007, val_acc:0.980]
Epoch [109/120    avg_loss:0.006, val_acc:0.981]
Epoch [110/120    avg_loss:0.009, val_acc:0.976]
Epoch [111/120    avg_loss:0.018, val_acc:0.978]
Epoch [112/120    avg_loss:0.008, val_acc:0.980]
Epoch [113/120    avg_loss:0.009, val_acc:0.981]
Epoch [114/120    avg_loss:0.009, val_acc:0.981]
Epoch [115/120    avg_loss:0.008, val_acc:0.976]
Epoch [116/120    avg_loss:0.212, val_acc:0.945]
Epoch [117/120    avg_loss:0.111, val_acc:0.963]
Epoch [118/120    avg_loss:0.042, val_acc:0.978]
Epoch [119/120    avg_loss:0.022, val_acc:0.979]
Epoch [120/120    avg_loss:0.032, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1254    1    6    0    0    0    0    0   17    5    2    0
     0    0    0]
 [   0    0    2  738    1    0    0    0    0    3    1    1    1    0
     0    0    0]
 [   0    0    0    0  208    0    0    0    0    0    0    0    5    0
     0    0    0]
 [   0    0    0    0    0  428    0    2    0    0    0    4    0    0
     1    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    4    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  428    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   14    0    0    0    0    0    0    1  843   15    1    0
     0    1    0]
 [   0    0   19    1    0    0    0    1    0    0   13 2164   11    0
     0    0    1]
 [   0    0    0    1    0    0    0    0    0    0    1    0  522    0
     0    0   10]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
  1131    5    1]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
   113  232    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.02981029810299

F1 scores:
[       nan 0.96296296 0.97360248 0.99193548 0.97196262 0.98731257
 0.99618029 0.94339623 0.997669   0.9        0.96342857 0.98296616
 0.96935933 1.         0.94842767 0.79316239 0.93333333]

Kappa:
0.9661141680059993
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fca534c0a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.666, val_acc:0.421]
Epoch [2/120    avg_loss:2.312, val_acc:0.522]
Epoch [3/120    avg_loss:2.023, val_acc:0.514]
Epoch [4/120    avg_loss:1.795, val_acc:0.550]
Epoch [5/120    avg_loss:1.596, val_acc:0.588]
Epoch [6/120    avg_loss:1.375, val_acc:0.646]
Epoch [7/120    avg_loss:1.190, val_acc:0.657]
Epoch [8/120    avg_loss:1.043, val_acc:0.717]
Epoch [9/120    avg_loss:0.940, val_acc:0.724]
Epoch [10/120    avg_loss:0.820, val_acc:0.739]
Epoch [11/120    avg_loss:0.793, val_acc:0.719]
Epoch [12/120    avg_loss:0.671, val_acc:0.801]
Epoch [13/120    avg_loss:0.545, val_acc:0.779]
Epoch [14/120    avg_loss:0.508, val_acc:0.819]
Epoch [15/120    avg_loss:0.513, val_acc:0.844]
Epoch [16/120    avg_loss:0.468, val_acc:0.849]
Epoch [17/120    avg_loss:0.442, val_acc:0.765]
Epoch [18/120    avg_loss:0.390, val_acc:0.839]
Epoch [19/120    avg_loss:0.350, val_acc:0.850]
Epoch [20/120    avg_loss:0.303, val_acc:0.890]
Epoch [21/120    avg_loss:0.307, val_acc:0.809]
Epoch [22/120    avg_loss:0.334, val_acc:0.878]
Epoch [23/120    avg_loss:0.282, val_acc:0.874]
Epoch [24/120    avg_loss:0.246, val_acc:0.897]
Epoch [25/120    avg_loss:0.215, val_acc:0.901]
Epoch [26/120    avg_loss:0.248, val_acc:0.907]
Epoch [27/120    avg_loss:0.183, val_acc:0.909]
Epoch [28/120    avg_loss:0.173, val_acc:0.914]
Epoch [29/120    avg_loss:0.167, val_acc:0.923]
Epoch [30/120    avg_loss:0.134, val_acc:0.897]
Epoch [31/120    avg_loss:0.156, val_acc:0.930]
Epoch [32/120    avg_loss:0.142, val_acc:0.912]
Epoch [33/120    avg_loss:0.168, val_acc:0.938]
Epoch [34/120    avg_loss:0.102, val_acc:0.939]
Epoch [35/120    avg_loss:0.115, val_acc:0.939]
Epoch [36/120    avg_loss:0.110, val_acc:0.920]
Epoch [37/120    avg_loss:0.144, val_acc:0.930]
Epoch [38/120    avg_loss:0.109, val_acc:0.951]
Epoch [39/120    avg_loss:0.098, val_acc:0.948]
Epoch [40/120    avg_loss:0.119, val_acc:0.934]
Epoch [41/120    avg_loss:0.097, val_acc:0.932]
Epoch [42/120    avg_loss:0.084, val_acc:0.923]
Epoch [43/120    avg_loss:0.124, val_acc:0.911]
Epoch [44/120    avg_loss:0.077, val_acc:0.953]
Epoch [45/120    avg_loss:0.125, val_acc:0.883]
Epoch [46/120    avg_loss:0.106, val_acc:0.947]
Epoch [47/120    avg_loss:0.071, val_acc:0.946]
Epoch [48/120    avg_loss:0.109, val_acc:0.950]
Epoch [49/120    avg_loss:0.091, val_acc:0.922]
Epoch [50/120    avg_loss:0.076, val_acc:0.959]
Epoch [51/120    avg_loss:0.067, val_acc:0.951]
Epoch [52/120    avg_loss:0.076, val_acc:0.949]
Epoch [53/120    avg_loss:0.085, val_acc:0.924]
Epoch [54/120    avg_loss:0.058, val_acc:0.954]
Epoch [55/120    avg_loss:0.087, val_acc:0.947]
Epoch [56/120    avg_loss:0.094, val_acc:0.955]
Epoch [57/120    avg_loss:0.065, val_acc:0.959]
Epoch [58/120    avg_loss:0.061, val_acc:0.951]
Epoch [59/120    avg_loss:0.048, val_acc:0.930]
Epoch [60/120    avg_loss:0.062, val_acc:0.935]
Epoch [61/120    avg_loss:0.065, val_acc:0.954]
Epoch [62/120    avg_loss:0.048, val_acc:0.960]
Epoch [63/120    avg_loss:0.042, val_acc:0.954]
Epoch [64/120    avg_loss:0.222, val_acc:0.932]
Epoch [65/120    avg_loss:0.069, val_acc:0.965]
Epoch [66/120    avg_loss:0.040, val_acc:0.967]
Epoch [67/120    avg_loss:0.044, val_acc:0.965]
Epoch [68/120    avg_loss:0.040, val_acc:0.955]
Epoch [69/120    avg_loss:0.040, val_acc:0.970]
Epoch [70/120    avg_loss:0.049, val_acc:0.959]
Epoch [71/120    avg_loss:0.039, val_acc:0.960]
Epoch [72/120    avg_loss:0.036, val_acc:0.963]
Epoch [73/120    avg_loss:0.031, val_acc:0.967]
Epoch [74/120    avg_loss:0.039, val_acc:0.965]
Epoch [75/120    avg_loss:0.027, val_acc:0.972]
Epoch [76/120    avg_loss:0.029, val_acc:0.975]
Epoch [77/120    avg_loss:0.020, val_acc:0.974]
Epoch [78/120    avg_loss:0.021, val_acc:0.972]
Epoch [79/120    avg_loss:0.027, val_acc:0.961]
Epoch [80/120    avg_loss:0.043, val_acc:0.967]
Epoch [81/120    avg_loss:0.037, val_acc:0.963]
Epoch [82/120    avg_loss:0.030, val_acc:0.969]
Epoch [83/120    avg_loss:0.019, val_acc:0.974]
Epoch [84/120    avg_loss:0.017, val_acc:0.971]
Epoch [85/120    avg_loss:0.017, val_acc:0.974]
Epoch [86/120    avg_loss:0.016, val_acc:0.975]
Epoch [87/120    avg_loss:0.017, val_acc:0.964]
Epoch [88/120    avg_loss:0.017, val_acc:0.957]
Epoch [89/120    avg_loss:0.029, val_acc:0.967]
Epoch [90/120    avg_loss:0.015, val_acc:0.972]
Epoch [91/120    avg_loss:0.010, val_acc:0.976]
Epoch [92/120    avg_loss:0.018, val_acc:0.972]
Epoch [93/120    avg_loss:0.015, val_acc:0.975]
Epoch [94/120    avg_loss:0.015, val_acc:0.965]
Epoch [95/120    avg_loss:0.024, val_acc:0.969]
Epoch [96/120    avg_loss:0.024, val_acc:0.974]
Epoch [97/120    avg_loss:0.069, val_acc:0.952]
Epoch [98/120    avg_loss:0.027, val_acc:0.961]
Epoch [99/120    avg_loss:0.035, val_acc:0.968]
Epoch [100/120    avg_loss:0.068, val_acc:0.931]
Epoch [101/120    avg_loss:0.038, val_acc:0.956]
Epoch [102/120    avg_loss:0.035, val_acc:0.976]
Epoch [103/120    avg_loss:0.026, val_acc:0.942]
Epoch [104/120    avg_loss:0.096, val_acc:0.945]
Epoch [105/120    avg_loss:0.049, val_acc:0.961]
Epoch [106/120    avg_loss:0.024, val_acc:0.974]
Epoch [107/120    avg_loss:0.027, val_acc:0.976]
Epoch [108/120    avg_loss:0.018, val_acc:0.977]
Epoch [109/120    avg_loss:0.017, val_acc:0.971]
Epoch [110/120    avg_loss:0.087, val_acc:0.935]
Epoch [111/120    avg_loss:0.068, val_acc:0.971]
Epoch [112/120    avg_loss:0.030, val_acc:0.975]
Epoch [113/120    avg_loss:0.020, val_acc:0.974]
Epoch [114/120    avg_loss:0.017, val_acc:0.970]
Epoch [115/120    avg_loss:0.030, val_acc:0.971]
Epoch [116/120    avg_loss:0.023, val_acc:0.976]
Epoch [117/120    avg_loss:0.027, val_acc:0.964]
Epoch [118/120    avg_loss:0.017, val_acc:0.977]
Epoch [119/120    avg_loss:0.063, val_acc:0.907]
Epoch [120/120    avg_loss:0.048, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1238   17    5    0    2    0    0    0    2   18    3    0
     0    0    0]
 [   0    0    0  728    6    0    0    0    0    5    0    3    0    4
     1    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    7    3    0    1    2    0    0    4  811   45    2    0
     0    0    0]
 [   0    0   13   21    0    0    6    0    0    3    1 2139   20    1
     0    5    1]
 [   0    0    0    3    1    1    0    0    0    0    0    0  521    0
     4    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    2    1    0    0    0    0    0
  1116   17    0]
 [   0    0    0    0    0    1   25    0    0    0    0    0    0    0
    20  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
96.78048780487805

F1 scores:
[       nan 0.96470588 0.97365317 0.95852535 0.97025172 0.98855835
 0.97181009 0.94117647 0.995338   0.72340426 0.96033156 0.96896942
 0.96125461 0.98666667 0.97680525 0.89716841 0.95857988]

Kappa:
0.9633167493473813
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fba19e0aa58>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.639, val_acc:0.432]
Epoch [2/120    avg_loss:2.273, val_acc:0.535]
Epoch [3/120    avg_loss:2.010, val_acc:0.584]
Epoch [4/120    avg_loss:1.786, val_acc:0.593]
Epoch [5/120    avg_loss:1.590, val_acc:0.633]
Epoch [6/120    avg_loss:1.359, val_acc:0.624]
Epoch [7/120    avg_loss:1.203, val_acc:0.631]
Epoch [8/120    avg_loss:1.042, val_acc:0.727]
Epoch [9/120    avg_loss:1.060, val_acc:0.684]
Epoch [10/120    avg_loss:0.954, val_acc:0.731]
Epoch [11/120    avg_loss:0.890, val_acc:0.776]
Epoch [12/120    avg_loss:0.765, val_acc:0.824]
Epoch [13/120    avg_loss:0.612, val_acc:0.831]
Epoch [14/120    avg_loss:0.568, val_acc:0.833]
Epoch [15/120    avg_loss:0.560, val_acc:0.819]
Epoch [16/120    avg_loss:0.467, val_acc:0.859]
Epoch [17/120    avg_loss:0.450, val_acc:0.874]
Epoch [18/120    avg_loss:0.368, val_acc:0.890]
Epoch [19/120    avg_loss:0.324, val_acc:0.915]
Epoch [20/120    avg_loss:0.290, val_acc:0.897]
Epoch [21/120    avg_loss:0.302, val_acc:0.897]
Epoch [22/120    avg_loss:0.256, val_acc:0.930]
Epoch [23/120    avg_loss:0.209, val_acc:0.915]
Epoch [24/120    avg_loss:0.183, val_acc:0.926]
Epoch [25/120    avg_loss:0.180, val_acc:0.945]
Epoch [26/120    avg_loss:0.187, val_acc:0.930]
Epoch [27/120    avg_loss:0.156, val_acc:0.946]
Epoch [28/120    avg_loss:0.140, val_acc:0.940]
Epoch [29/120    avg_loss:0.121, val_acc:0.943]
Epoch [30/120    avg_loss:0.322, val_acc:0.798]
Epoch [31/120    avg_loss:0.289, val_acc:0.904]
Epoch [32/120    avg_loss:0.214, val_acc:0.920]
Epoch [33/120    avg_loss:0.161, val_acc:0.941]
Epoch [34/120    avg_loss:0.116, val_acc:0.957]
Epoch [35/120    avg_loss:0.114, val_acc:0.955]
Epoch [36/120    avg_loss:0.084, val_acc:0.956]
Epoch [37/120    avg_loss:0.079, val_acc:0.947]
Epoch [38/120    avg_loss:0.097, val_acc:0.970]
Epoch [39/120    avg_loss:0.091, val_acc:0.970]
Epoch [40/120    avg_loss:0.074, val_acc:0.961]
Epoch [41/120    avg_loss:0.091, val_acc:0.951]
Epoch [42/120    avg_loss:0.089, val_acc:0.961]
Epoch [43/120    avg_loss:0.073, val_acc:0.974]
Epoch [44/120    avg_loss:0.056, val_acc:0.963]
Epoch [45/120    avg_loss:0.099, val_acc:0.963]
Epoch [46/120    avg_loss:0.067, val_acc:0.965]
Epoch [47/120    avg_loss:0.056, val_acc:0.966]
Epoch [48/120    avg_loss:0.076, val_acc:0.961]
Epoch [49/120    avg_loss:0.097, val_acc:0.926]
Epoch [50/120    avg_loss:0.058, val_acc:0.965]
Epoch [51/120    avg_loss:0.067, val_acc:0.966]
Epoch [52/120    avg_loss:0.068, val_acc:0.965]
Epoch [53/120    avg_loss:0.076, val_acc:0.970]
Epoch [54/120    avg_loss:0.059, val_acc:0.955]
Epoch [55/120    avg_loss:0.043, val_acc:0.943]
Epoch [56/120    avg_loss:0.055, val_acc:0.974]
Epoch [57/120    avg_loss:0.030, val_acc:0.974]
Epoch [58/120    avg_loss:0.033, val_acc:0.963]
Epoch [59/120    avg_loss:0.045, val_acc:0.976]
Epoch [60/120    avg_loss:0.023, val_acc:0.976]
Epoch [61/120    avg_loss:0.029, val_acc:0.978]
Epoch [62/120    avg_loss:0.028, val_acc:0.975]
Epoch [63/120    avg_loss:0.032, val_acc:0.967]
Epoch [64/120    avg_loss:0.051, val_acc:0.976]
Epoch [65/120    avg_loss:0.054, val_acc:0.975]
Epoch [66/120    avg_loss:0.028, val_acc:0.943]
Epoch [67/120    avg_loss:0.121, val_acc:0.912]
Epoch [68/120    avg_loss:0.093, val_acc:0.959]
Epoch [69/120    avg_loss:0.087, val_acc:0.967]
Epoch [70/120    avg_loss:0.027, val_acc:0.978]
Epoch [71/120    avg_loss:0.028, val_acc:0.975]
Epoch [72/120    avg_loss:0.021, val_acc:0.975]
Epoch [73/120    avg_loss:0.028, val_acc:0.974]
Epoch [74/120    avg_loss:0.040, val_acc:0.974]
Epoch [75/120    avg_loss:0.023, val_acc:0.970]
Epoch [76/120    avg_loss:0.018, val_acc:0.981]
Epoch [77/120    avg_loss:0.019, val_acc:0.979]
Epoch [78/120    avg_loss:0.021, val_acc:0.983]
Epoch [79/120    avg_loss:0.014, val_acc:0.973]
Epoch [80/120    avg_loss:0.018, val_acc:0.980]
Epoch [81/120    avg_loss:0.020, val_acc:0.979]
Epoch [82/120    avg_loss:0.017, val_acc:0.979]
Epoch [83/120    avg_loss:0.035, val_acc:0.965]
Epoch [84/120    avg_loss:0.017, val_acc:0.979]
Epoch [85/120    avg_loss:0.033, val_acc:0.972]
Epoch [86/120    avg_loss:0.060, val_acc:0.974]
Epoch [87/120    avg_loss:0.015, val_acc:0.983]
Epoch [88/120    avg_loss:0.023, val_acc:0.974]
Epoch [89/120    avg_loss:0.099, val_acc:0.936]
Epoch [90/120    avg_loss:0.048, val_acc:0.969]
Epoch [91/120    avg_loss:0.037, val_acc:0.973]
Epoch [92/120    avg_loss:0.020, val_acc:0.983]
Epoch [93/120    avg_loss:0.013, val_acc:0.977]
Epoch [94/120    avg_loss:0.019, val_acc:0.985]
Epoch [95/120    avg_loss:0.017, val_acc:0.972]
Epoch [96/120    avg_loss:0.011, val_acc:0.981]
Epoch [97/120    avg_loss:0.015, val_acc:0.973]
Epoch [98/120    avg_loss:0.023, val_acc:0.979]
Epoch [99/120    avg_loss:0.012, val_acc:0.981]
Epoch [100/120    avg_loss:0.011, val_acc:0.983]
Epoch [101/120    avg_loss:0.017, val_acc:0.982]
Epoch [102/120    avg_loss:0.010, val_acc:0.978]
Epoch [103/120    avg_loss:0.018, val_acc:0.982]
Epoch [104/120    avg_loss:0.009, val_acc:0.985]
Epoch [105/120    avg_loss:0.014, val_acc:0.973]
Epoch [106/120    avg_loss:0.015, val_acc:0.982]
Epoch [107/120    avg_loss:0.009, val_acc:0.983]
Epoch [108/120    avg_loss:0.008, val_acc:0.982]
Epoch [109/120    avg_loss:0.006, val_acc:0.984]
Epoch [110/120    avg_loss:0.008, val_acc:0.984]
Epoch [111/120    avg_loss:0.011, val_acc:0.980]
Epoch [112/120    avg_loss:0.010, val_acc:0.979]
Epoch [113/120    avg_loss:0.007, val_acc:0.984]
Epoch [114/120    avg_loss:0.006, val_acc:0.981]
Epoch [115/120    avg_loss:0.007, val_acc:0.983]
Epoch [116/120    avg_loss:0.006, val_acc:0.984]
Epoch [117/120    avg_loss:0.007, val_acc:0.976]
Epoch [118/120    avg_loss:0.007, val_acc:0.979]
Epoch [119/120    avg_loss:0.004, val_acc:0.983]
Epoch [120/120    avg_loss:0.006, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1241    8    9    0    0    0    0    0    7   19    1    0
     0    0    0]
 [   0    0    0  720    0    0    0    0    0    0    1    1   24    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    2    0    0    0    0    0    0    0
     0    1    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    2    2    0    0    0  862    7    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    1   11    0   21 2172    4    0
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    1  528    2
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    0    0    0    0
  1117   15    0]
 [   0    0    0    0    0    4    2    0    0    0    0    0    0    0
    63  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.5609756097561

F1 scores:
[       nan 0.98765432 0.98102767 0.97627119 0.97931034 0.98070375
 0.9939302  0.98039216 0.98737084 1.         0.97566497 0.9848107
 0.96791934 0.9919571  0.96293103 0.8673947  0.99408284]

Kappa:
0.9721981948546599
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb9a906da58>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.663, val_acc:0.446]
Epoch [2/120    avg_loss:2.288, val_acc:0.530]
Epoch [3/120    avg_loss:1.999, val_acc:0.548]
Epoch [4/120    avg_loss:1.796, val_acc:0.579]
Epoch [5/120    avg_loss:1.560, val_acc:0.589]
Epoch [6/120    avg_loss:1.439, val_acc:0.651]
Epoch [7/120    avg_loss:1.206, val_acc:0.627]
Epoch [8/120    avg_loss:1.178, val_acc:0.634]
Epoch [9/120    avg_loss:0.963, val_acc:0.740]
Epoch [10/120    avg_loss:0.976, val_acc:0.706]
Epoch [11/120    avg_loss:0.810, val_acc:0.736]
Epoch [12/120    avg_loss:0.669, val_acc:0.752]
Epoch [13/120    avg_loss:0.651, val_acc:0.785]
Epoch [14/120    avg_loss:0.589, val_acc:0.792]
Epoch [15/120    avg_loss:0.470, val_acc:0.793]
Epoch [16/120    avg_loss:0.426, val_acc:0.835]
Epoch [17/120    avg_loss:0.431, val_acc:0.844]
Epoch [18/120    avg_loss:0.379, val_acc:0.835]
Epoch [19/120    avg_loss:0.376, val_acc:0.854]
Epoch [20/120    avg_loss:0.415, val_acc:0.839]
Epoch [21/120    avg_loss:0.308, val_acc:0.875]
Epoch [22/120    avg_loss:0.264, val_acc:0.871]
Epoch [23/120    avg_loss:0.281, val_acc:0.899]
Epoch [24/120    avg_loss:0.247, val_acc:0.900]
Epoch [25/120    avg_loss:0.282, val_acc:0.893]
Epoch [26/120    avg_loss:0.209, val_acc:0.893]
Epoch [27/120    avg_loss:0.230, val_acc:0.877]
Epoch [28/120    avg_loss:0.213, val_acc:0.912]
Epoch [29/120    avg_loss:0.175, val_acc:0.906]
Epoch [30/120    avg_loss:0.193, val_acc:0.880]
Epoch [31/120    avg_loss:0.191, val_acc:0.924]
Epoch [32/120    avg_loss:0.140, val_acc:0.918]
Epoch [33/120    avg_loss:0.166, val_acc:0.929]
Epoch [34/120    avg_loss:0.141, val_acc:0.908]
Epoch [35/120    avg_loss:0.110, val_acc:0.930]
Epoch [36/120    avg_loss:0.089, val_acc:0.944]
Epoch [37/120    avg_loss:0.095, val_acc:0.941]
Epoch [38/120    avg_loss:0.096, val_acc:0.935]
Epoch [39/120    avg_loss:0.112, val_acc:0.956]
Epoch [40/120    avg_loss:0.121, val_acc:0.942]
Epoch [41/120    avg_loss:0.127, val_acc:0.951]
Epoch [42/120    avg_loss:0.099, val_acc:0.915]
Epoch [43/120    avg_loss:0.098, val_acc:0.951]
Epoch [44/120    avg_loss:0.093, val_acc:0.946]
Epoch [45/120    avg_loss:0.102, val_acc:0.905]
Epoch [46/120    avg_loss:0.140, val_acc:0.940]
Epoch [47/120    avg_loss:0.120, val_acc:0.956]
Epoch [48/120    avg_loss:0.110, val_acc:0.946]
Epoch [49/120    avg_loss:0.097, val_acc:0.954]
Epoch [50/120    avg_loss:0.071, val_acc:0.949]
Epoch [51/120    avg_loss:0.065, val_acc:0.960]
Epoch [52/120    avg_loss:0.044, val_acc:0.956]
Epoch [53/120    avg_loss:0.053, val_acc:0.962]
Epoch [54/120    avg_loss:0.058, val_acc:0.959]
Epoch [55/120    avg_loss:0.069, val_acc:0.965]
Epoch [56/120    avg_loss:0.037, val_acc:0.972]
Epoch [57/120    avg_loss:0.070, val_acc:0.947]
Epoch [58/120    avg_loss:0.058, val_acc:0.964]
Epoch [59/120    avg_loss:0.054, val_acc:0.962]
Epoch [60/120    avg_loss:0.034, val_acc:0.966]
Epoch [61/120    avg_loss:0.056, val_acc:0.945]
Epoch [62/120    avg_loss:0.064, val_acc:0.962]
Epoch [63/120    avg_loss:0.034, val_acc:0.972]
Epoch [64/120    avg_loss:0.072, val_acc:0.955]
Epoch [65/120    avg_loss:0.043, val_acc:0.946]
Epoch [66/120    avg_loss:0.039, val_acc:0.965]
Epoch [67/120    avg_loss:0.037, val_acc:0.969]
Epoch [68/120    avg_loss:0.023, val_acc:0.966]
Epoch [69/120    avg_loss:0.030, val_acc:0.966]
Epoch [70/120    avg_loss:0.028, val_acc:0.967]
Epoch [71/120    avg_loss:0.036, val_acc:0.951]
Epoch [72/120    avg_loss:0.032, val_acc:0.969]
Epoch [73/120    avg_loss:0.070, val_acc:0.928]
Epoch [74/120    avg_loss:0.039, val_acc:0.972]
Epoch [75/120    avg_loss:0.025, val_acc:0.975]
Epoch [76/120    avg_loss:0.028, val_acc:0.965]
Epoch [77/120    avg_loss:0.025, val_acc:0.968]
Epoch [78/120    avg_loss:0.119, val_acc:0.909]
Epoch [79/120    avg_loss:0.119, val_acc:0.954]
Epoch [80/120    avg_loss:0.150, val_acc:0.932]
Epoch [81/120    avg_loss:0.067, val_acc:0.964]
Epoch [82/120    avg_loss:0.058, val_acc:0.947]
Epoch [83/120    avg_loss:0.032, val_acc:0.970]
Epoch [84/120    avg_loss:0.077, val_acc:0.971]
Epoch [85/120    avg_loss:0.039, val_acc:0.972]
Epoch [86/120    avg_loss:0.040, val_acc:0.968]
Epoch [87/120    avg_loss:0.033, val_acc:0.971]
Epoch [88/120    avg_loss:0.037, val_acc:0.977]
Epoch [89/120    avg_loss:0.028, val_acc:0.975]
Epoch [90/120    avg_loss:0.037, val_acc:0.981]
Epoch [91/120    avg_loss:0.021, val_acc:0.969]
Epoch [92/120    avg_loss:0.023, val_acc:0.974]
Epoch [93/120    avg_loss:0.012, val_acc:0.978]
Epoch [94/120    avg_loss:0.014, val_acc:0.981]
Epoch [95/120    avg_loss:0.013, val_acc:0.979]
Epoch [96/120    avg_loss:0.015, val_acc:0.980]
Epoch [97/120    avg_loss:0.059, val_acc:0.972]
Epoch [98/120    avg_loss:0.022, val_acc:0.981]
Epoch [99/120    avg_loss:0.039, val_acc:0.969]
Epoch [100/120    avg_loss:0.034, val_acc:0.976]
Epoch [101/120    avg_loss:0.024, val_acc:0.977]
Epoch [102/120    avg_loss:0.018, val_acc:0.979]
Epoch [103/120    avg_loss:0.014, val_acc:0.981]
Epoch [104/120    avg_loss:0.026, val_acc:0.968]
Epoch [105/120    avg_loss:0.030, val_acc:0.949]
Epoch [106/120    avg_loss:0.041, val_acc:0.979]
Epoch [107/120    avg_loss:0.020, val_acc:0.980]
Epoch [108/120    avg_loss:0.022, val_acc:0.970]
Epoch [109/120    avg_loss:0.022, val_acc:0.950]
Epoch [110/120    avg_loss:0.031, val_acc:0.975]
Epoch [111/120    avg_loss:0.013, val_acc:0.976]
Epoch [112/120    avg_loss:0.015, val_acc:0.982]
Epoch [113/120    avg_loss:0.021, val_acc:0.982]
Epoch [114/120    avg_loss:0.009, val_acc:0.986]
Epoch [115/120    avg_loss:0.009, val_acc:0.981]
Epoch [116/120    avg_loss:0.011, val_acc:0.963]
Epoch [117/120    avg_loss:0.011, val_acc:0.981]
Epoch [118/120    avg_loss:0.014, val_acc:0.982]
Epoch [119/120    avg_loss:0.015, val_acc:0.977]
Epoch [120/120    avg_loss:0.023, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1269    5    2    0    0    0    0    0    3    6    0    0
     0    0    0]
 [   0    0    0  730    1    0    0    0    0    1    2    7    5    0
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    3    0    0    0    1  858    0    1    0
     2    3    0]
 [   0    0   55    0    0    0    1    0    0    0   55 2097    0    0
     2    0    0]
 [   0    0    1    1    0    2    0    0    0    0    1    6  518    0
     0    2    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    74  265    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.00813008130082

F1 scores:
[       nan 1.         0.96981276 0.98382749 0.99065421 0.99313501
 0.99167298 1.         1.         0.94736842 0.95652174 0.96948682
 0.97643732 1.         0.96173469 0.848      0.96428571]

Kappa:
0.9659141271892636
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f505a3b3ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.656, val_acc:0.437]
Epoch [2/120    avg_loss:2.310, val_acc:0.502]
Epoch [3/120    avg_loss:1.976, val_acc:0.596]
Epoch [4/120    avg_loss:1.695, val_acc:0.566]
Epoch [5/120    avg_loss:1.537, val_acc:0.671]
Epoch [6/120    avg_loss:1.293, val_acc:0.685]
Epoch [7/120    avg_loss:1.134, val_acc:0.724]
Epoch [8/120    avg_loss:1.009, val_acc:0.712]
Epoch [9/120    avg_loss:0.893, val_acc:0.737]
Epoch [10/120    avg_loss:0.787, val_acc:0.771]
Epoch [11/120    avg_loss:0.659, val_acc:0.813]
Epoch [12/120    avg_loss:0.573, val_acc:0.812]
Epoch [13/120    avg_loss:0.550, val_acc:0.839]
Epoch [14/120    avg_loss:0.502, val_acc:0.842]
Epoch [15/120    avg_loss:0.448, val_acc:0.830]
Epoch [16/120    avg_loss:0.386, val_acc:0.886]
Epoch [17/120    avg_loss:0.324, val_acc:0.854]
Epoch [18/120    avg_loss:0.334, val_acc:0.877]
Epoch [19/120    avg_loss:0.309, val_acc:0.892]
Epoch [20/120    avg_loss:0.332, val_acc:0.889]
Epoch [21/120    avg_loss:0.269, val_acc:0.901]
Epoch [22/120    avg_loss:0.225, val_acc:0.917]
Epoch [23/120    avg_loss:0.183, val_acc:0.920]
Epoch [24/120    avg_loss:0.187, val_acc:0.939]
Epoch [25/120    avg_loss:0.292, val_acc:0.891]
Epoch [26/120    avg_loss:0.232, val_acc:0.918]
Epoch [27/120    avg_loss:0.160, val_acc:0.920]
Epoch [28/120    avg_loss:0.123, val_acc:0.910]
Epoch [29/120    avg_loss:0.149, val_acc:0.916]
Epoch [30/120    avg_loss:0.291, val_acc:0.916]
Epoch [31/120    avg_loss:0.203, val_acc:0.910]
Epoch [32/120    avg_loss:0.162, val_acc:0.929]
Epoch [33/120    avg_loss:0.232, val_acc:0.930]
Epoch [34/120    avg_loss:0.182, val_acc:0.909]
Epoch [35/120    avg_loss:0.142, val_acc:0.937]
Epoch [36/120    avg_loss:0.097, val_acc:0.937]
Epoch [37/120    avg_loss:0.087, val_acc:0.937]
Epoch [38/120    avg_loss:0.082, val_acc:0.947]
Epoch [39/120    avg_loss:0.055, val_acc:0.947]
Epoch [40/120    avg_loss:0.069, val_acc:0.954]
Epoch [41/120    avg_loss:0.060, val_acc:0.952]
Epoch [42/120    avg_loss:0.050, val_acc:0.955]
Epoch [43/120    avg_loss:0.051, val_acc:0.954]
Epoch [44/120    avg_loss:0.059, val_acc:0.957]
Epoch [45/120    avg_loss:0.053, val_acc:0.956]
Epoch [46/120    avg_loss:0.050, val_acc:0.955]
Epoch [47/120    avg_loss:0.047, val_acc:0.956]
Epoch [48/120    avg_loss:0.056, val_acc:0.955]
Epoch [49/120    avg_loss:0.045, val_acc:0.958]
Epoch [50/120    avg_loss:0.047, val_acc:0.958]
Epoch [51/120    avg_loss:0.052, val_acc:0.959]
Epoch [52/120    avg_loss:0.056, val_acc:0.958]
Epoch [53/120    avg_loss:0.045, val_acc:0.959]
Epoch [54/120    avg_loss:0.046, val_acc:0.961]
Epoch [55/120    avg_loss:0.046, val_acc:0.958]
Epoch [56/120    avg_loss:0.044, val_acc:0.959]
Epoch [57/120    avg_loss:0.039, val_acc:0.960]
Epoch [58/120    avg_loss:0.048, val_acc:0.964]
Epoch [59/120    avg_loss:0.039, val_acc:0.959]
Epoch [60/120    avg_loss:0.046, val_acc:0.960]
Epoch [61/120    avg_loss:0.047, val_acc:0.965]
Epoch [62/120    avg_loss:0.038, val_acc:0.964]
Epoch [63/120    avg_loss:0.042, val_acc:0.964]
Epoch [64/120    avg_loss:0.037, val_acc:0.967]
Epoch [65/120    avg_loss:0.037, val_acc:0.966]
Epoch [66/120    avg_loss:0.052, val_acc:0.962]
Epoch [67/120    avg_loss:0.043, val_acc:0.960]
Epoch [68/120    avg_loss:0.041, val_acc:0.962]
Epoch [69/120    avg_loss:0.042, val_acc:0.964]
Epoch [70/120    avg_loss:0.046, val_acc:0.961]
Epoch [71/120    avg_loss:0.038, val_acc:0.964]
Epoch [72/120    avg_loss:0.037, val_acc:0.965]
Epoch [73/120    avg_loss:0.035, val_acc:0.966]
Epoch [74/120    avg_loss:0.039, val_acc:0.964]
Epoch [75/120    avg_loss:0.039, val_acc:0.964]
Epoch [76/120    avg_loss:0.035, val_acc:0.965]
Epoch [77/120    avg_loss:0.035, val_acc:0.967]
Epoch [78/120    avg_loss:0.033, val_acc:0.964]
Epoch [79/120    avg_loss:0.040, val_acc:0.965]
Epoch [80/120    avg_loss:0.039, val_acc:0.965]
Epoch [81/120    avg_loss:0.040, val_acc:0.966]
Epoch [82/120    avg_loss:0.031, val_acc:0.967]
Epoch [83/120    avg_loss:0.032, val_acc:0.967]
Epoch [84/120    avg_loss:0.030, val_acc:0.965]
Epoch [85/120    avg_loss:0.034, val_acc:0.967]
Epoch [86/120    avg_loss:0.035, val_acc:0.966]
Epoch [87/120    avg_loss:0.033, val_acc:0.967]
Epoch [88/120    avg_loss:0.038, val_acc:0.966]
Epoch [89/120    avg_loss:0.035, val_acc:0.968]
Epoch [90/120    avg_loss:0.043, val_acc:0.965]
Epoch [91/120    avg_loss:0.032, val_acc:0.965]
Epoch [92/120    avg_loss:0.034, val_acc:0.967]
Epoch [93/120    avg_loss:0.036, val_acc:0.965]
Epoch [94/120    avg_loss:0.035, val_acc:0.965]
Epoch [95/120    avg_loss:0.032, val_acc:0.967]
Epoch [96/120    avg_loss:0.029, val_acc:0.968]
Epoch [97/120    avg_loss:0.029, val_acc:0.971]
Epoch [98/120    avg_loss:0.038, val_acc:0.970]
Epoch [99/120    avg_loss:0.029, val_acc:0.965]
Epoch [100/120    avg_loss:0.037, val_acc:0.968]
Epoch [101/120    avg_loss:0.028, val_acc:0.966]
Epoch [102/120    avg_loss:0.030, val_acc:0.968]
Epoch [103/120    avg_loss:0.026, val_acc:0.970]
Epoch [104/120    avg_loss:0.028, val_acc:0.968]
Epoch [105/120    avg_loss:0.030, val_acc:0.970]
Epoch [106/120    avg_loss:0.029, val_acc:0.968]
Epoch [107/120    avg_loss:0.031, val_acc:0.967]
Epoch [108/120    avg_loss:0.026, val_acc:0.970]
Epoch [109/120    avg_loss:0.025, val_acc:0.968]
Epoch [110/120    avg_loss:0.026, val_acc:0.970]
Epoch [111/120    avg_loss:0.023, val_acc:0.970]
Epoch [112/120    avg_loss:0.026, val_acc:0.970]
Epoch [113/120    avg_loss:0.027, val_acc:0.970]
Epoch [114/120    avg_loss:0.026, val_acc:0.970]
Epoch [115/120    avg_loss:0.028, val_acc:0.970]
Epoch [116/120    avg_loss:0.025, val_acc:0.970]
Epoch [117/120    avg_loss:0.029, val_acc:0.970]
Epoch [118/120    avg_loss:0.029, val_acc:0.970]
Epoch [119/120    avg_loss:0.025, val_acc:0.970]
Epoch [120/120    avg_loss:0.031, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1248    3   12    0    7    0    0    0    4   11    0    0
     0    0    0]
 [   0    0    0  720    0    8    0    0    0    2    3    7    5    0
     2    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    4    4    0    0    0  834   20    0    0
     3    0    0]
 [   0    1   11    0    0    2    1    1    0    0   11 2177    3    0
     2    1    0]
 [   0    0    0    3    0    5    0    0    0    0    3    1  514    0
     0    5    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1125   13    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    42  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.4308943089431

F1 scores:
[       nan 0.96296296 0.97729052 0.97759674 0.97025172 0.9740113
 0.98050975 0.98039216 0.99883856 0.94736842 0.9630485  0.98306615
 0.9707271  1.         0.9710833  0.89090909 0.9704142 ]

Kappa:
0.9707026986522056
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4551988630>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.598, val_acc:0.361]
Epoch [2/120    avg_loss:2.235, val_acc:0.500]
Epoch [3/120    avg_loss:1.941, val_acc:0.593]
Epoch [4/120    avg_loss:1.724, val_acc:0.573]
Epoch [5/120    avg_loss:1.534, val_acc:0.636]
Epoch [6/120    avg_loss:1.357, val_acc:0.684]
Epoch [7/120    avg_loss:1.203, val_acc:0.674]
Epoch [8/120    avg_loss:1.118, val_acc:0.732]
Epoch [9/120    avg_loss:1.010, val_acc:0.742]
Epoch [10/120    avg_loss:0.878, val_acc:0.731]
Epoch [11/120    avg_loss:0.770, val_acc:0.755]
Epoch [12/120    avg_loss:0.709, val_acc:0.784]
Epoch [13/120    avg_loss:0.655, val_acc:0.815]
Epoch [14/120    avg_loss:0.547, val_acc:0.788]
Epoch [15/120    avg_loss:0.607, val_acc:0.802]
Epoch [16/120    avg_loss:0.517, val_acc:0.856]
Epoch [17/120    avg_loss:0.454, val_acc:0.886]
Epoch [18/120    avg_loss:0.379, val_acc:0.879]
Epoch [19/120    avg_loss:0.370, val_acc:0.855]
Epoch [20/120    avg_loss:0.409, val_acc:0.823]
Epoch [21/120    avg_loss:0.339, val_acc:0.855]
Epoch [22/120    avg_loss:0.369, val_acc:0.892]
Epoch [23/120    avg_loss:0.296, val_acc:0.904]
Epoch [24/120    avg_loss:0.271, val_acc:0.888]
Epoch [25/120    avg_loss:0.238, val_acc:0.897]
Epoch [26/120    avg_loss:0.268, val_acc:0.903]
Epoch [27/120    avg_loss:0.285, val_acc:0.923]
Epoch [28/120    avg_loss:0.209, val_acc:0.903]
Epoch [29/120    avg_loss:0.186, val_acc:0.910]
Epoch [30/120    avg_loss:0.273, val_acc:0.933]
Epoch [31/120    avg_loss:0.165, val_acc:0.952]
Epoch [32/120    avg_loss:0.251, val_acc:0.851]
Epoch [33/120    avg_loss:0.184, val_acc:0.940]
Epoch [34/120    avg_loss:0.205, val_acc:0.951]
Epoch [35/120    avg_loss:0.144, val_acc:0.926]
Epoch [36/120    avg_loss:0.177, val_acc:0.910]
Epoch [37/120    avg_loss:0.133, val_acc:0.953]
Epoch [38/120    avg_loss:0.105, val_acc:0.959]
Epoch [39/120    avg_loss:0.114, val_acc:0.932]
Epoch [40/120    avg_loss:0.156, val_acc:0.923]
Epoch [41/120    avg_loss:0.145, val_acc:0.958]
Epoch [42/120    avg_loss:0.096, val_acc:0.943]
Epoch [43/120    avg_loss:0.101, val_acc:0.962]
Epoch [44/120    avg_loss:0.066, val_acc:0.968]
Epoch [45/120    avg_loss:0.089, val_acc:0.959]
Epoch [46/120    avg_loss:0.101, val_acc:0.952]
Epoch [47/120    avg_loss:0.080, val_acc:0.965]
Epoch [48/120    avg_loss:0.081, val_acc:0.938]
Epoch [49/120    avg_loss:0.104, val_acc:0.924]
Epoch [50/120    avg_loss:0.083, val_acc:0.960]
Epoch [51/120    avg_loss:0.072, val_acc:0.970]
Epoch [52/120    avg_loss:0.100, val_acc:0.938]
Epoch [53/120    avg_loss:0.083, val_acc:0.953]
Epoch [54/120    avg_loss:0.066, val_acc:0.968]
Epoch [55/120    avg_loss:0.049, val_acc:0.968]
Epoch [56/120    avg_loss:0.053, val_acc:0.964]
Epoch [57/120    avg_loss:0.065, val_acc:0.969]
Epoch [58/120    avg_loss:0.034, val_acc:0.980]
Epoch [59/120    avg_loss:0.102, val_acc:0.953]
Epoch [60/120    avg_loss:0.096, val_acc:0.957]
Epoch [61/120    avg_loss:0.055, val_acc:0.972]
Epoch [62/120    avg_loss:0.047, val_acc:0.980]
Epoch [63/120    avg_loss:0.049, val_acc:0.962]
Epoch [64/120    avg_loss:0.034, val_acc:0.981]
Epoch [65/120    avg_loss:0.035, val_acc:0.978]
Epoch [66/120    avg_loss:0.034, val_acc:0.979]
Epoch [67/120    avg_loss:0.025, val_acc:0.967]
Epoch [68/120    avg_loss:0.047, val_acc:0.980]
Epoch [69/120    avg_loss:0.050, val_acc:0.980]
Epoch [70/120    avg_loss:0.045, val_acc:0.972]
Epoch [71/120    avg_loss:0.084, val_acc:0.965]
Epoch [72/120    avg_loss:0.047, val_acc:0.977]
Epoch [73/120    avg_loss:0.042, val_acc:0.959]
Epoch [74/120    avg_loss:0.045, val_acc:0.980]
Epoch [75/120    avg_loss:0.092, val_acc:0.929]
Epoch [76/120    avg_loss:0.093, val_acc:0.976]
Epoch [77/120    avg_loss:0.038, val_acc:0.969]
Epoch [78/120    avg_loss:0.035, val_acc:0.979]
Epoch [79/120    avg_loss:0.024, val_acc:0.982]
Epoch [80/120    avg_loss:0.024, val_acc:0.982]
Epoch [81/120    avg_loss:0.020, val_acc:0.983]
Epoch [82/120    avg_loss:0.021, val_acc:0.983]
Epoch [83/120    avg_loss:0.019, val_acc:0.983]
Epoch [84/120    avg_loss:0.023, val_acc:0.983]
Epoch [85/120    avg_loss:0.021, val_acc:0.982]
Epoch [86/120    avg_loss:0.016, val_acc:0.983]
Epoch [87/120    avg_loss:0.019, val_acc:0.984]
Epoch [88/120    avg_loss:0.015, val_acc:0.983]
Epoch [89/120    avg_loss:0.022, val_acc:0.982]
Epoch [90/120    avg_loss:0.023, val_acc:0.981]
Epoch [91/120    avg_loss:0.017, val_acc:0.984]
Epoch [92/120    avg_loss:0.019, val_acc:0.983]
Epoch [93/120    avg_loss:0.014, val_acc:0.983]
Epoch [94/120    avg_loss:0.014, val_acc:0.983]
Epoch [95/120    avg_loss:0.019, val_acc:0.983]
Epoch [96/120    avg_loss:0.017, val_acc:0.983]
Epoch [97/120    avg_loss:0.019, val_acc:0.983]
Epoch [98/120    avg_loss:0.020, val_acc:0.985]
Epoch [99/120    avg_loss:0.016, val_acc:0.985]
Epoch [100/120    avg_loss:0.011, val_acc:0.983]
Epoch [101/120    avg_loss:0.013, val_acc:0.982]
Epoch [102/120    avg_loss:0.016, val_acc:0.983]
Epoch [103/120    avg_loss:0.013, val_acc:0.984]
Epoch [104/120    avg_loss:0.015, val_acc:0.983]
Epoch [105/120    avg_loss:0.014, val_acc:0.981]
Epoch [106/120    avg_loss:0.013, val_acc:0.980]
Epoch [107/120    avg_loss:0.013, val_acc:0.980]
Epoch [108/120    avg_loss:0.017, val_acc:0.983]
Epoch [109/120    avg_loss:0.018, val_acc:0.983]
Epoch [110/120    avg_loss:0.013, val_acc:0.980]
Epoch [111/120    avg_loss:0.013, val_acc:0.980]
Epoch [112/120    avg_loss:0.023, val_acc:0.984]
Epoch [113/120    avg_loss:0.016, val_acc:0.984]
Epoch [114/120    avg_loss:0.017, val_acc:0.984]
Epoch [115/120    avg_loss:0.014, val_acc:0.984]
Epoch [116/120    avg_loss:0.016, val_acc:0.984]
Epoch [117/120    avg_loss:0.014, val_acc:0.984]
Epoch [118/120    avg_loss:0.014, val_acc:0.983]
Epoch [119/120    avg_loss:0.016, val_acc:0.984]
Epoch [120/120    avg_loss:0.018, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1237    1    9    4    1    0    0    0    3   30    0    0
     0    0    0]
 [   0    0    1  701   10    1    0    0    0   17    6    4    7    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    5    0    0    0  839   25    1    0
     1    0    0]
 [   0    0    2    0    0    1    3    0    0    1    6 2179   14    2
     2    0    0]
 [   0    0    0    1    0    0    0    0    0    2    0    1  524    0
     0    2    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1133    6    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    41  306    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.5609756097561

F1 scores:
[       nan 1.         0.97825227 0.96689655 0.95495495 0.992
 0.98937785 1.         1.         0.64285714 0.97050318 0.97954597
 0.96947271 0.99462366 0.97588286 0.92586989 0.97674419]

Kappa:
0.9721871424192773
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:15:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcf2d369a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.595, val_acc:0.532]
Epoch [2/120    avg_loss:2.230, val_acc:0.540]
Epoch [3/120    avg_loss:1.908, val_acc:0.584]
Epoch [4/120    avg_loss:1.656, val_acc:0.621]
Epoch [5/120    avg_loss:1.414, val_acc:0.649]
Epoch [6/120    avg_loss:1.319, val_acc:0.639]
Epoch [7/120    avg_loss:1.143, val_acc:0.745]
Epoch [8/120    avg_loss:1.028, val_acc:0.715]
Epoch [9/120    avg_loss:0.877, val_acc:0.762]
Epoch [10/120    avg_loss:0.797, val_acc:0.744]
Epoch [11/120    avg_loss:0.746, val_acc:0.792]
Epoch [12/120    avg_loss:0.696, val_acc:0.826]
Epoch [13/120    avg_loss:0.602, val_acc:0.820]
Epoch [14/120    avg_loss:0.595, val_acc:0.828]
Epoch [15/120    avg_loss:0.578, val_acc:0.850]
Epoch [16/120    avg_loss:0.522, val_acc:0.865]
Epoch [17/120    avg_loss:0.482, val_acc:0.798]
Epoch [18/120    avg_loss:0.420, val_acc:0.818]
Epoch [19/120    avg_loss:0.382, val_acc:0.836]
Epoch [20/120    avg_loss:0.329, val_acc:0.868]
Epoch [21/120    avg_loss:0.337, val_acc:0.891]
Epoch [22/120    avg_loss:0.262, val_acc:0.873]
Epoch [23/120    avg_loss:0.266, val_acc:0.895]
Epoch [24/120    avg_loss:0.266, val_acc:0.905]
Epoch [25/120    avg_loss:0.264, val_acc:0.876]
Epoch [26/120    avg_loss:0.206, val_acc:0.922]
Epoch [27/120    avg_loss:0.195, val_acc:0.910]
Epoch [28/120    avg_loss:0.206, val_acc:0.915]
Epoch [29/120    avg_loss:0.131, val_acc:0.935]
Epoch [30/120    avg_loss:0.137, val_acc:0.911]
Epoch [31/120    avg_loss:0.166, val_acc:0.926]
Epoch [32/120    avg_loss:0.154, val_acc:0.911]
Epoch [33/120    avg_loss:0.143, val_acc:0.899]
Epoch [34/120    avg_loss:0.194, val_acc:0.928]
Epoch [35/120    avg_loss:0.144, val_acc:0.934]
Epoch [36/120    avg_loss:0.096, val_acc:0.947]
Epoch [37/120    avg_loss:0.105, val_acc:0.955]
Epoch [38/120    avg_loss:0.094, val_acc:0.938]
Epoch [39/120    avg_loss:0.083, val_acc:0.943]
Epoch [40/120    avg_loss:0.163, val_acc:0.836]
Epoch [41/120    avg_loss:0.252, val_acc:0.947]
Epoch [42/120    avg_loss:0.113, val_acc:0.947]
Epoch [43/120    avg_loss:0.094, val_acc:0.949]
Epoch [44/120    avg_loss:0.099, val_acc:0.943]
Epoch [45/120    avg_loss:0.065, val_acc:0.960]
Epoch [46/120    avg_loss:0.087, val_acc:0.954]
Epoch [47/120    avg_loss:0.083, val_acc:0.957]
Epoch [48/120    avg_loss:0.049, val_acc:0.959]
Epoch [49/120    avg_loss:0.070, val_acc:0.953]
Epoch [50/120    avg_loss:0.063, val_acc:0.944]
Epoch [51/120    avg_loss:0.066, val_acc:0.954]
Epoch [52/120    avg_loss:0.075, val_acc:0.924]
Epoch [53/120    avg_loss:0.076, val_acc:0.951]
Epoch [54/120    avg_loss:0.050, val_acc:0.966]
Epoch [55/120    avg_loss:0.053, val_acc:0.965]
Epoch [56/120    avg_loss:0.044, val_acc:0.967]
Epoch [57/120    avg_loss:0.053, val_acc:0.940]
Epoch [58/120    avg_loss:0.087, val_acc:0.936]
Epoch [59/120    avg_loss:0.049, val_acc:0.969]
Epoch [60/120    avg_loss:0.048, val_acc:0.966]
Epoch [61/120    avg_loss:0.050, val_acc:0.964]
Epoch [62/120    avg_loss:0.038, val_acc:0.973]
Epoch [63/120    avg_loss:0.031, val_acc:0.968]
Epoch [64/120    avg_loss:0.050, val_acc:0.956]
Epoch [65/120    avg_loss:0.026, val_acc:0.977]
Epoch [66/120    avg_loss:0.024, val_acc:0.977]
Epoch [67/120    avg_loss:0.062, val_acc:0.960]
Epoch [68/120    avg_loss:0.050, val_acc:0.966]
Epoch [69/120    avg_loss:0.038, val_acc:0.976]
Epoch [70/120    avg_loss:0.051, val_acc:0.938]
Epoch [71/120    avg_loss:0.106, val_acc:0.968]
Epoch [72/120    avg_loss:0.100, val_acc:0.964]
Epoch [73/120    avg_loss:0.034, val_acc:0.952]
Epoch [74/120    avg_loss:0.080, val_acc:0.971]
Epoch [75/120    avg_loss:0.034, val_acc:0.972]
Epoch [76/120    avg_loss:0.035, val_acc:0.976]
Epoch [77/120    avg_loss:0.024, val_acc:0.976]
Epoch [78/120    avg_loss:0.029, val_acc:0.973]
Epoch [79/120    avg_loss:0.020, val_acc:0.979]
Epoch [80/120    avg_loss:0.022, val_acc:0.974]
Epoch [81/120    avg_loss:0.013, val_acc:0.980]
Epoch [82/120    avg_loss:0.020, val_acc:0.956]
Epoch [83/120    avg_loss:0.023, val_acc:0.974]
Epoch [84/120    avg_loss:0.026, val_acc:0.973]
Epoch [85/120    avg_loss:0.025, val_acc:0.978]
Epoch [86/120    avg_loss:0.020, val_acc:0.974]
Epoch [87/120    avg_loss:0.015, val_acc:0.976]
Epoch [88/120    avg_loss:0.021, val_acc:0.975]
Epoch [89/120    avg_loss:0.013, val_acc:0.982]
Epoch [90/120    avg_loss:0.012, val_acc:0.977]
Epoch [91/120    avg_loss:0.023, val_acc:0.976]
Epoch [92/120    avg_loss:0.016, val_acc:0.974]
Epoch [93/120    avg_loss:0.024, val_acc:0.977]
Epoch [94/120    avg_loss:0.015, val_acc:0.980]
Epoch [95/120    avg_loss:0.010, val_acc:0.980]
Epoch [96/120    avg_loss:0.007, val_acc:0.985]
Epoch [97/120    avg_loss:0.026, val_acc:0.972]
Epoch [98/120    avg_loss:0.043, val_acc:0.960]
Epoch [99/120    avg_loss:0.060, val_acc:0.966]
Epoch [100/120    avg_loss:0.020, val_acc:0.976]
Epoch [101/120    avg_loss:0.013, val_acc:0.980]
Epoch [102/120    avg_loss:0.012, val_acc:0.977]
Epoch [103/120    avg_loss:0.014, val_acc:0.979]
Epoch [104/120    avg_loss:0.009, val_acc:0.980]
Epoch [105/120    avg_loss:0.009, val_acc:0.983]
Epoch [106/120    avg_loss:0.010, val_acc:0.977]
Epoch [107/120    avg_loss:0.032, val_acc:0.973]
Epoch [108/120    avg_loss:0.011, val_acc:0.973]
Epoch [109/120    avg_loss:0.010, val_acc:0.980]
Epoch [110/120    avg_loss:0.011, val_acc:0.979]
Epoch [111/120    avg_loss:0.010, val_acc:0.980]
Epoch [112/120    avg_loss:0.007, val_acc:0.981]
Epoch [113/120    avg_loss:0.008, val_acc:0.981]
Epoch [114/120    avg_loss:0.008, val_acc:0.981]
Epoch [115/120    avg_loss:0.011, val_acc:0.981]
Epoch [116/120    avg_loss:0.011, val_acc:0.981]
Epoch [117/120    avg_loss:0.008, val_acc:0.980]
Epoch [118/120    avg_loss:0.009, val_acc:0.981]
Epoch [119/120    avg_loss:0.006, val_acc:0.982]
Epoch [120/120    avg_loss:0.009, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1255    6    7    0    0    0    0    0    1   12    4    0
     0    0    0]
 [   0    0    0  741    3    0    0    0    0    1    0    1    1    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    0    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    5    0    0    2    0    0    0    0  842   24    1    0
     1    0    0]
 [   0    0    5    0    0    0    0    0    9    0   31 2155    7    1
     2    0    0]
 [   0    0    0    3    0    0    0    0    0    0    5    0  522    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1135    3    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    91  256    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.3550135501355

F1 scores:
[       nan 1.         0.98431373 0.98997996 0.97706422 0.98845266
 0.99847561 0.97959184 0.98734177 0.94444444 0.96009122 0.97887804
 0.9738806  0.99730458 0.95538721 0.84488449 0.97076023]

Kappa:
0.969833012071687
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1a9cc11ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.653, val_acc:0.297]
Epoch [2/120    avg_loss:2.297, val_acc:0.542]
Epoch [3/120    avg_loss:1.977, val_acc:0.496]
Epoch [4/120    avg_loss:1.770, val_acc:0.575]
Epoch [5/120    avg_loss:1.595, val_acc:0.603]
Epoch [6/120    avg_loss:1.361, val_acc:0.650]
Epoch [7/120    avg_loss:1.234, val_acc:0.678]
Epoch [8/120    avg_loss:1.069, val_acc:0.718]
Epoch [9/120    avg_loss:0.951, val_acc:0.740]
Epoch [10/120    avg_loss:0.837, val_acc:0.812]
Epoch [11/120    avg_loss:0.767, val_acc:0.796]
Epoch [12/120    avg_loss:0.683, val_acc:0.786]
Epoch [13/120    avg_loss:0.585, val_acc:0.787]
Epoch [14/120    avg_loss:0.548, val_acc:0.829]
Epoch [15/120    avg_loss:0.491, val_acc:0.814]
Epoch [16/120    avg_loss:0.472, val_acc:0.827]
Epoch [17/120    avg_loss:0.438, val_acc:0.833]
Epoch [18/120    avg_loss:0.356, val_acc:0.819]
Epoch [19/120    avg_loss:0.456, val_acc:0.826]
Epoch [20/120    avg_loss:0.416, val_acc:0.887]
Epoch [21/120    avg_loss:0.290, val_acc:0.866]
Epoch [22/120    avg_loss:0.277, val_acc:0.841]
Epoch [23/120    avg_loss:0.241, val_acc:0.852]
Epoch [24/120    avg_loss:0.230, val_acc:0.900]
Epoch [25/120    avg_loss:0.229, val_acc:0.911]
Epoch [26/120    avg_loss:0.215, val_acc:0.907]
Epoch [27/120    avg_loss:0.247, val_acc:0.826]
Epoch [28/120    avg_loss:0.225, val_acc:0.882]
Epoch [29/120    avg_loss:0.155, val_acc:0.906]
Epoch [30/120    avg_loss:0.151, val_acc:0.915]
Epoch [31/120    avg_loss:0.137, val_acc:0.923]
Epoch [32/120    avg_loss:0.127, val_acc:0.925]
Epoch [33/120    avg_loss:0.093, val_acc:0.936]
Epoch [34/120    avg_loss:0.131, val_acc:0.907]
Epoch [35/120    avg_loss:0.116, val_acc:0.939]
Epoch [36/120    avg_loss:0.106, val_acc:0.917]
Epoch [37/120    avg_loss:0.118, val_acc:0.930]
Epoch [38/120    avg_loss:0.151, val_acc:0.936]
Epoch [39/120    avg_loss:0.122, val_acc:0.931]
Epoch [40/120    avg_loss:0.098, val_acc:0.954]
Epoch [41/120    avg_loss:0.089, val_acc:0.939]
Epoch [42/120    avg_loss:0.080, val_acc:0.954]
Epoch [43/120    avg_loss:0.071, val_acc:0.939]
Epoch [44/120    avg_loss:0.081, val_acc:0.936]
Epoch [45/120    avg_loss:0.084, val_acc:0.963]
Epoch [46/120    avg_loss:0.144, val_acc:0.881]
Epoch [47/120    avg_loss:0.216, val_acc:0.895]
Epoch [48/120    avg_loss:0.119, val_acc:0.920]
Epoch [49/120    avg_loss:0.106, val_acc:0.940]
Epoch [50/120    avg_loss:0.105, val_acc:0.954]
Epoch [51/120    avg_loss:0.056, val_acc:0.948]
Epoch [52/120    avg_loss:0.042, val_acc:0.966]
Epoch [53/120    avg_loss:0.042, val_acc:0.957]
Epoch [54/120    avg_loss:0.056, val_acc:0.955]
Epoch [55/120    avg_loss:0.082, val_acc:0.946]
Epoch [56/120    avg_loss:0.056, val_acc:0.961]
Epoch [57/120    avg_loss:0.058, val_acc:0.928]
Epoch [58/120    avg_loss:0.036, val_acc:0.965]
Epoch [59/120    avg_loss:0.054, val_acc:0.942]
Epoch [60/120    avg_loss:0.042, val_acc:0.967]
Epoch [61/120    avg_loss:0.046, val_acc:0.940]
Epoch [62/120    avg_loss:0.045, val_acc:0.971]
Epoch [63/120    avg_loss:0.037, val_acc:0.966]
Epoch [64/120    avg_loss:0.103, val_acc:0.951]
Epoch [65/120    avg_loss:0.073, val_acc:0.967]
Epoch [66/120    avg_loss:0.081, val_acc:0.944]
Epoch [67/120    avg_loss:0.054, val_acc:0.964]
Epoch [68/120    avg_loss:0.041, val_acc:0.960]
Epoch [69/120    avg_loss:0.041, val_acc:0.957]
Epoch [70/120    avg_loss:0.040, val_acc:0.966]
Epoch [71/120    avg_loss:0.025, val_acc:0.957]
Epoch [72/120    avg_loss:0.032, val_acc:0.968]
Epoch [73/120    avg_loss:0.033, val_acc:0.967]
Epoch [74/120    avg_loss:0.024, val_acc:0.967]
Epoch [75/120    avg_loss:0.029, val_acc:0.970]
Epoch [76/120    avg_loss:0.017, val_acc:0.975]
Epoch [77/120    avg_loss:0.022, val_acc:0.975]
Epoch [78/120    avg_loss:0.015, val_acc:0.974]
Epoch [79/120    avg_loss:0.014, val_acc:0.976]
Epoch [80/120    avg_loss:0.013, val_acc:0.974]
Epoch [81/120    avg_loss:0.015, val_acc:0.975]
Epoch [82/120    avg_loss:0.014, val_acc:0.974]
Epoch [83/120    avg_loss:0.014, val_acc:0.973]
Epoch [84/120    avg_loss:0.013, val_acc:0.975]
Epoch [85/120    avg_loss:0.014, val_acc:0.976]
Epoch [86/120    avg_loss:0.013, val_acc:0.975]
Epoch [87/120    avg_loss:0.017, val_acc:0.977]
Epoch [88/120    avg_loss:0.011, val_acc:0.977]
Epoch [89/120    avg_loss:0.010, val_acc:0.977]
Epoch [90/120    avg_loss:0.009, val_acc:0.976]
Epoch [91/120    avg_loss:0.012, val_acc:0.977]
Epoch [92/120    avg_loss:0.009, val_acc:0.977]
Epoch [93/120    avg_loss:0.012, val_acc:0.975]
Epoch [94/120    avg_loss:0.011, val_acc:0.974]
Epoch [95/120    avg_loss:0.012, val_acc:0.974]
Epoch [96/120    avg_loss:0.014, val_acc:0.976]
Epoch [97/120    avg_loss:0.012, val_acc:0.978]
Epoch [98/120    avg_loss:0.015, val_acc:0.979]
Epoch [99/120    avg_loss:0.013, val_acc:0.979]
Epoch [100/120    avg_loss:0.015, val_acc:0.979]
Epoch [101/120    avg_loss:0.010, val_acc:0.977]
Epoch [102/120    avg_loss:0.011, val_acc:0.978]
Epoch [103/120    avg_loss:0.014, val_acc:0.978]
Epoch [104/120    avg_loss:0.016, val_acc:0.980]
Epoch [105/120    avg_loss:0.009, val_acc:0.979]
Epoch [106/120    avg_loss:0.010, val_acc:0.977]
Epoch [107/120    avg_loss:0.009, val_acc:0.975]
Epoch [108/120    avg_loss:0.011, val_acc:0.975]
Epoch [109/120    avg_loss:0.012, val_acc:0.974]
Epoch [110/120    avg_loss:0.010, val_acc:0.978]
Epoch [111/120    avg_loss:0.011, val_acc:0.977]
Epoch [112/120    avg_loss:0.010, val_acc:0.977]
Epoch [113/120    avg_loss:0.009, val_acc:0.976]
Epoch [114/120    avg_loss:0.010, val_acc:0.976]
Epoch [115/120    avg_loss:0.011, val_acc:0.977]
Epoch [116/120    avg_loss:0.010, val_acc:0.975]
Epoch [117/120    avg_loss:0.012, val_acc:0.974]
Epoch [118/120    avg_loss:0.016, val_acc:0.975]
Epoch [119/120    avg_loss:0.008, val_acc:0.976]
Epoch [120/120    avg_loss:0.010, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1262    2    4    0    0    0    0    0    4   12    1    0
     0    0    0]
 [   0    0    0  731    0    2    0    0    0    0    2    8    3    0
     0    1    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    1    0    0    0  850   22    0    0
     0    0    0]
 [   0    0    7    0    2    0    0    0    0    0   14 2174   11    1
     1    0    0]
 [   0    0    0    8    0    0    0    0    0    0    0    0  519    0
     1    4    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    0    0    0    0
  1125    9    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
   103  243    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.40921409214093

F1 scores:
[       nan 0.975      0.98748044 0.98186702 0.98139535 0.99201824
 0.99771863 1.         1.         1.         0.97309674 0.98237686
 0.97009346 0.99730458 0.94936709 0.80463576 0.98224852]

Kappa:
0.9704356456506215
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6d7d103ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.659, val_acc:0.408]
Epoch [2/120    avg_loss:2.285, val_acc:0.512]
Epoch [3/120    avg_loss:1.948, val_acc:0.572]
Epoch [4/120    avg_loss:1.702, val_acc:0.581]
Epoch [5/120    avg_loss:1.487, val_acc:0.613]
Epoch [6/120    avg_loss:1.282, val_acc:0.656]
Epoch [7/120    avg_loss:1.111, val_acc:0.697]
Epoch [8/120    avg_loss:1.046, val_acc:0.709]
Epoch [9/120    avg_loss:0.899, val_acc:0.750]
Epoch [10/120    avg_loss:0.838, val_acc:0.726]
Epoch [11/120    avg_loss:0.697, val_acc:0.765]
Epoch [12/120    avg_loss:0.630, val_acc:0.818]
Epoch [13/120    avg_loss:0.520, val_acc:0.794]
Epoch [14/120    avg_loss:0.540, val_acc:0.827]
Epoch [15/120    avg_loss:0.448, val_acc:0.741]
Epoch [16/120    avg_loss:0.418, val_acc:0.858]
Epoch [17/120    avg_loss:0.341, val_acc:0.760]
Epoch [18/120    avg_loss:0.412, val_acc:0.851]
Epoch [19/120    avg_loss:0.329, val_acc:0.880]
Epoch [20/120    avg_loss:0.275, val_acc:0.889]
Epoch [21/120    avg_loss:0.240, val_acc:0.872]
Epoch [22/120    avg_loss:0.258, val_acc:0.894]
Epoch [23/120    avg_loss:0.218, val_acc:0.886]
Epoch [24/120    avg_loss:0.206, val_acc:0.910]
Epoch [25/120    avg_loss:0.177, val_acc:0.856]
Epoch [26/120    avg_loss:0.181, val_acc:0.907]
Epoch [27/120    avg_loss:0.299, val_acc:0.891]
Epoch [28/120    avg_loss:0.176, val_acc:0.923]
Epoch [29/120    avg_loss:0.162, val_acc:0.905]
Epoch [30/120    avg_loss:0.171, val_acc:0.915]
Epoch [31/120    avg_loss:0.137, val_acc:0.871]
Epoch [32/120    avg_loss:0.147, val_acc:0.948]
Epoch [33/120    avg_loss:0.120, val_acc:0.943]
Epoch [34/120    avg_loss:0.105, val_acc:0.938]
Epoch [35/120    avg_loss:0.091, val_acc:0.955]
Epoch [36/120    avg_loss:0.108, val_acc:0.953]
Epoch [37/120    avg_loss:0.131, val_acc:0.924]
Epoch [38/120    avg_loss:0.138, val_acc:0.941]
Epoch [39/120    avg_loss:0.092, val_acc:0.965]
Epoch [40/120    avg_loss:0.073, val_acc:0.948]
Epoch [41/120    avg_loss:0.075, val_acc:0.951]
Epoch [42/120    avg_loss:0.165, val_acc:0.941]
Epoch [43/120    avg_loss:0.167, val_acc:0.938]
Epoch [44/120    avg_loss:0.086, val_acc:0.951]
Epoch [45/120    avg_loss:0.082, val_acc:0.934]
Epoch [46/120    avg_loss:0.054, val_acc:0.950]
Epoch [47/120    avg_loss:0.041, val_acc:0.966]
Epoch [48/120    avg_loss:0.082, val_acc:0.954]
Epoch [49/120    avg_loss:0.101, val_acc:0.943]
Epoch [50/120    avg_loss:0.121, val_acc:0.923]
Epoch [51/120    avg_loss:0.086, val_acc:0.948]
Epoch [52/120    avg_loss:0.125, val_acc:0.924]
Epoch [53/120    avg_loss:0.074, val_acc:0.958]
Epoch [54/120    avg_loss:0.057, val_acc:0.967]
Epoch [55/120    avg_loss:0.061, val_acc:0.955]
Epoch [56/120    avg_loss:0.086, val_acc:0.952]
Epoch [57/120    avg_loss:0.046, val_acc:0.959]
Epoch [58/120    avg_loss:0.077, val_acc:0.906]
Epoch [59/120    avg_loss:0.099, val_acc:0.965]
Epoch [60/120    avg_loss:0.064, val_acc:0.968]
Epoch [61/120    avg_loss:0.043, val_acc:0.970]
Epoch [62/120    avg_loss:0.055, val_acc:0.960]
Epoch [63/120    avg_loss:0.045, val_acc:0.964]
Epoch [64/120    avg_loss:0.049, val_acc:0.958]
Epoch [65/120    avg_loss:0.071, val_acc:0.952]
Epoch [66/120    avg_loss:0.064, val_acc:0.952]
Epoch [67/120    avg_loss:0.075, val_acc:0.967]
Epoch [68/120    avg_loss:0.042, val_acc:0.955]
Epoch [69/120    avg_loss:0.046, val_acc:0.965]
Epoch [70/120    avg_loss:0.053, val_acc:0.964]
Epoch [71/120    avg_loss:0.035, val_acc:0.975]
Epoch [72/120    avg_loss:0.030, val_acc:0.968]
Epoch [73/120    avg_loss:0.036, val_acc:0.964]
Epoch [74/120    avg_loss:0.018, val_acc:0.973]
Epoch [75/120    avg_loss:0.025, val_acc:0.975]
Epoch [76/120    avg_loss:0.013, val_acc:0.973]
Epoch [77/120    avg_loss:0.035, val_acc:0.978]
Epoch [78/120    avg_loss:0.034, val_acc:0.975]
Epoch [79/120    avg_loss:0.030, val_acc:0.972]
Epoch [80/120    avg_loss:0.037, val_acc:0.956]
Epoch [81/120    avg_loss:0.038, val_acc:0.966]
Epoch [82/120    avg_loss:0.021, val_acc:0.968]
Epoch [83/120    avg_loss:0.031, val_acc:0.966]
Epoch [84/120    avg_loss:0.037, val_acc:0.967]
Epoch [85/120    avg_loss:0.037, val_acc:0.970]
Epoch [86/120    avg_loss:0.020, val_acc:0.974]
Epoch [87/120    avg_loss:0.037, val_acc:0.972]
Epoch [88/120    avg_loss:0.028, val_acc:0.968]
Epoch [89/120    avg_loss:0.029, val_acc:0.965]
Epoch [90/120    avg_loss:0.022, val_acc:0.969]
Epoch [91/120    avg_loss:0.014, val_acc:0.973]
Epoch [92/120    avg_loss:0.010, val_acc:0.975]
Epoch [93/120    avg_loss:0.017, val_acc:0.978]
Epoch [94/120    avg_loss:0.013, val_acc:0.973]
Epoch [95/120    avg_loss:0.008, val_acc:0.978]
Epoch [96/120    avg_loss:0.010, val_acc:0.979]
Epoch [97/120    avg_loss:0.008, val_acc:0.977]
Epoch [98/120    avg_loss:0.007, val_acc:0.978]
Epoch [99/120    avg_loss:0.011, val_acc:0.977]
Epoch [100/120    avg_loss:0.010, val_acc:0.979]
Epoch [101/120    avg_loss:0.010, val_acc:0.978]
Epoch [102/120    avg_loss:0.008, val_acc:0.977]
Epoch [103/120    avg_loss:0.009, val_acc:0.976]
Epoch [104/120    avg_loss:0.011, val_acc:0.976]
Epoch [105/120    avg_loss:0.009, val_acc:0.978]
Epoch [106/120    avg_loss:0.008, val_acc:0.979]
Epoch [107/120    avg_loss:0.012, val_acc:0.979]
Epoch [108/120    avg_loss:0.008, val_acc:0.979]
Epoch [109/120    avg_loss:0.008, val_acc:0.979]
Epoch [110/120    avg_loss:0.011, val_acc:0.982]
Epoch [111/120    avg_loss:0.008, val_acc:0.981]
Epoch [112/120    avg_loss:0.008, val_acc:0.979]
Epoch [113/120    avg_loss:0.007, val_acc:0.980]
Epoch [114/120    avg_loss:0.009, val_acc:0.979]
Epoch [115/120    avg_loss:0.009, val_acc:0.979]
Epoch [116/120    avg_loss:0.007, val_acc:0.981]
Epoch [117/120    avg_loss:0.008, val_acc:0.979]
Epoch [118/120    avg_loss:0.009, val_acc:0.978]
Epoch [119/120    avg_loss:0.008, val_acc:0.979]
Epoch [120/120    avg_loss:0.006, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    1    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1248    2    8    1    0    0    0    0    6   19    1    0
     0    0    0]
 [   0    0    0  722    7    7    0    0    0    1    2    1    6    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    1    0    0    0    1  847   20    0    0
     2    0    0]
 [   0    0    1    0    0    0    0    3    0    3   10 2187    5    1
     0    0    0]
 [   0    0    0    4    0    0    0    0    0    0    2    0  523    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1129    9    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    54  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.8970189701897

F1 scores:
[       nan 0.98765432 0.98345154 0.97898305 0.96598639 0.98748578
 0.99847561 0.92592593 0.99883856 0.87804878 0.97244546 0.98535706
 0.97665733 0.99462366 0.97160069 0.89877301 0.97619048]

Kappa:
0.9760158027535544
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f07019c9a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.675, val_acc:0.507]
Epoch [2/120    avg_loss:2.312, val_acc:0.537]
Epoch [3/120    avg_loss:2.024, val_acc:0.574]
Epoch [4/120    avg_loss:1.794, val_acc:0.580]
Epoch [5/120    avg_loss:1.613, val_acc:0.632]
Epoch [6/120    avg_loss:1.397, val_acc:0.679]
Epoch [7/120    avg_loss:1.230, val_acc:0.688]
Epoch [8/120    avg_loss:1.137, val_acc:0.723]
Epoch [9/120    avg_loss:0.979, val_acc:0.762]
Epoch [10/120    avg_loss:0.894, val_acc:0.790]
Epoch [11/120    avg_loss:0.760, val_acc:0.834]
Epoch [12/120    avg_loss:0.714, val_acc:0.801]
Epoch [13/120    avg_loss:0.646, val_acc:0.821]
Epoch [14/120    avg_loss:0.691, val_acc:0.803]
Epoch [15/120    avg_loss:0.556, val_acc:0.847]
Epoch [16/120    avg_loss:0.525, val_acc:0.817]
Epoch [17/120    avg_loss:0.513, val_acc:0.795]
Epoch [18/120    avg_loss:0.407, val_acc:0.845]
Epoch [19/120    avg_loss:0.365, val_acc:0.880]
Epoch [20/120    avg_loss:0.327, val_acc:0.863]
Epoch [21/120    avg_loss:0.333, val_acc:0.889]
Epoch [22/120    avg_loss:0.291, val_acc:0.847]
Epoch [23/120    avg_loss:0.303, val_acc:0.880]
Epoch [24/120    avg_loss:0.292, val_acc:0.869]
Epoch [25/120    avg_loss:0.226, val_acc:0.908]
Epoch [26/120    avg_loss:0.182, val_acc:0.918]
Epoch [27/120    avg_loss:0.216, val_acc:0.869]
Epoch [28/120    avg_loss:0.191, val_acc:0.913]
Epoch [29/120    avg_loss:0.193, val_acc:0.916]
Epoch [30/120    avg_loss:0.157, val_acc:0.904]
Epoch [31/120    avg_loss:0.172, val_acc:0.904]
Epoch [32/120    avg_loss:0.168, val_acc:0.927]
Epoch [33/120    avg_loss:0.178, val_acc:0.912]
Epoch [34/120    avg_loss:0.133, val_acc:0.944]
Epoch [35/120    avg_loss:0.142, val_acc:0.921]
Epoch [36/120    avg_loss:0.114, val_acc:0.936]
Epoch [37/120    avg_loss:0.134, val_acc:0.903]
Epoch [38/120    avg_loss:0.115, val_acc:0.908]
Epoch [39/120    avg_loss:0.106, val_acc:0.934]
Epoch [40/120    avg_loss:0.072, val_acc:0.952]
Epoch [41/120    avg_loss:0.102, val_acc:0.946]
Epoch [42/120    avg_loss:0.086, val_acc:0.944]
Epoch [43/120    avg_loss:0.111, val_acc:0.950]
Epoch [44/120    avg_loss:0.063, val_acc:0.949]
Epoch [45/120    avg_loss:0.100, val_acc:0.944]
Epoch [46/120    avg_loss:0.057, val_acc:0.951]
Epoch [47/120    avg_loss:0.056, val_acc:0.947]
Epoch [48/120    avg_loss:0.069, val_acc:0.952]
Epoch [49/120    avg_loss:0.078, val_acc:0.941]
Epoch [50/120    avg_loss:0.050, val_acc:0.954]
Epoch [51/120    avg_loss:0.055, val_acc:0.953]
Epoch [52/120    avg_loss:0.046, val_acc:0.965]
Epoch [53/120    avg_loss:0.081, val_acc:0.963]
Epoch [54/120    avg_loss:0.084, val_acc:0.956]
Epoch [55/120    avg_loss:0.082, val_acc:0.946]
Epoch [56/120    avg_loss:0.087, val_acc:0.953]
Epoch [57/120    avg_loss:0.072, val_acc:0.947]
Epoch [58/120    avg_loss:0.058, val_acc:0.960]
Epoch [59/120    avg_loss:0.060, val_acc:0.952]
Epoch [60/120    avg_loss:0.045, val_acc:0.940]
Epoch [61/120    avg_loss:0.039, val_acc:0.952]
Epoch [62/120    avg_loss:0.034, val_acc:0.968]
Epoch [63/120    avg_loss:0.040, val_acc:0.963]
Epoch [64/120    avg_loss:0.027, val_acc:0.971]
Epoch [65/120    avg_loss:0.039, val_acc:0.969]
Epoch [66/120    avg_loss:0.021, val_acc:0.959]
Epoch [67/120    avg_loss:0.047, val_acc:0.952]
Epoch [68/120    avg_loss:0.112, val_acc:0.941]
Epoch [69/120    avg_loss:0.054, val_acc:0.956]
Epoch [70/120    avg_loss:0.053, val_acc:0.970]
Epoch [71/120    avg_loss:0.033, val_acc:0.960]
Epoch [72/120    avg_loss:0.028, val_acc:0.960]
Epoch [73/120    avg_loss:0.024, val_acc:0.969]
Epoch [74/120    avg_loss:0.037, val_acc:0.968]
Epoch [75/120    avg_loss:0.019, val_acc:0.969]
Epoch [76/120    avg_loss:0.033, val_acc:0.962]
Epoch [77/120    avg_loss:0.033, val_acc:0.967]
Epoch [78/120    avg_loss:0.026, val_acc:0.972]
Epoch [79/120    avg_loss:0.014, val_acc:0.973]
Epoch [80/120    avg_loss:0.014, val_acc:0.975]
Epoch [81/120    avg_loss:0.013, val_acc:0.976]
Epoch [82/120    avg_loss:0.017, val_acc:0.976]
Epoch [83/120    avg_loss:0.013, val_acc:0.976]
Epoch [84/120    avg_loss:0.012, val_acc:0.975]
Epoch [85/120    avg_loss:0.015, val_acc:0.971]
Epoch [86/120    avg_loss:0.010, val_acc:0.974]
Epoch [87/120    avg_loss:0.013, val_acc:0.974]
Epoch [88/120    avg_loss:0.012, val_acc:0.973]
Epoch [89/120    avg_loss:0.013, val_acc:0.975]
Epoch [90/120    avg_loss:0.012, val_acc:0.975]
Epoch [91/120    avg_loss:0.012, val_acc:0.974]
Epoch [92/120    avg_loss:0.012, val_acc:0.974]
Epoch [93/120    avg_loss:0.012, val_acc:0.974]
Epoch [94/120    avg_loss:0.012, val_acc:0.975]
Epoch [95/120    avg_loss:0.013, val_acc:0.976]
Epoch [96/120    avg_loss:0.009, val_acc:0.975]
Epoch [97/120    avg_loss:0.009, val_acc:0.979]
Epoch [98/120    avg_loss:0.011, val_acc:0.979]
Epoch [99/120    avg_loss:0.011, val_acc:0.976]
Epoch [100/120    avg_loss:0.010, val_acc:0.978]
Epoch [101/120    avg_loss:0.012, val_acc:0.978]
Epoch [102/120    avg_loss:0.010, val_acc:0.976]
Epoch [103/120    avg_loss:0.010, val_acc:0.978]
Epoch [104/120    avg_loss:0.009, val_acc:0.978]
Epoch [105/120    avg_loss:0.013, val_acc:0.971]
Epoch [106/120    avg_loss:0.008, val_acc:0.971]
Epoch [107/120    avg_loss:0.013, val_acc:0.975]
Epoch [108/120    avg_loss:0.010, val_acc:0.974]
Epoch [109/120    avg_loss:0.009, val_acc:0.972]
Epoch [110/120    avg_loss:0.009, val_acc:0.976]
Epoch [111/120    avg_loss:0.010, val_acc:0.976]
Epoch [112/120    avg_loss:0.009, val_acc:0.975]
Epoch [113/120    avg_loss:0.008, val_acc:0.976]
Epoch [114/120    avg_loss:0.013, val_acc:0.978]
Epoch [115/120    avg_loss:0.008, val_acc:0.978]
Epoch [116/120    avg_loss:0.011, val_acc:0.976]
Epoch [117/120    avg_loss:0.011, val_acc:0.975]
Epoch [118/120    avg_loss:0.009, val_acc:0.976]
Epoch [119/120    avg_loss:0.009, val_acc:0.976]
Epoch [120/120    avg_loss:0.011, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1247    5   12    0    1    0    0    0   13    7    0    0
     0    0    0]
 [   0    0    0  707   10    0    0    0    0   22    0    0    6    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    2    0    0
     5    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    3    0    0    0  866    3    0    0
     0    0    0]
 [   0    0   12    0    0    1    1    1    0    1   20 2168    6    0
     0    0    0]
 [   0    0    2    2    0    2    0    0    0    2    6    2  510    0
     1    6    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    0    0    0    0
  1130    5    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    58  274    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.24661246612466

F1 scores:
[       nan 0.98765432 0.97842291 0.96783025 0.95089286 0.98627002
 0.97744361 0.96       0.99883586 0.51724138 0.97248737 0.98724954
 0.9631728  0.99462366 0.96705178 0.86708861 0.98203593]

Kappa:
0.9686208047191717
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcd773eba58>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.673, val_acc:0.378]
Epoch [2/120    avg_loss:2.307, val_acc:0.513]
Epoch [3/120    avg_loss:2.037, val_acc:0.529]
Epoch [4/120    avg_loss:1.804, val_acc:0.524]
Epoch [5/120    avg_loss:1.617, val_acc:0.564]
Epoch [6/120    avg_loss:1.424, val_acc:0.644]
Epoch [7/120    avg_loss:1.320, val_acc:0.661]
Epoch [8/120    avg_loss:1.168, val_acc:0.681]
Epoch [9/120    avg_loss:1.059, val_acc:0.677]
Epoch [10/120    avg_loss:0.937, val_acc:0.746]
Epoch [11/120    avg_loss:0.810, val_acc:0.762]
Epoch [12/120    avg_loss:0.731, val_acc:0.753]
Epoch [13/120    avg_loss:0.696, val_acc:0.761]
Epoch [14/120    avg_loss:0.751, val_acc:0.809]
Epoch [15/120    avg_loss:0.588, val_acc:0.794]
Epoch [16/120    avg_loss:0.515, val_acc:0.827]
Epoch [17/120    avg_loss:0.517, val_acc:0.812]
Epoch [18/120    avg_loss:0.447, val_acc:0.887]
Epoch [19/120    avg_loss:0.387, val_acc:0.880]
Epoch [20/120    avg_loss:0.358, val_acc:0.888]
Epoch [21/120    avg_loss:0.354, val_acc:0.844]
Epoch [22/120    avg_loss:0.390, val_acc:0.852]
Epoch [23/120    avg_loss:0.389, val_acc:0.875]
Epoch [24/120    avg_loss:0.280, val_acc:0.890]
Epoch [25/120    avg_loss:0.224, val_acc:0.891]
Epoch [26/120    avg_loss:0.206, val_acc:0.887]
Epoch [27/120    avg_loss:0.183, val_acc:0.916]
Epoch [28/120    avg_loss:0.191, val_acc:0.905]
Epoch [29/120    avg_loss:0.233, val_acc:0.914]
Epoch [30/120    avg_loss:0.194, val_acc:0.925]
Epoch [31/120    avg_loss:0.142, val_acc:0.930]
Epoch [32/120    avg_loss:0.153, val_acc:0.925]
Epoch [33/120    avg_loss:0.192, val_acc:0.917]
Epoch [34/120    avg_loss:0.133, val_acc:0.945]
Epoch [35/120    avg_loss:0.128, val_acc:0.907]
Epoch [36/120    avg_loss:0.150, val_acc:0.928]
Epoch [37/120    avg_loss:0.101, val_acc:0.938]
Epoch [38/120    avg_loss:0.168, val_acc:0.938]
Epoch [39/120    avg_loss:0.109, val_acc:0.945]
Epoch [40/120    avg_loss:0.082, val_acc:0.934]
Epoch [41/120    avg_loss:0.084, val_acc:0.938]
Epoch [42/120    avg_loss:0.078, val_acc:0.940]
Epoch [43/120    avg_loss:0.084, val_acc:0.948]
Epoch [44/120    avg_loss:0.083, val_acc:0.945]
Epoch [45/120    avg_loss:0.073, val_acc:0.942]
Epoch [46/120    avg_loss:0.067, val_acc:0.956]
Epoch [47/120    avg_loss:0.105, val_acc:0.944]
Epoch [48/120    avg_loss:0.110, val_acc:0.938]
Epoch [49/120    avg_loss:0.149, val_acc:0.923]
Epoch [50/120    avg_loss:0.105, val_acc:0.945]
Epoch [51/120    avg_loss:0.085, val_acc:0.952]
Epoch [52/120    avg_loss:0.057, val_acc:0.945]
Epoch [53/120    avg_loss:0.163, val_acc:0.798]
Epoch [54/120    avg_loss:0.137, val_acc:0.903]
Epoch [55/120    avg_loss:0.098, val_acc:0.915]
Epoch [56/120    avg_loss:0.090, val_acc:0.946]
Epoch [57/120    avg_loss:0.074, val_acc:0.949]
Epoch [58/120    avg_loss:0.077, val_acc:0.949]
Epoch [59/120    avg_loss:0.105, val_acc:0.906]
Epoch [60/120    avg_loss:0.086, val_acc:0.942]
Epoch [61/120    avg_loss:0.072, val_acc:0.952]
Epoch [62/120    avg_loss:0.054, val_acc:0.959]
Epoch [63/120    avg_loss:0.048, val_acc:0.961]
Epoch [64/120    avg_loss:0.044, val_acc:0.963]
Epoch [65/120    avg_loss:0.035, val_acc:0.967]
Epoch [66/120    avg_loss:0.043, val_acc:0.966]
Epoch [67/120    avg_loss:0.035, val_acc:0.968]
Epoch [68/120    avg_loss:0.038, val_acc:0.968]
Epoch [69/120    avg_loss:0.043, val_acc:0.971]
Epoch [70/120    avg_loss:0.036, val_acc:0.972]
Epoch [71/120    avg_loss:0.035, val_acc:0.973]
Epoch [72/120    avg_loss:0.030, val_acc:0.972]
Epoch [73/120    avg_loss:0.033, val_acc:0.973]
Epoch [74/120    avg_loss:0.041, val_acc:0.973]
Epoch [75/120    avg_loss:0.034, val_acc:0.974]
Epoch [76/120    avg_loss:0.033, val_acc:0.976]
Epoch [77/120    avg_loss:0.028, val_acc:0.975]
Epoch [78/120    avg_loss:0.024, val_acc:0.976]
Epoch [79/120    avg_loss:0.027, val_acc:0.976]
Epoch [80/120    avg_loss:0.032, val_acc:0.975]
Epoch [81/120    avg_loss:0.027, val_acc:0.973]
Epoch [82/120    avg_loss:0.029, val_acc:0.972]
Epoch [83/120    avg_loss:0.024, val_acc:0.973]
Epoch [84/120    avg_loss:0.032, val_acc:0.974]
Epoch [85/120    avg_loss:0.029, val_acc:0.975]
Epoch [86/120    avg_loss:0.028, val_acc:0.974]
Epoch [87/120    avg_loss:0.026, val_acc:0.973]
Epoch [88/120    avg_loss:0.029, val_acc:0.975]
Epoch [89/120    avg_loss:0.025, val_acc:0.974]
Epoch [90/120    avg_loss:0.023, val_acc:0.976]
Epoch [91/120    avg_loss:0.026, val_acc:0.977]
Epoch [92/120    avg_loss:0.027, val_acc:0.978]
Epoch [93/120    avg_loss:0.025, val_acc:0.977]
Epoch [94/120    avg_loss:0.025, val_acc:0.977]
Epoch [95/120    avg_loss:0.023, val_acc:0.975]
Epoch [96/120    avg_loss:0.023, val_acc:0.978]
Epoch [97/120    avg_loss:0.029, val_acc:0.975]
Epoch [98/120    avg_loss:0.023, val_acc:0.978]
Epoch [99/120    avg_loss:0.024, val_acc:0.978]
Epoch [100/120    avg_loss:0.022, val_acc:0.978]
Epoch [101/120    avg_loss:0.023, val_acc:0.976]
Epoch [102/120    avg_loss:0.022, val_acc:0.980]
Epoch [103/120    avg_loss:0.025, val_acc:0.977]
Epoch [104/120    avg_loss:0.022, val_acc:0.977]
Epoch [105/120    avg_loss:0.019, val_acc:0.977]
Epoch [106/120    avg_loss:0.018, val_acc:0.977]
Epoch [107/120    avg_loss:0.020, val_acc:0.976]
Epoch [108/120    avg_loss:0.020, val_acc:0.976]
Epoch [109/120    avg_loss:0.023, val_acc:0.976]
Epoch [110/120    avg_loss:0.021, val_acc:0.977]
Epoch [111/120    avg_loss:0.022, val_acc:0.976]
Epoch [112/120    avg_loss:0.018, val_acc:0.976]
Epoch [113/120    avg_loss:0.023, val_acc:0.977]
Epoch [114/120    avg_loss:0.019, val_acc:0.976]
Epoch [115/120    avg_loss:0.020, val_acc:0.977]
Epoch [116/120    avg_loss:0.019, val_acc:0.977]
Epoch [117/120    avg_loss:0.021, val_acc:0.977]
Epoch [118/120    avg_loss:0.020, val_acc:0.977]
Epoch [119/120    avg_loss:0.023, val_acc:0.977]
Epoch [120/120    avg_loss:0.025, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1233    9    6    0    0    0    0    0    6   30    1    0
     0    0    0]
 [   0    0    0  707    3    0    0    0    0    3    1    2   30    1
     0    0    0]
 [   0    0    0    2  210    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    3    0    0    0    1  854    7    2    0
     1    3    0]
 [   0    0   20    0   14    0    2    0    0    1    6 2153   13    0
     1    0    0]
 [   0    0    1    1    0    0    0    0    0    0    0    0  527    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1127   12    0]
 [   0    0    0    0    0    0   27    0    0    0    0    0    0    0
    27  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.23577235772358

F1 scores:
[       nan 1.         0.9697208  0.96387185 0.94170404 0.99194476
 0.9761194  1.         1.         0.82926829 0.9804822  0.97796957
 0.95040577 0.99730458 0.98       0.89057751 0.98224852]

Kappa:
0.9685037378576964
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd185ec5b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.681, val_acc:0.281]
Epoch [2/120    avg_loss:2.333, val_acc:0.355]
Epoch [3/120    avg_loss:2.069, val_acc:0.516]
Epoch [4/120    avg_loss:1.841, val_acc:0.554]
Epoch [5/120    avg_loss:1.676, val_acc:0.592]
Epoch [6/120    avg_loss:1.556, val_acc:0.623]
Epoch [7/120    avg_loss:1.399, val_acc:0.650]
Epoch [8/120    avg_loss:1.198, val_acc:0.712]
Epoch [9/120    avg_loss:1.026, val_acc:0.722]
Epoch [10/120    avg_loss:0.974, val_acc:0.764]
Epoch [11/120    avg_loss:0.920, val_acc:0.739]
Epoch [12/120    avg_loss:0.758, val_acc:0.800]
Epoch [13/120    avg_loss:0.640, val_acc:0.834]
Epoch [14/120    avg_loss:0.589, val_acc:0.853]
Epoch [15/120    avg_loss:0.519, val_acc:0.821]
Epoch [16/120    avg_loss:0.572, val_acc:0.863]
Epoch [17/120    avg_loss:0.458, val_acc:0.884]
Epoch [18/120    avg_loss:0.407, val_acc:0.871]
Epoch [19/120    avg_loss:0.446, val_acc:0.855]
Epoch [20/120    avg_loss:0.347, val_acc:0.887]
Epoch [21/120    avg_loss:0.284, val_acc:0.851]
Epoch [22/120    avg_loss:0.305, val_acc:0.902]
Epoch [23/120    avg_loss:0.299, val_acc:0.901]
Epoch [24/120    avg_loss:0.231, val_acc:0.904]
Epoch [25/120    avg_loss:0.240, val_acc:0.909]
Epoch [26/120    avg_loss:0.206, val_acc:0.917]
Epoch [27/120    avg_loss:0.274, val_acc:0.905]
Epoch [28/120    avg_loss:0.225, val_acc:0.916]
Epoch [29/120    avg_loss:0.196, val_acc:0.916]
Epoch [30/120    avg_loss:0.198, val_acc:0.901]
Epoch [31/120    avg_loss:0.182, val_acc:0.938]
Epoch [32/120    avg_loss:0.165, val_acc:0.893]
Epoch [33/120    avg_loss:0.192, val_acc:0.910]
Epoch [34/120    avg_loss:0.137, val_acc:0.899]
Epoch [35/120    avg_loss:0.131, val_acc:0.936]
Epoch [36/120    avg_loss:0.114, val_acc:0.932]
Epoch [37/120    avg_loss:0.200, val_acc:0.913]
Epoch [38/120    avg_loss:0.147, val_acc:0.939]
Epoch [39/120    avg_loss:0.097, val_acc:0.947]
Epoch [40/120    avg_loss:0.083, val_acc:0.950]
Epoch [41/120    avg_loss:0.095, val_acc:0.949]
Epoch [42/120    avg_loss:0.092, val_acc:0.950]
Epoch [43/120    avg_loss:0.073, val_acc:0.955]
Epoch [44/120    avg_loss:0.096, val_acc:0.933]
Epoch [45/120    avg_loss:0.151, val_acc:0.938]
Epoch [46/120    avg_loss:0.099, val_acc:0.948]
Epoch [47/120    avg_loss:0.076, val_acc:0.954]
Epoch [48/120    avg_loss:0.067, val_acc:0.954]
Epoch [49/120    avg_loss:0.074, val_acc:0.963]
Epoch [50/120    avg_loss:0.073, val_acc:0.932]
Epoch [51/120    avg_loss:0.086, val_acc:0.950]
Epoch [52/120    avg_loss:0.062, val_acc:0.963]
Epoch [53/120    avg_loss:0.142, val_acc:0.948]
Epoch [54/120    avg_loss:0.109, val_acc:0.963]
Epoch [55/120    avg_loss:0.063, val_acc:0.963]
Epoch [56/120    avg_loss:0.072, val_acc:0.944]
Epoch [57/120    avg_loss:0.058, val_acc:0.966]
Epoch [58/120    avg_loss:0.065, val_acc:0.952]
Epoch [59/120    avg_loss:0.048, val_acc:0.958]
Epoch [60/120    avg_loss:0.037, val_acc:0.952]
Epoch [61/120    avg_loss:0.049, val_acc:0.971]
Epoch [62/120    avg_loss:0.051, val_acc:0.954]
Epoch [63/120    avg_loss:0.042, val_acc:0.972]
Epoch [64/120    avg_loss:0.050, val_acc:0.971]
Epoch [65/120    avg_loss:0.045, val_acc:0.957]
Epoch [66/120    avg_loss:0.079, val_acc:0.964]
Epoch [67/120    avg_loss:0.056, val_acc:0.963]
Epoch [68/120    avg_loss:0.066, val_acc:0.963]
Epoch [69/120    avg_loss:0.055, val_acc:0.966]
Epoch [70/120    avg_loss:0.047, val_acc:0.967]
Epoch [71/120    avg_loss:0.050, val_acc:0.963]
Epoch [72/120    avg_loss:0.041, val_acc:0.942]
Epoch [73/120    avg_loss:0.192, val_acc:0.921]
Epoch [74/120    avg_loss:0.071, val_acc:0.967]
Epoch [75/120    avg_loss:0.048, val_acc:0.964]
Epoch [76/120    avg_loss:0.046, val_acc:0.969]
Epoch [77/120    avg_loss:0.025, val_acc:0.967]
Epoch [78/120    avg_loss:0.022, val_acc:0.968]
Epoch [79/120    avg_loss:0.027, val_acc:0.972]
Epoch [80/120    avg_loss:0.024, val_acc:0.971]
Epoch [81/120    avg_loss:0.019, val_acc:0.971]
Epoch [82/120    avg_loss:0.021, val_acc:0.973]
Epoch [83/120    avg_loss:0.022, val_acc:0.974]
Epoch [84/120    avg_loss:0.021, val_acc:0.974]
Epoch [85/120    avg_loss:0.024, val_acc:0.975]
Epoch [86/120    avg_loss:0.016, val_acc:0.975]
Epoch [87/120    avg_loss:0.016, val_acc:0.978]
Epoch [88/120    avg_loss:0.016, val_acc:0.979]
Epoch [89/120    avg_loss:0.016, val_acc:0.980]
Epoch [90/120    avg_loss:0.015, val_acc:0.978]
Epoch [91/120    avg_loss:0.015, val_acc:0.978]
Epoch [92/120    avg_loss:0.016, val_acc:0.979]
Epoch [93/120    avg_loss:0.017, val_acc:0.978]
Epoch [94/120    avg_loss:0.019, val_acc:0.975]
Epoch [95/120    avg_loss:0.020, val_acc:0.978]
Epoch [96/120    avg_loss:0.017, val_acc:0.975]
Epoch [97/120    avg_loss:0.014, val_acc:0.975]
Epoch [98/120    avg_loss:0.014, val_acc:0.978]
Epoch [99/120    avg_loss:0.016, val_acc:0.981]
Epoch [100/120    avg_loss:0.019, val_acc:0.982]
Epoch [101/120    avg_loss:0.015, val_acc:0.976]
Epoch [102/120    avg_loss:0.013, val_acc:0.982]
Epoch [103/120    avg_loss:0.018, val_acc:0.979]
Epoch [104/120    avg_loss:0.020, val_acc:0.981]
Epoch [105/120    avg_loss:0.016, val_acc:0.979]
Epoch [106/120    avg_loss:0.016, val_acc:0.979]
Epoch [107/120    avg_loss:0.014, val_acc:0.978]
Epoch [108/120    avg_loss:0.017, val_acc:0.975]
Epoch [109/120    avg_loss:0.013, val_acc:0.976]
Epoch [110/120    avg_loss:0.017, val_acc:0.978]
Epoch [111/120    avg_loss:0.015, val_acc:0.979]
Epoch [112/120    avg_loss:0.020, val_acc:0.982]
Epoch [113/120    avg_loss:0.014, val_acc:0.981]
Epoch [114/120    avg_loss:0.016, val_acc:0.979]
Epoch [115/120    avg_loss:0.015, val_acc:0.976]
Epoch [116/120    avg_loss:0.016, val_acc:0.980]
Epoch [117/120    avg_loss:0.013, val_acc:0.983]
Epoch [118/120    avg_loss:0.014, val_acc:0.981]
Epoch [119/120    avg_loss:0.012, val_acc:0.980]
Epoch [120/120    avg_loss:0.014, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1253    0    0    1    0    0    0    0   12   17    2    0
     0    0    0]
 [   0    0    0  717    1   23    0    0    0    1    0    0    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    0    1    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   21    0    0    4    0    0    0    0  849    0    0    0
     1    0    0]
 [   0    0    7    0    0    0    0    1    0    0   22 2177    2    1
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0    0    3  525    0
     1    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    1    0    0    0
  1129    7    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    58  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.6910569105691

F1 scores:
[       nan 0.98765432 0.97623685 0.9795082  0.99765808 0.96436526
 0.99093656 0.96153846 1.         0.94444444 0.96422487 0.98797368
 0.98314607 0.99730458 0.96951481 0.88151659 0.97674419]

Kappa:
0.9736720566155195
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f36e8111a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.709, val_acc:0.498]
Epoch [2/120    avg_loss:2.353, val_acc:0.517]
Epoch [3/120    avg_loss:2.033, val_acc:0.553]
Epoch [4/120    avg_loss:1.789, val_acc:0.578]
Epoch [5/120    avg_loss:1.556, val_acc:0.608]
Epoch [6/120    avg_loss:1.423, val_acc:0.626]
Epoch [7/120    avg_loss:1.260, val_acc:0.654]
Epoch [8/120    avg_loss:1.099, val_acc:0.704]
Epoch [9/120    avg_loss:0.959, val_acc:0.707]
Epoch [10/120    avg_loss:0.886, val_acc:0.757]
Epoch [11/120    avg_loss:0.750, val_acc:0.756]
Epoch [12/120    avg_loss:0.705, val_acc:0.816]
Epoch [13/120    avg_loss:0.681, val_acc:0.734]
Epoch [14/120    avg_loss:0.537, val_acc:0.796]
Epoch [15/120    avg_loss:0.592, val_acc:0.809]
Epoch [16/120    avg_loss:0.480, val_acc:0.779]
Epoch [17/120    avg_loss:0.469, val_acc:0.832]
Epoch [18/120    avg_loss:0.461, val_acc:0.847]
Epoch [19/120    avg_loss:0.426, val_acc:0.864]
Epoch [20/120    avg_loss:0.373, val_acc:0.875]
Epoch [21/120    avg_loss:0.303, val_acc:0.825]
Epoch [22/120    avg_loss:0.324, val_acc:0.884]
Epoch [23/120    avg_loss:0.410, val_acc:0.889]
Epoch [24/120    avg_loss:0.235, val_acc:0.903]
Epoch [25/120    avg_loss:0.257, val_acc:0.908]
Epoch [26/120    avg_loss:0.221, val_acc:0.916]
Epoch [27/120    avg_loss:0.261, val_acc:0.893]
Epoch [28/120    avg_loss:0.211, val_acc:0.916]
Epoch [29/120    avg_loss:0.202, val_acc:0.901]
Epoch [30/120    avg_loss:0.182, val_acc:0.933]
Epoch [31/120    avg_loss:0.144, val_acc:0.900]
Epoch [32/120    avg_loss:0.141, val_acc:0.892]
Epoch [33/120    avg_loss:0.168, val_acc:0.897]
Epoch [34/120    avg_loss:0.287, val_acc:0.877]
Epoch [35/120    avg_loss:0.153, val_acc:0.938]
Epoch [36/120    avg_loss:0.141, val_acc:0.935]
Epoch [37/120    avg_loss:0.161, val_acc:0.933]
Epoch [38/120    avg_loss:0.115, val_acc:0.934]
Epoch [39/120    avg_loss:0.128, val_acc:0.944]
Epoch [40/120    avg_loss:0.097, val_acc:0.952]
Epoch [41/120    avg_loss:0.100, val_acc:0.960]
Epoch [42/120    avg_loss:0.091, val_acc:0.948]
Epoch [43/120    avg_loss:0.103, val_acc:0.947]
Epoch [44/120    avg_loss:0.112, val_acc:0.954]
Epoch [45/120    avg_loss:0.078, val_acc:0.946]
Epoch [46/120    avg_loss:0.079, val_acc:0.955]
Epoch [47/120    avg_loss:0.097, val_acc:0.901]
Epoch [48/120    avg_loss:0.138, val_acc:0.915]
Epoch [49/120    avg_loss:0.110, val_acc:0.962]
Epoch [50/120    avg_loss:0.076, val_acc:0.949]
Epoch [51/120    avg_loss:0.070, val_acc:0.966]
Epoch [52/120    avg_loss:0.073, val_acc:0.965]
Epoch [53/120    avg_loss:0.089, val_acc:0.970]
Epoch [54/120    avg_loss:0.048, val_acc:0.962]
Epoch [55/120    avg_loss:0.069, val_acc:0.970]
Epoch [56/120    avg_loss:0.051, val_acc:0.967]
Epoch [57/120    avg_loss:0.085, val_acc:0.964]
Epoch [58/120    avg_loss:0.051, val_acc:0.970]
Epoch [59/120    avg_loss:0.058, val_acc:0.954]
Epoch [60/120    avg_loss:0.054, val_acc:0.920]
Epoch [61/120    avg_loss:0.064, val_acc:0.960]
Epoch [62/120    avg_loss:0.052, val_acc:0.957]
Epoch [63/120    avg_loss:0.064, val_acc:0.956]
Epoch [64/120    avg_loss:0.049, val_acc:0.964]
Epoch [65/120    avg_loss:0.027, val_acc:0.973]
Epoch [66/120    avg_loss:0.020, val_acc:0.973]
Epoch [67/120    avg_loss:0.029, val_acc:0.975]
Epoch [68/120    avg_loss:0.032, val_acc:0.969]
Epoch [69/120    avg_loss:0.032, val_acc:0.965]
Epoch [70/120    avg_loss:0.027, val_acc:0.969]
Epoch [71/120    avg_loss:0.048, val_acc:0.941]
Epoch [72/120    avg_loss:0.077, val_acc:0.967]
Epoch [73/120    avg_loss:0.060, val_acc:0.949]
Epoch [74/120    avg_loss:0.053, val_acc:0.966]
Epoch [75/120    avg_loss:0.046, val_acc:0.956]
Epoch [76/120    avg_loss:0.031, val_acc:0.969]
Epoch [77/120    avg_loss:0.040, val_acc:0.941]
Epoch [78/120    avg_loss:0.031, val_acc:0.978]
Epoch [79/120    avg_loss:0.044, val_acc:0.956]
Epoch [80/120    avg_loss:0.032, val_acc:0.975]
Epoch [81/120    avg_loss:0.056, val_acc:0.974]
Epoch [82/120    avg_loss:0.064, val_acc:0.955]
Epoch [83/120    avg_loss:0.035, val_acc:0.968]
Epoch [84/120    avg_loss:0.026, val_acc:0.967]
Epoch [85/120    avg_loss:0.023, val_acc:0.972]
Epoch [86/120    avg_loss:0.028, val_acc:0.960]
Epoch [87/120    avg_loss:0.027, val_acc:0.976]
Epoch [88/120    avg_loss:0.029, val_acc:0.984]
Epoch [89/120    avg_loss:0.075, val_acc:0.969]
Epoch [90/120    avg_loss:0.036, val_acc:0.969]
Epoch [91/120    avg_loss:0.026, val_acc:0.978]
Epoch [92/120    avg_loss:0.029, val_acc:0.968]
Epoch [93/120    avg_loss:0.032, val_acc:0.971]
Epoch [94/120    avg_loss:0.103, val_acc:0.940]
Epoch [95/120    avg_loss:0.030, val_acc:0.972]
Epoch [96/120    avg_loss:0.038, val_acc:0.939]
Epoch [97/120    avg_loss:0.027, val_acc:0.967]
Epoch [98/120    avg_loss:0.023, val_acc:0.978]
Epoch [99/120    avg_loss:0.022, val_acc:0.966]
Epoch [100/120    avg_loss:0.022, val_acc:0.979]
Epoch [101/120    avg_loss:0.017, val_acc:0.978]
Epoch [102/120    avg_loss:0.012, val_acc:0.979]
Epoch [103/120    avg_loss:0.008, val_acc:0.975]
Epoch [104/120    avg_loss:0.014, val_acc:0.976]
Epoch [105/120    avg_loss:0.007, val_acc:0.976]
Epoch [106/120    avg_loss:0.009, val_acc:0.975]
Epoch [107/120    avg_loss:0.012, val_acc:0.978]
Epoch [108/120    avg_loss:0.009, val_acc:0.979]
Epoch [109/120    avg_loss:0.008, val_acc:0.978]
Epoch [110/120    avg_loss:0.009, val_acc:0.979]
Epoch [111/120    avg_loss:0.009, val_acc:0.978]
Epoch [112/120    avg_loss:0.008, val_acc:0.980]
Epoch [113/120    avg_loss:0.007, val_acc:0.978]
Epoch [114/120    avg_loss:0.010, val_acc:0.979]
Epoch [115/120    avg_loss:0.009, val_acc:0.978]
Epoch [116/120    avg_loss:0.011, val_acc:0.978]
Epoch [117/120    avg_loss:0.010, val_acc:0.979]
Epoch [118/120    avg_loss:0.009, val_acc:0.979]
Epoch [119/120    avg_loss:0.009, val_acc:0.978]
Epoch [120/120    avg_loss:0.007, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1246    2    3    0    2    0    0    0    5   27    0    0
     0    0    0]
 [   0    0    0  727    0    3    0    0    0    2    2    0   13    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    7    1    0    0    0  846   11    1    0
     3    0    0]
 [   0    0    1    0    0    0    0    0    5    0   13 2190    0    0
     1    0    0]
 [   0    0    0    2    0    4    0    0    0    0    0    1  523    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1136    2    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    68  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.85365853658537

F1 scores:
[       nan 0.98765432 0.98187549 0.98376184 0.99300699 0.9841629
 0.9946687  1.         0.99188876 0.94736842 0.9707401  0.98604232
 0.97392924 1.         0.96763203 0.88571429 0.98224852]

Kappa:
0.9755108643783343
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ed4bb3a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.665, val_acc:0.466]
Epoch [2/120    avg_loss:2.316, val_acc:0.528]
Epoch [3/120    avg_loss:2.059, val_acc:0.547]
Epoch [4/120    avg_loss:1.880, val_acc:0.589]
Epoch [5/120    avg_loss:1.677, val_acc:0.623]
Epoch [6/120    avg_loss:1.469, val_acc:0.653]
Epoch [7/120    avg_loss:1.238, val_acc:0.696]
Epoch [8/120    avg_loss:1.140, val_acc:0.696]
Epoch [9/120    avg_loss:0.978, val_acc:0.732]
Epoch [10/120    avg_loss:0.872, val_acc:0.746]
Epoch [11/120    avg_loss:0.735, val_acc:0.763]
Epoch [12/120    avg_loss:0.647, val_acc:0.776]
Epoch [13/120    avg_loss:0.717, val_acc:0.788]
Epoch [14/120    avg_loss:0.692, val_acc:0.782]
Epoch [15/120    avg_loss:0.554, val_acc:0.807]
Epoch [16/120    avg_loss:0.515, val_acc:0.787]
Epoch [17/120    avg_loss:0.539, val_acc:0.821]
Epoch [18/120    avg_loss:0.416, val_acc:0.854]
Epoch [19/120    avg_loss:0.369, val_acc:0.799]
Epoch [20/120    avg_loss:0.402, val_acc:0.849]
Epoch [21/120    avg_loss:0.379, val_acc:0.861]
Epoch [22/120    avg_loss:0.286, val_acc:0.877]
Epoch [23/120    avg_loss:0.288, val_acc:0.842]
Epoch [24/120    avg_loss:0.309, val_acc:0.872]
Epoch [25/120    avg_loss:0.260, val_acc:0.862]
Epoch [26/120    avg_loss:0.236, val_acc:0.906]
Epoch [27/120    avg_loss:0.247, val_acc:0.891]
Epoch [28/120    avg_loss:0.212, val_acc:0.891]
Epoch [29/120    avg_loss:0.377, val_acc:0.815]
Epoch [30/120    avg_loss:0.267, val_acc:0.851]
Epoch [31/120    avg_loss:0.291, val_acc:0.897]
Epoch [32/120    avg_loss:0.164, val_acc:0.893]
Epoch [33/120    avg_loss:0.195, val_acc:0.913]
Epoch [34/120    avg_loss:0.220, val_acc:0.898]
Epoch [35/120    avg_loss:0.162, val_acc:0.865]
Epoch [36/120    avg_loss:0.177, val_acc:0.892]
Epoch [37/120    avg_loss:0.131, val_acc:0.913]
Epoch [38/120    avg_loss:0.112, val_acc:0.936]
Epoch [39/120    avg_loss:0.125, val_acc:0.898]
Epoch [40/120    avg_loss:0.140, val_acc:0.930]
Epoch [41/120    avg_loss:0.105, val_acc:0.921]
Epoch [42/120    avg_loss:0.070, val_acc:0.949]
Epoch [43/120    avg_loss:0.092, val_acc:0.926]
Epoch [44/120    avg_loss:0.103, val_acc:0.928]
Epoch [45/120    avg_loss:0.099, val_acc:0.946]
Epoch [46/120    avg_loss:0.101, val_acc:0.930]
Epoch [47/120    avg_loss:0.095, val_acc:0.932]
Epoch [48/120    avg_loss:0.085, val_acc:0.932]
Epoch [49/120    avg_loss:0.086, val_acc:0.950]
Epoch [50/120    avg_loss:0.118, val_acc:0.916]
Epoch [51/120    avg_loss:0.093, val_acc:0.949]
Epoch [52/120    avg_loss:0.055, val_acc:0.946]
Epoch [53/120    avg_loss:0.094, val_acc:0.928]
Epoch [54/120    avg_loss:0.057, val_acc:0.938]
Epoch [55/120    avg_loss:0.040, val_acc:0.940]
Epoch [56/120    avg_loss:0.098, val_acc:0.947]
Epoch [57/120    avg_loss:0.064, val_acc:0.961]
Epoch [58/120    avg_loss:0.046, val_acc:0.953]
Epoch [59/120    avg_loss:0.084, val_acc:0.950]
Epoch [60/120    avg_loss:0.105, val_acc:0.949]
Epoch [61/120    avg_loss:0.065, val_acc:0.949]
Epoch [62/120    avg_loss:0.081, val_acc:0.954]
Epoch [63/120    avg_loss:0.037, val_acc:0.960]
Epoch [64/120    avg_loss:0.035, val_acc:0.954]
Epoch [65/120    avg_loss:0.034, val_acc:0.960]
Epoch [66/120    avg_loss:0.118, val_acc:0.933]
Epoch [67/120    avg_loss:0.114, val_acc:0.958]
Epoch [68/120    avg_loss:0.062, val_acc:0.951]
Epoch [69/120    avg_loss:0.048, val_acc:0.948]
Epoch [70/120    avg_loss:0.039, val_acc:0.964]
Epoch [71/120    avg_loss:0.039, val_acc:0.960]
Epoch [72/120    avg_loss:0.031, val_acc:0.963]
Epoch [73/120    avg_loss:0.066, val_acc:0.954]
Epoch [74/120    avg_loss:0.038, val_acc:0.965]
Epoch [75/120    avg_loss:0.047, val_acc:0.954]
Epoch [76/120    avg_loss:0.036, val_acc:0.971]
Epoch [77/120    avg_loss:0.032, val_acc:0.969]
Epoch [78/120    avg_loss:0.034, val_acc:0.971]
Epoch [79/120    avg_loss:0.036, val_acc:0.974]
Epoch [80/120    avg_loss:0.043, val_acc:0.974]
Epoch [81/120    avg_loss:0.040, val_acc:0.963]
Epoch [82/120    avg_loss:0.037, val_acc:0.967]
Epoch [83/120    avg_loss:0.036, val_acc:0.942]
Epoch [84/120    avg_loss:0.086, val_acc:0.953]
Epoch [85/120    avg_loss:0.062, val_acc:0.956]
Epoch [86/120    avg_loss:0.033, val_acc:0.969]
Epoch [87/120    avg_loss:0.028, val_acc:0.967]
Epoch [88/120    avg_loss:0.022, val_acc:0.967]
Epoch [89/120    avg_loss:0.022, val_acc:0.968]
Epoch [90/120    avg_loss:0.024, val_acc:0.974]
Epoch [91/120    avg_loss:0.023, val_acc:0.959]
Epoch [92/120    avg_loss:0.026, val_acc:0.974]
Epoch [93/120    avg_loss:0.015, val_acc:0.980]
Epoch [94/120    avg_loss:0.011, val_acc:0.981]
Epoch [95/120    avg_loss:0.041, val_acc:0.954]
Epoch [96/120    avg_loss:0.051, val_acc:0.964]
Epoch [97/120    avg_loss:0.044, val_acc:0.962]
Epoch [98/120    avg_loss:0.032, val_acc:0.967]
Epoch [99/120    avg_loss:0.034, val_acc:0.960]
Epoch [100/120    avg_loss:0.042, val_acc:0.950]
Epoch [101/120    avg_loss:0.054, val_acc:0.965]
Epoch [102/120    avg_loss:0.031, val_acc:0.975]
Epoch [103/120    avg_loss:0.021, val_acc:0.974]
Epoch [104/120    avg_loss:0.035, val_acc:0.978]
Epoch [105/120    avg_loss:0.024, val_acc:0.974]
Epoch [106/120    avg_loss:0.024, val_acc:0.984]
Epoch [107/120    avg_loss:0.015, val_acc:0.977]
Epoch [108/120    avg_loss:0.020, val_acc:0.974]
Epoch [109/120    avg_loss:0.014, val_acc:0.971]
Epoch [110/120    avg_loss:0.018, val_acc:0.970]
Epoch [111/120    avg_loss:0.026, val_acc:0.977]
Epoch [112/120    avg_loss:0.017, val_acc:0.980]
Epoch [113/120    avg_loss:0.014, val_acc:0.979]
Epoch [114/120    avg_loss:0.013, val_acc:0.977]
Epoch [115/120    avg_loss:0.008, val_acc:0.978]
Epoch [116/120    avg_loss:0.009, val_acc:0.975]
Epoch [117/120    avg_loss:0.008, val_acc:0.977]
Epoch [118/120    avg_loss:0.007, val_acc:0.980]
Epoch [119/120    avg_loss:0.011, val_acc:0.967]
Epoch [120/120    avg_loss:0.009, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    1 1230    8   18    0    1    0    0    0    4   20    3    0
     0    0    0]
 [   0    0    0  709   10    0    5    0    0    5    3    0   14    0
     1    0    0]
 [   0    0    0    3  210    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    3    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    6    0    0   12    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    1    0    0    0  849   16    0    0
     0    4    0]
 [   0    0   10    2    0    0    1    0    1    0   13 2166   17    0
     0    0    0]
 [   0    0    0    1    0    0    8    0    0    0    5    2  517    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  184
     0    1    0]
 [   0    0    0    0    0    0    0    0    1    2    0    0    0    0
  1134    2    0]
 [   0    0    0    0    0    0   28    0    0    2    0    0    0    0
    96  221    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
96.35772357723577

F1 scores:
[       nan 0.98795181 0.97233202 0.96462585 0.93126386 0.99071926
 0.96258254 1.         0.99767981 0.57142857 0.97084048 0.98142275
 0.94775435 0.99728997 0.95454545 0.76869565 0.95705521]

Kappa:
0.9584560467762718
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc51e17ea90>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.626, val_acc:0.273]
Epoch [2/120    avg_loss:2.283, val_acc:0.485]
Epoch [3/120    avg_loss:2.088, val_acc:0.537]
Epoch [4/120    avg_loss:1.941, val_acc:0.562]
Epoch [5/120    avg_loss:1.739, val_acc:0.630]
Epoch [6/120    avg_loss:1.581, val_acc:0.611]
Epoch [7/120    avg_loss:1.443, val_acc:0.650]
Epoch [8/120    avg_loss:1.330, val_acc:0.716]
Epoch [9/120    avg_loss:1.165, val_acc:0.698]
Epoch [10/120    avg_loss:1.053, val_acc:0.737]
Epoch [11/120    avg_loss:0.924, val_acc:0.755]
Epoch [12/120    avg_loss:0.810, val_acc:0.757]
Epoch [13/120    avg_loss:0.718, val_acc:0.797]
Epoch [14/120    avg_loss:0.677, val_acc:0.788]
Epoch [15/120    avg_loss:0.606, val_acc:0.787]
Epoch [16/120    avg_loss:0.597, val_acc:0.784]
Epoch [17/120    avg_loss:0.595, val_acc:0.782]
Epoch [18/120    avg_loss:0.520, val_acc:0.841]
Epoch [19/120    avg_loss:0.442, val_acc:0.824]
Epoch [20/120    avg_loss:0.422, val_acc:0.792]
Epoch [21/120    avg_loss:0.421, val_acc:0.846]
Epoch [22/120    avg_loss:0.348, val_acc:0.855]
Epoch [23/120    avg_loss:0.359, val_acc:0.853]
Epoch [24/120    avg_loss:0.344, val_acc:0.867]
Epoch [25/120    avg_loss:0.314, val_acc:0.872]
Epoch [26/120    avg_loss:0.236, val_acc:0.892]
Epoch [27/120    avg_loss:0.261, val_acc:0.852]
Epoch [28/120    avg_loss:0.247, val_acc:0.885]
Epoch [29/120    avg_loss:0.259, val_acc:0.874]
Epoch [30/120    avg_loss:0.268, val_acc:0.899]
Epoch [31/120    avg_loss:0.192, val_acc:0.921]
Epoch [32/120    avg_loss:0.152, val_acc:0.868]
Epoch [33/120    avg_loss:0.191, val_acc:0.845]
Epoch [34/120    avg_loss:0.206, val_acc:0.904]
Epoch [35/120    avg_loss:0.186, val_acc:0.925]
Epoch [36/120    avg_loss:0.118, val_acc:0.940]
Epoch [37/120    avg_loss:0.118, val_acc:0.901]
Epoch [38/120    avg_loss:0.162, val_acc:0.909]
Epoch [39/120    avg_loss:0.119, val_acc:0.915]
Epoch [40/120    avg_loss:0.107, val_acc:0.911]
Epoch [41/120    avg_loss:0.120, val_acc:0.926]
Epoch [42/120    avg_loss:0.089, val_acc:0.929]
Epoch [43/120    avg_loss:0.119, val_acc:0.909]
Epoch [44/120    avg_loss:0.133, val_acc:0.932]
Epoch [45/120    avg_loss:0.101, val_acc:0.944]
Epoch [46/120    avg_loss:0.067, val_acc:0.959]
Epoch [47/120    avg_loss:0.071, val_acc:0.940]
Epoch [48/120    avg_loss:0.094, val_acc:0.911]
Epoch [49/120    avg_loss:0.097, val_acc:0.945]
Epoch [50/120    avg_loss:0.073, val_acc:0.936]
Epoch [51/120    avg_loss:0.089, val_acc:0.922]
Epoch [52/120    avg_loss:0.073, val_acc:0.913]
Epoch [53/120    avg_loss:0.129, val_acc:0.946]
Epoch [54/120    avg_loss:0.086, val_acc:0.939]
Epoch [55/120    avg_loss:0.079, val_acc:0.952]
Epoch [56/120    avg_loss:0.043, val_acc:0.951]
Epoch [57/120    avg_loss:0.080, val_acc:0.933]
Epoch [58/120    avg_loss:0.053, val_acc:0.949]
Epoch [59/120    avg_loss:0.076, val_acc:0.947]
Epoch [60/120    avg_loss:0.048, val_acc:0.964]
Epoch [61/120    avg_loss:0.043, val_acc:0.967]
Epoch [62/120    avg_loss:0.028, val_acc:0.967]
Epoch [63/120    avg_loss:0.032, val_acc:0.965]
Epoch [64/120    avg_loss:0.026, val_acc:0.966]
Epoch [65/120    avg_loss:0.025, val_acc:0.967]
Epoch [66/120    avg_loss:0.027, val_acc:0.967]
Epoch [67/120    avg_loss:0.030, val_acc:0.966]
Epoch [68/120    avg_loss:0.025, val_acc:0.968]
Epoch [69/120    avg_loss:0.035, val_acc:0.968]
Epoch [70/120    avg_loss:0.023, val_acc:0.968]
Epoch [71/120    avg_loss:0.026, val_acc:0.966]
Epoch [72/120    avg_loss:0.026, val_acc:0.967]
Epoch [73/120    avg_loss:0.021, val_acc:0.966]
Epoch [74/120    avg_loss:0.020, val_acc:0.968]
Epoch [75/120    avg_loss:0.024, val_acc:0.969]
Epoch [76/120    avg_loss:0.022, val_acc:0.969]
Epoch [77/120    avg_loss:0.021, val_acc:0.970]
Epoch [78/120    avg_loss:0.024, val_acc:0.971]
Epoch [79/120    avg_loss:0.023, val_acc:0.969]
Epoch [80/120    avg_loss:0.020, val_acc:0.970]
Epoch [81/120    avg_loss:0.018, val_acc:0.968]
Epoch [82/120    avg_loss:0.023, val_acc:0.968]
Epoch [83/120    avg_loss:0.019, val_acc:0.968]
Epoch [84/120    avg_loss:0.018, val_acc:0.970]
Epoch [85/120    avg_loss:0.024, val_acc:0.969]
Epoch [86/120    avg_loss:0.021, val_acc:0.970]
Epoch [87/120    avg_loss:0.020, val_acc:0.968]
Epoch [88/120    avg_loss:0.019, val_acc:0.968]
Epoch [89/120    avg_loss:0.019, val_acc:0.970]
Epoch [90/120    avg_loss:0.021, val_acc:0.971]
Epoch [91/120    avg_loss:0.017, val_acc:0.972]
Epoch [92/120    avg_loss:0.018, val_acc:0.969]
Epoch [93/120    avg_loss:0.023, val_acc:0.968]
Epoch [94/120    avg_loss:0.015, val_acc:0.969]
Epoch [95/120    avg_loss:0.020, val_acc:0.969]
Epoch [96/120    avg_loss:0.018, val_acc:0.972]
Epoch [97/120    avg_loss:0.017, val_acc:0.970]
Epoch [98/120    avg_loss:0.017, val_acc:0.969]
Epoch [99/120    avg_loss:0.014, val_acc:0.970]
Epoch [100/120    avg_loss:0.014, val_acc:0.974]
Epoch [101/120    avg_loss:0.014, val_acc:0.971]
Epoch [102/120    avg_loss:0.013, val_acc:0.971]
Epoch [103/120    avg_loss:0.017, val_acc:0.975]
Epoch [104/120    avg_loss:0.017, val_acc:0.975]
Epoch [105/120    avg_loss:0.022, val_acc:0.975]
Epoch [106/120    avg_loss:0.018, val_acc:0.972]
Epoch [107/120    avg_loss:0.016, val_acc:0.971]
Epoch [108/120    avg_loss:0.018, val_acc:0.970]
Epoch [109/120    avg_loss:0.018, val_acc:0.972]
Epoch [110/120    avg_loss:0.017, val_acc:0.969]
Epoch [111/120    avg_loss:0.013, val_acc:0.975]
Epoch [112/120    avg_loss:0.019, val_acc:0.972]
Epoch [113/120    avg_loss:0.016, val_acc:0.974]
Epoch [114/120    avg_loss:0.015, val_acc:0.976]
Epoch [115/120    avg_loss:0.013, val_acc:0.971]
Epoch [116/120    avg_loss:0.017, val_acc:0.974]
Epoch [117/120    avg_loss:0.013, val_acc:0.975]
Epoch [118/120    avg_loss:0.017, val_acc:0.972]
Epoch [119/120    avg_loss:0.013, val_acc:0.971]
Epoch [120/120    avg_loss:0.016, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    4 1260    1    7    0    0    0    0    0    6    7    0    0
     0    0    0]
 [   0    0    1  718    1    2    0    0    0    6    3    2   13    0
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    1    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0  857    9    0    0
     4    0    0]
 [   0    2    9    0    0    0    0    0    0    0   66 2119   14    0
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    0    7  518    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    1    0    0    0
  1132    2    0]
 [   0    0    0    0    0    1    2    0    0    3    0    0    0    0
    81  260    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.93224932249322

F1 scores:
[       nan 0.93181818 0.98630137 0.97886844 0.97921478 0.97963801
 0.99313501 1.         0.99883856 0.8        0.94800885 0.97335783
 0.95925926 1.         0.95769882 0.85106383 0.98224852]

Kappa:
0.9650494207957362
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2527891a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.624, val_acc:0.395]
Epoch [2/120    avg_loss:2.285, val_acc:0.439]
Epoch [3/120    avg_loss:2.026, val_acc:0.548]
Epoch [4/120    avg_loss:1.872, val_acc:0.584]
Epoch [5/120    avg_loss:1.641, val_acc:0.532]
Epoch [6/120    avg_loss:1.500, val_acc:0.645]
Epoch [7/120    avg_loss:1.392, val_acc:0.681]
Epoch [8/120    avg_loss:1.225, val_acc:0.668]
Epoch [9/120    avg_loss:1.021, val_acc:0.701]
Epoch [10/120    avg_loss:0.903, val_acc:0.745]
Epoch [11/120    avg_loss:0.821, val_acc:0.787]
Epoch [12/120    avg_loss:0.784, val_acc:0.763]
Epoch [13/120    avg_loss:0.701, val_acc:0.780]
Epoch [14/120    avg_loss:0.616, val_acc:0.830]
Epoch [15/120    avg_loss:0.578, val_acc:0.824]
Epoch [16/120    avg_loss:0.523, val_acc:0.835]
Epoch [17/120    avg_loss:0.465, val_acc:0.834]
Epoch [18/120    avg_loss:0.430, val_acc:0.863]
Epoch [19/120    avg_loss:0.440, val_acc:0.835]
Epoch [20/120    avg_loss:0.354, val_acc:0.863]
Epoch [21/120    avg_loss:0.356, val_acc:0.870]
Epoch [22/120    avg_loss:0.304, val_acc:0.893]
Epoch [23/120    avg_loss:0.283, val_acc:0.868]
Epoch [24/120    avg_loss:0.291, val_acc:0.819]
Epoch [25/120    avg_loss:0.328, val_acc:0.891]
Epoch [26/120    avg_loss:0.246, val_acc:0.892]
Epoch [27/120    avg_loss:0.256, val_acc:0.905]
Epoch [28/120    avg_loss:0.198, val_acc:0.911]
Epoch [29/120    avg_loss:0.228, val_acc:0.872]
Epoch [30/120    avg_loss:0.164, val_acc:0.920]
Epoch [31/120    avg_loss:0.161, val_acc:0.925]
Epoch [32/120    avg_loss:0.183, val_acc:0.841]
Epoch [33/120    avg_loss:0.190, val_acc:0.924]
Epoch [34/120    avg_loss:0.147, val_acc:0.918]
Epoch [35/120    avg_loss:0.093, val_acc:0.940]
Epoch [36/120    avg_loss:0.151, val_acc:0.916]
Epoch [37/120    avg_loss:0.122, val_acc:0.875]
Epoch [38/120    avg_loss:0.108, val_acc:0.955]
Epoch [39/120    avg_loss:0.086, val_acc:0.965]
Epoch [40/120    avg_loss:0.106, val_acc:0.892]
Epoch [41/120    avg_loss:0.112, val_acc:0.928]
Epoch [42/120    avg_loss:0.107, val_acc:0.956]
Epoch [43/120    avg_loss:0.103, val_acc:0.920]
Epoch [44/120    avg_loss:0.144, val_acc:0.936]
Epoch [45/120    avg_loss:0.090, val_acc:0.950]
Epoch [46/120    avg_loss:0.066, val_acc:0.942]
Epoch [47/120    avg_loss:0.223, val_acc:0.896]
Epoch [48/120    avg_loss:0.181, val_acc:0.937]
Epoch [49/120    avg_loss:0.098, val_acc:0.952]
Epoch [50/120    avg_loss:0.089, val_acc:0.937]
Epoch [51/120    avg_loss:0.061, val_acc:0.955]
Epoch [52/120    avg_loss:0.071, val_acc:0.958]
Epoch [53/120    avg_loss:0.053, val_acc:0.968]
Epoch [54/120    avg_loss:0.052, val_acc:0.971]
Epoch [55/120    avg_loss:0.042, val_acc:0.972]
Epoch [56/120    avg_loss:0.035, val_acc:0.974]
Epoch [57/120    avg_loss:0.035, val_acc:0.972]
Epoch [58/120    avg_loss:0.031, val_acc:0.974]
Epoch [59/120    avg_loss:0.027, val_acc:0.975]
Epoch [60/120    avg_loss:0.041, val_acc:0.978]
Epoch [61/120    avg_loss:0.032, val_acc:0.976]
Epoch [62/120    avg_loss:0.029, val_acc:0.976]
Epoch [63/120    avg_loss:0.027, val_acc:0.975]
Epoch [64/120    avg_loss:0.035, val_acc:0.974]
Epoch [65/120    avg_loss:0.028, val_acc:0.975]
Epoch [66/120    avg_loss:0.035, val_acc:0.976]
Epoch [67/120    avg_loss:0.031, val_acc:0.975]
Epoch [68/120    avg_loss:0.033, val_acc:0.976]
Epoch [69/120    avg_loss:0.032, val_acc:0.976]
Epoch [70/120    avg_loss:0.034, val_acc:0.975]
Epoch [71/120    avg_loss:0.032, val_acc:0.979]
Epoch [72/120    avg_loss:0.032, val_acc:0.976]
Epoch [73/120    avg_loss:0.030, val_acc:0.974]
Epoch [74/120    avg_loss:0.031, val_acc:0.978]
Epoch [75/120    avg_loss:0.029, val_acc:0.978]
Epoch [76/120    avg_loss:0.027, val_acc:0.979]
Epoch [77/120    avg_loss:0.030, val_acc:0.975]
Epoch [78/120    avg_loss:0.031, val_acc:0.975]
Epoch [79/120    avg_loss:0.031, val_acc:0.982]
Epoch [80/120    avg_loss:0.023, val_acc:0.982]
Epoch [81/120    avg_loss:0.022, val_acc:0.982]
Epoch [82/120    avg_loss:0.025, val_acc:0.981]
Epoch [83/120    avg_loss:0.029, val_acc:0.984]
Epoch [84/120    avg_loss:0.028, val_acc:0.981]
Epoch [85/120    avg_loss:0.030, val_acc:0.979]
Epoch [86/120    avg_loss:0.035, val_acc:0.975]
Epoch [87/120    avg_loss:0.024, val_acc:0.979]
Epoch [88/120    avg_loss:0.028, val_acc:0.976]
Epoch [89/120    avg_loss:0.028, val_acc:0.979]
Epoch [90/120    avg_loss:0.024, val_acc:0.979]
Epoch [91/120    avg_loss:0.035, val_acc:0.979]
Epoch [92/120    avg_loss:0.027, val_acc:0.980]
Epoch [93/120    avg_loss:0.025, val_acc:0.979]
Epoch [94/120    avg_loss:0.028, val_acc:0.979]
Epoch [95/120    avg_loss:0.026, val_acc:0.980]
Epoch [96/120    avg_loss:0.032, val_acc:0.980]
Epoch [97/120    avg_loss:0.027, val_acc:0.979]
Epoch [98/120    avg_loss:0.025, val_acc:0.979]
Epoch [99/120    avg_loss:0.031, val_acc:0.979]
Epoch [100/120    avg_loss:0.022, val_acc:0.980]
Epoch [101/120    avg_loss:0.020, val_acc:0.980]
Epoch [102/120    avg_loss:0.023, val_acc:0.980]
Epoch [103/120    avg_loss:0.026, val_acc:0.980]
Epoch [104/120    avg_loss:0.025, val_acc:0.980]
Epoch [105/120    avg_loss:0.025, val_acc:0.980]
Epoch [106/120    avg_loss:0.025, val_acc:0.981]
Epoch [107/120    avg_loss:0.026, val_acc:0.980]
Epoch [108/120    avg_loss:0.021, val_acc:0.980]
Epoch [109/120    avg_loss:0.022, val_acc:0.980]
Epoch [110/120    avg_loss:0.023, val_acc:0.980]
Epoch [111/120    avg_loss:0.026, val_acc:0.980]
Epoch [112/120    avg_loss:0.019, val_acc:0.980]
Epoch [113/120    avg_loss:0.022, val_acc:0.980]
Epoch [114/120    avg_loss:0.027, val_acc:0.980]
Epoch [115/120    avg_loss:0.026, val_acc:0.980]
Epoch [116/120    avg_loss:0.022, val_acc:0.980]
Epoch [117/120    avg_loss:0.024, val_acc:0.980]
Epoch [118/120    avg_loss:0.024, val_acc:0.980]
Epoch [119/120    avg_loss:0.023, val_acc:0.980]
Epoch [120/120    avg_loss:0.018, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1251    0    8    0    3    0    0    2    1   20    0    0
     0    0    0]
 [   0    0    2  699   17    9    0    0    0    8    2    1    9    0
     0    0    0]
 [   0    0    0    0  210    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   11    0    0    4    1    0    0    0  849    9    0    0
     0    1    0]
 [   0    0    6    0    0    0    0    0    0    0   17 2182    5    0
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    3    0  520    0
     0    3    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    63  282    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.44173441734418

F1 scores:
[       nan 0.975      0.97849042 0.96680498 0.9375     0.97737557
 0.99240122 0.98039216 1.         0.7826087  0.97195192 0.98643761
 0.97105509 1.         0.96746575 0.87850467 0.97674419]

Kappa:
0.9708272655068441
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb788575a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.669, val_acc:0.414]
Epoch [2/120    avg_loss:2.302, val_acc:0.480]
Epoch [3/120    avg_loss:1.998, val_acc:0.544]
Epoch [4/120    avg_loss:1.813, val_acc:0.569]
Epoch [5/120    avg_loss:1.630, val_acc:0.569]
Epoch [6/120    avg_loss:1.520, val_acc:0.588]
Epoch [7/120    avg_loss:1.383, val_acc:0.647]
Epoch [8/120    avg_loss:1.223, val_acc:0.633]
Epoch [9/120    avg_loss:1.093, val_acc:0.697]
Epoch [10/120    avg_loss:1.148, val_acc:0.689]
Epoch [11/120    avg_loss:0.952, val_acc:0.734]
Epoch [12/120    avg_loss:0.815, val_acc:0.727]
Epoch [13/120    avg_loss:0.729, val_acc:0.753]
Epoch [14/120    avg_loss:0.651, val_acc:0.814]
Epoch [15/120    avg_loss:0.587, val_acc:0.801]
Epoch [16/120    avg_loss:0.544, val_acc:0.785]
Epoch [17/120    avg_loss:0.445, val_acc:0.806]
Epoch [18/120    avg_loss:0.481, val_acc:0.822]
Epoch [19/120    avg_loss:0.374, val_acc:0.839]
Epoch [20/120    avg_loss:0.348, val_acc:0.857]
Epoch [21/120    avg_loss:0.337, val_acc:0.894]
Epoch [22/120    avg_loss:0.339, val_acc:0.852]
Epoch [23/120    avg_loss:0.278, val_acc:0.897]
Epoch [24/120    avg_loss:0.242, val_acc:0.870]
Epoch [25/120    avg_loss:0.225, val_acc:0.881]
Epoch [26/120    avg_loss:0.220, val_acc:0.887]
Epoch [27/120    avg_loss:0.189, val_acc:0.926]
Epoch [28/120    avg_loss:0.221, val_acc:0.900]
Epoch [29/120    avg_loss:0.197, val_acc:0.912]
Epoch [30/120    avg_loss:0.201, val_acc:0.921]
Epoch [31/120    avg_loss:0.137, val_acc:0.931]
Epoch [32/120    avg_loss:0.165, val_acc:0.909]
Epoch [33/120    avg_loss:0.128, val_acc:0.926]
Epoch [34/120    avg_loss:0.149, val_acc:0.932]
Epoch [35/120    avg_loss:0.132, val_acc:0.940]
Epoch [36/120    avg_loss:0.097, val_acc:0.930]
Epoch [37/120    avg_loss:0.132, val_acc:0.944]
Epoch [38/120    avg_loss:0.106, val_acc:0.919]
Epoch [39/120    avg_loss:0.094, val_acc:0.949]
Epoch [40/120    avg_loss:0.086, val_acc:0.954]
Epoch [41/120    avg_loss:0.134, val_acc:0.913]
Epoch [42/120    avg_loss:0.098, val_acc:0.950]
Epoch [43/120    avg_loss:0.097, val_acc:0.944]
Epoch [44/120    avg_loss:0.106, val_acc:0.934]
Epoch [45/120    avg_loss:0.079, val_acc:0.954]
Epoch [46/120    avg_loss:0.066, val_acc:0.939]
Epoch [47/120    avg_loss:0.084, val_acc:0.945]
Epoch [48/120    avg_loss:0.079, val_acc:0.963]
Epoch [49/120    avg_loss:0.050, val_acc:0.958]
Epoch [50/120    avg_loss:0.051, val_acc:0.938]
Epoch [51/120    avg_loss:0.077, val_acc:0.942]
Epoch [52/120    avg_loss:0.065, val_acc:0.945]
Epoch [53/120    avg_loss:0.078, val_acc:0.957]
Epoch [54/120    avg_loss:0.051, val_acc:0.960]
Epoch [55/120    avg_loss:0.049, val_acc:0.975]
Epoch [56/120    avg_loss:0.034, val_acc:0.961]
Epoch [57/120    avg_loss:0.034, val_acc:0.971]
Epoch [58/120    avg_loss:0.038, val_acc:0.976]
Epoch [59/120    avg_loss:0.033, val_acc:0.962]
Epoch [60/120    avg_loss:0.043, val_acc:0.949]
Epoch [61/120    avg_loss:0.043, val_acc:0.967]
Epoch [62/120    avg_loss:0.041, val_acc:0.961]
Epoch [63/120    avg_loss:0.026, val_acc:0.968]
Epoch [64/120    avg_loss:0.026, val_acc:0.959]
Epoch [65/120    avg_loss:0.031, val_acc:0.976]
Epoch [66/120    avg_loss:0.018, val_acc:0.976]
Epoch [67/120    avg_loss:0.032, val_acc:0.970]
Epoch [68/120    avg_loss:0.066, val_acc:0.952]
Epoch [69/120    avg_loss:0.060, val_acc:0.957]
Epoch [70/120    avg_loss:0.133, val_acc:0.867]
Epoch [71/120    avg_loss:0.059, val_acc:0.941]
Epoch [72/120    avg_loss:0.042, val_acc:0.944]
Epoch [73/120    avg_loss:0.048, val_acc:0.964]
Epoch [74/120    avg_loss:0.027, val_acc:0.970]
Epoch [75/120    avg_loss:0.026, val_acc:0.958]
Epoch [76/120    avg_loss:0.020, val_acc:0.971]
Epoch [77/120    avg_loss:0.016, val_acc:0.980]
Epoch [78/120    avg_loss:0.019, val_acc:0.964]
Epoch [79/120    avg_loss:0.019, val_acc:0.981]
Epoch [80/120    avg_loss:0.023, val_acc:0.959]
Epoch [81/120    avg_loss:0.015, val_acc:0.979]
Epoch [82/120    avg_loss:0.013, val_acc:0.976]
Epoch [83/120    avg_loss:0.028, val_acc:0.975]
Epoch [84/120    avg_loss:0.030, val_acc:0.963]
Epoch [85/120    avg_loss:0.023, val_acc:0.978]
Epoch [86/120    avg_loss:0.015, val_acc:0.977]
Epoch [87/120    avg_loss:0.024, val_acc:0.943]
Epoch [88/120    avg_loss:0.037, val_acc:0.966]
Epoch [89/120    avg_loss:0.064, val_acc:0.952]
Epoch [90/120    avg_loss:0.030, val_acc:0.968]
Epoch [91/120    avg_loss:0.017, val_acc:0.967]
Epoch [92/120    avg_loss:0.016, val_acc:0.966]
Epoch [93/120    avg_loss:0.019, val_acc:0.968]
Epoch [94/120    avg_loss:0.015, val_acc:0.971]
Epoch [95/120    avg_loss:0.011, val_acc:0.971]
Epoch [96/120    avg_loss:0.012, val_acc:0.972]
Epoch [97/120    avg_loss:0.010, val_acc:0.972]
Epoch [98/120    avg_loss:0.011, val_acc:0.974]
Epoch [99/120    avg_loss:0.013, val_acc:0.975]
Epoch [100/120    avg_loss:0.008, val_acc:0.976]
Epoch [101/120    avg_loss:0.011, val_acc:0.977]
Epoch [102/120    avg_loss:0.010, val_acc:0.976]
Epoch [103/120    avg_loss:0.009, val_acc:0.977]
Epoch [104/120    avg_loss:0.009, val_acc:0.977]
Epoch [105/120    avg_loss:0.010, val_acc:0.977]
Epoch [106/120    avg_loss:0.008, val_acc:0.977]
Epoch [107/120    avg_loss:0.010, val_acc:0.976]
Epoch [108/120    avg_loss:0.008, val_acc:0.976]
Epoch [109/120    avg_loss:0.009, val_acc:0.976]
Epoch [110/120    avg_loss:0.011, val_acc:0.976]
Epoch [111/120    avg_loss:0.008, val_acc:0.977]
Epoch [112/120    avg_loss:0.016, val_acc:0.976]
Epoch [113/120    avg_loss:0.011, val_acc:0.976]
Epoch [114/120    avg_loss:0.008, val_acc:0.976]
Epoch [115/120    avg_loss:0.009, val_acc:0.976]
Epoch [116/120    avg_loss:0.008, val_acc:0.976]
Epoch [117/120    avg_loss:0.007, val_acc:0.976]
Epoch [118/120    avg_loss:0.009, val_acc:0.976]
Epoch [119/120    avg_loss:0.010, val_acc:0.976]
Epoch [120/120    avg_loss:0.009, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1241    8    6    0    0    0    0    0    2   17    3    0
     0    8    0]
 [   0    0    3  708   19    3    0    0    0    1    0    7    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   13    0    0    5    0    0    0    0  852    3    0    0
     0    2    0]
 [   0    0    5    0    0    0    0    0    0    0   11 2189    4    0
     0    0    1]
 [   0    0    2    3    0   10    0    0    0    0    0    5  511    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   116  231    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.95392953929539

F1 scores:
[       nan 1.         0.97371518 0.96589359 0.94456763 0.97742664
 0.99618029 1.         1.         0.97297297 0.97931034 0.98803882
 0.96597353 1.         0.945257   0.77386935 0.98245614]

Kappa:
0.965239938724082
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f50a3d559b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.643, val_acc:0.487]
Epoch [2/120    avg_loss:2.254, val_acc:0.508]
Epoch [3/120    avg_loss:1.961, val_acc:0.587]
Epoch [4/120    avg_loss:1.785, val_acc:0.583]
Epoch [5/120    avg_loss:1.618, val_acc:0.616]
Epoch [6/120    avg_loss:1.479, val_acc:0.648]
Epoch [7/120    avg_loss:1.365, val_acc:0.678]
Epoch [8/120    avg_loss:1.213, val_acc:0.699]
Epoch [9/120    avg_loss:1.080, val_acc:0.722]
Epoch [10/120    avg_loss:1.036, val_acc:0.749]
Epoch [11/120    avg_loss:0.870, val_acc:0.766]
Epoch [12/120    avg_loss:0.789, val_acc:0.777]
Epoch [13/120    avg_loss:0.698, val_acc:0.771]
Epoch [14/120    avg_loss:0.634, val_acc:0.813]
Epoch [15/120    avg_loss:0.574, val_acc:0.829]
Epoch [16/120    avg_loss:0.540, val_acc:0.828]
Epoch [17/120    avg_loss:0.466, val_acc:0.816]
Epoch [18/120    avg_loss:0.576, val_acc:0.811]
Epoch [19/120    avg_loss:0.455, val_acc:0.820]
Epoch [20/120    avg_loss:0.444, val_acc:0.853]
Epoch [21/120    avg_loss:0.385, val_acc:0.843]
Epoch [22/120    avg_loss:0.323, val_acc:0.868]
Epoch [23/120    avg_loss:0.301, val_acc:0.888]
Epoch [24/120    avg_loss:0.272, val_acc:0.891]
Epoch [25/120    avg_loss:0.257, val_acc:0.901]
Epoch [26/120    avg_loss:0.243, val_acc:0.880]
Epoch [27/120    avg_loss:0.242, val_acc:0.908]
Epoch [28/120    avg_loss:0.213, val_acc:0.910]
Epoch [29/120    avg_loss:0.196, val_acc:0.918]
Epoch [30/120    avg_loss:0.236, val_acc:0.910]
Epoch [31/120    avg_loss:0.231, val_acc:0.923]
Epoch [32/120    avg_loss:0.214, val_acc:0.882]
Epoch [33/120    avg_loss:0.152, val_acc:0.930]
Epoch [34/120    avg_loss:0.145, val_acc:0.890]
Epoch [35/120    avg_loss:0.172, val_acc:0.893]
Epoch [36/120    avg_loss:0.607, val_acc:0.860]
Epoch [37/120    avg_loss:0.221, val_acc:0.930]
Epoch [38/120    avg_loss:0.145, val_acc:0.905]
Epoch [39/120    avg_loss:0.136, val_acc:0.905]
Epoch [40/120    avg_loss:0.109, val_acc:0.929]
Epoch [41/120    avg_loss:0.118, val_acc:0.938]
Epoch [42/120    avg_loss:0.118, val_acc:0.930]
Epoch [43/120    avg_loss:0.124, val_acc:0.915]
Epoch [44/120    avg_loss:0.159, val_acc:0.930]
Epoch [45/120    avg_loss:0.127, val_acc:0.936]
Epoch [46/120    avg_loss:0.095, val_acc:0.930]
Epoch [47/120    avg_loss:0.099, val_acc:0.943]
Epoch [48/120    avg_loss:0.083, val_acc:0.937]
Epoch [49/120    avg_loss:0.125, val_acc:0.945]
Epoch [50/120    avg_loss:0.089, val_acc:0.941]
Epoch [51/120    avg_loss:0.098, val_acc:0.937]
Epoch [52/120    avg_loss:0.089, val_acc:0.934]
Epoch [53/120    avg_loss:0.076, val_acc:0.947]
Epoch [54/120    avg_loss:0.081, val_acc:0.958]
Epoch [55/120    avg_loss:0.061, val_acc:0.945]
Epoch [56/120    avg_loss:0.055, val_acc:0.955]
Epoch [57/120    avg_loss:0.055, val_acc:0.958]
Epoch [58/120    avg_loss:0.065, val_acc:0.951]
Epoch [59/120    avg_loss:0.049, val_acc:0.964]
Epoch [60/120    avg_loss:0.052, val_acc:0.963]
Epoch [61/120    avg_loss:0.049, val_acc:0.970]
Epoch [62/120    avg_loss:0.081, val_acc:0.946]
Epoch [63/120    avg_loss:0.083, val_acc:0.937]
Epoch [64/120    avg_loss:0.073, val_acc:0.955]
Epoch [65/120    avg_loss:0.059, val_acc:0.968]
Epoch [66/120    avg_loss:0.043, val_acc:0.967]
Epoch [67/120    avg_loss:0.057, val_acc:0.962]
Epoch [68/120    avg_loss:0.038, val_acc:0.973]
Epoch [69/120    avg_loss:0.032, val_acc:0.964]
Epoch [70/120    avg_loss:0.049, val_acc:0.970]
Epoch [71/120    avg_loss:0.029, val_acc:0.973]
Epoch [72/120    avg_loss:0.025, val_acc:0.970]
Epoch [73/120    avg_loss:0.023, val_acc:0.970]
Epoch [74/120    avg_loss:0.028, val_acc:0.962]
Epoch [75/120    avg_loss:0.043, val_acc:0.966]
Epoch [76/120    avg_loss:0.039, val_acc:0.960]
Epoch [77/120    avg_loss:0.033, val_acc:0.965]
Epoch [78/120    avg_loss:0.063, val_acc:0.971]
Epoch [79/120    avg_loss:0.070, val_acc:0.948]
Epoch [80/120    avg_loss:0.086, val_acc:0.946]
Epoch [81/120    avg_loss:0.082, val_acc:0.946]
Epoch [82/120    avg_loss:0.035, val_acc:0.959]
Epoch [83/120    avg_loss:0.042, val_acc:0.973]
Epoch [84/120    avg_loss:0.036, val_acc:0.963]
Epoch [85/120    avg_loss:0.068, val_acc:0.965]
Epoch [86/120    avg_loss:0.026, val_acc:0.966]
Epoch [87/120    avg_loss:0.027, val_acc:0.966]
Epoch [88/120    avg_loss:0.046, val_acc:0.958]
Epoch [89/120    avg_loss:0.040, val_acc:0.973]
Epoch [90/120    avg_loss:0.027, val_acc:0.948]
Epoch [91/120    avg_loss:0.031, val_acc:0.957]
Epoch [92/120    avg_loss:0.045, val_acc:0.942]
Epoch [93/120    avg_loss:0.026, val_acc:0.966]
Epoch [94/120    avg_loss:0.017, val_acc:0.971]
Epoch [95/120    avg_loss:0.027, val_acc:0.967]
Epoch [96/120    avg_loss:0.083, val_acc:0.887]
Epoch [97/120    avg_loss:0.080, val_acc:0.958]
Epoch [98/120    avg_loss:0.034, val_acc:0.974]
Epoch [99/120    avg_loss:0.047, val_acc:0.909]
Epoch [100/120    avg_loss:0.053, val_acc:0.965]
Epoch [101/120    avg_loss:0.031, val_acc:0.964]
Epoch [102/120    avg_loss:0.021, val_acc:0.978]
Epoch [103/120    avg_loss:0.019, val_acc:0.977]
Epoch [104/120    avg_loss:0.021, val_acc:0.971]
Epoch [105/120    avg_loss:0.018, val_acc:0.975]
Epoch [106/120    avg_loss:0.012, val_acc:0.971]
Epoch [107/120    avg_loss:0.015, val_acc:0.975]
Epoch [108/120    avg_loss:0.013, val_acc:0.967]
Epoch [109/120    avg_loss:0.012, val_acc:0.977]
Epoch [110/120    avg_loss:0.009, val_acc:0.974]
Epoch [111/120    avg_loss:0.009, val_acc:0.966]
Epoch [112/120    avg_loss:0.012, val_acc:0.966]
Epoch [113/120    avg_loss:0.017, val_acc:0.972]
Epoch [114/120    avg_loss:0.011, val_acc:0.975]
Epoch [115/120    avg_loss:0.014, val_acc:0.967]
Epoch [116/120    avg_loss:0.013, val_acc:0.975]
Epoch [117/120    avg_loss:0.011, val_acc:0.975]
Epoch [118/120    avg_loss:0.008, val_acc:0.974]
Epoch [119/120    avg_loss:0.011, val_acc:0.974]
Epoch [120/120    avg_loss:0.008, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1273    0    5    0    0    0    0    0    2    3    2    0
     0    0    0]
 [   0    0    2  735    4    1    0    0    0    0    1    2    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    1    0    0    0    0
     4    1    0]
 [   0    0    0    0    0    0  653    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   12    0    0    5    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  868    0    2    0
     0    0    0]
 [   0    0   78    2    0    0    2    0    0    0   30 2098    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0    1  529    0
     1    2    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1134    5    0]
 [   0    0    0    0    0    0   14    0    0    1    0    0    0    0
    58  274    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
97.25745257452574

F1 scores:
[       nan 0.96202532 0.96329928 0.98923284 0.97931034 0.99074074
 0.98491704 0.98039216 0.99883586 0.75       0.97582912 0.97242178
 0.9787234  1.         0.96964515 0.87122417 0.96296296]

Kappa:
0.9687571194793051
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:16:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff2cdb11a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.681, val_acc:0.306]
Epoch [2/120    avg_loss:2.352, val_acc:0.424]
Epoch [3/120    avg_loss:2.127, val_acc:0.513]
Epoch [4/120    avg_loss:1.901, val_acc:0.562]
Epoch [5/120    avg_loss:1.686, val_acc:0.573]
Epoch [6/120    avg_loss:1.539, val_acc:0.594]
Epoch [7/120    avg_loss:1.391, val_acc:0.655]
Epoch [8/120    avg_loss:1.148, val_acc:0.657]
Epoch [9/120    avg_loss:1.020, val_acc:0.718]
Epoch [10/120    avg_loss:0.934, val_acc:0.721]
Epoch [11/120    avg_loss:0.845, val_acc:0.724]
Epoch [12/120    avg_loss:0.729, val_acc:0.757]
Epoch [13/120    avg_loss:0.639, val_acc:0.749]
Epoch [14/120    avg_loss:0.536, val_acc:0.837]
Epoch [15/120    avg_loss:0.446, val_acc:0.858]
Epoch [16/120    avg_loss:0.464, val_acc:0.844]
Epoch [17/120    avg_loss:0.449, val_acc:0.743]
Epoch [18/120    avg_loss:0.602, val_acc:0.774]
Epoch [19/120    avg_loss:0.383, val_acc:0.844]
Epoch [20/120    avg_loss:0.375, val_acc:0.798]
Epoch [21/120    avg_loss:0.408, val_acc:0.808]
Epoch [22/120    avg_loss:0.287, val_acc:0.889]
Epoch [23/120    avg_loss:0.245, val_acc:0.846]
Epoch [24/120    avg_loss:0.232, val_acc:0.871]
Epoch [25/120    avg_loss:0.248, val_acc:0.905]
Epoch [26/120    avg_loss:0.214, val_acc:0.894]
Epoch [27/120    avg_loss:0.235, val_acc:0.906]
Epoch [28/120    avg_loss:0.190, val_acc:0.893]
Epoch [29/120    avg_loss:0.204, val_acc:0.888]
Epoch [30/120    avg_loss:0.153, val_acc:0.918]
Epoch [31/120    avg_loss:0.151, val_acc:0.860]
Epoch [32/120    avg_loss:0.136, val_acc:0.878]
Epoch [33/120    avg_loss:0.183, val_acc:0.915]
Epoch [34/120    avg_loss:0.148, val_acc:0.875]
Epoch [35/120    avg_loss:0.120, val_acc:0.916]
Epoch [36/120    avg_loss:0.108, val_acc:0.927]
Epoch [37/120    avg_loss:0.103, val_acc:0.920]
Epoch [38/120    avg_loss:0.120, val_acc:0.913]
Epoch [39/120    avg_loss:0.124, val_acc:0.906]
Epoch [40/120    avg_loss:0.124, val_acc:0.934]
Epoch [41/120    avg_loss:0.107, val_acc:0.929]
Epoch [42/120    avg_loss:0.078, val_acc:0.946]
Epoch [43/120    avg_loss:0.094, val_acc:0.944]
Epoch [44/120    avg_loss:0.066, val_acc:0.937]
Epoch [45/120    avg_loss:0.064, val_acc:0.941]
Epoch [46/120    avg_loss:0.084, val_acc:0.946]
Epoch [47/120    avg_loss:0.085, val_acc:0.929]
Epoch [48/120    avg_loss:0.071, val_acc:0.950]
Epoch [49/120    avg_loss:0.069, val_acc:0.932]
Epoch [50/120    avg_loss:0.070, val_acc:0.959]
Epoch [51/120    avg_loss:0.068, val_acc:0.937]
Epoch [52/120    avg_loss:0.044, val_acc:0.970]
Epoch [53/120    avg_loss:0.046, val_acc:0.957]
Epoch [54/120    avg_loss:0.032, val_acc:0.963]
Epoch [55/120    avg_loss:0.061, val_acc:0.959]
Epoch [56/120    avg_loss:0.059, val_acc:0.953]
Epoch [57/120    avg_loss:0.046, val_acc:0.957]
Epoch [58/120    avg_loss:0.033, val_acc:0.965]
Epoch [59/120    avg_loss:0.046, val_acc:0.936]
Epoch [60/120    avg_loss:0.052, val_acc:0.970]
Epoch [61/120    avg_loss:0.036, val_acc:0.970]
Epoch [62/120    avg_loss:0.041, val_acc:0.951]
Epoch [63/120    avg_loss:0.051, val_acc:0.964]
Epoch [64/120    avg_loss:0.038, val_acc:0.970]
Epoch [65/120    avg_loss:0.029, val_acc:0.967]
Epoch [66/120    avg_loss:0.034, val_acc:0.974]
Epoch [67/120    avg_loss:0.034, val_acc:0.959]
Epoch [68/120    avg_loss:0.021, val_acc:0.965]
Epoch [69/120    avg_loss:0.044, val_acc:0.958]
Epoch [70/120    avg_loss:0.024, val_acc:0.971]
Epoch [71/120    avg_loss:0.038, val_acc:0.957]
Epoch [72/120    avg_loss:0.027, val_acc:0.943]
Epoch [73/120    avg_loss:0.026, val_acc:0.967]
Epoch [74/120    avg_loss:0.060, val_acc:0.970]
Epoch [75/120    avg_loss:0.033, val_acc:0.964]
Epoch [76/120    avg_loss:0.022, val_acc:0.970]
Epoch [77/120    avg_loss:0.030, val_acc:0.968]
Epoch [78/120    avg_loss:0.046, val_acc:0.955]
Epoch [79/120    avg_loss:0.045, val_acc:0.921]
Epoch [80/120    avg_loss:0.064, val_acc:0.959]
Epoch [81/120    avg_loss:0.029, val_acc:0.964]
Epoch [82/120    avg_loss:0.023, val_acc:0.971]
Epoch [83/120    avg_loss:0.018, val_acc:0.972]
Epoch [84/120    avg_loss:0.015, val_acc:0.972]
Epoch [85/120    avg_loss:0.016, val_acc:0.975]
Epoch [86/120    avg_loss:0.021, val_acc:0.976]
Epoch [87/120    avg_loss:0.022, val_acc:0.976]
Epoch [88/120    avg_loss:0.019, val_acc:0.976]
Epoch [89/120    avg_loss:0.014, val_acc:0.975]
Epoch [90/120    avg_loss:0.013, val_acc:0.976]
Epoch [91/120    avg_loss:0.017, val_acc:0.979]
Epoch [92/120    avg_loss:0.013, val_acc:0.976]
Epoch [93/120    avg_loss:0.011, val_acc:0.975]
Epoch [94/120    avg_loss:0.012, val_acc:0.975]
Epoch [95/120    avg_loss:0.012, val_acc:0.976]
Epoch [96/120    avg_loss:0.016, val_acc:0.975]
Epoch [97/120    avg_loss:0.012, val_acc:0.976]
Epoch [98/120    avg_loss:0.013, val_acc:0.980]
Epoch [99/120    avg_loss:0.011, val_acc:0.978]
Epoch [100/120    avg_loss:0.008, val_acc:0.976]
Epoch [101/120    avg_loss:0.013, val_acc:0.978]
Epoch [102/120    avg_loss:0.014, val_acc:0.976]
Epoch [103/120    avg_loss:0.013, val_acc:0.974]
Epoch [104/120    avg_loss:0.013, val_acc:0.975]
Epoch [105/120    avg_loss:0.009, val_acc:0.976]
Epoch [106/120    avg_loss:0.015, val_acc:0.976]
Epoch [107/120    avg_loss:0.012, val_acc:0.973]
Epoch [108/120    avg_loss:0.011, val_acc:0.976]
Epoch [109/120    avg_loss:0.014, val_acc:0.976]
Epoch [110/120    avg_loss:0.013, val_acc:0.974]
Epoch [111/120    avg_loss:0.011, val_acc:0.976]
Epoch [112/120    avg_loss:0.011, val_acc:0.979]
Epoch [113/120    avg_loss:0.009, val_acc:0.980]
Epoch [114/120    avg_loss:0.014, val_acc:0.979]
Epoch [115/120    avg_loss:0.014, val_acc:0.979]
Epoch [116/120    avg_loss:0.012, val_acc:0.979]
Epoch [117/120    avg_loss:0.011, val_acc:0.979]
Epoch [118/120    avg_loss:0.013, val_acc:0.979]
Epoch [119/120    avg_loss:0.010, val_acc:0.979]
Epoch [120/120    avg_loss:0.011, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1262    3    6    0    0    0    0    0    1   11    2    0
     0    0    0]
 [   0    0    0  724    9    0    0    0    0    0    2    0   12    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    2    0    0    0    0  863    6    0    0
     1    0    0]
 [   0    0    5    0    0    0    2    0    0    0    3 2192    7    1
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    1  528    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    1    0    0    0
  1131    2    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    63  275    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.1680216802168

F1 scores:
[       nan 0.975      0.98748044 0.98236092 0.96598639 0.99088838
 0.98864497 1.         1.         1.         0.98854525 0.9914066
 0.97416974 0.99730458 0.96832192 0.87859425 0.98224852]

Kappa:
0.9791052569539745
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd614104b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.643, val_acc:0.417]
Epoch [2/120    avg_loss:2.311, val_acc:0.527]
Epoch [3/120    avg_loss:2.095, val_acc:0.541]
Epoch [4/120    avg_loss:1.878, val_acc:0.569]
Epoch [5/120    avg_loss:1.729, val_acc:0.620]
Epoch [6/120    avg_loss:1.579, val_acc:0.645]
Epoch [7/120    avg_loss:1.462, val_acc:0.705]
Epoch [8/120    avg_loss:1.292, val_acc:0.703]
Epoch [9/120    avg_loss:1.177, val_acc:0.685]
Epoch [10/120    avg_loss:0.987, val_acc:0.725]
Epoch [11/120    avg_loss:0.935, val_acc:0.741]
Epoch [12/120    avg_loss:0.795, val_acc:0.716]
Epoch [13/120    avg_loss:0.850, val_acc:0.711]
Epoch [14/120    avg_loss:0.770, val_acc:0.762]
Epoch [15/120    avg_loss:0.713, val_acc:0.771]
Epoch [16/120    avg_loss:0.638, val_acc:0.797]
Epoch [17/120    avg_loss:0.558, val_acc:0.819]
Epoch [18/120    avg_loss:0.568, val_acc:0.845]
Epoch [19/120    avg_loss:0.500, val_acc:0.843]
Epoch [20/120    avg_loss:0.421, val_acc:0.825]
Epoch [21/120    avg_loss:0.410, val_acc:0.854]
Epoch [22/120    avg_loss:0.364, val_acc:0.862]
Epoch [23/120    avg_loss:0.397, val_acc:0.856]
Epoch [24/120    avg_loss:0.359, val_acc:0.887]
Epoch [25/120    avg_loss:0.339, val_acc:0.862]
Epoch [26/120    avg_loss:0.341, val_acc:0.894]
Epoch [27/120    avg_loss:0.264, val_acc:0.906]
Epoch [28/120    avg_loss:0.268, val_acc:0.909]
Epoch [29/120    avg_loss:0.228, val_acc:0.908]
Epoch [30/120    avg_loss:0.237, val_acc:0.906]
Epoch [31/120    avg_loss:0.268, val_acc:0.904]
Epoch [32/120    avg_loss:0.209, val_acc:0.913]
Epoch [33/120    avg_loss:0.194, val_acc:0.911]
Epoch [34/120    avg_loss:0.184, val_acc:0.908]
Epoch [35/120    avg_loss:0.168, val_acc:0.934]
Epoch [36/120    avg_loss:0.215, val_acc:0.917]
Epoch [37/120    avg_loss:0.150, val_acc:0.889]
Epoch [38/120    avg_loss:0.134, val_acc:0.935]
Epoch [39/120    avg_loss:0.120, val_acc:0.930]
Epoch [40/120    avg_loss:0.119, val_acc:0.941]
Epoch [41/120    avg_loss:0.143, val_acc:0.900]
Epoch [42/120    avg_loss:0.144, val_acc:0.904]
Epoch [43/120    avg_loss:0.229, val_acc:0.898]
Epoch [44/120    avg_loss:0.215, val_acc:0.922]
Epoch [45/120    avg_loss:0.132, val_acc:0.951]
Epoch [46/120    avg_loss:0.091, val_acc:0.944]
Epoch [47/120    avg_loss:0.153, val_acc:0.925]
Epoch [48/120    avg_loss:0.111, val_acc:0.946]
Epoch [49/120    avg_loss:0.130, val_acc:0.916]
Epoch [50/120    avg_loss:0.111, val_acc:0.934]
Epoch [51/120    avg_loss:0.080, val_acc:0.939]
Epoch [52/120    avg_loss:0.082, val_acc:0.942]
Epoch [53/120    avg_loss:0.073, val_acc:0.955]
Epoch [54/120    avg_loss:0.067, val_acc:0.948]
Epoch [55/120    avg_loss:0.070, val_acc:0.945]
Epoch [56/120    avg_loss:0.061, val_acc:0.962]
Epoch [57/120    avg_loss:0.055, val_acc:0.956]
Epoch [58/120    avg_loss:0.048, val_acc:0.945]
Epoch [59/120    avg_loss:0.061, val_acc:0.954]
Epoch [60/120    avg_loss:0.048, val_acc:0.947]
Epoch [61/120    avg_loss:0.034, val_acc:0.954]
Epoch [62/120    avg_loss:0.055, val_acc:0.939]
Epoch [63/120    avg_loss:0.071, val_acc:0.928]
Epoch [64/120    avg_loss:0.058, val_acc:0.955]
Epoch [65/120    avg_loss:0.065, val_acc:0.944]
Epoch [66/120    avg_loss:0.073, val_acc:0.931]
Epoch [67/120    avg_loss:0.146, val_acc:0.854]
Epoch [68/120    avg_loss:1.120, val_acc:0.234]
Epoch [69/120    avg_loss:2.330, val_acc:0.390]
Epoch [70/120    avg_loss:2.084, val_acc:0.417]
Epoch [71/120    avg_loss:2.019, val_acc:0.422]
Epoch [72/120    avg_loss:1.995, val_acc:0.433]
Epoch [73/120    avg_loss:1.994, val_acc:0.447]
Epoch [74/120    avg_loss:1.958, val_acc:0.451]
Epoch [75/120    avg_loss:1.937, val_acc:0.443]
Epoch [76/120    avg_loss:1.895, val_acc:0.471]
Epoch [77/120    avg_loss:1.870, val_acc:0.479]
Epoch [78/120    avg_loss:1.861, val_acc:0.484]
Epoch [79/120    avg_loss:1.830, val_acc:0.480]
Epoch [80/120    avg_loss:1.779, val_acc:0.484]
Epoch [81/120    avg_loss:1.819, val_acc:0.504]
Epoch [82/120    avg_loss:1.799, val_acc:0.495]
Epoch [83/120    avg_loss:1.756, val_acc:0.500]
Epoch [84/120    avg_loss:1.786, val_acc:0.507]
Epoch [85/120    avg_loss:1.757, val_acc:0.508]
Epoch [86/120    avg_loss:1.762, val_acc:0.508]
Epoch [87/120    avg_loss:1.768, val_acc:0.511]
Epoch [88/120    avg_loss:1.762, val_acc:0.514]
Epoch [89/120    avg_loss:1.753, val_acc:0.515]
Epoch [90/120    avg_loss:1.733, val_acc:0.518]
Epoch [91/120    avg_loss:1.756, val_acc:0.514]
Epoch [92/120    avg_loss:1.732, val_acc:0.516]
Epoch [93/120    avg_loss:1.751, val_acc:0.518]
Epoch [94/120    avg_loss:1.779, val_acc:0.518]
Epoch [95/120    avg_loss:1.728, val_acc:0.520]
Epoch [96/120    avg_loss:1.731, val_acc:0.520]
Epoch [97/120    avg_loss:1.750, val_acc:0.520]
Epoch [98/120    avg_loss:1.745, val_acc:0.520]
Epoch [99/120    avg_loss:1.753, val_acc:0.521]
Epoch [100/120    avg_loss:1.768, val_acc:0.522]
Epoch [101/120    avg_loss:1.735, val_acc:0.521]
Epoch [102/120    avg_loss:1.740, val_acc:0.520]
Epoch [103/120    avg_loss:1.755, val_acc:0.520]
Epoch [104/120    avg_loss:1.724, val_acc:0.521]
Epoch [105/120    avg_loss:1.728, val_acc:0.521]
Epoch [106/120    avg_loss:1.771, val_acc:0.520]
Epoch [107/120    avg_loss:1.718, val_acc:0.521]
Epoch [108/120    avg_loss:1.790, val_acc:0.521]
Epoch [109/120    avg_loss:1.717, val_acc:0.520]
Epoch [110/120    avg_loss:1.743, val_acc:0.520]
Epoch [111/120    avg_loss:1.759, val_acc:0.521]
Epoch [112/120    avg_loss:1.756, val_acc:0.520]
Epoch [113/120    avg_loss:1.744, val_acc:0.521]
Epoch [114/120    avg_loss:1.733, val_acc:0.521]
Epoch [115/120    avg_loss:1.726, val_acc:0.520]
Epoch [116/120    avg_loss:1.736, val_acc:0.521]
Epoch [117/120    avg_loss:1.748, val_acc:0.521]
Epoch [118/120    avg_loss:1.755, val_acc:0.521]
Epoch [119/120    avg_loss:1.748, val_acc:0.521]
Epoch [120/120    avg_loss:1.762, val_acc:0.521]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0   15    0    0   11    0    0   10    0    5    0    0    0
     0    0    0]
 [   0    0  625   25   46   13   45    0    0    0  189  306   16   16
     4    0    0]
 [   0    0  164   68    1   54   46    0    0    0  185  180    4   43
     2    0    0]
 [   0    0  113   16   41    3    0    0    0    0    0   21    0   19
     0    0    0]
 [   0    0    6   11    0  109   13    0   13    0    0   16    5   24
   238    0    0]
 [   0    0    0   12    3    0  445    0    0    0    0  162    0    6
    29    0    0]
 [   0    0    0    0    0    9    0    0   14    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    2    4    0  424    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    2   15    0    0    0    0    0    0    0
     0    0    0]
 [   0    0   86    1    5    3   47    0    0    0  514  132   44   27
    16    0    0]
 [   0    0  324   36   12   63   84    0   23    0  141 1222  227   44
    32    0    2]
 [   0    0  134    9   17   19   10    0    0    0   21   70  182    4
     0    0   68]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   55   76    0   36    0    1   13   13   26
   919    0    0]
 [   0    0    7    3    0    0  160    0    0    0    0  101   45   31
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
52.22764227642276

F1 scores:
[       nan 0.         0.4530627  0.14639397 0.24260355 0.28020566
 0.55555556 0.         0.89263158 0.         0.53236665 0.55131965
 0.34018692 0.60655738 0.77194456 0.         0.70588235]

Kappa:
0.4537219740016239
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5c4850fb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.641, val_acc:0.515]
Epoch [2/120    avg_loss:2.280, val_acc:0.549]
Epoch [3/120    avg_loss:2.031, val_acc:0.587]
Epoch [4/120    avg_loss:1.867, val_acc:0.586]
Epoch [5/120    avg_loss:1.684, val_acc:0.625]
Epoch [6/120    avg_loss:1.481, val_acc:0.686]
Epoch [7/120    avg_loss:1.329, val_acc:0.702]
Epoch [8/120    avg_loss:1.167, val_acc:0.694]
Epoch [9/120    avg_loss:0.993, val_acc:0.692]
Epoch [10/120    avg_loss:0.898, val_acc:0.781]
Epoch [11/120    avg_loss:0.868, val_acc:0.783]
Epoch [12/120    avg_loss:0.688, val_acc:0.735]
Epoch [13/120    avg_loss:0.687, val_acc:0.826]
Epoch [14/120    avg_loss:0.586, val_acc:0.817]
Epoch [15/120    avg_loss:0.624, val_acc:0.820]
Epoch [16/120    avg_loss:0.474, val_acc:0.824]
Epoch [17/120    avg_loss:0.539, val_acc:0.831]
Epoch [18/120    avg_loss:0.505, val_acc:0.848]
Epoch [19/120    avg_loss:0.487, val_acc:0.869]
Epoch [20/120    avg_loss:0.421, val_acc:0.845]
Epoch [21/120    avg_loss:0.370, val_acc:0.888]
Epoch [22/120    avg_loss:0.408, val_acc:0.879]
Epoch [23/120    avg_loss:0.367, val_acc:0.864]
Epoch [24/120    avg_loss:0.305, val_acc:0.859]
Epoch [25/120    avg_loss:0.297, val_acc:0.911]
Epoch [26/120    avg_loss:0.271, val_acc:0.898]
Epoch [27/120    avg_loss:0.264, val_acc:0.914]
Epoch [28/120    avg_loss:0.227, val_acc:0.891]
Epoch [29/120    avg_loss:0.194, val_acc:0.887]
Epoch [30/120    avg_loss:0.213, val_acc:0.900]
Epoch [31/120    avg_loss:0.191, val_acc:0.930]
Epoch [32/120    avg_loss:0.189, val_acc:0.901]
Epoch [33/120    avg_loss:0.128, val_acc:0.919]
Epoch [34/120    avg_loss:0.134, val_acc:0.916]
Epoch [35/120    avg_loss:0.130, val_acc:0.946]
Epoch [36/120    avg_loss:0.166, val_acc:0.915]
Epoch [37/120    avg_loss:0.261, val_acc:0.871]
Epoch [38/120    avg_loss:0.215, val_acc:0.912]
Epoch [39/120    avg_loss:0.153, val_acc:0.914]
Epoch [40/120    avg_loss:0.099, val_acc:0.960]
Epoch [41/120    avg_loss:0.133, val_acc:0.904]
Epoch [42/120    avg_loss:0.161, val_acc:0.920]
Epoch [43/120    avg_loss:0.111, val_acc:0.929]
Epoch [44/120    avg_loss:0.119, val_acc:0.948]
Epoch [45/120    avg_loss:0.080, val_acc:0.953]
Epoch [46/120    avg_loss:0.073, val_acc:0.944]
Epoch [47/120    avg_loss:0.058, val_acc:0.955]
Epoch [48/120    avg_loss:0.060, val_acc:0.962]
Epoch [49/120    avg_loss:0.052, val_acc:0.960]
Epoch [50/120    avg_loss:0.050, val_acc:0.960]
Epoch [51/120    avg_loss:0.083, val_acc:0.935]
Epoch [52/120    avg_loss:0.102, val_acc:0.935]
Epoch [53/120    avg_loss:0.103, val_acc:0.945]
Epoch [54/120    avg_loss:0.058, val_acc:0.951]
Epoch [55/120    avg_loss:0.050, val_acc:0.963]
Epoch [56/120    avg_loss:0.112, val_acc:0.948]
Epoch [57/120    avg_loss:0.127, val_acc:0.950]
Epoch [58/120    avg_loss:0.063, val_acc:0.955]
Epoch [59/120    avg_loss:0.044, val_acc:0.943]
Epoch [60/120    avg_loss:0.043, val_acc:0.961]
Epoch [61/120    avg_loss:0.032, val_acc:0.958]
Epoch [62/120    avg_loss:0.039, val_acc:0.959]
Epoch [63/120    avg_loss:0.043, val_acc:0.958]
Epoch [64/120    avg_loss:0.029, val_acc:0.971]
Epoch [65/120    avg_loss:0.057, val_acc:0.961]
Epoch [66/120    avg_loss:0.030, val_acc:0.956]
Epoch [67/120    avg_loss:0.032, val_acc:0.967]
Epoch [68/120    avg_loss:0.036, val_acc:0.967]
Epoch [69/120    avg_loss:0.068, val_acc:0.959]
Epoch [70/120    avg_loss:0.044, val_acc:0.964]
Epoch [71/120    avg_loss:0.167, val_acc:0.929]
Epoch [72/120    avg_loss:0.096, val_acc:0.963]
Epoch [73/120    avg_loss:0.061, val_acc:0.975]
Epoch [74/120    avg_loss:0.044, val_acc:0.972]
Epoch [75/120    avg_loss:0.031, val_acc:0.980]
Epoch [76/120    avg_loss:0.036, val_acc:0.975]
Epoch [77/120    avg_loss:0.028, val_acc:0.971]
Epoch [78/120    avg_loss:0.052, val_acc:0.955]
Epoch [79/120    avg_loss:0.033, val_acc:0.961]
Epoch [80/120    avg_loss:0.057, val_acc:0.979]
Epoch [81/120    avg_loss:0.030, val_acc:0.977]
Epoch [82/120    avg_loss:0.030, val_acc:0.961]
Epoch [83/120    avg_loss:0.028, val_acc:0.977]
Epoch [84/120    avg_loss:0.030, val_acc:0.973]
Epoch [85/120    avg_loss:0.020, val_acc:0.982]
Epoch [86/120    avg_loss:0.025, val_acc:0.977]
Epoch [87/120    avg_loss:0.013, val_acc:0.980]
Epoch [88/120    avg_loss:0.014, val_acc:0.979]
Epoch [89/120    avg_loss:0.024, val_acc:0.981]
Epoch [90/120    avg_loss:0.017, val_acc:0.982]
Epoch [91/120    avg_loss:0.030, val_acc:0.934]
Epoch [92/120    avg_loss:0.109, val_acc:0.941]
Epoch [93/120    avg_loss:0.093, val_acc:0.961]
Epoch [94/120    avg_loss:0.096, val_acc:0.945]
Epoch [95/120    avg_loss:0.057, val_acc:0.931]
Epoch [96/120    avg_loss:0.095, val_acc:0.953]
Epoch [97/120    avg_loss:0.043, val_acc:0.967]
Epoch [98/120    avg_loss:0.039, val_acc:0.970]
Epoch [99/120    avg_loss:0.025, val_acc:0.971]
Epoch [100/120    avg_loss:0.021, val_acc:0.972]
Epoch [101/120    avg_loss:0.029, val_acc:0.965]
Epoch [102/120    avg_loss:0.029, val_acc:0.978]
Epoch [103/120    avg_loss:0.043, val_acc:0.968]
Epoch [104/120    avg_loss:0.048, val_acc:0.979]
Epoch [105/120    avg_loss:0.022, val_acc:0.981]
Epoch [106/120    avg_loss:0.027, val_acc:0.981]
Epoch [107/120    avg_loss:0.016, val_acc:0.982]
Epoch [108/120    avg_loss:0.018, val_acc:0.984]
Epoch [109/120    avg_loss:0.017, val_acc:0.981]
Epoch [110/120    avg_loss:0.016, val_acc:0.981]
Epoch [111/120    avg_loss:0.032, val_acc:0.984]
Epoch [112/120    avg_loss:0.016, val_acc:0.985]
Epoch [113/120    avg_loss:0.014, val_acc:0.984]
Epoch [114/120    avg_loss:0.014, val_acc:0.982]
Epoch [115/120    avg_loss:0.012, val_acc:0.984]
Epoch [116/120    avg_loss:0.013, val_acc:0.983]
Epoch [117/120    avg_loss:0.012, val_acc:0.984]
Epoch [118/120    avg_loss:0.012, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.984]
Epoch [120/120    avg_loss:0.016, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1258    1    8    0    2    0    0    0    1   13    1    0
     0    0    1]
 [   0    0    2  737    3    2    0    0    0    0    0    1    1    1
     0    0    0]
 [   0    0    0    3  210    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    7    0    0   11    0    0    0    0
     0    0    0]
 [   0    0   10   40    0    0    0    0    0    0  803   20    0    0
     2    0    0]
 [   0    0    3    0    0    0    0    0    1    0   13 2190    2    0
     1    0    0]
 [   0    0    2    9    1    5    0    0    0    1    6   17  491    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    95  244    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.86720867208672

F1 scores:
[       nan 0.98765432 0.98242874 0.95901106 0.96551724 0.98971429
 0.98722765 1.         0.99767981 0.70967742 0.94581861 0.98404853
 0.95432459 0.99730458 0.95791246 0.82571912 0.98245614]

Kappa:
0.9642281995126762
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f89cac2fb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.649, val_acc:0.423]
Epoch [2/120    avg_loss:2.303, val_acc:0.504]
Epoch [3/120    avg_loss:2.047, val_acc:0.569]
Epoch [4/120    avg_loss:1.824, val_acc:0.583]
Epoch [5/120    avg_loss:1.658, val_acc:0.580]
Epoch [6/120    avg_loss:1.494, val_acc:0.593]
Epoch [7/120    avg_loss:1.359, val_acc:0.620]
Epoch [8/120    avg_loss:1.217, val_acc:0.661]
Epoch [9/120    avg_loss:1.094, val_acc:0.690]
Epoch [10/120    avg_loss:0.893, val_acc:0.761]
Epoch [11/120    avg_loss:0.810, val_acc:0.741]
Epoch [12/120    avg_loss:0.776, val_acc:0.755]
Epoch [13/120    avg_loss:0.650, val_acc:0.790]
Epoch [14/120    avg_loss:0.608, val_acc:0.789]
Epoch [15/120    avg_loss:0.597, val_acc:0.823]
Epoch [16/120    avg_loss:0.519, val_acc:0.764]
Epoch [17/120    avg_loss:0.544, val_acc:0.801]
Epoch [18/120    avg_loss:0.462, val_acc:0.841]
Epoch [19/120    avg_loss:0.398, val_acc:0.853]
Epoch [20/120    avg_loss:0.438, val_acc:0.846]
Epoch [21/120    avg_loss:0.431, val_acc:0.830]
Epoch [22/120    avg_loss:0.425, val_acc:0.838]
Epoch [23/120    avg_loss:0.374, val_acc:0.841]
Epoch [24/120    avg_loss:0.328, val_acc:0.869]
Epoch [25/120    avg_loss:0.294, val_acc:0.881]
Epoch [26/120    avg_loss:0.251, val_acc:0.878]
Epoch [27/120    avg_loss:0.238, val_acc:0.860]
Epoch [28/120    avg_loss:0.230, val_acc:0.907]
Epoch [29/120    avg_loss:0.225, val_acc:0.904]
Epoch [30/120    avg_loss:0.243, val_acc:0.901]
Epoch [31/120    avg_loss:0.211, val_acc:0.918]
Epoch [32/120    avg_loss:0.240, val_acc:0.902]
Epoch [33/120    avg_loss:0.165, val_acc:0.923]
Epoch [34/120    avg_loss:0.234, val_acc:0.887]
Epoch [35/120    avg_loss:0.166, val_acc:0.938]
Epoch [36/120    avg_loss:0.154, val_acc:0.920]
Epoch [37/120    avg_loss:0.137, val_acc:0.891]
Epoch [38/120    avg_loss:0.139, val_acc:0.891]
Epoch [39/120    avg_loss:0.215, val_acc:0.922]
Epoch [40/120    avg_loss:0.126, val_acc:0.930]
Epoch [41/120    avg_loss:0.115, val_acc:0.933]
Epoch [42/120    avg_loss:0.138, val_acc:0.914]
Epoch [43/120    avg_loss:0.111, val_acc:0.950]
Epoch [44/120    avg_loss:0.069, val_acc:0.947]
Epoch [45/120    avg_loss:0.071, val_acc:0.943]
Epoch [46/120    avg_loss:0.074, val_acc:0.945]
Epoch [47/120    avg_loss:0.231, val_acc:0.798]
Epoch [48/120    avg_loss:0.343, val_acc:0.917]
Epoch [49/120    avg_loss:0.128, val_acc:0.942]
Epoch [50/120    avg_loss:0.096, val_acc:0.935]
Epoch [51/120    avg_loss:0.088, val_acc:0.911]
Epoch [52/120    avg_loss:0.112, val_acc:0.928]
Epoch [53/120    avg_loss:0.069, val_acc:0.941]
Epoch [54/120    avg_loss:0.108, val_acc:0.942]
Epoch [55/120    avg_loss:0.071, val_acc:0.941]
Epoch [56/120    avg_loss:0.063, val_acc:0.955]
Epoch [57/120    avg_loss:0.071, val_acc:0.949]
Epoch [58/120    avg_loss:0.065, val_acc:0.957]
Epoch [59/120    avg_loss:0.066, val_acc:0.943]
Epoch [60/120    avg_loss:0.055, val_acc:0.943]
Epoch [61/120    avg_loss:0.052, val_acc:0.953]
Epoch [62/120    avg_loss:0.044, val_acc:0.946]
Epoch [63/120    avg_loss:0.056, val_acc:0.959]
Epoch [64/120    avg_loss:0.036, val_acc:0.955]
Epoch [65/120    avg_loss:0.040, val_acc:0.968]
Epoch [66/120    avg_loss:0.069, val_acc:0.946]
Epoch [67/120    avg_loss:0.052, val_acc:0.950]
Epoch [68/120    avg_loss:0.062, val_acc:0.913]
Epoch [69/120    avg_loss:0.073, val_acc:0.950]
Epoch [70/120    avg_loss:0.068, val_acc:0.962]
Epoch [71/120    avg_loss:0.044, val_acc:0.961]
Epoch [72/120    avg_loss:0.033, val_acc:0.963]
Epoch [73/120    avg_loss:0.021, val_acc:0.964]
Epoch [74/120    avg_loss:0.025, val_acc:0.962]
Epoch [75/120    avg_loss:0.021, val_acc:0.969]
Epoch [76/120    avg_loss:0.026, val_acc:0.966]
Epoch [77/120    avg_loss:0.054, val_acc:0.956]
Epoch [78/120    avg_loss:0.047, val_acc:0.952]
Epoch [79/120    avg_loss:0.069, val_acc:0.953]
Epoch [80/120    avg_loss:0.041, val_acc:0.947]
Epoch [81/120    avg_loss:0.037, val_acc:0.956]
Epoch [82/120    avg_loss:0.030, val_acc:0.961]
Epoch [83/120    avg_loss:0.028, val_acc:0.963]
Epoch [84/120    avg_loss:0.024, val_acc:0.957]
Epoch [85/120    avg_loss:0.022, val_acc:0.966]
Epoch [86/120    avg_loss:0.046, val_acc:0.952]
Epoch [87/120    avg_loss:0.028, val_acc:0.963]
Epoch [88/120    avg_loss:0.022, val_acc:0.960]
Epoch [89/120    avg_loss:0.013, val_acc:0.970]
Epoch [90/120    avg_loss:0.015, val_acc:0.970]
Epoch [91/120    avg_loss:0.015, val_acc:0.971]
Epoch [92/120    avg_loss:0.013, val_acc:0.970]
Epoch [93/120    avg_loss:0.016, val_acc:0.970]
Epoch [94/120    avg_loss:0.014, val_acc:0.972]
Epoch [95/120    avg_loss:0.010, val_acc:0.972]
Epoch [96/120    avg_loss:0.013, val_acc:0.972]
Epoch [97/120    avg_loss:0.013, val_acc:0.973]
Epoch [98/120    avg_loss:0.011, val_acc:0.972]
Epoch [99/120    avg_loss:0.013, val_acc:0.970]
Epoch [100/120    avg_loss:0.012, val_acc:0.972]
Epoch [101/120    avg_loss:0.010, val_acc:0.971]
Epoch [102/120    avg_loss:0.012, val_acc:0.968]
Epoch [103/120    avg_loss:0.015, val_acc:0.971]
Epoch [104/120    avg_loss:0.009, val_acc:0.974]
Epoch [105/120    avg_loss:0.011, val_acc:0.974]
Epoch [106/120    avg_loss:0.009, val_acc:0.975]
Epoch [107/120    avg_loss:0.008, val_acc:0.971]
Epoch [108/120    avg_loss:0.011, val_acc:0.969]
Epoch [109/120    avg_loss:0.010, val_acc:0.971]
Epoch [110/120    avg_loss:0.008, val_acc:0.974]
Epoch [111/120    avg_loss:0.012, val_acc:0.972]
Epoch [112/120    avg_loss:0.011, val_acc:0.976]
Epoch [113/120    avg_loss:0.011, val_acc:0.974]
Epoch [114/120    avg_loss:0.010, val_acc:0.975]
Epoch [115/120    avg_loss:0.010, val_acc:0.973]
Epoch [116/120    avg_loss:0.011, val_acc:0.975]
Epoch [117/120    avg_loss:0.014, val_acc:0.974]
Epoch [118/120    avg_loss:0.013, val_acc:0.972]
Epoch [119/120    avg_loss:0.009, val_acc:0.971]
Epoch [120/120    avg_loss:0.009, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1251    3    7    1    0    0    0    0    3   18    1    0
     0    0    1]
 [   0    0    0  742    1    0    0    0    0    0    0    1    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    1    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2   45    0    1    1    0    0    0  806   19    0    0
     1    0    0]
 [   0    0    7    3    0    0    3    0    1    0   15 2167   10    0
     4    0    0]
 [   0    0    2   12    0    7    0    0    0    0    1    0  505    0
     0    2    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    2    0    1    0    0    0
  1134    1    0]
 [   0    0    0    0    0    0   21    0    0    0    0    0    0    0
   104  222    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.60704607046071

F1 scores:
[       nan 1.         0.98233216 0.95618557 0.98156682 0.98633257
 0.98059701 1.         0.99652375 1.         0.94767784 0.98165345
 0.95825427 1.         0.9517415  0.77622378 0.95953757]

Kappa:
0.9612894437130474
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f372c065a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.707, val_acc:0.468]
Epoch [2/120    avg_loss:2.366, val_acc:0.541]
Epoch [3/120    avg_loss:2.088, val_acc:0.547]
Epoch [4/120    avg_loss:1.858, val_acc:0.579]
Epoch [5/120    avg_loss:1.647, val_acc:0.612]
Epoch [6/120    avg_loss:1.504, val_acc:0.624]
Epoch [7/120    avg_loss:1.392, val_acc:0.675]
Epoch [8/120    avg_loss:1.146, val_acc:0.705]
Epoch [9/120    avg_loss:1.130, val_acc:0.700]
Epoch [10/120    avg_loss:0.897, val_acc:0.773]
Epoch [11/120    avg_loss:0.771, val_acc:0.789]
Epoch [12/120    avg_loss:0.752, val_acc:0.805]
Epoch [13/120    avg_loss:0.655, val_acc:0.802]
Epoch [14/120    avg_loss:0.622, val_acc:0.833]
Epoch [15/120    avg_loss:0.512, val_acc:0.817]
Epoch [16/120    avg_loss:0.512, val_acc:0.836]
Epoch [17/120    avg_loss:0.487, val_acc:0.852]
Epoch [18/120    avg_loss:0.385, val_acc:0.878]
Epoch [19/120    avg_loss:0.385, val_acc:0.815]
Epoch [20/120    avg_loss:0.438, val_acc:0.873]
Epoch [21/120    avg_loss:0.384, val_acc:0.853]
Epoch [22/120    avg_loss:0.322, val_acc:0.860]
Epoch [23/120    avg_loss:0.321, val_acc:0.863]
Epoch [24/120    avg_loss:0.314, val_acc:0.883]
Epoch [25/120    avg_loss:0.280, val_acc:0.900]
Epoch [26/120    avg_loss:0.214, val_acc:0.918]
Epoch [27/120    avg_loss:0.233, val_acc:0.879]
Epoch [28/120    avg_loss:0.266, val_acc:0.867]
Epoch [29/120    avg_loss:0.223, val_acc:0.911]
Epoch [30/120    avg_loss:0.194, val_acc:0.941]
Epoch [31/120    avg_loss:0.172, val_acc:0.927]
Epoch [32/120    avg_loss:0.145, val_acc:0.946]
Epoch [33/120    avg_loss:0.170, val_acc:0.952]
Epoch [34/120    avg_loss:0.126, val_acc:0.951]
Epoch [35/120    avg_loss:0.124, val_acc:0.949]
Epoch [36/120    avg_loss:0.147, val_acc:0.954]
Epoch [37/120    avg_loss:0.124, val_acc:0.924]
Epoch [38/120    avg_loss:0.112, val_acc:0.950]
Epoch [39/120    avg_loss:0.101, val_acc:0.952]
Epoch [40/120    avg_loss:0.125, val_acc:0.947]
Epoch [41/120    avg_loss:0.104, val_acc:0.957]
Epoch [42/120    avg_loss:0.113, val_acc:0.905]
Epoch [43/120    avg_loss:0.154, val_acc:0.947]
Epoch [44/120    avg_loss:0.112, val_acc:0.925]
Epoch [45/120    avg_loss:0.084, val_acc:0.958]
Epoch [46/120    avg_loss:0.103, val_acc:0.945]
Epoch [47/120    avg_loss:0.105, val_acc:0.948]
Epoch [48/120    avg_loss:0.108, val_acc:0.921]
Epoch [49/120    avg_loss:0.103, val_acc:0.925]
Epoch [50/120    avg_loss:0.121, val_acc:0.958]
Epoch [51/120    avg_loss:0.090, val_acc:0.962]
Epoch [52/120    avg_loss:0.073, val_acc:0.961]
Epoch [53/120    avg_loss:0.067, val_acc:0.960]
Epoch [54/120    avg_loss:0.054, val_acc:0.951]
Epoch [55/120    avg_loss:0.066, val_acc:0.970]
Epoch [56/120    avg_loss:0.055, val_acc:0.966]
Epoch [57/120    avg_loss:0.080, val_acc:0.963]
Epoch [58/120    avg_loss:0.104, val_acc:0.952]
Epoch [59/120    avg_loss:0.162, val_acc:0.953]
Epoch [60/120    avg_loss:0.080, val_acc:0.961]
Epoch [61/120    avg_loss:0.057, val_acc:0.973]
Epoch [62/120    avg_loss:0.069, val_acc:0.961]
Epoch [63/120    avg_loss:0.050, val_acc:0.971]
Epoch [64/120    avg_loss:0.051, val_acc:0.974]
Epoch [65/120    avg_loss:0.035, val_acc:0.967]
Epoch [66/120    avg_loss:0.036, val_acc:0.962]
Epoch [67/120    avg_loss:0.063, val_acc:0.964]
Epoch [68/120    avg_loss:0.034, val_acc:0.970]
Epoch [69/120    avg_loss:0.026, val_acc:0.972]
Epoch [70/120    avg_loss:0.040, val_acc:0.973]
Epoch [71/120    avg_loss:0.037, val_acc:0.963]
Epoch [72/120    avg_loss:0.041, val_acc:0.975]
Epoch [73/120    avg_loss:0.029, val_acc:0.980]
Epoch [74/120    avg_loss:0.026, val_acc:0.973]
Epoch [75/120    avg_loss:0.027, val_acc:0.979]
Epoch [76/120    avg_loss:0.045, val_acc:0.975]
Epoch [77/120    avg_loss:0.026, val_acc:0.972]
Epoch [78/120    avg_loss:0.063, val_acc:0.972]
Epoch [79/120    avg_loss:0.062, val_acc:0.966]
Epoch [80/120    avg_loss:0.076, val_acc:0.970]
Epoch [81/120    avg_loss:0.037, val_acc:0.960]
Epoch [82/120    avg_loss:0.038, val_acc:0.974]
Epoch [83/120    avg_loss:0.023, val_acc:0.978]
Epoch [84/120    avg_loss:0.022, val_acc:0.980]
Epoch [85/120    avg_loss:0.030, val_acc:0.975]
Epoch [86/120    avg_loss:0.020, val_acc:0.974]
Epoch [87/120    avg_loss:0.029, val_acc:0.964]
Epoch [88/120    avg_loss:0.049, val_acc:0.971]
Epoch [89/120    avg_loss:0.044, val_acc:0.972]
Epoch [90/120    avg_loss:0.083, val_acc:0.962]
Epoch [91/120    avg_loss:0.037, val_acc:0.972]
Epoch [92/120    avg_loss:0.029, val_acc:0.972]
Epoch [93/120    avg_loss:0.023, val_acc:0.975]
Epoch [94/120    avg_loss:0.025, val_acc:0.976]
Epoch [95/120    avg_loss:0.026, val_acc:0.978]
Epoch [96/120    avg_loss:0.024, val_acc:0.980]
Epoch [97/120    avg_loss:0.065, val_acc:0.970]
Epoch [98/120    avg_loss:0.030, val_acc:0.966]
Epoch [99/120    avg_loss:0.019, val_acc:0.982]
Epoch [100/120    avg_loss:0.031, val_acc:0.979]
Epoch [101/120    avg_loss:0.026, val_acc:0.978]
Epoch [102/120    avg_loss:0.024, val_acc:0.976]
Epoch [103/120    avg_loss:0.015, val_acc:0.976]
Epoch [104/120    avg_loss:0.047, val_acc:0.948]
Epoch [105/120    avg_loss:0.068, val_acc:0.933]
Epoch [106/120    avg_loss:0.059, val_acc:0.962]
Epoch [107/120    avg_loss:0.029, val_acc:0.975]
Epoch [108/120    avg_loss:0.031, val_acc:0.960]
Epoch [109/120    avg_loss:0.040, val_acc:0.979]
Epoch [110/120    avg_loss:0.030, val_acc:0.958]
Epoch [111/120    avg_loss:0.038, val_acc:0.979]
Epoch [112/120    avg_loss:0.028, val_acc:0.975]
Epoch [113/120    avg_loss:0.018, val_acc:0.977]
Epoch [114/120    avg_loss:0.017, val_acc:0.980]
Epoch [115/120    avg_loss:0.014, val_acc:0.983]
Epoch [116/120    avg_loss:0.012, val_acc:0.983]
Epoch [117/120    avg_loss:0.016, val_acc:0.979]
Epoch [118/120    avg_loss:0.012, val_acc:0.983]
Epoch [119/120    avg_loss:0.014, val_acc:0.984]
Epoch [120/120    avg_loss:0.009, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1252    1    0    0    0    0    0    1    5   23    3    0
     0    0    0]
 [   0    0    1  719    1   10    0    0    0    3    0   10    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   18   50    0    2    0    0    0    0  798    5    0    0
     2    0    0]
 [   0    0   14    0    0    1    2    0    0    0   22 2161    9    1
     0    0    0]
 [   0    0    0   16    3    0    0    0    0    0    0    9  503    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    1    0    0    0
  1135    1    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
   102  233    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.22764227642277

F1 scores:
[       nan 0.98765432 0.97431907 0.93803001 0.99069767 0.97727273
 0.98644578 1.         0.99883586 0.84210526 0.93772033 0.97827071
 0.9544592  0.99730458 0.95178197 0.79931389 0.98809524]

Kappa:
0.9569497427705894
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f10cb56aa58>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.673, val_acc:0.486]
Epoch [2/120    avg_loss:2.344, val_acc:0.499]
Epoch [3/120    avg_loss:2.054, val_acc:0.567]
Epoch [4/120    avg_loss:1.821, val_acc:0.575]
Epoch [5/120    avg_loss:1.618, val_acc:0.652]
Epoch [6/120    avg_loss:1.379, val_acc:0.701]
Epoch [7/120    avg_loss:1.183, val_acc:0.740]
Epoch [8/120    avg_loss:1.032, val_acc:0.763]
Epoch [9/120    avg_loss:0.925, val_acc:0.730]
Epoch [10/120    avg_loss:0.889, val_acc:0.781]
Epoch [11/120    avg_loss:0.775, val_acc:0.797]
Epoch [12/120    avg_loss:0.611, val_acc:0.775]
Epoch [13/120    avg_loss:0.649, val_acc:0.867]
Epoch [14/120    avg_loss:0.591, val_acc:0.786]
Epoch [15/120    avg_loss:0.531, val_acc:0.840]
Epoch [16/120    avg_loss:0.433, val_acc:0.876]
Epoch [17/120    avg_loss:0.456, val_acc:0.856]
Epoch [18/120    avg_loss:0.434, val_acc:0.859]
Epoch [19/120    avg_loss:0.364, val_acc:0.850]
Epoch [20/120    avg_loss:0.383, val_acc:0.893]
Epoch [21/120    avg_loss:0.325, val_acc:0.896]
Epoch [22/120    avg_loss:0.289, val_acc:0.900]
Epoch [23/120    avg_loss:0.277, val_acc:0.899]
Epoch [24/120    avg_loss:0.356, val_acc:0.902]
Epoch [25/120    avg_loss:0.226, val_acc:0.917]
Epoch [26/120    avg_loss:0.245, val_acc:0.897]
Epoch [27/120    avg_loss:0.269, val_acc:0.914]
Epoch [28/120    avg_loss:0.166, val_acc:0.899]
Epoch [29/120    avg_loss:0.218, val_acc:0.918]
Epoch [30/120    avg_loss:0.147, val_acc:0.942]
Epoch [31/120    avg_loss:0.127, val_acc:0.918]
Epoch [32/120    avg_loss:0.139, val_acc:0.924]
Epoch [33/120    avg_loss:0.151, val_acc:0.947]
Epoch [34/120    avg_loss:0.126, val_acc:0.936]
Epoch [35/120    avg_loss:0.119, val_acc:0.942]
Epoch [36/120    avg_loss:0.116, val_acc:0.946]
Epoch [37/120    avg_loss:0.098, val_acc:0.905]
Epoch [38/120    avg_loss:0.137, val_acc:0.914]
Epoch [39/120    avg_loss:0.137, val_acc:0.954]
Epoch [40/120    avg_loss:0.092, val_acc:0.955]
Epoch [41/120    avg_loss:0.098, val_acc:0.930]
Epoch [42/120    avg_loss:0.115, val_acc:0.920]
Epoch [43/120    avg_loss:0.254, val_acc:0.922]
Epoch [44/120    avg_loss:0.147, val_acc:0.941]
Epoch [45/120    avg_loss:0.109, val_acc:0.935]
Epoch [46/120    avg_loss:0.103, val_acc:0.941]
Epoch [47/120    avg_loss:0.071, val_acc:0.952]
Epoch [48/120    avg_loss:0.071, val_acc:0.963]
Epoch [49/120    avg_loss:0.117, val_acc:0.914]
Epoch [50/120    avg_loss:0.120, val_acc:0.944]
Epoch [51/120    avg_loss:0.100, val_acc:0.935]
Epoch [52/120    avg_loss:0.055, val_acc:0.957]
Epoch [53/120    avg_loss:0.089, val_acc:0.951]
Epoch [54/120    avg_loss:0.065, val_acc:0.967]
Epoch [55/120    avg_loss:0.080, val_acc:0.947]
Epoch [56/120    avg_loss:0.061, val_acc:0.946]
Epoch [57/120    avg_loss:0.040, val_acc:0.952]
Epoch [58/120    avg_loss:0.051, val_acc:0.958]
Epoch [59/120    avg_loss:0.076, val_acc:0.959]
Epoch [60/120    avg_loss:0.048, val_acc:0.963]
Epoch [61/120    avg_loss:0.041, val_acc:0.970]
Epoch [62/120    avg_loss:0.056, val_acc:0.961]
Epoch [63/120    avg_loss:0.035, val_acc:0.969]
Epoch [64/120    avg_loss:0.024, val_acc:0.971]
Epoch [65/120    avg_loss:0.061, val_acc:0.945]
Epoch [66/120    avg_loss:0.038, val_acc:0.973]
Epoch [67/120    avg_loss:0.081, val_acc:0.963]
Epoch [68/120    avg_loss:0.036, val_acc:0.961]
Epoch [69/120    avg_loss:0.044, val_acc:0.963]
Epoch [70/120    avg_loss:0.032, val_acc:0.961]
Epoch [71/120    avg_loss:0.044, val_acc:0.956]
Epoch [72/120    avg_loss:0.033, val_acc:0.966]
Epoch [73/120    avg_loss:0.022, val_acc:0.966]
Epoch [74/120    avg_loss:0.027, val_acc:0.974]
Epoch [75/120    avg_loss:0.025, val_acc:0.970]
Epoch [76/120    avg_loss:0.079, val_acc:0.970]
Epoch [77/120    avg_loss:0.032, val_acc:0.962]
Epoch [78/120    avg_loss:0.029, val_acc:0.974]
Epoch [79/120    avg_loss:0.030, val_acc:0.975]
Epoch [80/120    avg_loss:0.037, val_acc:0.963]
Epoch [81/120    avg_loss:0.053, val_acc:0.957]
Epoch [82/120    avg_loss:0.059, val_acc:0.947]
Epoch [83/120    avg_loss:0.039, val_acc:0.966]
Epoch [84/120    avg_loss:0.023, val_acc:0.973]
Epoch [85/120    avg_loss:0.021, val_acc:0.971]
Epoch [86/120    avg_loss:0.029, val_acc:0.963]
Epoch [87/120    avg_loss:0.023, val_acc:0.982]
Epoch [88/120    avg_loss:0.036, val_acc:0.961]
Epoch [89/120    avg_loss:0.034, val_acc:0.977]
Epoch [90/120    avg_loss:0.035, val_acc:0.980]
Epoch [91/120    avg_loss:0.022, val_acc:0.974]
Epoch [92/120    avg_loss:0.018, val_acc:0.978]
Epoch [93/120    avg_loss:0.030, val_acc:0.968]
Epoch [94/120    avg_loss:0.020, val_acc:0.978]
Epoch [95/120    avg_loss:0.015, val_acc:0.980]
Epoch [96/120    avg_loss:0.023, val_acc:0.976]
Epoch [97/120    avg_loss:0.012, val_acc:0.978]
Epoch [98/120    avg_loss:0.014, val_acc:0.973]
Epoch [99/120    avg_loss:0.014, val_acc:0.978]
Epoch [100/120    avg_loss:0.013, val_acc:0.974]
Epoch [101/120    avg_loss:0.014, val_acc:0.977]
Epoch [102/120    avg_loss:0.010, val_acc:0.976]
Epoch [103/120    avg_loss:0.010, val_acc:0.977]
Epoch [104/120    avg_loss:0.011, val_acc:0.977]
Epoch [105/120    avg_loss:0.009, val_acc:0.979]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.982]
Epoch [108/120    avg_loss:0.009, val_acc:0.984]
Epoch [109/120    avg_loss:0.009, val_acc:0.982]
Epoch [110/120    avg_loss:0.010, val_acc:0.986]
Epoch [111/120    avg_loss:0.007, val_acc:0.985]
Epoch [112/120    avg_loss:0.008, val_acc:0.985]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.007, val_acc:0.986]
Epoch [116/120    avg_loss:0.013, val_acc:0.986]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.007, val_acc:0.987]
Epoch [119/120    avg_loss:0.011, val_acc:0.986]
Epoch [120/120    avg_loss:0.008, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    2    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1263    0    2    0    1    0    0    0    3    9    7    0
     0    0    0]
 [   0    0    0  738    0    2    0    0    0    3    0    0    3    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   10   14    0    1    2    0    0    0  839    8    0    0
     1    0    0]
 [   0    2   15    0    0    0    1    1    0    0   14 2159   17    1
     0    0    0]
 [   0    0    0   18    0    0    0    0    0    0   11   12  490    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1137    2    0]
 [   0    0    0    0    0    0   16    0    0    4    0    0    0    0
    89  238    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.91056910569105

F1 scores:
[       nan 0.925      0.98097087 0.97233202 0.9953271  0.99541284
 0.98195489 0.98039216 1.         0.79069767 0.96215596 0.98180991
 0.93067426 0.99462366 0.95949367 0.8109029  0.9704142 ]

Kappa:
0.9647576341971584
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0a7d3e2a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.660, val_acc:0.448]
Epoch [2/120    avg_loss:2.337, val_acc:0.507]
Epoch [3/120    avg_loss:2.078, val_acc:0.503]
Epoch [4/120    avg_loss:1.855, val_acc:0.559]
Epoch [5/120    avg_loss:1.649, val_acc:0.620]
Epoch [6/120    avg_loss:1.471, val_acc:0.671]
Epoch [7/120    avg_loss:1.260, val_acc:0.624]
Epoch [8/120    avg_loss:1.150, val_acc:0.707]
Epoch [9/120    avg_loss:1.020, val_acc:0.731]
Epoch [10/120    avg_loss:0.905, val_acc:0.749]
Epoch [11/120    avg_loss:0.761, val_acc:0.785]
Epoch [12/120    avg_loss:0.691, val_acc:0.781]
Epoch [13/120    avg_loss:0.741, val_acc:0.709]
Epoch [14/120    avg_loss:0.596, val_acc:0.791]
Epoch [15/120    avg_loss:0.622, val_acc:0.734]
Epoch [16/120    avg_loss:0.586, val_acc:0.831]
Epoch [17/120    avg_loss:0.497, val_acc:0.802]
Epoch [18/120    avg_loss:0.462, val_acc:0.854]
Epoch [19/120    avg_loss:0.447, val_acc:0.849]
Epoch [20/120    avg_loss:0.354, val_acc:0.849]
Epoch [21/120    avg_loss:0.344, val_acc:0.838]
Epoch [22/120    avg_loss:0.370, val_acc:0.860]
Epoch [23/120    avg_loss:0.369, val_acc:0.901]
Epoch [24/120    avg_loss:0.245, val_acc:0.882]
Epoch [25/120    avg_loss:0.275, val_acc:0.922]
Epoch [26/120    avg_loss:0.241, val_acc:0.905]
Epoch [27/120    avg_loss:0.205, val_acc:0.912]
Epoch [28/120    avg_loss:0.226, val_acc:0.891]
Epoch [29/120    avg_loss:0.269, val_acc:0.857]
Epoch [30/120    avg_loss:0.311, val_acc:0.905]
Epoch [31/120    avg_loss:0.254, val_acc:0.910]
Epoch [32/120    avg_loss:0.223, val_acc:0.920]
Epoch [33/120    avg_loss:0.163, val_acc:0.891]
Epoch [34/120    avg_loss:0.143, val_acc:0.946]
Epoch [35/120    avg_loss:0.134, val_acc:0.931]
Epoch [36/120    avg_loss:0.151, val_acc:0.938]
Epoch [37/120    avg_loss:0.170, val_acc:0.922]
Epoch [38/120    avg_loss:0.153, val_acc:0.924]
Epoch [39/120    avg_loss:0.191, val_acc:0.920]
Epoch [40/120    avg_loss:0.178, val_acc:0.933]
Epoch [41/120    avg_loss:0.112, val_acc:0.939]
Epoch [42/120    avg_loss:0.126, val_acc:0.920]
Epoch [43/120    avg_loss:0.151, val_acc:0.952]
Epoch [44/120    avg_loss:0.124, val_acc:0.930]
Epoch [45/120    avg_loss:0.091, val_acc:0.947]
Epoch [46/120    avg_loss:0.101, val_acc:0.930]
Epoch [47/120    avg_loss:0.096, val_acc:0.941]
Epoch [48/120    avg_loss:0.131, val_acc:0.950]
Epoch [49/120    avg_loss:0.078, val_acc:0.948]
Epoch [50/120    avg_loss:0.101, val_acc:0.944]
Epoch [51/120    avg_loss:0.087, val_acc:0.950]
Epoch [52/120    avg_loss:0.098, val_acc:0.953]
Epoch [53/120    avg_loss:0.089, val_acc:0.951]
Epoch [54/120    avg_loss:0.061, val_acc:0.958]
Epoch [55/120    avg_loss:0.086, val_acc:0.947]
Epoch [56/120    avg_loss:0.055, val_acc:0.952]
Epoch [57/120    avg_loss:0.082, val_acc:0.942]
Epoch [58/120    avg_loss:0.066, val_acc:0.950]
Epoch [59/120    avg_loss:0.047, val_acc:0.959]
Epoch [60/120    avg_loss:0.038, val_acc:0.973]
Epoch [61/120    avg_loss:0.050, val_acc:0.973]
Epoch [62/120    avg_loss:0.061, val_acc:0.957]
Epoch [63/120    avg_loss:0.052, val_acc:0.954]
Epoch [64/120    avg_loss:0.041, val_acc:0.957]
Epoch [65/120    avg_loss:0.041, val_acc:0.966]
Epoch [66/120    avg_loss:0.030, val_acc:0.954]
Epoch [67/120    avg_loss:0.030, val_acc:0.966]
Epoch [68/120    avg_loss:0.035, val_acc:0.974]
Epoch [69/120    avg_loss:0.029, val_acc:0.955]
Epoch [70/120    avg_loss:0.029, val_acc:0.963]
Epoch [71/120    avg_loss:0.034, val_acc:0.973]
Epoch [72/120    avg_loss:0.040, val_acc:0.952]
Epoch [73/120    avg_loss:0.041, val_acc:0.970]
Epoch [74/120    avg_loss:0.055, val_acc:0.950]
Epoch [75/120    avg_loss:0.083, val_acc:0.930]
Epoch [76/120    avg_loss:0.105, val_acc:0.955]
Epoch [77/120    avg_loss:0.056, val_acc:0.961]
Epoch [78/120    avg_loss:0.048, val_acc:0.956]
Epoch [79/120    avg_loss:0.059, val_acc:0.956]
Epoch [80/120    avg_loss:0.076, val_acc:0.929]
Epoch [81/120    avg_loss:0.062, val_acc:0.964]
Epoch [82/120    avg_loss:0.032, val_acc:0.967]
Epoch [83/120    avg_loss:0.030, val_acc:0.969]
Epoch [84/120    avg_loss:0.025, val_acc:0.970]
Epoch [85/120    avg_loss:0.027, val_acc:0.970]
Epoch [86/120    avg_loss:0.028, val_acc:0.974]
Epoch [87/120    avg_loss:0.027, val_acc:0.973]
Epoch [88/120    avg_loss:0.023, val_acc:0.975]
Epoch [89/120    avg_loss:0.020, val_acc:0.974]
Epoch [90/120    avg_loss:0.020, val_acc:0.973]
Epoch [91/120    avg_loss:0.029, val_acc:0.973]
Epoch [92/120    avg_loss:0.019, val_acc:0.975]
Epoch [93/120    avg_loss:0.020, val_acc:0.974]
Epoch [94/120    avg_loss:0.018, val_acc:0.971]
Epoch [95/120    avg_loss:0.017, val_acc:0.975]
Epoch [96/120    avg_loss:0.020, val_acc:0.973]
Epoch [97/120    avg_loss:0.017, val_acc:0.974]
Epoch [98/120    avg_loss:0.017, val_acc:0.975]
Epoch [99/120    avg_loss:0.018, val_acc:0.976]
Epoch [100/120    avg_loss:0.016, val_acc:0.976]
Epoch [101/120    avg_loss:0.016, val_acc:0.976]
Epoch [102/120    avg_loss:0.015, val_acc:0.975]
Epoch [103/120    avg_loss:0.016, val_acc:0.973]
Epoch [104/120    avg_loss:0.018, val_acc:0.976]
Epoch [105/120    avg_loss:0.017, val_acc:0.975]
Epoch [106/120    avg_loss:0.014, val_acc:0.978]
Epoch [107/120    avg_loss:0.018, val_acc:0.980]
Epoch [108/120    avg_loss:0.013, val_acc:0.979]
Epoch [109/120    avg_loss:0.021, val_acc:0.979]
Epoch [110/120    avg_loss:0.018, val_acc:0.979]
Epoch [111/120    avg_loss:0.017, val_acc:0.981]
Epoch [112/120    avg_loss:0.015, val_acc:0.978]
Epoch [113/120    avg_loss:0.020, val_acc:0.980]
Epoch [114/120    avg_loss:0.013, val_acc:0.980]
Epoch [115/120    avg_loss:0.014, val_acc:0.978]
Epoch [116/120    avg_loss:0.015, val_acc:0.980]
Epoch [117/120    avg_loss:0.015, val_acc:0.980]
Epoch [118/120    avg_loss:0.012, val_acc:0.981]
Epoch [119/120    avg_loss:0.025, val_acc:0.978]
Epoch [120/120    avg_loss:0.013, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1249    0    1    0    0    0    0    0    3   19   10    3
     0    0    0]
 [   0    0    4  729    0    6    0    0    0    0    0    2    4    0
     0    2    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3   25    0    3    0    0    0    0  830   10    4    0
     0    0    0]
 [   0    0   14    0    0    0    1    0    0    0   18 2176    0    1
     0    0    0]
 [   0    0    0   11    0    3    0    0    0    0    1   14  503    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    3    0    0    0    0    0
  1132    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
   119  225    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
96.66124661246613

F1 scores:
[       nan 1.         0.9776908  0.96428571 0.99765808 0.97616345
 0.99314547 0.98039216 0.99652375 1.         0.9612044  0.98217107
 0.9490566  0.98930481 0.94372655 0.78397213 0.95757576]

Kappa:
0.9618859532694931
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdc8e0a5b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.660, val_acc:0.334]
Epoch [2/120    avg_loss:2.273, val_acc:0.486]
Epoch [3/120    avg_loss:2.057, val_acc:0.517]
Epoch [4/120    avg_loss:1.886, val_acc:0.550]
Epoch [5/120    avg_loss:1.733, val_acc:0.592]
Epoch [6/120    avg_loss:1.553, val_acc:0.631]
Epoch [7/120    avg_loss:1.422, val_acc:0.645]
Epoch [8/120    avg_loss:1.226, val_acc:0.645]
Epoch [9/120    avg_loss:1.111, val_acc:0.685]
Epoch [10/120    avg_loss:0.972, val_acc:0.742]
Epoch [11/120    avg_loss:0.852, val_acc:0.717]
Epoch [12/120    avg_loss:0.802, val_acc:0.718]
Epoch [13/120    avg_loss:0.728, val_acc:0.783]
Epoch [14/120    avg_loss:0.666, val_acc:0.762]
Epoch [15/120    avg_loss:0.587, val_acc:0.804]
Epoch [16/120    avg_loss:0.526, val_acc:0.835]
Epoch [17/120    avg_loss:0.496, val_acc:0.843]
Epoch [18/120    avg_loss:0.515, val_acc:0.798]
Epoch [19/120    avg_loss:0.467, val_acc:0.826]
Epoch [20/120    avg_loss:0.423, val_acc:0.850]
Epoch [21/120    avg_loss:0.379, val_acc:0.856]
Epoch [22/120    avg_loss:0.361, val_acc:0.825]
Epoch [23/120    avg_loss:0.375, val_acc:0.872]
Epoch [24/120    avg_loss:0.311, val_acc:0.858]
Epoch [25/120    avg_loss:0.288, val_acc:0.883]
Epoch [26/120    avg_loss:0.251, val_acc:0.892]
Epoch [27/120    avg_loss:0.259, val_acc:0.909]
Epoch [28/120    avg_loss:0.223, val_acc:0.930]
Epoch [29/120    avg_loss:0.162, val_acc:0.906]
Epoch [30/120    avg_loss:0.195, val_acc:0.907]
Epoch [31/120    avg_loss:0.205, val_acc:0.920]
Epoch [32/120    avg_loss:0.174, val_acc:0.927]
Epoch [33/120    avg_loss:0.155, val_acc:0.929]
Epoch [34/120    avg_loss:0.168, val_acc:0.935]
Epoch [35/120    avg_loss:0.128, val_acc:0.936]
Epoch [36/120    avg_loss:0.110, val_acc:0.947]
Epoch [37/120    avg_loss:0.160, val_acc:0.931]
Epoch [38/120    avg_loss:0.128, val_acc:0.944]
Epoch [39/120    avg_loss:0.111, val_acc:0.940]
Epoch [40/120    avg_loss:0.138, val_acc:0.942]
Epoch [41/120    avg_loss:0.098, val_acc:0.954]
Epoch [42/120    avg_loss:0.138, val_acc:0.929]
Epoch [43/120    avg_loss:0.127, val_acc:0.938]
Epoch [44/120    avg_loss:0.091, val_acc:0.946]
Epoch [45/120    avg_loss:0.102, val_acc:0.918]
Epoch [46/120    avg_loss:0.110, val_acc:0.954]
Epoch [47/120    avg_loss:0.086, val_acc:0.951]
Epoch [48/120    avg_loss:0.081, val_acc:0.958]
Epoch [49/120    avg_loss:0.091, val_acc:0.939]
Epoch [50/120    avg_loss:0.086, val_acc:0.955]
Epoch [51/120    avg_loss:0.063, val_acc:0.963]
Epoch [52/120    avg_loss:0.091, val_acc:0.919]
Epoch [53/120    avg_loss:0.155, val_acc:0.958]
Epoch [54/120    avg_loss:0.074, val_acc:0.955]
Epoch [55/120    avg_loss:0.082, val_acc:0.949]
Epoch [56/120    avg_loss:0.152, val_acc:0.934]
Epoch [57/120    avg_loss:0.076, val_acc:0.955]
Epoch [58/120    avg_loss:0.059, val_acc:0.962]
Epoch [59/120    avg_loss:0.059, val_acc:0.947]
Epoch [60/120    avg_loss:0.046, val_acc:0.954]
Epoch [61/120    avg_loss:0.057, val_acc:0.944]
Epoch [62/120    avg_loss:0.049, val_acc:0.963]
Epoch [63/120    avg_loss:0.032, val_acc:0.967]
Epoch [64/120    avg_loss:0.071, val_acc:0.946]
Epoch [65/120    avg_loss:0.080, val_acc:0.955]
Epoch [66/120    avg_loss:0.051, val_acc:0.961]
Epoch [67/120    avg_loss:0.050, val_acc:0.969]
Epoch [68/120    avg_loss:0.040, val_acc:0.968]
Epoch [69/120    avg_loss:0.039, val_acc:0.962]
Epoch [70/120    avg_loss:0.041, val_acc:0.962]
Epoch [71/120    avg_loss:0.056, val_acc:0.962]
Epoch [72/120    avg_loss:0.058, val_acc:0.956]
Epoch [73/120    avg_loss:0.041, val_acc:0.973]
Epoch [74/120    avg_loss:0.041, val_acc:0.967]
Epoch [75/120    avg_loss:0.028, val_acc:0.969]
Epoch [76/120    avg_loss:0.058, val_acc:0.960]
Epoch [77/120    avg_loss:0.027, val_acc:0.980]
Epoch [78/120    avg_loss:0.024, val_acc:0.982]
Epoch [79/120    avg_loss:0.029, val_acc:0.956]
Epoch [80/120    avg_loss:0.035, val_acc:0.969]
Epoch [81/120    avg_loss:0.047, val_acc:0.959]
Epoch [82/120    avg_loss:0.039, val_acc:0.946]
Epoch [83/120    avg_loss:0.028, val_acc:0.972]
Epoch [84/120    avg_loss:0.019, val_acc:0.968]
Epoch [85/120    avg_loss:0.037, val_acc:0.963]
Epoch [86/120    avg_loss:0.023, val_acc:0.975]
Epoch [87/120    avg_loss:0.025, val_acc:0.979]
Epoch [88/120    avg_loss:0.018, val_acc:0.979]
Epoch [89/120    avg_loss:0.019, val_acc:0.970]
Epoch [90/120    avg_loss:0.057, val_acc:0.966]
Epoch [91/120    avg_loss:0.024, val_acc:0.971]
Epoch [92/120    avg_loss:0.023, val_acc:0.973]
Epoch [93/120    avg_loss:0.013, val_acc:0.976]
Epoch [94/120    avg_loss:0.015, val_acc:0.978]
Epoch [95/120    avg_loss:0.010, val_acc:0.979]
Epoch [96/120    avg_loss:0.014, val_acc:0.979]
Epoch [97/120    avg_loss:0.011, val_acc:0.979]
Epoch [98/120    avg_loss:0.015, val_acc:0.978]
Epoch [99/120    avg_loss:0.011, val_acc:0.978]
Epoch [100/120    avg_loss:0.016, val_acc:0.978]
Epoch [101/120    avg_loss:0.009, val_acc:0.980]
Epoch [102/120    avg_loss:0.010, val_acc:0.980]
Epoch [103/120    avg_loss:0.011, val_acc:0.982]
Epoch [104/120    avg_loss:0.009, val_acc:0.982]
Epoch [105/120    avg_loss:0.017, val_acc:0.982]
Epoch [106/120    avg_loss:0.012, val_acc:0.983]
Epoch [107/120    avg_loss:0.012, val_acc:0.983]
Epoch [108/120    avg_loss:0.013, val_acc:0.980]
Epoch [109/120    avg_loss:0.009, val_acc:0.982]
Epoch [110/120    avg_loss:0.008, val_acc:0.981]
Epoch [111/120    avg_loss:0.009, val_acc:0.981]
Epoch [112/120    avg_loss:0.016, val_acc:0.983]
Epoch [113/120    avg_loss:0.009, val_acc:0.981]
Epoch [114/120    avg_loss:0.013, val_acc:0.983]
Epoch [115/120    avg_loss:0.008, val_acc:0.982]
Epoch [116/120    avg_loss:0.015, val_acc:0.982]
Epoch [117/120    avg_loss:0.014, val_acc:0.983]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.013, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1257    1    9    0    2    0    0    1    1   14    0    0
     0    0    0]
 [   0    0    5  719    1    8    0    0    0    9    1    1    0    3
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    5    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    2    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    1    0    6    0    0   11    0    0    0    0
     0    0    0]
 [   0    0    2   42    0    1    2    0    0    0  817   11    0    0
     0    0    0]
 [   0    0   12    0    0   13    1    0    0    0   19 2161    1    2
     0    1    0]
 [   0    0    0   16    0   12    0    0    0    0    6   13  485    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    0    0    0    0    0    0
  1130    1    0]
 [   0    0    0    0    0    1   41    0    0    0    0    0    0    0
   100  205    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.78319783197831

F1 scores:
[       nan 1.         0.98164779 0.94171578 0.97011494 0.94247788
 0.95307918 1.         1.         0.56410256 0.95055265 0.97960109
 0.95004897 0.98666667 0.95037847 0.73873874 0.98809524]

Kappa:
0.9518863477239323
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6dcabd1a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.695, val_acc:0.504]
Epoch [2/120    avg_loss:2.326, val_acc:0.505]
Epoch [3/120    avg_loss:2.027, val_acc:0.501]
Epoch [4/120    avg_loss:1.804, val_acc:0.579]
Epoch [5/120    avg_loss:1.672, val_acc:0.597]
Epoch [6/120    avg_loss:1.487, val_acc:0.642]
Epoch [7/120    avg_loss:1.304, val_acc:0.583]
Epoch [8/120    avg_loss:1.189, val_acc:0.703]
Epoch [9/120    avg_loss:1.034, val_acc:0.704]
Epoch [10/120    avg_loss:0.953, val_acc:0.760]
Epoch [11/120    avg_loss:0.798, val_acc:0.777]
Epoch [12/120    avg_loss:0.756, val_acc:0.784]
Epoch [13/120    avg_loss:0.689, val_acc:0.792]
Epoch [14/120    avg_loss:0.624, val_acc:0.784]
Epoch [15/120    avg_loss:0.580, val_acc:0.808]
Epoch [16/120    avg_loss:0.513, val_acc:0.818]
Epoch [17/120    avg_loss:0.486, val_acc:0.854]
Epoch [18/120    avg_loss:0.399, val_acc:0.866]
Epoch [19/120    avg_loss:0.439, val_acc:0.838]
Epoch [20/120    avg_loss:0.403, val_acc:0.855]
Epoch [21/120    avg_loss:0.414, val_acc:0.854]
Epoch [22/120    avg_loss:0.412, val_acc:0.874]
Epoch [23/120    avg_loss:0.327, val_acc:0.866]
Epoch [24/120    avg_loss:0.319, val_acc:0.867]
Epoch [25/120    avg_loss:0.251, val_acc:0.851]
Epoch [26/120    avg_loss:0.273, val_acc:0.895]
Epoch [27/120    avg_loss:0.208, val_acc:0.911]
Epoch [28/120    avg_loss:0.232, val_acc:0.832]
Epoch [29/120    avg_loss:0.211, val_acc:0.899]
Epoch [30/120    avg_loss:0.270, val_acc:0.896]
Epoch [31/120    avg_loss:0.214, val_acc:0.913]
Epoch [32/120    avg_loss:0.185, val_acc:0.908]
Epoch [33/120    avg_loss:0.170, val_acc:0.934]
Epoch [34/120    avg_loss:0.140, val_acc:0.921]
Epoch [35/120    avg_loss:0.158, val_acc:0.913]
Epoch [36/120    avg_loss:0.127, val_acc:0.925]
Epoch [37/120    avg_loss:0.137, val_acc:0.935]
Epoch [38/120    avg_loss:0.116, val_acc:0.939]
Epoch [39/120    avg_loss:0.135, val_acc:0.934]
Epoch [40/120    avg_loss:0.123, val_acc:0.946]
Epoch [41/120    avg_loss:0.106, val_acc:0.934]
Epoch [42/120    avg_loss:0.103, val_acc:0.940]
Epoch [43/120    avg_loss:0.089, val_acc:0.927]
Epoch [44/120    avg_loss:0.183, val_acc:0.927]
Epoch [45/120    avg_loss:0.124, val_acc:0.926]
Epoch [46/120    avg_loss:0.091, val_acc:0.946]
Epoch [47/120    avg_loss:0.085, val_acc:0.949]
Epoch [48/120    avg_loss:0.091, val_acc:0.887]
Epoch [49/120    avg_loss:0.108, val_acc:0.958]
Epoch [50/120    avg_loss:0.074, val_acc:0.948]
Epoch [51/120    avg_loss:0.087, val_acc:0.955]
Epoch [52/120    avg_loss:0.074, val_acc:0.952]
Epoch [53/120    avg_loss:0.054, val_acc:0.954]
Epoch [54/120    avg_loss:0.042, val_acc:0.950]
Epoch [55/120    avg_loss:0.068, val_acc:0.942]
Epoch [56/120    avg_loss:0.076, val_acc:0.948]
Epoch [57/120    avg_loss:0.086, val_acc:0.925]
Epoch [58/120    avg_loss:0.056, val_acc:0.961]
Epoch [59/120    avg_loss:0.048, val_acc:0.959]
Epoch [60/120    avg_loss:0.143, val_acc:0.933]
Epoch [61/120    avg_loss:0.166, val_acc:0.928]
Epoch [62/120    avg_loss:0.095, val_acc:0.959]
Epoch [63/120    avg_loss:0.068, val_acc:0.964]
Epoch [64/120    avg_loss:0.049, val_acc:0.962]
Epoch [65/120    avg_loss:0.050, val_acc:0.965]
Epoch [66/120    avg_loss:0.041, val_acc:0.959]
Epoch [67/120    avg_loss:0.062, val_acc:0.947]
Epoch [68/120    avg_loss:0.067, val_acc:0.950]
Epoch [69/120    avg_loss:0.094, val_acc:0.953]
Epoch [70/120    avg_loss:0.078, val_acc:0.961]
Epoch [71/120    avg_loss:0.051, val_acc:0.963]
Epoch [72/120    avg_loss:0.052, val_acc:0.958]
Epoch [73/120    avg_loss:0.040, val_acc:0.966]
Epoch [74/120    avg_loss:0.039, val_acc:0.966]
Epoch [75/120    avg_loss:0.036, val_acc:0.973]
Epoch [76/120    avg_loss:0.029, val_acc:0.967]
Epoch [77/120    avg_loss:0.034, val_acc:0.965]
Epoch [78/120    avg_loss:0.027, val_acc:0.971]
Epoch [79/120    avg_loss:0.025, val_acc:0.974]
Epoch [80/120    avg_loss:0.016, val_acc:0.978]
Epoch [81/120    avg_loss:0.026, val_acc:0.975]
Epoch [82/120    avg_loss:0.020, val_acc:0.966]
Epoch [83/120    avg_loss:0.014, val_acc:0.979]
Epoch [84/120    avg_loss:0.073, val_acc:0.950]
Epoch [85/120    avg_loss:0.066, val_acc:0.961]
Epoch [86/120    avg_loss:0.049, val_acc:0.973]
Epoch [87/120    avg_loss:0.031, val_acc:0.975]
Epoch [88/120    avg_loss:0.025, val_acc:0.952]
Epoch [89/120    avg_loss:0.066, val_acc:0.945]
Epoch [90/120    avg_loss:0.039, val_acc:0.938]
Epoch [91/120    avg_loss:0.023, val_acc:0.971]
Epoch [92/120    avg_loss:0.016, val_acc:0.973]
Epoch [93/120    avg_loss:0.014, val_acc:0.977]
Epoch [94/120    avg_loss:0.022, val_acc:0.974]
Epoch [95/120    avg_loss:0.017, val_acc:0.951]
Epoch [96/120    avg_loss:0.016, val_acc:0.973]
Epoch [97/120    avg_loss:0.017, val_acc:0.977]
Epoch [98/120    avg_loss:0.009, val_acc:0.977]
Epoch [99/120    avg_loss:0.014, val_acc:0.977]
Epoch [100/120    avg_loss:0.011, val_acc:0.978]
Epoch [101/120    avg_loss:0.009, val_acc:0.978]
Epoch [102/120    avg_loss:0.011, val_acc:0.980]
Epoch [103/120    avg_loss:0.011, val_acc:0.978]
Epoch [104/120    avg_loss:0.011, val_acc:0.979]
Epoch [105/120    avg_loss:0.012, val_acc:0.980]
Epoch [106/120    avg_loss:0.011, val_acc:0.977]
Epoch [107/120    avg_loss:0.012, val_acc:0.979]
Epoch [108/120    avg_loss:0.007, val_acc:0.977]
Epoch [109/120    avg_loss:0.012, val_acc:0.978]
Epoch [110/120    avg_loss:0.009, val_acc:0.977]
Epoch [111/120    avg_loss:0.010, val_acc:0.978]
Epoch [112/120    avg_loss:0.009, val_acc:0.978]
Epoch [113/120    avg_loss:0.011, val_acc:0.979]
Epoch [114/120    avg_loss:0.010, val_acc:0.977]
Epoch [115/120    avg_loss:0.009, val_acc:0.980]
Epoch [116/120    avg_loss:0.010, val_acc:0.980]
Epoch [117/120    avg_loss:0.007, val_acc:0.977]
Epoch [118/120    avg_loss:0.016, val_acc:0.980]
Epoch [119/120    avg_loss:0.008, val_acc:0.980]
Epoch [120/120    avg_loss:0.009, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1234    0    9    0    0    0    0    0    5   28    1    0
     0    8    0]
 [   0    0    0  732    1    0    0    0    0    6    0    0    8    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    0   11    0    0    0    0    0    0  847   13    0    0
     0    4    0]
 [   0    0    1    0    0    0    0    0    0    0    7 2201    0    1
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    1   14   10  499    0
     0    7    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    2    0    1    0    0    0
  1128    6    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
    80  260    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.30081300813008

F1 scores:
[       nan 0.975      0.97936508 0.98123324 0.97471264 0.99308756
 0.99241275 1.         0.99767981 0.79069767 0.96744717 0.98655312
 0.9541109  0.99730458 0.95877603 0.82278481 0.98203593]

Kappa:
0.9691918199686659
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8ac735ea20>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.636, val_acc:0.424]
Epoch [2/120    avg_loss:2.340, val_acc:0.491]
Epoch [3/120    avg_loss:2.055, val_acc:0.510]
Epoch [4/120    avg_loss:1.819, val_acc:0.585]
Epoch [5/120    avg_loss:1.670, val_acc:0.633]
Epoch [6/120    avg_loss:1.456, val_acc:0.653]
Epoch [7/120    avg_loss:1.255, val_acc:0.674]
Epoch [8/120    avg_loss:1.112, val_acc:0.704]
Epoch [9/120    avg_loss:1.011, val_acc:0.715]
Epoch [10/120    avg_loss:0.874, val_acc:0.730]
Epoch [11/120    avg_loss:0.778, val_acc:0.725]
Epoch [12/120    avg_loss:0.733, val_acc:0.788]
Epoch [13/120    avg_loss:0.631, val_acc:0.778]
Epoch [14/120    avg_loss:0.560, val_acc:0.817]
Epoch [15/120    avg_loss:0.524, val_acc:0.847]
Epoch [16/120    avg_loss:0.445, val_acc:0.820]
Epoch [17/120    avg_loss:0.409, val_acc:0.797]
Epoch [18/120    avg_loss:0.440, val_acc:0.841]
Epoch [19/120    avg_loss:0.360, val_acc:0.841]
Epoch [20/120    avg_loss:0.347, val_acc:0.847]
Epoch [21/120    avg_loss:0.283, val_acc:0.884]
Epoch [22/120    avg_loss:0.255, val_acc:0.868]
Epoch [23/120    avg_loss:0.284, val_acc:0.890]
Epoch [24/120    avg_loss:0.239, val_acc:0.867]
Epoch [25/120    avg_loss:0.241, val_acc:0.886]
Epoch [26/120    avg_loss:0.232, val_acc:0.895]
Epoch [27/120    avg_loss:0.225, val_acc:0.888]
Epoch [28/120    avg_loss:1.836, val_acc:0.462]
Epoch [29/120    avg_loss:1.769, val_acc:0.500]
Epoch [30/120    avg_loss:1.647, val_acc:0.539]
Epoch [31/120    avg_loss:1.514, val_acc:0.529]
Epoch [32/120    avg_loss:1.392, val_acc:0.574]
Epoch [33/120    avg_loss:1.313, val_acc:0.609]
Epoch [34/120    avg_loss:1.230, val_acc:0.617]
Epoch [35/120    avg_loss:1.224, val_acc:0.624]
Epoch [36/120    avg_loss:1.177, val_acc:0.637]
Epoch [37/120    avg_loss:1.182, val_acc:0.664]
Epoch [38/120    avg_loss:1.151, val_acc:0.665]
Epoch [39/120    avg_loss:1.052, val_acc:0.672]
Epoch [40/120    avg_loss:1.027, val_acc:0.692]
Epoch [41/120    avg_loss:1.015, val_acc:0.700]
Epoch [42/120    avg_loss:0.954, val_acc:0.695]
Epoch [43/120    avg_loss:0.929, val_acc:0.700]
Epoch [44/120    avg_loss:0.930, val_acc:0.701]
Epoch [45/120    avg_loss:0.930, val_acc:0.702]
Epoch [46/120    avg_loss:0.934, val_acc:0.691]
Epoch [47/120    avg_loss:0.961, val_acc:0.697]
Epoch [48/120    avg_loss:0.928, val_acc:0.701]
Epoch [49/120    avg_loss:0.902, val_acc:0.705]
Epoch [50/120    avg_loss:0.911, val_acc:0.704]
Epoch [51/120    avg_loss:0.926, val_acc:0.711]
Epoch [52/120    avg_loss:0.901, val_acc:0.711]
Epoch [53/120    avg_loss:0.924, val_acc:0.710]
Epoch [54/120    avg_loss:0.869, val_acc:0.711]
Epoch [55/120    avg_loss:0.923, val_acc:0.710]
Epoch [56/120    avg_loss:0.880, val_acc:0.710]
Epoch [57/120    avg_loss:0.929, val_acc:0.712]
Epoch [58/120    avg_loss:0.902, val_acc:0.709]
Epoch [59/120    avg_loss:0.908, val_acc:0.711]
Epoch [60/120    avg_loss:0.856, val_acc:0.708]
Epoch [61/120    avg_loss:0.902, val_acc:0.708]
Epoch [62/120    avg_loss:0.922, val_acc:0.711]
Epoch [63/120    avg_loss:0.908, val_acc:0.708]
Epoch [64/120    avg_loss:0.890, val_acc:0.711]
Epoch [65/120    avg_loss:0.883, val_acc:0.712]
Epoch [66/120    avg_loss:0.883, val_acc:0.712]
Epoch [67/120    avg_loss:0.911, val_acc:0.712]
Epoch [68/120    avg_loss:0.874, val_acc:0.712]
Epoch [69/120    avg_loss:0.916, val_acc:0.712]
Epoch [70/120    avg_loss:0.921, val_acc:0.712]
Epoch [71/120    avg_loss:0.895, val_acc:0.712]
Epoch [72/120    avg_loss:0.861, val_acc:0.712]
Epoch [73/120    avg_loss:0.917, val_acc:0.712]
Epoch [74/120    avg_loss:0.878, val_acc:0.711]
Epoch [75/120    avg_loss:0.862, val_acc:0.711]
Epoch [76/120    avg_loss:0.928, val_acc:0.711]
Epoch [77/120    avg_loss:0.889, val_acc:0.711]
Epoch [78/120    avg_loss:0.877, val_acc:0.711]
Epoch [79/120    avg_loss:0.928, val_acc:0.711]
Epoch [80/120    avg_loss:0.903, val_acc:0.711]
Epoch [81/120    avg_loss:0.872, val_acc:0.711]
Epoch [82/120    avg_loss:0.933, val_acc:0.711]
Epoch [83/120    avg_loss:0.894, val_acc:0.711]
Epoch [84/120    avg_loss:0.901, val_acc:0.711]
Epoch [85/120    avg_loss:0.879, val_acc:0.711]
Epoch [86/120    avg_loss:0.904, val_acc:0.711]
Epoch [87/120    avg_loss:0.924, val_acc:0.711]
Epoch [88/120    avg_loss:0.902, val_acc:0.711]
Epoch [89/120    avg_loss:0.895, val_acc:0.711]
Epoch [90/120    avg_loss:0.878, val_acc:0.711]
Epoch [91/120    avg_loss:0.888, val_acc:0.711]
Epoch [92/120    avg_loss:0.864, val_acc:0.711]
Epoch [93/120    avg_loss:0.882, val_acc:0.711]
Epoch [94/120    avg_loss:0.883, val_acc:0.711]
Epoch [95/120    avg_loss:0.878, val_acc:0.711]
Epoch [96/120    avg_loss:0.904, val_acc:0.711]
Epoch [97/120    avg_loss:0.909, val_acc:0.711]
Epoch [98/120    avg_loss:0.881, val_acc:0.711]
Epoch [99/120    avg_loss:0.879, val_acc:0.711]
Epoch [100/120    avg_loss:0.882, val_acc:0.711]
Epoch [101/120    avg_loss:0.892, val_acc:0.711]
Epoch [102/120    avg_loss:0.889, val_acc:0.711]
Epoch [103/120    avg_loss:0.924, val_acc:0.711]
Epoch [104/120    avg_loss:0.904, val_acc:0.711]
Epoch [105/120    avg_loss:0.875, val_acc:0.711]
Epoch [106/120    avg_loss:0.879, val_acc:0.711]
Epoch [107/120    avg_loss:0.860, val_acc:0.711]
Epoch [108/120    avg_loss:0.891, val_acc:0.711]
Epoch [109/120    avg_loss:0.898, val_acc:0.711]
Epoch [110/120    avg_loss:0.870, val_acc:0.711]
Epoch [111/120    avg_loss:0.887, val_acc:0.711]
Epoch [112/120    avg_loss:0.875, val_acc:0.711]
Epoch [113/120    avg_loss:0.900, val_acc:0.711]
Epoch [114/120    avg_loss:0.866, val_acc:0.711]
Epoch [115/120    avg_loss:0.879, val_acc:0.711]
Epoch [116/120    avg_loss:0.873, val_acc:0.711]
Epoch [117/120    avg_loss:0.904, val_acc:0.711]
Epoch [118/120    avg_loss:0.898, val_acc:0.711]
Epoch [119/120    avg_loss:0.885, val_acc:0.711]
Epoch [120/120    avg_loss:0.882, val_acc:0.711]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    2    0    0    2    0    0    0
     0    0    0]
 [   0    6 1007   66   53    0   13    0    0    0   62   61   12    0
     2    0    3]
 [   0    0  108  268   51   88   13    0    0    0   19   91   63   44
     0    2    0]
 [   0    0   15    0  188    0    0    0    0    0    0    0   10    0
     0    0    0]
 [   0    0    2    3    0  355    8   38    0    0    4    0    0    0
    25    0    0]
 [   0    0    1   33    5    2  585    0    0    0    0    7    0    9
    15    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    6    0    0   11    0    0    0  410    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    4    1   13    0    0    0    0    0    0    0
     0    0    0]
 [   0   17   88   21   13   53   19    0    0    0  535   84   22   20
     0    3    0]
 [   0   11  243   60   32   55   45    0   11    0   45 1494  127   28
    29   27    3]
 [   0    0   28   16   51   20    0    0    0    0    2    0  344    0
     0   24   49]
 [   0    0    2    7    0    0    4    0    0    0    0    1    0  171
     0    0    0]
 [   0    0   15    1    2    3   13    0    8    0    0    0    0    0
  1030   67    0]
 [   0    0   27    2   26   16   45    0    0    0    0   92   13    0
    30   96    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
71.85907859078591

F1 scores:
[       nan 0.62711864 0.71393123 0.4379085  0.57935285 0.69066148
 0.82685512 0.55555556 0.95459837 0.         0.69300518 0.73960396
 0.60992908 0.74835886 0.90748899 0.33922261 0.75336323]

Kappa:
0.6822148096834534
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4242c0cac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.661, val_acc:0.374]
Epoch [2/120    avg_loss:2.348, val_acc:0.446]
Epoch [3/120    avg_loss:2.133, val_acc:0.555]
Epoch [4/120    avg_loss:1.922, val_acc:0.588]
Epoch [5/120    avg_loss:1.805, val_acc:0.608]
Epoch [6/120    avg_loss:1.581, val_acc:0.627]
Epoch [7/120    avg_loss:1.452, val_acc:0.681]
Epoch [8/120    avg_loss:1.293, val_acc:0.688]
Epoch [9/120    avg_loss:1.183, val_acc:0.702]
Epoch [10/120    avg_loss:1.089, val_acc:0.751]
Epoch [11/120    avg_loss:0.901, val_acc:0.756]
Epoch [12/120    avg_loss:0.854, val_acc:0.795]
Epoch [13/120    avg_loss:0.803, val_acc:0.783]
Epoch [14/120    avg_loss:0.675, val_acc:0.814]
Epoch [15/120    avg_loss:0.637, val_acc:0.842]
Epoch [16/120    avg_loss:0.593, val_acc:0.814]
Epoch [17/120    avg_loss:0.544, val_acc:0.835]
Epoch [18/120    avg_loss:0.456, val_acc:0.811]
Epoch [19/120    avg_loss:0.425, val_acc:0.860]
Epoch [20/120    avg_loss:0.384, val_acc:0.838]
Epoch [21/120    avg_loss:0.363, val_acc:0.790]
Epoch [22/120    avg_loss:2.209, val_acc:0.353]
Epoch [23/120    avg_loss:2.132, val_acc:0.468]
Epoch [24/120    avg_loss:1.980, val_acc:0.546]
Epoch [25/120    avg_loss:1.825, val_acc:0.482]
Epoch [26/120    avg_loss:1.717, val_acc:0.578]
Epoch [27/120    avg_loss:1.574, val_acc:0.593]
Epoch [28/120    avg_loss:1.500, val_acc:0.631]
Epoch [29/120    avg_loss:1.466, val_acc:0.631]
Epoch [30/120    avg_loss:1.255, val_acc:0.642]
Epoch [31/120    avg_loss:1.269, val_acc:0.637]
Epoch [32/120    avg_loss:1.250, val_acc:0.676]
Epoch [33/120    avg_loss:1.143, val_acc:0.676]
Epoch [34/120    avg_loss:1.124, val_acc:0.677]
Epoch [35/120    avg_loss:1.059, val_acc:0.677]
Epoch [36/120    avg_loss:1.123, val_acc:0.679]
Epoch [37/120    avg_loss:1.051, val_acc:0.680]
Epoch [38/120    avg_loss:1.038, val_acc:0.688]
Epoch [39/120    avg_loss:0.993, val_acc:0.684]
Epoch [40/120    avg_loss:1.037, val_acc:0.688]
Epoch [41/120    avg_loss:1.034, val_acc:0.689]
Epoch [42/120    avg_loss:1.018, val_acc:0.690]
Epoch [43/120    avg_loss:1.043, val_acc:0.693]
Epoch [44/120    avg_loss:0.993, val_acc:0.688]
Epoch [45/120    avg_loss:0.986, val_acc:0.681]
Epoch [46/120    avg_loss:0.969, val_acc:0.682]
Epoch [47/120    avg_loss:0.997, val_acc:0.683]
Epoch [48/120    avg_loss:0.984, val_acc:0.683]
Epoch [49/120    avg_loss:1.013, val_acc:0.686]
Epoch [50/120    avg_loss:0.957, val_acc:0.686]
Epoch [51/120    avg_loss:0.981, val_acc:0.684]
Epoch [52/120    avg_loss:0.951, val_acc:0.690]
Epoch [53/120    avg_loss:1.010, val_acc:0.688]
Epoch [54/120    avg_loss:0.977, val_acc:0.688]
Epoch [55/120    avg_loss:0.964, val_acc:0.692]
Epoch [56/120    avg_loss:1.016, val_acc:0.686]
Epoch [57/120    avg_loss:1.019, val_acc:0.692]
Epoch [58/120    avg_loss:1.008, val_acc:0.691]
Epoch [59/120    avg_loss:0.977, val_acc:0.691]
Epoch [60/120    avg_loss:0.986, val_acc:0.690]
Epoch [61/120    avg_loss:0.980, val_acc:0.690]
Epoch [62/120    avg_loss:0.976, val_acc:0.690]
Epoch [63/120    avg_loss:0.966, val_acc:0.690]
Epoch [64/120    avg_loss:1.009, val_acc:0.690]
Epoch [65/120    avg_loss:0.985, val_acc:0.690]
Epoch [66/120    avg_loss:0.991, val_acc:0.690]
Epoch [67/120    avg_loss:0.993, val_acc:0.690]
Epoch [68/120    avg_loss:0.948, val_acc:0.690]
Epoch [69/120    avg_loss:0.975, val_acc:0.691]
Epoch [70/120    avg_loss:0.987, val_acc:0.691]
Epoch [71/120    avg_loss:0.950, val_acc:0.691]
Epoch [72/120    avg_loss:1.058, val_acc:0.691]
Epoch [73/120    avg_loss:1.003, val_acc:0.691]
Epoch [74/120    avg_loss:1.014, val_acc:0.691]
Epoch [75/120    avg_loss:0.991, val_acc:0.691]
Epoch [76/120    avg_loss:0.963, val_acc:0.691]
Epoch [77/120    avg_loss:0.982, val_acc:0.691]
Epoch [78/120    avg_loss:1.030, val_acc:0.691]
Epoch [79/120    avg_loss:0.975, val_acc:0.691]
Epoch [80/120    avg_loss:1.013, val_acc:0.691]
Epoch [81/120    avg_loss:0.980, val_acc:0.691]
Epoch [82/120    avg_loss:0.953, val_acc:0.691]
Epoch [83/120    avg_loss:0.974, val_acc:0.691]
Epoch [84/120    avg_loss:0.983, val_acc:0.691]
Epoch [85/120    avg_loss:0.988, val_acc:0.691]
Epoch [86/120    avg_loss:0.974, val_acc:0.691]
Epoch [87/120    avg_loss:0.970, val_acc:0.691]
Epoch [88/120    avg_loss:1.004, val_acc:0.691]
Epoch [89/120    avg_loss:0.926, val_acc:0.691]
Epoch [90/120    avg_loss:0.999, val_acc:0.691]
Epoch [91/120    avg_loss:0.992, val_acc:0.691]
Epoch [92/120    avg_loss:0.972, val_acc:0.691]
Epoch [93/120    avg_loss:0.978, val_acc:0.691]
Epoch [94/120    avg_loss:1.008, val_acc:0.691]
Epoch [95/120    avg_loss:0.997, val_acc:0.691]
Epoch [96/120    avg_loss:0.939, val_acc:0.691]
Epoch [97/120    avg_loss:1.008, val_acc:0.691]
Epoch [98/120    avg_loss:0.986, val_acc:0.691]
Epoch [99/120    avg_loss:0.942, val_acc:0.691]
Epoch [100/120    avg_loss:0.995, val_acc:0.691]
Epoch [101/120    avg_loss:0.979, val_acc:0.691]
Epoch [102/120    avg_loss:0.971, val_acc:0.691]
Epoch [103/120    avg_loss:0.983, val_acc:0.691]
Epoch [104/120    avg_loss:0.977, val_acc:0.691]
Epoch [105/120    avg_loss:0.986, val_acc:0.691]
Epoch [106/120    avg_loss:0.972, val_acc:0.691]
Epoch [107/120    avg_loss:0.980, val_acc:0.691]
Epoch [108/120    avg_loss:0.981, val_acc:0.691]
Epoch [109/120    avg_loss:0.955, val_acc:0.691]
Epoch [110/120    avg_loss:1.004, val_acc:0.691]
Epoch [111/120    avg_loss:0.992, val_acc:0.691]
Epoch [112/120    avg_loss:0.984, val_acc:0.691]
Epoch [113/120    avg_loss:0.963, val_acc:0.691]
Epoch [114/120    avg_loss:0.966, val_acc:0.691]
Epoch [115/120    avg_loss:0.961, val_acc:0.691]
Epoch [116/120    avg_loss:0.991, val_acc:0.691]
Epoch [117/120    avg_loss:1.003, val_acc:0.691]
Epoch [118/120    avg_loss:0.988, val_acc:0.691]
Epoch [119/120    avg_loss:0.991, val_acc:0.691]
Epoch [120/120    avg_loss:0.979, val_acc:0.691]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   11    9    0    0    0    2    0   14    0    4    0    1    0
     0    0    0]
 [   0    2  787   39   20    1   34    0    0    0   91  294    0   13
     0    4    0]
 [   0    0   64  321    7   21   28    0    0    0  136  114    0   52
     0    4    0]
 [   0    0   34   38  141    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  360   57    0    0    0    0    0    0    2
    14    2    0]
 [   0    0    0    8    3    0  632    0    0    0    0    1    0    0
    13    0    0]
 [   0    0    0    0    0   25    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0   18    0    0    0    0    0    0    0
     0    0    0]
 [   0    0   64   60    0   39   22    0    0    0  431  206    9   26
     3   15    0]
 [   0    0  178   88    7   40   54    0   34    0   45 1512  174   52
    18    8    0]
 [   0    0  115   49   33   19   19    0    0    0   23   14  225    1
     6    4   26]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    0    0    0    0    3   13
  1115    0    0]
 [   0    0    2   25    0  106   40    0    0    0    0    0   74    3
    43   54    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
68.14092140921409

F1 scores:
[       nan 0.40740741 0.62017336 0.46690909 0.66509434 0.68311195
 0.80870122 0.         0.94597574 0.         0.53707165 0.69501264
 0.44031311 0.69548872 0.94853254 0.24657534 0.86010363]

Kappa:
0.6374877869641413
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1a746acac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.682, val_acc:0.380]
Epoch [2/120    avg_loss:2.353, val_acc:0.481]
Epoch [3/120    avg_loss:2.138, val_acc:0.582]
Epoch [4/120    avg_loss:1.922, val_acc:0.600]
Epoch [5/120    avg_loss:1.762, val_acc:0.618]
Epoch [6/120    avg_loss:1.552, val_acc:0.649]
Epoch [7/120    avg_loss:1.424, val_acc:0.673]
Epoch [8/120    avg_loss:1.283, val_acc:0.682]
Epoch [9/120    avg_loss:1.111, val_acc:0.688]
Epoch [10/120    avg_loss:0.990, val_acc:0.768]
Epoch [11/120    avg_loss:0.881, val_acc:0.815]
Epoch [12/120    avg_loss:0.831, val_acc:0.800]
Epoch [13/120    avg_loss:0.696, val_acc:0.785]
Epoch [14/120    avg_loss:0.681, val_acc:0.787]
Epoch [15/120    avg_loss:0.654, val_acc:0.822]
Epoch [16/120    avg_loss:0.548, val_acc:0.838]
Epoch [17/120    avg_loss:0.526, val_acc:0.819]
Epoch [18/120    avg_loss:0.553, val_acc:0.842]
Epoch [19/120    avg_loss:0.429, val_acc:0.858]
Epoch [20/120    avg_loss:0.409, val_acc:0.802]
Epoch [21/120    avg_loss:0.460, val_acc:0.861]
Epoch [22/120    avg_loss:0.363, val_acc:0.911]
Epoch [23/120    avg_loss:0.297, val_acc:0.881]
Epoch [24/120    avg_loss:0.249, val_acc:0.905]
Epoch [25/120    avg_loss:0.265, val_acc:0.908]
Epoch [26/120    avg_loss:0.294, val_acc:0.896]
Epoch [27/120    avg_loss:0.277, val_acc:0.889]
Epoch [28/120    avg_loss:0.235, val_acc:0.885]
Epoch [29/120    avg_loss:0.208, val_acc:0.929]
Epoch [30/120    avg_loss:0.202, val_acc:0.900]
Epoch [31/120    avg_loss:0.218, val_acc:0.932]
Epoch [32/120    avg_loss:0.177, val_acc:0.911]
Epoch [33/120    avg_loss:0.170, val_acc:0.921]
Epoch [34/120    avg_loss:0.348, val_acc:0.896]
Epoch [35/120    avg_loss:0.218, val_acc:0.934]
Epoch [36/120    avg_loss:0.161, val_acc:0.921]
Epoch [37/120    avg_loss:0.144, val_acc:0.925]
Epoch [38/120    avg_loss:0.140, val_acc:0.934]
Epoch [39/120    avg_loss:0.118, val_acc:0.949]
Epoch [40/120    avg_loss:0.108, val_acc:0.919]
Epoch [41/120    avg_loss:0.114, val_acc:0.927]
Epoch [42/120    avg_loss:0.117, val_acc:0.938]
Epoch [43/120    avg_loss:0.084, val_acc:0.919]
Epoch [44/120    avg_loss:0.110, val_acc:0.923]
Epoch [45/120    avg_loss:0.100, val_acc:0.934]
Epoch [46/120    avg_loss:0.142, val_acc:0.925]
Epoch [47/120    avg_loss:0.101, val_acc:0.955]
Epoch [48/120    avg_loss:0.090, val_acc:0.931]
Epoch [49/120    avg_loss:0.091, val_acc:0.941]
Epoch [50/120    avg_loss:0.060, val_acc:0.957]
Epoch [51/120    avg_loss:0.074, val_acc:0.950]
Epoch [52/120    avg_loss:0.094, val_acc:0.952]
Epoch [53/120    avg_loss:0.151, val_acc:0.921]
Epoch [54/120    avg_loss:0.119, val_acc:0.958]
Epoch [55/120    avg_loss:0.068, val_acc:0.940]
Epoch [56/120    avg_loss:0.138, val_acc:0.944]
Epoch [57/120    avg_loss:0.080, val_acc:0.949]
Epoch [58/120    avg_loss:0.069, val_acc:0.940]
Epoch [59/120    avg_loss:0.097, val_acc:0.949]
Epoch [60/120    avg_loss:0.061, val_acc:0.963]
Epoch [61/120    avg_loss:0.060, val_acc:0.954]
Epoch [62/120    avg_loss:0.060, val_acc:0.958]
Epoch [63/120    avg_loss:0.063, val_acc:0.940]
Epoch [64/120    avg_loss:0.072, val_acc:0.944]
Epoch [65/120    avg_loss:0.049, val_acc:0.956]
Epoch [66/120    avg_loss:0.048, val_acc:0.965]
Epoch [67/120    avg_loss:0.050, val_acc:0.963]
Epoch [68/120    avg_loss:0.054, val_acc:0.956]
Epoch [69/120    avg_loss:0.056, val_acc:0.938]
Epoch [70/120    avg_loss:0.048, val_acc:0.971]
Epoch [71/120    avg_loss:0.037, val_acc:0.973]
Epoch [72/120    avg_loss:0.028, val_acc:0.956]
Epoch [73/120    avg_loss:0.031, val_acc:0.965]
Epoch [74/120    avg_loss:0.031, val_acc:0.955]
Epoch [75/120    avg_loss:0.047, val_acc:0.967]
Epoch [76/120    avg_loss:0.034, val_acc:0.955]
Epoch [77/120    avg_loss:0.037, val_acc:0.964]
Epoch [78/120    avg_loss:0.028, val_acc:0.962]
Epoch [79/120    avg_loss:0.034, val_acc:0.967]
Epoch [80/120    avg_loss:0.030, val_acc:0.967]
Epoch [81/120    avg_loss:0.025, val_acc:0.968]
Epoch [82/120    avg_loss:0.039, val_acc:0.967]
Epoch [83/120    avg_loss:0.035, val_acc:0.968]
Epoch [84/120    avg_loss:0.025, val_acc:0.972]
Epoch [85/120    avg_loss:0.020, val_acc:0.975]
Epoch [86/120    avg_loss:0.018, val_acc:0.976]
Epoch [87/120    avg_loss:0.018, val_acc:0.973]
Epoch [88/120    avg_loss:0.015, val_acc:0.973]
Epoch [89/120    avg_loss:0.015, val_acc:0.973]
Epoch [90/120    avg_loss:0.016, val_acc:0.977]
Epoch [91/120    avg_loss:0.014, val_acc:0.976]
Epoch [92/120    avg_loss:0.017, val_acc:0.980]
Epoch [93/120    avg_loss:0.015, val_acc:0.980]
Epoch [94/120    avg_loss:0.017, val_acc:0.981]
Epoch [95/120    avg_loss:0.012, val_acc:0.977]
Epoch [96/120    avg_loss:0.016, val_acc:0.976]
Epoch [97/120    avg_loss:0.015, val_acc:0.977]
Epoch [98/120    avg_loss:0.013, val_acc:0.976]
Epoch [99/120    avg_loss:0.013, val_acc:0.977]
Epoch [100/120    avg_loss:0.012, val_acc:0.976]
Epoch [101/120    avg_loss:0.017, val_acc:0.975]
Epoch [102/120    avg_loss:0.012, val_acc:0.976]
Epoch [103/120    avg_loss:0.012, val_acc:0.977]
Epoch [104/120    avg_loss:0.014, val_acc:0.976]
Epoch [105/120    avg_loss:0.011, val_acc:0.977]
Epoch [106/120    avg_loss:0.018, val_acc:0.979]
Epoch [107/120    avg_loss:0.015, val_acc:0.979]
Epoch [108/120    avg_loss:0.013, val_acc:0.979]
Epoch [109/120    avg_loss:0.013, val_acc:0.979]
Epoch [110/120    avg_loss:0.018, val_acc:0.980]
Epoch [111/120    avg_loss:0.012, val_acc:0.980]
Epoch [112/120    avg_loss:0.010, val_acc:0.980]
Epoch [113/120    avg_loss:0.011, val_acc:0.977]
Epoch [114/120    avg_loss:0.012, val_acc:0.977]
Epoch [115/120    avg_loss:0.012, val_acc:0.977]
Epoch [116/120    avg_loss:0.013, val_acc:0.977]
Epoch [117/120    avg_loss:0.013, val_acc:0.979]
Epoch [118/120    avg_loss:0.022, val_acc:0.977]
Epoch [119/120    avg_loss:0.014, val_acc:0.979]
Epoch [120/120    avg_loss:0.016, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1259    3    0    0    5    0    0    2    4   12    0    0
     0    0    0]
 [   0    0    0  742    0    1    0    0    0    3    0    0    1    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   16   64    0    7    0    0    0    0  761   24    0    0
     3    0    0]
 [   0    0    9    1    0    0   11    0    0    0    6 2180    0    1
     2    0    0]
 [   0    0    0   22    1    3    0    0    0    0   14    4  487    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    1    0    0    0
  1135    0    0]
 [   0    0    0    0    0    1   50    0    0    0    0    0    0    1
    47  248    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
96.37940379403794

F1 scores:
[       nan 0.98765432 0.98014792 0.93983534 0.99530516 0.98305085
 0.94989107 1.         1.         0.87804878 0.91576414 0.9835326
 0.94839338 0.99462366 0.97592433 0.83221477 0.96385542]

Kappa:
0.9586882222396257
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f22362bba90>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.633, val_acc:0.475]
Epoch [2/120    avg_loss:2.267, val_acc:0.569]
Epoch [3/120    avg_loss:2.025, val_acc:0.614]
Epoch [4/120    avg_loss:1.837, val_acc:0.624]
Epoch [5/120    avg_loss:1.626, val_acc:0.661]
Epoch [6/120    avg_loss:1.433, val_acc:0.662]
Epoch [7/120    avg_loss:1.296, val_acc:0.681]
Epoch [8/120    avg_loss:1.144, val_acc:0.735]
Epoch [9/120    avg_loss:0.991, val_acc:0.738]
Epoch [10/120    avg_loss:0.876, val_acc:0.775]
Epoch [11/120    avg_loss:0.744, val_acc:0.791]
Epoch [12/120    avg_loss:0.780, val_acc:0.789]
Epoch [13/120    avg_loss:0.690, val_acc:0.784]
Epoch [14/120    avg_loss:0.666, val_acc:0.854]
Epoch [15/120    avg_loss:0.543, val_acc:0.843]
Epoch [16/120    avg_loss:0.829, val_acc:0.090]
Epoch [17/120    avg_loss:2.059, val_acc:0.623]
Epoch [18/120    avg_loss:1.311, val_acc:0.686]
Epoch [19/120    avg_loss:1.034, val_acc:0.713]
Epoch [20/120    avg_loss:0.855, val_acc:0.753]
Epoch [21/120    avg_loss:0.747, val_acc:0.803]
Epoch [22/120    avg_loss:0.694, val_acc:0.767]
Epoch [23/120    avg_loss:0.670, val_acc:0.828]
Epoch [24/120    avg_loss:0.529, val_acc:0.865]
Epoch [25/120    avg_loss:0.533, val_acc:0.846]
Epoch [26/120    avg_loss:0.442, val_acc:0.845]
Epoch [27/120    avg_loss:0.381, val_acc:0.887]
Epoch [28/120    avg_loss:0.365, val_acc:0.876]
Epoch [29/120    avg_loss:0.362, val_acc:0.889]
Epoch [30/120    avg_loss:0.326, val_acc:0.905]
Epoch [31/120    avg_loss:0.290, val_acc:0.899]
Epoch [32/120    avg_loss:0.284, val_acc:0.919]
Epoch [33/120    avg_loss:0.273, val_acc:0.909]
Epoch [34/120    avg_loss:0.253, val_acc:0.910]
Epoch [35/120    avg_loss:0.226, val_acc:0.922]
Epoch [36/120    avg_loss:0.195, val_acc:0.911]
Epoch [37/120    avg_loss:0.246, val_acc:0.883]
Epoch [38/120    avg_loss:0.232, val_acc:0.921]
Epoch [39/120    avg_loss:0.187, val_acc:0.944]
Epoch [40/120    avg_loss:0.190, val_acc:0.920]
Epoch [41/120    avg_loss:1.046, val_acc:0.655]
Epoch [42/120    avg_loss:1.118, val_acc:0.768]
Epoch [43/120    avg_loss:0.653, val_acc:0.845]
Epoch [44/120    avg_loss:0.429, val_acc:0.877]
Epoch [45/120    avg_loss:0.333, val_acc:0.912]
Epoch [46/120    avg_loss:0.229, val_acc:0.916]
Epoch [47/120    avg_loss:0.236, val_acc:0.934]
Epoch [48/120    avg_loss:0.200, val_acc:0.934]
Epoch [49/120    avg_loss:0.185, val_acc:0.920]
Epoch [50/120    avg_loss:0.140, val_acc:0.932]
Epoch [51/120    avg_loss:0.168, val_acc:0.921]
Epoch [52/120    avg_loss:0.173, val_acc:0.939]
Epoch [53/120    avg_loss:0.119, val_acc:0.952]
Epoch [54/120    avg_loss:0.118, val_acc:0.955]
Epoch [55/120    avg_loss:0.099, val_acc:0.957]
Epoch [56/120    avg_loss:0.107, val_acc:0.958]
Epoch [57/120    avg_loss:0.102, val_acc:0.962]
Epoch [58/120    avg_loss:0.092, val_acc:0.962]
Epoch [59/120    avg_loss:0.094, val_acc:0.959]
Epoch [60/120    avg_loss:0.098, val_acc:0.957]
Epoch [61/120    avg_loss:0.092, val_acc:0.957]
Epoch [62/120    avg_loss:0.096, val_acc:0.961]
Epoch [63/120    avg_loss:0.089, val_acc:0.961]
Epoch [64/120    avg_loss:0.084, val_acc:0.963]
Epoch [65/120    avg_loss:0.081, val_acc:0.961]
Epoch [66/120    avg_loss:0.086, val_acc:0.963]
Epoch [67/120    avg_loss:0.081, val_acc:0.964]
Epoch [68/120    avg_loss:0.079, val_acc:0.959]
Epoch [69/120    avg_loss:0.089, val_acc:0.964]
Epoch [70/120    avg_loss:0.077, val_acc:0.962]
Epoch [71/120    avg_loss:0.099, val_acc:0.964]
Epoch [72/120    avg_loss:0.074, val_acc:0.964]
Epoch [73/120    avg_loss:0.078, val_acc:0.963]
Epoch [74/120    avg_loss:0.076, val_acc:0.962]
Epoch [75/120    avg_loss:0.079, val_acc:0.959]
Epoch [76/120    avg_loss:0.072, val_acc:0.964]
Epoch [77/120    avg_loss:0.075, val_acc:0.966]
Epoch [78/120    avg_loss:0.070, val_acc:0.962]
Epoch [79/120    avg_loss:0.066, val_acc:0.962]
Epoch [80/120    avg_loss:0.072, val_acc:0.965]
Epoch [81/120    avg_loss:0.063, val_acc:0.964]
Epoch [82/120    avg_loss:0.075, val_acc:0.966]
Epoch [83/120    avg_loss:0.067, val_acc:0.963]
Epoch [84/120    avg_loss:0.064, val_acc:0.968]
Epoch [85/120    avg_loss:0.063, val_acc:0.967]
Epoch [86/120    avg_loss:0.061, val_acc:0.967]
Epoch [87/120    avg_loss:0.061, val_acc:0.966]
Epoch [88/120    avg_loss:0.073, val_acc:0.966]
Epoch [89/120    avg_loss:0.067, val_acc:0.962]
Epoch [90/120    avg_loss:0.062, val_acc:0.968]
Epoch [91/120    avg_loss:0.066, val_acc:0.966]
Epoch [92/120    avg_loss:0.066, val_acc:0.967]
Epoch [93/120    avg_loss:0.071, val_acc:0.971]
Epoch [94/120    avg_loss:0.053, val_acc:0.972]
Epoch [95/120    avg_loss:0.053, val_acc:0.968]
Epoch [96/120    avg_loss:0.061, val_acc:0.966]
Epoch [97/120    avg_loss:0.061, val_acc:0.970]
Epoch [98/120    avg_loss:0.069, val_acc:0.962]
Epoch [99/120    avg_loss:0.059, val_acc:0.966]
Epoch [100/120    avg_loss:0.054, val_acc:0.967]
Epoch [101/120    avg_loss:0.051, val_acc:0.968]
Epoch [102/120    avg_loss:0.064, val_acc:0.964]
Epoch [103/120    avg_loss:0.061, val_acc:0.967]
Epoch [104/120    avg_loss:0.056, val_acc:0.967]
Epoch [105/120    avg_loss:0.052, val_acc:0.967]
Epoch [106/120    avg_loss:0.056, val_acc:0.967]
Epoch [107/120    avg_loss:0.058, val_acc:0.966]
Epoch [108/120    avg_loss:0.053, val_acc:0.967]
Epoch [109/120    avg_loss:0.045, val_acc:0.964]
Epoch [110/120    avg_loss:0.057, val_acc:0.964]
Epoch [111/120    avg_loss:0.045, val_acc:0.964]
Epoch [112/120    avg_loss:0.050, val_acc:0.964]
Epoch [113/120    avg_loss:0.049, val_acc:0.964]
Epoch [114/120    avg_loss:0.046, val_acc:0.964]
Epoch [115/120    avg_loss:0.062, val_acc:0.966]
Epoch [116/120    avg_loss:0.055, val_acc:0.964]
Epoch [117/120    avg_loss:0.056, val_acc:0.964]
Epoch [118/120    avg_loss:0.047, val_acc:0.964]
Epoch [119/120    avg_loss:0.050, val_acc:0.966]
Epoch [120/120    avg_loss:0.054, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1224    1    0    2    3    0    0    0   14   37    0    0
     1    3    0]
 [   0    0    1  737    3    0    0    0    0    1    0    0    3    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  418    0   14    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    5    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   51   62    0    0    0    0    0    0  741   10    0    0
     4    7    0]
 [   0    0   23    0    0    3   15    0    0    0   10 2153    1    2
     3    0    0]
 [   0    0    0   12    0    4    0    0    0    0   15    0  490    0
     0    4    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    4    4    0    0
  1127    0    0]
 [   0    0    0    1    0    0   14    0    0    0    0    0   12    0
   111  209    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.76422764227642

F1 scores:
[       nan 0.94871795 0.94736842 0.94487179 0.99300699 0.96647399
 0.97240865 0.78125    0.99883856 0.94736842 0.89116055 0.9744286
 0.9414025  0.98930481 0.94428152 0.73333333 0.94318182]

Kappa:
0.94024826163869
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8d59145a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.647, val_acc:0.426]
Epoch [2/120    avg_loss:2.343, val_acc:0.417]
Epoch [3/120    avg_loss:2.107, val_acc:0.548]
Epoch [4/120    avg_loss:1.851, val_acc:0.556]
Epoch [5/120    avg_loss:1.707, val_acc:0.613]
Epoch [6/120    avg_loss:1.546, val_acc:0.635]
Epoch [7/120    avg_loss:1.397, val_acc:0.661]
Epoch [8/120    avg_loss:1.363, val_acc:0.686]
Epoch [9/120    avg_loss:1.164, val_acc:0.725]
Epoch [10/120    avg_loss:1.036, val_acc:0.699]
Epoch [11/120    avg_loss:0.947, val_acc:0.722]
Epoch [12/120    avg_loss:0.864, val_acc:0.740]
Epoch [13/120    avg_loss:0.738, val_acc:0.803]
Epoch [14/120    avg_loss:0.664, val_acc:0.785]
Epoch [15/120    avg_loss:0.634, val_acc:0.815]
Epoch [16/120    avg_loss:0.638, val_acc:0.817]
Epoch [17/120    avg_loss:0.554, val_acc:0.766]
Epoch [18/120    avg_loss:0.501, val_acc:0.847]
Epoch [19/120    avg_loss:0.453, val_acc:0.839]
Epoch [20/120    avg_loss:0.421, val_acc:0.875]
Epoch [21/120    avg_loss:0.430, val_acc:0.839]
Epoch [22/120    avg_loss:0.448, val_acc:0.865]
Epoch [23/120    avg_loss:0.341, val_acc:0.897]
Epoch [24/120    avg_loss:1.347, val_acc:0.236]
Epoch [25/120    avg_loss:2.311, val_acc:0.382]
Epoch [26/120    avg_loss:2.106, val_acc:0.451]
Epoch [27/120    avg_loss:1.892, val_acc:0.506]
Epoch [28/120    avg_loss:1.846, val_acc:0.541]
Epoch [29/120    avg_loss:1.753, val_acc:0.576]
Epoch [30/120    avg_loss:1.614, val_acc:0.567]
Epoch [31/120    avg_loss:1.553, val_acc:0.586]
Epoch [32/120    avg_loss:1.511, val_acc:0.568]
Epoch [33/120    avg_loss:1.473, val_acc:0.603]
Epoch [34/120    avg_loss:1.385, val_acc:0.627]
Epoch [35/120    avg_loss:1.350, val_acc:0.632]
Epoch [36/120    avg_loss:1.306, val_acc:0.628]
Epoch [37/120    avg_loss:1.267, val_acc:0.630]
Epoch [38/120    avg_loss:1.235, val_acc:0.641]
Epoch [39/120    avg_loss:1.230, val_acc:0.637]
Epoch [40/120    avg_loss:1.229, val_acc:0.641]
Epoch [41/120    avg_loss:1.212, val_acc:0.640]
Epoch [42/120    avg_loss:1.236, val_acc:0.648]
Epoch [43/120    avg_loss:1.172, val_acc:0.645]
Epoch [44/120    avg_loss:1.201, val_acc:0.649]
Epoch [45/120    avg_loss:1.212, val_acc:0.647]
Epoch [46/120    avg_loss:1.204, val_acc:0.647]
Epoch [47/120    avg_loss:1.159, val_acc:0.643]
Epoch [48/120    avg_loss:1.137, val_acc:0.651]
Epoch [49/120    avg_loss:1.204, val_acc:0.656]
Epoch [50/120    avg_loss:1.166, val_acc:0.652]
Epoch [51/120    avg_loss:1.154, val_acc:0.652]
Epoch [52/120    avg_loss:1.138, val_acc:0.652]
Epoch [53/120    avg_loss:1.134, val_acc:0.652]
Epoch [54/120    avg_loss:1.202, val_acc:0.652]
Epoch [55/120    avg_loss:1.173, val_acc:0.656]
Epoch [56/120    avg_loss:1.177, val_acc:0.653]
Epoch [57/120    avg_loss:1.157, val_acc:0.653]
Epoch [58/120    avg_loss:1.168, val_acc:0.653]
Epoch [59/120    avg_loss:1.153, val_acc:0.656]
Epoch [60/120    avg_loss:1.193, val_acc:0.655]
Epoch [61/120    avg_loss:1.149, val_acc:0.655]
Epoch [62/120    avg_loss:1.141, val_acc:0.656]
Epoch [63/120    avg_loss:1.140, val_acc:0.656]
Epoch [64/120    avg_loss:1.151, val_acc:0.656]
Epoch [65/120    avg_loss:1.171, val_acc:0.656]
Epoch [66/120    avg_loss:1.154, val_acc:0.656]
Epoch [67/120    avg_loss:1.174, val_acc:0.656]
Epoch [68/120    avg_loss:1.143, val_acc:0.656]
Epoch [69/120    avg_loss:1.197, val_acc:0.655]
Epoch [70/120    avg_loss:1.139, val_acc:0.656]
Epoch [71/120    avg_loss:1.146, val_acc:0.656]
Epoch [72/120    avg_loss:1.167, val_acc:0.656]
Epoch [73/120    avg_loss:1.159, val_acc:0.656]
Epoch [74/120    avg_loss:1.179, val_acc:0.656]
Epoch [75/120    avg_loss:1.123, val_acc:0.656]
Epoch [76/120    avg_loss:1.207, val_acc:0.656]
Epoch [77/120    avg_loss:1.187, val_acc:0.656]
Epoch [78/120    avg_loss:1.191, val_acc:0.656]
Epoch [79/120    avg_loss:1.112, val_acc:0.656]
Epoch [80/120    avg_loss:1.163, val_acc:0.656]
Epoch [81/120    avg_loss:1.181, val_acc:0.656]
Epoch [82/120    avg_loss:1.133, val_acc:0.656]
Epoch [83/120    avg_loss:1.179, val_acc:0.656]
Epoch [84/120    avg_loss:1.139, val_acc:0.656]
Epoch [85/120    avg_loss:1.120, val_acc:0.656]
Epoch [86/120    avg_loss:1.135, val_acc:0.656]
Epoch [87/120    avg_loss:1.189, val_acc:0.656]
Epoch [88/120    avg_loss:1.180, val_acc:0.656]
Epoch [89/120    avg_loss:1.187, val_acc:0.656]
Epoch [90/120    avg_loss:1.157, val_acc:0.656]
Epoch [91/120    avg_loss:1.156, val_acc:0.656]
Epoch [92/120    avg_loss:1.169, val_acc:0.656]
Epoch [93/120    avg_loss:1.134, val_acc:0.656]
Epoch [94/120    avg_loss:1.106, val_acc:0.656]
Epoch [95/120    avg_loss:1.111, val_acc:0.656]
Epoch [96/120    avg_loss:1.164, val_acc:0.656]
Epoch [97/120    avg_loss:1.173, val_acc:0.656]
Epoch [98/120    avg_loss:1.163, val_acc:0.656]
Epoch [99/120    avg_loss:1.167, val_acc:0.656]
Epoch [100/120    avg_loss:1.164, val_acc:0.656]
Epoch [101/120    avg_loss:1.156, val_acc:0.656]
Epoch [102/120    avg_loss:1.113, val_acc:0.656]
Epoch [103/120    avg_loss:1.132, val_acc:0.656]
Epoch [104/120    avg_loss:1.207, val_acc:0.656]
Epoch [105/120    avg_loss:1.157, val_acc:0.656]
Epoch [106/120    avg_loss:1.186, val_acc:0.656]
Epoch [107/120    avg_loss:1.172, val_acc:0.656]
Epoch [108/120    avg_loss:1.177, val_acc:0.656]
Epoch [109/120    avg_loss:1.147, val_acc:0.656]
Epoch [110/120    avg_loss:1.165, val_acc:0.656]
Epoch [111/120    avg_loss:1.121, val_acc:0.656]
Epoch [112/120    avg_loss:1.156, val_acc:0.656]
Epoch [113/120    avg_loss:1.184, val_acc:0.656]
Epoch [114/120    avg_loss:1.168, val_acc:0.656]
Epoch [115/120    avg_loss:1.169, val_acc:0.656]
Epoch [116/120    avg_loss:1.189, val_acc:0.656]
Epoch [117/120    avg_loss:1.161, val_acc:0.656]
Epoch [118/120    avg_loss:1.130, val_acc:0.656]
Epoch [119/120    avg_loss:1.214, val_acc:0.656]
Epoch [120/120    avg_loss:1.194, val_acc:0.656]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   23   16    0    0    0    0    0    0    0    0    2    0    0
     0    0    0]
 [   0    8  749   52   31    0  102    0    0    0  103  204   17    0
    11    1    7]
 [   0    0   74  126   16   36  110    0    0    0  191  139    0   41
     5    9    0]
 [   0    0   69   20   28    1   61    0    0    0    0   32    0    0
     2    0    0]
 [   0    0    0    0    0  162   94    6    0    0    1    3    0    2
   155   12    0]
 [   0    0    0   29    0    0  616    0    0    0    0    3    0    0
     9    0    0]
 [   0    0    0    0    0   18    7    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0  424    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    1   17    0    0    0    0    0    0    0
     0    0    0]
 [   0    0  132   42    0    7   36    0    0    0  450  140    2    7
     7   52    0]
 [   0    0  279  108    5   24   45    0   14    0   98 1374  171   38
    52    2    0]
 [   0    0   68    0   25    1   91    0    0    0    7   15  243    0
    18   12   54]
 [   0    0    0    0    0    0    4    0    0    0    0   12    0  169
     0    0    0]
 [   0    0    0    0    0   21    0    0    0    0    0    0    5    7
  1106    0    0]
 [   0    0    6    6    0    0  199    0    0    0    0    8   43    0
    69   16    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
60.36856368563686

F1 scores:
[       nan 0.63888889 0.55895522 0.22300885 0.17610063 0.45892351
 0.60421775 0.         0.97695853 0.         0.52173913 0.66344761
 0.47647059 0.75278396 0.85969685 0.07095344 0.72807018]

Kappa:
0.5489927737962055
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:17:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc14790ca58>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.648, val_acc:0.502]
Epoch [2/120    avg_loss:2.278, val_acc:0.526]
Epoch [3/120    avg_loss:2.046, val_acc:0.573]
Epoch [4/120    avg_loss:1.852, val_acc:0.609]
Epoch [5/120    avg_loss:1.704, val_acc:0.624]
Epoch [6/120    avg_loss:1.507, val_acc:0.654]
Epoch [7/120    avg_loss:1.371, val_acc:0.664]
Epoch [8/120    avg_loss:1.247, val_acc:0.673]
Epoch [9/120    avg_loss:1.110, val_acc:0.700]
Epoch [10/120    avg_loss:0.974, val_acc:0.751]
Epoch [11/120    avg_loss:0.874, val_acc:0.679]
Epoch [12/120    avg_loss:0.802, val_acc:0.778]
Epoch [13/120    avg_loss:0.709, val_acc:0.803]
Epoch [14/120    avg_loss:0.698, val_acc:0.793]
Epoch [15/120    avg_loss:0.638, val_acc:0.807]
Epoch [16/120    avg_loss:0.525, val_acc:0.841]
Epoch [17/120    avg_loss:0.537, val_acc:0.812]
Epoch [18/120    avg_loss:0.551, val_acc:0.833]
Epoch [19/120    avg_loss:0.441, val_acc:0.843]
Epoch [20/120    avg_loss:0.420, val_acc:0.868]
Epoch [21/120    avg_loss:0.396, val_acc:0.857]
Epoch [22/120    avg_loss:0.448, val_acc:0.841]
Epoch [23/120    avg_loss:0.353, val_acc:0.857]
Epoch [24/120    avg_loss:0.325, val_acc:0.903]
Epoch [25/120    avg_loss:0.322, val_acc:0.868]
Epoch [26/120    avg_loss:0.322, val_acc:0.872]
Epoch [27/120    avg_loss:0.427, val_acc:0.891]
Epoch [28/120    avg_loss:0.320, val_acc:0.880]
Epoch [29/120    avg_loss:0.284, val_acc:0.886]
Epoch [30/120    avg_loss:0.251, val_acc:0.903]
Epoch [31/120    avg_loss:0.268, val_acc:0.908]
Epoch [32/120    avg_loss:0.233, val_acc:0.936]
Epoch [33/120    avg_loss:0.219, val_acc:0.898]
Epoch [34/120    avg_loss:0.273, val_acc:0.910]
Epoch [35/120    avg_loss:0.248, val_acc:0.880]
Epoch [36/120    avg_loss:0.167, val_acc:0.934]
Epoch [37/120    avg_loss:0.209, val_acc:0.927]
Epoch [38/120    avg_loss:0.216, val_acc:0.886]
Epoch [39/120    avg_loss:0.221, val_acc:0.920]
Epoch [40/120    avg_loss:0.197, val_acc:0.900]
Epoch [41/120    avg_loss:0.160, val_acc:0.917]
Epoch [42/120    avg_loss:0.164, val_acc:0.919]
Epoch [43/120    avg_loss:0.165, val_acc:0.927]
Epoch [44/120    avg_loss:0.162, val_acc:0.919]
Epoch [45/120    avg_loss:0.148, val_acc:0.926]
Epoch [46/120    avg_loss:0.088, val_acc:0.943]
Epoch [47/120    avg_loss:0.084, val_acc:0.946]
Epoch [48/120    avg_loss:0.087, val_acc:0.950]
Epoch [49/120    avg_loss:0.084, val_acc:0.954]
Epoch [50/120    avg_loss:0.078, val_acc:0.956]
Epoch [51/120    avg_loss:0.076, val_acc:0.961]
Epoch [52/120    avg_loss:0.075, val_acc:0.963]
Epoch [53/120    avg_loss:0.067, val_acc:0.962]
Epoch [54/120    avg_loss:0.075, val_acc:0.963]
Epoch [55/120    avg_loss:0.070, val_acc:0.962]
Epoch [56/120    avg_loss:0.086, val_acc:0.965]
Epoch [57/120    avg_loss:0.073, val_acc:0.963]
Epoch [58/120    avg_loss:0.067, val_acc:0.965]
Epoch [59/120    avg_loss:0.061, val_acc:0.964]
Epoch [60/120    avg_loss:0.062, val_acc:0.961]
Epoch [61/120    avg_loss:0.064, val_acc:0.963]
Epoch [62/120    avg_loss:0.059, val_acc:0.964]
Epoch [63/120    avg_loss:0.059, val_acc:0.965]
Epoch [64/120    avg_loss:0.059, val_acc:0.965]
Epoch [65/120    avg_loss:0.064, val_acc:0.966]
Epoch [66/120    avg_loss:0.064, val_acc:0.967]
Epoch [67/120    avg_loss:0.067, val_acc:0.963]
Epoch [68/120    avg_loss:0.057, val_acc:0.963]
Epoch [69/120    avg_loss:0.060, val_acc:0.966]
Epoch [70/120    avg_loss:0.056, val_acc:0.970]
Epoch [71/120    avg_loss:0.054, val_acc:0.970]
Epoch [72/120    avg_loss:0.060, val_acc:0.962]
Epoch [73/120    avg_loss:0.054, val_acc:0.965]
Epoch [74/120    avg_loss:0.057, val_acc:0.964]
Epoch [75/120    avg_loss:0.059, val_acc:0.964]
Epoch [76/120    avg_loss:0.053, val_acc:0.966]
Epoch [77/120    avg_loss:0.051, val_acc:0.970]
Epoch [78/120    avg_loss:0.051, val_acc:0.971]
Epoch [79/120    avg_loss:0.055, val_acc:0.970]
Epoch [80/120    avg_loss:0.056, val_acc:0.964]
Epoch [81/120    avg_loss:0.050, val_acc:0.962]
Epoch [82/120    avg_loss:0.050, val_acc:0.970]
Epoch [83/120    avg_loss:0.051, val_acc:0.971]
Epoch [84/120    avg_loss:0.044, val_acc:0.972]
Epoch [85/120    avg_loss:0.046, val_acc:0.971]
Epoch [86/120    avg_loss:0.050, val_acc:0.967]
Epoch [87/120    avg_loss:0.054, val_acc:0.968]
Epoch [88/120    avg_loss:0.052, val_acc:0.972]
Epoch [89/120    avg_loss:0.042, val_acc:0.972]
Epoch [90/120    avg_loss:0.050, val_acc:0.972]
Epoch [91/120    avg_loss:0.048, val_acc:0.971]
Epoch [92/120    avg_loss:0.044, val_acc:0.972]
Epoch [93/120    avg_loss:0.048, val_acc:0.971]
Epoch [94/120    avg_loss:0.047, val_acc:0.967]
Epoch [95/120    avg_loss:0.051, val_acc:0.973]
Epoch [96/120    avg_loss:0.048, val_acc:0.968]
Epoch [97/120    avg_loss:0.044, val_acc:0.968]
Epoch [98/120    avg_loss:0.049, val_acc:0.970]
Epoch [99/120    avg_loss:0.045, val_acc:0.973]
Epoch [100/120    avg_loss:0.043, val_acc:0.976]
Epoch [101/120    avg_loss:0.052, val_acc:0.972]
Epoch [102/120    avg_loss:0.040, val_acc:0.971]
Epoch [103/120    avg_loss:0.054, val_acc:0.966]
Epoch [104/120    avg_loss:0.045, val_acc:0.974]
Epoch [105/120    avg_loss:0.041, val_acc:0.976]
Epoch [106/120    avg_loss:0.045, val_acc:0.968]
Epoch [107/120    avg_loss:0.043, val_acc:0.972]
Epoch [108/120    avg_loss:0.038, val_acc:0.974]
Epoch [109/120    avg_loss:0.048, val_acc:0.973]
Epoch [110/120    avg_loss:0.037, val_acc:0.972]
Epoch [111/120    avg_loss:0.038, val_acc:0.976]
Epoch [112/120    avg_loss:0.050, val_acc:0.972]
Epoch [113/120    avg_loss:0.034, val_acc:0.974]
Epoch [114/120    avg_loss:0.040, val_acc:0.975]
Epoch [115/120    avg_loss:0.041, val_acc:0.971]
Epoch [116/120    avg_loss:0.039, val_acc:0.976]
Epoch [117/120    avg_loss:0.039, val_acc:0.979]
Epoch [118/120    avg_loss:0.039, val_acc:0.974]
Epoch [119/120    avg_loss:0.040, val_acc:0.971]
Epoch [120/120    avg_loss:0.040, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    1 1216    7    0    0   11    0    0    0   11   38    1    0
     0    0    0]
 [   0    0    0  706    0    5    0    0    0   12    0    0   23    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    1    0    1    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    9    0    0    8    0    0    1    0
     0    0    0]
 [   0    0   23   90    0    5    0    0    0    0  748    3    1    0
     0    5    0]
 [   0    1   16    0    0    1   13    0    2    0   14 2159    1    3
     0    0    0]
 [   0    0    0   21    0    0    0    0    0    2    7    3  493    0
     0    2    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    1    0    1    1    0    0
  1123    3    0]
 [   0    0    0    0    0    0    7    0    0   18    0    0    0    0
   119  203    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.44986449864498

F1 scores:
[       nan 0.9382716  0.95748031 0.89879058 1.         0.96715742
 0.96893491 0.98039216 0.99652375 0.27118644 0.90174804 0.9775866
 0.93548387 0.98930481 0.94132439 0.725      0.96551724]

Kappa:
0.936693473247477
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc2a8235a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.639, val_acc:0.480]
Epoch [2/120    avg_loss:2.283, val_acc:0.569]
Epoch [3/120    avg_loss:2.018, val_acc:0.577]
Epoch [4/120    avg_loss:1.799, val_acc:0.624]
Epoch [5/120    avg_loss:1.596, val_acc:0.596]
Epoch [6/120    avg_loss:1.359, val_acc:0.675]
Epoch [7/120    avg_loss:1.227, val_acc:0.682]
Epoch [8/120    avg_loss:1.097, val_acc:0.700]
Epoch [9/120    avg_loss:1.002, val_acc:0.729]
Epoch [10/120    avg_loss:0.913, val_acc:0.759]
Epoch [11/120    avg_loss:0.818, val_acc:0.753]
Epoch [12/120    avg_loss:0.729, val_acc:0.746]
Epoch [13/120    avg_loss:0.710, val_acc:0.770]
Epoch [14/120    avg_loss:0.638, val_acc:0.785]
Epoch [15/120    avg_loss:0.587, val_acc:0.810]
Epoch [16/120    avg_loss:0.509, val_acc:0.829]
Epoch [17/120    avg_loss:0.480, val_acc:0.834]
Epoch [18/120    avg_loss:0.461, val_acc:0.852]
Epoch [19/120    avg_loss:0.422, val_acc:0.850]
Epoch [20/120    avg_loss:0.428, val_acc:0.825]
Epoch [21/120    avg_loss:0.381, val_acc:0.860]
Epoch [22/120    avg_loss:0.347, val_acc:0.878]
Epoch [23/120    avg_loss:0.398, val_acc:0.891]
Epoch [24/120    avg_loss:0.327, val_acc:0.877]
Epoch [25/120    avg_loss:0.330, val_acc:0.863]
Epoch [26/120    avg_loss:0.286, val_acc:0.873]
Epoch [27/120    avg_loss:0.285, val_acc:0.898]
Epoch [28/120    avg_loss:0.258, val_acc:0.917]
Epoch [29/120    avg_loss:0.226, val_acc:0.915]
Epoch [30/120    avg_loss:0.238, val_acc:0.907]
Epoch [31/120    avg_loss:0.202, val_acc:0.917]
Epoch [32/120    avg_loss:0.176, val_acc:0.904]
Epoch [33/120    avg_loss:0.201, val_acc:0.898]
Epoch [34/120    avg_loss:0.241, val_acc:0.903]
Epoch [35/120    avg_loss:0.174, val_acc:0.872]
Epoch [36/120    avg_loss:0.157, val_acc:0.917]
Epoch [37/120    avg_loss:0.141, val_acc:0.932]
Epoch [38/120    avg_loss:0.184, val_acc:0.940]
Epoch [39/120    avg_loss:0.160, val_acc:0.939]
Epoch [40/120    avg_loss:0.109, val_acc:0.939]
Epoch [41/120    avg_loss:0.133, val_acc:0.941]
Epoch [42/120    avg_loss:0.112, val_acc:0.946]
Epoch [43/120    avg_loss:0.116, val_acc:0.941]
Epoch [44/120    avg_loss:0.117, val_acc:0.948]
Epoch [45/120    avg_loss:0.109, val_acc:0.948]
Epoch [46/120    avg_loss:0.087, val_acc:0.935]
Epoch [47/120    avg_loss:0.075, val_acc:0.934]
Epoch [48/120    avg_loss:0.111, val_acc:0.926]
Epoch [49/120    avg_loss:0.093, val_acc:0.951]
Epoch [50/120    avg_loss:0.092, val_acc:0.959]
Epoch [51/120    avg_loss:0.072, val_acc:0.945]
Epoch [52/120    avg_loss:0.069, val_acc:0.948]
Epoch [53/120    avg_loss:0.092, val_acc:0.959]
Epoch [54/120    avg_loss:0.066, val_acc:0.959]
Epoch [55/120    avg_loss:0.083, val_acc:0.962]
Epoch [56/120    avg_loss:0.076, val_acc:0.959]
Epoch [57/120    avg_loss:0.082, val_acc:0.956]
Epoch [58/120    avg_loss:0.068, val_acc:0.953]
Epoch [59/120    avg_loss:0.069, val_acc:0.959]
Epoch [60/120    avg_loss:0.055, val_acc:0.960]
Epoch [61/120    avg_loss:0.051, val_acc:0.961]
Epoch [62/120    avg_loss:0.044, val_acc:0.968]
Epoch [63/120    avg_loss:0.037, val_acc:0.963]
Epoch [64/120    avg_loss:0.047, val_acc:0.956]
Epoch [65/120    avg_loss:0.044, val_acc:0.964]
Epoch [66/120    avg_loss:0.036, val_acc:0.959]
Epoch [67/120    avg_loss:0.057, val_acc:0.942]
Epoch [68/120    avg_loss:0.055, val_acc:0.956]
Epoch [69/120    avg_loss:0.039, val_acc:0.973]
Epoch [70/120    avg_loss:0.037, val_acc:0.962]
Epoch [71/120    avg_loss:0.044, val_acc:0.963]
Epoch [72/120    avg_loss:0.043, val_acc:0.959]
Epoch [73/120    avg_loss:0.031, val_acc:0.969]
Epoch [74/120    avg_loss:0.039, val_acc:0.965]
Epoch [75/120    avg_loss:0.042, val_acc:0.948]
Epoch [76/120    avg_loss:0.051, val_acc:0.953]
Epoch [77/120    avg_loss:0.042, val_acc:0.948]
Epoch [78/120    avg_loss:0.048, val_acc:0.968]
Epoch [79/120    avg_loss:0.036, val_acc:0.973]
Epoch [80/120    avg_loss:0.033, val_acc:0.969]
Epoch [81/120    avg_loss:0.028, val_acc:0.968]
Epoch [82/120    avg_loss:0.042, val_acc:0.963]
Epoch [83/120    avg_loss:0.028, val_acc:0.977]
Epoch [84/120    avg_loss:0.040, val_acc:0.970]
Epoch [85/120    avg_loss:0.054, val_acc:0.963]
Epoch [86/120    avg_loss:0.059, val_acc:0.955]
Epoch [87/120    avg_loss:0.089, val_acc:0.956]
Epoch [88/120    avg_loss:0.043, val_acc:0.960]
Epoch [89/120    avg_loss:0.041, val_acc:0.963]
Epoch [90/120    avg_loss:0.030, val_acc:0.971]
Epoch [91/120    avg_loss:0.038, val_acc:0.970]
Epoch [92/120    avg_loss:0.051, val_acc:0.968]
Epoch [93/120    avg_loss:0.035, val_acc:0.974]
Epoch [94/120    avg_loss:0.032, val_acc:0.942]
Epoch [95/120    avg_loss:0.053, val_acc:0.962]
Epoch [96/120    avg_loss:0.049, val_acc:0.963]
Epoch [97/120    avg_loss:0.029, val_acc:0.975]
Epoch [98/120    avg_loss:0.027, val_acc:0.980]
Epoch [99/120    avg_loss:0.020, val_acc:0.980]
Epoch [100/120    avg_loss:0.015, val_acc:0.981]
Epoch [101/120    avg_loss:0.014, val_acc:0.982]
Epoch [102/120    avg_loss:0.017, val_acc:0.981]
Epoch [103/120    avg_loss:0.016, val_acc:0.979]
Epoch [104/120    avg_loss:0.013, val_acc:0.980]
Epoch [105/120    avg_loss:0.016, val_acc:0.982]
Epoch [106/120    avg_loss:0.011, val_acc:0.981]
Epoch [107/120    avg_loss:0.014, val_acc:0.980]
Epoch [108/120    avg_loss:0.016, val_acc:0.979]
Epoch [109/120    avg_loss:0.011, val_acc:0.979]
Epoch [110/120    avg_loss:0.014, val_acc:0.978]
Epoch [111/120    avg_loss:0.018, val_acc:0.979]
Epoch [112/120    avg_loss:0.016, val_acc:0.983]
Epoch [113/120    avg_loss:0.018, val_acc:0.984]
Epoch [114/120    avg_loss:0.016, val_acc:0.982]
Epoch [115/120    avg_loss:0.019, val_acc:0.987]
Epoch [116/120    avg_loss:0.013, val_acc:0.985]
Epoch [117/120    avg_loss:0.015, val_acc:0.983]
Epoch [118/120    avg_loss:0.017, val_acc:0.981]
Epoch [119/120    avg_loss:0.014, val_acc:0.983]
Epoch [120/120    avg_loss:0.016, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1241    5    7    0    0    0    0    3    5   16    7    0
     0    1    0]
 [   0    0    0  724    7   12    0    0    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    0    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    7    0    0    0    0    0    0  423    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    1   69    0   10    5    0    0    2  775   10    0    0
     0    3    0]
 [   0    0   11    0    0    3    1    0    0    1   12 2173    7    1
     0    0    1]
 [   0    0    0   28    5    0    0    1    0    0   15    3  477    0
     0    4    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0    5    0    0    1    0    0    0    0
   161  180    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.1219512195122

F1 scores:
[       nan 0.92134831 0.97793538 0.92053401 0.95730337 0.96396396
 0.98716981 0.98039216 0.99063232 0.68181818 0.921522   0.9850408
 0.92982456 0.99730458 0.92973856 0.6728972  0.98224852]

Kappa:
0.9443306322598571
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feaf90ff6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.658, val_acc:0.496]
Epoch [2/120    avg_loss:2.303, val_acc:0.506]
Epoch [3/120    avg_loss:2.032, val_acc:0.593]
Epoch [4/120    avg_loss:1.856, val_acc:0.588]
Epoch [5/120    avg_loss:1.674, val_acc:0.632]
Epoch [6/120    avg_loss:1.526, val_acc:0.643]
Epoch [7/120    avg_loss:1.338, val_acc:0.654]
Epoch [8/120    avg_loss:1.240, val_acc:0.711]
Epoch [9/120    avg_loss:1.083, val_acc:0.700]
Epoch [10/120    avg_loss:0.949, val_acc:0.696]
Epoch [11/120    avg_loss:0.878, val_acc:0.738]
Epoch [12/120    avg_loss:0.825, val_acc:0.758]
Epoch [13/120    avg_loss:0.738, val_acc:0.781]
Epoch [14/120    avg_loss:0.598, val_acc:0.775]
Epoch [15/120    avg_loss:0.676, val_acc:0.779]
Epoch [16/120    avg_loss:0.533, val_acc:0.852]
Epoch [17/120    avg_loss:0.505, val_acc:0.825]
Epoch [18/120    avg_loss:0.536, val_acc:0.830]
Epoch [19/120    avg_loss:0.487, val_acc:0.860]
Epoch [20/120    avg_loss:0.413, val_acc:0.869]
Epoch [21/120    avg_loss:0.351, val_acc:0.895]
Epoch [22/120    avg_loss:0.500, val_acc:0.847]
Epoch [23/120    avg_loss:0.406, val_acc:0.842]
Epoch [24/120    avg_loss:0.346, val_acc:0.878]
Epoch [25/120    avg_loss:0.376, val_acc:0.876]
Epoch [26/120    avg_loss:0.441, val_acc:0.831]
Epoch [27/120    avg_loss:0.338, val_acc:0.846]
Epoch [28/120    avg_loss:0.274, val_acc:0.916]
Epoch [29/120    avg_loss:0.283, val_acc:0.877]
Epoch [30/120    avg_loss:0.246, val_acc:0.922]
Epoch [31/120    avg_loss:0.225, val_acc:0.917]
Epoch [32/120    avg_loss:0.248, val_acc:0.931]
Epoch [33/120    avg_loss:0.217, val_acc:0.945]
Epoch [34/120    avg_loss:0.198, val_acc:0.932]
Epoch [35/120    avg_loss:0.173, val_acc:0.936]
Epoch [36/120    avg_loss:0.239, val_acc:0.944]
Epoch [37/120    avg_loss:0.170, val_acc:0.941]
Epoch [38/120    avg_loss:0.159, val_acc:0.943]
Epoch [39/120    avg_loss:0.151, val_acc:0.940]
Epoch [40/120    avg_loss:0.110, val_acc:0.948]
Epoch [41/120    avg_loss:0.117, val_acc:0.949]
Epoch [42/120    avg_loss:0.125, val_acc:0.933]
Epoch [43/120    avg_loss:0.337, val_acc:0.940]
Epoch [44/120    avg_loss:0.151, val_acc:0.954]
Epoch [45/120    avg_loss:0.215, val_acc:0.941]
Epoch [46/120    avg_loss:0.124, val_acc:0.955]
Epoch [47/120    avg_loss:0.144, val_acc:0.944]
Epoch [48/120    avg_loss:0.139, val_acc:0.890]
Epoch [49/120    avg_loss:0.155, val_acc:0.949]
Epoch [50/120    avg_loss:0.097, val_acc:0.951]
Epoch [51/120    avg_loss:0.093, val_acc:0.958]
Epoch [52/120    avg_loss:0.088, val_acc:0.953]
Epoch [53/120    avg_loss:0.082, val_acc:0.955]
Epoch [54/120    avg_loss:0.087, val_acc:0.960]
Epoch [55/120    avg_loss:0.068, val_acc:0.964]
Epoch [56/120    avg_loss:0.113, val_acc:0.930]
Epoch [57/120    avg_loss:0.095, val_acc:0.953]
Epoch [58/120    avg_loss:0.109, val_acc:0.945]
Epoch [59/120    avg_loss:0.088, val_acc:0.961]
Epoch [60/120    avg_loss:0.063, val_acc:0.978]
Epoch [61/120    avg_loss:0.060, val_acc:0.969]
Epoch [62/120    avg_loss:0.055, val_acc:0.969]
Epoch [63/120    avg_loss:0.054, val_acc:0.971]
Epoch [64/120    avg_loss:0.045, val_acc:0.968]
Epoch [65/120    avg_loss:0.059, val_acc:0.960]
Epoch [66/120    avg_loss:0.047, val_acc:0.974]
Epoch [67/120    avg_loss:0.040, val_acc:0.972]
Epoch [68/120    avg_loss:0.045, val_acc:0.969]
Epoch [69/120    avg_loss:0.050, val_acc:0.969]
Epoch [70/120    avg_loss:0.058, val_acc:0.970]
Epoch [71/120    avg_loss:0.061, val_acc:0.978]
Epoch [72/120    avg_loss:0.049, val_acc:0.962]
Epoch [73/120    avg_loss:0.058, val_acc:0.977]
Epoch [74/120    avg_loss:0.037, val_acc:0.974]
Epoch [75/120    avg_loss:0.054, val_acc:0.893]
Epoch [76/120    avg_loss:0.049, val_acc:0.971]
Epoch [77/120    avg_loss:0.038, val_acc:0.974]
Epoch [78/120    avg_loss:0.028, val_acc:0.982]
Epoch [79/120    avg_loss:0.025, val_acc:0.977]
Epoch [80/120    avg_loss:0.034, val_acc:0.978]
Epoch [81/120    avg_loss:0.032, val_acc:0.977]
Epoch [82/120    avg_loss:0.040, val_acc:0.978]
Epoch [83/120    avg_loss:0.039, val_acc:0.982]
Epoch [84/120    avg_loss:0.027, val_acc:0.984]
Epoch [85/120    avg_loss:0.028, val_acc:0.979]
Epoch [86/120    avg_loss:0.027, val_acc:0.975]
Epoch [87/120    avg_loss:0.021, val_acc:0.979]
Epoch [88/120    avg_loss:0.041, val_acc:0.964]
Epoch [89/120    avg_loss:0.022, val_acc:0.981]
Epoch [90/120    avg_loss:0.020, val_acc:0.970]
Epoch [91/120    avg_loss:0.038, val_acc:0.965]
Epoch [92/120    avg_loss:0.025, val_acc:0.980]
Epoch [93/120    avg_loss:0.026, val_acc:0.979]
Epoch [94/120    avg_loss:0.063, val_acc:0.923]
Epoch [95/120    avg_loss:0.086, val_acc:0.973]
Epoch [96/120    avg_loss:0.044, val_acc:0.973]
Epoch [97/120    avg_loss:0.060, val_acc:0.531]
Epoch [98/120    avg_loss:0.085, val_acc:0.975]
Epoch [99/120    avg_loss:0.030, val_acc:0.980]
Epoch [100/120    avg_loss:0.025, val_acc:0.981]
Epoch [101/120    avg_loss:0.027, val_acc:0.982]
Epoch [102/120    avg_loss:0.023, val_acc:0.981]
Epoch [103/120    avg_loss:0.019, val_acc:0.981]
Epoch [104/120    avg_loss:0.018, val_acc:0.981]
Epoch [105/120    avg_loss:0.018, val_acc:0.980]
Epoch [106/120    avg_loss:0.019, val_acc:0.981]
Epoch [107/120    avg_loss:0.013, val_acc:0.981]
Epoch [108/120    avg_loss:0.019, val_acc:0.980]
Epoch [109/120    avg_loss:0.014, val_acc:0.980]
Epoch [110/120    avg_loss:0.021, val_acc:0.981]
Epoch [111/120    avg_loss:0.014, val_acc:0.982]
Epoch [112/120    avg_loss:0.015, val_acc:0.982]
Epoch [113/120    avg_loss:0.014, val_acc:0.982]
Epoch [114/120    avg_loss:0.015, val_acc:0.982]
Epoch [115/120    avg_loss:0.016, val_acc:0.982]
Epoch [116/120    avg_loss:0.017, val_acc:0.982]
Epoch [117/120    avg_loss:0.016, val_acc:0.982]
Epoch [118/120    avg_loss:0.014, val_acc:0.982]
Epoch [119/120    avg_loss:0.014, val_acc:0.982]
Epoch [120/120    avg_loss:0.016, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1266    1    0    0    5    0    0    0    2   10    0    0
     1    0    0]
 [   0    0    3  733    0    0    0    0    0    8    0    0    0    0
     3    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    0    0    0    0    0    0    0
    12    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    2    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2   76    0    5    0    0    0    0  781    3    0    1
     7    0    0]
 [   0    0    2    0    0    2    0    0    0    0    1 2189   12    1
     3    0    0]
 [   0    0    1   10    1   14    0    0    0    0    0    0  498    0
     1    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0   13    0    0    1    0    0    0    1
   124  208    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    7    0
     0    0   77]]

Accuracy:
96.15176151761517

F1 scores:
[       nan 0.975      0.989449   0.93435309 0.99294118 0.96245734
 0.98266767 1.         0.99883856 0.8        0.93870192 0.99184413
 0.94766889 0.9919571  0.93492586 0.74954955 0.90588235]

Kappa:
0.9560792824558764
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9d15247b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.679, val_acc:0.437]
Epoch [2/120    avg_loss:2.338, val_acc:0.505]
Epoch [3/120    avg_loss:2.078, val_acc:0.532]
Epoch [4/120    avg_loss:1.830, val_acc:0.560]
Epoch [5/120    avg_loss:1.674, val_acc:0.609]
Epoch [6/120    avg_loss:1.490, val_acc:0.637]
Epoch [7/120    avg_loss:1.346, val_acc:0.662]
Epoch [8/120    avg_loss:1.207, val_acc:0.690]
Epoch [9/120    avg_loss:1.061, val_acc:0.747]
Epoch [10/120    avg_loss:0.968, val_acc:0.741]
Epoch [11/120    avg_loss:0.856, val_acc:0.743]
Epoch [12/120    avg_loss:0.788, val_acc:0.787]
Epoch [13/120    avg_loss:0.679, val_acc:0.765]
Epoch [14/120    avg_loss:0.643, val_acc:0.796]
Epoch [15/120    avg_loss:0.555, val_acc:0.823]
Epoch [16/120    avg_loss:0.530, val_acc:0.846]
Epoch [17/120    avg_loss:0.527, val_acc:0.858]
Epoch [18/120    avg_loss:0.462, val_acc:0.857]
Epoch [19/120    avg_loss:0.447, val_acc:0.859]
Epoch [20/120    avg_loss:0.441, val_acc:0.855]
Epoch [21/120    avg_loss:0.537, val_acc:0.840]
Epoch [22/120    avg_loss:0.484, val_acc:0.896]
Epoch [23/120    avg_loss:0.337, val_acc:0.898]
Epoch [24/120    avg_loss:0.354, val_acc:0.840]
Epoch [25/120    avg_loss:0.387, val_acc:0.883]
Epoch [26/120    avg_loss:0.304, val_acc:0.908]
Epoch [27/120    avg_loss:0.327, val_acc:0.910]
Epoch [28/120    avg_loss:0.243, val_acc:0.912]
Epoch [29/120    avg_loss:0.251, val_acc:0.901]
Epoch [30/120    avg_loss:0.181, val_acc:0.908]
Epoch [31/120    avg_loss:0.258, val_acc:0.874]
Epoch [32/120    avg_loss:0.264, val_acc:0.912]
Epoch [33/120    avg_loss:0.202, val_acc:0.929]
Epoch [34/120    avg_loss:0.162, val_acc:0.934]
Epoch [35/120    avg_loss:0.180, val_acc:0.921]
Epoch [36/120    avg_loss:0.181, val_acc:0.930]
Epoch [37/120    avg_loss:0.142, val_acc:0.939]
Epoch [38/120    avg_loss:0.141, val_acc:0.954]
Epoch [39/120    avg_loss:0.118, val_acc:0.929]
Epoch [40/120    avg_loss:0.139, val_acc:0.968]
Epoch [41/120    avg_loss:0.115, val_acc:0.955]
Epoch [42/120    avg_loss:0.094, val_acc:0.929]
Epoch [43/120    avg_loss:0.093, val_acc:0.964]
Epoch [44/120    avg_loss:0.093, val_acc:0.961]
Epoch [45/120    avg_loss:0.086, val_acc:0.954]
Epoch [46/120    avg_loss:0.090, val_acc:0.973]
Epoch [47/120    avg_loss:0.073, val_acc:0.962]
Epoch [48/120    avg_loss:0.106, val_acc:0.934]
Epoch [49/120    avg_loss:0.092, val_acc:0.948]
Epoch [50/120    avg_loss:0.084, val_acc:0.940]
Epoch [51/120    avg_loss:0.069, val_acc:0.965]
Epoch [52/120    avg_loss:0.060, val_acc:0.970]
Epoch [53/120    avg_loss:0.062, val_acc:0.972]
Epoch [54/120    avg_loss:0.069, val_acc:0.971]
Epoch [55/120    avg_loss:0.064, val_acc:0.970]
Epoch [56/120    avg_loss:0.052, val_acc:0.975]
Epoch [57/120    avg_loss:0.075, val_acc:0.958]
Epoch [58/120    avg_loss:0.062, val_acc:0.949]
Epoch [59/120    avg_loss:0.047, val_acc:0.972]
Epoch [60/120    avg_loss:0.055, val_acc:0.972]
Epoch [61/120    avg_loss:0.058, val_acc:0.975]
Epoch [62/120    avg_loss:0.039, val_acc:0.974]
Epoch [63/120    avg_loss:0.046, val_acc:0.954]
Epoch [64/120    avg_loss:0.065, val_acc:0.972]
Epoch [65/120    avg_loss:0.055, val_acc:0.959]
Epoch [66/120    avg_loss:0.048, val_acc:0.980]
Epoch [67/120    avg_loss:0.046, val_acc:0.977]
Epoch [68/120    avg_loss:1.054, val_acc:0.661]
Epoch [69/120    avg_loss:0.912, val_acc:0.788]
Epoch [70/120    avg_loss:0.602, val_acc:0.858]
Epoch [71/120    avg_loss:0.409, val_acc:0.880]
Epoch [72/120    avg_loss:0.265, val_acc:0.911]
Epoch [73/120    avg_loss:0.202, val_acc:0.921]
Epoch [74/120    avg_loss:0.131, val_acc:0.938]
Epoch [75/120    avg_loss:0.119, val_acc:0.956]
Epoch [76/120    avg_loss:0.097, val_acc:0.947]
Epoch [77/120    avg_loss:0.089, val_acc:0.958]
Epoch [78/120    avg_loss:0.065, val_acc:0.970]
Epoch [79/120    avg_loss:0.054, val_acc:0.971]
Epoch [80/120    avg_loss:0.036, val_acc:0.977]
Epoch [81/120    avg_loss:0.031, val_acc:0.976]
Epoch [82/120    avg_loss:0.030, val_acc:0.976]
Epoch [83/120    avg_loss:0.030, val_acc:0.979]
Epoch [84/120    avg_loss:0.036, val_acc:0.980]
Epoch [85/120    avg_loss:0.027, val_acc:0.979]
Epoch [86/120    avg_loss:0.027, val_acc:0.977]
Epoch [87/120    avg_loss:0.029, val_acc:0.976]
Epoch [88/120    avg_loss:0.024, val_acc:0.981]
Epoch [89/120    avg_loss:0.030, val_acc:0.979]
Epoch [90/120    avg_loss:0.027, val_acc:0.980]
Epoch [91/120    avg_loss:0.025, val_acc:0.980]
Epoch [92/120    avg_loss:0.025, val_acc:0.976]
Epoch [93/120    avg_loss:0.026, val_acc:0.977]
Epoch [94/120    avg_loss:0.024, val_acc:0.977]
Epoch [95/120    avg_loss:0.029, val_acc:0.977]
Epoch [96/120    avg_loss:0.023, val_acc:0.982]
Epoch [97/120    avg_loss:0.021, val_acc:0.981]
Epoch [98/120    avg_loss:0.032, val_acc:0.979]
Epoch [99/120    avg_loss:0.024, val_acc:0.980]
Epoch [100/120    avg_loss:0.023, val_acc:0.980]
Epoch [101/120    avg_loss:0.020, val_acc:0.981]
Epoch [102/120    avg_loss:0.017, val_acc:0.982]
Epoch [103/120    avg_loss:0.025, val_acc:0.982]
Epoch [104/120    avg_loss:0.023, val_acc:0.980]
Epoch [105/120    avg_loss:0.017, val_acc:0.981]
Epoch [106/120    avg_loss:0.020, val_acc:0.979]
Epoch [107/120    avg_loss:0.020, val_acc:0.981]
Epoch [108/120    avg_loss:0.020, val_acc:0.980]
Epoch [109/120    avg_loss:0.019, val_acc:0.980]
Epoch [110/120    avg_loss:0.021, val_acc:0.980]
Epoch [111/120    avg_loss:0.023, val_acc:0.981]
Epoch [112/120    avg_loss:0.018, val_acc:0.981]
Epoch [113/120    avg_loss:0.020, val_acc:0.980]
Epoch [114/120    avg_loss:0.021, val_acc:0.981]
Epoch [115/120    avg_loss:0.017, val_acc:0.981]
Epoch [116/120    avg_loss:0.016, val_acc:0.981]
Epoch [117/120    avg_loss:0.020, val_acc:0.981]
Epoch [118/120    avg_loss:0.019, val_acc:0.983]
Epoch [119/120    avg_loss:0.018, val_acc:0.983]
Epoch [120/120    avg_loss:0.018, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    1 1239    4    0    0    4    0    0    3   17   17    0    0
     0    0    0]
 [   0    0    0  719    3    8    0    0    0    7    0    0   10    0
     0    0    0]
 [   0    0    0    7  204    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10   29    0    4    0    0    0    0  762   64    1    0
     3    2    0]
 [   0    0   10    0    0    1    5    0    0    0    2 2185    6    0
     1    0    0]
 [   0    0    0    3    0    7    0    0    0    0    8    5  507    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    0    0    1    0    1    1    0    0
  1127    0    0]
 [   0    0    0    0    0    0    0    0    0    2    0    0    0    0
   149  196    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.47967479674797

F1 scores:
[       nan 0.93670886 0.9740566  0.95294897 0.97142857 0.96659243
 0.99319728 1.         0.99883856 0.75       0.91312163 0.97501116
 0.95570217 1.         0.93140496 0.71926606 0.97076023]

Kappa:
0.9483480555233043
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f480838aa58>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.668, val_acc:0.340]
Epoch [2/120    avg_loss:2.317, val_acc:0.506]
Epoch [3/120    avg_loss:2.061, val_acc:0.500]
Epoch [4/120    avg_loss:1.878, val_acc:0.571]
Epoch [5/120    avg_loss:1.687, val_acc:0.606]
Epoch [6/120    avg_loss:1.556, val_acc:0.606]
Epoch [7/120    avg_loss:1.421, val_acc:0.631]
Epoch [8/120    avg_loss:1.255, val_acc:0.631]
Epoch [9/120    avg_loss:1.155, val_acc:0.723]
Epoch [10/120    avg_loss:1.044, val_acc:0.693]
Epoch [11/120    avg_loss:0.886, val_acc:0.731]
Epoch [12/120    avg_loss:0.826, val_acc:0.753]
Epoch [13/120    avg_loss:0.694, val_acc:0.775]
Epoch [14/120    avg_loss:0.674, val_acc:0.782]
Epoch [15/120    avg_loss:0.672, val_acc:0.787]
Epoch [16/120    avg_loss:0.674, val_acc:0.780]
Epoch [17/120    avg_loss:0.560, val_acc:0.814]
Epoch [18/120    avg_loss:0.449, val_acc:0.849]
Epoch [19/120    avg_loss:0.457, val_acc:0.816]
Epoch [20/120    avg_loss:0.474, val_acc:0.804]
Epoch [21/120    avg_loss:0.443, val_acc:0.843]
Epoch [22/120    avg_loss:0.404, val_acc:0.841]
Epoch [23/120    avg_loss:0.351, val_acc:0.859]
Epoch [24/120    avg_loss:0.328, val_acc:0.864]
Epoch [25/120    avg_loss:0.314, val_acc:0.830]
Epoch [26/120    avg_loss:0.306, val_acc:0.883]
Epoch [27/120    avg_loss:0.409, val_acc:0.856]
Epoch [28/120    avg_loss:0.301, val_acc:0.859]
Epoch [29/120    avg_loss:0.320, val_acc:0.887]
Epoch [30/120    avg_loss:0.251, val_acc:0.900]
Epoch [31/120    avg_loss:0.279, val_acc:0.843]
Epoch [32/120    avg_loss:0.257, val_acc:0.901]
Epoch [33/120    avg_loss:0.203, val_acc:0.891]
Epoch [34/120    avg_loss:0.198, val_acc:0.887]
Epoch [35/120    avg_loss:0.206, val_acc:0.910]
Epoch [36/120    avg_loss:0.207, val_acc:0.901]
Epoch [37/120    avg_loss:0.260, val_acc:0.883]
Epoch [38/120    avg_loss:0.215, val_acc:0.909]
Epoch [39/120    avg_loss:0.212, val_acc:0.896]
Epoch [40/120    avg_loss:0.240, val_acc:0.889]
Epoch [41/120    avg_loss:0.188, val_acc:0.904]
Epoch [42/120    avg_loss:0.203, val_acc:0.903]
Epoch [43/120    avg_loss:0.163, val_acc:0.905]
Epoch [44/120    avg_loss:0.149, val_acc:0.898]
Epoch [45/120    avg_loss:0.198, val_acc:0.910]
Epoch [46/120    avg_loss:0.149, val_acc:0.909]
Epoch [47/120    avg_loss:0.121, val_acc:0.938]
Epoch [48/120    avg_loss:0.115, val_acc:0.926]
Epoch [49/120    avg_loss:0.131, val_acc:0.898]
Epoch [50/120    avg_loss:0.130, val_acc:0.905]
Epoch [51/120    avg_loss:0.093, val_acc:0.913]
Epoch [52/120    avg_loss:0.104, val_acc:0.937]
Epoch [53/120    avg_loss:0.103, val_acc:0.939]
Epoch [54/120    avg_loss:0.096, val_acc:0.929]
Epoch [55/120    avg_loss:0.091, val_acc:0.938]
Epoch [56/120    avg_loss:0.080, val_acc:0.934]
Epoch [57/120    avg_loss:0.080, val_acc:0.918]
Epoch [58/120    avg_loss:0.074, val_acc:0.940]
Epoch [59/120    avg_loss:0.077, val_acc:0.939]
Epoch [60/120    avg_loss:0.074, val_acc:0.934]
Epoch [61/120    avg_loss:0.078, val_acc:0.934]
Epoch [62/120    avg_loss:0.059, val_acc:0.949]
Epoch [63/120    avg_loss:0.044, val_acc:0.944]
Epoch [64/120    avg_loss:0.582, val_acc:0.617]
Epoch [65/120    avg_loss:1.056, val_acc:0.820]
Epoch [66/120    avg_loss:0.530, val_acc:0.825]
Epoch [67/120    avg_loss:0.334, val_acc:0.894]
Epoch [68/120    avg_loss:0.161, val_acc:0.919]
Epoch [69/120    avg_loss:0.178, val_acc:0.903]
Epoch [70/120    avg_loss:0.113, val_acc:0.928]
Epoch [71/120    avg_loss:0.099, val_acc:0.936]
Epoch [72/120    avg_loss:0.092, val_acc:0.931]
Epoch [73/120    avg_loss:0.080, val_acc:0.941]
Epoch [74/120    avg_loss:0.072, val_acc:0.940]
Epoch [75/120    avg_loss:0.074, val_acc:0.947]
Epoch [76/120    avg_loss:0.053, val_acc:0.954]
Epoch [77/120    avg_loss:0.062, val_acc:0.958]
Epoch [78/120    avg_loss:0.037, val_acc:0.959]
Epoch [79/120    avg_loss:0.044, val_acc:0.959]
Epoch [80/120    avg_loss:0.035, val_acc:0.958]
Epoch [81/120    avg_loss:0.036, val_acc:0.964]
Epoch [82/120    avg_loss:0.040, val_acc:0.962]
Epoch [83/120    avg_loss:0.041, val_acc:0.963]
Epoch [84/120    avg_loss:0.038, val_acc:0.963]
Epoch [85/120    avg_loss:0.027, val_acc:0.963]
Epoch [86/120    avg_loss:0.037, val_acc:0.958]
Epoch [87/120    avg_loss:0.030, val_acc:0.961]
Epoch [88/120    avg_loss:0.031, val_acc:0.961]
Epoch [89/120    avg_loss:0.030, val_acc:0.961]
Epoch [90/120    avg_loss:0.033, val_acc:0.961]
Epoch [91/120    avg_loss:0.030, val_acc:0.962]
Epoch [92/120    avg_loss:0.036, val_acc:0.963]
Epoch [93/120    avg_loss:0.035, val_acc:0.962]
Epoch [94/120    avg_loss:0.038, val_acc:0.962]
Epoch [95/120    avg_loss:0.035, val_acc:0.962]
Epoch [96/120    avg_loss:0.025, val_acc:0.962]
Epoch [97/120    avg_loss:0.035, val_acc:0.962]
Epoch [98/120    avg_loss:0.032, val_acc:0.962]
Epoch [99/120    avg_loss:0.037, val_acc:0.963]
Epoch [100/120    avg_loss:0.028, val_acc:0.962]
Epoch [101/120    avg_loss:0.032, val_acc:0.962]
Epoch [102/120    avg_loss:0.027, val_acc:0.963]
Epoch [103/120    avg_loss:0.031, val_acc:0.964]
Epoch [104/120    avg_loss:0.030, val_acc:0.964]
Epoch [105/120    avg_loss:0.029, val_acc:0.962]
Epoch [106/120    avg_loss:0.022, val_acc:0.963]
Epoch [107/120    avg_loss:0.027, val_acc:0.963]
Epoch [108/120    avg_loss:0.031, val_acc:0.962]
Epoch [109/120    avg_loss:0.031, val_acc:0.962]
Epoch [110/120    avg_loss:0.033, val_acc:0.961]
Epoch [111/120    avg_loss:0.030, val_acc:0.962]
Epoch [112/120    avg_loss:0.027, val_acc:0.962]
Epoch [113/120    avg_loss:0.031, val_acc:0.962]
Epoch [114/120    avg_loss:0.030, val_acc:0.961]
Epoch [115/120    avg_loss:0.031, val_acc:0.962]
Epoch [116/120    avg_loss:0.025, val_acc:0.963]
Epoch [117/120    avg_loss:0.035, val_acc:0.962]
Epoch [118/120    avg_loss:0.029, val_acc:0.962]
Epoch [119/120    avg_loss:0.033, val_acc:0.962]
Epoch [120/120    avg_loss:0.029, val_acc:0.962]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1217    1    7    0    1    0    0    0    9   36   11    0
     1    0    2]
 [   0    0    3  712    2   21    0    0    0    3    0    0    1    3
     2    0    0]
 [   0    0    0    4  209    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   35   58    0    8    1    0    0    0  730   32    1    7
     0    3    0]
 [   0    0    4    1    0    2    6    0    4    0   10 2164   17    1
     1    0    0]
 [   0    0    5   12    1    3    0    0    0    0    2   10  497    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    3    0    3    4    0    0
  1129    0    0]
 [   0    0    0    0    0    0   63    0    0    0    0    0    0    0
    69  214    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.74254742547426

F1 scores:
[       nan 1.         0.95488427 0.92648016 0.96759259 0.95893452
 0.9472162  0.98039216 0.99192618 0.82051282 0.89625537 0.97105676
 0.93685203 0.97112861 0.96413322 0.75886525 0.96      ]

Kappa:
0.9400032899377748
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8299073a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.663, val_acc:0.472]
Epoch [2/120    avg_loss:2.311, val_acc:0.512]
Epoch [3/120    avg_loss:2.035, val_acc:0.579]
Epoch [4/120    avg_loss:1.863, val_acc:0.602]
Epoch [5/120    avg_loss:1.678, val_acc:0.610]
Epoch [6/120    avg_loss:1.510, val_acc:0.644]
Epoch [7/120    avg_loss:1.325, val_acc:0.698]
Epoch [8/120    avg_loss:1.159, val_acc:0.682]
Epoch [9/120    avg_loss:1.075, val_acc:0.719]
Epoch [10/120    avg_loss:0.925, val_acc:0.738]
Epoch [11/120    avg_loss:0.862, val_acc:0.748]
Epoch [12/120    avg_loss:0.752, val_acc:0.768]
Epoch [13/120    avg_loss:0.654, val_acc:0.794]
Epoch [14/120    avg_loss:0.600, val_acc:0.810]
Epoch [15/120    avg_loss:0.543, val_acc:0.831]
Epoch [16/120    avg_loss:0.501, val_acc:0.783]
Epoch [17/120    avg_loss:0.504, val_acc:0.842]
Epoch [18/120    avg_loss:0.400, val_acc:0.831]
Epoch [19/120    avg_loss:0.433, val_acc:0.843]
Epoch [20/120    avg_loss:0.422, val_acc:0.862]
Epoch [21/120    avg_loss:0.383, val_acc:0.860]
Epoch [22/120    avg_loss:0.357, val_acc:0.871]
Epoch [23/120    avg_loss:0.380, val_acc:0.817]
Epoch [24/120    avg_loss:0.335, val_acc:0.865]
Epoch [25/120    avg_loss:0.283, val_acc:0.892]
Epoch [26/120    avg_loss:0.286, val_acc:0.920]
Epoch [27/120    avg_loss:0.252, val_acc:0.894]
Epoch [28/120    avg_loss:0.255, val_acc:0.892]
Epoch [29/120    avg_loss:0.200, val_acc:0.901]
Epoch [30/120    avg_loss:0.202, val_acc:0.866]
Epoch [31/120    avg_loss:0.264, val_acc:0.911]
Epoch [32/120    avg_loss:0.165, val_acc:0.911]
Epoch [33/120    avg_loss:0.168, val_acc:0.913]
Epoch [34/120    avg_loss:0.145, val_acc:0.925]
Epoch [35/120    avg_loss:0.122, val_acc:0.942]
Epoch [36/120    avg_loss:0.212, val_acc:0.926]
Epoch [37/120    avg_loss:0.143, val_acc:0.929]
Epoch [38/120    avg_loss:0.126, val_acc:0.925]
Epoch [39/120    avg_loss:0.207, val_acc:0.921]
Epoch [40/120    avg_loss:0.124, val_acc:0.941]
Epoch [41/120    avg_loss:0.109, val_acc:0.949]
Epoch [42/120    avg_loss:0.102, val_acc:0.949]
Epoch [43/120    avg_loss:0.107, val_acc:0.941]
Epoch [44/120    avg_loss:0.121, val_acc:0.925]
Epoch [45/120    avg_loss:0.127, val_acc:0.925]
Epoch [46/120    avg_loss:0.077, val_acc:0.945]
Epoch [47/120    avg_loss:0.081, val_acc:0.944]
Epoch [48/120    avg_loss:0.066, val_acc:0.925]
Epoch [49/120    avg_loss:0.071, val_acc:0.943]
Epoch [50/120    avg_loss:0.061, val_acc:0.964]
Epoch [51/120    avg_loss:0.052, val_acc:0.971]
Epoch [52/120    avg_loss:0.092, val_acc:0.954]
Epoch [53/120    avg_loss:0.112, val_acc:0.961]
Epoch [54/120    avg_loss:0.082, val_acc:0.958]
Epoch [55/120    avg_loss:0.080, val_acc:0.940]
Epoch [56/120    avg_loss:0.091, val_acc:0.951]
Epoch [57/120    avg_loss:0.067, val_acc:0.954]
Epoch [58/120    avg_loss:0.055, val_acc:0.961]
Epoch [59/120    avg_loss:0.092, val_acc:0.964]
Epoch [60/120    avg_loss:0.051, val_acc:0.935]
Epoch [61/120    avg_loss:0.059, val_acc:0.967]
Epoch [62/120    avg_loss:0.063, val_acc:0.962]
Epoch [63/120    avg_loss:0.124, val_acc:0.931]
Epoch [64/120    avg_loss:0.082, val_acc:0.952]
Epoch [65/120    avg_loss:0.050, val_acc:0.970]
Epoch [66/120    avg_loss:0.037, val_acc:0.968]
Epoch [67/120    avg_loss:0.031, val_acc:0.969]
Epoch [68/120    avg_loss:0.031, val_acc:0.968]
Epoch [69/120    avg_loss:0.033, val_acc:0.968]
Epoch [70/120    avg_loss:0.031, val_acc:0.969]
Epoch [71/120    avg_loss:0.029, val_acc:0.965]
Epoch [72/120    avg_loss:0.033, val_acc:0.967]
Epoch [73/120    avg_loss:0.032, val_acc:0.968]
Epoch [74/120    avg_loss:0.030, val_acc:0.968]
Epoch [75/120    avg_loss:0.030, val_acc:0.968]
Epoch [76/120    avg_loss:0.032, val_acc:0.971]
Epoch [77/120    avg_loss:0.024, val_acc:0.969]
Epoch [78/120    avg_loss:0.023, val_acc:0.965]
Epoch [79/120    avg_loss:0.034, val_acc:0.972]
Epoch [80/120    avg_loss:0.023, val_acc:0.971]
Epoch [81/120    avg_loss:0.028, val_acc:0.973]
Epoch [82/120    avg_loss:0.025, val_acc:0.970]
Epoch [83/120    avg_loss:0.027, val_acc:0.970]
Epoch [84/120    avg_loss:0.026, val_acc:0.969]
Epoch [85/120    avg_loss:0.024, val_acc:0.969]
Epoch [86/120    avg_loss:0.019, val_acc:0.968]
Epoch [87/120    avg_loss:0.023, val_acc:0.973]
Epoch [88/120    avg_loss:0.023, val_acc:0.970]
Epoch [89/120    avg_loss:0.020, val_acc:0.971]
Epoch [90/120    avg_loss:0.023, val_acc:0.970]
Epoch [91/120    avg_loss:0.021, val_acc:0.970]
Epoch [92/120    avg_loss:0.022, val_acc:0.969]
Epoch [93/120    avg_loss:0.026, val_acc:0.972]
Epoch [94/120    avg_loss:0.020, val_acc:0.971]
Epoch [95/120    avg_loss:0.023, val_acc:0.971]
Epoch [96/120    avg_loss:0.022, val_acc:0.971]
Epoch [97/120    avg_loss:0.020, val_acc:0.971]
Epoch [98/120    avg_loss:0.023, val_acc:0.971]
Epoch [99/120    avg_loss:0.017, val_acc:0.971]
Epoch [100/120    avg_loss:0.019, val_acc:0.971]
Epoch [101/120    avg_loss:0.026, val_acc:0.971]
Epoch [102/120    avg_loss:0.022, val_acc:0.972]
Epoch [103/120    avg_loss:0.020, val_acc:0.972]
Epoch [104/120    avg_loss:0.024, val_acc:0.972]
Epoch [105/120    avg_loss:0.025, val_acc:0.972]
Epoch [106/120    avg_loss:0.020, val_acc:0.973]
Epoch [107/120    avg_loss:0.026, val_acc:0.972]
Epoch [108/120    avg_loss:0.022, val_acc:0.972]
Epoch [109/120    avg_loss:0.024, val_acc:0.972]
Epoch [110/120    avg_loss:0.019, val_acc:0.972]
Epoch [111/120    avg_loss:0.022, val_acc:0.972]
Epoch [112/120    avg_loss:0.023, val_acc:0.973]
Epoch [113/120    avg_loss:0.020, val_acc:0.973]
Epoch [114/120    avg_loss:0.019, val_acc:0.973]
Epoch [115/120    avg_loss:0.025, val_acc:0.973]
Epoch [116/120    avg_loss:0.022, val_acc:0.973]
Epoch [117/120    avg_loss:0.019, val_acc:0.973]
Epoch [118/120    avg_loss:0.018, val_acc:0.973]
Epoch [119/120    avg_loss:0.020, val_acc:0.973]
Epoch [120/120    avg_loss:0.019, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1223    0    0    1    4    0    0    2    5   37    5    0
     0    8    0]
 [   0    0    0  737    1    2    0    0    0    2    0    0    5    0
     0    0    0]
 [   0    0    0    1  211    0    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    0    0    0    0    0    0    1
    10    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    6    0    0   12    0    0    0    0
     0    0    0]
 [   0    0    3   89    0    2    0    0    0    0  767   11    0    0
     0    3    0]
 [   0    0   17    0    0    0    4    0    4    0   23 2158    0    4
     0    0    0]
 [   0    0    0   39    0   15    0    0    0    0    4    0  472    0
     0    3    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   23    0    0    2    0    0    0    0
    38  284    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.88075880758808

F1 scores:
[       nan 1.         0.96756329 0.91382517 0.99294118 0.96473265
 0.97185185 1.         0.99421965 0.64864865 0.91636798 0.9771338
 0.92730845 0.98666667 0.97892473 0.88062016 0.98203593]

Kappa:
0.9530356607442431
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6321feeb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.694, val_acc:0.424]
Epoch [2/120    avg_loss:2.348, val_acc:0.485]
Epoch [3/120    avg_loss:2.110, val_acc:0.520]
Epoch [4/120    avg_loss:1.947, val_acc:0.541]
Epoch [5/120    avg_loss:1.804, val_acc:0.569]
Epoch [6/120    avg_loss:1.654, val_acc:0.619]
Epoch [7/120    avg_loss:1.547, val_acc:0.615]
Epoch [8/120    avg_loss:1.427, val_acc:0.651]
Epoch [9/120    avg_loss:1.265, val_acc:0.691]
Epoch [10/120    avg_loss:1.195, val_acc:0.695]
Epoch [11/120    avg_loss:1.029, val_acc:0.689]
Epoch [12/120    avg_loss:0.970, val_acc:0.750]
Epoch [13/120    avg_loss:0.889, val_acc:0.728]
Epoch [14/120    avg_loss:0.742, val_acc:0.741]
Epoch [15/120    avg_loss:0.703, val_acc:0.768]
Epoch [16/120    avg_loss:0.639, val_acc:0.788]
Epoch [17/120    avg_loss:0.662, val_acc:0.810]
Epoch [18/120    avg_loss:0.612, val_acc:0.783]
Epoch [19/120    avg_loss:0.562, val_acc:0.831]
Epoch [20/120    avg_loss:0.441, val_acc:0.858]
Epoch [21/120    avg_loss:1.722, val_acc:0.279]
Epoch [22/120    avg_loss:2.063, val_acc:0.408]
Epoch [23/120    avg_loss:1.891, val_acc:0.502]
Epoch [24/120    avg_loss:1.643, val_acc:0.578]
Epoch [25/120    avg_loss:1.493, val_acc:0.612]
Epoch [26/120    avg_loss:1.418, val_acc:0.624]
Epoch [27/120    avg_loss:1.255, val_acc:0.642]
Epoch [28/120    avg_loss:1.103, val_acc:0.682]
Epoch [29/120    avg_loss:1.015, val_acc:0.690]
Epoch [30/120    avg_loss:0.898, val_acc:0.728]
Epoch [31/120    avg_loss:0.827, val_acc:0.754]
Epoch [32/120    avg_loss:0.820, val_acc:0.789]
Epoch [33/120    avg_loss:0.704, val_acc:0.763]
Epoch [34/120    avg_loss:0.601, val_acc:0.786]
Epoch [35/120    avg_loss:0.589, val_acc:0.798]
Epoch [36/120    avg_loss:0.573, val_acc:0.788]
Epoch [37/120    avg_loss:0.568, val_acc:0.794]
Epoch [38/120    avg_loss:0.541, val_acc:0.802]
Epoch [39/120    avg_loss:0.528, val_acc:0.798]
Epoch [40/120    avg_loss:0.549, val_acc:0.814]
Epoch [41/120    avg_loss:0.517, val_acc:0.802]
Epoch [42/120    avg_loss:0.546, val_acc:0.809]
Epoch [43/120    avg_loss:0.506, val_acc:0.815]
Epoch [44/120    avg_loss:0.517, val_acc:0.817]
Epoch [45/120    avg_loss:0.530, val_acc:0.816]
Epoch [46/120    avg_loss:0.518, val_acc:0.821]
Epoch [47/120    avg_loss:0.503, val_acc:0.823]
Epoch [48/120    avg_loss:0.504, val_acc:0.823]
Epoch [49/120    avg_loss:0.462, val_acc:0.824]
Epoch [50/120    avg_loss:0.493, val_acc:0.824]
Epoch [51/120    avg_loss:0.487, val_acc:0.823]
Epoch [52/120    avg_loss:0.494, val_acc:0.821]
Epoch [53/120    avg_loss:0.482, val_acc:0.824]
Epoch [54/120    avg_loss:0.466, val_acc:0.818]
Epoch [55/120    avg_loss:0.487, val_acc:0.825]
Epoch [56/120    avg_loss:0.462, val_acc:0.820]
Epoch [57/120    avg_loss:0.505, val_acc:0.821]
Epoch [58/120    avg_loss:0.477, val_acc:0.821]
Epoch [59/120    avg_loss:0.461, val_acc:0.820]
Epoch [60/120    avg_loss:0.491, val_acc:0.821]
Epoch [61/120    avg_loss:0.494, val_acc:0.821]
Epoch [62/120    avg_loss:0.486, val_acc:0.821]
Epoch [63/120    avg_loss:0.494, val_acc:0.821]
Epoch [64/120    avg_loss:0.489, val_acc:0.820]
Epoch [65/120    avg_loss:0.511, val_acc:0.820]
Epoch [66/120    avg_loss:0.459, val_acc:0.821]
Epoch [67/120    avg_loss:0.489, val_acc:0.821]
Epoch [68/120    avg_loss:0.438, val_acc:0.821]
Epoch [69/120    avg_loss:0.474, val_acc:0.821]
Epoch [70/120    avg_loss:0.493, val_acc:0.821]
Epoch [71/120    avg_loss:0.483, val_acc:0.821]
Epoch [72/120    avg_loss:0.505, val_acc:0.821]
Epoch [73/120    avg_loss:0.510, val_acc:0.821]
Epoch [74/120    avg_loss:0.484, val_acc:0.821]
Epoch [75/120    avg_loss:0.466, val_acc:0.821]
Epoch [76/120    avg_loss:0.488, val_acc:0.821]
Epoch [77/120    avg_loss:0.463, val_acc:0.821]
Epoch [78/120    avg_loss:0.472, val_acc:0.821]
Epoch [79/120    avg_loss:0.464, val_acc:0.821]
Epoch [80/120    avg_loss:0.484, val_acc:0.821]
Epoch [81/120    avg_loss:0.473, val_acc:0.821]
Epoch [82/120    avg_loss:0.473, val_acc:0.821]
Epoch [83/120    avg_loss:0.481, val_acc:0.823]
Epoch [84/120    avg_loss:0.502, val_acc:0.823]
Epoch [85/120    avg_loss:0.487, val_acc:0.823]
Epoch [86/120    avg_loss:0.505, val_acc:0.823]
Epoch [87/120    avg_loss:0.461, val_acc:0.823]
Epoch [88/120    avg_loss:0.472, val_acc:0.823]
Epoch [89/120    avg_loss:0.498, val_acc:0.823]
Epoch [90/120    avg_loss:0.488, val_acc:0.823]
Epoch [91/120    avg_loss:0.475, val_acc:0.823]
Epoch [92/120    avg_loss:0.450, val_acc:0.823]
Epoch [93/120    avg_loss:0.480, val_acc:0.823]
Epoch [94/120    avg_loss:0.516, val_acc:0.823]
Epoch [95/120    avg_loss:0.470, val_acc:0.823]
Epoch [96/120    avg_loss:0.469, val_acc:0.823]
Epoch [97/120    avg_loss:0.499, val_acc:0.823]
Epoch [98/120    avg_loss:0.482, val_acc:0.823]
Epoch [99/120    avg_loss:0.468, val_acc:0.823]
Epoch [100/120    avg_loss:0.498, val_acc:0.823]
Epoch [101/120    avg_loss:0.486, val_acc:0.823]
Epoch [102/120    avg_loss:0.483, val_acc:0.823]
Epoch [103/120    avg_loss:0.490, val_acc:0.823]
Epoch [104/120    avg_loss:0.505, val_acc:0.823]
Epoch [105/120    avg_loss:0.503, val_acc:0.823]
Epoch [106/120    avg_loss:0.460, val_acc:0.823]
Epoch [107/120    avg_loss:0.472, val_acc:0.823]
Epoch [108/120    avg_loss:0.453, val_acc:0.823]
Epoch [109/120    avg_loss:0.483, val_acc:0.823]
Epoch [110/120    avg_loss:0.453, val_acc:0.823]
Epoch [111/120    avg_loss:0.477, val_acc:0.823]
Epoch [112/120    avg_loss:0.482, val_acc:0.823]
Epoch [113/120    avg_loss:0.470, val_acc:0.823]
Epoch [114/120    avg_loss:0.489, val_acc:0.823]
Epoch [115/120    avg_loss:0.460, val_acc:0.823]
Epoch [116/120    avg_loss:0.471, val_acc:0.823]
Epoch [117/120    avg_loss:0.451, val_acc:0.823]
Epoch [118/120    avg_loss:0.471, val_acc:0.823]
Epoch [119/120    avg_loss:0.477, val_acc:0.823]
Epoch [120/120    avg_loss:0.498, val_acc:0.823]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    1    0    0    4    0    0    0
     0    0    0]
 [   0    5 1000   23    1    0   13    0    0    0   98  125    1    6
     3    3    7]
 [   0    0   32  599   15    8    0    0    0   17   22   14    0   28
    12    0    0]
 [   0    0   20   62  131    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    3    0    0  378    0   42    0    0    0    1    0    0
    11    0    0]
 [   0    0    0    0    2    0  633    0    0    0    0    9    0    0
    13    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   15    0    0    0    0    0    0  415    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    1    0    9    0    0    6    0    2    0    0
     0    0    0]
 [   0    0   41   46    0   21    0    0    0    0  632   71    5   20
     7   32    0]
 [   0   15  126   52   11   18   27    0    8    1  242 1574   69   24
    18   24    1]
 [   0    0    8   46   60    7    0    0    0    0   14    0  366    0
     0    4   29]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0  183
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    1    0    0    5    0
  1129    0    0]
 [   0    0    2    9    0    0    0    0    0    0    1   30    1    0
   238   66    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
78.66666666666667

F1 scores:
[       nan 0.64285714 0.79459674 0.75631313 0.60368664 0.86597938
 0.9454817  0.53763441 0.97303634 0.27906977 0.66949153 0.77998018
 0.74617737 0.8206278  0.87859922 0.27731092 0.8195122 ]

Kappa:
0.758341933997164
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc3f1bcdac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.739, val_acc:0.440]
Epoch [2/120    avg_loss:2.498, val_acc:0.492]
Epoch [3/120    avg_loss:2.295, val_acc:0.529]
Epoch [4/120    avg_loss:2.098, val_acc:0.549]
Epoch [5/120    avg_loss:1.934, val_acc:0.557]
Epoch [6/120    avg_loss:1.771, val_acc:0.608]
Epoch [7/120    avg_loss:1.575, val_acc:0.631]
Epoch [8/120    avg_loss:1.388, val_acc:0.638]
Epoch [9/120    avg_loss:1.265, val_acc:0.668]
Epoch [10/120    avg_loss:1.089, val_acc:0.695]
Epoch [11/120    avg_loss:0.991, val_acc:0.681]
Epoch [12/120    avg_loss:0.865, val_acc:0.697]
Epoch [13/120    avg_loss:0.830, val_acc:0.745]
Epoch [14/120    avg_loss:0.727, val_acc:0.735]
Epoch [15/120    avg_loss:0.674, val_acc:0.801]
Epoch [16/120    avg_loss:0.600, val_acc:0.712]
Epoch [17/120    avg_loss:0.544, val_acc:0.826]
Epoch [18/120    avg_loss:0.519, val_acc:0.821]
Epoch [19/120    avg_loss:0.464, val_acc:0.856]
Epoch [20/120    avg_loss:0.579, val_acc:0.798]
Epoch [21/120    avg_loss:0.424, val_acc:0.811]
Epoch [22/120    avg_loss:0.435, val_acc:0.855]
Epoch [23/120    avg_loss:0.315, val_acc:0.883]
Epoch [24/120    avg_loss:0.288, val_acc:0.842]
Epoch [25/120    avg_loss:0.266, val_acc:0.897]
Epoch [26/120    avg_loss:0.328, val_acc:0.869]
Epoch [27/120    avg_loss:0.288, val_acc:0.837]
Epoch [28/120    avg_loss:0.244, val_acc:0.902]
Epoch [29/120    avg_loss:0.166, val_acc:0.885]
Epoch [30/120    avg_loss:0.168, val_acc:0.847]
Epoch [31/120    avg_loss:0.204, val_acc:0.858]
Epoch [32/120    avg_loss:0.253, val_acc:0.905]
Epoch [33/120    avg_loss:0.183, val_acc:0.913]
Epoch [34/120    avg_loss:0.150, val_acc:0.921]
Epoch [35/120    avg_loss:0.127, val_acc:0.929]
Epoch [36/120    avg_loss:0.098, val_acc:0.934]
Epoch [37/120    avg_loss:0.103, val_acc:0.929]
Epoch [38/120    avg_loss:0.109, val_acc:0.923]
Epoch [39/120    avg_loss:0.102, val_acc:0.932]
Epoch [40/120    avg_loss:0.110, val_acc:0.921]
Epoch [41/120    avg_loss:0.111, val_acc:0.932]
Epoch [42/120    avg_loss:0.077, val_acc:0.947]
Epoch [43/120    avg_loss:0.083, val_acc:0.907]
Epoch [44/120    avg_loss:0.119, val_acc:0.940]
Epoch [45/120    avg_loss:0.078, val_acc:0.945]
Epoch [46/120    avg_loss:0.052, val_acc:0.945]
Epoch [47/120    avg_loss:0.053, val_acc:0.951]
Epoch [48/120    avg_loss:0.053, val_acc:0.941]
Epoch [49/120    avg_loss:0.073, val_acc:0.952]
Epoch [50/120    avg_loss:0.100, val_acc:0.945]
Epoch [51/120    avg_loss:0.090, val_acc:0.923]
Epoch [52/120    avg_loss:0.136, val_acc:0.929]
Epoch [53/120    avg_loss:0.104, val_acc:0.930]
Epoch [54/120    avg_loss:0.078, val_acc:0.950]
Epoch [55/120    avg_loss:0.062, val_acc:0.952]
Epoch [56/120    avg_loss:0.059, val_acc:0.931]
Epoch [57/120    avg_loss:0.084, val_acc:0.954]
Epoch [58/120    avg_loss:0.072, val_acc:0.932]
Epoch [59/120    avg_loss:0.051, val_acc:0.959]
Epoch [60/120    avg_loss:0.038, val_acc:0.965]
Epoch [61/120    avg_loss:0.047, val_acc:0.924]
Epoch [62/120    avg_loss:0.056, val_acc:0.932]
Epoch [63/120    avg_loss:0.045, val_acc:0.963]
Epoch [64/120    avg_loss:0.041, val_acc:0.964]
Epoch [65/120    avg_loss:0.045, val_acc:0.964]
Epoch [66/120    avg_loss:0.036, val_acc:0.962]
Epoch [67/120    avg_loss:0.045, val_acc:0.955]
Epoch [68/120    avg_loss:0.032, val_acc:0.961]
Epoch [69/120    avg_loss:0.029, val_acc:0.956]
Epoch [70/120    avg_loss:0.032, val_acc:0.958]
Epoch [71/120    avg_loss:0.029, val_acc:0.968]
Epoch [72/120    avg_loss:0.024, val_acc:0.961]
Epoch [73/120    avg_loss:0.017, val_acc:0.962]
Epoch [74/120    avg_loss:0.029, val_acc:0.969]
Epoch [75/120    avg_loss:0.034, val_acc:0.909]
Epoch [76/120    avg_loss:0.033, val_acc:0.960]
Epoch [77/120    avg_loss:0.029, val_acc:0.965]
Epoch [78/120    avg_loss:0.026, val_acc:0.967]
Epoch [79/120    avg_loss:0.024, val_acc:0.969]
Epoch [80/120    avg_loss:0.021, val_acc:0.973]
Epoch [81/120    avg_loss:0.024, val_acc:0.974]
Epoch [82/120    avg_loss:0.017, val_acc:0.970]
Epoch [83/120    avg_loss:0.049, val_acc:0.947]
Epoch [84/120    avg_loss:0.044, val_acc:0.961]
Epoch [85/120    avg_loss:1.815, val_acc:0.213]
Epoch [86/120    avg_loss:2.216, val_acc:0.343]
Epoch [87/120    avg_loss:1.931, val_acc:0.474]
Epoch [88/120    avg_loss:1.720, val_acc:0.501]
Epoch [89/120    avg_loss:1.618, val_acc:0.499]
Epoch [90/120    avg_loss:1.563, val_acc:0.545]
Epoch [91/120    avg_loss:1.451, val_acc:0.561]
Epoch [92/120    avg_loss:1.373, val_acc:0.562]
Epoch [93/120    avg_loss:1.329, val_acc:0.578]
Epoch [94/120    avg_loss:1.240, val_acc:0.577]
Epoch [95/120    avg_loss:1.164, val_acc:0.587]
Epoch [96/120    avg_loss:1.154, val_acc:0.600]
Epoch [97/120    avg_loss:1.178, val_acc:0.602]
Epoch [98/120    avg_loss:1.147, val_acc:0.605]
Epoch [99/120    avg_loss:1.132, val_acc:0.608]
Epoch [100/120    avg_loss:1.137, val_acc:0.610]
Epoch [101/120    avg_loss:1.128, val_acc:0.612]
Epoch [102/120    avg_loss:1.115, val_acc:0.618]
Epoch [103/120    avg_loss:1.112, val_acc:0.612]
Epoch [104/120    avg_loss:1.139, val_acc:0.612]
Epoch [105/120    avg_loss:1.081, val_acc:0.613]
Epoch [106/120    avg_loss:1.073, val_acc:0.623]
Epoch [107/120    avg_loss:1.123, val_acc:0.619]
Epoch [108/120    avg_loss:1.089, val_acc:0.618]
Epoch [109/120    avg_loss:1.070, val_acc:0.620]
Epoch [110/120    avg_loss:1.081, val_acc:0.620]
Epoch [111/120    avg_loss:1.062, val_acc:0.622]
Epoch [112/120    avg_loss:1.091, val_acc:0.624]
Epoch [113/120    avg_loss:1.086, val_acc:0.622]
Epoch [114/120    avg_loss:1.082, val_acc:0.623]
Epoch [115/120    avg_loss:1.082, val_acc:0.623]
Epoch [116/120    avg_loss:1.056, val_acc:0.622]
Epoch [117/120    avg_loss:1.054, val_acc:0.622]
Epoch [118/120    avg_loss:1.078, val_acc:0.621]
Epoch [119/120    avg_loss:1.069, val_acc:0.620]
Epoch [120/120    avg_loss:1.057, val_acc:0.621]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   32    3    0    0    3    0    0    2    0    0    0    1    0
     0    0    0]
 [   0    1  441  125  162   37    0    3    0    1  201  241   60    0
     1   12    0]
 [   0    0   14  471   33   24    0    0    0    0   97   39   25   30
     0   14    0]
 [   0    0    4   12  151    0    0    0    0    0   41    0    0    1
     0    4    0]
 [   0    0    0    0    0  211   12    1    0    0   13    0    1    0
   197    0    0]
 [   0    0    0   16    0   33  587    0    0    0    0    0    0    6
     9    6    0]
 [   0    0    0    0    0   10    0   15    0    0    0    0    0    0
     0    0    0]
 [   0   14    0    0    1    0    0    0  413    0    0    0    0    2
     0    0    0]
 [   0    0    0    0    0    0    7    0    0    8    0    0    0    0
     0    3    0]
 [   0    0   15  134    0    4    0    0    0   12  464  154   42    6
     1   43    0]
 [   0    0   99  340   15   65   30    1    0    5  161 1166  122   19
    70   42   75]
 [   0    0  108   21   46    9    0    0    0    0   54   42  222    0
     0   20   12]
 [   0    0    0   22    0    0    0    0    0    0    0    2    0  156
     5    0    0]
 [   0    0    0    0    0   35    0    0    0    0    0    0   11    5
  1014   74    0]
 [   0    0    6   31    0    2    4    0    0    5    1    4   49    9
   116  120    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
60.21680216802168

F1 scores:
[       nan 0.72727273 0.44658228 0.49088067 0.4863124  0.48617512
 0.90516577 0.66666667 0.97751479 0.32653061 0.48662821 0.60445827
 0.41611996 0.74463007 0.79467085 0.35036496 0.65882353]

Kappa:
0.5536325711823925
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f757a6d0a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.764, val_acc:0.335]
Epoch [2/120    avg_loss:2.567, val_acc:0.422]
Epoch [3/120    avg_loss:2.351, val_acc:0.448]
Epoch [4/120    avg_loss:2.163, val_acc:0.432]
Epoch [5/120    avg_loss:1.956, val_acc:0.582]
Epoch [6/120    avg_loss:1.732, val_acc:0.608]
Epoch [7/120    avg_loss:1.613, val_acc:0.632]
Epoch [8/120    avg_loss:1.443, val_acc:0.666]
Epoch [9/120    avg_loss:1.273, val_acc:0.680]
Epoch [10/120    avg_loss:1.112, val_acc:0.704]
Epoch [11/120    avg_loss:0.972, val_acc:0.620]
Epoch [12/120    avg_loss:0.890, val_acc:0.752]
Epoch [13/120    avg_loss:0.825, val_acc:0.726]
Epoch [14/120    avg_loss:0.761, val_acc:0.753]
Epoch [15/120    avg_loss:0.766, val_acc:0.764]
Epoch [16/120    avg_loss:0.584, val_acc:0.792]
Epoch [17/120    avg_loss:0.516, val_acc:0.825]
Epoch [18/120    avg_loss:0.460, val_acc:0.794]
Epoch [19/120    avg_loss:0.502, val_acc:0.815]
Epoch [20/120    avg_loss:0.451, val_acc:0.805]
Epoch [21/120    avg_loss:0.361, val_acc:0.812]
Epoch [22/120    avg_loss:0.323, val_acc:0.859]
Epoch [23/120    avg_loss:0.325, val_acc:0.878]
Epoch [24/120    avg_loss:0.370, val_acc:0.834]
Epoch [25/120    avg_loss:0.238, val_acc:0.875]
Epoch [26/120    avg_loss:0.214, val_acc:0.879]
Epoch [27/120    avg_loss:0.212, val_acc:0.900]
Epoch [28/120    avg_loss:0.207, val_acc:0.826]
Epoch [29/120    avg_loss:0.285, val_acc:0.879]
Epoch [30/120    avg_loss:0.221, val_acc:0.908]
Epoch [31/120    avg_loss:0.179, val_acc:0.911]
Epoch [32/120    avg_loss:0.155, val_acc:0.879]
Epoch [33/120    avg_loss:0.155, val_acc:0.900]
Epoch [34/120    avg_loss:0.188, val_acc:0.910]
Epoch [35/120    avg_loss:0.159, val_acc:0.886]
Epoch [36/120    avg_loss:0.155, val_acc:0.912]
Epoch [37/120    avg_loss:0.138, val_acc:0.909]
Epoch [38/120    avg_loss:0.104, val_acc:0.929]
Epoch [39/120    avg_loss:0.099, val_acc:0.925]
Epoch [40/120    avg_loss:0.097, val_acc:0.930]
Epoch [41/120    avg_loss:0.103, val_acc:0.919]
Epoch [42/120    avg_loss:0.125, val_acc:0.921]
Epoch [43/120    avg_loss:0.119, val_acc:0.911]
Epoch [44/120    avg_loss:0.107, val_acc:0.919]
Epoch [45/120    avg_loss:0.101, val_acc:0.931]
Epoch [46/120    avg_loss:0.091, val_acc:0.936]
Epoch [47/120    avg_loss:0.082, val_acc:0.934]
Epoch [48/120    avg_loss:0.072, val_acc:0.946]
Epoch [49/120    avg_loss:0.079, val_acc:0.941]
Epoch [50/120    avg_loss:0.081, val_acc:0.933]
Epoch [51/120    avg_loss:0.048, val_acc:0.938]
Epoch [52/120    avg_loss:0.164, val_acc:0.887]
Epoch [53/120    avg_loss:0.204, val_acc:0.925]
Epoch [54/120    avg_loss:0.119, val_acc:0.939]
Epoch [55/120    avg_loss:0.073, val_acc:0.950]
Epoch [56/120    avg_loss:0.056, val_acc:0.953]
Epoch [57/120    avg_loss:0.050, val_acc:0.955]
Epoch [58/120    avg_loss:0.067, val_acc:0.946]
Epoch [59/120    avg_loss:0.062, val_acc:0.940]
Epoch [60/120    avg_loss:0.051, val_acc:0.954]
Epoch [61/120    avg_loss:0.089, val_acc:0.932]
Epoch [62/120    avg_loss:0.062, val_acc:0.924]
Epoch [63/120    avg_loss:0.050, val_acc:0.953]
Epoch [64/120    avg_loss:0.044, val_acc:0.938]
Epoch [65/120    avg_loss:0.056, val_acc:0.946]
Epoch [66/120    avg_loss:0.043, val_acc:0.944]
Epoch [67/120    avg_loss:0.045, val_acc:0.947]
Epoch [68/120    avg_loss:0.043, val_acc:0.951]
Epoch [69/120    avg_loss:0.027, val_acc:0.965]
Epoch [70/120    avg_loss:0.026, val_acc:0.960]
Epoch [71/120    avg_loss:0.024, val_acc:0.961]
Epoch [72/120    avg_loss:0.028, val_acc:0.961]
Epoch [73/120    avg_loss:0.042, val_acc:0.947]
Epoch [74/120    avg_loss:0.035, val_acc:0.958]
Epoch [75/120    avg_loss:0.026, val_acc:0.950]
Epoch [76/120    avg_loss:0.021, val_acc:0.960]
Epoch [77/120    avg_loss:0.070, val_acc:0.961]
Epoch [78/120    avg_loss:0.041, val_acc:0.945]
Epoch [79/120    avg_loss:0.049, val_acc:0.924]
Epoch [80/120    avg_loss:0.047, val_acc:0.950]
Epoch [81/120    avg_loss:0.032, val_acc:0.961]
Epoch [82/120    avg_loss:0.020, val_acc:0.972]
Epoch [83/120    avg_loss:0.035, val_acc:0.940]
Epoch [84/120    avg_loss:0.033, val_acc:0.958]
Epoch [85/120    avg_loss:0.025, val_acc:0.958]
Epoch [86/120    avg_loss:0.020, val_acc:0.969]
Epoch [87/120    avg_loss:0.039, val_acc:0.934]
Epoch [88/120    avg_loss:0.029, val_acc:0.964]
Epoch [89/120    avg_loss:0.018, val_acc:0.964]
Epoch [90/120    avg_loss:0.027, val_acc:0.963]
Epoch [91/120    avg_loss:0.018, val_acc:0.957]
Epoch [92/120    avg_loss:0.016, val_acc:0.969]
Epoch [93/120    avg_loss:0.010, val_acc:0.968]
Epoch [94/120    avg_loss:0.014, val_acc:0.958]
Epoch [95/120    avg_loss:0.016, val_acc:0.973]
Epoch [96/120    avg_loss:0.011, val_acc:0.960]
Epoch [97/120    avg_loss:0.023, val_acc:0.948]
Epoch [98/120    avg_loss:0.011, val_acc:0.968]
Epoch [99/120    avg_loss:0.011, val_acc:0.972]
Epoch [100/120    avg_loss:0.021, val_acc:0.959]
Epoch [101/120    avg_loss:0.013, val_acc:0.970]
Epoch [102/120    avg_loss:0.009, val_acc:0.974]
Epoch [103/120    avg_loss:0.008, val_acc:0.964]
Epoch [104/120    avg_loss:0.012, val_acc:0.960]
Epoch [105/120    avg_loss:0.017, val_acc:0.961]
Epoch [106/120    avg_loss:0.018, val_acc:0.960]
Epoch [107/120    avg_loss:0.022, val_acc:0.956]
Epoch [108/120    avg_loss:0.023, val_acc:0.972]
Epoch [109/120    avg_loss:0.014, val_acc:0.965]
Epoch [110/120    avg_loss:0.019, val_acc:0.964]
Epoch [111/120    avg_loss:0.025, val_acc:0.953]
Epoch [112/120    avg_loss:0.024, val_acc:0.963]
Epoch [113/120    avg_loss:0.011, val_acc:0.966]
Epoch [114/120    avg_loss:0.010, val_acc:0.965]
Epoch [115/120    avg_loss:0.009, val_acc:0.969]
Epoch [116/120    avg_loss:0.008, val_acc:0.966]
Epoch [117/120    avg_loss:0.022, val_acc:0.971]
Epoch [118/120    avg_loss:0.008, val_acc:0.971]
Epoch [119/120    avg_loss:0.009, val_acc:0.970]
Epoch [120/120    avg_loss:0.007, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1251    1    7    0    0    0    0    0    5   21    0    0
     0    0    0]
 [   0    0    1  726    5    0    0    0    0    0    2   12    1    0
     0    0    0]
 [   0    0    0    2  210    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  429    2    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    0    0    0    0    0  833   32    0    0
     0    0    0]
 [   0    0    8    0    0    6    0    0    0    0   14 2168   13    0
     0    1    0]
 [   0    0    0    3    0    0    0    0    0    0    5    2  519    0
     1    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1127   11    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
    62  283    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.31165311653116

F1 scores:
[       nan 1.         0.97925636 0.98108108 0.96551724 0.98169336
 0.99542683 0.97959184 1.         0.97142857 0.96078431 0.97547807
 0.96918768 1.         0.96448438 0.87888199 0.97005988]

Kappa:
0.969323615077474
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdc258a6b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.767, val_acc:0.148]
Epoch [2/120    avg_loss:2.551, val_acc:0.479]
Epoch [3/120    avg_loss:2.360, val_acc:0.473]
Epoch [4/120    avg_loss:2.165, val_acc:0.510]
Epoch [5/120    avg_loss:1.998, val_acc:0.551]
Epoch [6/120    avg_loss:1.845, val_acc:0.606]
Epoch [7/120    avg_loss:1.679, val_acc:0.616]
Epoch [8/120    avg_loss:1.516, val_acc:0.637]
Epoch [9/120    avg_loss:1.407, val_acc:0.644]
Epoch [10/120    avg_loss:1.235, val_acc:0.682]
Epoch [11/120    avg_loss:1.146, val_acc:0.669]
Epoch [12/120    avg_loss:1.030, val_acc:0.742]
Epoch [13/120    avg_loss:0.973, val_acc:0.702]
Epoch [14/120    avg_loss:0.926, val_acc:0.688]
Epoch [15/120    avg_loss:0.841, val_acc:0.765]
Epoch [16/120    avg_loss:0.692, val_acc:0.752]
Epoch [17/120    avg_loss:0.652, val_acc:0.769]
Epoch [18/120    avg_loss:0.623, val_acc:0.812]
Epoch [19/120    avg_loss:0.512, val_acc:0.797]
Epoch [20/120    avg_loss:0.540, val_acc:0.742]
Epoch [21/120    avg_loss:0.487, val_acc:0.802]
Epoch [22/120    avg_loss:0.458, val_acc:0.845]
Epoch [23/120    avg_loss:0.344, val_acc:0.858]
Epoch [24/120    avg_loss:0.322, val_acc:0.866]
Epoch [25/120    avg_loss:0.313, val_acc:0.870]
Epoch [26/120    avg_loss:0.306, val_acc:0.848]
Epoch [27/120    avg_loss:0.316, val_acc:0.884]
Epoch [28/120    avg_loss:0.266, val_acc:0.892]
Epoch [29/120    avg_loss:0.230, val_acc:0.896]
Epoch [30/120    avg_loss:0.327, val_acc:0.880]
Epoch [31/120    avg_loss:0.242, val_acc:0.901]
Epoch [32/120    avg_loss:0.211, val_acc:0.907]
Epoch [33/120    avg_loss:0.207, val_acc:0.894]
Epoch [34/120    avg_loss:0.208, val_acc:0.895]
Epoch [35/120    avg_loss:0.154, val_acc:0.915]
Epoch [36/120    avg_loss:0.227, val_acc:0.926]
Epoch [37/120    avg_loss:0.135, val_acc:0.887]
Epoch [38/120    avg_loss:0.150, val_acc:0.906]
Epoch [39/120    avg_loss:0.158, val_acc:0.919]
Epoch [40/120    avg_loss:0.149, val_acc:0.891]
Epoch [41/120    avg_loss:0.153, val_acc:0.927]
Epoch [42/120    avg_loss:0.144, val_acc:0.921]
Epoch [43/120    avg_loss:0.111, val_acc:0.947]
Epoch [44/120    avg_loss:0.089, val_acc:0.938]
Epoch [45/120    avg_loss:0.094, val_acc:0.933]
Epoch [46/120    avg_loss:0.107, val_acc:0.935]
Epoch [47/120    avg_loss:0.098, val_acc:0.940]
Epoch [48/120    avg_loss:0.141, val_acc:0.912]
Epoch [49/120    avg_loss:0.175, val_acc:0.895]
Epoch [50/120    avg_loss:0.179, val_acc:0.944]
Epoch [51/120    avg_loss:0.114, val_acc:0.945]
Epoch [52/120    avg_loss:0.085, val_acc:0.961]
Epoch [53/120    avg_loss:0.067, val_acc:0.949]
Epoch [54/120    avg_loss:0.055, val_acc:0.956]
Epoch [55/120    avg_loss:0.061, val_acc:0.951]
Epoch [56/120    avg_loss:0.064, val_acc:0.946]
Epoch [57/120    avg_loss:0.049, val_acc:0.966]
Epoch [58/120    avg_loss:0.054, val_acc:0.947]
Epoch [59/120    avg_loss:0.061, val_acc:0.960]
Epoch [60/120    avg_loss:0.043, val_acc:0.956]
Epoch [61/120    avg_loss:0.039, val_acc:0.956]
Epoch [62/120    avg_loss:0.037, val_acc:0.951]
Epoch [63/120    avg_loss:0.043, val_acc:0.961]
Epoch [64/120    avg_loss:0.033, val_acc:0.955]
Epoch [65/120    avg_loss:0.047, val_acc:0.959]
Epoch [66/120    avg_loss:0.049, val_acc:0.970]
Epoch [67/120    avg_loss:0.044, val_acc:0.963]
Epoch [68/120    avg_loss:0.043, val_acc:0.959]
Epoch [69/120    avg_loss:0.059, val_acc:0.964]
Epoch [70/120    avg_loss:0.047, val_acc:0.961]
Epoch [71/120    avg_loss:0.048, val_acc:0.965]
Epoch [72/120    avg_loss:0.049, val_acc:0.960]
Epoch [73/120    avg_loss:0.034, val_acc:0.964]
Epoch [74/120    avg_loss:0.038, val_acc:0.945]
Epoch [75/120    avg_loss:0.043, val_acc:0.957]
Epoch [76/120    avg_loss:0.030, val_acc:0.971]
Epoch [77/120    avg_loss:0.036, val_acc:0.963]
Epoch [78/120    avg_loss:0.034, val_acc:0.971]
Epoch [79/120    avg_loss:0.019, val_acc:0.974]
Epoch [80/120    avg_loss:0.018, val_acc:0.975]
Epoch [81/120    avg_loss:0.021, val_acc:0.971]
Epoch [82/120    avg_loss:0.025, val_acc:0.971]
Epoch [83/120    avg_loss:0.020, val_acc:0.971]
Epoch [84/120    avg_loss:0.018, val_acc:0.968]
Epoch [85/120    avg_loss:0.018, val_acc:0.971]
Epoch [86/120    avg_loss:0.018, val_acc:0.974]
Epoch [87/120    avg_loss:0.017, val_acc:0.978]
Epoch [88/120    avg_loss:0.018, val_acc:0.974]
Epoch [89/120    avg_loss:0.020, val_acc:0.976]
Epoch [90/120    avg_loss:0.014, val_acc:0.977]
Epoch [91/120    avg_loss:0.019, val_acc:0.968]
Epoch [92/120    avg_loss:0.014, val_acc:0.969]
Epoch [93/120    avg_loss:0.013, val_acc:0.975]
Epoch [94/120    avg_loss:0.015, val_acc:0.976]
Epoch [95/120    avg_loss:0.011, val_acc:0.970]
Epoch [96/120    avg_loss:0.023, val_acc:0.974]
Epoch [97/120    avg_loss:0.013, val_acc:0.977]
Epoch [98/120    avg_loss:0.012, val_acc:0.971]
Epoch [99/120    avg_loss:0.019, val_acc:0.971]
Epoch [100/120    avg_loss:0.016, val_acc:0.977]
Epoch [101/120    avg_loss:0.011, val_acc:0.978]
Epoch [102/120    avg_loss:0.010, val_acc:0.979]
Epoch [103/120    avg_loss:0.008, val_acc:0.978]
Epoch [104/120    avg_loss:0.012, val_acc:0.977]
Epoch [105/120    avg_loss:0.007, val_acc:0.978]
Epoch [106/120    avg_loss:0.008, val_acc:0.979]
Epoch [107/120    avg_loss:0.007, val_acc:0.978]
Epoch [108/120    avg_loss:0.008, val_acc:0.978]
Epoch [109/120    avg_loss:0.010, val_acc:0.979]
Epoch [110/120    avg_loss:0.008, val_acc:0.977]
Epoch [111/120    avg_loss:0.008, val_acc:0.980]
Epoch [112/120    avg_loss:0.008, val_acc:0.980]
Epoch [113/120    avg_loss:0.011, val_acc:0.980]
Epoch [114/120    avg_loss:0.009, val_acc:0.981]
Epoch [115/120    avg_loss:0.009, val_acc:0.980]
Epoch [116/120    avg_loss:0.009, val_acc:0.980]
Epoch [117/120    avg_loss:0.010, val_acc:0.977]
Epoch [118/120    avg_loss:0.006, val_acc:0.977]
Epoch [119/120    avg_loss:0.007, val_acc:0.976]
Epoch [120/120    avg_loss:0.007, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1247    6    2    0    0    0    0    0    4   26    0    0
     0    0    0]
 [   0    0    0  727    1    0    0    0    0    8    3    4    3    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  857   18    0    0
     0    0    0]
 [   0    0    2    0    0    1    0    0    0    0   18 2175   13    1
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0    6  523    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1130    8    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    59  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.73441734417344

F1 scores:
[       nan 1.         0.98421468 0.98176907 0.99300699 0.99076212
 0.99168556 0.98039216 1.         0.81818182 0.97552647 0.97972973
 0.97302326 0.99730458 0.96829477 0.87559055 0.97619048]

Kappa:
0.9741541758756302
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6b29c80ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.749, val_acc:0.369]
Epoch [2/120    avg_loss:2.536, val_acc:0.482]
Epoch [3/120    avg_loss:2.334, val_acc:0.492]
Epoch [4/120    avg_loss:2.167, val_acc:0.546]
Epoch [5/120    avg_loss:1.998, val_acc:0.567]
Epoch [6/120    avg_loss:1.840, val_acc:0.597]
Epoch [7/120    avg_loss:1.700, val_acc:0.642]
Epoch [8/120    avg_loss:1.523, val_acc:0.628]
Epoch [9/120    avg_loss:1.468, val_acc:0.671]
Epoch [10/120    avg_loss:1.245, val_acc:0.643]
Epoch [11/120    avg_loss:1.213, val_acc:0.698]
Epoch [12/120    avg_loss:1.097, val_acc:0.683]
Epoch [13/120    avg_loss:0.962, val_acc:0.726]
Epoch [14/120    avg_loss:0.816, val_acc:0.774]
Epoch [15/120    avg_loss:0.816, val_acc:0.721]
Epoch [16/120    avg_loss:0.728, val_acc:0.787]
Epoch [17/120    avg_loss:0.621, val_acc:0.799]
Epoch [18/120    avg_loss:0.624, val_acc:0.810]
Epoch [19/120    avg_loss:0.479, val_acc:0.844]
Epoch [20/120    avg_loss:0.532, val_acc:0.817]
Epoch [21/120    avg_loss:0.477, val_acc:0.831]
Epoch [22/120    avg_loss:0.427, val_acc:0.847]
Epoch [23/120    avg_loss:0.397, val_acc:0.842]
Epoch [24/120    avg_loss:0.426, val_acc:0.846]
Epoch [25/120    avg_loss:0.334, val_acc:0.865]
Epoch [26/120    avg_loss:0.349, val_acc:0.792]
Epoch [27/120    avg_loss:0.572, val_acc:0.845]
Epoch [28/120    avg_loss:0.361, val_acc:0.876]
Epoch [29/120    avg_loss:0.371, val_acc:0.857]
Epoch [30/120    avg_loss:0.282, val_acc:0.880]
Epoch [31/120    avg_loss:0.223, val_acc:0.878]
Epoch [32/120    avg_loss:0.189, val_acc:0.915]
Epoch [33/120    avg_loss:0.169, val_acc:0.912]
Epoch [34/120    avg_loss:0.164, val_acc:0.927]
Epoch [35/120    avg_loss:0.154, val_acc:0.931]
Epoch [36/120    avg_loss:0.171, val_acc:0.916]
Epoch [37/120    avg_loss:0.137, val_acc:0.948]
Epoch [38/120    avg_loss:0.125, val_acc:0.930]
Epoch [39/120    avg_loss:0.119, val_acc:0.928]
Epoch [40/120    avg_loss:0.100, val_acc:0.939]
Epoch [41/120    avg_loss:0.103, val_acc:0.947]
Epoch [42/120    avg_loss:0.117, val_acc:0.928]
Epoch [43/120    avg_loss:0.109, val_acc:0.939]
Epoch [44/120    avg_loss:0.115, val_acc:0.951]
Epoch [45/120    avg_loss:0.107, val_acc:0.954]
Epoch [46/120    avg_loss:0.080, val_acc:0.960]
Epoch [47/120    avg_loss:0.099, val_acc:0.919]
Epoch [48/120    avg_loss:0.092, val_acc:0.946]
Epoch [49/120    avg_loss:0.069, val_acc:0.958]
Epoch [50/120    avg_loss:0.052, val_acc:0.958]
Epoch [51/120    avg_loss:0.046, val_acc:0.959]
Epoch [52/120    avg_loss:0.065, val_acc:0.964]
Epoch [53/120    avg_loss:0.053, val_acc:0.965]
Epoch [54/120    avg_loss:0.050, val_acc:0.978]
Epoch [55/120    avg_loss:0.047, val_acc:0.976]
Epoch [56/120    avg_loss:0.038, val_acc:0.968]
Epoch [57/120    avg_loss:0.051, val_acc:0.966]
Epoch [58/120    avg_loss:0.048, val_acc:0.970]
Epoch [59/120    avg_loss:0.039, val_acc:0.967]
Epoch [60/120    avg_loss:0.033, val_acc:0.972]
Epoch [61/120    avg_loss:0.029, val_acc:0.973]
Epoch [62/120    avg_loss:0.030, val_acc:0.976]
Epoch [63/120    avg_loss:0.055, val_acc:0.951]
Epoch [64/120    avg_loss:0.074, val_acc:0.969]
Epoch [65/120    avg_loss:0.062, val_acc:0.965]
Epoch [66/120    avg_loss:0.045, val_acc:0.974]
Epoch [67/120    avg_loss:0.037, val_acc:0.966]
Epoch [68/120    avg_loss:0.027, val_acc:0.975]
Epoch [69/120    avg_loss:0.020, val_acc:0.977]
Epoch [70/120    avg_loss:0.019, val_acc:0.977]
Epoch [71/120    avg_loss:0.024, val_acc:0.974]
Epoch [72/120    avg_loss:0.020, val_acc:0.979]
Epoch [73/120    avg_loss:0.021, val_acc:0.977]
Epoch [74/120    avg_loss:0.019, val_acc:0.977]
Epoch [75/120    avg_loss:0.020, val_acc:0.977]
Epoch [76/120    avg_loss:0.020, val_acc:0.977]
Epoch [77/120    avg_loss:0.019, val_acc:0.977]
Epoch [78/120    avg_loss:0.019, val_acc:0.978]
Epoch [79/120    avg_loss:0.015, val_acc:0.978]
Epoch [80/120    avg_loss:0.017, val_acc:0.979]
Epoch [81/120    avg_loss:0.018, val_acc:0.980]
Epoch [82/120    avg_loss:0.015, val_acc:0.978]
Epoch [83/120    avg_loss:0.019, val_acc:0.978]
Epoch [84/120    avg_loss:0.016, val_acc:0.980]
Epoch [85/120    avg_loss:0.017, val_acc:0.979]
Epoch [86/120    avg_loss:0.021, val_acc:0.980]
Epoch [87/120    avg_loss:0.018, val_acc:0.980]
Epoch [88/120    avg_loss:0.021, val_acc:0.980]
Epoch [89/120    avg_loss:0.016, val_acc:0.979]
Epoch [90/120    avg_loss:0.016, val_acc:0.977]
Epoch [91/120    avg_loss:0.017, val_acc:0.976]
Epoch [92/120    avg_loss:0.019, val_acc:0.978]
Epoch [93/120    avg_loss:0.018, val_acc:0.980]
Epoch [94/120    avg_loss:0.015, val_acc:0.980]
Epoch [95/120    avg_loss:0.015, val_acc:0.979]
Epoch [96/120    avg_loss:0.016, val_acc:0.976]
Epoch [97/120    avg_loss:0.014, val_acc:0.977]
Epoch [98/120    avg_loss:0.015, val_acc:0.976]
Epoch [99/120    avg_loss:0.019, val_acc:0.977]
Epoch [100/120    avg_loss:0.013, val_acc:0.977]
Epoch [101/120    avg_loss:0.019, val_acc:0.978]
Epoch [102/120    avg_loss:0.012, val_acc:0.978]
Epoch [103/120    avg_loss:0.015, val_acc:0.979]
Epoch [104/120    avg_loss:0.014, val_acc:0.978]
Epoch [105/120    avg_loss:0.015, val_acc:0.978]
Epoch [106/120    avg_loss:0.019, val_acc:0.977]
Epoch [107/120    avg_loss:0.016, val_acc:0.979]
Epoch [108/120    avg_loss:0.018, val_acc:0.979]
Epoch [109/120    avg_loss:0.016, val_acc:0.979]
Epoch [110/120    avg_loss:0.016, val_acc:0.978]
Epoch [111/120    avg_loss:0.017, val_acc:0.978]
Epoch [112/120    avg_loss:0.013, val_acc:0.977]
Epoch [113/120    avg_loss:0.015, val_acc:0.977]
Epoch [114/120    avg_loss:0.013, val_acc:0.977]
Epoch [115/120    avg_loss:0.013, val_acc:0.978]
Epoch [116/120    avg_loss:0.013, val_acc:0.978]
Epoch [117/120    avg_loss:0.014, val_acc:0.978]
Epoch [118/120    avg_loss:0.013, val_acc:0.978]
Epoch [119/120    avg_loss:0.017, val_acc:0.977]
Epoch [120/120    avg_loss:0.016, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    2    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1245    5    7    1    0    0    0    0   11   16    0    0
     0    0    0]
 [   0    0    1  733    3    0    0    0    0    2    0    4    3    0
     0    1    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    2    0    1    0    0
     3    0    0]
 [   0    0    1    0    0    0  655    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    0    0    0    0    1  843   22    0    0
     0    1    0]
 [   0    0    7    0    0    0    0    0    0    2   19 2155   21    5
     0    1    0]
 [   0    0    0    2    0    0    0    0    0    0    3    0  520    0
     0    8    1]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    91  256    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.02981029810299

F1 scores:
[       nan 0.975      0.97762073 0.98587761 0.97471264 0.98961938
 0.99847561 1.         1.         0.81818182 0.96287836 0.97754593
 0.96296296 0.98395722 0.95375477 0.81399046 0.98809524]

Kappa:
0.9661300098162874
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f81404d49e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.736, val_acc:0.368]
Epoch [2/120    avg_loss:2.515, val_acc:0.496]
Epoch [3/120    avg_loss:2.324, val_acc:0.541]
Epoch [4/120    avg_loss:2.122, val_acc:0.538]
Epoch [5/120    avg_loss:2.019, val_acc:0.578]
Epoch [6/120    avg_loss:1.891, val_acc:0.543]
Epoch [7/120    avg_loss:1.762, val_acc:0.624]
Epoch [8/120    avg_loss:1.620, val_acc:0.649]
Epoch [9/120    avg_loss:1.524, val_acc:0.650]
Epoch [10/120    avg_loss:1.435, val_acc:0.634]
Epoch [11/120    avg_loss:1.292, val_acc:0.664]
Epoch [12/120    avg_loss:1.180, val_acc:0.728]
Epoch [13/120    avg_loss:1.038, val_acc:0.750]
Epoch [14/120    avg_loss:0.913, val_acc:0.767]
Epoch [15/120    avg_loss:0.833, val_acc:0.766]
Epoch [16/120    avg_loss:0.730, val_acc:0.785]
Epoch [17/120    avg_loss:0.647, val_acc:0.828]
Epoch [18/120    avg_loss:0.626, val_acc:0.829]
Epoch [19/120    avg_loss:0.482, val_acc:0.838]
Epoch [20/120    avg_loss:0.484, val_acc:0.848]
Epoch [21/120    avg_loss:0.385, val_acc:0.849]
Epoch [22/120    avg_loss:0.436, val_acc:0.832]
Epoch [23/120    avg_loss:0.381, val_acc:0.799]
Epoch [24/120    avg_loss:0.299, val_acc:0.863]
Epoch [25/120    avg_loss:0.340, val_acc:0.853]
Epoch [26/120    avg_loss:0.296, val_acc:0.883]
Epoch [27/120    avg_loss:0.260, val_acc:0.885]
Epoch [28/120    avg_loss:0.233, val_acc:0.870]
Epoch [29/120    avg_loss:0.263, val_acc:0.872]
Epoch [30/120    avg_loss:0.188, val_acc:0.891]
Epoch [31/120    avg_loss:0.277, val_acc:0.823]
Epoch [32/120    avg_loss:0.176, val_acc:0.919]
Epoch [33/120    avg_loss:0.186, val_acc:0.888]
Epoch [34/120    avg_loss:0.160, val_acc:0.895]
Epoch [35/120    avg_loss:0.143, val_acc:0.931]
Epoch [36/120    avg_loss:0.165, val_acc:0.897]
Epoch [37/120    avg_loss:0.165, val_acc:0.913]
Epoch [38/120    avg_loss:0.134, val_acc:0.935]
Epoch [39/120    avg_loss:0.157, val_acc:0.920]
Epoch [40/120    avg_loss:0.121, val_acc:0.919]
Epoch [41/120    avg_loss:0.129, val_acc:0.920]
Epoch [42/120    avg_loss:0.104, val_acc:0.939]
Epoch [43/120    avg_loss:0.102, val_acc:0.933]
Epoch [44/120    avg_loss:0.147, val_acc:0.898]
Epoch [45/120    avg_loss:0.129, val_acc:0.939]
Epoch [46/120    avg_loss:0.092, val_acc:0.932]
Epoch [47/120    avg_loss:0.092, val_acc:0.925]
Epoch [48/120    avg_loss:0.070, val_acc:0.942]
Epoch [49/120    avg_loss:0.077, val_acc:0.931]
Epoch [50/120    avg_loss:0.075, val_acc:0.922]
Epoch [51/120    avg_loss:0.054, val_acc:0.946]
Epoch [52/120    avg_loss:0.076, val_acc:0.921]
Epoch [53/120    avg_loss:0.126, val_acc:0.944]
Epoch [54/120    avg_loss:0.086, val_acc:0.914]
Epoch [55/120    avg_loss:0.173, val_acc:0.916]
Epoch [56/120    avg_loss:0.072, val_acc:0.949]
Epoch [57/120    avg_loss:0.044, val_acc:0.943]
Epoch [58/120    avg_loss:0.051, val_acc:0.932]
Epoch [59/120    avg_loss:0.070, val_acc:0.954]
Epoch [60/120    avg_loss:0.068, val_acc:0.942]
Epoch [61/120    avg_loss:0.068, val_acc:0.952]
Epoch [62/120    avg_loss:0.041, val_acc:0.947]
Epoch [63/120    avg_loss:0.057, val_acc:0.950]
Epoch [64/120    avg_loss:0.062, val_acc:0.954]
Epoch [65/120    avg_loss:0.034, val_acc:0.953]
Epoch [66/120    avg_loss:0.060, val_acc:0.939]
Epoch [67/120    avg_loss:0.040, val_acc:0.960]
Epoch [68/120    avg_loss:0.039, val_acc:0.958]
Epoch [69/120    avg_loss:0.031, val_acc:0.954]
Epoch [70/120    avg_loss:0.041, val_acc:0.960]
Epoch [71/120    avg_loss:0.026, val_acc:0.953]
Epoch [72/120    avg_loss:0.029, val_acc:0.961]
Epoch [73/120    avg_loss:0.055, val_acc:0.948]
Epoch [74/120    avg_loss:0.035, val_acc:0.954]
Epoch [75/120    avg_loss:0.047, val_acc:0.963]
Epoch [76/120    avg_loss:0.029, val_acc:0.953]
Epoch [77/120    avg_loss:0.028, val_acc:0.962]
Epoch [78/120    avg_loss:0.019, val_acc:0.964]
Epoch [79/120    avg_loss:0.021, val_acc:0.964]
Epoch [80/120    avg_loss:0.020, val_acc:0.964]
Epoch [81/120    avg_loss:0.018, val_acc:0.965]
Epoch [82/120    avg_loss:0.032, val_acc:0.960]
Epoch [83/120    avg_loss:0.027, val_acc:0.969]
Epoch [84/120    avg_loss:0.021, val_acc:0.962]
Epoch [85/120    avg_loss:0.023, val_acc:0.965]
Epoch [86/120    avg_loss:0.014, val_acc:0.966]
Epoch [87/120    avg_loss:0.044, val_acc:0.962]
Epoch [88/120    avg_loss:0.042, val_acc:0.955]
Epoch [89/120    avg_loss:0.033, val_acc:0.963]
Epoch [90/120    avg_loss:0.024, val_acc:0.955]
Epoch [91/120    avg_loss:0.019, val_acc:0.968]
Epoch [92/120    avg_loss:0.017, val_acc:0.966]
Epoch [93/120    avg_loss:0.015, val_acc:0.964]
Epoch [94/120    avg_loss:0.014, val_acc:0.960]
Epoch [95/120    avg_loss:0.022, val_acc:0.963]
Epoch [96/120    avg_loss:0.036, val_acc:0.955]
Epoch [97/120    avg_loss:0.021, val_acc:0.962]
Epoch [98/120    avg_loss:0.013, val_acc:0.963]
Epoch [99/120    avg_loss:0.014, val_acc:0.961]
Epoch [100/120    avg_loss:0.013, val_acc:0.962]
Epoch [101/120    avg_loss:0.015, val_acc:0.965]
Epoch [102/120    avg_loss:0.015, val_acc:0.964]
Epoch [103/120    avg_loss:0.010, val_acc:0.967]
Epoch [104/120    avg_loss:0.013, val_acc:0.968]
Epoch [105/120    avg_loss:0.013, val_acc:0.969]
Epoch [106/120    avg_loss:0.012, val_acc:0.970]
Epoch [107/120    avg_loss:0.011, val_acc:0.968]
Epoch [108/120    avg_loss:0.010, val_acc:0.967]
Epoch [109/120    avg_loss:0.012, val_acc:0.968]
Epoch [110/120    avg_loss:0.010, val_acc:0.967]
Epoch [111/120    avg_loss:0.011, val_acc:0.968]
Epoch [112/120    avg_loss:0.009, val_acc:0.967]
Epoch [113/120    avg_loss:0.011, val_acc:0.966]
Epoch [114/120    avg_loss:0.009, val_acc:0.969]
Epoch [115/120    avg_loss:0.012, val_acc:0.968]
Epoch [116/120    avg_loss:0.010, val_acc:0.967]
Epoch [117/120    avg_loss:0.009, val_acc:0.968]
Epoch [118/120    avg_loss:0.011, val_acc:0.971]
Epoch [119/120    avg_loss:0.009, val_acc:0.969]
Epoch [120/120    avg_loss:0.009, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   33    0    0    0    0    0    0    0    0    0    0    8    0
     0    0    0]
 [   0    0 1254    1    7    2    0    0    0    0    4   17    0    0
     0    0    0]
 [   0    0    1  730    4    0    1    0    0    0    2    0    9    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    0    1    0    0    1
     6    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   12    3    0    0    0    0    0    0  851    8    1    0
     0    0    0]
 [   0    0   18    0    0    0    4    0    0    1   23 2156    8    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    1    0  528    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    97  248    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
97.10569105691057

F1 scores:
[       nan 0.89189189 0.97587549 0.98316498 0.97011494 0.98842593
 0.99393939 1.         1.         0.97297297 0.96869664 0.98200865
 0.96526508 0.99730458 0.95109612 0.81848185 0.95121951]

Kappa:
0.9669896401286108
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe7153baa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.744, val_acc:0.207]
Epoch [2/120    avg_loss:2.495, val_acc:0.376]
Epoch [3/120    avg_loss:2.294, val_acc:0.466]
Epoch [4/120    avg_loss:2.106, val_acc:0.518]
Epoch [5/120    avg_loss:1.961, val_acc:0.516]
Epoch [6/120    avg_loss:1.877, val_acc:0.567]
Epoch [7/120    avg_loss:1.709, val_acc:0.579]
Epoch [8/120    avg_loss:1.598, val_acc:0.530]
Epoch [9/120    avg_loss:1.503, val_acc:0.615]
Epoch [10/120    avg_loss:1.364, val_acc:0.675]
Epoch [11/120    avg_loss:1.242, val_acc:0.636]
Epoch [12/120    avg_loss:1.128, val_acc:0.688]
Epoch [13/120    avg_loss:0.908, val_acc:0.713]
Epoch [14/120    avg_loss:0.940, val_acc:0.753]
Epoch [15/120    avg_loss:0.792, val_acc:0.804]
Epoch [16/120    avg_loss:0.738, val_acc:0.746]
Epoch [17/120    avg_loss:0.642, val_acc:0.838]
Epoch [18/120    avg_loss:0.584, val_acc:0.797]
Epoch [19/120    avg_loss:0.626, val_acc:0.793]
Epoch [20/120    avg_loss:0.622, val_acc:0.764]
Epoch [21/120    avg_loss:0.577, val_acc:0.773]
Epoch [22/120    avg_loss:0.439, val_acc:0.832]
Epoch [23/120    avg_loss:0.449, val_acc:0.851]
Epoch [24/120    avg_loss:0.313, val_acc:0.803]
Epoch [25/120    avg_loss:0.403, val_acc:0.889]
Epoch [26/120    avg_loss:0.333, val_acc:0.867]
Epoch [27/120    avg_loss:0.322, val_acc:0.893]
Epoch [28/120    avg_loss:0.305, val_acc:0.863]
Epoch [29/120    avg_loss:0.224, val_acc:0.897]
Epoch [30/120    avg_loss:0.210, val_acc:0.924]
Epoch [31/120    avg_loss:0.202, val_acc:0.845]
Epoch [32/120    avg_loss:0.188, val_acc:0.921]
Epoch [33/120    avg_loss:0.187, val_acc:0.914]
Epoch [34/120    avg_loss:0.137, val_acc:0.919]
Epoch [35/120    avg_loss:0.150, val_acc:0.919]
Epoch [36/120    avg_loss:0.116, val_acc:0.942]
Epoch [37/120    avg_loss:0.121, val_acc:0.919]
Epoch [38/120    avg_loss:0.152, val_acc:0.930]
Epoch [39/120    avg_loss:0.134, val_acc:0.931]
Epoch [40/120    avg_loss:0.105, val_acc:0.907]
Epoch [41/120    avg_loss:0.106, val_acc:0.940]
Epoch [42/120    avg_loss:0.091, val_acc:0.929]
Epoch [43/120    avg_loss:0.167, val_acc:0.761]
Epoch [44/120    avg_loss:0.261, val_acc:0.898]
Epoch [45/120    avg_loss:0.140, val_acc:0.934]
Epoch [46/120    avg_loss:0.098, val_acc:0.940]
Epoch [47/120    avg_loss:0.093, val_acc:0.941]
Epoch [48/120    avg_loss:0.116, val_acc:0.926]
Epoch [49/120    avg_loss:0.077, val_acc:0.943]
Epoch [50/120    avg_loss:0.087, val_acc:0.940]
Epoch [51/120    avg_loss:0.101, val_acc:0.925]
Epoch [52/120    avg_loss:0.075, val_acc:0.934]
Epoch [53/120    avg_loss:0.070, val_acc:0.946]
Epoch [54/120    avg_loss:0.052, val_acc:0.950]
Epoch [55/120    avg_loss:0.050, val_acc:0.953]
Epoch [56/120    avg_loss:0.054, val_acc:0.934]
Epoch [57/120    avg_loss:0.054, val_acc:0.938]
Epoch [58/120    avg_loss:0.044, val_acc:0.954]
Epoch [59/120    avg_loss:0.040, val_acc:0.958]
Epoch [60/120    avg_loss:0.046, val_acc:0.964]
Epoch [61/120    avg_loss:0.041, val_acc:0.947]
Epoch [62/120    avg_loss:0.071, val_acc:0.935]
Epoch [63/120    avg_loss:0.061, val_acc:0.945]
Epoch [64/120    avg_loss:0.060, val_acc:0.961]
Epoch [65/120    avg_loss:0.064, val_acc:0.950]
Epoch [66/120    avg_loss:0.079, val_acc:0.961]
Epoch [67/120    avg_loss:0.055, val_acc:0.951]
Epoch [68/120    avg_loss:0.038, val_acc:0.948]
Epoch [69/120    avg_loss:0.030, val_acc:0.958]
Epoch [70/120    avg_loss:0.043, val_acc:0.957]
Epoch [71/120    avg_loss:0.038, val_acc:0.957]
Epoch [72/120    avg_loss:0.038, val_acc:0.945]
Epoch [73/120    avg_loss:0.054, val_acc:0.955]
Epoch [74/120    avg_loss:0.029, val_acc:0.964]
Epoch [75/120    avg_loss:0.022, val_acc:0.966]
Epoch [76/120    avg_loss:0.022, val_acc:0.969]
Epoch [77/120    avg_loss:0.022, val_acc:0.970]
Epoch [78/120    avg_loss:0.020, val_acc:0.971]
Epoch [79/120    avg_loss:0.018, val_acc:0.968]
Epoch [80/120    avg_loss:0.019, val_acc:0.970]
Epoch [81/120    avg_loss:0.021, val_acc:0.967]
Epoch [82/120    avg_loss:0.019, val_acc:0.969]
Epoch [83/120    avg_loss:0.017, val_acc:0.968]
Epoch [84/120    avg_loss:0.017, val_acc:0.968]
Epoch [85/120    avg_loss:0.023, val_acc:0.969]
Epoch [86/120    avg_loss:0.014, val_acc:0.971]
Epoch [87/120    avg_loss:0.017, val_acc:0.969]
Epoch [88/120    avg_loss:0.014, val_acc:0.968]
Epoch [89/120    avg_loss:0.018, val_acc:0.970]
Epoch [90/120    avg_loss:0.018, val_acc:0.968]
Epoch [91/120    avg_loss:0.019, val_acc:0.969]
Epoch [92/120    avg_loss:0.015, val_acc:0.969]
Epoch [93/120    avg_loss:0.016, val_acc:0.970]
Epoch [94/120    avg_loss:0.013, val_acc:0.970]
Epoch [95/120    avg_loss:0.017, val_acc:0.968]
Epoch [96/120    avg_loss:0.016, val_acc:0.971]
Epoch [97/120    avg_loss:0.018, val_acc:0.971]
Epoch [98/120    avg_loss:0.015, val_acc:0.972]
Epoch [99/120    avg_loss:0.020, val_acc:0.970]
Epoch [100/120    avg_loss:0.014, val_acc:0.971]
Epoch [101/120    avg_loss:0.012, val_acc:0.973]
Epoch [102/120    avg_loss:0.022, val_acc:0.973]
Epoch [103/120    avg_loss:0.015, val_acc:0.973]
Epoch [104/120    avg_loss:0.015, val_acc:0.972]
Epoch [105/120    avg_loss:0.016, val_acc:0.970]
Epoch [106/120    avg_loss:0.014, val_acc:0.971]
Epoch [107/120    avg_loss:0.014, val_acc:0.973]
Epoch [108/120    avg_loss:0.017, val_acc:0.972]
Epoch [109/120    avg_loss:0.013, val_acc:0.972]
Epoch [110/120    avg_loss:0.015, val_acc:0.973]
Epoch [111/120    avg_loss:0.013, val_acc:0.973]
Epoch [112/120    avg_loss:0.014, val_acc:0.971]
Epoch [113/120    avg_loss:0.013, val_acc:0.972]
Epoch [114/120    avg_loss:0.012, val_acc:0.971]
Epoch [115/120    avg_loss:0.019, val_acc:0.976]
Epoch [116/120    avg_loss:0.015, val_acc:0.971]
Epoch [117/120    avg_loss:0.015, val_acc:0.971]
Epoch [118/120    avg_loss:0.015, val_acc:0.972]
Epoch [119/120    avg_loss:0.011, val_acc:0.970]
Epoch [120/120    avg_loss:0.012, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1250    8    1    0    0    0    0    0    2   21    0    0
     0    3    0]
 [   0    0    5  711    0    0    0    0    0   10    2   13    6    0
     0    0    0]
 [   0    0    0    1  210    0    2    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    0    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  837   31    0    0
     2    0    0]
 [   0    0    3    0    0    0    0    0    0    0   21 2174   12    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0    4  524    0
     0    4    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    3    0    0    0    0
  1124   12    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    88  254    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.99728997289972

F1 scores:
[       nan 0.98795181 0.9811617  0.96866485 0.99056604 0.99188876
 0.99317665 1.         0.99883586 0.73469388 0.96373057 0.97642039
 0.97307335 1.         0.95173582 0.81935484 0.98809524]

Kappa:
0.9657258226768282
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4f863aeac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.698, val_acc:0.369]
Epoch [2/120    avg_loss:2.481, val_acc:0.394]
Epoch [3/120    avg_loss:2.319, val_acc:0.537]
Epoch [4/120    avg_loss:2.158, val_acc:0.545]
Epoch [5/120    avg_loss:2.036, val_acc:0.572]
Epoch [6/120    avg_loss:1.856, val_acc:0.591]
Epoch [7/120    avg_loss:1.722, val_acc:0.628]
Epoch [8/120    avg_loss:1.555, val_acc:0.633]
Epoch [9/120    avg_loss:1.455, val_acc:0.641]
Epoch [10/120    avg_loss:1.304, val_acc:0.720]
Epoch [11/120    avg_loss:1.202, val_acc:0.700]
Epoch [12/120    avg_loss:1.044, val_acc:0.730]
Epoch [13/120    avg_loss:0.963, val_acc:0.756]
Epoch [14/120    avg_loss:0.787, val_acc:0.775]
Epoch [15/120    avg_loss:0.779, val_acc:0.765]
Epoch [16/120    avg_loss:0.726, val_acc:0.755]
Epoch [17/120    avg_loss:0.614, val_acc:0.792]
Epoch [18/120    avg_loss:0.572, val_acc:0.808]
Epoch [19/120    avg_loss:0.494, val_acc:0.814]
Epoch [20/120    avg_loss:0.477, val_acc:0.817]
Epoch [21/120    avg_loss:0.483, val_acc:0.856]
Epoch [22/120    avg_loss:0.458, val_acc:0.845]
Epoch [23/120    avg_loss:0.364, val_acc:0.893]
Epoch [24/120    avg_loss:0.420, val_acc:0.815]
Epoch [25/120    avg_loss:0.369, val_acc:0.830]
Epoch [26/120    avg_loss:0.295, val_acc:0.903]
Epoch [27/120    avg_loss:0.281, val_acc:0.858]
Epoch [28/120    avg_loss:0.369, val_acc:0.883]
Epoch [29/120    avg_loss:0.308, val_acc:0.856]
Epoch [30/120    avg_loss:0.316, val_acc:0.892]
Epoch [31/120    avg_loss:0.252, val_acc:0.901]
Epoch [32/120    avg_loss:0.333, val_acc:0.773]
Epoch [33/120    avg_loss:0.351, val_acc:0.888]
Epoch [34/120    avg_loss:0.241, val_acc:0.866]
Epoch [35/120    avg_loss:0.203, val_acc:0.889]
Epoch [36/120    avg_loss:0.199, val_acc:0.900]
Epoch [37/120    avg_loss:0.181, val_acc:0.899]
Epoch [38/120    avg_loss:0.157, val_acc:0.919]
Epoch [39/120    avg_loss:0.136, val_acc:0.915]
Epoch [40/120    avg_loss:0.190, val_acc:0.893]
Epoch [41/120    avg_loss:0.157, val_acc:0.911]
Epoch [42/120    avg_loss:0.161, val_acc:0.910]
Epoch [43/120    avg_loss:0.128, val_acc:0.935]
Epoch [44/120    avg_loss:0.137, val_acc:0.940]
Epoch [45/120    avg_loss:0.112, val_acc:0.912]
Epoch [46/120    avg_loss:0.193, val_acc:0.900]
Epoch [47/120    avg_loss:0.145, val_acc:0.928]
Epoch [48/120    avg_loss:0.131, val_acc:0.938]
Epoch [49/120    avg_loss:0.161, val_acc:0.929]
Epoch [50/120    avg_loss:0.117, val_acc:0.941]
Epoch [51/120    avg_loss:0.088, val_acc:0.958]
Epoch [52/120    avg_loss:0.075, val_acc:0.959]
Epoch [53/120    avg_loss:0.076, val_acc:0.947]
Epoch [54/120    avg_loss:0.075, val_acc:0.951]
Epoch [55/120    avg_loss:0.070, val_acc:0.952]
Epoch [56/120    avg_loss:0.097, val_acc:0.949]
Epoch [57/120    avg_loss:0.083, val_acc:0.959]
Epoch [58/120    avg_loss:0.083, val_acc:0.923]
Epoch [59/120    avg_loss:0.088, val_acc:0.947]
Epoch [60/120    avg_loss:0.068, val_acc:0.958]
Epoch [61/120    avg_loss:0.050, val_acc:0.962]
Epoch [62/120    avg_loss:0.048, val_acc:0.960]
Epoch [63/120    avg_loss:0.059, val_acc:0.961]
Epoch [64/120    avg_loss:0.046, val_acc:0.962]
Epoch [65/120    avg_loss:0.061, val_acc:0.967]
Epoch [66/120    avg_loss:0.037, val_acc:0.957]
Epoch [67/120    avg_loss:0.037, val_acc:0.965]
Epoch [68/120    avg_loss:0.068, val_acc:0.949]
Epoch [69/120    avg_loss:0.037, val_acc:0.970]
Epoch [70/120    avg_loss:0.034, val_acc:0.954]
Epoch [71/120    avg_loss:0.028, val_acc:0.971]
Epoch [72/120    avg_loss:0.036, val_acc:0.962]
Epoch [73/120    avg_loss:0.034, val_acc:0.967]
Epoch [74/120    avg_loss:0.022, val_acc:0.970]
Epoch [75/120    avg_loss:0.028, val_acc:0.970]
Epoch [76/120    avg_loss:0.040, val_acc:0.949]
Epoch [77/120    avg_loss:0.038, val_acc:0.966]
Epoch [78/120    avg_loss:0.028, val_acc:0.967]
Epoch [79/120    avg_loss:0.079, val_acc:0.965]
Epoch [80/120    avg_loss:0.038, val_acc:0.956]
Epoch [81/120    avg_loss:0.040, val_acc:0.971]
Epoch [82/120    avg_loss:0.082, val_acc:0.947]
Epoch [83/120    avg_loss:0.041, val_acc:0.966]
Epoch [84/120    avg_loss:0.045, val_acc:0.954]
Epoch [85/120    avg_loss:0.031, val_acc:0.964]
Epoch [86/120    avg_loss:0.030, val_acc:0.959]
Epoch [87/120    avg_loss:0.031, val_acc:0.963]
Epoch [88/120    avg_loss:0.030, val_acc:0.965]
Epoch [89/120    avg_loss:0.035, val_acc:0.944]
Epoch [90/120    avg_loss:0.030, val_acc:0.970]
Epoch [91/120    avg_loss:0.029, val_acc:0.968]
Epoch [92/120    avg_loss:0.165, val_acc:0.902]
Epoch [93/120    avg_loss:0.155, val_acc:0.933]
Epoch [94/120    avg_loss:0.072, val_acc:0.955]
Epoch [95/120    avg_loss:0.050, val_acc:0.961]
Epoch [96/120    avg_loss:0.044, val_acc:0.958]
Epoch [97/120    avg_loss:0.053, val_acc:0.957]
Epoch [98/120    avg_loss:0.053, val_acc:0.958]
Epoch [99/120    avg_loss:0.035, val_acc:0.961]
Epoch [100/120    avg_loss:0.041, val_acc:0.964]
Epoch [101/120    avg_loss:0.032, val_acc:0.962]
Epoch [102/120    avg_loss:0.033, val_acc:0.961]
Epoch [103/120    avg_loss:0.033, val_acc:0.961]
Epoch [104/120    avg_loss:0.041, val_acc:0.962]
Epoch [105/120    avg_loss:0.032, val_acc:0.966]
Epoch [106/120    avg_loss:0.032, val_acc:0.966]
Epoch [107/120    avg_loss:0.026, val_acc:0.966]
Epoch [108/120    avg_loss:0.037, val_acc:0.966]
Epoch [109/120    avg_loss:0.037, val_acc:0.967]
Epoch [110/120    avg_loss:0.032, val_acc:0.965]
Epoch [111/120    avg_loss:0.028, val_acc:0.967]
Epoch [112/120    avg_loss:0.028, val_acc:0.967]
Epoch [113/120    avg_loss:0.028, val_acc:0.966]
Epoch [114/120    avg_loss:0.024, val_acc:0.967]
Epoch [115/120    avg_loss:0.027, val_acc:0.967]
Epoch [116/120    avg_loss:0.027, val_acc:0.968]
Epoch [117/120    avg_loss:0.024, val_acc:0.968]
Epoch [118/120    avg_loss:0.028, val_acc:0.968]
Epoch [119/120    avg_loss:0.027, val_acc:0.968]
Epoch [120/120    avg_loss:0.027, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    1    0    0    0    0    1    0    0
     0    0    0]
 [   0    0 1239    3    7    0    0    0    0    0    2   34    0    0
     0    0    0]
 [   0    0    5  730    4    0    0    0    0    0    0    0    7    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    1    1    0    0    0  833   29    1    0
     0    2    0]
 [   0    0   25    8    0    0    0    0    0    0   16 2152    5    0
     0    3    1]
 [   0    0    0    2    0    0    0    0    0    0    3    0  527    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    74  272    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.07317073170732

F1 scores:
[       nan 0.975      0.96721311 0.97986577 0.97482838 0.99424626
 0.99619772 0.97959184 1.         1.         0.96356275 0.97221595
 0.9795539  1.         0.96027339 0.84867395 0.97619048]

Kappa:
0.9666118421251059
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd0b92fdac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.764, val_acc:0.340]
Epoch [2/120    avg_loss:2.520, val_acc:0.405]
Epoch [3/120    avg_loss:2.330, val_acc:0.469]
Epoch [4/120    avg_loss:2.130, val_acc:0.529]
Epoch [5/120    avg_loss:1.992, val_acc:0.568]
Epoch [6/120    avg_loss:1.834, val_acc:0.572]
Epoch [7/120    avg_loss:1.710, val_acc:0.517]
Epoch [8/120    avg_loss:1.592, val_acc:0.571]
Epoch [9/120    avg_loss:1.426, val_acc:0.602]
Epoch [10/120    avg_loss:1.310, val_acc:0.610]
Epoch [11/120    avg_loss:1.173, val_acc:0.632]
Epoch [12/120    avg_loss:1.089, val_acc:0.730]
Epoch [13/120    avg_loss:0.925, val_acc:0.711]
Epoch [14/120    avg_loss:0.760, val_acc:0.725]
Epoch [15/120    avg_loss:0.828, val_acc:0.707]
Epoch [16/120    avg_loss:0.672, val_acc:0.773]
Epoch [17/120    avg_loss:0.587, val_acc:0.770]
Epoch [18/120    avg_loss:0.594, val_acc:0.778]
Epoch [19/120    avg_loss:0.500, val_acc:0.814]
Epoch [20/120    avg_loss:0.424, val_acc:0.803]
Epoch [21/120    avg_loss:0.405, val_acc:0.787]
Epoch [22/120    avg_loss:0.385, val_acc:0.843]
Epoch [23/120    avg_loss:0.368, val_acc:0.811]
Epoch [24/120    avg_loss:0.292, val_acc:0.839]
Epoch [25/120    avg_loss:0.337, val_acc:0.826]
Epoch [26/120    avg_loss:0.333, val_acc:0.861]
Epoch [27/120    avg_loss:0.280, val_acc:0.861]
Epoch [28/120    avg_loss:0.236, val_acc:0.874]
Epoch [29/120    avg_loss:0.220, val_acc:0.878]
Epoch [30/120    avg_loss:0.207, val_acc:0.889]
Epoch [31/120    avg_loss:0.163, val_acc:0.889]
Epoch [32/120    avg_loss:0.220, val_acc:0.869]
Epoch [33/120    avg_loss:0.160, val_acc:0.878]
Epoch [34/120    avg_loss:0.147, val_acc:0.899]
Epoch [35/120    avg_loss:0.155, val_acc:0.883]
Epoch [36/120    avg_loss:0.187, val_acc:0.893]
Epoch [37/120    avg_loss:0.138, val_acc:0.915]
Epoch [38/120    avg_loss:0.126, val_acc:0.851]
Epoch [39/120    avg_loss:0.139, val_acc:0.912]
Epoch [40/120    avg_loss:0.106, val_acc:0.925]
Epoch [41/120    avg_loss:0.101, val_acc:0.924]
Epoch [42/120    avg_loss:0.159, val_acc:0.875]
Epoch [43/120    avg_loss:0.141, val_acc:0.903]
Epoch [44/120    avg_loss:0.145, val_acc:0.900]
Epoch [45/120    avg_loss:0.141, val_acc:0.916]
Epoch [46/120    avg_loss:0.157, val_acc:0.915]
Epoch [47/120    avg_loss:0.090, val_acc:0.928]
Epoch [48/120    avg_loss:0.162, val_acc:0.915]
Epoch [49/120    avg_loss:0.106, val_acc:0.921]
Epoch [50/120    avg_loss:0.078, val_acc:0.945]
Epoch [51/120    avg_loss:0.066, val_acc:0.948]
Epoch [52/120    avg_loss:0.071, val_acc:0.941]
Epoch [53/120    avg_loss:0.058, val_acc:0.938]
Epoch [54/120    avg_loss:0.079, val_acc:0.922]
Epoch [55/120    avg_loss:0.106, val_acc:0.918]
Epoch [56/120    avg_loss:0.075, val_acc:0.954]
Epoch [57/120    avg_loss:0.057, val_acc:0.943]
Epoch [58/120    avg_loss:0.072, val_acc:0.947]
Epoch [59/120    avg_loss:0.058, val_acc:0.963]
Epoch [60/120    avg_loss:0.043, val_acc:0.948]
Epoch [61/120    avg_loss:0.044, val_acc:0.953]
Epoch [62/120    avg_loss:0.061, val_acc:0.950]
Epoch [63/120    avg_loss:0.054, val_acc:0.948]
Epoch [64/120    avg_loss:0.067, val_acc:0.922]
Epoch [65/120    avg_loss:0.083, val_acc:0.919]
Epoch [66/120    avg_loss:0.066, val_acc:0.935]
Epoch [67/120    avg_loss:0.043, val_acc:0.955]
Epoch [68/120    avg_loss:0.055, val_acc:0.956]
Epoch [69/120    avg_loss:0.039, val_acc:0.957]
Epoch [70/120    avg_loss:0.038, val_acc:0.947]
Epoch [71/120    avg_loss:0.039, val_acc:0.946]
Epoch [72/120    avg_loss:0.042, val_acc:0.952]
Epoch [73/120    avg_loss:0.032, val_acc:0.964]
Epoch [74/120    avg_loss:0.023, val_acc:0.969]
Epoch [75/120    avg_loss:0.027, val_acc:0.967]
Epoch [76/120    avg_loss:0.024, val_acc:0.969]
Epoch [77/120    avg_loss:0.019, val_acc:0.967]
Epoch [78/120    avg_loss:0.029, val_acc:0.967]
Epoch [79/120    avg_loss:0.019, val_acc:0.964]
Epoch [80/120    avg_loss:0.020, val_acc:0.966]
Epoch [81/120    avg_loss:0.018, val_acc:0.963]
Epoch [82/120    avg_loss:0.021, val_acc:0.968]
Epoch [83/120    avg_loss:0.027, val_acc:0.964]
Epoch [84/120    avg_loss:0.020, val_acc:0.967]
Epoch [85/120    avg_loss:0.025, val_acc:0.963]
Epoch [86/120    avg_loss:0.022, val_acc:0.963]
Epoch [87/120    avg_loss:0.020, val_acc:0.963]
Epoch [88/120    avg_loss:0.024, val_acc:0.967]
Epoch [89/120    avg_loss:0.017, val_acc:0.967]
Epoch [90/120    avg_loss:0.020, val_acc:0.967]
Epoch [91/120    avg_loss:0.016, val_acc:0.967]
Epoch [92/120    avg_loss:0.014, val_acc:0.967]
Epoch [93/120    avg_loss:0.020, val_acc:0.967]
Epoch [94/120    avg_loss:0.016, val_acc:0.967]
Epoch [95/120    avg_loss:0.015, val_acc:0.967]
Epoch [96/120    avg_loss:0.018, val_acc:0.966]
Epoch [97/120    avg_loss:0.017, val_acc:0.967]
Epoch [98/120    avg_loss:0.021, val_acc:0.966]
Epoch [99/120    avg_loss:0.022, val_acc:0.966]
Epoch [100/120    avg_loss:0.017, val_acc:0.966]
Epoch [101/120    avg_loss:0.017, val_acc:0.966]
Epoch [102/120    avg_loss:0.017, val_acc:0.967]
Epoch [103/120    avg_loss:0.013, val_acc:0.967]
Epoch [104/120    avg_loss:0.015, val_acc:0.967]
Epoch [105/120    avg_loss:0.019, val_acc:0.967]
Epoch [106/120    avg_loss:0.023, val_acc:0.967]
Epoch [107/120    avg_loss:0.020, val_acc:0.967]
Epoch [108/120    avg_loss:0.019, val_acc:0.967]
Epoch [109/120    avg_loss:0.014, val_acc:0.967]
Epoch [110/120    avg_loss:0.018, val_acc:0.967]
Epoch [111/120    avg_loss:0.016, val_acc:0.967]
Epoch [112/120    avg_loss:0.018, val_acc:0.967]
Epoch [113/120    avg_loss:0.022, val_acc:0.967]
Epoch [114/120    avg_loss:0.021, val_acc:0.967]
Epoch [115/120    avg_loss:0.017, val_acc:0.967]
Epoch [116/120    avg_loss:0.014, val_acc:0.967]
Epoch [117/120    avg_loss:0.015, val_acc:0.967]
Epoch [118/120    avg_loss:0.018, val_acc:0.967]
Epoch [119/120    avg_loss:0.016, val_acc:0.967]
Epoch [120/120    avg_loss:0.017, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1267    3    3    0    0    0    0    0    1   11    0    0
     0    0    0]
 [   0    0    1  731    4    0    0    0    0    0    1    7    3    0
     0    0    0]
 [   0    0    0    3  210    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    0    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    1    0    0    0    0  856   11    0    0
     0    0    0]
 [   0    0   21    0    0    0    1    0    0    0   54 2127    6    0
     0    1    0]
 [   0    0    2    4    0    0    0    0    0    0    0    4  518    0
     0    5    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    1    0    0    0    0
  1125   12    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    44  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.4959349593496

F1 scores:
[       nan 0.975      0.98102981 0.98252688 0.97674419 0.98842593
 0.99468489 1.         1.         0.88235294 0.9569592  0.97345538
 0.97460019 1.         0.97066437 0.90554723 0.98203593]

Kappa:
0.9714630912201092
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fca46503b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.727, val_acc:0.302]
Epoch [2/120    avg_loss:2.464, val_acc:0.474]
Epoch [3/120    avg_loss:2.295, val_acc:0.508]
Epoch [4/120    avg_loss:2.141, val_acc:0.550]
Epoch [5/120    avg_loss:2.029, val_acc:0.587]
Epoch [6/120    avg_loss:1.860, val_acc:0.603]
Epoch [7/120    avg_loss:1.709, val_acc:0.627]
Epoch [8/120    avg_loss:1.548, val_acc:0.666]
Epoch [9/120    avg_loss:1.368, val_acc:0.665]
Epoch [10/120    avg_loss:1.212, val_acc:0.699]
Epoch [11/120    avg_loss:1.138, val_acc:0.696]
Epoch [12/120    avg_loss:0.900, val_acc:0.757]
Epoch [13/120    avg_loss:0.797, val_acc:0.755]
Epoch [14/120    avg_loss:0.790, val_acc:0.772]
Epoch [15/120    avg_loss:0.726, val_acc:0.738]
Epoch [16/120    avg_loss:0.601, val_acc:0.819]
Epoch [17/120    avg_loss:0.633, val_acc:0.736]
Epoch [18/120    avg_loss:0.554, val_acc:0.770]
Epoch [19/120    avg_loss:0.473, val_acc:0.835]
Epoch [20/120    avg_loss:0.419, val_acc:0.779]
Epoch [21/120    avg_loss:0.421, val_acc:0.819]
Epoch [22/120    avg_loss:0.344, val_acc:0.851]
Epoch [23/120    avg_loss:0.305, val_acc:0.881]
Epoch [24/120    avg_loss:0.276, val_acc:0.890]
Epoch [25/120    avg_loss:0.282, val_acc:0.893]
Epoch [26/120    avg_loss:0.240, val_acc:0.870]
Epoch [27/120    avg_loss:0.229, val_acc:0.851]
Epoch [28/120    avg_loss:0.212, val_acc:0.889]
Epoch [29/120    avg_loss:0.191, val_acc:0.905]
Epoch [30/120    avg_loss:0.193, val_acc:0.899]
Epoch [31/120    avg_loss:0.220, val_acc:0.906]
Epoch [32/120    avg_loss:0.185, val_acc:0.912]
Epoch [33/120    avg_loss:0.169, val_acc:0.923]
Epoch [34/120    avg_loss:0.167, val_acc:0.913]
Epoch [35/120    avg_loss:0.149, val_acc:0.910]
Epoch [36/120    avg_loss:0.115, val_acc:0.934]
Epoch [37/120    avg_loss:0.100, val_acc:0.926]
Epoch [38/120    avg_loss:0.111, val_acc:0.932]
Epoch [39/120    avg_loss:0.085, val_acc:0.907]
Epoch [40/120    avg_loss:0.119, val_acc:0.936]
Epoch [41/120    avg_loss:0.105, val_acc:0.940]
Epoch [42/120    avg_loss:0.111, val_acc:0.912]
Epoch [43/120    avg_loss:0.121, val_acc:0.940]
Epoch [44/120    avg_loss:0.084, val_acc:0.934]
Epoch [45/120    avg_loss:0.077, val_acc:0.928]
Epoch [46/120    avg_loss:0.083, val_acc:0.921]
Epoch [47/120    avg_loss:0.097, val_acc:0.952]
Epoch [48/120    avg_loss:0.064, val_acc:0.936]
Epoch [49/120    avg_loss:0.058, val_acc:0.944]
Epoch [50/120    avg_loss:0.047, val_acc:0.949]
Epoch [51/120    avg_loss:0.043, val_acc:0.949]
Epoch [52/120    avg_loss:0.046, val_acc:0.945]
Epoch [53/120    avg_loss:0.047, val_acc:0.951]
Epoch [54/120    avg_loss:0.041, val_acc:0.952]
Epoch [55/120    avg_loss:0.036, val_acc:0.950]
Epoch [56/120    avg_loss:0.035, val_acc:0.955]
Epoch [57/120    avg_loss:0.036, val_acc:0.955]
Epoch [58/120    avg_loss:0.030, val_acc:0.961]
Epoch [59/120    avg_loss:0.032, val_acc:0.953]
Epoch [60/120    avg_loss:0.044, val_acc:0.948]
Epoch [61/120    avg_loss:0.054, val_acc:0.942]
Epoch [62/120    avg_loss:0.081, val_acc:0.933]
Epoch [63/120    avg_loss:0.076, val_acc:0.954]
Epoch [64/120    avg_loss:0.048, val_acc:0.960]
Epoch [65/120    avg_loss:0.056, val_acc:0.944]
Epoch [66/120    avg_loss:0.055, val_acc:0.954]
Epoch [67/120    avg_loss:0.044, val_acc:0.949]
Epoch [68/120    avg_loss:0.055, val_acc:0.945]
Epoch [69/120    avg_loss:0.056, val_acc:0.947]
Epoch [70/120    avg_loss:0.048, val_acc:0.953]
Epoch [71/120    avg_loss:0.433, val_acc:0.548]
Epoch [72/120    avg_loss:1.367, val_acc:0.654]
Epoch [73/120    avg_loss:0.975, val_acc:0.731]
Epoch [74/120    avg_loss:0.692, val_acc:0.789]
Epoch [75/120    avg_loss:0.437, val_acc:0.863]
Epoch [76/120    avg_loss:0.295, val_acc:0.887]
Epoch [77/120    avg_loss:0.225, val_acc:0.907]
Epoch [78/120    avg_loss:0.190, val_acc:0.910]
Epoch [79/120    avg_loss:0.176, val_acc:0.919]
Epoch [80/120    avg_loss:0.138, val_acc:0.924]
Epoch [81/120    avg_loss:0.110, val_acc:0.924]
Epoch [82/120    avg_loss:0.124, val_acc:0.929]
Epoch [83/120    avg_loss:0.098, val_acc:0.938]
Epoch [84/120    avg_loss:0.089, val_acc:0.941]
Epoch [85/120    avg_loss:0.085, val_acc:0.943]
Epoch [86/120    avg_loss:0.080, val_acc:0.942]
Epoch [87/120    avg_loss:0.078, val_acc:0.942]
Epoch [88/120    avg_loss:0.077, val_acc:0.941]
Epoch [89/120    avg_loss:0.076, val_acc:0.943]
Epoch [90/120    avg_loss:0.078, val_acc:0.944]
Epoch [91/120    avg_loss:0.082, val_acc:0.944]
Epoch [92/120    avg_loss:0.084, val_acc:0.943]
Epoch [93/120    avg_loss:0.075, val_acc:0.944]
Epoch [94/120    avg_loss:0.075, val_acc:0.945]
Epoch [95/120    avg_loss:0.081, val_acc:0.945]
Epoch [96/120    avg_loss:0.068, val_acc:0.944]
Epoch [97/120    avg_loss:0.084, val_acc:0.945]
Epoch [98/120    avg_loss:0.076, val_acc:0.945]
Epoch [99/120    avg_loss:0.077, val_acc:0.945]
Epoch [100/120    avg_loss:0.092, val_acc:0.945]
Epoch [101/120    avg_loss:0.074, val_acc:0.945]
Epoch [102/120    avg_loss:0.067, val_acc:0.945]
Epoch [103/120    avg_loss:0.071, val_acc:0.945]
Epoch [104/120    avg_loss:0.077, val_acc:0.945]
Epoch [105/120    avg_loss:0.077, val_acc:0.944]
Epoch [106/120    avg_loss:0.082, val_acc:0.944]
Epoch [107/120    avg_loss:0.075, val_acc:0.944]
Epoch [108/120    avg_loss:0.072, val_acc:0.944]
Epoch [109/120    avg_loss:0.081, val_acc:0.944]
Epoch [110/120    avg_loss:0.072, val_acc:0.944]
Epoch [111/120    avg_loss:0.079, val_acc:0.944]
Epoch [112/120    avg_loss:0.069, val_acc:0.944]
Epoch [113/120    avg_loss:0.104, val_acc:0.944]
Epoch [114/120    avg_loss:0.070, val_acc:0.944]
Epoch [115/120    avg_loss:0.086, val_acc:0.944]
Epoch [116/120    avg_loss:0.106, val_acc:0.944]
Epoch [117/120    avg_loss:0.078, val_acc:0.944]
Epoch [118/120    avg_loss:0.071, val_acc:0.944]
Epoch [119/120    avg_loss:0.071, val_acc:0.944]
Epoch [120/120    avg_loss:0.073, val_acc:0.944]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    1    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0 1231    6    3    0    1    0    0    0    8   34    2    0
     0    0    0]
 [   0    0    4  724    0    1    0    0    0    0    0   15    2    0
     0    1    0]
 [   0    0    3    1  201    0    0    0    0    0    0    0    8    0
     0    0    0]
 [   0    0    0    1    0  426    0    7    0    0    1    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    0    0
     0    2    0]
 [   0    0    0    1    0    0    0    0    0   15    0    2    0    0
     0    0    0]
 [   0    0   15    0    4    0    0    0    0    0  825   25    5    0
     0    1    0]
 [   0    0   21    8    9    1    1    1    0    0   59 2053   45    1
     1    5    5]
 [   0    0    1    9    0    2    0    0    0    0    5    0  510    0
     0    2    5]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    1    0
  1122   15    0]
 [   0    0    0    0    0    1    1    0    0    0    0    0    0    0
    83  262    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.1869918699187

F1 scores:
[       nan 0.96202532 0.96134323 0.96726787 0.93488372 0.98043728
 0.9946687  0.86206897 0.997669   0.90909091 0.93062606 0.94586501
 0.92140921 0.99459459 0.95570698 0.82519685 0.94382022]

Kappa:
0.9451889767898422
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:18:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f35b5015a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.767, val_acc:0.182]
Epoch [2/120    avg_loss:2.524, val_acc:0.316]
Epoch [3/120    avg_loss:2.344, val_acc:0.539]
Epoch [4/120    avg_loss:2.175, val_acc:0.569]
Epoch [5/120    avg_loss:1.997, val_acc:0.573]
Epoch [6/120    avg_loss:1.822, val_acc:0.580]
Epoch [7/120    avg_loss:1.638, val_acc:0.613]
Epoch [8/120    avg_loss:1.446, val_acc:0.614]
Epoch [9/120    avg_loss:1.366, val_acc:0.639]
Epoch [10/120    avg_loss:1.220, val_acc:0.692]
Epoch [11/120    avg_loss:1.058, val_acc:0.681]
Epoch [12/120    avg_loss:0.956, val_acc:0.732]
Epoch [13/120    avg_loss:0.826, val_acc:0.792]
Epoch [14/120    avg_loss:0.735, val_acc:0.776]
Epoch [15/120    avg_loss:0.681, val_acc:0.783]
Epoch [16/120    avg_loss:0.661, val_acc:0.784]
Epoch [17/120    avg_loss:0.562, val_acc:0.827]
Epoch [18/120    avg_loss:0.499, val_acc:0.863]
Epoch [19/120    avg_loss:0.450, val_acc:0.849]
Epoch [20/120    avg_loss:0.417, val_acc:0.886]
Epoch [21/120    avg_loss:0.317, val_acc:0.893]
Epoch [22/120    avg_loss:0.332, val_acc:0.808]
Epoch [23/120    avg_loss:0.357, val_acc:0.881]
Epoch [24/120    avg_loss:0.431, val_acc:0.798]
Epoch [25/120    avg_loss:0.263, val_acc:0.894]
Epoch [26/120    avg_loss:0.262, val_acc:0.907]
Epoch [27/120    avg_loss:0.242, val_acc:0.902]
Epoch [28/120    avg_loss:0.238, val_acc:0.892]
Epoch [29/120    avg_loss:0.221, val_acc:0.904]
Epoch [30/120    avg_loss:0.174, val_acc:0.915]
Epoch [31/120    avg_loss:0.147, val_acc:0.943]
Epoch [32/120    avg_loss:0.138, val_acc:0.942]
Epoch [33/120    avg_loss:0.120, val_acc:0.906]
Epoch [34/120    avg_loss:0.140, val_acc:0.939]
Epoch [35/120    avg_loss:0.112, val_acc:0.930]
Epoch [36/120    avg_loss:0.100, val_acc:0.920]
Epoch [37/120    avg_loss:0.129, val_acc:0.935]
Epoch [38/120    avg_loss:0.150, val_acc:0.939]
Epoch [39/120    avg_loss:0.108, val_acc:0.929]
Epoch [40/120    avg_loss:0.153, val_acc:0.928]
Epoch [41/120    avg_loss:0.120, val_acc:0.939]
Epoch [42/120    avg_loss:0.156, val_acc:0.936]
Epoch [43/120    avg_loss:0.123, val_acc:0.940]
Epoch [44/120    avg_loss:0.088, val_acc:0.963]
Epoch [45/120    avg_loss:0.078, val_acc:0.949]
Epoch [46/120    avg_loss:0.076, val_acc:0.957]
Epoch [47/120    avg_loss:0.068, val_acc:0.945]
Epoch [48/120    avg_loss:0.052, val_acc:0.944]
Epoch [49/120    avg_loss:0.042, val_acc:0.947]
Epoch [50/120    avg_loss:0.057, val_acc:0.957]
Epoch [51/120    avg_loss:0.058, val_acc:0.961]
Epoch [52/120    avg_loss:0.030, val_acc:0.963]
Epoch [53/120    avg_loss:0.039, val_acc:0.965]
Epoch [54/120    avg_loss:0.034, val_acc:0.968]
Epoch [55/120    avg_loss:0.043, val_acc:0.924]
Epoch [56/120    avg_loss:0.055, val_acc:0.958]
Epoch [57/120    avg_loss:0.126, val_acc:0.952]
Epoch [58/120    avg_loss:0.121, val_acc:0.943]
Epoch [59/120    avg_loss:0.097, val_acc:0.921]
Epoch [60/120    avg_loss:0.093, val_acc:0.942]
Epoch [61/120    avg_loss:0.074, val_acc:0.961]
Epoch [62/120    avg_loss:0.043, val_acc:0.960]
Epoch [63/120    avg_loss:0.047, val_acc:0.963]
Epoch [64/120    avg_loss:0.034, val_acc:0.966]
Epoch [65/120    avg_loss:0.040, val_acc:0.957]
Epoch [66/120    avg_loss:0.056, val_acc:0.959]
Epoch [67/120    avg_loss:0.036, val_acc:0.968]
Epoch [68/120    avg_loss:0.068, val_acc:0.956]
Epoch [69/120    avg_loss:0.049, val_acc:0.975]
Epoch [70/120    avg_loss:0.036, val_acc:0.978]
Epoch [71/120    avg_loss:0.025, val_acc:0.970]
Epoch [72/120    avg_loss:0.036, val_acc:0.973]
Epoch [73/120    avg_loss:0.039, val_acc:0.973]
Epoch [74/120    avg_loss:0.028, val_acc:0.969]
Epoch [75/120    avg_loss:0.032, val_acc:0.952]
Epoch [76/120    avg_loss:0.030, val_acc:0.961]
Epoch [77/120    avg_loss:0.025, val_acc:0.969]
Epoch [78/120    avg_loss:0.027, val_acc:0.968]
Epoch [79/120    avg_loss:0.023, val_acc:0.979]
Epoch [80/120    avg_loss:0.018, val_acc:0.965]
Epoch [81/120    avg_loss:0.057, val_acc:0.951]
Epoch [82/120    avg_loss:0.043, val_acc:0.956]
Epoch [83/120    avg_loss:0.033, val_acc:0.957]
Epoch [84/120    avg_loss:0.023, val_acc:0.963]
Epoch [85/120    avg_loss:0.022, val_acc:0.960]
Epoch [86/120    avg_loss:0.025, val_acc:0.970]
Epoch [87/120    avg_loss:0.016, val_acc:0.966]
Epoch [88/120    avg_loss:0.026, val_acc:0.971]
Epoch [89/120    avg_loss:0.024, val_acc:0.969]
Epoch [90/120    avg_loss:0.019, val_acc:0.974]
Epoch [91/120    avg_loss:0.019, val_acc:0.971]
Epoch [92/120    avg_loss:0.018, val_acc:0.981]
Epoch [93/120    avg_loss:0.012, val_acc:0.973]
Epoch [94/120    avg_loss:0.026, val_acc:0.970]
Epoch [95/120    avg_loss:0.021, val_acc:0.976]
Epoch [96/120    avg_loss:0.013, val_acc:0.977]
Epoch [97/120    avg_loss:0.014, val_acc:0.979]
Epoch [98/120    avg_loss:0.018, val_acc:0.975]
Epoch [99/120    avg_loss:0.011, val_acc:0.970]
Epoch [100/120    avg_loss:0.015, val_acc:0.974]
Epoch [101/120    avg_loss:0.012, val_acc:0.972]
Epoch [102/120    avg_loss:0.015, val_acc:0.976]
Epoch [103/120    avg_loss:0.017, val_acc:0.978]
Epoch [104/120    avg_loss:0.010, val_acc:0.981]
Epoch [105/120    avg_loss:0.008, val_acc:0.980]
Epoch [106/120    avg_loss:0.008, val_acc:0.982]
Epoch [107/120    avg_loss:0.013, val_acc:0.971]
Epoch [108/120    avg_loss:0.011, val_acc:0.982]
Epoch [109/120    avg_loss:0.012, val_acc:0.975]
Epoch [110/120    avg_loss:0.012, val_acc:0.979]
Epoch [111/120    avg_loss:0.009, val_acc:0.980]
Epoch [112/120    avg_loss:0.008, val_acc:0.976]
Epoch [113/120    avg_loss:0.006, val_acc:0.980]
Epoch [114/120    avg_loss:0.006, val_acc:0.981]
Epoch [115/120    avg_loss:0.014, val_acc:0.973]
Epoch [116/120    avg_loss:0.017, val_acc:0.975]
Epoch [117/120    avg_loss:0.012, val_acc:0.976]
Epoch [118/120    avg_loss:0.008, val_acc:0.973]
Epoch [119/120    avg_loss:0.016, val_acc:0.975]
Epoch [120/120    avg_loss:0.008, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1259    3    6    0    0    0    0    0    1   16    0    0
     0    0    0]
 [   0    0    1  722   10    1    0    0    0    3    3    5    1    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    2    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   24    1    0    1    0    0    0    0  810   38    1    0
     0    0    0]
 [   0    0   37    0    0    0    2    0    0    1   10 2147   12    1
     0    0    0]
 [   0    0    0    9    0    0    0    0    0    2   10    4  505    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1123   15    0]
 [   0    0    0    0    0    1   15    0    0    0    0    0    0    1
    35  295    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.85636856368563

F1 scores:
[       nan 0.95121951 0.96623177 0.97435897 0.9638009  0.99078341
 0.982009   1.         0.997669   0.8        0.94681473 0.97105382
 0.95825427 0.9919571  0.97567333 0.89802131 0.98224852]

Kappa:
0.964145269256262
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc926512a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.741, val_acc:0.471]
Epoch [2/120    avg_loss:2.480, val_acc:0.489]
Epoch [3/120    avg_loss:2.289, val_acc:0.484]
Epoch [4/120    avg_loss:2.142, val_acc:0.532]
Epoch [5/120    avg_loss:1.958, val_acc:0.571]
Epoch [6/120    avg_loss:1.828, val_acc:0.603]
Epoch [7/120    avg_loss:1.685, val_acc:0.606]
Epoch [8/120    avg_loss:1.602, val_acc:0.641]
Epoch [9/120    avg_loss:1.379, val_acc:0.669]
Epoch [10/120    avg_loss:1.244, val_acc:0.672]
Epoch [11/120    avg_loss:1.164, val_acc:0.703]
Epoch [12/120    avg_loss:1.006, val_acc:0.749]
Epoch [13/120    avg_loss:0.971, val_acc:0.710]
Epoch [14/120    avg_loss:0.865, val_acc:0.767]
Epoch [15/120    avg_loss:0.691, val_acc:0.794]
Epoch [16/120    avg_loss:0.687, val_acc:0.793]
Epoch [17/120    avg_loss:0.704, val_acc:0.781]
Epoch [18/120    avg_loss:0.641, val_acc:0.795]
Epoch [19/120    avg_loss:0.476, val_acc:0.860]
Epoch [20/120    avg_loss:0.427, val_acc:0.867]
Epoch [21/120    avg_loss:0.419, val_acc:0.843]
Epoch [22/120    avg_loss:0.417, val_acc:0.864]
Epoch [23/120    avg_loss:0.386, val_acc:0.852]
Epoch [24/120    avg_loss:0.386, val_acc:0.850]
Epoch [25/120    avg_loss:0.443, val_acc:0.842]
Epoch [26/120    avg_loss:0.356, val_acc:0.883]
Epoch [27/120    avg_loss:0.288, val_acc:0.884]
Epoch [28/120    avg_loss:0.285, val_acc:0.901]
Epoch [29/120    avg_loss:0.342, val_acc:0.875]
Epoch [30/120    avg_loss:0.274, val_acc:0.886]
Epoch [31/120    avg_loss:0.219, val_acc:0.902]
Epoch [32/120    avg_loss:0.198, val_acc:0.922]
Epoch [33/120    avg_loss:0.163, val_acc:0.936]
Epoch [34/120    avg_loss:0.172, val_acc:0.919]
Epoch [35/120    avg_loss:0.219, val_acc:0.921]
Epoch [36/120    avg_loss:0.160, val_acc:0.908]
Epoch [37/120    avg_loss:0.169, val_acc:0.917]
Epoch [38/120    avg_loss:0.134, val_acc:0.938]
Epoch [39/120    avg_loss:0.129, val_acc:0.939]
Epoch [40/120    avg_loss:0.167, val_acc:0.933]
Epoch [41/120    avg_loss:0.148, val_acc:0.933]
Epoch [42/120    avg_loss:0.128, val_acc:0.908]
Epoch [43/120    avg_loss:0.099, val_acc:0.945]
Epoch [44/120    avg_loss:0.119, val_acc:0.945]
Epoch [45/120    avg_loss:0.103, val_acc:0.956]
Epoch [46/120    avg_loss:0.073, val_acc:0.942]
Epoch [47/120    avg_loss:0.078, val_acc:0.950]
Epoch [48/120    avg_loss:0.063, val_acc:0.958]
Epoch [49/120    avg_loss:0.073, val_acc:0.955]
Epoch [50/120    avg_loss:0.069, val_acc:0.963]
Epoch [51/120    avg_loss:0.060, val_acc:0.945]
Epoch [52/120    avg_loss:0.069, val_acc:0.954]
Epoch [53/120    avg_loss:0.069, val_acc:0.964]
Epoch [54/120    avg_loss:0.065, val_acc:0.955]
Epoch [55/120    avg_loss:0.058, val_acc:0.960]
Epoch [56/120    avg_loss:0.053, val_acc:0.959]
Epoch [57/120    avg_loss:0.047, val_acc:0.966]
Epoch [58/120    avg_loss:0.041, val_acc:0.955]
Epoch [59/120    avg_loss:0.044, val_acc:0.966]
Epoch [60/120    avg_loss:0.037, val_acc:0.968]
Epoch [61/120    avg_loss:0.038, val_acc:0.966]
Epoch [62/120    avg_loss:0.031, val_acc:0.971]
Epoch [63/120    avg_loss:0.035, val_acc:0.972]
Epoch [64/120    avg_loss:0.028, val_acc:0.969]
Epoch [65/120    avg_loss:0.034, val_acc:0.973]
Epoch [66/120    avg_loss:0.036, val_acc:0.970]
Epoch [67/120    avg_loss:0.040, val_acc:0.965]
Epoch [68/120    avg_loss:0.065, val_acc:0.963]
Epoch [69/120    avg_loss:0.058, val_acc:0.974]
Epoch [70/120    avg_loss:0.065, val_acc:0.966]
Epoch [71/120    avg_loss:0.046, val_acc:0.967]
Epoch [72/120    avg_loss:0.031, val_acc:0.966]
Epoch [73/120    avg_loss:0.032, val_acc:0.961]
Epoch [74/120    avg_loss:0.027, val_acc:0.972]
Epoch [75/120    avg_loss:0.022, val_acc:0.969]
Epoch [76/120    avg_loss:0.022, val_acc:0.977]
Epoch [77/120    avg_loss:0.023, val_acc:0.972]
Epoch [78/120    avg_loss:0.023, val_acc:0.968]
Epoch [79/120    avg_loss:0.027, val_acc:0.969]
Epoch [80/120    avg_loss:0.040, val_acc:0.963]
Epoch [81/120    avg_loss:0.027, val_acc:0.975]
Epoch [82/120    avg_loss:0.019, val_acc:0.976]
Epoch [83/120    avg_loss:0.016, val_acc:0.967]
Epoch [84/120    avg_loss:0.017, val_acc:0.966]
Epoch [85/120    avg_loss:0.020, val_acc:0.971]
Epoch [86/120    avg_loss:0.023, val_acc:0.977]
Epoch [87/120    avg_loss:0.021, val_acc:0.973]
Epoch [88/120    avg_loss:0.032, val_acc:0.967]
Epoch [89/120    avg_loss:0.022, val_acc:0.971]
Epoch [90/120    avg_loss:0.051, val_acc:0.960]
Epoch [91/120    avg_loss:0.043, val_acc:0.975]
Epoch [92/120    avg_loss:0.022, val_acc:0.973]
Epoch [93/120    avg_loss:0.019, val_acc:0.976]
Epoch [94/120    avg_loss:0.018, val_acc:0.973]
Epoch [95/120    avg_loss:0.019, val_acc:0.978]
Epoch [96/120    avg_loss:0.075, val_acc:0.953]
Epoch [97/120    avg_loss:0.044, val_acc:0.975]
Epoch [98/120    avg_loss:0.027, val_acc:0.974]
Epoch [99/120    avg_loss:0.021, val_acc:0.975]
Epoch [100/120    avg_loss:0.016, val_acc:0.977]
Epoch [101/120    avg_loss:0.015, val_acc:0.975]
Epoch [102/120    avg_loss:0.017, val_acc:0.981]
Epoch [103/120    avg_loss:0.011, val_acc:0.979]
Epoch [104/120    avg_loss:0.010, val_acc:0.982]
Epoch [105/120    avg_loss:0.017, val_acc:0.974]
Epoch [106/120    avg_loss:0.023, val_acc:0.963]
Epoch [107/120    avg_loss:0.039, val_acc:0.965]
Epoch [108/120    avg_loss:0.030, val_acc:0.978]
Epoch [109/120    avg_loss:0.012, val_acc:0.980]
Epoch [110/120    avg_loss:0.013, val_acc:0.979]
Epoch [111/120    avg_loss:0.011, val_acc:0.978]
Epoch [112/120    avg_loss:0.011, val_acc:0.976]
Epoch [113/120    avg_loss:0.315, val_acc:0.781]
Epoch [114/120    avg_loss:0.424, val_acc:0.902]
Epoch [115/120    avg_loss:0.480, val_acc:0.918]
Epoch [116/120    avg_loss:0.219, val_acc:0.909]
Epoch [117/120    avg_loss:0.212, val_acc:0.916]
Epoch [118/120    avg_loss:0.127, val_acc:0.950]
Epoch [119/120    avg_loss:0.096, val_acc:0.951]
Epoch [120/120    avg_loss:0.074, val_acc:0.949]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    1    0    1    0
     0    0    0]
 [   0    0 1215    2    4    0    1    0    0    0   18   38    7    0
     0    0    0]
 [   0    0    2  711    1    0    0    0    0   11   11    0   11    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    1  430    0    3    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  649    0    0    0    0    1    0    0
     7    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   13    0    0    6    5    0    0    0  783   64    2    0
     1    1    0]
 [   0    0   10   11    0    1    2    1    0    0   68 2082   27    0
     0    3    5]
 [   0    0    0    8    1    0    0    0    0    1    0    2  516    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    0    0    0    0
  1124    8    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    63  261    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.99186991869918

F1 scores:
[       nan 0.975      0.96237624 0.96146045 0.98148148 0.97838453
 0.97083022 0.92592593 0.99883586 0.75       0.89179954 0.94700932
 0.93818182 1.         0.9627409  0.84193548 0.93854749]

Kappa:
0.9429312454975981
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f583c3e6ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.760, val_acc:0.252]
Epoch [2/120    avg_loss:2.558, val_acc:0.297]
Epoch [3/120    avg_loss:2.368, val_acc:0.446]
Epoch [4/120    avg_loss:2.183, val_acc:0.530]
Epoch [5/120    avg_loss:2.048, val_acc:0.548]
Epoch [6/120    avg_loss:1.868, val_acc:0.606]
Epoch [7/120    avg_loss:1.692, val_acc:0.623]
Epoch [8/120    avg_loss:1.587, val_acc:0.659]
Epoch [9/120    avg_loss:1.463, val_acc:0.694]
Epoch [10/120    avg_loss:1.298, val_acc:0.668]
Epoch [11/120    avg_loss:1.186, val_acc:0.695]
Epoch [12/120    avg_loss:1.009, val_acc:0.744]
Epoch [13/120    avg_loss:0.974, val_acc:0.739]
Epoch [14/120    avg_loss:0.859, val_acc:0.784]
Epoch [15/120    avg_loss:0.704, val_acc:0.781]
Epoch [16/120    avg_loss:0.661, val_acc:0.803]
Epoch [17/120    avg_loss:0.587, val_acc:0.826]
Epoch [18/120    avg_loss:0.628, val_acc:0.820]
Epoch [19/120    avg_loss:0.477, val_acc:0.805]
Epoch [20/120    avg_loss:0.473, val_acc:0.865]
Epoch [21/120    avg_loss:0.385, val_acc:0.864]
Epoch [22/120    avg_loss:0.388, val_acc:0.848]
Epoch [23/120    avg_loss:0.404, val_acc:0.881]
Epoch [24/120    avg_loss:0.299, val_acc:0.882]
Epoch [25/120    avg_loss:0.310, val_acc:0.901]
Epoch [26/120    avg_loss:0.250, val_acc:0.892]
Epoch [27/120    avg_loss:0.277, val_acc:0.905]
Epoch [28/120    avg_loss:0.217, val_acc:0.894]
Epoch [29/120    avg_loss:0.224, val_acc:0.916]
Epoch [30/120    avg_loss:0.247, val_acc:0.928]
Epoch [31/120    avg_loss:0.178, val_acc:0.896]
Epoch [32/120    avg_loss:0.164, val_acc:0.919]
Epoch [33/120    avg_loss:0.211, val_acc:0.914]
Epoch [34/120    avg_loss:0.227, val_acc:0.919]
Epoch [35/120    avg_loss:0.176, val_acc:0.912]
Epoch [36/120    avg_loss:0.185, val_acc:0.918]
Epoch [37/120    avg_loss:0.170, val_acc:0.903]
Epoch [38/120    avg_loss:0.163, val_acc:0.930]
Epoch [39/120    avg_loss:0.143, val_acc:0.942]
Epoch [40/120    avg_loss:0.131, val_acc:0.935]
Epoch [41/120    avg_loss:0.114, val_acc:0.940]
Epoch [42/120    avg_loss:0.102, val_acc:0.960]
Epoch [43/120    avg_loss:0.091, val_acc:0.955]
Epoch [44/120    avg_loss:0.081, val_acc:0.951]
Epoch [45/120    avg_loss:0.073, val_acc:0.955]
Epoch [46/120    avg_loss:0.089, val_acc:0.942]
Epoch [47/120    avg_loss:0.119, val_acc:0.934]
Epoch [48/120    avg_loss:0.072, val_acc:0.960]
Epoch [49/120    avg_loss:0.092, val_acc:0.944]
Epoch [50/120    avg_loss:0.089, val_acc:0.954]
Epoch [51/120    avg_loss:0.072, val_acc:0.957]
Epoch [52/120    avg_loss:0.079, val_acc:0.947]
Epoch [53/120    avg_loss:0.072, val_acc:0.958]
Epoch [54/120    avg_loss:0.061, val_acc:0.964]
Epoch [55/120    avg_loss:0.088, val_acc:0.948]
Epoch [56/120    avg_loss:0.058, val_acc:0.960]
Epoch [57/120    avg_loss:0.051, val_acc:0.949]
Epoch [58/120    avg_loss:0.063, val_acc:0.934]
Epoch [59/120    avg_loss:0.070, val_acc:0.964]
Epoch [60/120    avg_loss:0.052, val_acc:0.963]
Epoch [61/120    avg_loss:0.045, val_acc:0.968]
Epoch [62/120    avg_loss:0.053, val_acc:0.958]
Epoch [63/120    avg_loss:0.037, val_acc:0.966]
Epoch [64/120    avg_loss:0.049, val_acc:0.961]
Epoch [65/120    avg_loss:0.040, val_acc:0.964]
Epoch [66/120    avg_loss:0.069, val_acc:0.950]
Epoch [67/120    avg_loss:0.050, val_acc:0.971]
Epoch [68/120    avg_loss:0.041, val_acc:0.973]
Epoch [69/120    avg_loss:0.029, val_acc:0.969]
Epoch [70/120    avg_loss:0.031, val_acc:0.973]
Epoch [71/120    avg_loss:0.029, val_acc:0.978]
Epoch [72/120    avg_loss:0.034, val_acc:0.969]
Epoch [73/120    avg_loss:0.025, val_acc:0.971]
Epoch [74/120    avg_loss:0.030, val_acc:0.971]
Epoch [75/120    avg_loss:0.033, val_acc:0.966]
Epoch [76/120    avg_loss:0.064, val_acc:0.971]
Epoch [77/120    avg_loss:0.028, val_acc:0.976]
Epoch [78/120    avg_loss:0.030, val_acc:0.970]
Epoch [79/120    avg_loss:0.025, val_acc:0.972]
Epoch [80/120    avg_loss:0.026, val_acc:0.975]
Epoch [81/120    avg_loss:0.025, val_acc:0.978]
Epoch [82/120    avg_loss:0.021, val_acc:0.978]
Epoch [83/120    avg_loss:0.025, val_acc:0.976]
Epoch [84/120    avg_loss:0.029, val_acc:0.950]
Epoch [85/120    avg_loss:0.030, val_acc:0.975]
Epoch [86/120    avg_loss:0.035, val_acc:0.964]
Epoch [87/120    avg_loss:0.022, val_acc:0.970]
Epoch [88/120    avg_loss:0.026, val_acc:0.965]
Epoch [89/120    avg_loss:0.037, val_acc:0.966]
Epoch [90/120    avg_loss:0.038, val_acc:0.960]
Epoch [91/120    avg_loss:0.034, val_acc:0.958]
Epoch [92/120    avg_loss:0.028, val_acc:0.969]
Epoch [93/120    avg_loss:0.041, val_acc:0.930]
Epoch [94/120    avg_loss:0.315, val_acc:0.907]
Epoch [95/120    avg_loss:0.130, val_acc:0.920]
Epoch [96/120    avg_loss:0.055, val_acc:0.954]
Epoch [97/120    avg_loss:0.052, val_acc:0.961]
Epoch [98/120    avg_loss:0.057, val_acc:0.965]
Epoch [99/120    avg_loss:0.035, val_acc:0.967]
Epoch [100/120    avg_loss:0.036, val_acc:0.971]
Epoch [101/120    avg_loss:0.032, val_acc:0.971]
Epoch [102/120    avg_loss:0.038, val_acc:0.970]
Epoch [103/120    avg_loss:0.028, val_acc:0.971]
Epoch [104/120    avg_loss:0.036, val_acc:0.972]
Epoch [105/120    avg_loss:0.035, val_acc:0.976]
Epoch [106/120    avg_loss:0.024, val_acc:0.975]
Epoch [107/120    avg_loss:0.031, val_acc:0.977]
Epoch [108/120    avg_loss:0.027, val_acc:0.976]
Epoch [109/120    avg_loss:0.024, val_acc:0.977]
Epoch [110/120    avg_loss:0.031, val_acc:0.976]
Epoch [111/120    avg_loss:0.024, val_acc:0.977]
Epoch [112/120    avg_loss:0.027, val_acc:0.977]
Epoch [113/120    avg_loss:0.031, val_acc:0.976]
Epoch [114/120    avg_loss:0.031, val_acc:0.975]
Epoch [115/120    avg_loss:0.033, val_acc:0.976]
Epoch [116/120    avg_loss:0.028, val_acc:0.976]
Epoch [117/120    avg_loss:0.023, val_acc:0.976]
Epoch [118/120    avg_loss:0.029, val_acc:0.976]
Epoch [119/120    avg_loss:0.027, val_acc:0.975]
Epoch [120/120    avg_loss:0.023, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    0    2    0    1    0    0    0    3   12    7    0
     0    0    0]
 [   0    0    1  722    0    0    1    0    0    6    3    4    9    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    1    2    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    0    1    0    0    1  847   17    0    0
     1    0    0]
 [   0    0   10    1    0    0    1    0    0    1   13 2174    8    0
     0    2    0]
 [   0    0    0    2    4    0    0    0    0    0    4    4  516    0
     1    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    3    0    0    0    0
  1131    4    0]
 [   0    0    0    0    0    0   30    0    0    0    0    0    0    0
    19  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.84281842818429

F1 scores:
[       nan 0.98765432 0.98245614 0.98097826 0.98611111 0.99538106
 0.97100372 0.96153846 0.99883856 0.76595745 0.97077364 0.98282098
 0.96       0.99730458 0.98648059 0.91411043 0.98224852]

Kappa:
0.9754033158143104
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8ecf560ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.756, val_acc:0.279]
Epoch [2/120    avg_loss:2.520, val_acc:0.472]
Epoch [3/120    avg_loss:2.323, val_acc:0.529]
Epoch [4/120    avg_loss:2.189, val_acc:0.547]
Epoch [5/120    avg_loss:2.036, val_acc:0.564]
Epoch [6/120    avg_loss:1.860, val_acc:0.585]
Epoch [7/120    avg_loss:1.763, val_acc:0.595]
Epoch [8/120    avg_loss:1.607, val_acc:0.643]
Epoch [9/120    avg_loss:1.416, val_acc:0.638]
Epoch [10/120    avg_loss:1.346, val_acc:0.668]
Epoch [11/120    avg_loss:1.188, val_acc:0.708]
Epoch [12/120    avg_loss:1.105, val_acc:0.727]
Epoch [13/120    avg_loss:0.973, val_acc:0.771]
Epoch [14/120    avg_loss:0.858, val_acc:0.721]
Epoch [15/120    avg_loss:0.795, val_acc:0.776]
Epoch [16/120    avg_loss:0.651, val_acc:0.758]
Epoch [17/120    avg_loss:0.665, val_acc:0.812]
Epoch [18/120    avg_loss:0.584, val_acc:0.796]
Epoch [19/120    avg_loss:0.509, val_acc:0.832]
Epoch [20/120    avg_loss:0.477, val_acc:0.786]
Epoch [21/120    avg_loss:0.453, val_acc:0.811]
Epoch [22/120    avg_loss:0.466, val_acc:0.836]
Epoch [23/120    avg_loss:0.420, val_acc:0.841]
Epoch [24/120    avg_loss:0.373, val_acc:0.841]
Epoch [25/120    avg_loss:0.352, val_acc:0.862]
Epoch [26/120    avg_loss:0.310, val_acc:0.876]
Epoch [27/120    avg_loss:0.307, val_acc:0.879]
Epoch [28/120    avg_loss:0.291, val_acc:0.871]
Epoch [29/120    avg_loss:0.286, val_acc:0.880]
Epoch [30/120    avg_loss:0.239, val_acc:0.915]
Epoch [31/120    avg_loss:0.211, val_acc:0.870]
Epoch [32/120    avg_loss:0.301, val_acc:0.873]
Epoch [33/120    avg_loss:0.253, val_acc:0.887]
Epoch [34/120    avg_loss:0.207, val_acc:0.900]
Epoch [35/120    avg_loss:0.156, val_acc:0.910]
Epoch [36/120    avg_loss:0.153, val_acc:0.929]
Epoch [37/120    avg_loss:0.144, val_acc:0.880]
Epoch [38/120    avg_loss:0.293, val_acc:0.859]
Epoch [39/120    avg_loss:0.258, val_acc:0.907]
Epoch [40/120    avg_loss:0.166, val_acc:0.917]
Epoch [41/120    avg_loss:0.142, val_acc:0.922]
Epoch [42/120    avg_loss:0.115, val_acc:0.911]
Epoch [43/120    avg_loss:0.106, val_acc:0.915]
Epoch [44/120    avg_loss:0.123, val_acc:0.896]
Epoch [45/120    avg_loss:0.110, val_acc:0.932]
Epoch [46/120    avg_loss:0.119, val_acc:0.871]
Epoch [47/120    avg_loss:0.133, val_acc:0.925]
Epoch [48/120    avg_loss:0.138, val_acc:0.932]
Epoch [49/120    avg_loss:0.088, val_acc:0.933]
Epoch [50/120    avg_loss:0.079, val_acc:0.932]
Epoch [51/120    avg_loss:0.108, val_acc:0.930]
Epoch [52/120    avg_loss:0.089, val_acc:0.946]
Epoch [53/120    avg_loss:0.073, val_acc:0.949]
Epoch [54/120    avg_loss:0.081, val_acc:0.914]
Epoch [55/120    avg_loss:0.056, val_acc:0.943]
Epoch [56/120    avg_loss:0.076, val_acc:0.925]
Epoch [57/120    avg_loss:0.060, val_acc:0.936]
Epoch [58/120    avg_loss:0.047, val_acc:0.938]
Epoch [59/120    avg_loss:0.045, val_acc:0.946]
Epoch [60/120    avg_loss:0.063, val_acc:0.940]
Epoch [61/120    avg_loss:0.068, val_acc:0.946]
Epoch [62/120    avg_loss:0.060, val_acc:0.950]
Epoch [63/120    avg_loss:0.058, val_acc:0.952]
Epoch [64/120    avg_loss:0.042, val_acc:0.957]
Epoch [65/120    avg_loss:0.072, val_acc:0.932]
Epoch [66/120    avg_loss:0.085, val_acc:0.938]
Epoch [67/120    avg_loss:0.099, val_acc:0.935]
Epoch [68/120    avg_loss:0.054, val_acc:0.958]
Epoch [69/120    avg_loss:0.043, val_acc:0.966]
Epoch [70/120    avg_loss:0.058, val_acc:0.945]
Epoch [71/120    avg_loss:0.043, val_acc:0.944]
Epoch [72/120    avg_loss:0.027, val_acc:0.952]
Epoch [73/120    avg_loss:0.034, val_acc:0.947]
Epoch [74/120    avg_loss:0.035, val_acc:0.958]
Epoch [75/120    avg_loss:0.029, val_acc:0.958]
Epoch [76/120    avg_loss:0.031, val_acc:0.949]
Epoch [77/120    avg_loss:0.026, val_acc:0.959]
Epoch [78/120    avg_loss:0.025, val_acc:0.954]
Epoch [79/120    avg_loss:0.017, val_acc:0.954]
Epoch [80/120    avg_loss:0.030, val_acc:0.949]
Epoch [81/120    avg_loss:0.035, val_acc:0.965]
Epoch [82/120    avg_loss:0.024, val_acc:0.953]
Epoch [83/120    avg_loss:0.020, val_acc:0.959]
Epoch [84/120    avg_loss:0.015, val_acc:0.962]
Epoch [85/120    avg_loss:0.017, val_acc:0.967]
Epoch [86/120    avg_loss:0.012, val_acc:0.966]
Epoch [87/120    avg_loss:0.012, val_acc:0.966]
Epoch [88/120    avg_loss:0.015, val_acc:0.969]
Epoch [89/120    avg_loss:0.014, val_acc:0.968]
Epoch [90/120    avg_loss:0.013, val_acc:0.964]
Epoch [91/120    avg_loss:0.012, val_acc:0.965]
Epoch [92/120    avg_loss:0.018, val_acc:0.966]
Epoch [93/120    avg_loss:0.013, val_acc:0.966]
Epoch [94/120    avg_loss:0.012, val_acc:0.967]
Epoch [95/120    avg_loss:0.013, val_acc:0.967]
Epoch [96/120    avg_loss:0.012, val_acc:0.966]
Epoch [97/120    avg_loss:0.013, val_acc:0.966]
Epoch [98/120    avg_loss:0.011, val_acc:0.962]
Epoch [99/120    avg_loss:0.017, val_acc:0.968]
Epoch [100/120    avg_loss:0.012, val_acc:0.966]
Epoch [101/120    avg_loss:0.013, val_acc:0.963]
Epoch [102/120    avg_loss:0.012, val_acc:0.963]
Epoch [103/120    avg_loss:0.013, val_acc:0.963]
Epoch [104/120    avg_loss:0.017, val_acc:0.964]
Epoch [105/120    avg_loss:0.018, val_acc:0.964]
Epoch [106/120    avg_loss:0.011, val_acc:0.963]
Epoch [107/120    avg_loss:0.012, val_acc:0.962]
Epoch [108/120    avg_loss:0.016, val_acc:0.963]
Epoch [109/120    avg_loss:0.018, val_acc:0.964]
Epoch [110/120    avg_loss:0.013, val_acc:0.963]
Epoch [111/120    avg_loss:0.021, val_acc:0.963]
Epoch [112/120    avg_loss:0.012, val_acc:0.964]
Epoch [113/120    avg_loss:0.013, val_acc:0.964]
Epoch [114/120    avg_loss:0.011, val_acc:0.963]
Epoch [115/120    avg_loss:0.013, val_acc:0.963]
Epoch [116/120    avg_loss:0.017, val_acc:0.963]
Epoch [117/120    avg_loss:0.012, val_acc:0.963]
Epoch [118/120    avg_loss:0.010, val_acc:0.963]
Epoch [119/120    avg_loss:0.011, val_acc:0.963]
Epoch [120/120    avg_loss:0.012, val_acc:0.963]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1266    0    2    0    1    0    0    0    3   11    2    0
     0    0    0]
 [   0    0    0  714    5    0    0    0    0    2    1   13   10    1
     1    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  431    0    3    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    2    3    0    0    0  851   14    0    0
     0    0    0]
 [   0    0   13    0    0    0    1    1    0    0   17 2175    3    0
     0    0    0]
 [   0    0    0    1    0    1    0    0    0    0    0    1  527    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    1    0    0    0    0    0    0
  1122   16    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    54  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.79945799457994

F1 scores:
[       nan 0.975      0.98483081 0.97674419 0.98148148 0.99194476
 0.99168556 0.90909091 1.         0.94736842 0.97424156 0.98327306
 0.97773655 0.99730458 0.96807593 0.8820827  0.98224852]

Kappa:
0.9749026195370336
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd05dec0ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.757, val_acc:0.183]
Epoch [2/120    avg_loss:2.559, val_acc:0.405]
Epoch [3/120    avg_loss:2.355, val_acc:0.425]
Epoch [4/120    avg_loss:2.162, val_acc:0.456]
Epoch [5/120    avg_loss:1.974, val_acc:0.510]
Epoch [6/120    avg_loss:1.777, val_acc:0.552]
Epoch [7/120    avg_loss:1.571, val_acc:0.607]
Epoch [8/120    avg_loss:1.461, val_acc:0.645]
Epoch [9/120    avg_loss:1.268, val_acc:0.635]
Epoch [10/120    avg_loss:1.092, val_acc:0.688]
Epoch [11/120    avg_loss:1.000, val_acc:0.690]
Epoch [12/120    avg_loss:0.968, val_acc:0.700]
Epoch [13/120    avg_loss:0.872, val_acc:0.704]
Epoch [14/120    avg_loss:0.832, val_acc:0.736]
Epoch [15/120    avg_loss:0.729, val_acc:0.767]
Epoch [16/120    avg_loss:0.659, val_acc:0.720]
Epoch [17/120    avg_loss:0.591, val_acc:0.790]
Epoch [18/120    avg_loss:0.568, val_acc:0.798]
Epoch [19/120    avg_loss:0.511, val_acc:0.815]
Epoch [20/120    avg_loss:0.446, val_acc:0.823]
Epoch [21/120    avg_loss:0.402, val_acc:0.809]
Epoch [22/120    avg_loss:0.388, val_acc:0.864]
Epoch [23/120    avg_loss:0.418, val_acc:0.856]
Epoch [24/120    avg_loss:0.353, val_acc:0.870]
Epoch [25/120    avg_loss:0.290, val_acc:0.870]
Epoch [26/120    avg_loss:0.301, val_acc:0.883]
Epoch [27/120    avg_loss:0.314, val_acc:0.885]
Epoch [28/120    avg_loss:0.263, val_acc:0.841]
Epoch [29/120    avg_loss:0.228, val_acc:0.895]
Epoch [30/120    avg_loss:0.247, val_acc:0.840]
Epoch [31/120    avg_loss:0.302, val_acc:0.850]
Epoch [32/120    avg_loss:0.254, val_acc:0.881]
Epoch [33/120    avg_loss:0.184, val_acc:0.914]
Epoch [34/120    avg_loss:0.259, val_acc:0.901]
Epoch [35/120    avg_loss:0.181, val_acc:0.912]
Epoch [36/120    avg_loss:0.163, val_acc:0.930]
Epoch [37/120    avg_loss:0.170, val_acc:0.921]
Epoch [38/120    avg_loss:0.139, val_acc:0.930]
Epoch [39/120    avg_loss:0.114, val_acc:0.936]
Epoch [40/120    avg_loss:0.402, val_acc:0.633]
Epoch [41/120    avg_loss:0.641, val_acc:0.779]
Epoch [42/120    avg_loss:0.325, val_acc:0.877]
Epoch [43/120    avg_loss:0.325, val_acc:0.889]
Epoch [44/120    avg_loss:0.183, val_acc:0.900]
Epoch [45/120    avg_loss:0.170, val_acc:0.922]
Epoch [46/120    avg_loss:0.174, val_acc:0.899]
Epoch [47/120    avg_loss:0.140, val_acc:0.896]
Epoch [48/120    avg_loss:0.138, val_acc:0.936]
Epoch [49/120    avg_loss:0.132, val_acc:0.924]
Epoch [50/120    avg_loss:0.124, val_acc:0.931]
Epoch [51/120    avg_loss:0.102, val_acc:0.927]
Epoch [52/120    avg_loss:0.113, val_acc:0.936]
Epoch [53/120    avg_loss:0.085, val_acc:0.945]
Epoch [54/120    avg_loss:0.075, val_acc:0.954]
Epoch [55/120    avg_loss:0.070, val_acc:0.949]
Epoch [56/120    avg_loss:0.065, val_acc:0.949]
Epoch [57/120    avg_loss:0.085, val_acc:0.942]
Epoch [58/120    avg_loss:0.086, val_acc:0.921]
Epoch [59/120    avg_loss:0.077, val_acc:0.948]
Epoch [60/120    avg_loss:0.070, val_acc:0.938]
Epoch [61/120    avg_loss:0.075, val_acc:0.948]
Epoch [62/120    avg_loss:0.085, val_acc:0.953]
Epoch [63/120    avg_loss:0.053, val_acc:0.952]
Epoch [64/120    avg_loss:0.039, val_acc:0.959]
Epoch [65/120    avg_loss:0.037, val_acc:0.958]
Epoch [66/120    avg_loss:0.041, val_acc:0.957]
Epoch [67/120    avg_loss:0.047, val_acc:0.964]
Epoch [68/120    avg_loss:0.036, val_acc:0.954]
Epoch [69/120    avg_loss:0.047, val_acc:0.936]
Epoch [70/120    avg_loss:0.048, val_acc:0.966]
Epoch [71/120    avg_loss:0.042, val_acc:0.947]
Epoch [72/120    avg_loss:0.041, val_acc:0.948]
Epoch [73/120    avg_loss:0.030, val_acc:0.951]
Epoch [74/120    avg_loss:0.028, val_acc:0.966]
Epoch [75/120    avg_loss:0.027, val_acc:0.948]
Epoch [76/120    avg_loss:0.027, val_acc:0.966]
Epoch [77/120    avg_loss:0.070, val_acc:0.956]
Epoch [78/120    avg_loss:0.052, val_acc:0.968]
Epoch [79/120    avg_loss:0.034, val_acc:0.964]
Epoch [80/120    avg_loss:0.034, val_acc:0.974]
Epoch [81/120    avg_loss:0.043, val_acc:0.963]
Epoch [82/120    avg_loss:0.024, val_acc:0.967]
Epoch [83/120    avg_loss:0.051, val_acc:0.951]
Epoch [84/120    avg_loss:0.028, val_acc:0.950]
Epoch [85/120    avg_loss:0.029, val_acc:0.969]
Epoch [86/120    avg_loss:0.026, val_acc:0.970]
Epoch [87/120    avg_loss:0.043, val_acc:0.952]
Epoch [88/120    avg_loss:0.036, val_acc:0.965]
Epoch [89/120    avg_loss:0.021, val_acc:0.968]
Epoch [90/120    avg_loss:0.017, val_acc:0.965]
Epoch [91/120    avg_loss:0.021, val_acc:0.969]
Epoch [92/120    avg_loss:0.015, val_acc:0.964]
Epoch [93/120    avg_loss:0.022, val_acc:0.963]
Epoch [94/120    avg_loss:0.012, val_acc:0.971]
Epoch [95/120    avg_loss:0.014, val_acc:0.972]
Epoch [96/120    avg_loss:0.013, val_acc:0.974]
Epoch [97/120    avg_loss:0.011, val_acc:0.974]
Epoch [98/120    avg_loss:0.014, val_acc:0.975]
Epoch [99/120    avg_loss:0.017, val_acc:0.976]
Epoch [100/120    avg_loss:0.011, val_acc:0.978]
Epoch [101/120    avg_loss:0.010, val_acc:0.977]
Epoch [102/120    avg_loss:0.009, val_acc:0.977]
Epoch [103/120    avg_loss:0.011, val_acc:0.977]
Epoch [104/120    avg_loss:0.016, val_acc:0.975]
Epoch [105/120    avg_loss:0.013, val_acc:0.977]
Epoch [106/120    avg_loss:0.009, val_acc:0.975]
Epoch [107/120    avg_loss:0.008, val_acc:0.975]
Epoch [108/120    avg_loss:0.009, val_acc:0.976]
Epoch [109/120    avg_loss:0.011, val_acc:0.976]
Epoch [110/120    avg_loss:0.011, val_acc:0.976]
Epoch [111/120    avg_loss:0.013, val_acc:0.977]
Epoch [112/120    avg_loss:0.010, val_acc:0.977]
Epoch [113/120    avg_loss:0.008, val_acc:0.976]
Epoch [114/120    avg_loss:0.011, val_acc:0.976]
Epoch [115/120    avg_loss:0.010, val_acc:0.976]
Epoch [116/120    avg_loss:0.017, val_acc:0.976]
Epoch [117/120    avg_loss:0.011, val_acc:0.977]
Epoch [118/120    avg_loss:0.009, val_acc:0.977]
Epoch [119/120    avg_loss:0.009, val_acc:0.977]
Epoch [120/120    avg_loss:0.011, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1243    3    9    0    0    0    0    0    6   24    0    0
     0    0    0]
 [   0    0    2  717    5   14    0    0    0    2    1    2    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    2  652    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    1    0    0    0    2  849   15    1    0
     0    0    0]
 [   0    0    1    0    0    0    0    2    0    0   14 2183   10    0
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0  528    0
     3    0    2]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1133    6    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    59  287    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.76693766937669

F1 scores:
[       nan 1.         0.97951143 0.97750511 0.96818182 0.97627119
 0.99541985 0.96153846 1.         0.9        0.97250859 0.98422002
 0.98050139 0.99728997 0.96920445 0.896875   0.98823529]

Kappa:
0.9745301555198475
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa3f08e6a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.753, val_acc:0.227]
Epoch [2/120    avg_loss:2.510, val_acc:0.379]
Epoch [3/120    avg_loss:2.334, val_acc:0.457]
Epoch [4/120    avg_loss:2.159, val_acc:0.454]
Epoch [5/120    avg_loss:2.026, val_acc:0.562]
Epoch [6/120    avg_loss:1.871, val_acc:0.584]
Epoch [7/120    avg_loss:1.736, val_acc:0.612]
Epoch [8/120    avg_loss:1.589, val_acc:0.653]
Epoch [9/120    avg_loss:1.523, val_acc:0.661]
Epoch [10/120    avg_loss:1.440, val_acc:0.709]
Epoch [11/120    avg_loss:1.314, val_acc:0.678]
Epoch [12/120    avg_loss:1.127, val_acc:0.724]
Epoch [13/120    avg_loss:1.040, val_acc:0.761]
Epoch [14/120    avg_loss:0.943, val_acc:0.736]
Epoch [15/120    avg_loss:0.826, val_acc:0.761]
Epoch [16/120    avg_loss:0.837, val_acc:0.736]
Epoch [17/120    avg_loss:0.755, val_acc:0.801]
Epoch [18/120    avg_loss:0.641, val_acc:0.805]
Epoch [19/120    avg_loss:0.520, val_acc:0.828]
Epoch [20/120    avg_loss:0.543, val_acc:0.814]
Epoch [21/120    avg_loss:0.441, val_acc:0.864]
Epoch [22/120    avg_loss:0.467, val_acc:0.854]
Epoch [23/120    avg_loss:0.423, val_acc:0.894]
Epoch [24/120    avg_loss:0.353, val_acc:0.900]
Epoch [25/120    avg_loss:0.264, val_acc:0.909]
Epoch [26/120    avg_loss:0.254, val_acc:0.887]
Epoch [27/120    avg_loss:0.235, val_acc:0.931]
Epoch [28/120    avg_loss:0.190, val_acc:0.896]
Epoch [29/120    avg_loss:0.263, val_acc:0.911]
Epoch [30/120    avg_loss:0.218, val_acc:0.911]
Epoch [31/120    avg_loss:0.226, val_acc:0.895]
Epoch [32/120    avg_loss:0.190, val_acc:0.912]
Epoch [33/120    avg_loss:0.180, val_acc:0.915]
Epoch [34/120    avg_loss:0.184, val_acc:0.912]
Epoch [35/120    avg_loss:0.196, val_acc:0.915]
Epoch [36/120    avg_loss:0.149, val_acc:0.946]
Epoch [37/120    avg_loss:0.116, val_acc:0.954]
Epoch [38/120    avg_loss:0.117, val_acc:0.951]
Epoch [39/120    avg_loss:0.091, val_acc:0.950]
Epoch [40/120    avg_loss:0.122, val_acc:0.929]
Epoch [41/120    avg_loss:0.105, val_acc:0.944]
Epoch [42/120    avg_loss:0.082, val_acc:0.957]
Epoch [43/120    avg_loss:0.099, val_acc:0.953]
Epoch [44/120    avg_loss:0.085, val_acc:0.943]
Epoch [45/120    avg_loss:0.099, val_acc:0.941]
Epoch [46/120    avg_loss:0.066, val_acc:0.961]
Epoch [47/120    avg_loss:0.081, val_acc:0.960]
Epoch [48/120    avg_loss:0.083, val_acc:0.954]
Epoch [49/120    avg_loss:0.067, val_acc:0.972]
Epoch [50/120    avg_loss:0.081, val_acc:0.950]
Epoch [51/120    avg_loss:0.079, val_acc:0.943]
Epoch [52/120    avg_loss:0.079, val_acc:0.943]
Epoch [53/120    avg_loss:0.067, val_acc:0.957]
Epoch [54/120    avg_loss:0.052, val_acc:0.964]
Epoch [55/120    avg_loss:0.082, val_acc:0.958]
Epoch [56/120    avg_loss:0.060, val_acc:0.966]
Epoch [57/120    avg_loss:0.091, val_acc:0.958]
Epoch [58/120    avg_loss:0.082, val_acc:0.956]
Epoch [59/120    avg_loss:0.062, val_acc:0.969]
Epoch [60/120    avg_loss:0.052, val_acc:0.949]
Epoch [61/120    avg_loss:0.052, val_acc:0.960]
Epoch [62/120    avg_loss:0.058, val_acc:0.960]
Epoch [63/120    avg_loss:0.035, val_acc:0.966]
Epoch [64/120    avg_loss:0.037, val_acc:0.968]
Epoch [65/120    avg_loss:0.028, val_acc:0.971]
Epoch [66/120    avg_loss:0.023, val_acc:0.972]
Epoch [67/120    avg_loss:0.023, val_acc:0.972]
Epoch [68/120    avg_loss:0.023, val_acc:0.974]
Epoch [69/120    avg_loss:0.026, val_acc:0.974]
Epoch [70/120    avg_loss:0.024, val_acc:0.975]
Epoch [71/120    avg_loss:0.020, val_acc:0.977]
Epoch [72/120    avg_loss:0.023, val_acc:0.976]
Epoch [73/120    avg_loss:0.020, val_acc:0.977]
Epoch [74/120    avg_loss:0.021, val_acc:0.978]
Epoch [75/120    avg_loss:0.019, val_acc:0.977]
Epoch [76/120    avg_loss:0.017, val_acc:0.977]
Epoch [77/120    avg_loss:0.019, val_acc:0.978]
Epoch [78/120    avg_loss:0.019, val_acc:0.975]
Epoch [79/120    avg_loss:0.020, val_acc:0.977]
Epoch [80/120    avg_loss:0.018, val_acc:0.978]
Epoch [81/120    avg_loss:0.020, val_acc:0.978]
Epoch [82/120    avg_loss:0.021, val_acc:0.976]
Epoch [83/120    avg_loss:0.028, val_acc:0.975]
Epoch [84/120    avg_loss:0.022, val_acc:0.979]
Epoch [85/120    avg_loss:0.017, val_acc:0.978]
Epoch [86/120    avg_loss:0.022, val_acc:0.978]
Epoch [87/120    avg_loss:0.018, val_acc:0.979]
Epoch [88/120    avg_loss:0.015, val_acc:0.979]
Epoch [89/120    avg_loss:0.019, val_acc:0.980]
Epoch [90/120    avg_loss:0.018, val_acc:0.979]
Epoch [91/120    avg_loss:0.016, val_acc:0.978]
Epoch [92/120    avg_loss:0.024, val_acc:0.979]
Epoch [93/120    avg_loss:0.020, val_acc:0.979]
Epoch [94/120    avg_loss:0.016, val_acc:0.981]
Epoch [95/120    avg_loss:0.017, val_acc:0.977]
Epoch [96/120    avg_loss:0.020, val_acc:0.980]
Epoch [97/120    avg_loss:0.020, val_acc:0.979]
Epoch [98/120    avg_loss:0.018, val_acc:0.980]
Epoch [99/120    avg_loss:0.016, val_acc:0.980]
Epoch [100/120    avg_loss:0.017, val_acc:0.980]
Epoch [101/120    avg_loss:0.016, val_acc:0.980]
Epoch [102/120    avg_loss:0.016, val_acc:0.980]
Epoch [103/120    avg_loss:0.019, val_acc:0.977]
Epoch [104/120    avg_loss:0.015, val_acc:0.979]
Epoch [105/120    avg_loss:0.013, val_acc:0.979]
Epoch [106/120    avg_loss:0.012, val_acc:0.980]
Epoch [107/120    avg_loss:0.014, val_acc:0.981]
Epoch [108/120    avg_loss:0.022, val_acc:0.981]
Epoch [109/120    avg_loss:0.018, val_acc:0.981]
Epoch [110/120    avg_loss:0.016, val_acc:0.982]
Epoch [111/120    avg_loss:0.016, val_acc:0.982]
Epoch [112/120    avg_loss:0.015, val_acc:0.982]
Epoch [113/120    avg_loss:0.014, val_acc:0.981]
Epoch [114/120    avg_loss:0.014, val_acc:0.982]
Epoch [115/120    avg_loss:0.018, val_acc:0.981]
Epoch [116/120    avg_loss:0.014, val_acc:0.983]
Epoch [117/120    avg_loss:0.014, val_acc:0.981]
Epoch [118/120    avg_loss:0.013, val_acc:0.979]
Epoch [119/120    avg_loss:0.016, val_acc:0.979]
Epoch [120/120    avg_loss:0.018, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1249    0    8    0    0    0    0    0    3   25    0    0
     0    0    0]
 [   0    0    0  705   13    9    0    0    0    3    3   10    4    0
     0    0    0]
 [   0    0    0    0  209    0    0    0    0    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    1    0    0    0    1  848   16    0    0
     0    0    0]
 [   0    0    6    0    0    0    2    0    0    1   18 2163   20    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    2    5  521    0
     2    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    49  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.36585365853658

F1 scores:
[       nan 1.         0.97999215 0.96973865 0.94356659 0.98405467
 0.99018868 1.         1.         0.87804878 0.96969697 0.97608303
 0.9621422  0.99728997 0.97327586 0.89302326 0.98823529]

Kappa:
0.9699594755949007
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe6ee6f2a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.738, val_acc:0.297]
Epoch [2/120    avg_loss:2.505, val_acc:0.485]
Epoch [3/120    avg_loss:2.311, val_acc:0.506]
Epoch [4/120    avg_loss:2.170, val_acc:0.527]
Epoch [5/120    avg_loss:2.029, val_acc:0.556]
Epoch [6/120    avg_loss:1.923, val_acc:0.571]
Epoch [7/120    avg_loss:1.771, val_acc:0.597]
Epoch [8/120    avg_loss:1.652, val_acc:0.608]
Epoch [9/120    avg_loss:1.617, val_acc:0.623]
Epoch [10/120    avg_loss:1.466, val_acc:0.632]
Epoch [11/120    avg_loss:1.332, val_acc:0.689]
Epoch [12/120    avg_loss:1.257, val_acc:0.650]
Epoch [13/120    avg_loss:1.138, val_acc:0.698]
Epoch [14/120    avg_loss:1.058, val_acc:0.665]
Epoch [15/120    avg_loss:0.955, val_acc:0.768]
Epoch [16/120    avg_loss:0.811, val_acc:0.755]
Epoch [17/120    avg_loss:0.720, val_acc:0.783]
Epoch [18/120    avg_loss:0.640, val_acc:0.799]
Epoch [19/120    avg_loss:0.606, val_acc:0.786]
Epoch [20/120    avg_loss:0.545, val_acc:0.798]
Epoch [21/120    avg_loss:0.483, val_acc:0.805]
Epoch [22/120    avg_loss:0.513, val_acc:0.836]
Epoch [23/120    avg_loss:0.455, val_acc:0.798]
Epoch [24/120    avg_loss:0.391, val_acc:0.847]
Epoch [25/120    avg_loss:0.343, val_acc:0.877]
Epoch [26/120    avg_loss:0.289, val_acc:0.877]
Epoch [27/120    avg_loss:0.281, val_acc:0.865]
Epoch [28/120    avg_loss:0.234, val_acc:0.883]
Epoch [29/120    avg_loss:0.248, val_acc:0.872]
Epoch [30/120    avg_loss:0.424, val_acc:0.775]
Epoch [31/120    avg_loss:0.338, val_acc:0.847]
Epoch [32/120    avg_loss:0.239, val_acc:0.845]
Epoch [33/120    avg_loss:0.319, val_acc:0.864]
Epoch [34/120    avg_loss:0.217, val_acc:0.897]
Epoch [35/120    avg_loss:0.174, val_acc:0.905]
Epoch [36/120    avg_loss:0.165, val_acc:0.919]
Epoch [37/120    avg_loss:0.205, val_acc:0.889]
Epoch [38/120    avg_loss:0.250, val_acc:0.908]
Epoch [39/120    avg_loss:0.137, val_acc:0.920]
Epoch [40/120    avg_loss:0.134, val_acc:0.932]
Epoch [41/120    avg_loss:0.112, val_acc:0.923]
Epoch [42/120    avg_loss:0.088, val_acc:0.939]
Epoch [43/120    avg_loss:0.112, val_acc:0.928]
Epoch [44/120    avg_loss:0.093, val_acc:0.923]
Epoch [45/120    avg_loss:0.077, val_acc:0.935]
Epoch [46/120    avg_loss:0.075, val_acc:0.929]
Epoch [47/120    avg_loss:0.106, val_acc:0.950]
Epoch [48/120    avg_loss:0.102, val_acc:0.905]
Epoch [49/120    avg_loss:0.082, val_acc:0.932]
Epoch [50/120    avg_loss:0.099, val_acc:0.925]
Epoch [51/120    avg_loss:0.066, val_acc:0.917]
Epoch [52/120    avg_loss:0.091, val_acc:0.940]
Epoch [53/120    avg_loss:0.063, val_acc:0.949]
Epoch [54/120    avg_loss:0.049, val_acc:0.944]
Epoch [55/120    avg_loss:0.048, val_acc:0.949]
Epoch [56/120    avg_loss:0.047, val_acc:0.944]
Epoch [57/120    avg_loss:0.046, val_acc:0.957]
Epoch [58/120    avg_loss:0.060, val_acc:0.932]
Epoch [59/120    avg_loss:0.109, val_acc:0.933]
Epoch [60/120    avg_loss:0.079, val_acc:0.948]
Epoch [61/120    avg_loss:0.046, val_acc:0.917]
Epoch [62/120    avg_loss:0.043, val_acc:0.949]
Epoch [63/120    avg_loss:0.045, val_acc:0.954]
Epoch [64/120    avg_loss:0.051, val_acc:0.945]
Epoch [65/120    avg_loss:0.030, val_acc:0.963]
Epoch [66/120    avg_loss:0.032, val_acc:0.955]
Epoch [67/120    avg_loss:0.024, val_acc:0.957]
Epoch [68/120    avg_loss:0.027, val_acc:0.967]
Epoch [69/120    avg_loss:0.048, val_acc:0.963]
Epoch [70/120    avg_loss:0.032, val_acc:0.965]
Epoch [71/120    avg_loss:0.025, val_acc:0.960]
Epoch [72/120    avg_loss:0.021, val_acc:0.954]
Epoch [73/120    avg_loss:0.025, val_acc:0.967]
Epoch [74/120    avg_loss:0.021, val_acc:0.948]
Epoch [75/120    avg_loss:0.057, val_acc:0.945]
Epoch [76/120    avg_loss:0.053, val_acc:0.947]
Epoch [77/120    avg_loss:0.027, val_acc:0.965]
Epoch [78/120    avg_loss:0.023, val_acc:0.961]
Epoch [79/120    avg_loss:0.029, val_acc:0.960]
Epoch [80/120    avg_loss:0.023, val_acc:0.943]
Epoch [81/120    avg_loss:0.037, val_acc:0.965]
Epoch [82/120    avg_loss:0.032, val_acc:0.959]
Epoch [83/120    avg_loss:0.029, val_acc:0.966]
Epoch [84/120    avg_loss:0.022, val_acc:0.951]
Epoch [85/120    avg_loss:0.017, val_acc:0.970]
Epoch [86/120    avg_loss:0.016, val_acc:0.977]
Epoch [87/120    avg_loss:0.023, val_acc:0.956]
Epoch [88/120    avg_loss:0.028, val_acc:0.970]
Epoch [89/120    avg_loss:0.031, val_acc:0.963]
Epoch [90/120    avg_loss:0.041, val_acc:0.974]
Epoch [91/120    avg_loss:0.013, val_acc:0.977]
Epoch [92/120    avg_loss:0.018, val_acc:0.974]
Epoch [93/120    avg_loss:0.019, val_acc:0.966]
Epoch [94/120    avg_loss:0.018, val_acc:0.960]
Epoch [95/120    avg_loss:0.015, val_acc:0.966]
Epoch [96/120    avg_loss:0.025, val_acc:0.967]
Epoch [97/120    avg_loss:0.017, val_acc:0.973]
Epoch [98/120    avg_loss:0.017, val_acc:0.975]
Epoch [99/120    avg_loss:0.027, val_acc:0.980]
Epoch [100/120    avg_loss:0.022, val_acc:0.967]
Epoch [101/120    avg_loss:0.022, val_acc:0.975]
Epoch [102/120    avg_loss:0.027, val_acc:0.947]
Epoch [103/120    avg_loss:0.074, val_acc:0.944]
Epoch [104/120    avg_loss:0.029, val_acc:0.960]
Epoch [105/120    avg_loss:0.017, val_acc:0.977]
Epoch [106/120    avg_loss:0.015, val_acc:0.964]
Epoch [107/120    avg_loss:0.021, val_acc:0.969]
Epoch [108/120    avg_loss:0.010, val_acc:0.976]
Epoch [109/120    avg_loss:0.011, val_acc:0.975]
Epoch [110/120    avg_loss:0.013, val_acc:0.969]
Epoch [111/120    avg_loss:0.015, val_acc:0.972]
Epoch [112/120    avg_loss:0.037, val_acc:0.934]
Epoch [113/120    avg_loss:0.042, val_acc:0.961]
Epoch [114/120    avg_loss:0.022, val_acc:0.967]
Epoch [115/120    avg_loss:0.018, val_acc:0.973]
Epoch [116/120    avg_loss:0.011, val_acc:0.971]
Epoch [117/120    avg_loss:0.016, val_acc:0.972]
Epoch [118/120    avg_loss:0.010, val_acc:0.974]
Epoch [119/120    avg_loss:0.012, val_acc:0.975]
Epoch [120/120    avg_loss:0.010, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1230    0   11    0    2    0    0    0    4   38    0    0
     0    0    0]
 [   0    0    0  704    7    3    0    0    0    9    2   12   10    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    3    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    3    0    0    0    5  853    9    0    0
     2    1    0]
 [   0    0    2    1    0    2    1    0    0    2   19 2170   12    1
     0    0    0]
 [   0    0    0    2    0    1    0    0    0    1    4    8  515    0
     1    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    1    1    0    7    0    0    0    0
  1125    5    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    28  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.24661246612466

F1 scores:
[       nan 0.96202532 0.97657801 0.96836314 0.95945946 0.98289624
 0.98127341 0.96       0.99649942 0.57142857 0.96931818 0.97571942
 0.95903166 0.99459459 0.97953853 0.91883614 0.98823529]

Kappa:
0.9686064638194369
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f071e3d5a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.724, val_acc:0.155]
Epoch [2/120    avg_loss:2.505, val_acc:0.326]
Epoch [3/120    avg_loss:2.302, val_acc:0.492]
Epoch [4/120    avg_loss:2.177, val_acc:0.555]
Epoch [5/120    avg_loss:2.050, val_acc:0.580]
Epoch [6/120    avg_loss:1.882, val_acc:0.605]
Epoch [7/120    avg_loss:1.763, val_acc:0.628]
Epoch [8/120    avg_loss:1.631, val_acc:0.635]
Epoch [9/120    avg_loss:1.535, val_acc:0.632]
Epoch [10/120    avg_loss:1.343, val_acc:0.677]
Epoch [11/120    avg_loss:1.353, val_acc:0.682]
Epoch [12/120    avg_loss:1.181, val_acc:0.692]
Epoch [13/120    avg_loss:1.062, val_acc:0.617]
Epoch [14/120    avg_loss:1.009, val_acc:0.742]
Epoch [15/120    avg_loss:0.815, val_acc:0.753]
Epoch [16/120    avg_loss:0.786, val_acc:0.790]
Epoch [17/120    avg_loss:0.748, val_acc:0.744]
Epoch [18/120    avg_loss:0.729, val_acc:0.814]
Epoch [19/120    avg_loss:0.718, val_acc:0.782]
Epoch [20/120    avg_loss:0.718, val_acc:0.735]
Epoch [21/120    avg_loss:0.581, val_acc:0.800]
Epoch [22/120    avg_loss:0.511, val_acc:0.854]
Epoch [23/120    avg_loss:0.454, val_acc:0.866]
Epoch [24/120    avg_loss:0.443, val_acc:0.856]
Epoch [25/120    avg_loss:0.375, val_acc:0.809]
Epoch [26/120    avg_loss:0.339, val_acc:0.863]
Epoch [27/120    avg_loss:0.383, val_acc:0.834]
Epoch [28/120    avg_loss:0.316, val_acc:0.864]
Epoch [29/120    avg_loss:0.254, val_acc:0.909]
Epoch [30/120    avg_loss:0.227, val_acc:0.904]
Epoch [31/120    avg_loss:0.257, val_acc:0.891]
Epoch [32/120    avg_loss:0.195, val_acc:0.896]
Epoch [33/120    avg_loss:0.174, val_acc:0.920]
Epoch [34/120    avg_loss:0.186, val_acc:0.923]
Epoch [35/120    avg_loss:0.140, val_acc:0.932]
Epoch [36/120    avg_loss:0.155, val_acc:0.903]
Epoch [37/120    avg_loss:0.137, val_acc:0.918]
Epoch [38/120    avg_loss:0.136, val_acc:0.935]
Epoch [39/120    avg_loss:0.235, val_acc:0.914]
Epoch [40/120    avg_loss:0.216, val_acc:0.919]
Epoch [41/120    avg_loss:0.176, val_acc:0.926]
Epoch [42/120    avg_loss:0.141, val_acc:0.947]
Epoch [43/120    avg_loss:0.113, val_acc:0.911]
Epoch [44/120    avg_loss:0.118, val_acc:0.941]
Epoch [45/120    avg_loss:0.084, val_acc:0.948]
Epoch [46/120    avg_loss:0.084, val_acc:0.911]
Epoch [47/120    avg_loss:0.097, val_acc:0.944]
Epoch [48/120    avg_loss:0.077, val_acc:0.940]
Epoch [49/120    avg_loss:0.082, val_acc:0.932]
Epoch [50/120    avg_loss:0.318, val_acc:0.299]
Epoch [51/120    avg_loss:1.503, val_acc:0.545]
Epoch [52/120    avg_loss:0.725, val_acc:0.796]
Epoch [53/120    avg_loss:0.418, val_acc:0.857]
Epoch [54/120    avg_loss:0.216, val_acc:0.927]
Epoch [55/120    avg_loss:0.144, val_acc:0.936]
Epoch [56/120    avg_loss:0.119, val_acc:0.947]
Epoch [57/120    avg_loss:0.164, val_acc:0.915]
Epoch [58/120    avg_loss:0.211, val_acc:0.920]
Epoch [59/120    avg_loss:0.108, val_acc:0.946]
Epoch [60/120    avg_loss:0.116, val_acc:0.946]
Epoch [61/120    avg_loss:0.084, val_acc:0.950]
Epoch [62/120    avg_loss:0.075, val_acc:0.957]
Epoch [63/120    avg_loss:0.078, val_acc:0.953]
Epoch [64/120    avg_loss:0.078, val_acc:0.954]
Epoch [65/120    avg_loss:0.074, val_acc:0.956]
Epoch [66/120    avg_loss:0.068, val_acc:0.951]
Epoch [67/120    avg_loss:0.062, val_acc:0.955]
Epoch [68/120    avg_loss:0.071, val_acc:0.955]
Epoch [69/120    avg_loss:0.058, val_acc:0.957]
Epoch [70/120    avg_loss:0.067, val_acc:0.955]
Epoch [71/120    avg_loss:0.070, val_acc:0.956]
Epoch [72/120    avg_loss:0.063, val_acc:0.958]
Epoch [73/120    avg_loss:0.064, val_acc:0.956]
Epoch [74/120    avg_loss:0.058, val_acc:0.959]
Epoch [75/120    avg_loss:0.062, val_acc:0.957]
Epoch [76/120    avg_loss:0.052, val_acc:0.958]
Epoch [77/120    avg_loss:0.067, val_acc:0.955]
Epoch [78/120    avg_loss:0.050, val_acc:0.958]
Epoch [79/120    avg_loss:0.050, val_acc:0.954]
Epoch [80/120    avg_loss:0.048, val_acc:0.956]
Epoch [81/120    avg_loss:0.053, val_acc:0.958]
Epoch [82/120    avg_loss:0.052, val_acc:0.959]
Epoch [83/120    avg_loss:0.049, val_acc:0.958]
Epoch [84/120    avg_loss:0.041, val_acc:0.963]
Epoch [85/120    avg_loss:0.057, val_acc:0.960]
Epoch [86/120    avg_loss:0.052, val_acc:0.960]
Epoch [87/120    avg_loss:0.051, val_acc:0.960]
Epoch [88/120    avg_loss:0.044, val_acc:0.958]
Epoch [89/120    avg_loss:0.047, val_acc:0.960]
Epoch [90/120    avg_loss:0.049, val_acc:0.958]
Epoch [91/120    avg_loss:0.055, val_acc:0.963]
Epoch [92/120    avg_loss:0.055, val_acc:0.959]
Epoch [93/120    avg_loss:0.045, val_acc:0.956]
Epoch [94/120    avg_loss:0.042, val_acc:0.959]
Epoch [95/120    avg_loss:0.044, val_acc:0.960]
Epoch [96/120    avg_loss:0.045, val_acc:0.961]
Epoch [97/120    avg_loss:0.040, val_acc:0.964]
Epoch [98/120    avg_loss:0.049, val_acc:0.960]
Epoch [99/120    avg_loss:0.039, val_acc:0.963]
Epoch [100/120    avg_loss:0.043, val_acc:0.964]
Epoch [101/120    avg_loss:0.042, val_acc:0.964]
Epoch [102/120    avg_loss:0.044, val_acc:0.967]
Epoch [103/120    avg_loss:0.040, val_acc:0.959]
Epoch [104/120    avg_loss:0.042, val_acc:0.960]
Epoch [105/120    avg_loss:0.042, val_acc:0.960]
Epoch [106/120    avg_loss:0.043, val_acc:0.963]
Epoch [107/120    avg_loss:0.041, val_acc:0.964]
Epoch [108/120    avg_loss:0.041, val_acc:0.965]
Epoch [109/120    avg_loss:0.039, val_acc:0.965]
Epoch [110/120    avg_loss:0.036, val_acc:0.964]
Epoch [111/120    avg_loss:0.039, val_acc:0.965]
Epoch [112/120    avg_loss:0.036, val_acc:0.964]
Epoch [113/120    avg_loss:0.041, val_acc:0.961]
Epoch [114/120    avg_loss:0.041, val_acc:0.964]
Epoch [115/120    avg_loss:0.041, val_acc:0.966]
Epoch [116/120    avg_loss:0.036, val_acc:0.966]
Epoch [117/120    avg_loss:0.031, val_acc:0.966]
Epoch [118/120    avg_loss:0.036, val_acc:0.967]
Epoch [119/120    avg_loss:0.040, val_acc:0.966]
Epoch [120/120    avg_loss:0.035, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1251    2    6    0    2    0    0    0    3   20    1    0
     0    0    0]
 [   0    0    2  710    9   10    1    0    0    6    2    4    3    0
     0    0    0]
 [   0    0    0    5  203    0    0    0    0    0    0    0    5    0
     0    0    0]
 [   0    0    0    0    0  427    2    2    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    1    4    0    0    0  848    8    4    0
     2    0    0]
 [   0    0   23    1    0    0    1    0    0    3   42 2133    0    0
     7    0    0]
 [   0    0    0    0    0    5    0    0    0    0    1    0  524    0
     0    3    1]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    1    0    0    0
  1130    7    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    35  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.11653116531166

F1 scores:
[       nan 0.98765432 0.97391981 0.96928328 0.94199536 0.97266515
 0.97837435 0.96153846 1.         0.8        0.95657078 0.97486289
 0.97852474 0.99728997 0.97539922 0.90658499 0.99408284]

Kappa:
0.9671460455177725
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7561832a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.722, val_acc:0.293]
Epoch [2/120    avg_loss:2.511, val_acc:0.346]
Epoch [3/120    avg_loss:2.345, val_acc:0.350]
Epoch [4/120    avg_loss:2.172, val_acc:0.411]
Epoch [5/120    avg_loss:2.019, val_acc:0.506]
Epoch [6/120    avg_loss:1.905, val_acc:0.533]
Epoch [7/120    avg_loss:1.787, val_acc:0.581]
Epoch [8/120    avg_loss:1.680, val_acc:0.595]
Epoch [9/120    avg_loss:1.546, val_acc:0.647]
Epoch [10/120    avg_loss:1.458, val_acc:0.680]
Epoch [11/120    avg_loss:1.362, val_acc:0.693]
Epoch [12/120    avg_loss:1.233, val_acc:0.686]
Epoch [13/120    avg_loss:1.091, val_acc:0.699]
Epoch [14/120    avg_loss:1.039, val_acc:0.733]
Epoch [15/120    avg_loss:0.925, val_acc:0.691]
Epoch [16/120    avg_loss:0.947, val_acc:0.689]
Epoch [17/120    avg_loss:0.790, val_acc:0.686]
Epoch [18/120    avg_loss:0.706, val_acc:0.772]
Epoch [19/120    avg_loss:0.572, val_acc:0.793]
Epoch [20/120    avg_loss:0.565, val_acc:0.797]
Epoch [21/120    avg_loss:0.488, val_acc:0.817]
Epoch [22/120    avg_loss:0.439, val_acc:0.821]
Epoch [23/120    avg_loss:0.391, val_acc:0.831]
Epoch [24/120    avg_loss:0.348, val_acc:0.829]
Epoch [25/120    avg_loss:0.306, val_acc:0.851]
Epoch [26/120    avg_loss:0.266, val_acc:0.873]
Epoch [27/120    avg_loss:0.337, val_acc:0.832]
Epoch [28/120    avg_loss:0.274, val_acc:0.845]
Epoch [29/120    avg_loss:0.301, val_acc:0.859]
Epoch [30/120    avg_loss:0.212, val_acc:0.890]
Epoch [31/120    avg_loss:0.190, val_acc:0.896]
Epoch [32/120    avg_loss:0.159, val_acc:0.898]
Epoch [33/120    avg_loss:0.174, val_acc:0.904]
Epoch [34/120    avg_loss:0.166, val_acc:0.901]
Epoch [35/120    avg_loss:0.191, val_acc:0.886]
Epoch [36/120    avg_loss:0.171, val_acc:0.894]
Epoch [37/120    avg_loss:0.138, val_acc:0.929]
Epoch [38/120    avg_loss:0.133, val_acc:0.933]
Epoch [39/120    avg_loss:0.111, val_acc:0.893]
Epoch [40/120    avg_loss:0.099, val_acc:0.938]
Epoch [41/120    avg_loss:0.098, val_acc:0.918]
Epoch [42/120    avg_loss:0.086, val_acc:0.943]
Epoch [43/120    avg_loss:0.071, val_acc:0.949]
Epoch [44/120    avg_loss:0.082, val_acc:0.954]
Epoch [45/120    avg_loss:0.058, val_acc:0.953]
Epoch [46/120    avg_loss:0.082, val_acc:0.939]
Epoch [47/120    avg_loss:0.072, val_acc:0.933]
Epoch [48/120    avg_loss:0.065, val_acc:0.950]
Epoch [49/120    avg_loss:0.055, val_acc:0.927]
Epoch [50/120    avg_loss:0.077, val_acc:0.908]
Epoch [51/120    avg_loss:0.074, val_acc:0.949]
Epoch [52/120    avg_loss:0.053, val_acc:0.963]
Epoch [53/120    avg_loss:0.047, val_acc:0.958]
Epoch [54/120    avg_loss:0.062, val_acc:0.951]
Epoch [55/120    avg_loss:0.048, val_acc:0.947]
Epoch [56/120    avg_loss:0.049, val_acc:0.954]
Epoch [57/120    avg_loss:0.053, val_acc:0.946]
Epoch [58/120    avg_loss:0.056, val_acc:0.957]
Epoch [59/120    avg_loss:0.058, val_acc:0.950]
Epoch [60/120    avg_loss:0.045, val_acc:0.963]
Epoch [61/120    avg_loss:0.032, val_acc:0.973]
Epoch [62/120    avg_loss:0.037, val_acc:0.965]
Epoch [63/120    avg_loss:0.040, val_acc:0.957]
Epoch [64/120    avg_loss:0.053, val_acc:0.963]
Epoch [65/120    avg_loss:0.060, val_acc:0.919]
Epoch [66/120    avg_loss:0.089, val_acc:0.943]
Epoch [67/120    avg_loss:0.072, val_acc:0.943]
Epoch [68/120    avg_loss:0.041, val_acc:0.956]
Epoch [69/120    avg_loss:0.035, val_acc:0.948]
Epoch [70/120    avg_loss:0.039, val_acc:0.959]
Epoch [71/120    avg_loss:1.015, val_acc:0.689]
Epoch [72/120    avg_loss:0.643, val_acc:0.746]
Epoch [73/120    avg_loss:0.395, val_acc:0.858]
Epoch [74/120    avg_loss:0.275, val_acc:0.808]
Epoch [75/120    avg_loss:0.174, val_acc:0.925]
Epoch [76/120    avg_loss:0.112, val_acc:0.947]
Epoch [77/120    avg_loss:0.101, val_acc:0.947]
Epoch [78/120    avg_loss:0.093, val_acc:0.947]
Epoch [79/120    avg_loss:0.079, val_acc:0.951]
Epoch [80/120    avg_loss:0.068, val_acc:0.950]
Epoch [81/120    avg_loss:0.069, val_acc:0.952]
Epoch [82/120    avg_loss:0.065, val_acc:0.951]
Epoch [83/120    avg_loss:0.073, val_acc:0.945]
Epoch [84/120    avg_loss:0.064, val_acc:0.951]
Epoch [85/120    avg_loss:0.068, val_acc:0.952]
Epoch [86/120    avg_loss:0.060, val_acc:0.953]
Epoch [87/120    avg_loss:0.067, val_acc:0.954]
Epoch [88/120    avg_loss:0.057, val_acc:0.953]
Epoch [89/120    avg_loss:0.056, val_acc:0.954]
Epoch [90/120    avg_loss:0.053, val_acc:0.953]
Epoch [91/120    avg_loss:0.052, val_acc:0.952]
Epoch [92/120    avg_loss:0.051, val_acc:0.953]
Epoch [93/120    avg_loss:0.057, val_acc:0.954]
Epoch [94/120    avg_loss:0.057, val_acc:0.953]
Epoch [95/120    avg_loss:0.054, val_acc:0.954]
Epoch [96/120    avg_loss:0.054, val_acc:0.954]
Epoch [97/120    avg_loss:0.056, val_acc:0.954]
Epoch [98/120    avg_loss:0.051, val_acc:0.954]
Epoch [99/120    avg_loss:0.049, val_acc:0.956]
Epoch [100/120    avg_loss:0.058, val_acc:0.955]
Epoch [101/120    avg_loss:0.051, val_acc:0.955]
Epoch [102/120    avg_loss:0.052, val_acc:0.955]
Epoch [103/120    avg_loss:0.058, val_acc:0.956]
Epoch [104/120    avg_loss:0.056, val_acc:0.955]
Epoch [105/120    avg_loss:0.059, val_acc:0.955]
Epoch [106/120    avg_loss:0.059, val_acc:0.955]
Epoch [107/120    avg_loss:0.057, val_acc:0.955]
Epoch [108/120    avg_loss:0.054, val_acc:0.955]
Epoch [109/120    avg_loss:0.054, val_acc:0.955]
Epoch [110/120    avg_loss:0.067, val_acc:0.955]
Epoch [111/120    avg_loss:0.051, val_acc:0.955]
Epoch [112/120    avg_loss:0.055, val_acc:0.955]
Epoch [113/120    avg_loss:0.047, val_acc:0.955]
Epoch [114/120    avg_loss:0.048, val_acc:0.955]
Epoch [115/120    avg_loss:0.046, val_acc:0.955]
Epoch [116/120    avg_loss:0.062, val_acc:0.955]
Epoch [117/120    avg_loss:0.050, val_acc:0.955]
Epoch [118/120    avg_loss:0.053, val_acc:0.955]
Epoch [119/120    avg_loss:0.057, val_acc:0.955]
Epoch [120/120    avg_loss:0.048, val_acc:0.955]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1171   11    8    0    0    0    0    0   24   69    2    0
     0    0    0]
 [   0    0    0  721    7    2    0    0    0    0    2   10    2    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    7    0    0    0    0    0    0  423    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   22    0    0    0    1    0    0    0  838   13    1    0
     0    0    0]
 [   0    0   29    0    0    2    1    0    0    2   15 2146   12    1
     0    2    0]
 [   0    0    0    5    0    0    0    0    0    0    3    3  519    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1132    6    0]
 [   0    0    0    0    0    0   20    0    0    0    0    0    0    0
    34  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.45528455284553

F1 scores:
[       nan 0.90909091 0.9338118  0.97169811 0.96598639 0.99542334
 0.982009   1.         0.99179367 0.94736842 0.95335609 0.9638446
 0.97009346 0.98930481 0.98221258 0.90292758 0.98245614]

Kappa:
0.95957330412016
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9364017a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.709, val_acc:0.198]
Epoch [2/120    avg_loss:2.474, val_acc:0.492]
Epoch [3/120    avg_loss:2.285, val_acc:0.554]
Epoch [4/120    avg_loss:2.130, val_acc:0.547]
Epoch [5/120    avg_loss:1.956, val_acc:0.575]
Epoch [6/120    avg_loss:1.880, val_acc:0.595]
Epoch [7/120    avg_loss:1.735, val_acc:0.623]
Epoch [8/120    avg_loss:1.541, val_acc:0.654]
Epoch [9/120    avg_loss:1.452, val_acc:0.651]
Epoch [10/120    avg_loss:1.358, val_acc:0.672]
Epoch [11/120    avg_loss:1.198, val_acc:0.682]
Epoch [12/120    avg_loss:1.055, val_acc:0.679]
Epoch [13/120    avg_loss:1.007, val_acc:0.747]
Epoch [14/120    avg_loss:0.928, val_acc:0.735]
Epoch [15/120    avg_loss:0.789, val_acc:0.767]
Epoch [16/120    avg_loss:0.712, val_acc:0.802]
Epoch [17/120    avg_loss:0.659, val_acc:0.778]
Epoch [18/120    avg_loss:0.683, val_acc:0.800]
Epoch [19/120    avg_loss:0.584, val_acc:0.823]
Epoch [20/120    avg_loss:0.507, val_acc:0.796]
Epoch [21/120    avg_loss:0.471, val_acc:0.800]
Epoch [22/120    avg_loss:0.423, val_acc:0.853]
Epoch [23/120    avg_loss:0.362, val_acc:0.810]
Epoch [24/120    avg_loss:0.317, val_acc:0.893]
Epoch [25/120    avg_loss:0.298, val_acc:0.867]
Epoch [26/120    avg_loss:0.253, val_acc:0.869]
Epoch [27/120    avg_loss:0.283, val_acc:0.787]
Epoch [28/120    avg_loss:0.262, val_acc:0.901]
Epoch [29/120    avg_loss:0.197, val_acc:0.896]
Epoch [30/120    avg_loss:0.211, val_acc:0.885]
Epoch [31/120    avg_loss:0.234, val_acc:0.904]
Epoch [32/120    avg_loss:0.265, val_acc:0.897]
Epoch [33/120    avg_loss:0.197, val_acc:0.910]
Epoch [34/120    avg_loss:0.181, val_acc:0.905]
Epoch [35/120    avg_loss:0.198, val_acc:0.907]
Epoch [36/120    avg_loss:0.147, val_acc:0.905]
Epoch [37/120    avg_loss:0.134, val_acc:0.860]
Epoch [38/120    avg_loss:0.153, val_acc:0.933]
Epoch [39/120    avg_loss:0.155, val_acc:0.922]
Epoch [40/120    avg_loss:0.114, val_acc:0.935]
Epoch [41/120    avg_loss:0.119, val_acc:0.944]
Epoch [42/120    avg_loss:0.126, val_acc:0.930]
Epoch [43/120    avg_loss:0.087, val_acc:0.879]
Epoch [44/120    avg_loss:0.114, val_acc:0.932]
Epoch [45/120    avg_loss:0.179, val_acc:0.927]
Epoch [46/120    avg_loss:0.105, val_acc:0.939]
Epoch [47/120    avg_loss:0.091, val_acc:0.940]
Epoch [48/120    avg_loss:0.101, val_acc:0.935]
Epoch [49/120    avg_loss:0.115, val_acc:0.944]
Epoch [50/120    avg_loss:0.117, val_acc:0.939]
Epoch [51/120    avg_loss:0.092, val_acc:0.930]
Epoch [52/120    avg_loss:0.079, val_acc:0.966]
Epoch [53/120    avg_loss:0.073, val_acc:0.951]
Epoch [54/120    avg_loss:0.068, val_acc:0.952]
Epoch [55/120    avg_loss:0.056, val_acc:0.950]
Epoch [56/120    avg_loss:0.085, val_acc:0.943]
Epoch [57/120    avg_loss:0.128, val_acc:0.942]
Epoch [58/120    avg_loss:0.089, val_acc:0.939]
Epoch [59/120    avg_loss:0.071, val_acc:0.951]
Epoch [60/120    avg_loss:0.062, val_acc:0.954]
Epoch [61/120    avg_loss:0.044, val_acc:0.962]
Epoch [62/120    avg_loss:0.053, val_acc:0.953]
Epoch [63/120    avg_loss:0.054, val_acc:0.952]
Epoch [64/120    avg_loss:0.057, val_acc:0.942]
Epoch [65/120    avg_loss:0.033, val_acc:0.960]
Epoch [66/120    avg_loss:0.036, val_acc:0.964]
Epoch [67/120    avg_loss:0.029, val_acc:0.966]
Epoch [68/120    avg_loss:0.025, val_acc:0.964]
Epoch [69/120    avg_loss:0.026, val_acc:0.967]
Epoch [70/120    avg_loss:0.021, val_acc:0.967]
Epoch [71/120    avg_loss:0.026, val_acc:0.967]
Epoch [72/120    avg_loss:0.025, val_acc:0.966]
Epoch [73/120    avg_loss:0.023, val_acc:0.967]
Epoch [74/120    avg_loss:0.018, val_acc:0.966]
Epoch [75/120    avg_loss:0.021, val_acc:0.967]
Epoch [76/120    avg_loss:0.032, val_acc:0.966]
Epoch [77/120    avg_loss:0.020, val_acc:0.965]
Epoch [78/120    avg_loss:0.021, val_acc:0.966]
Epoch [79/120    avg_loss:0.022, val_acc:0.966]
Epoch [80/120    avg_loss:0.019, val_acc:0.968]
Epoch [81/120    avg_loss:0.020, val_acc:0.967]
Epoch [82/120    avg_loss:0.019, val_acc:0.968]
Epoch [83/120    avg_loss:0.022, val_acc:0.966]
Epoch [84/120    avg_loss:0.021, val_acc:0.968]
Epoch [85/120    avg_loss:0.020, val_acc:0.967]
Epoch [86/120    avg_loss:0.020, val_acc:0.967]
Epoch [87/120    avg_loss:0.017, val_acc:0.967]
Epoch [88/120    avg_loss:0.021, val_acc:0.967]
Epoch [89/120    avg_loss:0.019, val_acc:0.966]
Epoch [90/120    avg_loss:0.019, val_acc:0.969]
Epoch [91/120    avg_loss:0.017, val_acc:0.969]
Epoch [92/120    avg_loss:0.020, val_acc:0.967]
Epoch [93/120    avg_loss:0.027, val_acc:0.969]
Epoch [94/120    avg_loss:0.017, val_acc:0.968]
Epoch [95/120    avg_loss:0.022, val_acc:0.970]
Epoch [96/120    avg_loss:0.017, val_acc:0.969]
Epoch [97/120    avg_loss:0.024, val_acc:0.970]
Epoch [98/120    avg_loss:0.022, val_acc:0.971]
Epoch [99/120    avg_loss:0.018, val_acc:0.971]
Epoch [100/120    avg_loss:0.020, val_acc:0.968]
Epoch [101/120    avg_loss:0.026, val_acc:0.966]
Epoch [102/120    avg_loss:0.019, val_acc:0.967]
Epoch [103/120    avg_loss:0.023, val_acc:0.967]
Epoch [104/120    avg_loss:0.019, val_acc:0.969]
Epoch [105/120    avg_loss:0.019, val_acc:0.967]
Epoch [106/120    avg_loss:0.021, val_acc:0.967]
Epoch [107/120    avg_loss:0.020, val_acc:0.968]
Epoch [108/120    avg_loss:0.020, val_acc:0.969]
Epoch [109/120    avg_loss:0.019, val_acc:0.967]
Epoch [110/120    avg_loss:0.016, val_acc:0.968]
Epoch [111/120    avg_loss:0.016, val_acc:0.968]
Epoch [112/120    avg_loss:0.017, val_acc:0.968]
Epoch [113/120    avg_loss:0.017, val_acc:0.969]
Epoch [114/120    avg_loss:0.022, val_acc:0.969]
Epoch [115/120    avg_loss:0.019, val_acc:0.969]
Epoch [116/120    avg_loss:0.019, val_acc:0.969]
Epoch [117/120    avg_loss:0.020, val_acc:0.969]
Epoch [118/120    avg_loss:0.016, val_acc:0.969]
Epoch [119/120    avg_loss:0.019, val_acc:0.969]
Epoch [120/120    avg_loss:0.019, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1253   14    4    0    0    0    0    0    6    8    0    0
     0    0    0]
 [   0    0    0  731    5    0    0    0    0    2    3    2    4    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   10    1    0    1    2    0    0    0  855    6    0    0
     0    0    0]
 [   0    0    9    0    0    1    4    1    0    0   17 2171    7    0
     0    0    0]
 [   0    0    1    2    3    0    0    0    0    0    0    7  514    0
     0    4    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1121   17    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    47  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.79945799457994

F1 scores:
[       nan 1.         0.97967162 0.97792642 0.97025172 0.99192618
 0.99319728 0.98039216 0.99883856 0.91891892 0.9738041  0.98569807
 0.96798493 1.         0.9701428  0.89489489 0.9704142 ]

Kappa:
0.9749153174712759
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa12463eb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.731, val_acc:0.271]
Epoch [2/120    avg_loss:2.523, val_acc:0.433]
Epoch [3/120    avg_loss:2.330, val_acc:0.496]
Epoch [4/120    avg_loss:2.170, val_acc:0.543]
Epoch [5/120    avg_loss:1.981, val_acc:0.600]
Epoch [6/120    avg_loss:1.803, val_acc:0.642]
Epoch [7/120    avg_loss:1.685, val_acc:0.625]
Epoch [8/120    avg_loss:1.502, val_acc:0.649]
Epoch [9/120    avg_loss:1.362, val_acc:0.626]
Epoch [10/120    avg_loss:1.301, val_acc:0.765]
Epoch [11/120    avg_loss:1.132, val_acc:0.734]
Epoch [12/120    avg_loss:1.093, val_acc:0.740]
Epoch [13/120    avg_loss:0.905, val_acc:0.797]
Epoch [14/120    avg_loss:0.801, val_acc:0.829]
Epoch [15/120    avg_loss:0.701, val_acc:0.810]
Epoch [16/120    avg_loss:0.621, val_acc:0.832]
Epoch [17/120    avg_loss:0.555, val_acc:0.810]
Epoch [18/120    avg_loss:0.549, val_acc:0.780]
Epoch [19/120    avg_loss:0.592, val_acc:0.789]
Epoch [20/120    avg_loss:0.469, val_acc:0.867]
Epoch [21/120    avg_loss:0.494, val_acc:0.836]
Epoch [22/120    avg_loss:0.450, val_acc:0.892]
Epoch [23/120    avg_loss:0.362, val_acc:0.847]
Epoch [24/120    avg_loss:0.314, val_acc:0.878]
Epoch [25/120    avg_loss:0.276, val_acc:0.890]
Epoch [26/120    avg_loss:0.238, val_acc:0.907]
Epoch [27/120    avg_loss:0.309, val_acc:0.862]
Epoch [28/120    avg_loss:0.309, val_acc:0.908]
Epoch [29/120    avg_loss:0.214, val_acc:0.910]
Epoch [30/120    avg_loss:0.226, val_acc:0.869]
Epoch [31/120    avg_loss:0.359, val_acc:0.906]
Epoch [32/120    avg_loss:0.216, val_acc:0.930]
Epoch [33/120    avg_loss:0.178, val_acc:0.928]
Epoch [34/120    avg_loss:0.163, val_acc:0.923]
Epoch [35/120    avg_loss:0.136, val_acc:0.943]
Epoch [36/120    avg_loss:0.143, val_acc:0.879]
Epoch [37/120    avg_loss:0.138, val_acc:0.940]
Epoch [38/120    avg_loss:0.128, val_acc:0.941]
Epoch [39/120    avg_loss:0.145, val_acc:0.938]
Epoch [40/120    avg_loss:0.128, val_acc:0.934]
Epoch [41/120    avg_loss:0.090, val_acc:0.943]
Epoch [42/120    avg_loss:0.136, val_acc:0.949]
Epoch [43/120    avg_loss:0.111, val_acc:0.960]
Epoch [44/120    avg_loss:0.082, val_acc:0.941]
Epoch [45/120    avg_loss:0.072, val_acc:0.945]
Epoch [46/120    avg_loss:0.084, val_acc:0.958]
Epoch [47/120    avg_loss:0.052, val_acc:0.966]
Epoch [48/120    avg_loss:0.066, val_acc:0.964]
Epoch [49/120    avg_loss:0.053, val_acc:0.960]
Epoch [50/120    avg_loss:0.053, val_acc:0.962]
Epoch [51/120    avg_loss:0.070, val_acc:0.944]
Epoch [52/120    avg_loss:0.081, val_acc:0.952]
Epoch [53/120    avg_loss:0.051, val_acc:0.952]
Epoch [54/120    avg_loss:0.075, val_acc:0.959]
Epoch [55/120    avg_loss:0.119, val_acc:0.960]
Epoch [56/120    avg_loss:0.077, val_acc:0.956]
Epoch [57/120    avg_loss:0.053, val_acc:0.970]
Epoch [58/120    avg_loss:0.075, val_acc:0.961]
Epoch [59/120    avg_loss:0.054, val_acc:0.964]
Epoch [60/120    avg_loss:0.045, val_acc:0.960]
Epoch [61/120    avg_loss:0.055, val_acc:0.962]
Epoch [62/120    avg_loss:0.044, val_acc:0.964]
Epoch [63/120    avg_loss:0.052, val_acc:0.962]
Epoch [64/120    avg_loss:0.039, val_acc:0.957]
Epoch [65/120    avg_loss:0.042, val_acc:0.966]
Epoch [66/120    avg_loss:0.040, val_acc:0.950]
Epoch [67/120    avg_loss:0.044, val_acc:0.963]
Epoch [68/120    avg_loss:0.031, val_acc:0.964]
Epoch [69/120    avg_loss:0.043, val_acc:0.962]
Epoch [70/120    avg_loss:0.046, val_acc:0.964]
Epoch [71/120    avg_loss:0.021, val_acc:0.969]
Epoch [72/120    avg_loss:0.022, val_acc:0.968]
Epoch [73/120    avg_loss:0.015, val_acc:0.970]
Epoch [74/120    avg_loss:0.020, val_acc:0.968]
Epoch [75/120    avg_loss:0.020, val_acc:0.974]
Epoch [76/120    avg_loss:0.016, val_acc:0.972]
Epoch [77/120    avg_loss:0.018, val_acc:0.972]
Epoch [78/120    avg_loss:0.017, val_acc:0.971]
Epoch [79/120    avg_loss:0.016, val_acc:0.970]
Epoch [80/120    avg_loss:0.022, val_acc:0.969]
Epoch [81/120    avg_loss:0.014, val_acc:0.974]
Epoch [82/120    avg_loss:0.017, val_acc:0.968]
Epoch [83/120    avg_loss:0.014, val_acc:0.969]
Epoch [84/120    avg_loss:0.017, val_acc:0.970]
Epoch [85/120    avg_loss:0.018, val_acc:0.972]
Epoch [86/120    avg_loss:0.016, val_acc:0.969]
Epoch [87/120    avg_loss:0.014, val_acc:0.971]
Epoch [88/120    avg_loss:0.014, val_acc:0.975]
Epoch [89/120    avg_loss:0.013, val_acc:0.974]
Epoch [90/120    avg_loss:0.013, val_acc:0.972]
Epoch [91/120    avg_loss:0.014, val_acc:0.971]
Epoch [92/120    avg_loss:0.013, val_acc:0.971]
Epoch [93/120    avg_loss:0.015, val_acc:0.971]
Epoch [94/120    avg_loss:0.012, val_acc:0.971]
Epoch [95/120    avg_loss:0.020, val_acc:0.972]
Epoch [96/120    avg_loss:0.012, val_acc:0.975]
Epoch [97/120    avg_loss:0.014, val_acc:0.974]
Epoch [98/120    avg_loss:0.010, val_acc:0.972]
Epoch [99/120    avg_loss:0.016, val_acc:0.971]
Epoch [100/120    avg_loss:0.014, val_acc:0.972]
Epoch [101/120    avg_loss:0.014, val_acc:0.970]
Epoch [102/120    avg_loss:0.015, val_acc:0.970]
Epoch [103/120    avg_loss:0.013, val_acc:0.972]
Epoch [104/120    avg_loss:0.010, val_acc:0.972]
Epoch [105/120    avg_loss:0.013, val_acc:0.972]
Epoch [106/120    avg_loss:0.014, val_acc:0.972]
Epoch [107/120    avg_loss:0.012, val_acc:0.974]
Epoch [108/120    avg_loss:0.012, val_acc:0.974]
Epoch [109/120    avg_loss:0.013, val_acc:0.972]
Epoch [110/120    avg_loss:0.012, val_acc:0.972]
Epoch [111/120    avg_loss:0.015, val_acc:0.972]
Epoch [112/120    avg_loss:0.015, val_acc:0.972]
Epoch [113/120    avg_loss:0.011, val_acc:0.972]
Epoch [114/120    avg_loss:0.012, val_acc:0.972]
Epoch [115/120    avg_loss:0.012, val_acc:0.972]
Epoch [116/120    avg_loss:0.009, val_acc:0.971]
Epoch [117/120    avg_loss:0.013, val_acc:0.971]
Epoch [118/120    avg_loss:0.013, val_acc:0.971]
Epoch [119/120    avg_loss:0.011, val_acc:0.971]
Epoch [120/120    avg_loss:0.016, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1254    2   11    0    2    0    0    0    0   16    0    0
     0    0    0]
 [   0    0    0  696    3    6    0    0    0    2    4   15   17    4
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    1    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    1  418    0    0    0    9    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    1    0    0    0    0  844   25    0    0
     0    1    0]
 [   0    0   15    0    0    0    1    0    0    0    4 2190    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0    1  527    0
     1    3    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    41  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.59349593495935

F1 scores:
[       nan 0.95121951 0.9796875  0.9626556  0.96583144 0.98855835
 0.98795181 0.98039216 0.98584906 0.92307692 0.97741749 0.98250336
 0.96786042 0.98930481 0.97708604 0.89908257 0.98809524]

Kappa:
0.9725409660441985
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f29e992ca20>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.737, val_acc:0.370]
Epoch [2/120    avg_loss:2.532, val_acc:0.453]
Epoch [3/120    avg_loss:2.316, val_acc:0.522]
Epoch [4/120    avg_loss:2.185, val_acc:0.536]
Epoch [5/120    avg_loss:1.994, val_acc:0.547]
Epoch [6/120    avg_loss:1.870, val_acc:0.561]
Epoch [7/120    avg_loss:1.750, val_acc:0.585]
Epoch [8/120    avg_loss:1.651, val_acc:0.591]
Epoch [9/120    avg_loss:1.477, val_acc:0.629]
Epoch [10/120    avg_loss:1.378, val_acc:0.639]
Epoch [11/120    avg_loss:1.228, val_acc:0.649]
Epoch [12/120    avg_loss:1.111, val_acc:0.710]
Epoch [13/120    avg_loss:0.979, val_acc:0.693]
Epoch [14/120    avg_loss:0.962, val_acc:0.731]
Epoch [15/120    avg_loss:0.828, val_acc:0.730]
Epoch [16/120    avg_loss:0.826, val_acc:0.738]
Epoch [17/120    avg_loss:0.821, val_acc:0.742]
Epoch [18/120    avg_loss:0.683, val_acc:0.742]
Epoch [19/120    avg_loss:0.640, val_acc:0.756]
Epoch [20/120    avg_loss:0.574, val_acc:0.812]
Epoch [21/120    avg_loss:0.541, val_acc:0.788]
Epoch [22/120    avg_loss:0.409, val_acc:0.825]
Epoch [23/120    avg_loss:0.419, val_acc:0.847]
Epoch [24/120    avg_loss:0.375, val_acc:0.817]
Epoch [25/120    avg_loss:0.382, val_acc:0.830]
Epoch [26/120    avg_loss:0.364, val_acc:0.828]
Epoch [27/120    avg_loss:0.412, val_acc:0.851]
Epoch [28/120    avg_loss:0.368, val_acc:0.858]
Epoch [29/120    avg_loss:0.295, val_acc:0.838]
Epoch [30/120    avg_loss:0.250, val_acc:0.851]
Epoch [31/120    avg_loss:0.274, val_acc:0.885]
Epoch [32/120    avg_loss:0.279, val_acc:0.873]
Epoch [33/120    avg_loss:0.302, val_acc:0.842]
Epoch [34/120    avg_loss:0.237, val_acc:0.878]
Epoch [35/120    avg_loss:0.180, val_acc:0.899]
Epoch [36/120    avg_loss:0.176, val_acc:0.908]
Epoch [37/120    avg_loss:0.156, val_acc:0.914]
Epoch [38/120    avg_loss:0.181, val_acc:0.932]
Epoch [39/120    avg_loss:0.199, val_acc:0.895]
Epoch [40/120    avg_loss:0.201, val_acc:0.904]
Epoch [41/120    avg_loss:0.129, val_acc:0.923]
Epoch [42/120    avg_loss:0.124, val_acc:0.928]
Epoch [43/120    avg_loss:0.135, val_acc:0.899]
Epoch [44/120    avg_loss:0.140, val_acc:0.944]
Epoch [45/120    avg_loss:0.113, val_acc:0.949]
Epoch [46/120    avg_loss:0.104, val_acc:0.938]
Epoch [47/120    avg_loss:0.111, val_acc:0.919]
Epoch [48/120    avg_loss:0.134, val_acc:0.932]
Epoch [49/120    avg_loss:0.098, val_acc:0.945]
Epoch [50/120    avg_loss:0.107, val_acc:0.949]
Epoch [51/120    avg_loss:0.081, val_acc:0.950]
Epoch [52/120    avg_loss:0.092, val_acc:0.953]
Epoch [53/120    avg_loss:0.102, val_acc:0.944]
Epoch [54/120    avg_loss:0.087, val_acc:0.943]
Epoch [55/120    avg_loss:0.072, val_acc:0.944]
Epoch [56/120    avg_loss:0.076, val_acc:0.950]
Epoch [57/120    avg_loss:0.064, val_acc:0.940]
Epoch [58/120    avg_loss:0.063, val_acc:0.940]
Epoch [59/120    avg_loss:0.070, val_acc:0.947]
Epoch [60/120    avg_loss:0.067, val_acc:0.957]
Epoch [61/120    avg_loss:0.060, val_acc:0.955]
Epoch [62/120    avg_loss:0.057, val_acc:0.945]
Epoch [63/120    avg_loss:0.065, val_acc:0.948]
Epoch [64/120    avg_loss:0.080, val_acc:0.932]
Epoch [65/120    avg_loss:0.057, val_acc:0.955]
Epoch [66/120    avg_loss:0.057, val_acc:0.953]
Epoch [67/120    avg_loss:0.048, val_acc:0.949]
Epoch [68/120    avg_loss:0.050, val_acc:0.962]
Epoch [69/120    avg_loss:0.051, val_acc:0.967]
Epoch [70/120    avg_loss:0.041, val_acc:0.968]
Epoch [71/120    avg_loss:0.040, val_acc:0.972]
Epoch [72/120    avg_loss:0.037, val_acc:0.968]
Epoch [73/120    avg_loss:0.031, val_acc:0.968]
Epoch [74/120    avg_loss:0.035, val_acc:0.967]
Epoch [75/120    avg_loss:0.055, val_acc:0.960]
Epoch [76/120    avg_loss:0.046, val_acc:0.967]
Epoch [77/120    avg_loss:0.031, val_acc:0.955]
Epoch [78/120    avg_loss:0.033, val_acc:0.969]
Epoch [79/120    avg_loss:0.039, val_acc:0.970]
Epoch [80/120    avg_loss:0.039, val_acc:0.971]
Epoch [81/120    avg_loss:0.034, val_acc:0.972]
Epoch [82/120    avg_loss:0.032, val_acc:0.971]
Epoch [83/120    avg_loss:0.025, val_acc:0.978]
Epoch [84/120    avg_loss:0.020, val_acc:0.980]
Epoch [85/120    avg_loss:0.025, val_acc:0.973]
Epoch [86/120    avg_loss:0.018, val_acc:0.975]
Epoch [87/120    avg_loss:0.026, val_acc:0.969]
Epoch [88/120    avg_loss:0.041, val_acc:0.963]
Epoch [89/120    avg_loss:0.039, val_acc:0.978]
Epoch [90/120    avg_loss:0.053, val_acc:0.967]
Epoch [91/120    avg_loss:0.032, val_acc:0.974]
Epoch [92/120    avg_loss:0.022, val_acc:0.976]
Epoch [93/120    avg_loss:0.017, val_acc:0.977]
Epoch [94/120    avg_loss:0.017, val_acc:0.980]
Epoch [95/120    avg_loss:0.017, val_acc:0.977]
Epoch [96/120    avg_loss:0.021, val_acc:0.961]
Epoch [97/120    avg_loss:0.028, val_acc:0.976]
Epoch [98/120    avg_loss:0.018, val_acc:0.971]
Epoch [99/120    avg_loss:0.043, val_acc:0.961]
Epoch [100/120    avg_loss:0.036, val_acc:0.973]
Epoch [101/120    avg_loss:0.021, val_acc:0.970]
Epoch [102/120    avg_loss:0.019, val_acc:0.972]
Epoch [103/120    avg_loss:0.019, val_acc:0.967]
Epoch [104/120    avg_loss:0.013, val_acc:0.972]
Epoch [105/120    avg_loss:0.022, val_acc:0.969]
Epoch [106/120    avg_loss:0.022, val_acc:0.966]
Epoch [107/120    avg_loss:0.025, val_acc:0.970]
Epoch [108/120    avg_loss:0.018, val_acc:0.974]
Epoch [109/120    avg_loss:0.013, val_acc:0.980]
Epoch [110/120    avg_loss:0.011, val_acc:0.980]
Epoch [111/120    avg_loss:0.010, val_acc:0.980]
Epoch [112/120    avg_loss:0.011, val_acc:0.983]
Epoch [113/120    avg_loss:0.012, val_acc:0.982]
Epoch [114/120    avg_loss:0.012, val_acc:0.984]
Epoch [115/120    avg_loss:0.011, val_acc:0.983]
Epoch [116/120    avg_loss:0.010, val_acc:0.983]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.014, val_acc:0.984]
Epoch [119/120    avg_loss:0.012, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    1    6    0    3    0    0    0    4    9    0    0
     0    1    0]
 [   0    0    0  728    5    2    0    0    0    7    0    0    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    0    0    0    0    0    0    0
    10    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   10    0    0    4    0    0    0    0  848   12    0    0
     1    0    0]
 [   0    0   14    0    0    0    1    0    0    0    2 2182   10    1
     0    0    0]
 [   0    0    0    0    0   15    0    0    0    0    3    0  513    0
     0    1    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1137    2    0]
 [   0    0    0    0    0    0   10    0    0    2    0    0    0    0
    53  282    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.76693766937669

F1 scores:
[       nan 0.975      0.98055988 0.98644986 0.97482838 0.96371882
 0.98869631 1.         0.99883586 0.77272727 0.97921478 0.98867241
 0.96067416 0.99459459 0.97179487 0.89099526 0.96385542]

Kappa:
0.9745324868293885
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb29e477b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.797, val_acc:0.404]
Epoch [2/120    avg_loss:2.627, val_acc:0.488]
Epoch [3/120    avg_loss:2.436, val_acc:0.558]
Epoch [4/120    avg_loss:2.284, val_acc:0.566]
Epoch [5/120    avg_loss:2.110, val_acc:0.542]
Epoch [6/120    avg_loss:1.976, val_acc:0.566]
Epoch [7/120    avg_loss:1.844, val_acc:0.582]
Epoch [8/120    avg_loss:1.729, val_acc:0.613]
Epoch [9/120    avg_loss:1.642, val_acc:0.618]
Epoch [10/120    avg_loss:1.553, val_acc:0.671]
Epoch [11/120    avg_loss:1.439, val_acc:0.688]
Epoch [12/120    avg_loss:1.323, val_acc:0.709]
Epoch [13/120    avg_loss:1.204, val_acc:0.659]
Epoch [14/120    avg_loss:1.115, val_acc:0.749]
Epoch [15/120    avg_loss:1.015, val_acc:0.716]
Epoch [16/120    avg_loss:0.865, val_acc:0.752]
Epoch [17/120    avg_loss:0.795, val_acc:0.699]
Epoch [18/120    avg_loss:0.789, val_acc:0.751]
Epoch [19/120    avg_loss:0.631, val_acc:0.796]
Epoch [20/120    avg_loss:0.626, val_acc:0.762]
Epoch [21/120    avg_loss:0.582, val_acc:0.825]
Epoch [22/120    avg_loss:0.545, val_acc:0.844]
Epoch [23/120    avg_loss:0.573, val_acc:0.808]
Epoch [24/120    avg_loss:0.517, val_acc:0.843]
Epoch [25/120    avg_loss:0.503, val_acc:0.837]
Epoch [26/120    avg_loss:0.418, val_acc:0.853]
Epoch [27/120    avg_loss:0.434, val_acc:0.851]
Epoch [28/120    avg_loss:0.417, val_acc:0.835]
Epoch [29/120    avg_loss:0.326, val_acc:0.872]
Epoch [30/120    avg_loss:0.376, val_acc:0.854]
Epoch [31/120    avg_loss:0.334, val_acc:0.844]
Epoch [32/120    avg_loss:0.254, val_acc:0.887]
Epoch [33/120    avg_loss:0.293, val_acc:0.872]
Epoch [34/120    avg_loss:0.288, val_acc:0.862]
Epoch [35/120    avg_loss:0.245, val_acc:0.896]
Epoch [36/120    avg_loss:0.188, val_acc:0.907]
Epoch [37/120    avg_loss:0.196, val_acc:0.907]
Epoch [38/120    avg_loss:0.230, val_acc:0.910]
Epoch [39/120    avg_loss:0.178, val_acc:0.899]
Epoch [40/120    avg_loss:0.174, val_acc:0.930]
Epoch [41/120    avg_loss:0.173, val_acc:0.912]
Epoch [42/120    avg_loss:0.176, val_acc:0.923]
Epoch [43/120    avg_loss:0.150, val_acc:0.929]
Epoch [44/120    avg_loss:0.183, val_acc:0.924]
Epoch [45/120    avg_loss:0.123, val_acc:0.916]
Epoch [46/120    avg_loss:0.142, val_acc:0.913]
Epoch [47/120    avg_loss:0.141, val_acc:0.916]
Epoch [48/120    avg_loss:0.102, val_acc:0.942]
Epoch [49/120    avg_loss:0.080, val_acc:0.938]
Epoch [50/120    avg_loss:0.101, val_acc:0.940]
Epoch [51/120    avg_loss:0.105, val_acc:0.936]
Epoch [52/120    avg_loss:0.122, val_acc:0.933]
Epoch [53/120    avg_loss:0.114, val_acc:0.913]
Epoch [54/120    avg_loss:0.096, val_acc:0.940]
Epoch [55/120    avg_loss:0.078, val_acc:0.947]
Epoch [56/120    avg_loss:0.088, val_acc:0.943]
Epoch [57/120    avg_loss:0.076, val_acc:0.960]
Epoch [58/120    avg_loss:0.063, val_acc:0.962]
Epoch [59/120    avg_loss:0.059, val_acc:0.965]
Epoch [60/120    avg_loss:0.071, val_acc:0.956]
Epoch [61/120    avg_loss:0.076, val_acc:0.956]
Epoch [62/120    avg_loss:0.081, val_acc:0.962]
Epoch [63/120    avg_loss:0.060, val_acc:0.967]
Epoch [64/120    avg_loss:0.064, val_acc:0.957]
Epoch [65/120    avg_loss:0.059, val_acc:0.956]
Epoch [66/120    avg_loss:0.068, val_acc:0.931]
Epoch [67/120    avg_loss:0.060, val_acc:0.960]
Epoch [68/120    avg_loss:0.056, val_acc:0.962]
Epoch [69/120    avg_loss:0.049, val_acc:0.947]
Epoch [70/120    avg_loss:0.057, val_acc:0.964]
Epoch [71/120    avg_loss:0.039, val_acc:0.954]
Epoch [72/120    avg_loss:0.052, val_acc:0.974]
Epoch [73/120    avg_loss:0.037, val_acc:0.970]
Epoch [74/120    avg_loss:0.034, val_acc:0.962]
Epoch [75/120    avg_loss:0.054, val_acc:0.961]
Epoch [76/120    avg_loss:0.046, val_acc:0.977]
Epoch [77/120    avg_loss:0.034, val_acc:0.956]
Epoch [78/120    avg_loss:0.029, val_acc:0.969]
Epoch [79/120    avg_loss:0.032, val_acc:0.968]
Epoch [80/120    avg_loss:0.023, val_acc:0.969]
Epoch [81/120    avg_loss:0.023, val_acc:0.975]
Epoch [82/120    avg_loss:0.047, val_acc:0.950]
Epoch [83/120    avg_loss:0.038, val_acc:0.970]
Epoch [84/120    avg_loss:0.025, val_acc:0.979]
Epoch [85/120    avg_loss:0.028, val_acc:0.969]
Epoch [86/120    avg_loss:0.022, val_acc:0.978]
Epoch [87/120    avg_loss:0.035, val_acc:0.974]
Epoch [88/120    avg_loss:0.022, val_acc:0.972]
Epoch [89/120    avg_loss:0.026, val_acc:0.966]
Epoch [90/120    avg_loss:0.024, val_acc:0.980]
Epoch [91/120    avg_loss:0.027, val_acc:0.970]
Epoch [92/120    avg_loss:0.041, val_acc:0.962]
Epoch [93/120    avg_loss:0.051, val_acc:0.943]
Epoch [94/120    avg_loss:0.043, val_acc:0.944]
Epoch [95/120    avg_loss:0.039, val_acc:0.974]
Epoch [96/120    avg_loss:0.017, val_acc:0.972]
Epoch [97/120    avg_loss:0.021, val_acc:0.972]
Epoch [98/120    avg_loss:0.027, val_acc:0.959]
Epoch [99/120    avg_loss:0.028, val_acc:0.968]
Epoch [100/120    avg_loss:0.027, val_acc:0.958]
Epoch [101/120    avg_loss:0.049, val_acc:0.959]
Epoch [102/120    avg_loss:0.045, val_acc:0.963]
Epoch [103/120    avg_loss:0.035, val_acc:0.966]
Epoch [104/120    avg_loss:0.026, val_acc:0.976]
Epoch [105/120    avg_loss:0.018, val_acc:0.977]
Epoch [106/120    avg_loss:0.018, val_acc:0.976]
Epoch [107/120    avg_loss:0.014, val_acc:0.977]
Epoch [108/120    avg_loss:0.014, val_acc:0.977]
Epoch [109/120    avg_loss:0.018, val_acc:0.976]
Epoch [110/120    avg_loss:0.022, val_acc:0.978]
Epoch [111/120    avg_loss:0.016, val_acc:0.978]
Epoch [112/120    avg_loss:0.027, val_acc:0.977]
Epoch [113/120    avg_loss:0.012, val_acc:0.976]
Epoch [114/120    avg_loss:0.017, val_acc:0.976]
Epoch [115/120    avg_loss:0.017, val_acc:0.977]
Epoch [116/120    avg_loss:0.014, val_acc:0.977]
Epoch [117/120    avg_loss:0.011, val_acc:0.977]
Epoch [118/120    avg_loss:0.012, val_acc:0.977]
Epoch [119/120    avg_loss:0.012, val_acc:0.977]
Epoch [120/120    avg_loss:0.018, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1264    0    4    0    0    0    0    0    4   11    0    0
     0    2    0]
 [   0    0    3  707    0    2    0    0    0   12    1    1   21    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   15    0    0    8    0    0    0    0  846    4    1    0
     1    0    0]
 [   0    1   10    4    0    0    2    0    0    0   16 2173    0    4
     0    0    0]
 [   0    0    0    0    0   15    0    0    0    1    0    0  509    0
     2    3    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    4    0    0    0    1    0    0    0
  1127    7    0]
 [   0    0    0    0    0    0   37    0    0    0    0    0    0    0
    71  239    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.98644986449864

F1 scores:
[       nan 0.98795181 0.98098564 0.96982167 0.99069767 0.9674523
 0.96831245 1.         0.99883586 0.73469388 0.9707401  0.98795181
 0.95407685 0.98930481 0.9616041  0.7993311  0.97076023]

Kappa:
0.9656382021780896
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f27545dfac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.748, val_acc:0.400]
Epoch [2/120    avg_loss:2.553, val_acc:0.469]
Epoch [3/120    avg_loss:2.352, val_acc:0.506]
Epoch [4/120    avg_loss:2.164, val_acc:0.520]
Epoch [5/120    avg_loss:1.998, val_acc:0.560]
Epoch [6/120    avg_loss:1.846, val_acc:0.603]
Epoch [7/120    avg_loss:1.675, val_acc:0.643]
Epoch [8/120    avg_loss:1.552, val_acc:0.669]
Epoch [9/120    avg_loss:1.426, val_acc:0.659]
Epoch [10/120    avg_loss:1.313, val_acc:0.655]
Epoch [11/120    avg_loss:1.147, val_acc:0.726]
Epoch [12/120    avg_loss:1.011, val_acc:0.727]
Epoch [13/120    avg_loss:0.975, val_acc:0.794]
Epoch [14/120    avg_loss:0.937, val_acc:0.783]
Epoch [15/120    avg_loss:0.825, val_acc:0.767]
Epoch [16/120    avg_loss:0.774, val_acc:0.758]
Epoch [17/120    avg_loss:0.661, val_acc:0.802]
Epoch [18/120    avg_loss:0.615, val_acc:0.804]
Epoch [19/120    avg_loss:0.606, val_acc:0.827]
Epoch [20/120    avg_loss:0.529, val_acc:0.855]
Epoch [21/120    avg_loss:0.453, val_acc:0.856]
Epoch [22/120    avg_loss:0.454, val_acc:0.855]
Epoch [23/120    avg_loss:0.427, val_acc:0.862]
Epoch [24/120    avg_loss:0.368, val_acc:0.851]
Epoch [25/120    avg_loss:0.349, val_acc:0.891]
Epoch [26/120    avg_loss:0.302, val_acc:0.885]
Epoch [27/120    avg_loss:0.257, val_acc:0.875]
Epoch [28/120    avg_loss:0.280, val_acc:0.846]
Epoch [29/120    avg_loss:0.323, val_acc:0.877]
Epoch [30/120    avg_loss:0.236, val_acc:0.903]
Epoch [31/120    avg_loss:0.234, val_acc:0.912]
Epoch [32/120    avg_loss:0.197, val_acc:0.912]
Epoch [33/120    avg_loss:0.244, val_acc:0.879]
Epoch [34/120    avg_loss:0.207, val_acc:0.927]
Epoch [35/120    avg_loss:0.207, val_acc:0.925]
Epoch [36/120    avg_loss:0.179, val_acc:0.939]
Epoch [37/120    avg_loss:0.129, val_acc:0.935]
Epoch [38/120    avg_loss:0.152, val_acc:0.940]
Epoch [39/120    avg_loss:0.148, val_acc:0.936]
Epoch [40/120    avg_loss:0.133, val_acc:0.933]
Epoch [41/120    avg_loss:0.125, val_acc:0.943]
Epoch [42/120    avg_loss:0.137, val_acc:0.925]
Epoch [43/120    avg_loss:0.102, val_acc:0.929]
Epoch [44/120    avg_loss:0.092, val_acc:0.943]
Epoch [45/120    avg_loss:0.097, val_acc:0.929]
Epoch [46/120    avg_loss:0.113, val_acc:0.941]
Epoch [47/120    avg_loss:0.092, val_acc:0.943]
Epoch [48/120    avg_loss:0.120, val_acc:0.934]
Epoch [49/120    avg_loss:0.175, val_acc:0.909]
Epoch [50/120    avg_loss:0.426, val_acc:0.878]
Epoch [51/120    avg_loss:0.253, val_acc:0.929]
Epoch [52/120    avg_loss:0.146, val_acc:0.940]
Epoch [53/120    avg_loss:0.109, val_acc:0.949]
Epoch [54/120    avg_loss:0.095, val_acc:0.952]
Epoch [55/120    avg_loss:0.095, val_acc:0.911]
Epoch [56/120    avg_loss:0.104, val_acc:0.936]
Epoch [57/120    avg_loss:0.085, val_acc:0.948]
Epoch [58/120    avg_loss:0.085, val_acc:0.957]
Epoch [59/120    avg_loss:0.075, val_acc:0.959]
Epoch [60/120    avg_loss:0.068, val_acc:0.959]
Epoch [61/120    avg_loss:0.073, val_acc:0.944]
Epoch [62/120    avg_loss:0.066, val_acc:0.960]
Epoch [63/120    avg_loss:0.087, val_acc:0.945]
Epoch [64/120    avg_loss:0.064, val_acc:0.956]
Epoch [65/120    avg_loss:0.044, val_acc:0.959]
Epoch [66/120    avg_loss:0.061, val_acc:0.966]
Epoch [67/120    avg_loss:0.068, val_acc:0.958]
Epoch [68/120    avg_loss:0.063, val_acc:0.957]
Epoch [69/120    avg_loss:0.063, val_acc:0.934]
Epoch [70/120    avg_loss:0.046, val_acc:0.963]
Epoch [71/120    avg_loss:0.041, val_acc:0.958]
Epoch [72/120    avg_loss:0.059, val_acc:0.957]
Epoch [73/120    avg_loss:0.044, val_acc:0.966]
Epoch [74/120    avg_loss:0.062, val_acc:0.967]
Epoch [75/120    avg_loss:0.080, val_acc:0.960]
Epoch [76/120    avg_loss:0.044, val_acc:0.967]
Epoch [77/120    avg_loss:0.043, val_acc:0.955]
Epoch [78/120    avg_loss:0.058, val_acc:0.969]
Epoch [79/120    avg_loss:0.044, val_acc:0.969]
Epoch [80/120    avg_loss:0.042, val_acc:0.974]
Epoch [81/120    avg_loss:0.032, val_acc:0.963]
Epoch [82/120    avg_loss:0.034, val_acc:0.972]
Epoch [83/120    avg_loss:0.031, val_acc:0.967]
Epoch [84/120    avg_loss:0.032, val_acc:0.975]
Epoch [85/120    avg_loss:0.035, val_acc:0.972]
Epoch [86/120    avg_loss:0.029, val_acc:0.970]
Epoch [87/120    avg_loss:0.033, val_acc:0.974]
Epoch [88/120    avg_loss:0.033, val_acc:0.970]
Epoch [89/120    avg_loss:0.029, val_acc:0.964]
Epoch [90/120    avg_loss:0.024, val_acc:0.981]
Epoch [91/120    avg_loss:0.040, val_acc:0.977]
Epoch [92/120    avg_loss:0.022, val_acc:0.977]
Epoch [93/120    avg_loss:0.020, val_acc:0.974]
Epoch [94/120    avg_loss:0.027, val_acc:0.967]
Epoch [95/120    avg_loss:0.019, val_acc:0.976]
Epoch [96/120    avg_loss:0.019, val_acc:0.980]
Epoch [97/120    avg_loss:0.032, val_acc:0.973]
Epoch [98/120    avg_loss:0.032, val_acc:0.970]
Epoch [99/120    avg_loss:0.018, val_acc:0.980]
Epoch [100/120    avg_loss:0.022, val_acc:0.960]
Epoch [101/120    avg_loss:0.029, val_acc:0.978]
Epoch [102/120    avg_loss:0.023, val_acc:0.977]
Epoch [103/120    avg_loss:0.019, val_acc:0.984]
Epoch [104/120    avg_loss:0.027, val_acc:0.983]
Epoch [105/120    avg_loss:0.019, val_acc:0.963]
Epoch [106/120    avg_loss:0.022, val_acc:0.971]
Epoch [107/120    avg_loss:0.022, val_acc:0.974]
Epoch [108/120    avg_loss:0.025, val_acc:0.975]
Epoch [109/120    avg_loss:0.013, val_acc:0.981]
Epoch [110/120    avg_loss:0.011, val_acc:0.980]
Epoch [111/120    avg_loss:0.013, val_acc:0.983]
Epoch [112/120    avg_loss:0.017, val_acc:0.978]
Epoch [113/120    avg_loss:0.010, val_acc:0.981]
Epoch [114/120    avg_loss:0.009, val_acc:0.977]
Epoch [115/120    avg_loss:0.015, val_acc:0.976]
Epoch [116/120    avg_loss:0.012, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.982]
Epoch [118/120    avg_loss:0.012, val_acc:0.982]
Epoch [119/120    avg_loss:0.009, val_acc:0.980]
Epoch [120/120    avg_loss:0.008, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1255    0    3    2    1    0    0    0    3   17    0    0
     0    4    0]
 [   0    0    0  698    0   20    0    0    0    3    0    1   25    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    1    0    0    5    0    0    0    0  847   18    0    0
     0    4    0]
 [   0    0    0    6    0    0    1    0    0    0   24 2176    0    1
     2    0    0]
 [   0    0    0    0    0    6    0    0    0    0    6    4  514    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    0    0    0
  1134    3    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    40  297    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.5609756097561

F1 scores:
[       nan 0.975      0.98780008 0.96209511 0.99300699 0.95893452
 0.99095023 0.98039216 0.99883586 0.89473684 0.96359499 0.9830585
 0.95450325 0.99730458 0.97842968 0.90687023 0.97619048]

Kappa:
0.9721885633959462
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3584f48a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.744, val_acc:0.335]
Epoch [2/120    avg_loss:2.542, val_acc:0.426]
Epoch [3/120    avg_loss:2.345, val_acc:0.471]
Epoch [4/120    avg_loss:2.168, val_acc:0.541]
Epoch [5/120    avg_loss:2.037, val_acc:0.565]
Epoch [6/120    avg_loss:1.897, val_acc:0.596]
Epoch [7/120    avg_loss:1.773, val_acc:0.627]
Epoch [8/120    avg_loss:1.601, val_acc:0.654]
Epoch [9/120    avg_loss:1.503, val_acc:0.620]
Epoch [10/120    avg_loss:1.404, val_acc:0.671]
Epoch [11/120    avg_loss:1.259, val_acc:0.682]
Epoch [12/120    avg_loss:1.144, val_acc:0.702]
Epoch [13/120    avg_loss:0.991, val_acc:0.742]
Epoch [14/120    avg_loss:0.927, val_acc:0.737]
Epoch [15/120    avg_loss:0.790, val_acc:0.752]
Epoch [16/120    avg_loss:0.722, val_acc:0.814]
Epoch [17/120    avg_loss:0.707, val_acc:0.758]
Epoch [18/120    avg_loss:0.747, val_acc:0.822]
Epoch [19/120    avg_loss:0.594, val_acc:0.837]
Epoch [20/120    avg_loss:0.626, val_acc:0.793]
Epoch [21/120    avg_loss:0.525, val_acc:0.836]
Epoch [22/120    avg_loss:0.456, val_acc:0.846]
Epoch [23/120    avg_loss:0.407, val_acc:0.879]
Epoch [24/120    avg_loss:0.333, val_acc:0.862]
Epoch [25/120    avg_loss:0.342, val_acc:0.795]
Epoch [26/120    avg_loss:0.404, val_acc:0.852]
Epoch [27/120    avg_loss:0.296, val_acc:0.873]
Epoch [28/120    avg_loss:0.316, val_acc:0.871]
Epoch [29/120    avg_loss:0.267, val_acc:0.898]
Epoch [30/120    avg_loss:0.268, val_acc:0.911]
Epoch [31/120    avg_loss:0.223, val_acc:0.908]
Epoch [32/120    avg_loss:0.203, val_acc:0.923]
Epoch [33/120    avg_loss:0.177, val_acc:0.914]
Epoch [34/120    avg_loss:0.244, val_acc:0.851]
Epoch [35/120    avg_loss:0.231, val_acc:0.892]
Epoch [36/120    avg_loss:0.228, val_acc:0.892]
Epoch [37/120    avg_loss:0.165, val_acc:0.927]
Epoch [38/120    avg_loss:0.163, val_acc:0.916]
Epoch [39/120    avg_loss:0.185, val_acc:0.926]
Epoch [40/120    avg_loss:0.160, val_acc:0.946]
Epoch [41/120    avg_loss:0.133, val_acc:0.900]
Epoch [42/120    avg_loss:0.144, val_acc:0.902]
Epoch [43/120    avg_loss:0.109, val_acc:0.949]
Epoch [44/120    avg_loss:0.151, val_acc:0.938]
Epoch [45/120    avg_loss:0.104, val_acc:0.943]
Epoch [46/120    avg_loss:0.139, val_acc:0.870]
Epoch [47/120    avg_loss:0.123, val_acc:0.934]
Epoch [48/120    avg_loss:0.125, val_acc:0.934]
Epoch [49/120    avg_loss:0.109, val_acc:0.944]
Epoch [50/120    avg_loss:0.102, val_acc:0.949]
Epoch [51/120    avg_loss:0.078, val_acc:0.955]
Epoch [52/120    avg_loss:0.066, val_acc:0.958]
Epoch [53/120    avg_loss:0.061, val_acc:0.958]
Epoch [54/120    avg_loss:0.056, val_acc:0.960]
Epoch [55/120    avg_loss:0.056, val_acc:0.959]
Epoch [56/120    avg_loss:0.072, val_acc:0.948]
Epoch [57/120    avg_loss:0.120, val_acc:0.943]
Epoch [58/120    avg_loss:0.081, val_acc:0.956]
Epoch [59/120    avg_loss:0.065, val_acc:0.953]
Epoch [60/120    avg_loss:0.097, val_acc:0.939]
Epoch [61/120    avg_loss:0.064, val_acc:0.960]
Epoch [62/120    avg_loss:0.091, val_acc:0.944]
Epoch [63/120    avg_loss:0.079, val_acc:0.950]
Epoch [64/120    avg_loss:0.059, val_acc:0.939]
Epoch [65/120    avg_loss:0.073, val_acc:0.943]
Epoch [66/120    avg_loss:0.046, val_acc:0.948]
Epoch [67/120    avg_loss:0.054, val_acc:0.952]
Epoch [68/120    avg_loss:0.041, val_acc:0.957]
Epoch [69/120    avg_loss:0.047, val_acc:0.953]
Epoch [70/120    avg_loss:0.074, val_acc:0.934]
Epoch [71/120    avg_loss:0.073, val_acc:0.942]
Epoch [72/120    avg_loss:0.047, val_acc:0.955]
Epoch [73/120    avg_loss:0.089, val_acc:0.954]
Epoch [74/120    avg_loss:0.043, val_acc:0.962]
Epoch [75/120    avg_loss:0.047, val_acc:0.938]
Epoch [76/120    avg_loss:0.067, val_acc:0.933]
Epoch [77/120    avg_loss:0.040, val_acc:0.961]
Epoch [78/120    avg_loss:0.038, val_acc:0.956]
Epoch [79/120    avg_loss:0.037, val_acc:0.968]
Epoch [80/120    avg_loss:0.033, val_acc:0.969]
Epoch [81/120    avg_loss:0.034, val_acc:0.957]
Epoch [82/120    avg_loss:0.044, val_acc:0.961]
Epoch [83/120    avg_loss:0.036, val_acc:0.956]
Epoch [84/120    avg_loss:0.043, val_acc:0.958]
Epoch [85/120    avg_loss:0.048, val_acc:0.950]
Epoch [86/120    avg_loss:0.033, val_acc:0.958]
Epoch [87/120    avg_loss:0.036, val_acc:0.954]
Epoch [88/120    avg_loss:0.035, val_acc:0.963]
Epoch [89/120    avg_loss:0.033, val_acc:0.970]
Epoch [90/120    avg_loss:0.039, val_acc:0.972]
Epoch [91/120    avg_loss:0.037, val_acc:0.960]
Epoch [92/120    avg_loss:0.026, val_acc:0.968]
Epoch [93/120    avg_loss:0.028, val_acc:0.970]
Epoch [94/120    avg_loss:0.018, val_acc:0.966]
Epoch [95/120    avg_loss:0.025, val_acc:0.969]
Epoch [96/120    avg_loss:0.024, val_acc:0.971]
Epoch [97/120    avg_loss:0.027, val_acc:0.967]
Epoch [98/120    avg_loss:0.026, val_acc:0.962]
Epoch [99/120    avg_loss:0.025, val_acc:0.960]
Epoch [100/120    avg_loss:0.032, val_acc:0.966]
Epoch [101/120    avg_loss:0.019, val_acc:0.973]
Epoch [102/120    avg_loss:0.015, val_acc:0.973]
Epoch [103/120    avg_loss:0.015, val_acc:0.972]
Epoch [104/120    avg_loss:0.015, val_acc:0.970]
Epoch [105/120    avg_loss:0.014, val_acc:0.975]
Epoch [106/120    avg_loss:0.016, val_acc:0.973]
Epoch [107/120    avg_loss:0.026, val_acc:0.970]
Epoch [108/120    avg_loss:0.015, val_acc:0.969]
Epoch [109/120    avg_loss:0.011, val_acc:0.972]
Epoch [110/120    avg_loss:0.017, val_acc:0.969]
Epoch [111/120    avg_loss:0.014, val_acc:0.976]
Epoch [112/120    avg_loss:0.016, val_acc:0.974]
Epoch [113/120    avg_loss:0.043, val_acc:0.933]
Epoch [114/120    avg_loss:0.124, val_acc:0.907]
Epoch [115/120    avg_loss:0.114, val_acc:0.962]
Epoch [116/120    avg_loss:0.055, val_acc:0.958]
Epoch [117/120    avg_loss:0.048, val_acc:0.963]
Epoch [118/120    avg_loss:0.040, val_acc:0.964]
Epoch [119/120    avg_loss:0.036, val_acc:0.942]
Epoch [120/120    avg_loss:0.053, val_acc:0.952]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1143    1    0    6    0    0    0    0   25  108    0    0
     0    2    0]
 [   0    0    7  699    1   14    0    0    0    7    0   13    5    0
     1    0    0]
 [   0    0    0    0  212    0    1    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    3    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  649    0    0    0    0    2    0    0
     6    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   15    0    1    1    0
     0    0    0]
 [   0    0    7    6    0   14    2    0    0    0  711  133    0    0
     0    2    0]
 [   0    0    2    0    0    0    2    0    0    0    0 2187   16    0
     2    0    1]
 [   0    0    0    0    2    7    0    0    0    0    0   26  491    0
     0    4    4]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1130    8    0]
 [   0    0    0    0    0    0   32    0    0    0    0    0    0    0
    13  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
94.5691056910569

F1 scores:
[       nan 0.94871795 0.93535188 0.96148556 0.99065421 0.95038589
 0.96649293 0.94339623 0.99649942 0.75       0.8799505  0.93421615
 0.9325736  0.99728997 0.98646879 0.90827068 0.95294118]

Kappa:
0.937832212905945
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5cfb3eeb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.725, val_acc:0.409]
Epoch [2/120    avg_loss:2.504, val_acc:0.486]
Epoch [3/120    avg_loss:2.350, val_acc:0.489]
Epoch [4/120    avg_loss:2.204, val_acc:0.499]
Epoch [5/120    avg_loss:2.059, val_acc:0.528]
Epoch [6/120    avg_loss:1.902, val_acc:0.578]
Epoch [7/120    avg_loss:1.767, val_acc:0.625]
Epoch [8/120    avg_loss:1.660, val_acc:0.642]
Epoch [9/120    avg_loss:1.474, val_acc:0.696]
Epoch [10/120    avg_loss:1.320, val_acc:0.674]
Epoch [11/120    avg_loss:1.185, val_acc:0.727]
Epoch [12/120    avg_loss:1.062, val_acc:0.736]
Epoch [13/120    avg_loss:0.928, val_acc:0.740]
Epoch [14/120    avg_loss:0.864, val_acc:0.769]
Epoch [15/120    avg_loss:0.782, val_acc:0.785]
Epoch [16/120    avg_loss:0.723, val_acc:0.800]
Epoch [17/120    avg_loss:0.655, val_acc:0.807]
Epoch [18/120    avg_loss:0.596, val_acc:0.832]
Epoch [19/120    avg_loss:0.621, val_acc:0.802]
Epoch [20/120    avg_loss:0.519, val_acc:0.828]
Epoch [21/120    avg_loss:0.457, val_acc:0.853]
Epoch [22/120    avg_loss:0.475, val_acc:0.775]
Epoch [23/120    avg_loss:0.422, val_acc:0.842]
Epoch [24/120    avg_loss:0.421, val_acc:0.862]
Epoch [25/120    avg_loss:0.430, val_acc:0.806]
Epoch [26/120    avg_loss:0.340, val_acc:0.859]
Epoch [27/120    avg_loss:0.330, val_acc:0.882]
Epoch [28/120    avg_loss:0.287, val_acc:0.871]
Epoch [29/120    avg_loss:0.266, val_acc:0.901]
Epoch [30/120    avg_loss:0.225, val_acc:0.899]
Epoch [31/120    avg_loss:0.362, val_acc:0.898]
Epoch [32/120    avg_loss:0.271, val_acc:0.887]
Epoch [33/120    avg_loss:0.255, val_acc:0.903]
Epoch [34/120    avg_loss:0.181, val_acc:0.916]
Epoch [35/120    avg_loss:0.141, val_acc:0.914]
Epoch [36/120    avg_loss:0.170, val_acc:0.917]
Epoch [37/120    avg_loss:0.130, val_acc:0.931]
Epoch [38/120    avg_loss:0.193, val_acc:0.912]
Epoch [39/120    avg_loss:0.160, val_acc:0.924]
Epoch [40/120    avg_loss:0.377, val_acc:0.840]
Epoch [41/120    avg_loss:0.288, val_acc:0.886]
Epoch [42/120    avg_loss:0.173, val_acc:0.917]
Epoch [43/120    avg_loss:0.156, val_acc:0.911]
Epoch [44/120    avg_loss:0.130, val_acc:0.931]
Epoch [45/120    avg_loss:0.149, val_acc:0.930]
Epoch [46/120    avg_loss:0.115, val_acc:0.936]
Epoch [47/120    avg_loss:0.123, val_acc:0.928]
Epoch [48/120    avg_loss:0.124, val_acc:0.928]
Epoch [49/120    avg_loss:0.110, val_acc:0.945]
Epoch [50/120    avg_loss:0.101, val_acc:0.924]
Epoch [51/120    avg_loss:0.108, val_acc:0.924]
Epoch [52/120    avg_loss:0.081, val_acc:0.938]
Epoch [53/120    avg_loss:0.063, val_acc:0.954]
Epoch [54/120    avg_loss:0.065, val_acc:0.946]
Epoch [55/120    avg_loss:0.068, val_acc:0.951]
Epoch [56/120    avg_loss:0.071, val_acc:0.952]
Epoch [57/120    avg_loss:0.075, val_acc:0.962]
Epoch [58/120    avg_loss:0.058, val_acc:0.959]
Epoch [59/120    avg_loss:0.068, val_acc:0.947]
Epoch [60/120    avg_loss:0.065, val_acc:0.946]
Epoch [61/120    avg_loss:0.068, val_acc:0.931]
Epoch [62/120    avg_loss:0.081, val_acc:0.952]
Epoch [63/120    avg_loss:0.086, val_acc:0.960]
Epoch [64/120    avg_loss:0.070, val_acc:0.964]
Epoch [65/120    avg_loss:0.076, val_acc:0.953]
Epoch [66/120    avg_loss:0.048, val_acc:0.968]
Epoch [67/120    avg_loss:0.066, val_acc:0.966]
Epoch [68/120    avg_loss:0.037, val_acc:0.975]
Epoch [69/120    avg_loss:0.045, val_acc:0.968]
Epoch [70/120    avg_loss:0.028, val_acc:0.978]
Epoch [71/120    avg_loss:0.048, val_acc:0.948]
Epoch [72/120    avg_loss:0.062, val_acc:0.961]
Epoch [73/120    avg_loss:0.045, val_acc:0.971]
Epoch [74/120    avg_loss:0.044, val_acc:0.974]
Epoch [75/120    avg_loss:0.038, val_acc:0.974]
Epoch [76/120    avg_loss:0.023, val_acc:0.976]
Epoch [77/120    avg_loss:0.033, val_acc:0.962]
Epoch [78/120    avg_loss:0.044, val_acc:0.963]
Epoch [79/120    avg_loss:0.045, val_acc:0.967]
Epoch [80/120    avg_loss:0.038, val_acc:0.965]
Epoch [81/120    avg_loss:0.035, val_acc:0.968]
Epoch [82/120    avg_loss:0.038, val_acc:0.963]
Epoch [83/120    avg_loss:0.032, val_acc:0.950]
Epoch [84/120    avg_loss:0.032, val_acc:0.976]
Epoch [85/120    avg_loss:0.024, val_acc:0.976]
Epoch [86/120    avg_loss:0.017, val_acc:0.977]
Epoch [87/120    avg_loss:0.019, val_acc:0.982]
Epoch [88/120    avg_loss:0.020, val_acc:0.982]
Epoch [89/120    avg_loss:0.016, val_acc:0.981]
Epoch [90/120    avg_loss:0.016, val_acc:0.980]
Epoch [91/120    avg_loss:0.013, val_acc:0.980]
Epoch [92/120    avg_loss:0.018, val_acc:0.981]
Epoch [93/120    avg_loss:0.019, val_acc:0.982]
Epoch [94/120    avg_loss:0.024, val_acc:0.984]
Epoch [95/120    avg_loss:0.018, val_acc:0.982]
Epoch [96/120    avg_loss:0.020, val_acc:0.982]
Epoch [97/120    avg_loss:0.014, val_acc:0.982]
Epoch [98/120    avg_loss:0.018, val_acc:0.981]
Epoch [99/120    avg_loss:0.013, val_acc:0.983]
Epoch [100/120    avg_loss:0.013, val_acc:0.983]
Epoch [101/120    avg_loss:0.016, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.983]
Epoch [103/120    avg_loss:0.013, val_acc:0.982]
Epoch [104/120    avg_loss:0.013, val_acc:0.981]
Epoch [105/120    avg_loss:0.015, val_acc:0.981]
Epoch [106/120    avg_loss:0.014, val_acc:0.981]
Epoch [107/120    avg_loss:0.015, val_acc:0.982]
Epoch [108/120    avg_loss:0.014, val_acc:0.983]
Epoch [109/120    avg_loss:0.019, val_acc:0.983]
Epoch [110/120    avg_loss:0.011, val_acc:0.983]
Epoch [111/120    avg_loss:0.012, val_acc:0.983]
Epoch [112/120    avg_loss:0.013, val_acc:0.983]
Epoch [113/120    avg_loss:0.016, val_acc:0.983]
Epoch [114/120    avg_loss:0.014, val_acc:0.984]
Epoch [115/120    avg_loss:0.014, val_acc:0.983]
Epoch [116/120    avg_loss:0.014, val_acc:0.982]
Epoch [117/120    avg_loss:0.013, val_acc:0.982]
Epoch [118/120    avg_loss:0.015, val_acc:0.982]
Epoch [119/120    avg_loss:0.011, val_acc:0.983]
Epoch [120/120    avg_loss:0.011, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1241    2    5    0    1    0    0    1    9   26    0    0
     0    0    0]
 [   0    0    1  723    6    0    0    0    0    3    1    2   10    1
     0    0    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    6    0    0    0    0  848   10    1    0
     0    0    0]
 [   0    0    7    0    0    0    5    0    0    0   16 2178    4    0
     0    0    0]
 [   0    0    0    3    8    9    0    0    0    1    1    5  505    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1138    1    0]
 [   0    0    0    0    0    0    0    0    0    3    0    0    0    0
    55  289    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.53929539295393

F1 scores:
[       nan 0.975      0.97562893 0.9796748  0.95259594 0.98074745
 0.99394856 1.         1.         0.74418605 0.96803653 0.98285199
 0.95372993 0.99730458 0.97598628 0.90737834 0.96385542]

Kappa:
0.9719333520598453
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f569edddac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.795, val_acc:0.221]
Epoch [2/120    avg_loss:2.607, val_acc:0.356]
Epoch [3/120    avg_loss:2.410, val_acc:0.418]
Epoch [4/120    avg_loss:2.236, val_acc:0.491]
Epoch [5/120    avg_loss:2.057, val_acc:0.510]
Epoch [6/120    avg_loss:1.931, val_acc:0.550]
Epoch [7/120    avg_loss:1.854, val_acc:0.577]
Epoch [8/120    avg_loss:1.781, val_acc:0.617]
Epoch [9/120    avg_loss:1.619, val_acc:0.656]
Epoch [10/120    avg_loss:1.487, val_acc:0.630]
Epoch [11/120    avg_loss:1.394, val_acc:0.663]
Epoch [12/120    avg_loss:1.283, val_acc:0.691]
Epoch [13/120    avg_loss:1.123, val_acc:0.724]
Epoch [14/120    avg_loss:0.995, val_acc:0.739]
Epoch [15/120    avg_loss:0.909, val_acc:0.776]
Epoch [16/120    avg_loss:0.761, val_acc:0.770]
Epoch [17/120    avg_loss:0.755, val_acc:0.778]
Epoch [18/120    avg_loss:0.674, val_acc:0.794]
Epoch [19/120    avg_loss:0.655, val_acc:0.781]
Epoch [20/120    avg_loss:0.615, val_acc:0.778]
Epoch [21/120    avg_loss:0.580, val_acc:0.774]
Epoch [22/120    avg_loss:0.541, val_acc:0.782]
Epoch [23/120    avg_loss:0.516, val_acc:0.831]
Epoch [24/120    avg_loss:0.459, val_acc:0.818]
Epoch [25/120    avg_loss:0.468, val_acc:0.825]
Epoch [26/120    avg_loss:0.443, val_acc:0.859]
Epoch [27/120    avg_loss:0.308, val_acc:0.835]
Epoch [28/120    avg_loss:0.277, val_acc:0.908]
Epoch [29/120    avg_loss:0.239, val_acc:0.898]
Epoch [30/120    avg_loss:0.270, val_acc:0.883]
Epoch [31/120    avg_loss:0.239, val_acc:0.905]
Epoch [32/120    avg_loss:0.213, val_acc:0.902]
Epoch [33/120    avg_loss:0.253, val_acc:0.893]
Epoch [34/120    avg_loss:0.251, val_acc:0.870]
Epoch [35/120    avg_loss:0.225, val_acc:0.894]
Epoch [36/120    avg_loss:0.214, val_acc:0.923]
Epoch [37/120    avg_loss:0.164, val_acc:0.922]
Epoch [38/120    avg_loss:0.158, val_acc:0.912]
Epoch [39/120    avg_loss:0.128, val_acc:0.932]
Epoch [40/120    avg_loss:0.117, val_acc:0.945]
Epoch [41/120    avg_loss:0.110, val_acc:0.917]
Epoch [42/120    avg_loss:0.117, val_acc:0.930]
Epoch [43/120    avg_loss:0.122, val_acc:0.933]
Epoch [44/120    avg_loss:0.115, val_acc:0.921]
Epoch [45/120    avg_loss:0.133, val_acc:0.909]
Epoch [46/120    avg_loss:0.179, val_acc:0.909]
Epoch [47/120    avg_loss:0.172, val_acc:0.922]
Epoch [48/120    avg_loss:0.140, val_acc:0.930]
Epoch [49/120    avg_loss:0.113, val_acc:0.939]
Epoch [50/120    avg_loss:0.067, val_acc:0.948]
Epoch [51/120    avg_loss:0.070, val_acc:0.953]
Epoch [52/120    avg_loss:0.097, val_acc:0.921]
Epoch [53/120    avg_loss:0.094, val_acc:0.931]
Epoch [54/120    avg_loss:0.094, val_acc:0.956]
Epoch [55/120    avg_loss:0.094, val_acc:0.930]
Epoch [56/120    avg_loss:0.068, val_acc:0.954]
Epoch [57/120    avg_loss:0.075, val_acc:0.959]
Epoch [58/120    avg_loss:0.056, val_acc:0.956]
Epoch [59/120    avg_loss:0.057, val_acc:0.960]
Epoch [60/120    avg_loss:0.060, val_acc:0.960]
Epoch [61/120    avg_loss:0.057, val_acc:0.959]
Epoch [62/120    avg_loss:0.050, val_acc:0.899]
Epoch [63/120    avg_loss:0.057, val_acc:0.957]
Epoch [64/120    avg_loss:0.068, val_acc:0.947]
Epoch [65/120    avg_loss:0.304, val_acc:0.905]
Epoch [66/120    avg_loss:0.140, val_acc:0.942]
Epoch [67/120    avg_loss:0.099, val_acc:0.922]
Epoch [68/120    avg_loss:0.065, val_acc:0.949]
Epoch [69/120    avg_loss:0.048, val_acc:0.971]
Epoch [70/120    avg_loss:0.047, val_acc:0.966]
Epoch [71/120    avg_loss:0.071, val_acc:0.961]
Epoch [72/120    avg_loss:0.061, val_acc:0.946]
Epoch [73/120    avg_loss:0.058, val_acc:0.958]
Epoch [74/120    avg_loss:0.049, val_acc:0.955]
Epoch [75/120    avg_loss:0.048, val_acc:0.956]
Epoch [76/120    avg_loss:0.051, val_acc:0.960]
Epoch [77/120    avg_loss:0.042, val_acc:0.964]
Epoch [78/120    avg_loss:0.038, val_acc:0.967]
Epoch [79/120    avg_loss:0.039, val_acc:0.960]
Epoch [80/120    avg_loss:0.051, val_acc:0.958]
Epoch [81/120    avg_loss:0.040, val_acc:0.960]
Epoch [82/120    avg_loss:0.024, val_acc:0.964]
Epoch [83/120    avg_loss:0.029, val_acc:0.975]
Epoch [84/120    avg_loss:0.027, val_acc:0.976]
Epoch [85/120    avg_loss:0.018, val_acc:0.977]
Epoch [86/120    avg_loss:0.025, val_acc:0.976]
Epoch [87/120    avg_loss:0.019, val_acc:0.976]
Epoch [88/120    avg_loss:0.022, val_acc:0.974]
Epoch [89/120    avg_loss:0.020, val_acc:0.976]
Epoch [90/120    avg_loss:0.016, val_acc:0.976]
Epoch [91/120    avg_loss:0.021, val_acc:0.977]
Epoch [92/120    avg_loss:0.016, val_acc:0.976]
Epoch [93/120    avg_loss:0.023, val_acc:0.977]
Epoch [94/120    avg_loss:0.016, val_acc:0.975]
Epoch [95/120    avg_loss:0.014, val_acc:0.975]
Epoch [96/120    avg_loss:0.017, val_acc:0.974]
Epoch [97/120    avg_loss:0.015, val_acc:0.974]
Epoch [98/120    avg_loss:0.018, val_acc:0.975]
Epoch [99/120    avg_loss:0.018, val_acc:0.976]
Epoch [100/120    avg_loss:0.018, val_acc:0.975]
Epoch [101/120    avg_loss:0.014, val_acc:0.975]
Epoch [102/120    avg_loss:0.013, val_acc:0.976]
Epoch [103/120    avg_loss:0.018, val_acc:0.974]
Epoch [104/120    avg_loss:0.017, val_acc:0.977]
Epoch [105/120    avg_loss:0.015, val_acc:0.977]
Epoch [106/120    avg_loss:0.019, val_acc:0.978]
Epoch [107/120    avg_loss:0.018, val_acc:0.976]
Epoch [108/120    avg_loss:0.015, val_acc:0.978]
Epoch [109/120    avg_loss:0.018, val_acc:0.976]
Epoch [110/120    avg_loss:0.017, val_acc:0.976]
Epoch [111/120    avg_loss:0.020, val_acc:0.977]
Epoch [112/120    avg_loss:0.017, val_acc:0.981]
Epoch [113/120    avg_loss:0.012, val_acc:0.976]
Epoch [114/120    avg_loss:0.015, val_acc:0.977]
Epoch [115/120    avg_loss:0.017, val_acc:0.975]
Epoch [116/120    avg_loss:0.020, val_acc:0.977]
Epoch [117/120    avg_loss:0.017, val_acc:0.977]
Epoch [118/120    avg_loss:0.016, val_acc:0.977]
Epoch [119/120    avg_loss:0.015, val_acc:0.978]
Epoch [120/120    avg_loss:0.014, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1258    0    2    0    0    0    0    0    3   22    0    0
     0    0    0]
 [   0    0    5  709   10   10    0    0    0    2    0    0   11    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    3    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   14    0    0    2    0    0    0    0  848   10    0    0
     0    1    0]
 [   0    0    6    0    0    0    2    0    0    0   17 2176    9    0
     0    0    0]
 [   0    0    6    0    0    1    0    0    0    0    7    0  516    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1133    5    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    82  265    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.3550135501355

F1 scores:
[       nan 0.96296296 0.97746698 0.9739011  0.97260274 0.97716895
 0.99848024 0.94339623 0.99883586 0.87179487 0.96748431 0.98506111
 0.96268657 1.         0.96098388 0.85760518 0.98224852]

Kappa:
0.9698253226819721
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f55d715eb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.756, val_acc:0.240]
Epoch [2/120    avg_loss:2.555, val_acc:0.446]
Epoch [3/120    avg_loss:2.380, val_acc:0.573]
Epoch [4/120    avg_loss:2.217, val_acc:0.588]
Epoch [5/120    avg_loss:2.054, val_acc:0.606]
Epoch [6/120    avg_loss:1.884, val_acc:0.630]
Epoch [7/120    avg_loss:1.748, val_acc:0.610]
Epoch [8/120    avg_loss:1.534, val_acc:0.636]
Epoch [9/120    avg_loss:1.399, val_acc:0.688]
Epoch [10/120    avg_loss:1.264, val_acc:0.690]
Epoch [11/120    avg_loss:1.104, val_acc:0.722]
Epoch [12/120    avg_loss:0.964, val_acc:0.749]
Epoch [13/120    avg_loss:0.915, val_acc:0.768]
Epoch [14/120    avg_loss:0.783, val_acc:0.748]
Epoch [15/120    avg_loss:0.648, val_acc:0.821]
Epoch [16/120    avg_loss:0.665, val_acc:0.802]
Epoch [17/120    avg_loss:0.602, val_acc:0.776]
Epoch [18/120    avg_loss:0.589, val_acc:0.809]
Epoch [19/120    avg_loss:0.473, val_acc:0.858]
Epoch [20/120    avg_loss:0.455, val_acc:0.860]
Epoch [21/120    avg_loss:0.451, val_acc:0.831]
Epoch [22/120    avg_loss:0.399, val_acc:0.849]
Epoch [23/120    avg_loss:0.343, val_acc:0.877]
Epoch [24/120    avg_loss:0.407, val_acc:0.875]
Epoch [25/120    avg_loss:0.347, val_acc:0.874]
Epoch [26/120    avg_loss:0.339, val_acc:0.874]
Epoch [27/120    avg_loss:0.301, val_acc:0.900]
Epoch [28/120    avg_loss:0.248, val_acc:0.898]
Epoch [29/120    avg_loss:0.246, val_acc:0.889]
Epoch [30/120    avg_loss:0.245, val_acc:0.895]
Epoch [31/120    avg_loss:0.246, val_acc:0.901]
Epoch [32/120    avg_loss:0.195, val_acc:0.923]
Epoch [33/120    avg_loss:0.198, val_acc:0.911]
Epoch [34/120    avg_loss:0.159, val_acc:0.925]
Epoch [35/120    avg_loss:0.166, val_acc:0.923]
Epoch [36/120    avg_loss:0.170, val_acc:0.889]
Epoch [37/120    avg_loss:0.164, val_acc:0.918]
Epoch [38/120    avg_loss:0.126, val_acc:0.925]
Epoch [39/120    avg_loss:0.137, val_acc:0.903]
Epoch [40/120    avg_loss:0.139, val_acc:0.934]
Epoch [41/120    avg_loss:0.142, val_acc:0.893]
Epoch [42/120    avg_loss:0.162, val_acc:0.943]
Epoch [43/120    avg_loss:0.098, val_acc:0.936]
Epoch [44/120    avg_loss:0.119, val_acc:0.935]
Epoch [45/120    avg_loss:0.108, val_acc:0.932]
Epoch [46/120    avg_loss:0.294, val_acc:0.881]
Epoch [47/120    avg_loss:0.203, val_acc:0.917]
Epoch [48/120    avg_loss:0.114, val_acc:0.946]
Epoch [49/120    avg_loss:0.074, val_acc:0.935]
Epoch [50/120    avg_loss:0.076, val_acc:0.958]
Epoch [51/120    avg_loss:0.098, val_acc:0.917]
Epoch [52/120    avg_loss:0.094, val_acc:0.948]
Epoch [53/120    avg_loss:0.092, val_acc:0.938]
Epoch [54/120    avg_loss:0.052, val_acc:0.954]
Epoch [55/120    avg_loss:0.066, val_acc:0.952]
Epoch [56/120    avg_loss:0.082, val_acc:0.942]
Epoch [57/120    avg_loss:0.320, val_acc:0.895]
Epoch [58/120    avg_loss:0.184, val_acc:0.929]
Epoch [59/120    avg_loss:0.117, val_acc:0.914]
Epoch [60/120    avg_loss:0.154, val_acc:0.940]
Epoch [61/120    avg_loss:0.119, val_acc:0.945]
Epoch [62/120    avg_loss:0.080, val_acc:0.947]
Epoch [63/120    avg_loss:0.074, val_acc:0.962]
Epoch [64/120    avg_loss:0.073, val_acc:0.962]
Epoch [65/120    avg_loss:0.057, val_acc:0.962]
Epoch [66/120    avg_loss:0.065, val_acc:0.955]
Epoch [67/120    avg_loss:0.059, val_acc:0.964]
Epoch [68/120    avg_loss:0.068, val_acc:0.953]
Epoch [69/120    avg_loss:0.064, val_acc:0.942]
Epoch [70/120    avg_loss:0.040, val_acc:0.958]
Epoch [71/120    avg_loss:0.054, val_acc:0.948]
Epoch [72/120    avg_loss:0.058, val_acc:0.956]
Epoch [73/120    avg_loss:0.046, val_acc:0.966]
Epoch [74/120    avg_loss:0.038, val_acc:0.967]
Epoch [75/120    avg_loss:0.031, val_acc:0.967]
Epoch [76/120    avg_loss:0.035, val_acc:0.962]
Epoch [77/120    avg_loss:0.051, val_acc:0.968]
Epoch [78/120    avg_loss:0.052, val_acc:0.950]
Epoch [79/120    avg_loss:0.034, val_acc:0.972]
Epoch [80/120    avg_loss:0.041, val_acc:0.963]
Epoch [81/120    avg_loss:0.036, val_acc:0.968]
Epoch [82/120    avg_loss:0.030, val_acc:0.975]
Epoch [83/120    avg_loss:0.041, val_acc:0.952]
Epoch [84/120    avg_loss:0.044, val_acc:0.964]
Epoch [85/120    avg_loss:0.036, val_acc:0.958]
Epoch [86/120    avg_loss:0.031, val_acc:0.960]
Epoch [87/120    avg_loss:0.024, val_acc:0.966]
Epoch [88/120    avg_loss:0.026, val_acc:0.968]
Epoch [89/120    avg_loss:0.106, val_acc:0.918]
Epoch [90/120    avg_loss:0.105, val_acc:0.945]
Epoch [91/120    avg_loss:0.061, val_acc:0.966]
Epoch [92/120    avg_loss:0.053, val_acc:0.960]
Epoch [93/120    avg_loss:0.040, val_acc:0.967]
Epoch [94/120    avg_loss:0.031, val_acc:0.950]
Epoch [95/120    avg_loss:0.035, val_acc:0.970]
Epoch [96/120    avg_loss:0.020, val_acc:0.972]
Epoch [97/120    avg_loss:0.021, val_acc:0.974]
Epoch [98/120    avg_loss:0.022, val_acc:0.974]
Epoch [99/120    avg_loss:0.023, val_acc:0.975]
Epoch [100/120    avg_loss:0.017, val_acc:0.975]
Epoch [101/120    avg_loss:0.016, val_acc:0.975]
Epoch [102/120    avg_loss:0.013, val_acc:0.974]
Epoch [103/120    avg_loss:0.020, val_acc:0.974]
Epoch [104/120    avg_loss:0.020, val_acc:0.976]
Epoch [105/120    avg_loss:0.026, val_acc:0.977]
Epoch [106/120    avg_loss:0.017, val_acc:0.977]
Epoch [107/120    avg_loss:0.018, val_acc:0.975]
Epoch [108/120    avg_loss:0.018, val_acc:0.978]
Epoch [109/120    avg_loss:0.015, val_acc:0.977]
Epoch [110/120    avg_loss:0.018, val_acc:0.976]
Epoch [111/120    avg_loss:0.014, val_acc:0.977]
Epoch [112/120    avg_loss:0.018, val_acc:0.976]
Epoch [113/120    avg_loss:0.018, val_acc:0.976]
Epoch [114/120    avg_loss:0.019, val_acc:0.977]
Epoch [115/120    avg_loss:0.013, val_acc:0.977]
Epoch [116/120    avg_loss:0.017, val_acc:0.976]
Epoch [117/120    avg_loss:0.018, val_acc:0.976]
Epoch [118/120    avg_loss:0.014, val_acc:0.977]
Epoch [119/120    avg_loss:0.015, val_acc:0.974]
Epoch [120/120    avg_loss:0.015, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1258    5    0    0    0    0    0    0    7   13    1    0
     0    1    0]
 [   0    0    0  704    4    1    0    0    0    6    2    4   25    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    3    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    5    0    0    0    0  853    1    0    0
     0    7    0]
 [   0    0   12    0    0    1    1    0    0    0   24 2158   13    1
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    0   18  506    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1125   14    0]
 [   0    0    0    0    0    0    5    0    0    3    0    0    0    0
     6  333    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
97.68021680216802

F1 scores:
[       nan 1.         0.98127925 0.96703297 0.99069767 0.97838453
 0.99317665 0.98039216 1.         0.72340426 0.96876775 0.97979569
 0.93271889 0.99462366 0.98988121 0.94871795 0.95121951]

Kappa:
0.973568354132079
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:19:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f28d7173a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.736, val_acc:0.357]
Epoch [2/120    avg_loss:2.527, val_acc:0.407]
Epoch [3/120    avg_loss:2.334, val_acc:0.460]
Epoch [4/120    avg_loss:2.147, val_acc:0.482]
Epoch [5/120    avg_loss:2.003, val_acc:0.516]
Epoch [6/120    avg_loss:1.896, val_acc:0.617]
Epoch [7/120    avg_loss:1.798, val_acc:0.584]
Epoch [8/120    avg_loss:1.659, val_acc:0.655]
Epoch [9/120    avg_loss:1.469, val_acc:0.677]
Epoch [10/120    avg_loss:1.295, val_acc:0.702]
Epoch [11/120    avg_loss:1.223, val_acc:0.731]
Epoch [12/120    avg_loss:1.051, val_acc:0.750]
Epoch [13/120    avg_loss:0.980, val_acc:0.766]
Epoch [14/120    avg_loss:0.882, val_acc:0.798]
Epoch [15/120    avg_loss:0.775, val_acc:0.794]
Epoch [16/120    avg_loss:0.803, val_acc:0.818]
Epoch [17/120    avg_loss:0.646, val_acc:0.816]
Epoch [18/120    avg_loss:0.583, val_acc:0.810]
Epoch [19/120    avg_loss:0.572, val_acc:0.826]
Epoch [20/120    avg_loss:0.516, val_acc:0.827]
Epoch [21/120    avg_loss:0.489, val_acc:0.855]
Epoch [22/120    avg_loss:0.428, val_acc:0.870]
Epoch [23/120    avg_loss:0.441, val_acc:0.842]
Epoch [24/120    avg_loss:0.420, val_acc:0.876]
Epoch [25/120    avg_loss:0.357, val_acc:0.864]
Epoch [26/120    avg_loss:0.447, val_acc:0.821]
Epoch [27/120    avg_loss:0.387, val_acc:0.888]
Epoch [28/120    avg_loss:0.323, val_acc:0.898]
Epoch [29/120    avg_loss:0.276, val_acc:0.853]
Epoch [30/120    avg_loss:0.284, val_acc:0.858]
Epoch [31/120    avg_loss:0.225, val_acc:0.924]
Epoch [32/120    avg_loss:0.193, val_acc:0.928]
Epoch [33/120    avg_loss:0.198, val_acc:0.889]
Epoch [34/120    avg_loss:0.186, val_acc:0.926]
Epoch [35/120    avg_loss:0.205, val_acc:0.926]
Epoch [36/120    avg_loss:0.263, val_acc:0.914]
Epoch [37/120    avg_loss:0.233, val_acc:0.911]
Epoch [38/120    avg_loss:0.151, val_acc:0.945]
Epoch [39/120    avg_loss:0.156, val_acc:0.933]
Epoch [40/120    avg_loss:0.122, val_acc:0.941]
Epoch [41/120    avg_loss:0.148, val_acc:0.927]
Epoch [42/120    avg_loss:0.134, val_acc:0.931]
Epoch [43/120    avg_loss:0.117, val_acc:0.947]
Epoch [44/120    avg_loss:0.135, val_acc:0.907]
Epoch [45/120    avg_loss:0.144, val_acc:0.939]
Epoch [46/120    avg_loss:0.135, val_acc:0.943]
Epoch [47/120    avg_loss:0.101, val_acc:0.929]
Epoch [48/120    avg_loss:0.100, val_acc:0.941]
Epoch [49/120    avg_loss:0.105, val_acc:0.943]
Epoch [50/120    avg_loss:0.097, val_acc:0.948]
Epoch [51/120    avg_loss:0.080, val_acc:0.943]
Epoch [52/120    avg_loss:0.063, val_acc:0.958]
Epoch [53/120    avg_loss:0.088, val_acc:0.947]
Epoch [54/120    avg_loss:0.084, val_acc:0.944]
Epoch [55/120    avg_loss:0.077, val_acc:0.947]
Epoch [56/120    avg_loss:0.062, val_acc:0.971]
Epoch [57/120    avg_loss:0.075, val_acc:0.960]
Epoch [58/120    avg_loss:0.057, val_acc:0.960]
Epoch [59/120    avg_loss:0.055, val_acc:0.952]
Epoch [60/120    avg_loss:0.057, val_acc:0.966]
Epoch [61/120    avg_loss:0.052, val_acc:0.962]
Epoch [62/120    avg_loss:0.050, val_acc:0.951]
Epoch [63/120    avg_loss:0.074, val_acc:0.949]
Epoch [64/120    avg_loss:0.070, val_acc:0.962]
Epoch [65/120    avg_loss:0.047, val_acc:0.959]
Epoch [66/120    avg_loss:0.052, val_acc:0.960]
Epoch [67/120    avg_loss:0.049, val_acc:0.970]
Epoch [68/120    avg_loss:0.080, val_acc:0.967]
Epoch [69/120    avg_loss:0.042, val_acc:0.965]
Epoch [70/120    avg_loss:0.041, val_acc:0.976]
Epoch [71/120    avg_loss:0.029, val_acc:0.977]
Epoch [72/120    avg_loss:0.033, val_acc:0.976]
Epoch [73/120    avg_loss:0.030, val_acc:0.975]
Epoch [74/120    avg_loss:0.027, val_acc:0.975]
Epoch [75/120    avg_loss:0.021, val_acc:0.974]
Epoch [76/120    avg_loss:0.022, val_acc:0.975]
Epoch [77/120    avg_loss:0.025, val_acc:0.976]
Epoch [78/120    avg_loss:0.024, val_acc:0.975]
Epoch [79/120    avg_loss:0.020, val_acc:0.975]
Epoch [80/120    avg_loss:0.021, val_acc:0.974]
Epoch [81/120    avg_loss:0.019, val_acc:0.976]
Epoch [82/120    avg_loss:0.019, val_acc:0.977]
Epoch [83/120    avg_loss:0.022, val_acc:0.978]
Epoch [84/120    avg_loss:0.019, val_acc:0.979]
Epoch [85/120    avg_loss:0.022, val_acc:0.977]
Epoch [86/120    avg_loss:0.021, val_acc:0.977]
Epoch [87/120    avg_loss:0.018, val_acc:0.979]
Epoch [88/120    avg_loss:0.020, val_acc:0.978]
Epoch [89/120    avg_loss:0.018, val_acc:0.978]
Epoch [90/120    avg_loss:0.022, val_acc:0.977]
Epoch [91/120    avg_loss:0.029, val_acc:0.979]
Epoch [92/120    avg_loss:0.023, val_acc:0.977]
Epoch [93/120    avg_loss:0.020, val_acc:0.979]
Epoch [94/120    avg_loss:0.022, val_acc:0.979]
Epoch [95/120    avg_loss:0.020, val_acc:0.978]
Epoch [96/120    avg_loss:0.014, val_acc:0.978]
Epoch [97/120    avg_loss:0.020, val_acc:0.979]
Epoch [98/120    avg_loss:0.019, val_acc:0.979]
Epoch [99/120    avg_loss:0.019, val_acc:0.977]
Epoch [100/120    avg_loss:0.018, val_acc:0.977]
Epoch [101/120    avg_loss:0.019, val_acc:0.980]
Epoch [102/120    avg_loss:0.019, val_acc:0.980]
Epoch [103/120    avg_loss:0.018, val_acc:0.979]
Epoch [104/120    avg_loss:0.020, val_acc:0.981]
Epoch [105/120    avg_loss:0.019, val_acc:0.983]
Epoch [106/120    avg_loss:0.020, val_acc:0.978]
Epoch [107/120    avg_loss:0.018, val_acc:0.981]
Epoch [108/120    avg_loss:0.019, val_acc:0.978]
Epoch [109/120    avg_loss:0.019, val_acc:0.979]
Epoch [110/120    avg_loss:0.021, val_acc:0.978]
Epoch [111/120    avg_loss:0.018, val_acc:0.979]
Epoch [112/120    avg_loss:0.014, val_acc:0.975]
Epoch [113/120    avg_loss:0.017, val_acc:0.977]
Epoch [114/120    avg_loss:0.018, val_acc:0.979]
Epoch [115/120    avg_loss:0.017, val_acc:0.979]
Epoch [116/120    avg_loss:0.017, val_acc:0.979]
Epoch [117/120    avg_loss:0.016, val_acc:0.980]
Epoch [118/120    avg_loss:0.014, val_acc:0.981]
Epoch [119/120    avg_loss:0.014, val_acc:0.980]
Epoch [120/120    avg_loss:0.016, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1242    1    3    0    4    0    0    0    8   25    1    0
     0    1    0]
 [   0    0    2  717    1    4    0    0    0    1    2    2   18    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    2    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0   10    0    0    5    2    0    0    0  847    9    1    0
     0    1    0]
 [   0    0    7    0    0    0    2    0    0    0   14 2187    0    0
     0    0    0]
 [   0    0    5    1    0    9    0    0    0    0    5    0  509    0
     4    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0    0    0    0    2    0    0    0    0
    32  313    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.88617886178862

F1 scores:
[       nan 0.98765432 0.97373579 0.9781719  0.99069767 0.97627119
 0.9908953  0.96153846 1.         0.83333333 0.96689498 0.98602344
 0.95407685 1.         0.9835924  0.94561934 0.98809524]

Kappa:
0.9758897194460673
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7e994caa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.739, val_acc:0.194]
Epoch [2/120    avg_loss:2.507, val_acc:0.301]
Epoch [3/120    avg_loss:2.334, val_acc:0.453]
Epoch [4/120    avg_loss:2.188, val_acc:0.513]
Epoch [5/120    avg_loss:2.056, val_acc:0.546]
Epoch [6/120    avg_loss:1.883, val_acc:0.588]
Epoch [7/120    avg_loss:1.783, val_acc:0.617]
Epoch [8/120    avg_loss:1.697, val_acc:0.630]
Epoch [9/120    avg_loss:1.573, val_acc:0.671]
Epoch [10/120    avg_loss:1.384, val_acc:0.683]
Epoch [11/120    avg_loss:1.244, val_acc:0.701]
Epoch [12/120    avg_loss:1.151, val_acc:0.711]
Epoch [13/120    avg_loss:1.032, val_acc:0.747]
Epoch [14/120    avg_loss:1.036, val_acc:0.729]
Epoch [15/120    avg_loss:0.838, val_acc:0.753]
Epoch [16/120    avg_loss:0.823, val_acc:0.797]
Epoch [17/120    avg_loss:0.693, val_acc:0.798]
Epoch [18/120    avg_loss:0.708, val_acc:0.280]
Epoch [19/120    avg_loss:1.141, val_acc:0.747]
Epoch [20/120    avg_loss:0.663, val_acc:0.779]
Epoch [21/120    avg_loss:0.627, val_acc:0.750]
Epoch [22/120    avg_loss:0.551, val_acc:0.862]
Epoch [23/120    avg_loss:0.480, val_acc:0.861]
Epoch [24/120    avg_loss:0.480, val_acc:0.845]
Epoch [25/120    avg_loss:0.461, val_acc:0.838]
Epoch [26/120    avg_loss:0.423, val_acc:0.807]
Epoch [27/120    avg_loss:0.458, val_acc:0.842]
Epoch [28/120    avg_loss:0.391, val_acc:0.877]
Epoch [29/120    avg_loss:0.306, val_acc:0.889]
Epoch [30/120    avg_loss:0.365, val_acc:0.880]
Epoch [31/120    avg_loss:0.291, val_acc:0.892]
Epoch [32/120    avg_loss:0.298, val_acc:0.835]
Epoch [33/120    avg_loss:0.265, val_acc:0.905]
Epoch [34/120    avg_loss:0.237, val_acc:0.915]
Epoch [35/120    avg_loss:0.200, val_acc:0.886]
Epoch [36/120    avg_loss:0.184, val_acc:0.906]
Epoch [37/120    avg_loss:0.210, val_acc:0.931]
Epoch [38/120    avg_loss:0.197, val_acc:0.918]
Epoch [39/120    avg_loss:0.180, val_acc:0.927]
Epoch [40/120    avg_loss:0.191, val_acc:0.915]
Epoch [41/120    avg_loss:0.158, val_acc:0.930]
Epoch [42/120    avg_loss:0.154, val_acc:0.908]
Epoch [43/120    avg_loss:0.175, val_acc:0.911]
Epoch [44/120    avg_loss:0.121, val_acc:0.942]
Epoch [45/120    avg_loss:0.112, val_acc:0.936]
Epoch [46/120    avg_loss:0.168, val_acc:0.923]
Epoch [47/120    avg_loss:0.127, val_acc:0.935]
Epoch [48/120    avg_loss:0.116, val_acc:0.941]
Epoch [49/120    avg_loss:0.139, val_acc:0.918]
Epoch [50/120    avg_loss:0.122, val_acc:0.949]
Epoch [51/120    avg_loss:0.133, val_acc:0.947]
Epoch [52/120    avg_loss:0.083, val_acc:0.935]
Epoch [53/120    avg_loss:0.085, val_acc:0.949]
Epoch [54/120    avg_loss:0.106, val_acc:0.938]
Epoch [55/120    avg_loss:0.235, val_acc:0.872]
Epoch [56/120    avg_loss:0.158, val_acc:0.943]
Epoch [57/120    avg_loss:0.096, val_acc:0.953]
Epoch [58/120    avg_loss:0.095, val_acc:0.949]
Epoch [59/120    avg_loss:0.092, val_acc:0.951]
Epoch [60/120    avg_loss:0.079, val_acc:0.957]
Epoch [61/120    avg_loss:0.081, val_acc:0.932]
Epoch [62/120    avg_loss:0.084, val_acc:0.947]
Epoch [63/120    avg_loss:0.139, val_acc:0.942]
Epoch [64/120    avg_loss:0.095, val_acc:0.954]
Epoch [65/120    avg_loss:0.055, val_acc:0.961]
Epoch [66/120    avg_loss:0.067, val_acc:0.961]
Epoch [67/120    avg_loss:0.052, val_acc:0.968]
Epoch [68/120    avg_loss:0.051, val_acc:0.967]
Epoch [69/120    avg_loss:0.044, val_acc:0.926]
Epoch [70/120    avg_loss:0.040, val_acc:0.967]
Epoch [71/120    avg_loss:0.058, val_acc:0.953]
Epoch [72/120    avg_loss:0.062, val_acc:0.950]
Epoch [73/120    avg_loss:0.066, val_acc:0.963]
Epoch [74/120    avg_loss:0.055, val_acc:0.954]
Epoch [75/120    avg_loss:0.038, val_acc:0.966]
Epoch [76/120    avg_loss:0.036, val_acc:0.974]
Epoch [77/120    avg_loss:0.030, val_acc:0.963]
Epoch [78/120    avg_loss:0.073, val_acc:0.975]
Epoch [79/120    avg_loss:0.043, val_acc:0.981]
Epoch [80/120    avg_loss:0.036, val_acc:0.967]
Epoch [81/120    avg_loss:0.040, val_acc:0.972]
Epoch [82/120    avg_loss:0.028, val_acc:0.972]
Epoch [83/120    avg_loss:0.035, val_acc:0.976]
Epoch [84/120    avg_loss:0.036, val_acc:0.978]
Epoch [85/120    avg_loss:0.036, val_acc:0.966]
Epoch [86/120    avg_loss:0.050, val_acc:0.971]
Epoch [87/120    avg_loss:0.039, val_acc:0.968]
Epoch [88/120    avg_loss:0.023, val_acc:0.969]
Epoch [89/120    avg_loss:0.036, val_acc:0.977]
Epoch [90/120    avg_loss:0.041, val_acc:0.978]
Epoch [91/120    avg_loss:0.031, val_acc:0.979]
Epoch [92/120    avg_loss:0.021, val_acc:0.978]
Epoch [93/120    avg_loss:0.014, val_acc:0.981]
Epoch [94/120    avg_loss:0.018, val_acc:0.980]
Epoch [95/120    avg_loss:0.014, val_acc:0.980]
Epoch [96/120    avg_loss:0.014, val_acc:0.982]
Epoch [97/120    avg_loss:0.019, val_acc:0.982]
Epoch [98/120    avg_loss:0.023, val_acc:0.981]
Epoch [99/120    avg_loss:0.014, val_acc:0.981]
Epoch [100/120    avg_loss:0.015, val_acc:0.982]
Epoch [101/120    avg_loss:0.012, val_acc:0.982]
Epoch [102/120    avg_loss:0.015, val_acc:0.982]
Epoch [103/120    avg_loss:0.012, val_acc:0.981]
Epoch [104/120    avg_loss:0.014, val_acc:0.981]
Epoch [105/120    avg_loss:0.013, val_acc:0.981]
Epoch [106/120    avg_loss:0.013, val_acc:0.981]
Epoch [107/120    avg_loss:0.014, val_acc:0.981]
Epoch [108/120    avg_loss:0.012, val_acc:0.981]
Epoch [109/120    avg_loss:0.010, val_acc:0.981]
Epoch [110/120    avg_loss:0.011, val_acc:0.981]
Epoch [111/120    avg_loss:0.012, val_acc:0.980]
Epoch [112/120    avg_loss:0.012, val_acc:0.981]
Epoch [113/120    avg_loss:0.011, val_acc:0.981]
Epoch [114/120    avg_loss:0.015, val_acc:0.982]
Epoch [115/120    avg_loss:0.010, val_acc:0.982]
Epoch [116/120    avg_loss:0.011, val_acc:0.982]
Epoch [117/120    avg_loss:0.010, val_acc:0.981]
Epoch [118/120    avg_loss:0.015, val_acc:0.982]
Epoch [119/120    avg_loss:0.010, val_acc:0.980]
Epoch [120/120    avg_loss:0.010, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1245    2    7    0    1    0    0    0    4   25    1    0
     0    0    0]
 [   0    0    1  727    2    0    0    0    0    2    0    0   15    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    7    0    0    6    0    0    0    0  836   26    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0   19 2175    8    1
     4    0    0]
 [   0    0    0    0    0    6    0    0    0    0    2    3  518    0
     0    2    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    3    1    0    0
  1129    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    54  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.57181571815718

F1 scores:
[       nan 1.         0.97992916 0.98509485 0.97931034 0.97742664
 0.99465241 1.         0.99883856 0.87179487 0.96147211 0.97972973
 0.96193129 0.99459459 0.96826758 0.91277259 0.98245614]

Kappa:
0.9723022232773757
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff4055bdb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.774, val_acc:0.321]
Epoch [2/120    avg_loss:2.585, val_acc:0.327]
Epoch [3/120    avg_loss:2.391, val_acc:0.426]
Epoch [4/120    avg_loss:2.220, val_acc:0.495]
Epoch [5/120    avg_loss:2.063, val_acc:0.572]
Epoch [6/120    avg_loss:1.927, val_acc:0.581]
Epoch [7/120    avg_loss:1.789, val_acc:0.546]
Epoch [8/120    avg_loss:1.622, val_acc:0.639]
Epoch [9/120    avg_loss:1.519, val_acc:0.613]
Epoch [10/120    avg_loss:1.391, val_acc:0.658]
Epoch [11/120    avg_loss:1.232, val_acc:0.679]
Epoch [12/120    avg_loss:1.110, val_acc:0.705]
Epoch [13/120    avg_loss:1.067, val_acc:0.742]
Epoch [14/120    avg_loss:0.896, val_acc:0.759]
Epoch [15/120    avg_loss:0.803, val_acc:0.776]
Epoch [16/120    avg_loss:0.736, val_acc:0.822]
Epoch [17/120    avg_loss:0.669, val_acc:0.827]
Epoch [18/120    avg_loss:0.622, val_acc:0.832]
Epoch [19/120    avg_loss:0.580, val_acc:0.835]
Epoch [20/120    avg_loss:0.474, val_acc:0.842]
Epoch [21/120    avg_loss:0.486, val_acc:0.860]
Epoch [22/120    avg_loss:0.410, val_acc:0.849]
Epoch [23/120    avg_loss:0.417, val_acc:0.853]
Epoch [24/120    avg_loss:0.456, val_acc:0.852]
Epoch [25/120    avg_loss:0.331, val_acc:0.879]
Epoch [26/120    avg_loss:0.443, val_acc:0.824]
Epoch [27/120    avg_loss:0.385, val_acc:0.849]
Epoch [28/120    avg_loss:0.318, val_acc:0.864]
Epoch [29/120    avg_loss:0.323, val_acc:0.833]
Epoch [30/120    avg_loss:0.264, val_acc:0.897]
Epoch [31/120    avg_loss:0.220, val_acc:0.913]
Epoch [32/120    avg_loss:0.204, val_acc:0.915]
Epoch [33/120    avg_loss:0.193, val_acc:0.903]
Epoch [34/120    avg_loss:0.202, val_acc:0.884]
Epoch [35/120    avg_loss:0.218, val_acc:0.852]
Epoch [36/120    avg_loss:0.192, val_acc:0.901]
Epoch [37/120    avg_loss:0.191, val_acc:0.872]
Epoch [38/120    avg_loss:0.342, val_acc:0.857]
Epoch [39/120    avg_loss:0.220, val_acc:0.913]
Epoch [40/120    avg_loss:0.142, val_acc:0.921]
Epoch [41/120    avg_loss:0.142, val_acc:0.927]
Epoch [42/120    avg_loss:0.123, val_acc:0.921]
Epoch [43/120    avg_loss:0.121, val_acc:0.903]
Epoch [44/120    avg_loss:0.131, val_acc:0.929]
Epoch [45/120    avg_loss:0.133, val_acc:0.923]
Epoch [46/120    avg_loss:0.105, val_acc:0.922]
Epoch [47/120    avg_loss:0.110, val_acc:0.928]
Epoch [48/120    avg_loss:0.081, val_acc:0.946]
Epoch [49/120    avg_loss:0.094, val_acc:0.905]
Epoch [50/120    avg_loss:0.111, val_acc:0.933]
Epoch [51/120    avg_loss:0.073, val_acc:0.952]
Epoch [52/120    avg_loss:0.086, val_acc:0.945]
Epoch [53/120    avg_loss:0.070, val_acc:0.952]
Epoch [54/120    avg_loss:0.060, val_acc:0.953]
Epoch [55/120    avg_loss:0.074, val_acc:0.917]
Epoch [56/120    avg_loss:0.076, val_acc:0.911]
Epoch [57/120    avg_loss:0.083, val_acc:0.941]
Epoch [58/120    avg_loss:0.062, val_acc:0.946]
Epoch [59/120    avg_loss:0.059, val_acc:0.959]
Epoch [60/120    avg_loss:0.059, val_acc:0.949]
Epoch [61/120    avg_loss:0.041, val_acc:0.941]
Epoch [62/120    avg_loss:0.043, val_acc:0.949]
Epoch [63/120    avg_loss:0.044, val_acc:0.944]
Epoch [64/120    avg_loss:0.051, val_acc:0.955]
Epoch [65/120    avg_loss:0.042, val_acc:0.956]
Epoch [66/120    avg_loss:0.045, val_acc:0.968]
Epoch [67/120    avg_loss:0.056, val_acc:0.964]
Epoch [68/120    avg_loss:0.037, val_acc:0.955]
Epoch [69/120    avg_loss:0.038, val_acc:0.940]
Epoch [70/120    avg_loss:0.043, val_acc:0.952]
Epoch [71/120    avg_loss:0.032, val_acc:0.959]
Epoch [72/120    avg_loss:0.042, val_acc:0.961]
Epoch [73/120    avg_loss:0.040, val_acc:0.964]
Epoch [74/120    avg_loss:0.037, val_acc:0.948]
Epoch [75/120    avg_loss:0.053, val_acc:0.955]
Epoch [76/120    avg_loss:0.035, val_acc:0.958]
Epoch [77/120    avg_loss:0.026, val_acc:0.947]
Epoch [78/120    avg_loss:0.024, val_acc:0.974]
Epoch [79/120    avg_loss:0.033, val_acc:0.970]
Epoch [80/120    avg_loss:0.029, val_acc:0.954]
Epoch [81/120    avg_loss:0.030, val_acc:0.955]
Epoch [82/120    avg_loss:0.039, val_acc:0.966]
Epoch [83/120    avg_loss:0.028, val_acc:0.961]
Epoch [84/120    avg_loss:0.024, val_acc:0.964]
Epoch [85/120    avg_loss:0.023, val_acc:0.966]
Epoch [86/120    avg_loss:0.030, val_acc:0.973]
Epoch [87/120    avg_loss:0.019, val_acc:0.968]
Epoch [88/120    avg_loss:0.022, val_acc:0.966]
Epoch [89/120    avg_loss:0.015, val_acc:0.966]
Epoch [90/120    avg_loss:0.016, val_acc:0.967]
Epoch [91/120    avg_loss:0.020, val_acc:0.968]
Epoch [92/120    avg_loss:0.020, val_acc:0.969]
Epoch [93/120    avg_loss:0.013, val_acc:0.968]
Epoch [94/120    avg_loss:0.011, val_acc:0.971]
Epoch [95/120    avg_loss:0.013, val_acc:0.974]
Epoch [96/120    avg_loss:0.014, val_acc:0.970]
Epoch [97/120    avg_loss:0.009, val_acc:0.971]
Epoch [98/120    avg_loss:0.009, val_acc:0.971]
Epoch [99/120    avg_loss:0.011, val_acc:0.968]
Epoch [100/120    avg_loss:0.008, val_acc:0.967]
Epoch [101/120    avg_loss:0.009, val_acc:0.967]
Epoch [102/120    avg_loss:0.011, val_acc:0.967]
Epoch [103/120    avg_loss:0.010, val_acc:0.968]
Epoch [104/120    avg_loss:0.013, val_acc:0.969]
Epoch [105/120    avg_loss:0.015, val_acc:0.970]
Epoch [106/120    avg_loss:0.017, val_acc:0.968]
Epoch [107/120    avg_loss:0.010, val_acc:0.970]
Epoch [108/120    avg_loss:0.008, val_acc:0.970]
Epoch [109/120    avg_loss:0.010, val_acc:0.970]
Epoch [110/120    avg_loss:0.009, val_acc:0.970]
Epoch [111/120    avg_loss:0.010, val_acc:0.970]
Epoch [112/120    avg_loss:0.010, val_acc:0.970]
Epoch [113/120    avg_loss:0.010, val_acc:0.970]
Epoch [114/120    avg_loss:0.010, val_acc:0.970]
Epoch [115/120    avg_loss:0.011, val_acc:0.970]
Epoch [116/120    avg_loss:0.014, val_acc:0.970]
Epoch [117/120    avg_loss:0.011, val_acc:0.970]
Epoch [118/120    avg_loss:0.011, val_acc:0.971]
Epoch [119/120    avg_loss:0.008, val_acc:0.971]
Epoch [120/120    avg_loss:0.009, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1250    4    6    0    1    0    0    1    3   20    0    0
     0    0    0]
 [   0    0    0  727    2    3    0    0    0    8    0    2    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    4    0    2    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    6    0    0    0    0  842   18    1    0
     0    0    0]
 [   0    0    8    2    0    0    0    0    0    0   11 2180    8    0
     1    0    0]
 [   0    0    3    3    0    8    0    0    0    0    0    6  511    0
     2    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    1    0    0    0
  1127    6    0]
 [   0    0    0    0    0    0   41    0    0    0    0    0    0    0
    23  283    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
97.39837398373983

F1 scores:
[       nan 0.975      0.97847358 0.98044504 0.98156682 0.96254257
 0.96674058 0.92592593 0.997669   0.76595745 0.97172533 0.98264593
 0.9587242  1.         0.98042627 0.88993711 0.96341463]

Kappa:
0.9703275492783148
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff41412ea58>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.735, val_acc:0.289]
Epoch [2/120    avg_loss:2.539, val_acc:0.387]
Epoch [3/120    avg_loss:2.363, val_acc:0.468]
Epoch [4/120    avg_loss:2.223, val_acc:0.488]
Epoch [5/120    avg_loss:2.062, val_acc:0.534]
Epoch [6/120    avg_loss:1.928, val_acc:0.558]
Epoch [7/120    avg_loss:1.822, val_acc:0.586]
Epoch [8/120    avg_loss:1.709, val_acc:0.613]
Epoch [9/120    avg_loss:1.532, val_acc:0.654]
Epoch [10/120    avg_loss:1.421, val_acc:0.663]
Epoch [11/120    avg_loss:1.288, val_acc:0.697]
Epoch [12/120    avg_loss:1.229, val_acc:0.663]
Epoch [13/120    avg_loss:1.047, val_acc:0.716]
Epoch [14/120    avg_loss:0.963, val_acc:0.726]
Epoch [15/120    avg_loss:0.924, val_acc:0.717]
Epoch [16/120    avg_loss:0.848, val_acc:0.720]
Epoch [17/120    avg_loss:0.744, val_acc:0.769]
Epoch [18/120    avg_loss:0.683, val_acc:0.786]
Epoch [19/120    avg_loss:0.616, val_acc:0.814]
Epoch [20/120    avg_loss:0.649, val_acc:0.824]
Epoch [21/120    avg_loss:0.566, val_acc:0.802]
Epoch [22/120    avg_loss:0.462, val_acc:0.825]
Epoch [23/120    avg_loss:0.452, val_acc:0.864]
Epoch [24/120    avg_loss:0.407, val_acc:0.855]
Epoch [25/120    avg_loss:0.414, val_acc:0.863]
Epoch [26/120    avg_loss:0.413, val_acc:0.864]
Epoch [27/120    avg_loss:0.384, val_acc:0.852]
Epoch [28/120    avg_loss:0.365, val_acc:0.894]
Epoch [29/120    avg_loss:0.334, val_acc:0.881]
Epoch [30/120    avg_loss:0.285, val_acc:0.880]
Epoch [31/120    avg_loss:0.302, val_acc:0.885]
Epoch [32/120    avg_loss:0.270, val_acc:0.892]
Epoch [33/120    avg_loss:0.285, val_acc:0.881]
Epoch [34/120    avg_loss:0.266, val_acc:0.902]
Epoch [35/120    avg_loss:0.237, val_acc:0.929]
Epoch [36/120    avg_loss:0.206, val_acc:0.903]
Epoch [37/120    avg_loss:0.215, val_acc:0.914]
Epoch [38/120    avg_loss:0.174, val_acc:0.908]
Epoch [39/120    avg_loss:0.195, val_acc:0.909]
Epoch [40/120    avg_loss:0.232, val_acc:0.909]
Epoch [41/120    avg_loss:0.183, val_acc:0.856]
Epoch [42/120    avg_loss:0.200, val_acc:0.920]
Epoch [43/120    avg_loss:0.139, val_acc:0.938]
Epoch [44/120    avg_loss:0.155, val_acc:0.933]
Epoch [45/120    avg_loss:0.144, val_acc:0.930]
Epoch [46/120    avg_loss:0.144, val_acc:0.929]
Epoch [47/120    avg_loss:0.157, val_acc:0.933]
Epoch [48/120    avg_loss:0.147, val_acc:0.907]
Epoch [49/120    avg_loss:0.187, val_acc:0.922]
Epoch [50/120    avg_loss:0.158, val_acc:0.932]
Epoch [51/120    avg_loss:0.155, val_acc:0.944]
Epoch [52/120    avg_loss:0.090, val_acc:0.947]
Epoch [53/120    avg_loss:0.083, val_acc:0.953]
Epoch [54/120    avg_loss:0.106, val_acc:0.940]
Epoch [55/120    avg_loss:0.117, val_acc:0.930]
Epoch [56/120    avg_loss:0.096, val_acc:0.947]
Epoch [57/120    avg_loss:0.097, val_acc:0.949]
Epoch [58/120    avg_loss:0.081, val_acc:0.953]
Epoch [59/120    avg_loss:0.078, val_acc:0.944]
Epoch [60/120    avg_loss:0.081, val_acc:0.955]
Epoch [61/120    avg_loss:0.087, val_acc:0.947]
Epoch [62/120    avg_loss:0.108, val_acc:0.933]
Epoch [63/120    avg_loss:0.069, val_acc:0.954]
Epoch [64/120    avg_loss:0.067, val_acc:0.943]
Epoch [65/120    avg_loss:0.077, val_acc:0.947]
Epoch [66/120    avg_loss:0.082, val_acc:0.958]
Epoch [67/120    avg_loss:0.061, val_acc:0.961]
Epoch [68/120    avg_loss:0.061, val_acc:0.963]
Epoch [69/120    avg_loss:0.048, val_acc:0.960]
Epoch [70/120    avg_loss:0.045, val_acc:0.952]
Epoch [71/120    avg_loss:0.083, val_acc:0.941]
Epoch [72/120    avg_loss:0.080, val_acc:0.954]
Epoch [73/120    avg_loss:0.044, val_acc:0.955]
Epoch [74/120    avg_loss:0.056, val_acc:0.948]
Epoch [75/120    avg_loss:0.083, val_acc:0.908]
Epoch [76/120    avg_loss:0.082, val_acc:0.952]
Epoch [77/120    avg_loss:0.072, val_acc:0.956]
Epoch [78/120    avg_loss:0.052, val_acc:0.927]
Epoch [79/120    avg_loss:0.078, val_acc:0.950]
Epoch [80/120    avg_loss:0.056, val_acc:0.952]
Epoch [81/120    avg_loss:0.046, val_acc:0.957]
Epoch [82/120    avg_loss:0.040, val_acc:0.964]
Epoch [83/120    avg_loss:0.039, val_acc:0.963]
Epoch [84/120    avg_loss:0.034, val_acc:0.967]
Epoch [85/120    avg_loss:0.037, val_acc:0.971]
Epoch [86/120    avg_loss:0.027, val_acc:0.968]
Epoch [87/120    avg_loss:0.027, val_acc:0.967]
Epoch [88/120    avg_loss:0.031, val_acc:0.966]
Epoch [89/120    avg_loss:0.028, val_acc:0.969]
Epoch [90/120    avg_loss:0.025, val_acc:0.968]
Epoch [91/120    avg_loss:0.026, val_acc:0.971]
Epoch [92/120    avg_loss:0.030, val_acc:0.971]
Epoch [93/120    avg_loss:0.026, val_acc:0.970]
Epoch [94/120    avg_loss:0.029, val_acc:0.970]
Epoch [95/120    avg_loss:0.025, val_acc:0.970]
Epoch [96/120    avg_loss:0.026, val_acc:0.971]
Epoch [97/120    avg_loss:0.021, val_acc:0.971]
Epoch [98/120    avg_loss:0.028, val_acc:0.972]
Epoch [99/120    avg_loss:0.023, val_acc:0.971]
Epoch [100/120    avg_loss:0.023, val_acc:0.971]
Epoch [101/120    avg_loss:0.022, val_acc:0.971]
Epoch [102/120    avg_loss:0.019, val_acc:0.970]
Epoch [103/120    avg_loss:0.024, val_acc:0.973]
Epoch [104/120    avg_loss:0.022, val_acc:0.973]
Epoch [105/120    avg_loss:0.028, val_acc:0.970]
Epoch [106/120    avg_loss:0.029, val_acc:0.968]
Epoch [107/120    avg_loss:0.026, val_acc:0.969]
Epoch [108/120    avg_loss:0.024, val_acc:0.970]
Epoch [109/120    avg_loss:0.023, val_acc:0.972]
Epoch [110/120    avg_loss:0.020, val_acc:0.969]
Epoch [111/120    avg_loss:0.019, val_acc:0.967]
Epoch [112/120    avg_loss:0.029, val_acc:0.969]
Epoch [113/120    avg_loss:0.023, val_acc:0.970]
Epoch [114/120    avg_loss:0.023, val_acc:0.972]
Epoch [115/120    avg_loss:0.025, val_acc:0.972]
Epoch [116/120    avg_loss:0.024, val_acc:0.970]
Epoch [117/120    avg_loss:0.019, val_acc:0.971]
Epoch [118/120    avg_loss:0.019, val_acc:0.971]
Epoch [119/120    avg_loss:0.023, val_acc:0.972]
Epoch [120/120    avg_loss:0.026, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1249    1    0    0    6    0    0    0    3   23    1    0
     0    2    0]
 [   0    0    2  718    1   11    0    0    0    9    0    0    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    4    0    1    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    3    0    0   11    0    0    1    0
     0    0    0]
 [   0    0   28   73    0    6    0    0    0    0  753    4    0    0
     0   11    0]
 [   0    0    8    1    0    0    7    0    0    0   20 2170    2    2
     0    0    0]
 [   0    0    0   25   10   13    0    0    0    0    7    0  474    0
     0    2    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   15    0    0    0    0    2    0    0    0
  1121    1    0]
 [   0    0    0    1    0    0    0    0    0    0    0    0    0    0
    50  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.84823848238483

F1 scores:
[       nan 0.94871795 0.97122862 0.91523263 0.97482838 0.93687708
 0.98644578 0.92592593 1.         0.56410256 0.90504808 0.98479691
 0.92941176 0.99462366 0.96679603 0.8983308  0.9704142 ]

Kappa:
0.9526725674921328
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc6ff514b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.766, val_acc:0.340]
Epoch [2/120    avg_loss:2.558, val_acc:0.470]
Epoch [3/120    avg_loss:2.359, val_acc:0.542]
Epoch [4/120    avg_loss:2.174, val_acc:0.579]
Epoch [5/120    avg_loss:2.082, val_acc:0.564]
Epoch [6/120    avg_loss:1.930, val_acc:0.596]
Epoch [7/120    avg_loss:1.809, val_acc:0.593]
Epoch [8/120    avg_loss:1.742, val_acc:0.609]
Epoch [9/120    avg_loss:1.599, val_acc:0.626]
Epoch [10/120    avg_loss:1.483, val_acc:0.618]
Epoch [11/120    avg_loss:1.380, val_acc:0.654]
Epoch [12/120    avg_loss:1.326, val_acc:0.678]
Epoch [13/120    avg_loss:1.207, val_acc:0.667]
Epoch [14/120    avg_loss:1.096, val_acc:0.743]
Epoch [15/120    avg_loss:1.014, val_acc:0.671]
Epoch [16/120    avg_loss:0.933, val_acc:0.771]
Epoch [17/120    avg_loss:0.824, val_acc:0.775]
Epoch [18/120    avg_loss:0.713, val_acc:0.741]
Epoch [19/120    avg_loss:0.807, val_acc:0.760]
Epoch [20/120    avg_loss:0.758, val_acc:0.772]
Epoch [21/120    avg_loss:0.659, val_acc:0.811]
Epoch [22/120    avg_loss:0.571, val_acc:0.785]
Epoch [23/120    avg_loss:0.594, val_acc:0.762]
Epoch [24/120    avg_loss:0.710, val_acc:0.787]
Epoch [25/120    avg_loss:0.487, val_acc:0.812]
Epoch [26/120    avg_loss:0.458, val_acc:0.825]
Epoch [27/120    avg_loss:0.418, val_acc:0.856]
Epoch [28/120    avg_loss:0.339, val_acc:0.862]
Epoch [29/120    avg_loss:0.331, val_acc:0.873]
Epoch [30/120    avg_loss:0.290, val_acc:0.883]
Epoch [31/120    avg_loss:0.300, val_acc:0.887]
Epoch [32/120    avg_loss:0.369, val_acc:0.877]
Epoch [33/120    avg_loss:0.258, val_acc:0.898]
Epoch [34/120    avg_loss:0.227, val_acc:0.883]
Epoch [35/120    avg_loss:0.239, val_acc:0.891]
Epoch [36/120    avg_loss:0.243, val_acc:0.888]
Epoch [37/120    avg_loss:0.230, val_acc:0.896]
Epoch [38/120    avg_loss:0.215, val_acc:0.912]
Epoch [39/120    avg_loss:0.220, val_acc:0.914]
Epoch [40/120    avg_loss:0.201, val_acc:0.905]
Epoch [41/120    avg_loss:0.170, val_acc:0.897]
Epoch [42/120    avg_loss:0.239, val_acc:0.931]
Epoch [43/120    avg_loss:0.161, val_acc:0.923]
Epoch [44/120    avg_loss:0.145, val_acc:0.917]
Epoch [45/120    avg_loss:0.120, val_acc:0.901]
Epoch [46/120    avg_loss:0.175, val_acc:0.880]
Epoch [47/120    avg_loss:0.117, val_acc:0.940]
Epoch [48/120    avg_loss:0.115, val_acc:0.916]
Epoch [49/120    avg_loss:0.097, val_acc:0.925]
Epoch [50/120    avg_loss:0.108, val_acc:0.928]
Epoch [51/120    avg_loss:0.108, val_acc:0.942]
Epoch [52/120    avg_loss:0.072, val_acc:0.951]
Epoch [53/120    avg_loss:0.082, val_acc:0.943]
Epoch [54/120    avg_loss:0.079, val_acc:0.947]
Epoch [55/120    avg_loss:0.071, val_acc:0.943]
Epoch [56/120    avg_loss:0.070, val_acc:0.953]
Epoch [57/120    avg_loss:0.065, val_acc:0.930]
Epoch [58/120    avg_loss:0.060, val_acc:0.952]
Epoch [59/120    avg_loss:0.051, val_acc:0.954]
Epoch [60/120    avg_loss:0.054, val_acc:0.955]
Epoch [61/120    avg_loss:0.058, val_acc:0.934]
Epoch [62/120    avg_loss:0.059, val_acc:0.959]
Epoch [63/120    avg_loss:0.059, val_acc:0.959]
Epoch [64/120    avg_loss:0.048, val_acc:0.950]
Epoch [65/120    avg_loss:0.076, val_acc:0.928]
Epoch [66/120    avg_loss:0.050, val_acc:0.952]
Epoch [67/120    avg_loss:0.059, val_acc:0.956]
Epoch [68/120    avg_loss:0.056, val_acc:0.961]
Epoch [69/120    avg_loss:0.047, val_acc:0.961]
Epoch [70/120    avg_loss:0.067, val_acc:0.955]
Epoch [71/120    avg_loss:0.047, val_acc:0.947]
Epoch [72/120    avg_loss:0.041, val_acc:0.956]
Epoch [73/120    avg_loss:0.043, val_acc:0.959]
Epoch [74/120    avg_loss:0.038, val_acc:0.942]
Epoch [75/120    avg_loss:0.067, val_acc:0.945]
Epoch [76/120    avg_loss:0.076, val_acc:0.941]
Epoch [77/120    avg_loss:0.069, val_acc:0.957]
Epoch [78/120    avg_loss:0.064, val_acc:0.952]
Epoch [79/120    avg_loss:0.048, val_acc:0.964]
Epoch [80/120    avg_loss:0.053, val_acc:0.940]
Epoch [81/120    avg_loss:0.082, val_acc:0.945]
Epoch [82/120    avg_loss:0.056, val_acc:0.961]
Epoch [83/120    avg_loss:0.056, val_acc:0.919]
Epoch [84/120    avg_loss:0.058, val_acc:0.963]
Epoch [85/120    avg_loss:0.060, val_acc:0.952]
Epoch [86/120    avg_loss:0.041, val_acc:0.962]
Epoch [87/120    avg_loss:0.066, val_acc:0.962]
Epoch [88/120    avg_loss:0.041, val_acc:0.962]
Epoch [89/120    avg_loss:0.038, val_acc:0.939]
Epoch [90/120    avg_loss:0.038, val_acc:0.962]
Epoch [91/120    avg_loss:0.071, val_acc:0.945]
Epoch [92/120    avg_loss:0.075, val_acc:0.942]
Epoch [93/120    avg_loss:0.051, val_acc:0.956]
Epoch [94/120    avg_loss:0.044, val_acc:0.965]
Epoch [95/120    avg_loss:0.029, val_acc:0.970]
Epoch [96/120    avg_loss:0.034, val_acc:0.969]
Epoch [97/120    avg_loss:0.033, val_acc:0.973]
Epoch [98/120    avg_loss:0.025, val_acc:0.971]
Epoch [99/120    avg_loss:0.025, val_acc:0.971]
Epoch [100/120    avg_loss:0.020, val_acc:0.974]
Epoch [101/120    avg_loss:0.019, val_acc:0.975]
Epoch [102/120    avg_loss:0.020, val_acc:0.975]
Epoch [103/120    avg_loss:0.026, val_acc:0.973]
Epoch [104/120    avg_loss:0.016, val_acc:0.974]
Epoch [105/120    avg_loss:0.023, val_acc:0.974]
Epoch [106/120    avg_loss:0.020, val_acc:0.974]
Epoch [107/120    avg_loss:0.017, val_acc:0.974]
Epoch [108/120    avg_loss:0.018, val_acc:0.973]
Epoch [109/120    avg_loss:0.022, val_acc:0.974]
Epoch [110/120    avg_loss:0.021, val_acc:0.974]
Epoch [111/120    avg_loss:0.021, val_acc:0.974]
Epoch [112/120    avg_loss:0.018, val_acc:0.974]
Epoch [113/120    avg_loss:0.016, val_acc:0.975]
Epoch [114/120    avg_loss:0.022, val_acc:0.973]
Epoch [115/120    avg_loss:0.017, val_acc:0.973]
Epoch [116/120    avg_loss:0.016, val_acc:0.974]
Epoch [117/120    avg_loss:0.015, val_acc:0.974]
Epoch [118/120    avg_loss:0.018, val_acc:0.973]
Epoch [119/120    avg_loss:0.017, val_acc:0.973]
Epoch [120/120    avg_loss:0.018, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1253    0    0    0    1    0    0    2    2   19    8    0
     0    0    0]
 [   0    0    6  728    1    0    0    0    0    3    4    1    3    1
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  429    0    2    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10   14    0    0    1    0    0    0  846    1    1    0
     1    1    0]
 [   0    0   13    0    0    0    0    0    2    0   25 2168    0    1
     0    0    1]
 [   0    0    0   16    0    0    0    0    2    0    9   16  490    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1132    4    0]
 [   0    0    1    0    0    0   25    0    0    2    0    0    0    0
    56  263    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.11653116531166

F1 scores:
[       nan 1.         0.9758567  0.96744186 0.99530516 0.98961938
 0.97986577 0.96153846 0.99537037 0.81818182 0.96081772 0.98210646
 0.94321463 0.99462366 0.97125697 0.85528455 0.97619048]

Kappa:
0.9671080313076691
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f51e4a0fa20>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.742, val_acc:0.287]
Epoch [2/120    avg_loss:2.504, val_acc:0.467]
Epoch [3/120    avg_loss:2.331, val_acc:0.462]
Epoch [4/120    avg_loss:2.134, val_acc:0.530]
Epoch [5/120    avg_loss:1.991, val_acc:0.565]
Epoch [6/120    avg_loss:1.905, val_acc:0.566]
Epoch [7/120    avg_loss:1.813, val_acc:0.620]
Epoch [8/120    avg_loss:1.676, val_acc:0.643]
Epoch [9/120    avg_loss:1.547, val_acc:0.650]
Epoch [10/120    avg_loss:1.463, val_acc:0.696]
Epoch [11/120    avg_loss:1.233, val_acc:0.711]
Epoch [12/120    avg_loss:1.114, val_acc:0.728]
Epoch [13/120    avg_loss:1.039, val_acc:0.727]
Epoch [14/120    avg_loss:0.896, val_acc:0.747]
Epoch [15/120    avg_loss:0.892, val_acc:0.762]
Epoch [16/120    avg_loss:0.869, val_acc:0.754]
Epoch [17/120    avg_loss:0.785, val_acc:0.773]
Epoch [18/120    avg_loss:0.754, val_acc:0.798]
Epoch [19/120    avg_loss:0.687, val_acc:0.767]
Epoch [20/120    avg_loss:0.638, val_acc:0.796]
Epoch [21/120    avg_loss:0.547, val_acc:0.830]
Epoch [22/120    avg_loss:0.555, val_acc:0.828]
Epoch [23/120    avg_loss:0.486, val_acc:0.821]
Epoch [24/120    avg_loss:0.580, val_acc:0.792]
Epoch [25/120    avg_loss:0.602, val_acc:0.811]
Epoch [26/120    avg_loss:0.434, val_acc:0.814]
Epoch [27/120    avg_loss:0.415, val_acc:0.856]
Epoch [28/120    avg_loss:0.343, val_acc:0.839]
Epoch [29/120    avg_loss:0.348, val_acc:0.873]
Epoch [30/120    avg_loss:0.293, val_acc:0.892]
Epoch [31/120    avg_loss:0.258, val_acc:0.889]
Epoch [32/120    avg_loss:0.266, val_acc:0.857]
Epoch [33/120    avg_loss:0.296, val_acc:0.893]
Epoch [34/120    avg_loss:0.214, val_acc:0.883]
Epoch [35/120    avg_loss:0.209, val_acc:0.912]
Epoch [36/120    avg_loss:0.199, val_acc:0.907]
Epoch [37/120    avg_loss:0.186, val_acc:0.920]
Epoch [38/120    avg_loss:0.215, val_acc:0.908]
Epoch [39/120    avg_loss:0.174, val_acc:0.918]
Epoch [40/120    avg_loss:0.160, val_acc:0.927]
Epoch [41/120    avg_loss:0.186, val_acc:0.920]
Epoch [42/120    avg_loss:0.150, val_acc:0.894]
Epoch [43/120    avg_loss:0.157, val_acc:0.926]
Epoch [44/120    avg_loss:0.241, val_acc:0.903]
Epoch [45/120    avg_loss:0.159, val_acc:0.921]
Epoch [46/120    avg_loss:0.118, val_acc:0.932]
Epoch [47/120    avg_loss:0.116, val_acc:0.939]
Epoch [48/120    avg_loss:0.130, val_acc:0.909]
Epoch [49/120    avg_loss:0.134, val_acc:0.936]
Epoch [50/120    avg_loss:0.114, val_acc:0.917]
Epoch [51/120    avg_loss:0.259, val_acc:0.908]
Epoch [52/120    avg_loss:0.176, val_acc:0.904]
Epoch [53/120    avg_loss:0.133, val_acc:0.942]
Epoch [54/120    avg_loss:0.136, val_acc:0.912]
Epoch [55/120    avg_loss:0.159, val_acc:0.922]
Epoch [56/120    avg_loss:0.098, val_acc:0.921]
Epoch [57/120    avg_loss:0.105, val_acc:0.939]
Epoch [58/120    avg_loss:0.075, val_acc:0.948]
Epoch [59/120    avg_loss:0.077, val_acc:0.950]
Epoch [60/120    avg_loss:0.092, val_acc:0.942]
Epoch [61/120    avg_loss:0.090, val_acc:0.939]
Epoch [62/120    avg_loss:0.081, val_acc:0.945]
Epoch [63/120    avg_loss:0.062, val_acc:0.953]
Epoch [64/120    avg_loss:0.062, val_acc:0.946]
Epoch [65/120    avg_loss:0.058, val_acc:0.949]
Epoch [66/120    avg_loss:0.052, val_acc:0.958]
Epoch [67/120    avg_loss:0.043, val_acc:0.941]
Epoch [68/120    avg_loss:0.063, val_acc:0.946]
Epoch [69/120    avg_loss:0.078, val_acc:0.954]
Epoch [70/120    avg_loss:0.079, val_acc:0.956]
Epoch [71/120    avg_loss:0.065, val_acc:0.942]
Epoch [72/120    avg_loss:0.057, val_acc:0.953]
Epoch [73/120    avg_loss:0.050, val_acc:0.949]
Epoch [74/120    avg_loss:0.051, val_acc:0.959]
Epoch [75/120    avg_loss:0.042, val_acc:0.960]
Epoch [76/120    avg_loss:0.096, val_acc:0.946]
Epoch [77/120    avg_loss:0.061, val_acc:0.958]
Epoch [78/120    avg_loss:0.060, val_acc:0.962]
Epoch [79/120    avg_loss:0.041, val_acc:0.960]
Epoch [80/120    avg_loss:0.049, val_acc:0.957]
Epoch [81/120    avg_loss:0.036, val_acc:0.958]
Epoch [82/120    avg_loss:0.036, val_acc:0.954]
Epoch [83/120    avg_loss:0.046, val_acc:0.938]
Epoch [84/120    avg_loss:0.047, val_acc:0.955]
Epoch [85/120    avg_loss:0.026, val_acc:0.956]
Epoch [86/120    avg_loss:0.044, val_acc:0.949]
Epoch [87/120    avg_loss:0.055, val_acc:0.952]
Epoch [88/120    avg_loss:0.043, val_acc:0.956]
Epoch [89/120    avg_loss:0.046, val_acc:0.958]
Epoch [90/120    avg_loss:0.043, val_acc:0.947]
Epoch [91/120    avg_loss:0.033, val_acc:0.949]
Epoch [92/120    avg_loss:0.033, val_acc:0.959]
Epoch [93/120    avg_loss:0.021, val_acc:0.959]
Epoch [94/120    avg_loss:0.023, val_acc:0.960]
Epoch [95/120    avg_loss:0.019, val_acc:0.961]
Epoch [96/120    avg_loss:0.022, val_acc:0.962]
Epoch [97/120    avg_loss:0.022, val_acc:0.963]
Epoch [98/120    avg_loss:0.018, val_acc:0.966]
Epoch [99/120    avg_loss:0.022, val_acc:0.967]
Epoch [100/120    avg_loss:0.023, val_acc:0.966]
Epoch [101/120    avg_loss:0.020, val_acc:0.966]
Epoch [102/120    avg_loss:0.019, val_acc:0.963]
Epoch [103/120    avg_loss:0.017, val_acc:0.964]
Epoch [104/120    avg_loss:0.020, val_acc:0.964]
Epoch [105/120    avg_loss:0.018, val_acc:0.966]
Epoch [106/120    avg_loss:0.018, val_acc:0.963]
Epoch [107/120    avg_loss:0.018, val_acc:0.964]
Epoch [108/120    avg_loss:0.018, val_acc:0.967]
Epoch [109/120    avg_loss:0.019, val_acc:0.967]
Epoch [110/120    avg_loss:0.018, val_acc:0.968]
Epoch [111/120    avg_loss:0.013, val_acc:0.968]
Epoch [112/120    avg_loss:0.020, val_acc:0.967]
Epoch [113/120    avg_loss:0.014, val_acc:0.966]
Epoch [114/120    avg_loss:0.015, val_acc:0.967]
Epoch [115/120    avg_loss:0.016, val_acc:0.968]
Epoch [116/120    avg_loss:0.017, val_acc:0.966]
Epoch [117/120    avg_loss:0.015, val_acc:0.966]
Epoch [118/120    avg_loss:0.018, val_acc:0.964]
Epoch [119/120    avg_loss:0.019, val_acc:0.966]
Epoch [120/120    avg_loss:0.016, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1261    4    2    2    1    0    0    0    2   13    0    0
     0    0    0]
 [   0    0    1  719    0    5    0    0    0   14    0    0    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   15   35    0   10    2    0    0    0  808    4    0    0
     0    1    0]
 [   0    0   12    0    0    0    4    0    2    0   10 2181    1    0
     0    0    0]
 [   0    0    3    7    0   15    0    0    0    0    2   24  480    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    3    0    0    1    0    0
  1130    0    0]
 [   0    0    0    0    0    0   38    0    0    2    0    0    0    0
    66  241    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
96.39024390243902

F1 scores:
[       nan 0.975      0.97865735 0.9510582  0.9953271  0.95227525
 0.96383764 1.         0.99421965 0.59649123 0.95114773 0.98354002
 0.93476144 1.         0.96705178 0.81694915 0.97005988]

Kappa:
0.9588105206988153
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1622670a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.726, val_acc:0.307]
Epoch [2/120    avg_loss:2.533, val_acc:0.422]
Epoch [3/120    avg_loss:2.377, val_acc:0.536]
Epoch [4/120    avg_loss:2.240, val_acc:0.554]
Epoch [5/120    avg_loss:2.118, val_acc:0.590]
Epoch [6/120    avg_loss:2.002, val_acc:0.601]
Epoch [7/120    avg_loss:1.922, val_acc:0.645]
Epoch [8/120    avg_loss:1.803, val_acc:0.625]
Epoch [9/120    avg_loss:1.674, val_acc:0.659]
Epoch [10/120    avg_loss:1.523, val_acc:0.678]
Epoch [11/120    avg_loss:1.391, val_acc:0.727]
Epoch [12/120    avg_loss:1.308, val_acc:0.739]
Epoch [13/120    avg_loss:1.144, val_acc:0.739]
Epoch [14/120    avg_loss:1.063, val_acc:0.746]
Epoch [15/120    avg_loss:0.874, val_acc:0.809]
Epoch [16/120    avg_loss:0.814, val_acc:0.712]
Epoch [17/120    avg_loss:0.866, val_acc:0.788]
Epoch [18/120    avg_loss:0.693, val_acc:0.776]
Epoch [19/120    avg_loss:0.634, val_acc:0.843]
Epoch [20/120    avg_loss:0.593, val_acc:0.828]
Epoch [21/120    avg_loss:0.545, val_acc:0.820]
Epoch [22/120    avg_loss:0.607, val_acc:0.820]
Epoch [23/120    avg_loss:0.488, val_acc:0.871]
Epoch [24/120    avg_loss:0.390, val_acc:0.879]
Epoch [25/120    avg_loss:0.362, val_acc:0.895]
Epoch [26/120    avg_loss:0.356, val_acc:0.841]
Epoch [27/120    avg_loss:0.384, val_acc:0.843]
Epoch [28/120    avg_loss:0.365, val_acc:0.899]
Epoch [29/120    avg_loss:0.256, val_acc:0.914]
Epoch [30/120    avg_loss:0.348, val_acc:0.887]
Epoch [31/120    avg_loss:0.366, val_acc:0.875]
Epoch [32/120    avg_loss:0.244, val_acc:0.906]
Epoch [33/120    avg_loss:0.209, val_acc:0.928]
Epoch [34/120    avg_loss:0.234, val_acc:0.918]
Epoch [35/120    avg_loss:0.189, val_acc:0.932]
Epoch [36/120    avg_loss:0.246, val_acc:0.930]
Epoch [37/120    avg_loss:0.157, val_acc:0.868]
Epoch [38/120    avg_loss:0.185, val_acc:0.928]
Epoch [39/120    avg_loss:0.162, val_acc:0.953]
Epoch [40/120    avg_loss:0.195, val_acc:0.862]
Epoch [41/120    avg_loss:0.344, val_acc:0.897]
Epoch [42/120    avg_loss:0.254, val_acc:0.922]
Epoch [43/120    avg_loss:0.168, val_acc:0.918]
Epoch [44/120    avg_loss:0.150, val_acc:0.934]
Epoch [45/120    avg_loss:0.126, val_acc:0.925]
Epoch [46/120    avg_loss:0.131, val_acc:0.936]
Epoch [47/120    avg_loss:0.123, val_acc:0.951]
Epoch [48/120    avg_loss:0.091, val_acc:0.958]
Epoch [49/120    avg_loss:0.088, val_acc:0.946]
Epoch [50/120    avg_loss:0.087, val_acc:0.948]
Epoch [51/120    avg_loss:0.085, val_acc:0.943]
Epoch [52/120    avg_loss:0.082, val_acc:0.931]
Epoch [53/120    avg_loss:0.089, val_acc:0.965]
Epoch [54/120    avg_loss:0.096, val_acc:0.946]
Epoch [55/120    avg_loss:0.096, val_acc:0.961]
Epoch [56/120    avg_loss:0.075, val_acc:0.962]
Epoch [57/120    avg_loss:0.050, val_acc:0.964]
Epoch [58/120    avg_loss:0.052, val_acc:0.969]
Epoch [59/120    avg_loss:0.076, val_acc:0.943]
Epoch [60/120    avg_loss:0.066, val_acc:0.947]
Epoch [61/120    avg_loss:0.077, val_acc:0.952]
Epoch [62/120    avg_loss:0.201, val_acc:0.947]
Epoch [63/120    avg_loss:0.091, val_acc:0.963]
Epoch [64/120    avg_loss:0.114, val_acc:0.940]
Epoch [65/120    avg_loss:0.142, val_acc:0.944]
Epoch [66/120    avg_loss:0.075, val_acc:0.971]
Epoch [67/120    avg_loss:0.061, val_acc:0.969]
Epoch [68/120    avg_loss:0.051, val_acc:0.974]
Epoch [69/120    avg_loss:0.047, val_acc:0.970]
Epoch [70/120    avg_loss:0.052, val_acc:0.958]
Epoch [71/120    avg_loss:0.058, val_acc:0.944]
Epoch [72/120    avg_loss:0.071, val_acc:0.971]
Epoch [73/120    avg_loss:0.054, val_acc:0.973]
Epoch [74/120    avg_loss:0.058, val_acc:0.963]
Epoch [75/120    avg_loss:0.075, val_acc:0.959]
Epoch [76/120    avg_loss:0.053, val_acc:0.956]
Epoch [77/120    avg_loss:0.031, val_acc:0.978]
Epoch [78/120    avg_loss:0.030, val_acc:0.978]
Epoch [79/120    avg_loss:0.024, val_acc:0.979]
Epoch [80/120    avg_loss:0.030, val_acc:0.976]
Epoch [81/120    avg_loss:0.018, val_acc:0.982]
Epoch [82/120    avg_loss:0.020, val_acc:0.989]
Epoch [83/120    avg_loss:0.021, val_acc:0.971]
Epoch [84/120    avg_loss:0.062, val_acc:0.980]
Epoch [85/120    avg_loss:0.040, val_acc:0.970]
Epoch [86/120    avg_loss:0.037, val_acc:0.979]
Epoch [87/120    avg_loss:0.028, val_acc:0.973]
Epoch [88/120    avg_loss:0.031, val_acc:0.974]
Epoch [89/120    avg_loss:0.015, val_acc:0.987]
Epoch [90/120    avg_loss:0.016, val_acc:0.984]
Epoch [91/120    avg_loss:0.020, val_acc:0.980]
Epoch [92/120    avg_loss:0.018, val_acc:0.976]
Epoch [93/120    avg_loss:0.039, val_acc:0.975]
Epoch [94/120    avg_loss:0.037, val_acc:0.980]
Epoch [95/120    avg_loss:0.024, val_acc:0.984]
Epoch [96/120    avg_loss:0.020, val_acc:0.984]
Epoch [97/120    avg_loss:0.018, val_acc:0.984]
Epoch [98/120    avg_loss:0.018, val_acc:0.985]
Epoch [99/120    avg_loss:0.011, val_acc:0.987]
Epoch [100/120    avg_loss:0.013, val_acc:0.987]
Epoch [101/120    avg_loss:0.013, val_acc:0.987]
Epoch [102/120    avg_loss:0.013, val_acc:0.987]
Epoch [103/120    avg_loss:0.013, val_acc:0.988]
Epoch [104/120    avg_loss:0.013, val_acc:0.988]
Epoch [105/120    avg_loss:0.016, val_acc:0.988]
Epoch [106/120    avg_loss:0.013, val_acc:0.988]
Epoch [107/120    avg_loss:0.012, val_acc:0.987]
Epoch [108/120    avg_loss:0.013, val_acc:0.986]
Epoch [109/120    avg_loss:0.010, val_acc:0.986]
Epoch [110/120    avg_loss:0.019, val_acc:0.987]
Epoch [111/120    avg_loss:0.016, val_acc:0.987]
Epoch [112/120    avg_loss:0.011, val_acc:0.987]
Epoch [113/120    avg_loss:0.011, val_acc:0.987]
Epoch [114/120    avg_loss:0.011, val_acc:0.987]
Epoch [115/120    avg_loss:0.010, val_acc:0.986]
Epoch [116/120    avg_loss:0.011, val_acc:0.986]
Epoch [117/120    avg_loss:0.011, val_acc:0.986]
Epoch [118/120    avg_loss:0.011, val_acc:0.986]
Epoch [119/120    avg_loss:0.009, val_acc:0.986]
Epoch [120/120    avg_loss:0.012, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1260    0    0    0    1    0    0    0    4   19    1    0
     0    0    0]
 [   0    0    1  704   13    1    0    0    0    9    0    4   15    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    1    0    0    0    0    1    0    0
     7    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    6    0    0   12    0    0    0    0
     0    0    0]
 [   0    0   11   30    0    3    0    0    0    0  826    0    2    0
     2    1    0]
 [   0    0    7    0    0    0    1    0    0    0   23 2179    0    0
     0    0    0]
 [   0    0    0    7    0    1    0    0    0    0    7    2  514    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0    0
  1138    0    0]
 [   0    0    0    0    0    0    7    0    0    2    0    0    0    0
    49  289    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.33333333333333

F1 scores:
[       nan 0.98765432 0.98283931 0.94623656 0.97038724 0.98383372
 0.98568199 1.         1.         0.58536585 0.9516129  0.98641919
 0.96435272 1.         0.97431507 0.90595611 0.98823529]

Kappa:
0.9695867867745683
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff791e28ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.789, val_acc:0.128]
Epoch [2/120    avg_loss:2.573, val_acc:0.384]
Epoch [3/120    avg_loss:2.411, val_acc:0.412]
Epoch [4/120    avg_loss:2.241, val_acc:0.442]
Epoch [5/120    avg_loss:2.110, val_acc:0.519]
Epoch [6/120    avg_loss:1.957, val_acc:0.576]
Epoch [7/120    avg_loss:1.889, val_acc:0.581]
Epoch [8/120    avg_loss:1.813, val_acc:0.568]
Epoch [9/120    avg_loss:1.721, val_acc:0.587]
Epoch [10/120    avg_loss:1.606, val_acc:0.606]
Epoch [11/120    avg_loss:1.476, val_acc:0.640]
Epoch [12/120    avg_loss:1.426, val_acc:0.638]
Epoch [13/120    avg_loss:1.385, val_acc:0.699]
Epoch [14/120    avg_loss:1.178, val_acc:0.717]
Epoch [15/120    avg_loss:1.033, val_acc:0.733]
Epoch [16/120    avg_loss:0.995, val_acc:0.628]
Epoch [17/120    avg_loss:0.993, val_acc:0.747]
Epoch [18/120    avg_loss:0.841, val_acc:0.729]
Epoch [19/120    avg_loss:0.779, val_acc:0.752]
Epoch [20/120    avg_loss:0.687, val_acc:0.806]
Epoch [21/120    avg_loss:0.588, val_acc:0.805]
Epoch [22/120    avg_loss:0.574, val_acc:0.795]
Epoch [23/120    avg_loss:0.582, val_acc:0.849]
Epoch [24/120    avg_loss:0.453, val_acc:0.872]
Epoch [25/120    avg_loss:0.543, val_acc:0.820]
Epoch [26/120    avg_loss:0.471, val_acc:0.875]
Epoch [27/120    avg_loss:0.366, val_acc:0.871]
Epoch [28/120    avg_loss:0.348, val_acc:0.856]
Epoch [29/120    avg_loss:0.360, val_acc:0.888]
Epoch [30/120    avg_loss:0.337, val_acc:0.895]
Epoch [31/120    avg_loss:0.309, val_acc:0.891]
Epoch [32/120    avg_loss:0.288, val_acc:0.878]
Epoch [33/120    avg_loss:0.273, val_acc:0.901]
Epoch [34/120    avg_loss:0.417, val_acc:0.900]
Epoch [35/120    avg_loss:0.285, val_acc:0.908]
Epoch [36/120    avg_loss:0.283, val_acc:0.886]
Epoch [37/120    avg_loss:0.241, val_acc:0.906]
Epoch [38/120    avg_loss:0.271, val_acc:0.866]
Epoch [39/120    avg_loss:0.235, val_acc:0.897]
Epoch [40/120    avg_loss:0.229, val_acc:0.915]
Epoch [41/120    avg_loss:0.181, val_acc:0.919]
Epoch [42/120    avg_loss:0.168, val_acc:0.924]
Epoch [43/120    avg_loss:0.137, val_acc:0.903]
Epoch [44/120    avg_loss:0.215, val_acc:0.901]
Epoch [45/120    avg_loss:0.171, val_acc:0.927]
Epoch [46/120    avg_loss:0.138, val_acc:0.913]
Epoch [47/120    avg_loss:0.135, val_acc:0.938]
Epoch [48/120    avg_loss:0.150, val_acc:0.922]
Epoch [49/120    avg_loss:0.180, val_acc:0.915]
Epoch [50/120    avg_loss:0.180, val_acc:0.924]
Epoch [51/120    avg_loss:0.122, val_acc:0.935]
Epoch [52/120    avg_loss:0.125, val_acc:0.939]
Epoch [53/120    avg_loss:0.126, val_acc:0.946]
Epoch [54/120    avg_loss:0.101, val_acc:0.942]
Epoch [55/120    avg_loss:0.140, val_acc:0.943]
Epoch [56/120    avg_loss:0.127, val_acc:0.936]
Epoch [57/120    avg_loss:0.122, val_acc:0.950]
Epoch [58/120    avg_loss:0.083, val_acc:0.938]
Epoch [59/120    avg_loss:0.111, val_acc:0.939]
Epoch [60/120    avg_loss:0.097, val_acc:0.940]
Epoch [61/120    avg_loss:0.081, val_acc:0.953]
Epoch [62/120    avg_loss:0.091, val_acc:0.951]
Epoch [63/120    avg_loss:0.173, val_acc:0.935]
Epoch [64/120    avg_loss:0.112, val_acc:0.940]
Epoch [65/120    avg_loss:0.077, val_acc:0.943]
Epoch [66/120    avg_loss:0.058, val_acc:0.948]
Epoch [67/120    avg_loss:0.054, val_acc:0.958]
Epoch [68/120    avg_loss:0.064, val_acc:0.948]
Epoch [69/120    avg_loss:0.055, val_acc:0.956]
Epoch [70/120    avg_loss:0.068, val_acc:0.959]
Epoch [71/120    avg_loss:0.062, val_acc:0.950]
Epoch [72/120    avg_loss:0.053, val_acc:0.955]
Epoch [73/120    avg_loss:0.065, val_acc:0.961]
Epoch [74/120    avg_loss:0.066, val_acc:0.955]
Epoch [75/120    avg_loss:0.071, val_acc:0.960]
Epoch [76/120    avg_loss:0.062, val_acc:0.927]
Epoch [77/120    avg_loss:0.048, val_acc:0.967]
Epoch [78/120    avg_loss:0.043, val_acc:0.955]
Epoch [79/120    avg_loss:0.046, val_acc:0.971]
Epoch [80/120    avg_loss:0.033, val_acc:0.967]
Epoch [81/120    avg_loss:0.034, val_acc:0.971]
Epoch [82/120    avg_loss:0.048, val_acc:0.971]
Epoch [83/120    avg_loss:0.048, val_acc:0.943]
Epoch [84/120    avg_loss:0.033, val_acc:0.965]
Epoch [85/120    avg_loss:0.031, val_acc:0.961]
Epoch [86/120    avg_loss:0.045, val_acc:0.960]
Epoch [87/120    avg_loss:0.041, val_acc:0.955]
Epoch [88/120    avg_loss:0.038, val_acc:0.964]
Epoch [89/120    avg_loss:0.040, val_acc:0.968]
Epoch [90/120    avg_loss:0.037, val_acc:0.972]
Epoch [91/120    avg_loss:0.039, val_acc:0.954]
Epoch [92/120    avg_loss:0.046, val_acc:0.963]
Epoch [93/120    avg_loss:0.034, val_acc:0.964]
Epoch [94/120    avg_loss:0.041, val_acc:0.969]
Epoch [95/120    avg_loss:0.027, val_acc:0.972]
Epoch [96/120    avg_loss:0.029, val_acc:0.964]
Epoch [97/120    avg_loss:0.031, val_acc:0.969]
Epoch [98/120    avg_loss:0.032, val_acc:0.967]
Epoch [99/120    avg_loss:0.031, val_acc:0.969]
Epoch [100/120    avg_loss:0.029, val_acc:0.965]
Epoch [101/120    avg_loss:0.022, val_acc:0.969]
Epoch [102/120    avg_loss:0.026, val_acc:0.968]
Epoch [103/120    avg_loss:0.039, val_acc:0.967]
Epoch [104/120    avg_loss:0.044, val_acc:0.970]
Epoch [105/120    avg_loss:0.035, val_acc:0.969]
Epoch [106/120    avg_loss:0.028, val_acc:0.965]
Epoch [107/120    avg_loss:0.023, val_acc:0.971]
Epoch [108/120    avg_loss:0.025, val_acc:0.970]
Epoch [109/120    avg_loss:0.018, val_acc:0.972]
Epoch [110/120    avg_loss:0.017, val_acc:0.971]
Epoch [111/120    avg_loss:0.015, val_acc:0.973]
Epoch [112/120    avg_loss:0.016, val_acc:0.973]
Epoch [113/120    avg_loss:0.017, val_acc:0.977]
Epoch [114/120    avg_loss:0.016, val_acc:0.977]
Epoch [115/120    avg_loss:0.017, val_acc:0.977]
Epoch [116/120    avg_loss:0.014, val_acc:0.975]
Epoch [117/120    avg_loss:0.012, val_acc:0.973]
Epoch [118/120    avg_loss:0.014, val_acc:0.975]
Epoch [119/120    avg_loss:0.014, val_acc:0.973]
Epoch [120/120    avg_loss:0.018, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1246    3    0    0    0    0    0    0    8   26    2    0
     0    0    0]
 [   0    0    0  712    0    0    0    0    0    6    0    0   26    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    2    1    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   20   57    0    2    0    0    0    0  795    0    1    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0   18 2182    3    1
     0    0    0]
 [   0    0    0   15    0    0    0    0    0    3   12   11  490    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   36    0    0    1    0    0    0    0
    66  244    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.30352303523036

F1 scores:
[       nan 0.96202532 0.97457959 0.92829205 1.         0.98734177
 0.97189349 0.98039216 1.         0.7826087  0.92928112 0.985324
 0.9280303  0.98930481 0.96928328 0.82571912 0.98245614]

Kappa:
0.9578242777386196
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f231b649b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.691, val_acc:0.244]
Epoch [2/120    avg_loss:2.469, val_acc:0.419]
Epoch [3/120    avg_loss:2.274, val_acc:0.492]
Epoch [4/120    avg_loss:2.143, val_acc:0.548]
Epoch [5/120    avg_loss:2.003, val_acc:0.544]
Epoch [6/120    avg_loss:1.886, val_acc:0.609]
Epoch [7/120    avg_loss:1.817, val_acc:0.624]
Epoch [8/120    avg_loss:1.667, val_acc:0.658]
Epoch [9/120    avg_loss:1.494, val_acc:0.627]
Epoch [10/120    avg_loss:1.366, val_acc:0.689]
Epoch [11/120    avg_loss:1.299, val_acc:0.691]
Epoch [12/120    avg_loss:1.196, val_acc:0.717]
Epoch [13/120    avg_loss:1.094, val_acc:0.719]
Epoch [14/120    avg_loss:0.964, val_acc:0.739]
Epoch [15/120    avg_loss:0.963, val_acc:0.763]
Epoch [16/120    avg_loss:0.860, val_acc:0.772]
Epoch [17/120    avg_loss:0.757, val_acc:0.743]
Epoch [18/120    avg_loss:0.708, val_acc:0.790]
Epoch [19/120    avg_loss:0.700, val_acc:0.775]
Epoch [20/120    avg_loss:0.647, val_acc:0.805]
Epoch [21/120    avg_loss:0.542, val_acc:0.817]
Epoch [22/120    avg_loss:0.503, val_acc:0.838]
Epoch [23/120    avg_loss:0.563, val_acc:0.833]
Epoch [24/120    avg_loss:0.467, val_acc:0.839]
Epoch [25/120    avg_loss:0.460, val_acc:0.886]
Epoch [26/120    avg_loss:0.325, val_acc:0.882]
Epoch [27/120    avg_loss:0.392, val_acc:0.881]
Epoch [28/120    avg_loss:0.292, val_acc:0.896]
Epoch [29/120    avg_loss:0.277, val_acc:0.898]
Epoch [30/120    avg_loss:0.335, val_acc:0.877]
Epoch [31/120    avg_loss:0.290, val_acc:0.896]
Epoch [32/120    avg_loss:0.266, val_acc:0.884]
Epoch [33/120    avg_loss:0.211, val_acc:0.927]
Epoch [34/120    avg_loss:0.248, val_acc:0.920]
Epoch [35/120    avg_loss:0.200, val_acc:0.895]
Epoch [36/120    avg_loss:0.251, val_acc:0.859]
Epoch [37/120    avg_loss:0.253, val_acc:0.901]
Epoch [38/120    avg_loss:0.200, val_acc:0.923]
Epoch [39/120    avg_loss:0.189, val_acc:0.923]
Epoch [40/120    avg_loss:0.157, val_acc:0.933]
Epoch [41/120    avg_loss:0.137, val_acc:0.940]
Epoch [42/120    avg_loss:0.149, val_acc:0.922]
Epoch [43/120    avg_loss:0.120, val_acc:0.917]
Epoch [44/120    avg_loss:0.131, val_acc:0.934]
Epoch [45/120    avg_loss:0.196, val_acc:0.935]
Epoch [46/120    avg_loss:0.162, val_acc:0.940]
Epoch [47/120    avg_loss:0.152, val_acc:0.942]
Epoch [48/120    avg_loss:0.100, val_acc:0.927]
Epoch [49/120    avg_loss:0.121, val_acc:0.942]
Epoch [50/120    avg_loss:0.087, val_acc:0.951]
Epoch [51/120    avg_loss:0.093, val_acc:0.953]
Epoch [52/120    avg_loss:0.079, val_acc:0.943]
Epoch [53/120    avg_loss:0.101, val_acc:0.939]
Epoch [54/120    avg_loss:0.078, val_acc:0.946]
Epoch [55/120    avg_loss:0.082, val_acc:0.913]
Epoch [56/120    avg_loss:0.086, val_acc:0.950]
Epoch [57/120    avg_loss:0.074, val_acc:0.955]
Epoch [58/120    avg_loss:0.062, val_acc:0.964]
Epoch [59/120    avg_loss:0.061, val_acc:0.959]
Epoch [60/120    avg_loss:0.056, val_acc:0.954]
Epoch [61/120    avg_loss:0.049, val_acc:0.960]
Epoch [62/120    avg_loss:0.053, val_acc:0.969]
Epoch [63/120    avg_loss:0.054, val_acc:0.956]
Epoch [64/120    avg_loss:0.048, val_acc:0.960]
Epoch [65/120    avg_loss:0.041, val_acc:0.964]
Epoch [66/120    avg_loss:0.055, val_acc:0.952]
Epoch [67/120    avg_loss:0.052, val_acc:0.953]
Epoch [68/120    avg_loss:0.156, val_acc:0.895]
Epoch [69/120    avg_loss:0.182, val_acc:0.911]
Epoch [70/120    avg_loss:0.149, val_acc:0.943]
Epoch [71/120    avg_loss:0.082, val_acc:0.944]
Epoch [72/120    avg_loss:0.079, val_acc:0.958]
Epoch [73/120    avg_loss:0.064, val_acc:0.954]
Epoch [74/120    avg_loss:0.050, val_acc:0.959]
Epoch [75/120    avg_loss:0.045, val_acc:0.949]
Epoch [76/120    avg_loss:0.037, val_acc:0.963]
Epoch [77/120    avg_loss:0.031, val_acc:0.962]
Epoch [78/120    avg_loss:0.027, val_acc:0.963]
Epoch [79/120    avg_loss:0.030, val_acc:0.960]
Epoch [80/120    avg_loss:0.032, val_acc:0.961]
Epoch [81/120    avg_loss:0.028, val_acc:0.967]
Epoch [82/120    avg_loss:0.022, val_acc:0.964]
Epoch [83/120    avg_loss:0.026, val_acc:0.968]
Epoch [84/120    avg_loss:0.025, val_acc:0.967]
Epoch [85/120    avg_loss:0.029, val_acc:0.968]
Epoch [86/120    avg_loss:0.028, val_acc:0.965]
Epoch [87/120    avg_loss:0.021, val_acc:0.964]
Epoch [88/120    avg_loss:0.029, val_acc:0.965]
Epoch [89/120    avg_loss:0.026, val_acc:0.964]
Epoch [90/120    avg_loss:0.046, val_acc:0.965]
Epoch [91/120    avg_loss:0.023, val_acc:0.965]
Epoch [92/120    avg_loss:0.020, val_acc:0.964]
Epoch [93/120    avg_loss:0.021, val_acc:0.964]
Epoch [94/120    avg_loss:0.031, val_acc:0.964]
Epoch [95/120    avg_loss:0.031, val_acc:0.964]
Epoch [96/120    avg_loss:0.020, val_acc:0.964]
Epoch [97/120    avg_loss:0.031, val_acc:0.965]
Epoch [98/120    avg_loss:0.022, val_acc:0.967]
Epoch [99/120    avg_loss:0.029, val_acc:0.965]
Epoch [100/120    avg_loss:0.024, val_acc:0.965]
Epoch [101/120    avg_loss:0.021, val_acc:0.965]
Epoch [102/120    avg_loss:0.025, val_acc:0.965]
Epoch [103/120    avg_loss:0.021, val_acc:0.965]
Epoch [104/120    avg_loss:0.018, val_acc:0.965]
Epoch [105/120    avg_loss:0.021, val_acc:0.965]
Epoch [106/120    avg_loss:0.022, val_acc:0.965]
Epoch [107/120    avg_loss:0.023, val_acc:0.965]
Epoch [108/120    avg_loss:0.027, val_acc:0.965]
Epoch [109/120    avg_loss:0.023, val_acc:0.965]
Epoch [110/120    avg_loss:0.021, val_acc:0.965]
Epoch [111/120    avg_loss:0.026, val_acc:0.965]
Epoch [112/120    avg_loss:0.029, val_acc:0.965]
Epoch [113/120    avg_loss:0.024, val_acc:0.965]
Epoch [114/120    avg_loss:0.022, val_acc:0.965]
Epoch [115/120    avg_loss:0.022, val_acc:0.965]
Epoch [116/120    avg_loss:0.021, val_acc:0.965]
Epoch [117/120    avg_loss:0.022, val_acc:0.965]
Epoch [118/120    avg_loss:0.029, val_acc:0.965]
Epoch [119/120    avg_loss:0.022, val_acc:0.965]
Epoch [120/120    avg_loss:0.022, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1230    6   15    0    5    0    0    0    2   24    3    0
     0    0    0]
 [   0    0    4  733    0    4    0    0    0    1    1    0    4    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    1    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   28   66    0    7    0    0    0    0  764    7    1    0
     1    1    0]
 [   0    0    4    0    0    0    1    0    2    0   21 2146   36    0
     0    0    0]
 [   0    0    0    1    3    9    0    0    0    1    0    6  506    0
     1    3    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  184
     0    1    0]
 [   0    1    0    0    0   13    0    0    1    0    3    0    0    0
  1115    6    0]
 [   0    0    0    0    0    0    9    0    0    2    0    0    0    0
    54  282    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.95663956639567

F1 scores:
[       nan 0.96296296 0.96432771 0.94337194 0.95711061 0.95535714
 0.98795181 1.         0.99652375 0.87804878 0.91606715 0.97678653
 0.93357934 0.99728997 0.96286701 0.88125    0.97674419]

Kappa:
0.9539329382841613
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:20:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8b3c2eda58>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.757, val_acc:0.331]
Epoch [2/120    avg_loss:2.560, val_acc:0.426]
Epoch [3/120    avg_loss:2.383, val_acc:0.502]
Epoch [4/120    avg_loss:2.221, val_acc:0.531]
Epoch [5/120    avg_loss:2.118, val_acc:0.519]
Epoch [6/120    avg_loss:1.966, val_acc:0.552]
Epoch [7/120    avg_loss:1.897, val_acc:0.593]
Epoch [8/120    avg_loss:1.754, val_acc:0.617]
Epoch [9/120    avg_loss:1.659, val_acc:0.641]
Epoch [10/120    avg_loss:1.574, val_acc:0.621]
Epoch [11/120    avg_loss:1.451, val_acc:0.644]
Epoch [12/120    avg_loss:1.405, val_acc:0.686]
Epoch [13/120    avg_loss:1.260, val_acc:0.732]
Epoch [14/120    avg_loss:1.136, val_acc:0.725]
Epoch [15/120    avg_loss:1.054, val_acc:0.765]
Epoch [16/120    avg_loss:0.938, val_acc:0.718]
Epoch [17/120    avg_loss:0.933, val_acc:0.773]
Epoch [18/120    avg_loss:0.799, val_acc:0.810]
Epoch [19/120    avg_loss:0.766, val_acc:0.778]
Epoch [20/120    avg_loss:0.729, val_acc:0.815]
Epoch [21/120    avg_loss:0.581, val_acc:0.809]
Epoch [22/120    avg_loss:0.555, val_acc:0.818]
Epoch [23/120    avg_loss:0.537, val_acc:0.749]
Epoch [24/120    avg_loss:0.522, val_acc:0.848]
Epoch [25/120    avg_loss:0.498, val_acc:0.857]
Epoch [26/120    avg_loss:0.435, val_acc:0.820]
Epoch [27/120    avg_loss:0.452, val_acc:0.869]
Epoch [28/120    avg_loss:0.399, val_acc:0.872]
Epoch [29/120    avg_loss:0.347, val_acc:0.876]
Epoch [30/120    avg_loss:0.315, val_acc:0.898]
Epoch [31/120    avg_loss:0.316, val_acc:0.862]
Epoch [32/120    avg_loss:0.279, val_acc:0.913]
Epoch [33/120    avg_loss:0.280, val_acc:0.886]
Epoch [34/120    avg_loss:0.242, val_acc:0.883]
Epoch [35/120    avg_loss:0.273, val_acc:0.911]
Epoch [36/120    avg_loss:0.252, val_acc:0.891]
Epoch [37/120    avg_loss:0.269, val_acc:0.913]
Epoch [38/120    avg_loss:0.214, val_acc:0.872]
Epoch [39/120    avg_loss:0.200, val_acc:0.912]
Epoch [40/120    avg_loss:0.187, val_acc:0.925]
Epoch [41/120    avg_loss:0.179, val_acc:0.912]
Epoch [42/120    avg_loss:0.194, val_acc:0.889]
Epoch [43/120    avg_loss:0.196, val_acc:0.905]
Epoch [44/120    avg_loss:0.187, val_acc:0.923]
Epoch [45/120    avg_loss:0.156, val_acc:0.913]
Epoch [46/120    avg_loss:0.128, val_acc:0.903]
Epoch [47/120    avg_loss:0.112, val_acc:0.926]
Epoch [48/120    avg_loss:0.143, val_acc:0.929]
Epoch [49/120    avg_loss:0.163, val_acc:0.915]
Epoch [50/120    avg_loss:0.125, val_acc:0.945]
Epoch [51/120    avg_loss:0.126, val_acc:0.932]
Epoch [52/120    avg_loss:0.098, val_acc:0.942]
Epoch [53/120    avg_loss:0.071, val_acc:0.949]
Epoch [54/120    avg_loss:0.087, val_acc:0.952]
Epoch [55/120    avg_loss:0.077, val_acc:0.955]
Epoch [56/120    avg_loss:0.113, val_acc:0.919]
Epoch [57/120    avg_loss:0.155, val_acc:0.939]
Epoch [58/120    avg_loss:0.099, val_acc:0.954]
Epoch [59/120    avg_loss:0.092, val_acc:0.942]
Epoch [60/120    avg_loss:0.080, val_acc:0.949]
Epoch [61/120    avg_loss:0.080, val_acc:0.932]
Epoch [62/120    avg_loss:0.141, val_acc:0.939]
Epoch [63/120    avg_loss:0.128, val_acc:0.946]
Epoch [64/120    avg_loss:0.081, val_acc:0.944]
Epoch [65/120    avg_loss:0.079, val_acc:0.936]
Epoch [66/120    avg_loss:0.198, val_acc:0.942]
Epoch [67/120    avg_loss:0.068, val_acc:0.952]
Epoch [68/120    avg_loss:0.060, val_acc:0.962]
Epoch [69/120    avg_loss:0.057, val_acc:0.964]
Epoch [70/120    avg_loss:0.048, val_acc:0.958]
Epoch [71/120    avg_loss:0.049, val_acc:0.950]
Epoch [72/120    avg_loss:0.042, val_acc:0.956]
Epoch [73/120    avg_loss:0.038, val_acc:0.962]
Epoch [74/120    avg_loss:0.041, val_acc:0.961]
Epoch [75/120    avg_loss:0.032, val_acc:0.964]
Epoch [76/120    avg_loss:0.042, val_acc:0.966]
Epoch [77/120    avg_loss:0.036, val_acc:0.959]
Epoch [78/120    avg_loss:0.032, val_acc:0.962]
Epoch [79/120    avg_loss:0.030, val_acc:0.960]
Epoch [80/120    avg_loss:0.037, val_acc:0.955]
Epoch [81/120    avg_loss:0.029, val_acc:0.974]
Epoch [82/120    avg_loss:0.032, val_acc:0.957]
Epoch [83/120    avg_loss:0.031, val_acc:0.960]
Epoch [84/120    avg_loss:0.052, val_acc:0.952]
Epoch [85/120    avg_loss:0.034, val_acc:0.958]
Epoch [86/120    avg_loss:0.037, val_acc:0.967]
Epoch [87/120    avg_loss:0.038, val_acc:0.962]
Epoch [88/120    avg_loss:0.038, val_acc:0.969]
Epoch [89/120    avg_loss:0.048, val_acc:0.962]
Epoch [90/120    avg_loss:0.048, val_acc:0.952]
Epoch [91/120    avg_loss:0.059, val_acc:0.964]
Epoch [92/120    avg_loss:0.043, val_acc:0.958]
Epoch [93/120    avg_loss:0.029, val_acc:0.970]
Epoch [94/120    avg_loss:0.026, val_acc:0.975]
Epoch [95/120    avg_loss:0.020, val_acc:0.976]
Epoch [96/120    avg_loss:0.046, val_acc:0.954]
Epoch [97/120    avg_loss:0.032, val_acc:0.960]
Epoch [98/120    avg_loss:0.027, val_acc:0.972]
Epoch [99/120    avg_loss:0.020, val_acc:0.970]
Epoch [100/120    avg_loss:0.017, val_acc:0.963]
Epoch [101/120    avg_loss:0.018, val_acc:0.973]
Epoch [102/120    avg_loss:0.023, val_acc:0.974]
Epoch [103/120    avg_loss:0.021, val_acc:0.966]
Epoch [104/120    avg_loss:0.025, val_acc:0.969]
Epoch [105/120    avg_loss:0.019, val_acc:0.968]
Epoch [106/120    avg_loss:0.015, val_acc:0.961]
Epoch [107/120    avg_loss:0.027, val_acc:0.963]
Epoch [108/120    avg_loss:0.025, val_acc:0.964]
Epoch [109/120    avg_loss:0.014, val_acc:0.970]
Epoch [110/120    avg_loss:0.016, val_acc:0.973]
Epoch [111/120    avg_loss:0.012, val_acc:0.974]
Epoch [112/120    avg_loss:0.010, val_acc:0.975]
Epoch [113/120    avg_loss:0.009, val_acc:0.973]
Epoch [114/120    avg_loss:0.015, val_acc:0.973]
Epoch [115/120    avg_loss:0.010, val_acc:0.972]
Epoch [116/120    avg_loss:0.016, val_acc:0.972]
Epoch [117/120    avg_loss:0.010, val_acc:0.974]
Epoch [118/120    avg_loss:0.011, val_acc:0.974]
Epoch [119/120    avg_loss:0.011, val_acc:0.972]
Epoch [120/120    avg_loss:0.012, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1244    1    0    0    3    0    0    0   12   24    1    0
     0    0    0]
 [   0    0    0  720    1    3    0    0    0   11    2    1    8    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    2   16    0    8    0    0    0    0  816   32    1    0
     0    0    0]
 [   0    3    1    0    0    2    2    0    0    0   12 2186    3    0
     1    0    0]
 [   0    0    0    0    0   15    0    0    0    0    1   25  492    0
     0    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1137    1    0]
 [   0    0    0    0    0    1   28    0    0    0    0    0    0    0
    52  266    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
96.92140921409214

F1 scores:
[       nan 0.93975904 0.98262243 0.9703504  0.99765808 0.96544036
 0.97477745 1.         0.99883856 0.72340426 0.94883721 0.97632872
 0.94433781 0.99730458 0.97596567 0.86504065 0.98181818]

Kappa:
0.9648549115801006
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fadff7f8ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.760, val_acc:0.285]
Epoch [2/120    avg_loss:2.559, val_acc:0.339]
Epoch [3/120    avg_loss:2.390, val_acc:0.372]
Epoch [4/120    avg_loss:2.249, val_acc:0.477]
Epoch [5/120    avg_loss:2.105, val_acc:0.568]
Epoch [6/120    avg_loss:1.955, val_acc:0.594]
Epoch [7/120    avg_loss:1.871, val_acc:0.602]
Epoch [8/120    avg_loss:1.783, val_acc:0.628]
Epoch [9/120    avg_loss:1.636, val_acc:0.646]
Epoch [10/120    avg_loss:1.453, val_acc:0.670]
Epoch [11/120    avg_loss:1.415, val_acc:0.669]
Epoch [12/120    avg_loss:1.233, val_acc:0.723]
Epoch [13/120    avg_loss:1.144, val_acc:0.726]
Epoch [14/120    avg_loss:1.025, val_acc:0.737]
Epoch [15/120    avg_loss:0.900, val_acc:0.702]
Epoch [16/120    avg_loss:0.973, val_acc:0.670]
Epoch [17/120    avg_loss:0.886, val_acc:0.772]
Epoch [18/120    avg_loss:0.762, val_acc:0.771]
Epoch [19/120    avg_loss:0.667, val_acc:0.800]
Epoch [20/120    avg_loss:0.593, val_acc:0.822]
Epoch [21/120    avg_loss:0.690, val_acc:0.836]
Epoch [22/120    avg_loss:0.640, val_acc:0.840]
Epoch [23/120    avg_loss:0.496, val_acc:0.871]
Epoch [24/120    avg_loss:0.461, val_acc:0.862]
Epoch [25/120    avg_loss:0.414, val_acc:0.873]
Epoch [26/120    avg_loss:0.372, val_acc:0.875]
Epoch [27/120    avg_loss:0.329, val_acc:0.873]
Epoch [28/120    avg_loss:0.321, val_acc:0.877]
Epoch [29/120    avg_loss:0.357, val_acc:0.891]
Epoch [30/120    avg_loss:0.292, val_acc:0.907]
Epoch [31/120    avg_loss:0.267, val_acc:0.916]
Epoch [32/120    avg_loss:0.251, val_acc:0.913]
Epoch [33/120    avg_loss:0.235, val_acc:0.899]
Epoch [34/120    avg_loss:0.222, val_acc:0.919]
Epoch [35/120    avg_loss:0.235, val_acc:0.884]
Epoch [36/120    avg_loss:0.247, val_acc:0.889]
Epoch [37/120    avg_loss:0.258, val_acc:0.880]
Epoch [38/120    avg_loss:0.250, val_acc:0.918]
Epoch [39/120    avg_loss:0.179, val_acc:0.934]
Epoch [40/120    avg_loss:0.158, val_acc:0.917]
Epoch [41/120    avg_loss:0.147, val_acc:0.931]
Epoch [42/120    avg_loss:0.130, val_acc:0.920]
Epoch [43/120    avg_loss:0.127, val_acc:0.944]
Epoch [44/120    avg_loss:0.141, val_acc:0.942]
Epoch [45/120    avg_loss:0.169, val_acc:0.917]
Epoch [46/120    avg_loss:0.182, val_acc:0.924]
Epoch [47/120    avg_loss:0.153, val_acc:0.935]
Epoch [48/120    avg_loss:0.200, val_acc:0.917]
Epoch [49/120    avg_loss:0.164, val_acc:0.942]
Epoch [50/120    avg_loss:0.152, val_acc:0.933]
Epoch [51/120    avg_loss:0.118, val_acc:0.942]
Epoch [52/120    avg_loss:0.105, val_acc:0.931]
Epoch [53/120    avg_loss:0.088, val_acc:0.951]
Epoch [54/120    avg_loss:0.757, val_acc:0.628]
Epoch [55/120    avg_loss:0.583, val_acc:0.895]
Epoch [56/120    avg_loss:0.210, val_acc:0.928]
Epoch [57/120    avg_loss:0.129, val_acc:0.921]
Epoch [58/120    avg_loss:0.142, val_acc:0.933]
Epoch [59/120    avg_loss:0.116, val_acc:0.930]
Epoch [60/120    avg_loss:0.102, val_acc:0.940]
Epoch [61/120    avg_loss:0.085, val_acc:0.942]
Epoch [62/120    avg_loss:0.094, val_acc:0.940]
Epoch [63/120    avg_loss:0.087, val_acc:0.954]
Epoch [64/120    avg_loss:0.070, val_acc:0.948]
Epoch [65/120    avg_loss:0.075, val_acc:0.955]
Epoch [66/120    avg_loss:0.059, val_acc:0.954]
Epoch [67/120    avg_loss:0.054, val_acc:0.965]
Epoch [68/120    avg_loss:0.070, val_acc:0.959]
Epoch [69/120    avg_loss:0.077, val_acc:0.953]
Epoch [70/120    avg_loss:0.057, val_acc:0.964]
Epoch [71/120    avg_loss:0.058, val_acc:0.964]
Epoch [72/120    avg_loss:0.053, val_acc:0.959]
Epoch [73/120    avg_loss:0.037, val_acc:0.966]
Epoch [74/120    avg_loss:0.057, val_acc:0.955]
Epoch [75/120    avg_loss:0.055, val_acc:0.958]
Epoch [76/120    avg_loss:0.070, val_acc:0.947]
Epoch [77/120    avg_loss:0.076, val_acc:0.963]
Epoch [78/120    avg_loss:0.055, val_acc:0.966]
Epoch [79/120    avg_loss:0.040, val_acc:0.966]
Epoch [80/120    avg_loss:0.043, val_acc:0.963]
Epoch [81/120    avg_loss:0.028, val_acc:0.962]
Epoch [82/120    avg_loss:0.042, val_acc:0.967]
Epoch [83/120    avg_loss:0.036, val_acc:0.967]
Epoch [84/120    avg_loss:0.032, val_acc:0.973]
Epoch [85/120    avg_loss:0.035, val_acc:0.962]
Epoch [86/120    avg_loss:0.038, val_acc:0.973]
Epoch [87/120    avg_loss:0.028, val_acc:0.973]
Epoch [88/120    avg_loss:0.026, val_acc:0.970]
Epoch [89/120    avg_loss:0.052, val_acc:0.959]
Epoch [90/120    avg_loss:0.041, val_acc:0.974]
Epoch [91/120    avg_loss:0.037, val_acc:0.933]
Epoch [92/120    avg_loss:0.063, val_acc:0.974]
Epoch [93/120    avg_loss:0.030, val_acc:0.967]
Epoch [94/120    avg_loss:0.029, val_acc:0.970]
Epoch [95/120    avg_loss:0.022, val_acc:0.964]
Epoch [96/120    avg_loss:0.027, val_acc:0.969]
Epoch [97/120    avg_loss:0.027, val_acc:0.978]
Epoch [98/120    avg_loss:0.019, val_acc:0.981]
Epoch [99/120    avg_loss:0.020, val_acc:0.986]
Epoch [100/120    avg_loss:0.016, val_acc:0.977]
Epoch [101/120    avg_loss:0.019, val_acc:0.964]
Epoch [102/120    avg_loss:0.028, val_acc:0.969]
Epoch [103/120    avg_loss:0.027, val_acc:0.977]
Epoch [104/120    avg_loss:0.016, val_acc:0.973]
Epoch [105/120    avg_loss:0.024, val_acc:0.977]
Epoch [106/120    avg_loss:0.025, val_acc:0.975]
Epoch [107/120    avg_loss:0.016, val_acc:0.966]
Epoch [108/120    avg_loss:0.017, val_acc:0.975]
Epoch [109/120    avg_loss:0.019, val_acc:0.973]
Epoch [110/120    avg_loss:0.017, val_acc:0.974]
Epoch [111/120    avg_loss:0.018, val_acc:0.971]
Epoch [112/120    avg_loss:0.021, val_acc:0.982]
Epoch [113/120    avg_loss:0.017, val_acc:0.981]
Epoch [114/120    avg_loss:0.014, val_acc:0.981]
Epoch [115/120    avg_loss:0.014, val_acc:0.982]
Epoch [116/120    avg_loss:0.011, val_acc:0.982]
Epoch [117/120    avg_loss:0.015, val_acc:0.984]
Epoch [118/120    avg_loss:0.019, val_acc:0.984]
Epoch [119/120    avg_loss:0.013, val_acc:0.985]
Epoch [120/120    avg_loss:0.013, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1252    1    1    0    0    0    0    0    2   24    5    0
     0    0    0]
 [   0    0    0  729    1   12    0    0    0    2    0    0    3    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    2    1    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    1    0    0   14    0    0    1    0
     0    0    0]
 [   0    0   11   58    0    0    1    0    0    0  805    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    2    0    6    0    9 2188    3    1
     0    0    0]
 [   0    0    0   12    2    2    0    0    0    0    7   18  486    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1135    2    0]
 [   0    0    0    0    0    0   23    0    0    5    0    0    0    0
    91  228    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.43360433604336

F1 scores:
[       nan 0.975      0.98234602 0.94125242 0.98834499 0.97838453
 0.9761194  0.98039216 0.99192618 0.7        0.94650206 0.98536366
 0.94186047 0.99730458 0.95821022 0.79029463 0.96      ]

Kappa:
0.9592924624170253
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0bd840db00>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.782, val_acc:0.288]
Epoch [2/120    avg_loss:2.597, val_acc:0.435]
Epoch [3/120    avg_loss:2.406, val_acc:0.525]
Epoch [4/120    avg_loss:2.235, val_acc:0.580]
Epoch [5/120    avg_loss:2.101, val_acc:0.592]
Epoch [6/120    avg_loss:1.968, val_acc:0.602]
Epoch [7/120    avg_loss:1.812, val_acc:0.659]
Epoch [8/120    avg_loss:1.756, val_acc:0.620]
Epoch [9/120    avg_loss:1.619, val_acc:0.680]
Epoch [10/120    avg_loss:1.499, val_acc:0.683]
Epoch [11/120    avg_loss:1.370, val_acc:0.718]
Epoch [12/120    avg_loss:1.185, val_acc:0.726]
Epoch [13/120    avg_loss:1.061, val_acc:0.723]
Epoch [14/120    avg_loss:0.986, val_acc:0.727]
Epoch [15/120    avg_loss:0.925, val_acc:0.752]
Epoch [16/120    avg_loss:0.814, val_acc:0.718]
Epoch [17/120    avg_loss:0.732, val_acc:0.825]
Epoch [18/120    avg_loss:0.648, val_acc:0.795]
Epoch [19/120    avg_loss:0.595, val_acc:0.826]
Epoch [20/120    avg_loss:0.513, val_acc:0.842]
Epoch [21/120    avg_loss:0.472, val_acc:0.866]
Epoch [22/120    avg_loss:0.459, val_acc:0.853]
Epoch [23/120    avg_loss:0.427, val_acc:0.839]
Epoch [24/120    avg_loss:0.432, val_acc:0.873]
Epoch [25/120    avg_loss:0.392, val_acc:0.872]
Epoch [26/120    avg_loss:0.422, val_acc:0.845]
Epoch [27/120    avg_loss:0.365, val_acc:0.880]
Epoch [28/120    avg_loss:0.330, val_acc:0.888]
Epoch [29/120    avg_loss:0.283, val_acc:0.891]
Epoch [30/120    avg_loss:0.256, val_acc:0.907]
Epoch [31/120    avg_loss:0.219, val_acc:0.917]
Epoch [32/120    avg_loss:0.261, val_acc:0.905]
Epoch [33/120    avg_loss:0.220, val_acc:0.897]
Epoch [34/120    avg_loss:0.185, val_acc:0.928]
Epoch [35/120    avg_loss:0.198, val_acc:0.918]
Epoch [36/120    avg_loss:0.173, val_acc:0.895]
Epoch [37/120    avg_loss:0.174, val_acc:0.914]
Epoch [38/120    avg_loss:0.209, val_acc:0.877]
Epoch [39/120    avg_loss:0.161, val_acc:0.932]
Epoch [40/120    avg_loss:0.155, val_acc:0.920]
Epoch [41/120    avg_loss:0.167, val_acc:0.930]
Epoch [42/120    avg_loss:0.147, val_acc:0.924]
Epoch [43/120    avg_loss:0.109, val_acc:0.927]
Epoch [44/120    avg_loss:0.155, val_acc:0.893]
Epoch [45/120    avg_loss:0.120, val_acc:0.932]
Epoch [46/120    avg_loss:0.105, val_acc:0.930]
Epoch [47/120    avg_loss:0.129, val_acc:0.932]
Epoch [48/120    avg_loss:0.098, val_acc:0.947]
Epoch [49/120    avg_loss:0.080, val_acc:0.954]
Epoch [50/120    avg_loss:0.093, val_acc:0.936]
Epoch [51/120    avg_loss:0.090, val_acc:0.936]
Epoch [52/120    avg_loss:0.090, val_acc:0.933]
Epoch [53/120    avg_loss:0.091, val_acc:0.927]
Epoch [54/120    avg_loss:0.099, val_acc:0.944]
Epoch [55/120    avg_loss:0.090, val_acc:0.930]
Epoch [56/120    avg_loss:0.093, val_acc:0.942]
Epoch [57/120    avg_loss:0.067, val_acc:0.951]
Epoch [58/120    avg_loss:0.077, val_acc:0.941]
Epoch [59/120    avg_loss:0.064, val_acc:0.942]
Epoch [60/120    avg_loss:0.077, val_acc:0.964]
Epoch [61/120    avg_loss:0.054, val_acc:0.961]
Epoch [62/120    avg_loss:0.066, val_acc:0.953]
Epoch [63/120    avg_loss:0.050, val_acc:0.928]
Epoch [64/120    avg_loss:0.053, val_acc:0.954]
Epoch [65/120    avg_loss:0.055, val_acc:0.957]
Epoch [66/120    avg_loss:0.073, val_acc:0.931]
Epoch [67/120    avg_loss:0.137, val_acc:0.947]
Epoch [68/120    avg_loss:0.094, val_acc:0.951]
Epoch [69/120    avg_loss:0.053, val_acc:0.940]
Epoch [70/120    avg_loss:0.093, val_acc:0.945]
Epoch [71/120    avg_loss:0.054, val_acc:0.953]
Epoch [72/120    avg_loss:0.095, val_acc:0.944]
Epoch [73/120    avg_loss:0.088, val_acc:0.911]
Epoch [74/120    avg_loss:0.069, val_acc:0.958]
Epoch [75/120    avg_loss:0.042, val_acc:0.965]
Epoch [76/120    avg_loss:0.032, val_acc:0.967]
Epoch [77/120    avg_loss:0.034, val_acc:0.967]
Epoch [78/120    avg_loss:0.032, val_acc:0.968]
Epoch [79/120    avg_loss:0.037, val_acc:0.968]
Epoch [80/120    avg_loss:0.036, val_acc:0.968]
Epoch [81/120    avg_loss:0.027, val_acc:0.969]
Epoch [82/120    avg_loss:0.034, val_acc:0.968]
Epoch [83/120    avg_loss:0.029, val_acc:0.971]
Epoch [84/120    avg_loss:0.027, val_acc:0.966]
Epoch [85/120    avg_loss:0.029, val_acc:0.971]
Epoch [86/120    avg_loss:0.029, val_acc:0.967]
Epoch [87/120    avg_loss:0.031, val_acc:0.971]
Epoch [88/120    avg_loss:0.024, val_acc:0.970]
Epoch [89/120    avg_loss:0.025, val_acc:0.967]
Epoch [90/120    avg_loss:0.027, val_acc:0.966]
Epoch [91/120    avg_loss:0.024, val_acc:0.966]
Epoch [92/120    avg_loss:0.026, val_acc:0.968]
Epoch [93/120    avg_loss:0.024, val_acc:0.969]
Epoch [94/120    avg_loss:0.022, val_acc:0.968]
Epoch [95/120    avg_loss:0.022, val_acc:0.967]
Epoch [96/120    avg_loss:0.020, val_acc:0.967]
Epoch [97/120    avg_loss:0.020, val_acc:0.968]
Epoch [98/120    avg_loss:0.029, val_acc:0.971]
Epoch [99/120    avg_loss:0.020, val_acc:0.970]
Epoch [100/120    avg_loss:0.025, val_acc:0.971]
Epoch [101/120    avg_loss:0.023, val_acc:0.970]
Epoch [102/120    avg_loss:0.022, val_acc:0.969]
Epoch [103/120    avg_loss:0.020, val_acc:0.969]
Epoch [104/120    avg_loss:0.020, val_acc:0.969]
Epoch [105/120    avg_loss:0.019, val_acc:0.967]
Epoch [106/120    avg_loss:0.020, val_acc:0.968]
Epoch [107/120    avg_loss:0.022, val_acc:0.966]
Epoch [108/120    avg_loss:0.022, val_acc:0.967]
Epoch [109/120    avg_loss:0.027, val_acc:0.968]
Epoch [110/120    avg_loss:0.025, val_acc:0.967]
Epoch [111/120    avg_loss:0.022, val_acc:0.967]
Epoch [112/120    avg_loss:0.021, val_acc:0.967]
Epoch [113/120    avg_loss:0.019, val_acc:0.969]
Epoch [114/120    avg_loss:0.019, val_acc:0.968]
Epoch [115/120    avg_loss:0.017, val_acc:0.968]
Epoch [116/120    avg_loss:0.018, val_acc:0.967]
Epoch [117/120    avg_loss:0.017, val_acc:0.968]
Epoch [118/120    avg_loss:0.018, val_acc:0.967]
Epoch [119/120    avg_loss:0.019, val_acc:0.967]
Epoch [120/120    avg_loss:0.020, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1240    2    0    0    3    0    0    0   10   22    6    0
     0    2    0]
 [   0    0    0  700   17   22    0    0    0    5    2    0    1    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    3    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   12    0    2    1    0
     0    0    0]
 [   0    0   17   17    0   13    0    0    0    0  817    0    3    0
     0    8    0]
 [   0    0    9    0    0    0    9    1    0    0   17 2168    0    1
     3    2    0]
 [   0    0    0    0    0    9    0    0    0    0    0   15  506    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   11    0    0    0    0    1    0    0    0
  1127    0    0]
 [   0    0    0    0    0    0   15    0    0    3    0    0    0    0
    33  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.76964769647697

F1 scores:
[       nan 0.975      0.97216778 0.95497954 0.96162528 0.9359392
 0.97691735 0.92592593 0.997669   0.61538462 0.94779582 0.98143957
 0.96106363 0.99730458 0.97914857 0.90106545 0.98823529]

Kappa:
0.9631817000546814
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f73322dcac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.730, val_acc:0.265]
Epoch [2/120    avg_loss:2.525, val_acc:0.510]
Epoch [3/120    avg_loss:2.346, val_acc:0.541]
Epoch [4/120    avg_loss:2.187, val_acc:0.533]
Epoch [5/120    avg_loss:2.033, val_acc:0.541]
Epoch [6/120    avg_loss:1.928, val_acc:0.558]
Epoch [7/120    avg_loss:1.808, val_acc:0.557]
Epoch [8/120    avg_loss:1.693, val_acc:0.566]
Epoch [9/120    avg_loss:1.604, val_acc:0.596]
Epoch [10/120    avg_loss:1.449, val_acc:0.639]
Epoch [11/120    avg_loss:1.293, val_acc:0.642]
Epoch [12/120    avg_loss:1.183, val_acc:0.659]
Epoch [13/120    avg_loss:1.151, val_acc:0.653]
Epoch [14/120    avg_loss:0.949, val_acc:0.712]
Epoch [15/120    avg_loss:0.870, val_acc:0.714]
Epoch [16/120    avg_loss:0.818, val_acc:0.724]
Epoch [17/120    avg_loss:0.751, val_acc:0.747]
Epoch [18/120    avg_loss:0.754, val_acc:0.780]
Epoch [19/120    avg_loss:0.770, val_acc:0.754]
Epoch [20/120    avg_loss:0.654, val_acc:0.783]
Epoch [21/120    avg_loss:0.544, val_acc:0.783]
Epoch [22/120    avg_loss:0.500, val_acc:0.829]
Epoch [23/120    avg_loss:0.524, val_acc:0.804]
Epoch [24/120    avg_loss:0.427, val_acc:0.821]
Epoch [25/120    avg_loss:0.464, val_acc:0.794]
Epoch [26/120    avg_loss:0.482, val_acc:0.817]
Epoch [27/120    avg_loss:0.411, val_acc:0.853]
Epoch [28/120    avg_loss:0.319, val_acc:0.872]
Epoch [29/120    avg_loss:0.305, val_acc:0.851]
Epoch [30/120    avg_loss:0.288, val_acc:0.834]
Epoch [31/120    avg_loss:0.312, val_acc:0.861]
Epoch [32/120    avg_loss:0.220, val_acc:0.890]
Epoch [33/120    avg_loss:0.205, val_acc:0.920]
Epoch [34/120    avg_loss:0.250, val_acc:0.838]
Epoch [35/120    avg_loss:0.306, val_acc:0.860]
Epoch [36/120    avg_loss:0.287, val_acc:0.898]
Epoch [37/120    avg_loss:0.210, val_acc:0.912]
Epoch [38/120    avg_loss:0.188, val_acc:0.906]
Epoch [39/120    avg_loss:0.208, val_acc:0.905]
Epoch [40/120    avg_loss:0.177, val_acc:0.939]
Epoch [41/120    avg_loss:0.131, val_acc:0.926]
Epoch [42/120    avg_loss:0.141, val_acc:0.944]
Epoch [43/120    avg_loss:0.132, val_acc:0.930]
Epoch [44/120    avg_loss:0.322, val_acc:0.874]
Epoch [45/120    avg_loss:0.353, val_acc:0.737]
Epoch [46/120    avg_loss:0.451, val_acc:0.849]
Epoch [47/120    avg_loss:0.299, val_acc:0.900]
Epoch [48/120    avg_loss:0.181, val_acc:0.871]
Epoch [49/120    avg_loss:0.154, val_acc:0.923]
Epoch [50/120    avg_loss:0.100, val_acc:0.929]
Epoch [51/120    avg_loss:0.100, val_acc:0.922]
Epoch [52/120    avg_loss:0.074, val_acc:0.935]
Epoch [53/120    avg_loss:0.103, val_acc:0.929]
Epoch [54/120    avg_loss:0.102, val_acc:0.918]
Epoch [55/120    avg_loss:0.107, val_acc:0.930]
Epoch [56/120    avg_loss:0.075, val_acc:0.942]
Epoch [57/120    avg_loss:0.060, val_acc:0.947]
Epoch [58/120    avg_loss:0.061, val_acc:0.950]
Epoch [59/120    avg_loss:0.054, val_acc:0.953]
Epoch [60/120    avg_loss:0.051, val_acc:0.955]
Epoch [61/120    avg_loss:0.052, val_acc:0.950]
Epoch [62/120    avg_loss:0.047, val_acc:0.953]
Epoch [63/120    avg_loss:0.057, val_acc:0.956]
Epoch [64/120    avg_loss:0.048, val_acc:0.957]
Epoch [65/120    avg_loss:0.053, val_acc:0.956]
Epoch [66/120    avg_loss:0.066, val_acc:0.957]
Epoch [67/120    avg_loss:0.046, val_acc:0.957]
Epoch [68/120    avg_loss:0.044, val_acc:0.958]
Epoch [69/120    avg_loss:0.045, val_acc:0.956]
Epoch [70/120    avg_loss:0.050, val_acc:0.956]
Epoch [71/120    avg_loss:0.050, val_acc:0.955]
Epoch [72/120    avg_loss:0.051, val_acc:0.956]
Epoch [73/120    avg_loss:0.046, val_acc:0.958]
Epoch [74/120    avg_loss:0.051, val_acc:0.959]
Epoch [75/120    avg_loss:0.050, val_acc:0.959]
Epoch [76/120    avg_loss:0.054, val_acc:0.957]
Epoch [77/120    avg_loss:0.050, val_acc:0.960]
Epoch [78/120    avg_loss:0.048, val_acc:0.961]
Epoch [79/120    avg_loss:0.039, val_acc:0.959]
Epoch [80/120    avg_loss:0.051, val_acc:0.960]
Epoch [81/120    avg_loss:0.041, val_acc:0.959]
Epoch [82/120    avg_loss:0.044, val_acc:0.960]
Epoch [83/120    avg_loss:0.041, val_acc:0.960]
Epoch [84/120    avg_loss:0.046, val_acc:0.959]
Epoch [85/120    avg_loss:0.043, val_acc:0.959]
Epoch [86/120    avg_loss:0.041, val_acc:0.960]
Epoch [87/120    avg_loss:0.044, val_acc:0.960]
Epoch [88/120    avg_loss:0.046, val_acc:0.962]
Epoch [89/120    avg_loss:0.041, val_acc:0.959]
Epoch [90/120    avg_loss:0.040, val_acc:0.959]
Epoch [91/120    avg_loss:0.041, val_acc:0.961]
Epoch [92/120    avg_loss:0.042, val_acc:0.959]
Epoch [93/120    avg_loss:0.036, val_acc:0.960]
Epoch [94/120    avg_loss:0.036, val_acc:0.961]
Epoch [95/120    avg_loss:0.039, val_acc:0.960]
Epoch [96/120    avg_loss:0.040, val_acc:0.960]
Epoch [97/120    avg_loss:0.042, val_acc:0.961]
Epoch [98/120    avg_loss:0.041, val_acc:0.960]
Epoch [99/120    avg_loss:0.038, val_acc:0.960]
Epoch [100/120    avg_loss:0.040, val_acc:0.963]
Epoch [101/120    avg_loss:0.038, val_acc:0.963]
Epoch [102/120    avg_loss:0.036, val_acc:0.963]
Epoch [103/120    avg_loss:0.039, val_acc:0.964]
Epoch [104/120    avg_loss:0.032, val_acc:0.962]
Epoch [105/120    avg_loss:0.037, val_acc:0.961]
Epoch [106/120    avg_loss:0.035, val_acc:0.962]
Epoch [107/120    avg_loss:0.037, val_acc:0.963]
Epoch [108/120    avg_loss:0.034, val_acc:0.963]
Epoch [109/120    avg_loss:0.037, val_acc:0.961]
Epoch [110/120    avg_loss:0.041, val_acc:0.964]
Epoch [111/120    avg_loss:0.036, val_acc:0.963]
Epoch [112/120    avg_loss:0.032, val_acc:0.960]
Epoch [113/120    avg_loss:0.033, val_acc:0.966]
Epoch [114/120    avg_loss:0.038, val_acc:0.963]
Epoch [115/120    avg_loss:0.032, val_acc:0.963]
Epoch [116/120    avg_loss:0.035, val_acc:0.967]
Epoch [117/120    avg_loss:0.034, val_acc:0.963]
Epoch [118/120    avg_loss:0.032, val_acc:0.964]
Epoch [119/120    avg_loss:0.032, val_acc:0.964]
Epoch [120/120    avg_loss:0.035, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1249    0    1    0    0    0    0    0    8   25    0    0
     2    0    0]
 [   0    0    2  710    1   27    0    0    0    0    0    2    4    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    7   77    0    5    0    0    0    0  741   41    0    0
     4    0    0]
 [   0    0   10    0    0    3    1    0    0    0   13 2156   19    0
     8    0    0]
 [   0    0    0    2    4    5    0    0    0    0   10    5  506    0
     0    0    2]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0  183
     0    0    0]
 [   0    5    0    0    0    0    0    0    0    0    0    0    0    0
  1132    2    0]
 [   0    0    0    0    0    0   21    0    0    3    0    0    0    0
   116  207    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.14363143631437

F1 scores:
[       nan 0.91764706 0.97845672 0.92447917 0.98611111 0.94818082
 0.98130142 1.         1.         0.86486486 0.8987265  0.97117117
 0.95202258 0.99456522 0.94058995 0.74460432 0.98823529]

Kappa:
0.9445630634991958
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f061d855ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.754, val_acc:0.438]
Epoch [2/120    avg_loss:2.568, val_acc:0.452]
Epoch [3/120    avg_loss:2.415, val_acc:0.472]
Epoch [4/120    avg_loss:2.220, val_acc:0.480]
Epoch [5/120    avg_loss:2.128, val_acc:0.553]
Epoch [6/120    avg_loss:1.963, val_acc:0.540]
Epoch [7/120    avg_loss:1.882, val_acc:0.576]
Epoch [8/120    avg_loss:1.809, val_acc:0.627]
Epoch [9/120    avg_loss:1.686, val_acc:0.633]
Epoch [10/120    avg_loss:1.545, val_acc:0.657]
Epoch [11/120    avg_loss:1.424, val_acc:0.660]
Epoch [12/120    avg_loss:1.341, val_acc:0.681]
Epoch [13/120    avg_loss:1.194, val_acc:0.713]
Epoch [14/120    avg_loss:1.027, val_acc:0.760]
Epoch [15/120    avg_loss:0.992, val_acc:0.759]
Epoch [16/120    avg_loss:0.851, val_acc:0.750]
Epoch [17/120    avg_loss:0.838, val_acc:0.760]
Epoch [18/120    avg_loss:0.715, val_acc:0.728]
Epoch [19/120    avg_loss:0.689, val_acc:0.777]
Epoch [20/120    avg_loss:0.645, val_acc:0.773]
Epoch [21/120    avg_loss:0.555, val_acc:0.797]
Epoch [22/120    avg_loss:0.572, val_acc:0.778]
Epoch [23/120    avg_loss:0.513, val_acc:0.809]
Epoch [24/120    avg_loss:0.501, val_acc:0.810]
Epoch [25/120    avg_loss:0.466, val_acc:0.836]
Epoch [26/120    avg_loss:0.430, val_acc:0.845]
Epoch [27/120    avg_loss:0.408, val_acc:0.807]
Epoch [28/120    avg_loss:0.411, val_acc:0.844]
Epoch [29/120    avg_loss:0.385, val_acc:0.853]
Epoch [30/120    avg_loss:0.323, val_acc:0.866]
Epoch [31/120    avg_loss:0.332, val_acc:0.859]
Epoch [32/120    avg_loss:0.360, val_acc:0.847]
Epoch [33/120    avg_loss:0.251, val_acc:0.859]
Epoch [34/120    avg_loss:0.255, val_acc:0.853]
Epoch [35/120    avg_loss:0.249, val_acc:0.870]
Epoch [36/120    avg_loss:0.241, val_acc:0.853]
Epoch [37/120    avg_loss:0.245, val_acc:0.876]
Epoch [38/120    avg_loss:0.208, val_acc:0.876]
Epoch [39/120    avg_loss:0.248, val_acc:0.872]
Epoch [40/120    avg_loss:0.283, val_acc:0.858]
Epoch [41/120    avg_loss:0.209, val_acc:0.897]
Epoch [42/120    avg_loss:0.177, val_acc:0.893]
Epoch [43/120    avg_loss:0.194, val_acc:0.891]
Epoch [44/120    avg_loss:0.238, val_acc:0.892]
Epoch [45/120    avg_loss:0.174, val_acc:0.897]
Epoch [46/120    avg_loss:0.209, val_acc:0.918]
Epoch [47/120    avg_loss:0.135, val_acc:0.909]
Epoch [48/120    avg_loss:0.147, val_acc:0.919]
Epoch [49/120    avg_loss:0.134, val_acc:0.914]
Epoch [50/120    avg_loss:0.113, val_acc:0.908]
Epoch [51/120    avg_loss:0.132, val_acc:0.914]
Epoch [52/120    avg_loss:0.120, val_acc:0.909]
Epoch [53/120    avg_loss:0.133, val_acc:0.903]
Epoch [54/120    avg_loss:0.117, val_acc:0.903]
Epoch [55/120    avg_loss:0.122, val_acc:0.911]
Epoch [56/120    avg_loss:0.111, val_acc:0.932]
Epoch [57/120    avg_loss:0.102, val_acc:0.934]
Epoch [58/120    avg_loss:0.106, val_acc:0.928]
Epoch [59/120    avg_loss:0.110, val_acc:0.922]
Epoch [60/120    avg_loss:0.094, val_acc:0.918]
Epoch [61/120    avg_loss:0.119, val_acc:0.922]
Epoch [62/120    avg_loss:0.100, val_acc:0.931]
Epoch [63/120    avg_loss:0.085, val_acc:0.933]
Epoch [64/120    avg_loss:0.089, val_acc:0.934]
Epoch [65/120    avg_loss:0.106, val_acc:0.924]
Epoch [66/120    avg_loss:0.099, val_acc:0.930]
Epoch [67/120    avg_loss:0.088, val_acc:0.940]
Epoch [68/120    avg_loss:0.082, val_acc:0.950]
Epoch [69/120    avg_loss:0.064, val_acc:0.938]
Epoch [70/120    avg_loss:0.085, val_acc:0.924]
Epoch [71/120    avg_loss:0.087, val_acc:0.925]
Epoch [72/120    avg_loss:0.078, val_acc:0.928]
Epoch [73/120    avg_loss:0.069, val_acc:0.931]
Epoch [74/120    avg_loss:0.054, val_acc:0.939]
Epoch [75/120    avg_loss:0.058, val_acc:0.936]
Epoch [76/120    avg_loss:0.050, val_acc:0.955]
Epoch [77/120    avg_loss:0.046, val_acc:0.950]
Epoch [78/120    avg_loss:0.095, val_acc:0.907]
Epoch [79/120    avg_loss:0.078, val_acc:0.932]
Epoch [80/120    avg_loss:0.063, val_acc:0.943]
Epoch [81/120    avg_loss:0.066, val_acc:0.939]
Epoch [82/120    avg_loss:0.055, val_acc:0.939]
Epoch [83/120    avg_loss:0.048, val_acc:0.943]
Epoch [84/120    avg_loss:0.069, val_acc:0.932]
Epoch [85/120    avg_loss:0.089, val_acc:0.920]
Epoch [86/120    avg_loss:0.079, val_acc:0.944]
Epoch [87/120    avg_loss:0.088, val_acc:0.940]
Epoch [88/120    avg_loss:0.097, val_acc:0.933]
Epoch [89/120    avg_loss:0.057, val_acc:0.948]
Epoch [90/120    avg_loss:0.054, val_acc:0.945]
Epoch [91/120    avg_loss:0.034, val_acc:0.948]
Epoch [92/120    avg_loss:0.041, val_acc:0.952]
Epoch [93/120    avg_loss:0.040, val_acc:0.955]
Epoch [94/120    avg_loss:0.037, val_acc:0.956]
Epoch [95/120    avg_loss:0.034, val_acc:0.956]
Epoch [96/120    avg_loss:0.038, val_acc:0.957]
Epoch [97/120    avg_loss:0.036, val_acc:0.957]
Epoch [98/120    avg_loss:0.034, val_acc:0.956]
Epoch [99/120    avg_loss:0.035, val_acc:0.957]
Epoch [100/120    avg_loss:0.029, val_acc:0.958]
Epoch [101/120    avg_loss:0.030, val_acc:0.957]
Epoch [102/120    avg_loss:0.029, val_acc:0.960]
Epoch [103/120    avg_loss:0.031, val_acc:0.960]
Epoch [104/120    avg_loss:0.029, val_acc:0.961]
Epoch [105/120    avg_loss:0.031, val_acc:0.958]
Epoch [106/120    avg_loss:0.034, val_acc:0.959]
Epoch [107/120    avg_loss:0.030, val_acc:0.958]
Epoch [108/120    avg_loss:0.026, val_acc:0.959]
Epoch [109/120    avg_loss:0.027, val_acc:0.960]
Epoch [110/120    avg_loss:0.029, val_acc:0.957]
Epoch [111/120    avg_loss:0.031, val_acc:0.959]
Epoch [112/120    avg_loss:0.029, val_acc:0.957]
Epoch [113/120    avg_loss:0.024, val_acc:0.961]
Epoch [114/120    avg_loss:0.023, val_acc:0.963]
Epoch [115/120    avg_loss:0.026, val_acc:0.964]
Epoch [116/120    avg_loss:0.028, val_acc:0.958]
Epoch [117/120    avg_loss:0.026, val_acc:0.961]
Epoch [118/120    avg_loss:0.027, val_acc:0.960]
Epoch [119/120    avg_loss:0.026, val_acc:0.961]
Epoch [120/120    avg_loss:0.026, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1213    8    0    0   14    0    0    0    5   43    0    0
     0    2    0]
 [   0    0    0  708    3   24    0    0    0    6    0    0    6    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  413    0    6    0    2    0    0    0    0
    14    0    0]
 [   0    0    0    0    0    0  648    0    0    0    0    5    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    2    0    0   10    0    0    3    0
     0    0    0]
 [   0    0   36   86    0    0    0    0    0    0  741    4    2    0
     0    6    0]
 [   0    0   19    0    0    0    4    0    0    0    6 2102   71    2
     6    0    0]
 [   0    0    6   28    1    0    0    0    0    0    6    6  475    0
     0    9    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    2    0    1    0    0    0
  1126    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
   136  210    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
93.4308943089431

F1 scores:
[       nan 0.975      0.94802657 0.89563567 0.98834499 0.93650794
 0.97811321 0.89285714 0.99767981 0.54054054 0.90586797 0.96201373
 0.86916743 0.99462366 0.92865979 0.73170732 0.9704142 ]

Kappa:
0.9251120165009025
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6c53894ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.774, val_acc:0.188]
Epoch [2/120    avg_loss:2.569, val_acc:0.393]
Epoch [3/120    avg_loss:2.381, val_acc:0.456]
Epoch [4/120    avg_loss:2.255, val_acc:0.492]
Epoch [5/120    avg_loss:2.128, val_acc:0.509]
Epoch [6/120    avg_loss:2.000, val_acc:0.555]
Epoch [7/120    avg_loss:1.956, val_acc:0.603]
Epoch [8/120    avg_loss:1.810, val_acc:0.617]
Epoch [9/120    avg_loss:1.726, val_acc:0.654]
Epoch [10/120    avg_loss:1.580, val_acc:0.677]
Epoch [11/120    avg_loss:1.463, val_acc:0.669]
Epoch [12/120    avg_loss:1.301, val_acc:0.683]
Epoch [13/120    avg_loss:1.228, val_acc:0.689]
Epoch [14/120    avg_loss:1.053, val_acc:0.717]
Epoch [15/120    avg_loss:1.011, val_acc:0.734]
Epoch [16/120    avg_loss:0.912, val_acc:0.742]
Epoch [17/120    avg_loss:0.797, val_acc:0.767]
Epoch [18/120    avg_loss:0.769, val_acc:0.751]
Epoch [19/120    avg_loss:0.703, val_acc:0.763]
Epoch [20/120    avg_loss:0.645, val_acc:0.797]
Epoch [21/120    avg_loss:0.682, val_acc:0.806]
Epoch [22/120    avg_loss:0.611, val_acc:0.762]
Epoch [23/120    avg_loss:0.557, val_acc:0.750]
Epoch [24/120    avg_loss:0.533, val_acc:0.819]
Epoch [25/120    avg_loss:0.518, val_acc:0.800]
Epoch [26/120    avg_loss:0.521, val_acc:0.842]
Epoch [27/120    avg_loss:0.493, val_acc:0.853]
Epoch [28/120    avg_loss:0.395, val_acc:0.850]
Epoch [29/120    avg_loss:0.406, val_acc:0.853]
Epoch [30/120    avg_loss:0.343, val_acc:0.847]
Epoch [31/120    avg_loss:0.338, val_acc:0.867]
Epoch [32/120    avg_loss:0.303, val_acc:0.903]
Epoch [33/120    avg_loss:0.400, val_acc:0.883]
Epoch [34/120    avg_loss:0.310, val_acc:0.885]
Epoch [35/120    avg_loss:0.272, val_acc:0.884]
Epoch [36/120    avg_loss:0.300, val_acc:0.873]
Epoch [37/120    avg_loss:0.275, val_acc:0.882]
Epoch [38/120    avg_loss:0.233, val_acc:0.912]
Epoch [39/120    avg_loss:0.246, val_acc:0.905]
Epoch [40/120    avg_loss:0.210, val_acc:0.917]
Epoch [41/120    avg_loss:0.210, val_acc:0.902]
Epoch [42/120    avg_loss:0.217, val_acc:0.912]
Epoch [43/120    avg_loss:0.180, val_acc:0.916]
Epoch [44/120    avg_loss:0.190, val_acc:0.919]
Epoch [45/120    avg_loss:0.186, val_acc:0.935]
Epoch [46/120    avg_loss:0.144, val_acc:0.929]
Epoch [47/120    avg_loss:0.200, val_acc:0.897]
Epoch [48/120    avg_loss:0.156, val_acc:0.946]
Epoch [49/120    avg_loss:0.161, val_acc:0.941]
Epoch [50/120    avg_loss:0.147, val_acc:0.935]
Epoch [51/120    avg_loss:0.141, val_acc:0.942]
Epoch [52/120    avg_loss:0.134, val_acc:0.942]
Epoch [53/120    avg_loss:0.122, val_acc:0.938]
Epoch [54/120    avg_loss:0.109, val_acc:0.948]
Epoch [55/120    avg_loss:0.103, val_acc:0.950]
Epoch [56/120    avg_loss:0.101, val_acc:0.950]
Epoch [57/120    avg_loss:0.093, val_acc:0.944]
Epoch [58/120    avg_loss:0.091, val_acc:0.942]
Epoch [59/120    avg_loss:0.082, val_acc:0.946]
Epoch [60/120    avg_loss:0.096, val_acc:0.955]
Epoch [61/120    avg_loss:0.105, val_acc:0.941]
Epoch [62/120    avg_loss:0.097, val_acc:0.948]
Epoch [63/120    avg_loss:0.135, val_acc:0.940]
Epoch [64/120    avg_loss:0.102, val_acc:0.943]
Epoch [65/120    avg_loss:0.097, val_acc:0.940]
Epoch [66/120    avg_loss:0.078, val_acc:0.955]
Epoch [67/120    avg_loss:0.060, val_acc:0.962]
Epoch [68/120    avg_loss:0.059, val_acc:0.958]
Epoch [69/120    avg_loss:0.062, val_acc:0.965]
Epoch [70/120    avg_loss:0.057, val_acc:0.951]
Epoch [71/120    avg_loss:0.076, val_acc:0.959]
Epoch [72/120    avg_loss:0.059, val_acc:0.958]
Epoch [73/120    avg_loss:0.055, val_acc:0.951]
Epoch [74/120    avg_loss:0.047, val_acc:0.965]
Epoch [75/120    avg_loss:0.057, val_acc:0.959]
Epoch [76/120    avg_loss:0.069, val_acc:0.964]
Epoch [77/120    avg_loss:0.046, val_acc:0.963]
Epoch [78/120    avg_loss:0.042, val_acc:0.968]
Epoch [79/120    avg_loss:0.041, val_acc:0.970]
Epoch [80/120    avg_loss:0.042, val_acc:0.968]
Epoch [81/120    avg_loss:0.046, val_acc:0.968]
Epoch [82/120    avg_loss:0.052, val_acc:0.961]
Epoch [83/120    avg_loss:0.058, val_acc:0.968]
Epoch [84/120    avg_loss:0.058, val_acc:0.965]
Epoch [85/120    avg_loss:0.054, val_acc:0.962]
Epoch [86/120    avg_loss:0.036, val_acc:0.967]
Epoch [87/120    avg_loss:0.035, val_acc:0.968]
Epoch [88/120    avg_loss:0.031, val_acc:0.974]
Epoch [89/120    avg_loss:0.034, val_acc:0.958]
Epoch [90/120    avg_loss:0.047, val_acc:0.963]
Epoch [91/120    avg_loss:0.032, val_acc:0.965]
Epoch [92/120    avg_loss:0.032, val_acc:0.973]
Epoch [93/120    avg_loss:0.032, val_acc:0.973]
Epoch [94/120    avg_loss:0.048, val_acc:0.950]
Epoch [95/120    avg_loss:0.043, val_acc:0.943]
Epoch [96/120    avg_loss:0.047, val_acc:0.965]
Epoch [97/120    avg_loss:0.033, val_acc:0.964]
Epoch [98/120    avg_loss:0.044, val_acc:0.973]
Epoch [99/120    avg_loss:0.037, val_acc:0.955]
Epoch [100/120    avg_loss:0.068, val_acc:0.959]
Epoch [101/120    avg_loss:0.122, val_acc:0.936]
Epoch [102/120    avg_loss:0.066, val_acc:0.960]
Epoch [103/120    avg_loss:0.048, val_acc:0.963]
Epoch [104/120    avg_loss:0.032, val_acc:0.963]
Epoch [105/120    avg_loss:0.028, val_acc:0.967]
Epoch [106/120    avg_loss:0.034, val_acc:0.967]
Epoch [107/120    avg_loss:0.027, val_acc:0.968]
Epoch [108/120    avg_loss:0.024, val_acc:0.969]
Epoch [109/120    avg_loss:0.026, val_acc:0.970]
Epoch [110/120    avg_loss:0.025, val_acc:0.971]
Epoch [111/120    avg_loss:0.026, val_acc:0.973]
Epoch [112/120    avg_loss:0.025, val_acc:0.972]
Epoch [113/120    avg_loss:0.026, val_acc:0.974]
Epoch [114/120    avg_loss:0.023, val_acc:0.973]
Epoch [115/120    avg_loss:0.023, val_acc:0.973]
Epoch [116/120    avg_loss:0.025, val_acc:0.972]
Epoch [117/120    avg_loss:0.025, val_acc:0.972]
Epoch [118/120    avg_loss:0.024, val_acc:0.969]
Epoch [119/120    avg_loss:0.021, val_acc:0.973]
Epoch [120/120    avg_loss:0.024, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1232    2    0    0    3    0    0    1   15   16    8    0
     0    8    0]
 [   0    0    1  732    0    4    0    0    0    7    0    0    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  419    0    0    0    0    0    0    0    0
    16    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    3    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   17   89    0    6    1    0    0    0  757    4    0    0
     0    1    0]
 [   0    0   11    0    0    0    2    0    0    0   20 2172    4    1
     0    0    0]
 [   0    0    0   21    9    6    0    0    0    0    2    0  494    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    1    0    2    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    1
   107  238    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
95.59891598915989

F1 scores:
[       nan 0.975      0.96779262 0.92017599 0.97931034 0.96100917
 0.98937785 1.         0.99883856 0.76190476 0.90496115 0.9861521
 0.94455067 0.99462366 0.94578816 0.8013468  0.97005988]

Kappa:
0.9497984371093449
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5c0d2b2b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.736, val_acc:0.225]
Epoch [2/120    avg_loss:2.515, val_acc:0.449]
Epoch [3/120    avg_loss:2.351, val_acc:0.504]
Epoch [4/120    avg_loss:2.218, val_acc:0.503]
Epoch [5/120    avg_loss:2.075, val_acc:0.521]
Epoch [6/120    avg_loss:1.999, val_acc:0.538]
Epoch [7/120    avg_loss:1.891, val_acc:0.577]
Epoch [8/120    avg_loss:1.769, val_acc:0.608]
Epoch [9/120    avg_loss:1.677, val_acc:0.609]
Epoch [10/120    avg_loss:1.656, val_acc:0.628]
Epoch [11/120    avg_loss:1.530, val_acc:0.599]
Epoch [12/120    avg_loss:1.456, val_acc:0.654]
Epoch [13/120    avg_loss:1.322, val_acc:0.679]
Epoch [14/120    avg_loss:1.246, val_acc:0.702]
Epoch [15/120    avg_loss:1.114, val_acc:0.702]
Epoch [16/120    avg_loss:1.064, val_acc:0.720]
Epoch [17/120    avg_loss:0.944, val_acc:0.772]
Epoch [18/120    avg_loss:0.962, val_acc:0.766]
Epoch [19/120    avg_loss:0.803, val_acc:0.756]
Epoch [20/120    avg_loss:0.793, val_acc:0.785]
Epoch [21/120    avg_loss:0.731, val_acc:0.718]
Epoch [22/120    avg_loss:0.667, val_acc:0.814]
Epoch [23/120    avg_loss:0.576, val_acc:0.815]
Epoch [24/120    avg_loss:0.524, val_acc:0.825]
Epoch [25/120    avg_loss:0.526, val_acc:0.829]
Epoch [26/120    avg_loss:0.476, val_acc:0.792]
Epoch [27/120    avg_loss:0.424, val_acc:0.858]
Epoch [28/120    avg_loss:0.415, val_acc:0.858]
Epoch [29/120    avg_loss:0.428, val_acc:0.857]
Epoch [30/120    avg_loss:0.386, val_acc:0.838]
Epoch [31/120    avg_loss:0.337, val_acc:0.881]
Epoch [32/120    avg_loss:0.303, val_acc:0.882]
Epoch [33/120    avg_loss:0.271, val_acc:0.886]
Epoch [34/120    avg_loss:0.314, val_acc:0.869]
Epoch [35/120    avg_loss:0.264, val_acc:0.888]
Epoch [36/120    avg_loss:0.292, val_acc:0.859]
Epoch [37/120    avg_loss:0.262, val_acc:0.873]
Epoch [38/120    avg_loss:0.253, val_acc:0.865]
Epoch [39/120    avg_loss:0.269, val_acc:0.898]
Epoch [40/120    avg_loss:0.229, val_acc:0.871]
Epoch [41/120    avg_loss:0.291, val_acc:0.840]
Epoch [42/120    avg_loss:0.282, val_acc:0.869]
Epoch [43/120    avg_loss:0.240, val_acc:0.898]
Epoch [44/120    avg_loss:0.188, val_acc:0.908]
Epoch [45/120    avg_loss:0.204, val_acc:0.906]
Epoch [46/120    avg_loss:0.179, val_acc:0.904]
Epoch [47/120    avg_loss:0.142, val_acc:0.922]
Epoch [48/120    avg_loss:0.154, val_acc:0.920]
Epoch [49/120    avg_loss:0.147, val_acc:0.903]
Epoch [50/120    avg_loss:0.122, val_acc:0.927]
Epoch [51/120    avg_loss:0.124, val_acc:0.931]
Epoch [52/120    avg_loss:0.380, val_acc:0.860]
Epoch [53/120    avg_loss:0.174, val_acc:0.922]
Epoch [54/120    avg_loss:0.093, val_acc:0.930]
Epoch [55/120    avg_loss:0.112, val_acc:0.935]
Epoch [56/120    avg_loss:0.140, val_acc:0.888]
Epoch [57/120    avg_loss:0.255, val_acc:0.859]
Epoch [58/120    avg_loss:0.337, val_acc:0.910]
Epoch [59/120    avg_loss:0.123, val_acc:0.934]
Epoch [60/120    avg_loss:0.109, val_acc:0.930]
Epoch [61/120    avg_loss:0.096, val_acc:0.898]
Epoch [62/120    avg_loss:0.098, val_acc:0.932]
Epoch [63/120    avg_loss:0.090, val_acc:0.954]
Epoch [64/120    avg_loss:0.073, val_acc:0.936]
Epoch [65/120    avg_loss:0.064, val_acc:0.952]
Epoch [66/120    avg_loss:0.058, val_acc:0.951]
Epoch [67/120    avg_loss:0.084, val_acc:0.939]
Epoch [68/120    avg_loss:0.082, val_acc:0.944]
Epoch [69/120    avg_loss:0.051, val_acc:0.934]
Epoch [70/120    avg_loss:0.060, val_acc:0.950]
Epoch [71/120    avg_loss:0.099, val_acc:0.922]
Epoch [72/120    avg_loss:0.088, val_acc:0.946]
Epoch [73/120    avg_loss:0.075, val_acc:0.949]
Epoch [74/120    avg_loss:0.062, val_acc:0.960]
Epoch [75/120    avg_loss:0.051, val_acc:0.953]
Epoch [76/120    avg_loss:0.046, val_acc:0.954]
Epoch [77/120    avg_loss:0.045, val_acc:0.954]
Epoch [78/120    avg_loss:0.047, val_acc:0.965]
Epoch [79/120    avg_loss:0.048, val_acc:0.967]
Epoch [80/120    avg_loss:0.065, val_acc:0.944]
Epoch [81/120    avg_loss:0.071, val_acc:0.963]
Epoch [82/120    avg_loss:0.077, val_acc:0.951]
Epoch [83/120    avg_loss:0.052, val_acc:0.960]
Epoch [84/120    avg_loss:0.042, val_acc:0.963]
Epoch [85/120    avg_loss:0.043, val_acc:0.959]
Epoch [86/120    avg_loss:0.055, val_acc:0.960]
Epoch [87/120    avg_loss:0.039, val_acc:0.950]
Epoch [88/120    avg_loss:0.071, val_acc:0.953]
Epoch [89/120    avg_loss:0.041, val_acc:0.958]
Epoch [90/120    avg_loss:0.042, val_acc:0.969]
Epoch [91/120    avg_loss:0.045, val_acc:0.962]
Epoch [92/120    avg_loss:0.030, val_acc:0.970]
Epoch [93/120    avg_loss:0.051, val_acc:0.958]
Epoch [94/120    avg_loss:0.033, val_acc:0.968]
Epoch [95/120    avg_loss:0.036, val_acc:0.972]
Epoch [96/120    avg_loss:0.083, val_acc:0.954]
Epoch [97/120    avg_loss:0.041, val_acc:0.969]
Epoch [98/120    avg_loss:0.041, val_acc:0.961]
Epoch [99/120    avg_loss:0.038, val_acc:0.958]
Epoch [100/120    avg_loss:0.036, val_acc:0.973]
Epoch [101/120    avg_loss:0.026, val_acc:0.974]
Epoch [102/120    avg_loss:0.033, val_acc:0.965]
Epoch [103/120    avg_loss:0.026, val_acc:0.965]
Epoch [104/120    avg_loss:0.033, val_acc:0.953]
Epoch [105/120    avg_loss:0.023, val_acc:0.972]
Epoch [106/120    avg_loss:0.021, val_acc:0.969]
Epoch [107/120    avg_loss:0.024, val_acc:0.979]
Epoch [108/120    avg_loss:0.021, val_acc:0.970]
Epoch [109/120    avg_loss:0.019, val_acc:0.978]
Epoch [110/120    avg_loss:0.016, val_acc:0.972]
Epoch [111/120    avg_loss:0.019, val_acc:0.978]
Epoch [112/120    avg_loss:0.020, val_acc:0.970]
Epoch [113/120    avg_loss:0.019, val_acc:0.970]
Epoch [114/120    avg_loss:0.035, val_acc:0.970]
Epoch [115/120    avg_loss:0.022, val_acc:0.978]
Epoch [116/120    avg_loss:0.024, val_acc:0.965]
Epoch [117/120    avg_loss:0.023, val_acc:0.979]
Epoch [118/120    avg_loss:0.019, val_acc:0.973]
Epoch [119/120    avg_loss:0.019, val_acc:0.981]
Epoch [120/120    avg_loss:0.023, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1195   22    0    0    0    0    0    3   14   47    4    0
     0    0    0]
 [   0    0    0  729    0    0    0    0    0   17    0    0    0    1
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    2    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    5    0    0   13    0    0    0    0
     0    0    0]
 [   0    0   33   89    0    3    0    0    0    1  745    2    0    0
     0    2    0]
 [   0    0    4    0    0    3    1    0    0    0   23 2170    3    2
     4    0    0]
 [   0    0    0   17    3    8    0    0    0    0   12   14  478    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    2    0    0    0
  1129    0    0]
 [   0    0    0    0    0    0   33    0    0    4    0    0    0    0
   108  202    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.4390243902439

F1 scores:
[       nan 0.96202532 0.94954311 0.90841121 0.99065421 0.96598639
 0.9704142  1.         0.99416569 0.44827586 0.89008363 0.97659766
 0.93359375 0.9919571  0.94595727 0.73321234 0.98224852]

Kappa:
0.9365295761573064
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fde37252b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.768, val_acc:0.216]
Epoch [2/120    avg_loss:2.568, val_acc:0.442]
Epoch [3/120    avg_loss:2.406, val_acc:0.505]
Epoch [4/120    avg_loss:2.267, val_acc:0.511]
Epoch [5/120    avg_loss:2.130, val_acc:0.534]
Epoch [6/120    avg_loss:2.026, val_acc:0.581]
Epoch [7/120    avg_loss:1.862, val_acc:0.614]
Epoch [8/120    avg_loss:1.756, val_acc:0.616]
Epoch [9/120    avg_loss:1.688, val_acc:0.657]
Epoch [10/120    avg_loss:1.528, val_acc:0.675]
Epoch [11/120    avg_loss:1.417, val_acc:0.719]
Epoch [12/120    avg_loss:1.295, val_acc:0.751]
Epoch [13/120    avg_loss:1.162, val_acc:0.748]
Epoch [14/120    avg_loss:1.054, val_acc:0.758]
Epoch [15/120    avg_loss:0.971, val_acc:0.784]
Epoch [16/120    avg_loss:0.828, val_acc:0.778]
Epoch [17/120    avg_loss:0.839, val_acc:0.753]
Epoch [18/120    avg_loss:0.788, val_acc:0.772]
Epoch [19/120    avg_loss:0.714, val_acc:0.815]
Epoch [20/120    avg_loss:0.654, val_acc:0.812]
Epoch [21/120    avg_loss:0.585, val_acc:0.809]
Epoch [22/120    avg_loss:0.560, val_acc:0.827]
Epoch [23/120    avg_loss:0.530, val_acc:0.859]
Epoch [24/120    avg_loss:0.505, val_acc:0.835]
Epoch [25/120    avg_loss:0.448, val_acc:0.877]
Epoch [26/120    avg_loss:0.430, val_acc:0.875]
Epoch [27/120    avg_loss:0.551, val_acc:0.845]
Epoch [28/120    avg_loss:0.444, val_acc:0.843]
Epoch [29/120    avg_loss:0.370, val_acc:0.884]
Epoch [30/120    avg_loss:0.332, val_acc:0.895]
Epoch [31/120    avg_loss:0.311, val_acc:0.912]
Epoch [32/120    avg_loss:0.315, val_acc:0.889]
Epoch [33/120    avg_loss:0.278, val_acc:0.881]
Epoch [34/120    avg_loss:0.257, val_acc:0.919]
Epoch [35/120    avg_loss:0.232, val_acc:0.942]
Epoch [36/120    avg_loss:0.233, val_acc:0.905]
Epoch [37/120    avg_loss:0.236, val_acc:0.926]
Epoch [38/120    avg_loss:0.207, val_acc:0.908]
Epoch [39/120    avg_loss:0.221, val_acc:0.958]
Epoch [40/120    avg_loss:0.175, val_acc:0.942]
Epoch [41/120    avg_loss:0.170, val_acc:0.939]
Epoch [42/120    avg_loss:0.185, val_acc:0.902]
Epoch [43/120    avg_loss:0.344, val_acc:0.932]
Epoch [44/120    avg_loss:0.234, val_acc:0.891]
Epoch [45/120    avg_loss:0.239, val_acc:0.902]
Epoch [46/120    avg_loss:0.219, val_acc:0.920]
Epoch [47/120    avg_loss:0.184, val_acc:0.943]
Epoch [48/120    avg_loss:0.137, val_acc:0.931]
Epoch [49/120    avg_loss:0.146, val_acc:0.906]
Epoch [50/120    avg_loss:0.126, val_acc:0.943]
Epoch [51/120    avg_loss:0.131, val_acc:0.943]
Epoch [52/120    avg_loss:0.124, val_acc:0.952]
Epoch [53/120    avg_loss:0.082, val_acc:0.970]
Epoch [54/120    avg_loss:0.068, val_acc:0.974]
Epoch [55/120    avg_loss:0.072, val_acc:0.973]
Epoch [56/120    avg_loss:0.072, val_acc:0.975]
Epoch [57/120    avg_loss:0.066, val_acc:0.975]
Epoch [58/120    avg_loss:0.070, val_acc:0.975]
Epoch [59/120    avg_loss:0.059, val_acc:0.975]
Epoch [60/120    avg_loss:0.060, val_acc:0.975]
Epoch [61/120    avg_loss:0.061, val_acc:0.976]
Epoch [62/120    avg_loss:0.061, val_acc:0.974]
Epoch [63/120    avg_loss:0.068, val_acc:0.976]
Epoch [64/120    avg_loss:0.070, val_acc:0.977]
Epoch [65/120    avg_loss:0.063, val_acc:0.977]
Epoch [66/120    avg_loss:0.060, val_acc:0.976]
Epoch [67/120    avg_loss:0.063, val_acc:0.980]
Epoch [68/120    avg_loss:0.053, val_acc:0.977]
Epoch [69/120    avg_loss:0.056, val_acc:0.978]
Epoch [70/120    avg_loss:0.056, val_acc:0.981]
Epoch [71/120    avg_loss:0.051, val_acc:0.977]
Epoch [72/120    avg_loss:0.059, val_acc:0.977]
Epoch [73/120    avg_loss:0.051, val_acc:0.980]
Epoch [74/120    avg_loss:0.055, val_acc:0.980]
Epoch [75/120    avg_loss:0.054, val_acc:0.980]
Epoch [76/120    avg_loss:0.056, val_acc:0.982]
Epoch [77/120    avg_loss:0.052, val_acc:0.978]
Epoch [78/120    avg_loss:0.051, val_acc:0.977]
Epoch [79/120    avg_loss:0.052, val_acc:0.978]
Epoch [80/120    avg_loss:0.055, val_acc:0.982]
Epoch [81/120    avg_loss:0.050, val_acc:0.973]
Epoch [82/120    avg_loss:0.045, val_acc:0.976]
Epoch [83/120    avg_loss:0.050, val_acc:0.974]
Epoch [84/120    avg_loss:0.050, val_acc:0.977]
Epoch [85/120    avg_loss:0.060, val_acc:0.980]
Epoch [86/120    avg_loss:0.036, val_acc:0.977]
Epoch [87/120    avg_loss:0.046, val_acc:0.980]
Epoch [88/120    avg_loss:0.043, val_acc:0.980]
Epoch [89/120    avg_loss:0.040, val_acc:0.983]
Epoch [90/120    avg_loss:0.050, val_acc:0.985]
Epoch [91/120    avg_loss:0.042, val_acc:0.980]
Epoch [92/120    avg_loss:0.048, val_acc:0.978]
Epoch [93/120    avg_loss:0.050, val_acc:0.974]
Epoch [94/120    avg_loss:0.055, val_acc:0.977]
Epoch [95/120    avg_loss:0.051, val_acc:0.981]
Epoch [96/120    avg_loss:0.053, val_acc:0.975]
Epoch [97/120    avg_loss:0.047, val_acc:0.977]
Epoch [98/120    avg_loss:0.046, val_acc:0.976]
Epoch [99/120    avg_loss:0.044, val_acc:0.976]
Epoch [100/120    avg_loss:0.040, val_acc:0.980]
Epoch [101/120    avg_loss:0.044, val_acc:0.977]
Epoch [102/120    avg_loss:0.049, val_acc:0.975]
Epoch [103/120    avg_loss:0.050, val_acc:0.978]
Epoch [104/120    avg_loss:0.044, val_acc:0.978]
Epoch [105/120    avg_loss:0.042, val_acc:0.980]
Epoch [106/120    avg_loss:0.041, val_acc:0.980]
Epoch [107/120    avg_loss:0.043, val_acc:0.978]
Epoch [108/120    avg_loss:0.048, val_acc:0.978]
Epoch [109/120    avg_loss:0.034, val_acc:0.977]
Epoch [110/120    avg_loss:0.046, val_acc:0.978]
Epoch [111/120    avg_loss:0.047, val_acc:0.978]
Epoch [112/120    avg_loss:0.036, val_acc:0.980]
Epoch [113/120    avg_loss:0.045, val_acc:0.980]
Epoch [114/120    avg_loss:0.048, val_acc:0.980]
Epoch [115/120    avg_loss:0.034, val_acc:0.981]
Epoch [116/120    avg_loss:0.040, val_acc:0.980]
Epoch [117/120    avg_loss:0.047, val_acc:0.978]
Epoch [118/120    avg_loss:0.040, val_acc:0.981]
Epoch [119/120    avg_loss:0.033, val_acc:0.980]
Epoch [120/120    avg_loss:0.039, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1218    2    0    0    3    0    0    1   12   34    9    0
     0    3    3]
 [   0    0    8  711    0    5    0    0    0    6    0    0   10    7
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0  422    0    1    0    1    0    0    0    0
    10    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    6    0    0    0    0    0    0  424    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   36   90    0    1    1    0    0    0  742    4    0    0
     0    1    0]
 [   0    0    8    0    0    0   13    0    2    0    5 2180    1    1
     0    0    0]
 [   0    0    0   45    2    0    0    0    0    0    5    4  474    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   25    0    0    5    0    0    0    0
    92  225    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.81842818428184

F1 scores:
[       nan 0.90697674 0.95342466 0.88986233 0.99061033 0.97798378
 0.96826568 0.98039216 0.98949825 0.70833333 0.90322581 0.9835326
 0.91949564 0.97883598 0.95582667 0.77989601 0.95348837]

Kappa:
0.9408693344309248
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f012ae76a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.781, val_acc:0.183]
Epoch [2/120    avg_loss:2.579, val_acc:0.297]
Epoch [3/120    avg_loss:2.428, val_acc:0.390]
Epoch [4/120    avg_loss:2.307, val_acc:0.490]
Epoch [5/120    avg_loss:2.148, val_acc:0.531]
Epoch [6/120    avg_loss:1.991, val_acc:0.548]
Epoch [7/120    avg_loss:1.889, val_acc:0.560]
Epoch [8/120    avg_loss:1.762, val_acc:0.568]
Epoch [9/120    avg_loss:1.658, val_acc:0.607]
Epoch [10/120    avg_loss:1.564, val_acc:0.646]
Epoch [11/120    avg_loss:1.423, val_acc:0.686]
Epoch [12/120    avg_loss:1.302, val_acc:0.681]
Epoch [13/120    avg_loss:1.223, val_acc:0.695]
Epoch [14/120    avg_loss:1.115, val_acc:0.713]
Epoch [15/120    avg_loss:1.050, val_acc:0.729]
Epoch [16/120    avg_loss:0.989, val_acc:0.714]
Epoch [17/120    avg_loss:0.873, val_acc:0.751]
Epoch [18/120    avg_loss:0.794, val_acc:0.766]
Epoch [19/120    avg_loss:0.753, val_acc:0.756]
Epoch [20/120    avg_loss:0.691, val_acc:0.739]
Epoch [21/120    avg_loss:0.715, val_acc:0.781]
Epoch [22/120    avg_loss:0.623, val_acc:0.789]
Epoch [23/120    avg_loss:0.583, val_acc:0.783]
Epoch [24/120    avg_loss:0.561, val_acc:0.800]
Epoch [25/120    avg_loss:0.484, val_acc:0.838]
Epoch [26/120    avg_loss:0.475, val_acc:0.818]
Epoch [27/120    avg_loss:0.478, val_acc:0.828]
Epoch [28/120    avg_loss:0.416, val_acc:0.848]
Epoch [29/120    avg_loss:0.394, val_acc:0.801]
Epoch [30/120    avg_loss:0.373, val_acc:0.859]
Epoch [31/120    avg_loss:0.374, val_acc:0.879]
Epoch [32/120    avg_loss:0.362, val_acc:0.871]
Epoch [33/120    avg_loss:0.333, val_acc:0.873]
Epoch [34/120    avg_loss:0.269, val_acc:0.860]
Epoch [35/120    avg_loss:0.265, val_acc:0.873]
Epoch [36/120    avg_loss:0.284, val_acc:0.887]
Epoch [37/120    avg_loss:0.284, val_acc:0.862]
Epoch [38/120    avg_loss:0.247, val_acc:0.888]
Epoch [39/120    avg_loss:0.237, val_acc:0.922]
Epoch [40/120    avg_loss:0.215, val_acc:0.912]
Epoch [41/120    avg_loss:0.176, val_acc:0.896]
Epoch [42/120    avg_loss:0.216, val_acc:0.906]
Epoch [43/120    avg_loss:0.196, val_acc:0.917]
Epoch [44/120    avg_loss:0.201, val_acc:0.908]
Epoch [45/120    avg_loss:0.270, val_acc:0.905]
Epoch [46/120    avg_loss:0.201, val_acc:0.931]
Epoch [47/120    avg_loss:0.143, val_acc:0.926]
Epoch [48/120    avg_loss:0.168, val_acc:0.926]
Epoch [49/120    avg_loss:0.135, val_acc:0.931]
Epoch [50/120    avg_loss:0.167, val_acc:0.938]
Epoch [51/120    avg_loss:0.206, val_acc:0.890]
Epoch [52/120    avg_loss:0.197, val_acc:0.873]
Epoch [53/120    avg_loss:0.235, val_acc:0.905]
Epoch [54/120    avg_loss:0.258, val_acc:0.902]
Epoch [55/120    avg_loss:0.173, val_acc:0.920]
Epoch [56/120    avg_loss:0.150, val_acc:0.933]
Epoch [57/120    avg_loss:0.115, val_acc:0.943]
Epoch [58/120    avg_loss:0.133, val_acc:0.917]
Epoch [59/120    avg_loss:0.110, val_acc:0.933]
Epoch [60/120    avg_loss:0.114, val_acc:0.956]
Epoch [61/120    avg_loss:0.095, val_acc:0.949]
Epoch [62/120    avg_loss:0.091, val_acc:0.953]
Epoch [63/120    avg_loss:0.084, val_acc:0.952]
Epoch [64/120    avg_loss:0.084, val_acc:0.944]
Epoch [65/120    avg_loss:0.092, val_acc:0.943]
Epoch [66/120    avg_loss:0.115, val_acc:0.944]
Epoch [67/120    avg_loss:0.146, val_acc:0.942]
Epoch [68/120    avg_loss:0.110, val_acc:0.935]
Epoch [69/120    avg_loss:0.101, val_acc:0.943]
Epoch [70/120    avg_loss:0.069, val_acc:0.946]
Epoch [71/120    avg_loss:0.077, val_acc:0.948]
Epoch [72/120    avg_loss:0.083, val_acc:0.956]
Epoch [73/120    avg_loss:0.102, val_acc:0.933]
Epoch [74/120    avg_loss:0.088, val_acc:0.952]
Epoch [75/120    avg_loss:0.202, val_acc:0.941]
Epoch [76/120    avg_loss:0.110, val_acc:0.948]
Epoch [77/120    avg_loss:0.076, val_acc:0.933]
Epoch [78/120    avg_loss:0.068, val_acc:0.943]
Epoch [79/120    avg_loss:0.083, val_acc:0.948]
Epoch [80/120    avg_loss:0.076, val_acc:0.955]
Epoch [81/120    avg_loss:0.083, val_acc:0.949]
Epoch [82/120    avg_loss:0.066, val_acc:0.949]
Epoch [83/120    avg_loss:0.062, val_acc:0.956]
Epoch [84/120    avg_loss:0.057, val_acc:0.964]
Epoch [85/120    avg_loss:0.046, val_acc:0.952]
Epoch [86/120    avg_loss:0.067, val_acc:0.952]
Epoch [87/120    avg_loss:0.052, val_acc:0.951]
Epoch [88/120    avg_loss:0.056, val_acc:0.958]
Epoch [89/120    avg_loss:0.083, val_acc:0.940]
Epoch [90/120    avg_loss:0.065, val_acc:0.950]
Epoch [91/120    avg_loss:0.053, val_acc:0.958]
Epoch [92/120    avg_loss:0.036, val_acc:0.969]
Epoch [93/120    avg_loss:0.049, val_acc:0.951]
Epoch [94/120    avg_loss:0.095, val_acc:0.955]
Epoch [95/120    avg_loss:0.088, val_acc:0.949]
Epoch [96/120    avg_loss:0.082, val_acc:0.956]
Epoch [97/120    avg_loss:0.065, val_acc:0.941]
Epoch [98/120    avg_loss:0.148, val_acc:0.944]
Epoch [99/120    avg_loss:0.088, val_acc:0.953]
Epoch [100/120    avg_loss:0.061, val_acc:0.948]
Epoch [101/120    avg_loss:0.075, val_acc:0.964]
Epoch [102/120    avg_loss:0.041, val_acc:0.959]
Epoch [103/120    avg_loss:0.037, val_acc:0.960]
Epoch [104/120    avg_loss:0.039, val_acc:0.965]
Epoch [105/120    avg_loss:0.028, val_acc:0.965]
Epoch [106/120    avg_loss:0.024, val_acc:0.968]
Epoch [107/120    avg_loss:0.033, val_acc:0.971]
Epoch [108/120    avg_loss:0.031, val_acc:0.973]
Epoch [109/120    avg_loss:0.028, val_acc:0.972]
Epoch [110/120    avg_loss:0.026, val_acc:0.971]
Epoch [111/120    avg_loss:0.022, val_acc:0.970]
Epoch [112/120    avg_loss:0.021, val_acc:0.971]
Epoch [113/120    avg_loss:0.020, val_acc:0.971]
Epoch [114/120    avg_loss:0.025, val_acc:0.972]
Epoch [115/120    avg_loss:0.032, val_acc:0.971]
Epoch [116/120    avg_loss:0.017, val_acc:0.971]
Epoch [117/120    avg_loss:0.024, val_acc:0.971]
Epoch [118/120    avg_loss:0.029, val_acc:0.971]
Epoch [119/120    avg_loss:0.020, val_acc:0.972]
Epoch [120/120    avg_loss:0.022, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1228    0    0    0    1    0    0    1    3   48    3    0
     0    1    0]
 [   0    0    3  722    5    2    0    0    0    7    0    0    4    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    2    0    1    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   15    0    0    2    0
     0    0    0]
 [   0    0   21   90    0    3    0    0    0    0  746    8    1    0
     1    5    0]
 [   0    0   36    0    0    0    4    0    0    0    6 2159    3    2
     0    0    0]
 [   0    0    4   26    4    2    0    0    0    0   14    7  473    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    1    0    1    2    0    0
  1125    0    0]
 [   0    0    0    0    0    0   65    0    0    3    0    0    0    0
   105  174    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.27642276422765

F1 scores:
[       nan 1.         0.95304618 0.91046658 0.97931034 0.97038724
 0.94637681 0.96153846 0.99883856 0.66666667 0.90699088 0.97318008
 0.92563601 0.98404255 0.94657131 0.65909091 0.9704142 ]

Kappa:
0.9346544489751302
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc3748a5ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.776, val_acc:0.259]
Epoch [2/120    avg_loss:2.600, val_acc:0.354]
Epoch [3/120    avg_loss:2.396, val_acc:0.483]
Epoch [4/120    avg_loss:2.227, val_acc:0.530]
Epoch [5/120    avg_loss:2.077, val_acc:0.556]
Epoch [6/120    avg_loss:1.931, val_acc:0.567]
Epoch [7/120    avg_loss:1.797, val_acc:0.604]
Epoch [8/120    avg_loss:1.639, val_acc:0.670]
Epoch [9/120    avg_loss:1.542, val_acc:0.674]
Epoch [10/120    avg_loss:1.358, val_acc:0.685]
Epoch [11/120    avg_loss:1.245, val_acc:0.681]
Epoch [12/120    avg_loss:1.165, val_acc:0.670]
Epoch [13/120    avg_loss:1.084, val_acc:0.702]
Epoch [14/120    avg_loss:0.988, val_acc:0.683]
Epoch [15/120    avg_loss:0.856, val_acc:0.752]
Epoch [16/120    avg_loss:0.809, val_acc:0.800]
Epoch [17/120    avg_loss:0.747, val_acc:0.826]
Epoch [18/120    avg_loss:0.690, val_acc:0.835]
Epoch [19/120    avg_loss:0.659, val_acc:0.815]
Epoch [20/120    avg_loss:0.565, val_acc:0.826]
Epoch [21/120    avg_loss:0.522, val_acc:0.815]
Epoch [22/120    avg_loss:0.494, val_acc:0.836]
Epoch [23/120    avg_loss:0.462, val_acc:0.881]
Epoch [24/120    avg_loss:0.430, val_acc:0.840]
Epoch [25/120    avg_loss:0.395, val_acc:0.862]
Epoch [26/120    avg_loss:0.383, val_acc:0.864]
Epoch [27/120    avg_loss:0.464, val_acc:0.859]
Epoch [28/120    avg_loss:0.362, val_acc:0.872]
Epoch [29/120    avg_loss:0.349, val_acc:0.869]
Epoch [30/120    avg_loss:0.323, val_acc:0.867]
Epoch [31/120    avg_loss:0.286, val_acc:0.898]
Epoch [32/120    avg_loss:0.280, val_acc:0.901]
Epoch [33/120    avg_loss:0.260, val_acc:0.905]
Epoch [34/120    avg_loss:0.246, val_acc:0.895]
Epoch [35/120    avg_loss:0.234, val_acc:0.912]
Epoch [36/120    avg_loss:0.190, val_acc:0.921]
Epoch [37/120    avg_loss:0.169, val_acc:0.924]
Epoch [38/120    avg_loss:0.170, val_acc:0.925]
Epoch [39/120    avg_loss:0.162, val_acc:0.922]
Epoch [40/120    avg_loss:0.147, val_acc:0.920]
Epoch [41/120    avg_loss:0.165, val_acc:0.920]
Epoch [42/120    avg_loss:0.195, val_acc:0.904]
Epoch [43/120    avg_loss:0.203, val_acc:0.913]
Epoch [44/120    avg_loss:0.169, val_acc:0.919]
Epoch [45/120    avg_loss:0.161, val_acc:0.922]
Epoch [46/120    avg_loss:0.147, val_acc:0.929]
Epoch [47/120    avg_loss:0.141, val_acc:0.904]
Epoch [48/120    avg_loss:0.160, val_acc:0.939]
Epoch [49/120    avg_loss:0.111, val_acc:0.950]
Epoch [50/120    avg_loss:0.146, val_acc:0.920]
Epoch [51/120    avg_loss:0.161, val_acc:0.945]
Epoch [52/120    avg_loss:0.153, val_acc:0.949]
Epoch [53/120    avg_loss:0.098, val_acc:0.949]
Epoch [54/120    avg_loss:0.115, val_acc:0.935]
Epoch [55/120    avg_loss:0.088, val_acc:0.943]
Epoch [56/120    avg_loss:0.109, val_acc:0.958]
Epoch [57/120    avg_loss:0.097, val_acc:0.945]
Epoch [58/120    avg_loss:0.089, val_acc:0.948]
Epoch [59/120    avg_loss:0.093, val_acc:0.940]
Epoch [60/120    avg_loss:0.087, val_acc:0.948]
Epoch [61/120    avg_loss:0.083, val_acc:0.941]
Epoch [62/120    avg_loss:0.086, val_acc:0.954]
Epoch [63/120    avg_loss:0.114, val_acc:0.940]
Epoch [64/120    avg_loss:0.072, val_acc:0.938]
Epoch [65/120    avg_loss:0.071, val_acc:0.951]
Epoch [66/120    avg_loss:0.081, val_acc:0.942]
Epoch [67/120    avg_loss:0.116, val_acc:0.941]
Epoch [68/120    avg_loss:0.071, val_acc:0.964]
Epoch [69/120    avg_loss:0.053, val_acc:0.960]
Epoch [70/120    avg_loss:0.061, val_acc:0.951]
Epoch [71/120    avg_loss:0.056, val_acc:0.942]
Epoch [72/120    avg_loss:0.083, val_acc:0.956]
Epoch [73/120    avg_loss:0.073, val_acc:0.940]
Epoch [74/120    avg_loss:0.067, val_acc:0.963]
Epoch [75/120    avg_loss:0.044, val_acc:0.956]
Epoch [76/120    avg_loss:0.067, val_acc:0.954]
Epoch [77/120    avg_loss:0.054, val_acc:0.945]
Epoch [78/120    avg_loss:0.061, val_acc:0.956]
Epoch [79/120    avg_loss:0.050, val_acc:0.958]
Epoch [80/120    avg_loss:0.049, val_acc:0.958]
Epoch [81/120    avg_loss:0.052, val_acc:0.961]
Epoch [82/120    avg_loss:0.033, val_acc:0.965]
Epoch [83/120    avg_loss:0.029, val_acc:0.972]
Epoch [84/120    avg_loss:0.030, val_acc:0.970]
Epoch [85/120    avg_loss:0.024, val_acc:0.971]
Epoch [86/120    avg_loss:0.026, val_acc:0.972]
Epoch [87/120    avg_loss:0.022, val_acc:0.970]
Epoch [88/120    avg_loss:0.022, val_acc:0.969]
Epoch [89/120    avg_loss:0.022, val_acc:0.970]
Epoch [90/120    avg_loss:0.026, val_acc:0.971]
Epoch [91/120    avg_loss:0.020, val_acc:0.972]
Epoch [92/120    avg_loss:0.023, val_acc:0.972]
Epoch [93/120    avg_loss:0.027, val_acc:0.970]
Epoch [94/120    avg_loss:0.018, val_acc:0.970]
Epoch [95/120    avg_loss:0.019, val_acc:0.972]
Epoch [96/120    avg_loss:0.019, val_acc:0.970]
Epoch [97/120    avg_loss:0.020, val_acc:0.969]
Epoch [98/120    avg_loss:0.023, val_acc:0.972]
Epoch [99/120    avg_loss:0.019, val_acc:0.972]
Epoch [100/120    avg_loss:0.023, val_acc:0.972]
Epoch [101/120    avg_loss:0.024, val_acc:0.971]
Epoch [102/120    avg_loss:0.020, val_acc:0.971]
Epoch [103/120    avg_loss:0.020, val_acc:0.973]
Epoch [104/120    avg_loss:0.024, val_acc:0.974]
Epoch [105/120    avg_loss:0.018, val_acc:0.971]
Epoch [106/120    avg_loss:0.017, val_acc:0.971]
Epoch [107/120    avg_loss:0.019, val_acc:0.969]
Epoch [108/120    avg_loss:0.020, val_acc:0.968]
Epoch [109/120    avg_loss:0.021, val_acc:0.969]
Epoch [110/120    avg_loss:0.015, val_acc:0.967]
Epoch [111/120    avg_loss:0.021, val_acc:0.967]
Epoch [112/120    avg_loss:0.017, val_acc:0.969]
Epoch [113/120    avg_loss:0.017, val_acc:0.970]
Epoch [114/120    avg_loss:0.020, val_acc:0.972]
Epoch [115/120    avg_loss:0.023, val_acc:0.971]
Epoch [116/120    avg_loss:0.015, val_acc:0.975]
Epoch [117/120    avg_loss:0.018, val_acc:0.970]
Epoch [118/120    avg_loss:0.016, val_acc:0.972]
Epoch [119/120    avg_loss:0.018, val_acc:0.972]
Epoch [120/120    avg_loss:0.020, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1244    0    0    0    2    0    0    0   24   11    4    0
     0    0    0]
 [   0    0    4  712    1    3    0    0    0    7    0    0   19    1
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    1    0    0    0    0
     4    0    0]
 [   0    0    1    0    0    0  654    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    2    0    0   12    0    0    3    0
     0    0    0]
 [   0    0    8   78    0    1    9    0    0    0  773    3    0    0
     0    3    0]
 [   0    0   22    0    0    0    1    0    0    0   12 2174    1    0
     0    0    0]
 [   0    0    0    6    3    0    0    0    0    0   15    9  499    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    1    0    0
  1136    0    0]
 [   0    0    0    0    0    0   45    0    0    0    0    0    0    0
    99  203    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
95.47967479674797

F1 scores:
[       nan 1.         0.97035881 0.92227979 0.98834499 0.98847926
 0.95474453 0.98039216 0.99883856 0.63157895 0.90941176 0.98594104
 0.9370892  0.99730458 0.95462185 0.73417722 0.96341463]

Kappa:
0.948418344939041
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:21:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4af3f78ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.776, val_acc:0.309]
Epoch [2/120    avg_loss:2.544, val_acc:0.356]
Epoch [3/120    avg_loss:2.371, val_acc:0.456]
Epoch [4/120    avg_loss:2.198, val_acc:0.562]
Epoch [5/120    avg_loss:2.114, val_acc:0.507]
Epoch [6/120    avg_loss:2.001, val_acc:0.586]
Epoch [7/120    avg_loss:1.839, val_acc:0.602]
Epoch [8/120    avg_loss:1.748, val_acc:0.608]
Epoch [9/120    avg_loss:1.677, val_acc:0.623]
Epoch [10/120    avg_loss:1.532, val_acc:0.664]
Epoch [11/120    avg_loss:1.458, val_acc:0.685]
Epoch [12/120    avg_loss:1.270, val_acc:0.699]
Epoch [13/120    avg_loss:1.214, val_acc:0.711]
Epoch [14/120    avg_loss:1.163, val_acc:0.691]
Epoch [15/120    avg_loss:1.039, val_acc:0.696]
Epoch [16/120    avg_loss:0.949, val_acc:0.733]
Epoch [17/120    avg_loss:0.858, val_acc:0.683]
Epoch [18/120    avg_loss:0.808, val_acc:0.724]
Epoch [19/120    avg_loss:0.783, val_acc:0.765]
Epoch [20/120    avg_loss:0.646, val_acc:0.765]
Epoch [21/120    avg_loss:0.590, val_acc:0.829]
Epoch [22/120    avg_loss:0.663, val_acc:0.790]
Epoch [23/120    avg_loss:0.556, val_acc:0.826]
Epoch [24/120    avg_loss:0.491, val_acc:0.834]
Epoch [25/120    avg_loss:0.655, val_acc:0.807]
Epoch [26/120    avg_loss:0.531, val_acc:0.790]
Epoch [27/120    avg_loss:0.451, val_acc:0.829]
Epoch [28/120    avg_loss:0.418, val_acc:0.854]
Epoch [29/120    avg_loss:0.342, val_acc:0.847]
Epoch [30/120    avg_loss:0.318, val_acc:0.864]
Epoch [31/120    avg_loss:0.302, val_acc:0.866]
Epoch [32/120    avg_loss:0.292, val_acc:0.843]
Epoch [33/120    avg_loss:0.339, val_acc:0.869]
Epoch [34/120    avg_loss:0.291, val_acc:0.893]
Epoch [35/120    avg_loss:0.270, val_acc:0.896]
Epoch [36/120    avg_loss:0.257, val_acc:0.896]
Epoch [37/120    avg_loss:0.227, val_acc:0.888]
Epoch [38/120    avg_loss:0.209, val_acc:0.866]
Epoch [39/120    avg_loss:0.230, val_acc:0.915]
Epoch [40/120    avg_loss:0.200, val_acc:0.906]
Epoch [41/120    avg_loss:0.180, val_acc:0.906]
Epoch [42/120    avg_loss:0.153, val_acc:0.920]
Epoch [43/120    avg_loss:0.152, val_acc:0.915]
Epoch [44/120    avg_loss:0.168, val_acc:0.883]
Epoch [45/120    avg_loss:0.172, val_acc:0.908]
Epoch [46/120    avg_loss:0.131, val_acc:0.923]
Epoch [47/120    avg_loss:0.143, val_acc:0.914]
Epoch [48/120    avg_loss:0.143, val_acc:0.901]
Epoch [49/120    avg_loss:0.133, val_acc:0.931]
Epoch [50/120    avg_loss:0.137, val_acc:0.931]
Epoch [51/120    avg_loss:0.195, val_acc:0.913]
Epoch [52/120    avg_loss:0.162, val_acc:0.931]
Epoch [53/120    avg_loss:0.205, val_acc:0.907]
Epoch [54/120    avg_loss:0.130, val_acc:0.934]
Epoch [55/120    avg_loss:0.095, val_acc:0.915]
Epoch [56/120    avg_loss:0.119, val_acc:0.923]
Epoch [57/120    avg_loss:0.103, val_acc:0.936]
Epoch [58/120    avg_loss:0.083, val_acc:0.934]
Epoch [59/120    avg_loss:0.086, val_acc:0.944]
Epoch [60/120    avg_loss:0.069, val_acc:0.943]
Epoch [61/120    avg_loss:0.067, val_acc:0.945]
Epoch [62/120    avg_loss:0.057, val_acc:0.943]
Epoch [63/120    avg_loss:0.065, val_acc:0.945]
Epoch [64/120    avg_loss:0.060, val_acc:0.941]
Epoch [65/120    avg_loss:0.060, val_acc:0.949]
Epoch [66/120    avg_loss:0.064, val_acc:0.935]
Epoch [67/120    avg_loss:0.059, val_acc:0.946]
Epoch [68/120    avg_loss:0.064, val_acc:0.948]
Epoch [69/120    avg_loss:0.048, val_acc:0.955]
Epoch [70/120    avg_loss:0.047, val_acc:0.946]
Epoch [71/120    avg_loss:0.058, val_acc:0.940]
Epoch [72/120    avg_loss:0.089, val_acc:0.925]
Epoch [73/120    avg_loss:0.066, val_acc:0.943]
Epoch [74/120    avg_loss:0.054, val_acc:0.951]
Epoch [75/120    avg_loss:0.050, val_acc:0.956]
Epoch [76/120    avg_loss:0.046, val_acc:0.956]
Epoch [77/120    avg_loss:0.037, val_acc:0.959]
Epoch [78/120    avg_loss:0.043, val_acc:0.950]
Epoch [79/120    avg_loss:0.047, val_acc:0.958]
Epoch [80/120    avg_loss:0.034, val_acc:0.942]
Epoch [81/120    avg_loss:0.038, val_acc:0.954]
Epoch [82/120    avg_loss:0.040, val_acc:0.958]
Epoch [83/120    avg_loss:0.044, val_acc:0.969]
Epoch [84/120    avg_loss:0.036, val_acc:0.965]
Epoch [85/120    avg_loss:0.037, val_acc:0.958]
Epoch [86/120    avg_loss:0.037, val_acc:0.964]
Epoch [87/120    avg_loss:0.028, val_acc:0.967]
Epoch [88/120    avg_loss:0.033, val_acc:0.972]
Epoch [89/120    avg_loss:0.028, val_acc:0.968]
Epoch [90/120    avg_loss:0.023, val_acc:0.963]
Epoch [91/120    avg_loss:0.021, val_acc:0.971]
Epoch [92/120    avg_loss:0.021, val_acc:0.965]
Epoch [93/120    avg_loss:0.026, val_acc:0.964]
Epoch [94/120    avg_loss:0.021, val_acc:0.968]
Epoch [95/120    avg_loss:0.034, val_acc:0.965]
Epoch [96/120    avg_loss:0.029, val_acc:0.959]
Epoch [97/120    avg_loss:0.026, val_acc:0.972]
Epoch [98/120    avg_loss:0.023, val_acc:0.967]
Epoch [99/120    avg_loss:0.020, val_acc:0.964]
Epoch [100/120    avg_loss:0.031, val_acc:0.963]
Epoch [101/120    avg_loss:0.037, val_acc:0.940]
Epoch [102/120    avg_loss:0.039, val_acc:0.960]
Epoch [103/120    avg_loss:0.021, val_acc:0.969]
Epoch [104/120    avg_loss:0.025, val_acc:0.974]
Epoch [105/120    avg_loss:0.026, val_acc:0.969]
Epoch [106/120    avg_loss:0.024, val_acc:0.968]
Epoch [107/120    avg_loss:0.032, val_acc:0.961]
Epoch [108/120    avg_loss:0.024, val_acc:0.967]
Epoch [109/120    avg_loss:0.020, val_acc:0.972]
Epoch [110/120    avg_loss:0.017, val_acc:0.960]
Epoch [111/120    avg_loss:0.021, val_acc:0.973]
Epoch [112/120    avg_loss:0.019, val_acc:0.970]
Epoch [113/120    avg_loss:0.017, val_acc:0.967]
Epoch [114/120    avg_loss:0.016, val_acc:0.967]
Epoch [115/120    avg_loss:0.016, val_acc:0.973]
Epoch [116/120    avg_loss:0.012, val_acc:0.975]
Epoch [117/120    avg_loss:0.013, val_acc:0.973]
Epoch [118/120    avg_loss:0.014, val_acc:0.972]
Epoch [119/120    avg_loss:0.017, val_acc:0.971]
Epoch [120/120    avg_loss:0.015, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1224    1    8    0    5    0    0    0    5   34    8    0
     0    0    0]
 [   0    0    4  733    0    1    0    0    0    3    0    0    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    2    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   13   89    0    3    0    0    0    0  744   15    6    0
     0    5    0]
 [   0    0   13    0    0    0    1    0    0    0   17 2177    2    0
     0    0    0]
 [   0    0    0    8    5    0    0    0    0    1    0    5  514    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   153  194    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.41463414634147

F1 scores:
[       nan 0.98765432 0.96415912 0.92902408 0.97038724 0.98847926
 0.99316629 1.         1.         0.85714286 0.90621194 0.97974797
 0.95985061 1.         0.93552361 0.71062271 0.98809524]

Kappa:
0.9476423805429348
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f788d496a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.747, val_acc:0.208]
Epoch [2/120    avg_loss:2.560, val_acc:0.366]
Epoch [3/120    avg_loss:2.388, val_acc:0.461]
Epoch [4/120    avg_loss:2.242, val_acc:0.535]
Epoch [5/120    avg_loss:2.133, val_acc:0.548]
Epoch [6/120    avg_loss:2.012, val_acc:0.580]
Epoch [7/120    avg_loss:1.876, val_acc:0.608]
Epoch [8/120    avg_loss:1.824, val_acc:0.627]
Epoch [9/120    avg_loss:1.687, val_acc:0.676]
Epoch [10/120    avg_loss:1.508, val_acc:0.673]
Epoch [11/120    avg_loss:1.383, val_acc:0.678]
Epoch [12/120    avg_loss:1.225, val_acc:0.678]
Epoch [13/120    avg_loss:1.165, val_acc:0.709]
Epoch [14/120    avg_loss:1.043, val_acc:0.725]
Epoch [15/120    avg_loss:0.995, val_acc:0.744]
Epoch [16/120    avg_loss:1.014, val_acc:0.756]
Epoch [17/120    avg_loss:0.898, val_acc:0.798]
Epoch [18/120    avg_loss:0.806, val_acc:0.798]
Epoch [19/120    avg_loss:0.719, val_acc:0.802]
Epoch [20/120    avg_loss:0.624, val_acc:0.825]
Epoch [21/120    avg_loss:0.631, val_acc:0.826]
Epoch [22/120    avg_loss:0.642, val_acc:0.775]
Epoch [23/120    avg_loss:0.662, val_acc:0.831]
Epoch [24/120    avg_loss:0.649, val_acc:0.845]
Epoch [25/120    avg_loss:0.516, val_acc:0.848]
Epoch [26/120    avg_loss:0.461, val_acc:0.833]
Epoch [27/120    avg_loss:0.489, val_acc:0.859]
Epoch [28/120    avg_loss:0.421, val_acc:0.849]
Epoch [29/120    avg_loss:0.407, val_acc:0.863]
Epoch [30/120    avg_loss:0.365, val_acc:0.853]
Epoch [31/120    avg_loss:0.303, val_acc:0.899]
Epoch [32/120    avg_loss:0.305, val_acc:0.901]
Epoch [33/120    avg_loss:0.281, val_acc:0.885]
Epoch [34/120    avg_loss:0.383, val_acc:0.865]
Epoch [35/120    avg_loss:0.366, val_acc:0.885]
Epoch [36/120    avg_loss:0.291, val_acc:0.905]
Epoch [37/120    avg_loss:0.259, val_acc:0.909]
Epoch [38/120    avg_loss:0.253, val_acc:0.923]
Epoch [39/120    avg_loss:0.203, val_acc:0.897]
Epoch [40/120    avg_loss:0.278, val_acc:0.911]
Epoch [41/120    avg_loss:0.225, val_acc:0.853]
Epoch [42/120    avg_loss:0.265, val_acc:0.901]
Epoch [43/120    avg_loss:0.199, val_acc:0.928]
Epoch [44/120    avg_loss:0.153, val_acc:0.909]
Epoch [45/120    avg_loss:0.252, val_acc:0.917]
Epoch [46/120    avg_loss:0.185, val_acc:0.927]
Epoch [47/120    avg_loss:0.163, val_acc:0.935]
Epoch [48/120    avg_loss:0.136, val_acc:0.944]
Epoch [49/120    avg_loss:0.152, val_acc:0.928]
Epoch [50/120    avg_loss:0.151, val_acc:0.926]
Epoch [51/120    avg_loss:0.170, val_acc:0.914]
Epoch [52/120    avg_loss:0.158, val_acc:0.942]
Epoch [53/120    avg_loss:0.108, val_acc:0.947]
Epoch [54/120    avg_loss:0.101, val_acc:0.932]
Epoch [55/120    avg_loss:0.133, val_acc:0.941]
Epoch [56/120    avg_loss:0.168, val_acc:0.922]
Epoch [57/120    avg_loss:0.112, val_acc:0.953]
Epoch [58/120    avg_loss:0.097, val_acc:0.952]
Epoch [59/120    avg_loss:0.130, val_acc:0.933]
Epoch [60/120    avg_loss:0.128, val_acc:0.950]
Epoch [61/120    avg_loss:0.095, val_acc:0.959]
Epoch [62/120    avg_loss:0.082, val_acc:0.956]
Epoch [63/120    avg_loss:0.074, val_acc:0.956]
Epoch [64/120    avg_loss:0.070, val_acc:0.960]
Epoch [65/120    avg_loss:0.074, val_acc:0.963]
Epoch [66/120    avg_loss:0.068, val_acc:0.960]
Epoch [67/120    avg_loss:0.072, val_acc:0.931]
Epoch [68/120    avg_loss:0.092, val_acc:0.961]
Epoch [69/120    avg_loss:0.160, val_acc:0.915]
Epoch [70/120    avg_loss:0.150, val_acc:0.951]
Epoch [71/120    avg_loss:0.111, val_acc:0.944]
Epoch [72/120    avg_loss:0.111, val_acc:0.945]
Epoch [73/120    avg_loss:0.091, val_acc:0.958]
Epoch [74/120    avg_loss:0.062, val_acc:0.966]
Epoch [75/120    avg_loss:0.085, val_acc:0.967]
Epoch [76/120    avg_loss:0.060, val_acc:0.972]
Epoch [77/120    avg_loss:0.062, val_acc:0.957]
Epoch [78/120    avg_loss:0.062, val_acc:0.964]
Epoch [79/120    avg_loss:0.051, val_acc:0.969]
Epoch [80/120    avg_loss:0.041, val_acc:0.973]
Epoch [81/120    avg_loss:0.045, val_acc:0.965]
Epoch [82/120    avg_loss:0.045, val_acc:0.960]
Epoch [83/120    avg_loss:0.045, val_acc:0.968]
Epoch [84/120    avg_loss:0.040, val_acc:0.973]
Epoch [85/120    avg_loss:0.038, val_acc:0.982]
Epoch [86/120    avg_loss:0.032, val_acc:0.963]
Epoch [87/120    avg_loss:0.038, val_acc:0.975]
Epoch [88/120    avg_loss:0.043, val_acc:0.964]
Epoch [89/120    avg_loss:0.052, val_acc:0.983]
Epoch [90/120    avg_loss:0.047, val_acc:0.967]
Epoch [91/120    avg_loss:0.055, val_acc:0.976]
Epoch [92/120    avg_loss:0.044, val_acc:0.976]
Epoch [93/120    avg_loss:0.041, val_acc:0.982]
Epoch [94/120    avg_loss:0.031, val_acc:0.977]
Epoch [95/120    avg_loss:0.033, val_acc:0.974]
Epoch [96/120    avg_loss:0.033, val_acc:0.969]
Epoch [97/120    avg_loss:0.032, val_acc:0.983]
Epoch [98/120    avg_loss:0.038, val_acc:0.975]
Epoch [99/120    avg_loss:0.054, val_acc:0.972]
Epoch [100/120    avg_loss:0.026, val_acc:0.977]
Epoch [101/120    avg_loss:0.036, val_acc:0.967]
Epoch [102/120    avg_loss:0.056, val_acc:0.976]
Epoch [103/120    avg_loss:0.028, val_acc:0.975]
Epoch [104/120    avg_loss:0.023, val_acc:0.976]
Epoch [105/120    avg_loss:0.022, val_acc:0.981]
Epoch [106/120    avg_loss:0.034, val_acc:0.982]
Epoch [107/120    avg_loss:0.031, val_acc:0.973]
Epoch [108/120    avg_loss:0.025, val_acc:0.984]
Epoch [109/120    avg_loss:0.025, val_acc:0.985]
Epoch [110/120    avg_loss:0.024, val_acc:0.972]
Epoch [111/120    avg_loss:0.028, val_acc:0.976]
Epoch [112/120    avg_loss:0.044, val_acc:0.959]
Epoch [113/120    avg_loss:0.052, val_acc:0.970]
Epoch [114/120    avg_loss:0.036, val_acc:0.981]
Epoch [115/120    avg_loss:0.036, val_acc:0.967]
Epoch [116/120    avg_loss:0.032, val_acc:0.982]
Epoch [117/120    avg_loss:0.021, val_acc:0.986]
Epoch [118/120    avg_loss:0.022, val_acc:0.983]
Epoch [119/120    avg_loss:0.019, val_acc:0.980]
Epoch [120/120    avg_loss:0.019, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1261    3    0    0    4    0    0    0    2    6    8    0
     1    0    0]
 [   0    0    3  724    3    5    0    0    0    4    0    0    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    9    0    0    9    0    0    0    0
     0    0    0]
 [   0    0   27   82    0    3    7    0    0    0  744    7    1    0
     2    2    0]
 [   0    0   44    0    0    1    5    0    0    0    7 2149    1    2
     1    0    0]
 [   0    0    0    2   18    3    0    0    0    0   11    3  491    0
     0    2    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   26    0    0    2    0    0    0    0
    70  249    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.71815718157181

F1 scores:
[       nan 0.98765432 0.96259542 0.92939666 0.95302013 0.98524404
 0.96035242 1.         0.99883856 0.52941176 0.90731707 0.9824
 0.94151486 0.99462366 0.96686491 0.83       0.97674419]

Kappa:
0.9511838843310956
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f55ec9e3ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.790, val_acc:0.262]
Epoch [2/120    avg_loss:2.588, val_acc:0.479]
Epoch [3/120    avg_loss:2.391, val_acc:0.561]
Epoch [4/120    avg_loss:2.227, val_acc:0.559]
Epoch [5/120    avg_loss:2.078, val_acc:0.588]
Epoch [6/120    avg_loss:1.927, val_acc:0.597]
Epoch [7/120    avg_loss:1.784, val_acc:0.605]
Epoch [8/120    avg_loss:1.606, val_acc:0.637]
Epoch [9/120    avg_loss:1.454, val_acc:0.666]
Epoch [10/120    avg_loss:1.358, val_acc:0.691]
Epoch [11/120    avg_loss:1.165, val_acc:0.672]
Epoch [12/120    avg_loss:1.053, val_acc:0.694]
Epoch [13/120    avg_loss:1.007, val_acc:0.709]
Epoch [14/120    avg_loss:0.955, val_acc:0.741]
Epoch [15/120    avg_loss:0.843, val_acc:0.756]
Epoch [16/120    avg_loss:0.752, val_acc:0.773]
Epoch [17/120    avg_loss:0.723, val_acc:0.779]
Epoch [18/120    avg_loss:0.644, val_acc:0.795]
Epoch [19/120    avg_loss:0.595, val_acc:0.801]
Epoch [20/120    avg_loss:0.547, val_acc:0.809]
Epoch [21/120    avg_loss:0.541, val_acc:0.826]
Epoch [22/120    avg_loss:0.579, val_acc:0.814]
Epoch [23/120    avg_loss:0.515, val_acc:0.850]
Epoch [24/120    avg_loss:0.464, val_acc:0.853]
Epoch [25/120    avg_loss:0.436, val_acc:0.826]
Epoch [26/120    avg_loss:0.383, val_acc:0.845]
Epoch [27/120    avg_loss:0.348, val_acc:0.856]
Epoch [28/120    avg_loss:0.371, val_acc:0.838]
Epoch [29/120    avg_loss:0.366, val_acc:0.856]
Epoch [30/120    avg_loss:0.317, val_acc:0.856]
Epoch [31/120    avg_loss:0.315, val_acc:0.849]
Epoch [32/120    avg_loss:0.339, val_acc:0.863]
Epoch [33/120    avg_loss:0.312, val_acc:0.854]
Epoch [34/120    avg_loss:0.264, val_acc:0.844]
Epoch [35/120    avg_loss:0.229, val_acc:0.868]
Epoch [36/120    avg_loss:0.222, val_acc:0.887]
Epoch [37/120    avg_loss:0.190, val_acc:0.886]
Epoch [38/120    avg_loss:0.200, val_acc:0.890]
Epoch [39/120    avg_loss:0.196, val_acc:0.884]
Epoch [40/120    avg_loss:0.208, val_acc:0.897]
Epoch [41/120    avg_loss:0.182, val_acc:0.906]
Epoch [42/120    avg_loss:0.176, val_acc:0.916]
Epoch [43/120    avg_loss:0.186, val_acc:0.908]
Epoch [44/120    avg_loss:0.150, val_acc:0.917]
Epoch [45/120    avg_loss:0.114, val_acc:0.917]
Epoch [46/120    avg_loss:0.146, val_acc:0.910]
Epoch [47/120    avg_loss:0.163, val_acc:0.896]
Epoch [48/120    avg_loss:0.192, val_acc:0.903]
Epoch [49/120    avg_loss:0.151, val_acc:0.897]
Epoch [50/120    avg_loss:0.115, val_acc:0.926]
Epoch [51/120    avg_loss:0.147, val_acc:0.908]
Epoch [52/120    avg_loss:0.116, val_acc:0.892]
Epoch [53/120    avg_loss:0.115, val_acc:0.903]
Epoch [54/120    avg_loss:0.146, val_acc:0.904]
Epoch [55/120    avg_loss:0.136, val_acc:0.916]
Epoch [56/120    avg_loss:0.164, val_acc:0.912]
Epoch [57/120    avg_loss:0.147, val_acc:0.916]
Epoch [58/120    avg_loss:0.300, val_acc:0.835]
Epoch [59/120    avg_loss:0.306, val_acc:0.885]
Epoch [60/120    avg_loss:0.191, val_acc:0.893]
Epoch [61/120    avg_loss:0.170, val_acc:0.922]
Epoch [62/120    avg_loss:0.132, val_acc:0.922]
Epoch [63/120    avg_loss:0.113, val_acc:0.916]
Epoch [64/120    avg_loss:0.094, val_acc:0.927]
Epoch [65/120    avg_loss:0.075, val_acc:0.933]
Epoch [66/120    avg_loss:0.080, val_acc:0.942]
Epoch [67/120    avg_loss:0.089, val_acc:0.939]
Epoch [68/120    avg_loss:0.068, val_acc:0.941]
Epoch [69/120    avg_loss:0.066, val_acc:0.943]
Epoch [70/120    avg_loss:0.076, val_acc:0.946]
Epoch [71/120    avg_loss:0.070, val_acc:0.946]
Epoch [72/120    avg_loss:0.072, val_acc:0.949]
Epoch [73/120    avg_loss:0.060, val_acc:0.945]
Epoch [74/120    avg_loss:0.064, val_acc:0.949]
Epoch [75/120    avg_loss:0.066, val_acc:0.951]
Epoch [76/120    avg_loss:0.059, val_acc:0.949]
Epoch [77/120    avg_loss:0.067, val_acc:0.951]
Epoch [78/120    avg_loss:0.059, val_acc:0.952]
Epoch [79/120    avg_loss:0.063, val_acc:0.943]
Epoch [80/120    avg_loss:0.049, val_acc:0.949]
Epoch [81/120    avg_loss:0.058, val_acc:0.950]
Epoch [82/120    avg_loss:0.061, val_acc:0.950]
Epoch [83/120    avg_loss:0.056, val_acc:0.952]
Epoch [84/120    avg_loss:0.060, val_acc:0.951]
Epoch [85/120    avg_loss:0.057, val_acc:0.952]
Epoch [86/120    avg_loss:0.052, val_acc:0.952]
Epoch [87/120    avg_loss:0.061, val_acc:0.954]
Epoch [88/120    avg_loss:0.056, val_acc:0.956]
Epoch [89/120    avg_loss:0.059, val_acc:0.954]
Epoch [90/120    avg_loss:0.051, val_acc:0.958]
Epoch [91/120    avg_loss:0.051, val_acc:0.953]
Epoch [92/120    avg_loss:0.054, val_acc:0.952]
Epoch [93/120    avg_loss:0.063, val_acc:0.954]
Epoch [94/120    avg_loss:0.049, val_acc:0.953]
Epoch [95/120    avg_loss:0.053, val_acc:0.952]
Epoch [96/120    avg_loss:0.051, val_acc:0.958]
Epoch [97/120    avg_loss:0.059, val_acc:0.956]
Epoch [98/120    avg_loss:0.056, val_acc:0.959]
Epoch [99/120    avg_loss:0.048, val_acc:0.959]
Epoch [100/120    avg_loss:0.051, val_acc:0.958]
Epoch [101/120    avg_loss:0.054, val_acc:0.959]
Epoch [102/120    avg_loss:0.050, val_acc:0.958]
Epoch [103/120    avg_loss:0.062, val_acc:0.955]
Epoch [104/120    avg_loss:0.053, val_acc:0.953]
Epoch [105/120    avg_loss:0.051, val_acc:0.958]
Epoch [106/120    avg_loss:0.045, val_acc:0.959]
Epoch [107/120    avg_loss:0.042, val_acc:0.959]
Epoch [108/120    avg_loss:0.043, val_acc:0.960]
Epoch [109/120    avg_loss:0.043, val_acc:0.959]
Epoch [110/120    avg_loss:0.052, val_acc:0.960]
Epoch [111/120    avg_loss:0.046, val_acc:0.961]
Epoch [112/120    avg_loss:0.052, val_acc:0.961]
Epoch [113/120    avg_loss:0.046, val_acc:0.956]
Epoch [114/120    avg_loss:0.046, val_acc:0.960]
Epoch [115/120    avg_loss:0.055, val_acc:0.959]
Epoch [116/120    avg_loss:0.050, val_acc:0.955]
Epoch [117/120    avg_loss:0.036, val_acc:0.958]
Epoch [118/120    avg_loss:0.046, val_acc:0.960]
Epoch [119/120    avg_loss:0.043, val_acc:0.960]
Epoch [120/120    avg_loss:0.046, val_acc:0.958]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1211    2    0    0    8    0    0    2   16   36    8    0
     0    0    2]
 [   0    0    2  727    0    1    0    0    0    7    0    0    5    5
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    2    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    5    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    4    0    0    9    0    0    2    0
     0    0    0]
 [   0    0   35   83    0    5    7    0    0    0  733    0    0    0
     3    9    0]
 [   0    0   19    0    0    0   13    0    1    0   13 2158    2    1
     3    0    0]
 [   0    0    0   26    4    3    0    0    0    0    4    1  485    0
     0    5    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    1    0    0    3    0    0    2    0    1    1    2    0
  1129    0    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    5    0
   112  211    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.5040650406504

F1 scores:
[       nan 0.975      0.94868782 0.91561713 0.99069767 0.97945205
 0.95882353 0.96153846 0.99652375 0.45       0.89172749 0.97846293
 0.92822967 0.98404255 0.94635373 0.73776224 0.94252874]

Kappa:
0.9373071197742814
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3b70cc3a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.739, val_acc:0.256]
Epoch [2/120    avg_loss:2.535, val_acc:0.323]
Epoch [3/120    avg_loss:2.381, val_acc:0.424]
Epoch [4/120    avg_loss:2.248, val_acc:0.441]
Epoch [5/120    avg_loss:2.111, val_acc:0.525]
Epoch [6/120    avg_loss:2.021, val_acc:0.588]
Epoch [7/120    avg_loss:1.897, val_acc:0.608]
Epoch [8/120    avg_loss:1.784, val_acc:0.618]
Epoch [9/120    avg_loss:1.657, val_acc:0.632]
Epoch [10/120    avg_loss:1.513, val_acc:0.637]
Epoch [11/120    avg_loss:1.419, val_acc:0.686]
Epoch [12/120    avg_loss:1.301, val_acc:0.676]
Epoch [13/120    avg_loss:1.156, val_acc:0.730]
Epoch [14/120    avg_loss:1.059, val_acc:0.736]
Epoch [15/120    avg_loss:0.974, val_acc:0.727]
Epoch [16/120    avg_loss:0.892, val_acc:0.777]
Epoch [17/120    avg_loss:0.883, val_acc:0.775]
Epoch [18/120    avg_loss:0.770, val_acc:0.794]
Epoch [19/120    avg_loss:0.745, val_acc:0.825]
Epoch [20/120    avg_loss:0.609, val_acc:0.830]
Epoch [21/120    avg_loss:0.554, val_acc:0.836]
Epoch [22/120    avg_loss:0.664, val_acc:0.772]
Epoch [23/120    avg_loss:0.617, val_acc:0.815]
Epoch [24/120    avg_loss:0.587, val_acc:0.845]
Epoch [25/120    avg_loss:0.573, val_acc:0.832]
Epoch [26/120    avg_loss:0.491, val_acc:0.814]
Epoch [27/120    avg_loss:0.595, val_acc:0.770]
Epoch [28/120    avg_loss:0.477, val_acc:0.809]
Epoch [29/120    avg_loss:0.451, val_acc:0.855]
Epoch [30/120    avg_loss:0.403, val_acc:0.857]
Epoch [31/120    avg_loss:0.361, val_acc:0.880]
Epoch [32/120    avg_loss:0.316, val_acc:0.894]
Epoch [33/120    avg_loss:0.273, val_acc:0.841]
Epoch [34/120    avg_loss:0.310, val_acc:0.842]
Epoch [35/120    avg_loss:0.375, val_acc:0.819]
Epoch [36/120    avg_loss:0.375, val_acc:0.839]
Epoch [37/120    avg_loss:0.334, val_acc:0.902]
Epoch [38/120    avg_loss:0.280, val_acc:0.890]
Epoch [39/120    avg_loss:0.253, val_acc:0.902]
Epoch [40/120    avg_loss:0.225, val_acc:0.908]
Epoch [41/120    avg_loss:0.329, val_acc:0.882]
Epoch [42/120    avg_loss:0.203, val_acc:0.915]
Epoch [43/120    avg_loss:0.154, val_acc:0.906]
Epoch [44/120    avg_loss:0.224, val_acc:0.918]
Epoch [45/120    avg_loss:0.214, val_acc:0.922]
Epoch [46/120    avg_loss:0.173, val_acc:0.926]
Epoch [47/120    avg_loss:0.151, val_acc:0.939]
Epoch [48/120    avg_loss:0.145, val_acc:0.927]
Epoch [49/120    avg_loss:0.136, val_acc:0.941]
Epoch [50/120    avg_loss:0.154, val_acc:0.944]
Epoch [51/120    avg_loss:0.109, val_acc:0.952]
Epoch [52/120    avg_loss:0.130, val_acc:0.945]
Epoch [53/120    avg_loss:0.156, val_acc:0.939]
Epoch [54/120    avg_loss:0.164, val_acc:0.927]
Epoch [55/120    avg_loss:0.183, val_acc:0.922]
Epoch [56/120    avg_loss:0.168, val_acc:0.927]
Epoch [57/120    avg_loss:0.144, val_acc:0.926]
Epoch [58/120    avg_loss:0.155, val_acc:0.939]
Epoch [59/120    avg_loss:0.126, val_acc:0.939]
Epoch [60/120    avg_loss:0.138, val_acc:0.926]
Epoch [61/120    avg_loss:0.176, val_acc:0.940]
Epoch [62/120    avg_loss:0.126, val_acc:0.942]
Epoch [63/120    avg_loss:0.102, val_acc:0.947]
Epoch [64/120    avg_loss:0.107, val_acc:0.936]
Epoch [65/120    avg_loss:0.073, val_acc:0.956]
Epoch [66/120    avg_loss:0.066, val_acc:0.957]
Epoch [67/120    avg_loss:0.059, val_acc:0.961]
Epoch [68/120    avg_loss:0.059, val_acc:0.963]
Epoch [69/120    avg_loss:0.060, val_acc:0.960]
Epoch [70/120    avg_loss:0.049, val_acc:0.959]
Epoch [71/120    avg_loss:0.046, val_acc:0.960]
Epoch [72/120    avg_loss:0.053, val_acc:0.960]
Epoch [73/120    avg_loss:0.044, val_acc:0.960]
Epoch [74/120    avg_loss:0.052, val_acc:0.960]
Epoch [75/120    avg_loss:0.045, val_acc:0.959]
Epoch [76/120    avg_loss:0.047, val_acc:0.960]
Epoch [77/120    avg_loss:0.045, val_acc:0.960]
Epoch [78/120    avg_loss:0.048, val_acc:0.957]
Epoch [79/120    avg_loss:0.048, val_acc:0.958]
Epoch [80/120    avg_loss:0.045, val_acc:0.961]
Epoch [81/120    avg_loss:0.042, val_acc:0.959]
Epoch [82/120    avg_loss:0.043, val_acc:0.959]
Epoch [83/120    avg_loss:0.038, val_acc:0.959]
Epoch [84/120    avg_loss:0.050, val_acc:0.959]
Epoch [85/120    avg_loss:0.044, val_acc:0.960]
Epoch [86/120    avg_loss:0.043, val_acc:0.960]
Epoch [87/120    avg_loss:0.051, val_acc:0.960]
Epoch [88/120    avg_loss:0.043, val_acc:0.960]
Epoch [89/120    avg_loss:0.046, val_acc:0.960]
Epoch [90/120    avg_loss:0.035, val_acc:0.960]
Epoch [91/120    avg_loss:0.043, val_acc:0.960]
Epoch [92/120    avg_loss:0.043, val_acc:0.960]
Epoch [93/120    avg_loss:0.045, val_acc:0.960]
Epoch [94/120    avg_loss:0.048, val_acc:0.960]
Epoch [95/120    avg_loss:0.048, val_acc:0.960]
Epoch [96/120    avg_loss:0.042, val_acc:0.960]
Epoch [97/120    avg_loss:0.039, val_acc:0.960]
Epoch [98/120    avg_loss:0.041, val_acc:0.960]
Epoch [99/120    avg_loss:0.050, val_acc:0.960]
Epoch [100/120    avg_loss:0.041, val_acc:0.960]
Epoch [101/120    avg_loss:0.037, val_acc:0.960]
Epoch [102/120    avg_loss:0.044, val_acc:0.960]
Epoch [103/120    avg_loss:0.040, val_acc:0.960]
Epoch [104/120    avg_loss:0.045, val_acc:0.960]
Epoch [105/120    avg_loss:0.042, val_acc:0.960]
Epoch [106/120    avg_loss:0.044, val_acc:0.960]
Epoch [107/120    avg_loss:0.043, val_acc:0.960]
Epoch [108/120    avg_loss:0.044, val_acc:0.960]
Epoch [109/120    avg_loss:0.046, val_acc:0.960]
Epoch [110/120    avg_loss:0.039, val_acc:0.960]
Epoch [111/120    avg_loss:0.043, val_acc:0.960]
Epoch [112/120    avg_loss:0.048, val_acc:0.960]
Epoch [113/120    avg_loss:0.042, val_acc:0.960]
Epoch [114/120    avg_loss:0.046, val_acc:0.960]
Epoch [115/120    avg_loss:0.042, val_acc:0.960]
Epoch [116/120    avg_loss:0.048, val_acc:0.960]
Epoch [117/120    avg_loss:0.042, val_acc:0.960]
Epoch [118/120    avg_loss:0.049, val_acc:0.960]
Epoch [119/120    avg_loss:0.038, val_acc:0.960]
Epoch [120/120    avg_loss:0.041, val_acc:0.960]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1182    3    0    0    6    0    0    0   11   76    6    0
     0    1    0]
 [   0    0    5  705    1    9    0    0    0    8    0    0   17    2
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0  421    0    6    0    0    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    6    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    2    0    0    3    0    0   12    0    0    1    0
     0    0    0]
 [   0    0   45   91    0    4    0    0    0    0  725    1    3    0
     3    3    0]
 [   0    0    7    0    0    2   11    0    0    0    9 2171    8    1
     1    0    0]
 [   0    0    0    1    4    5    0    0    0    0    8   25  488    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    1    1    1    0
  1133    0    0]
 [   0    0    0    0    0    0   29    0    0    4    0    0    0    0
    73  241    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.27642276422765

F1 scores:
[       nan 0.94871795 0.93660856 0.91026469 0.98604651 0.95790671
 0.95946942 0.89285714 0.9953271  0.57142857 0.88793631 0.96682253
 0.91815616 0.9919571  0.9613916  0.81418919 0.97647059]

Kappa:
0.9346404737618296
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:22:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff5f4ecd9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.768, val_acc:0.185]
Epoch [2/120    avg_loss:2.575, val_acc:0.393]
Epoch [3/120    avg_loss:2.422, val_acc:0.438]
Epoch [4/120    avg_loss:2.293, val_acc:0.463]
Epoch [5/120    avg_loss:2.155, val_acc:0.495]
Epoch [6/120    avg_loss:2.057, val_acc:0.522]
Epoch [7/120    avg_loss:1.954, val_acc:0.545]
Epoch [8/120    avg_loss:1.771, val_acc:0.582]
Epoch [9/120    avg_loss:1.640, val_acc:0.576]
Epoch [10/120    avg_loss:1.548, val_acc:0.575]
Epoch [11/120    avg_loss:1.428, val_acc:0.544]
Epoch [12/120    avg_loss:1.243, val_acc:0.630]
Epoch [13/120    avg_loss:1.107, val_acc:0.661]
Epoch [14/120    avg_loss:1.061, val_acc:0.692]
Epoch [15/120    avg_loss:0.979, val_acc:0.730]
Epoch [16/120    avg_loss:0.843, val_acc:0.697]
Epoch [17/120    avg_loss:0.767, val_acc:0.742]
Epoch [18/120    avg_loss:0.710, val_acc:0.748]
Epoch [19/120    avg_loss:0.689, val_acc:0.783]
Epoch [20/120    avg_loss:0.601, val_acc:0.805]
Epoch [21/120    avg_loss:0.521, val_acc:0.799]
Epoch [22/120    avg_loss:0.535, val_acc:0.818]
Epoch [23/120    avg_loss:0.489, val_acc:0.840]
Epoch [24/120    avg_loss:0.434, val_acc:0.809]
Epoch [25/120    avg_loss:0.350, val_acc:0.850]
Epoch [26/120    avg_loss:0.329, val_acc:0.875]
Epoch [27/120    avg_loss:0.319, val_acc:0.866]
Epoch [28/120    avg_loss:0.269, val_acc:0.881]
Epoch [29/120    avg_loss:0.228, val_acc:0.882]
Epoch [30/120    avg_loss:0.220, val_acc:0.899]
Epoch [31/120    avg_loss:0.302, val_acc:0.914]
Epoch [32/120    avg_loss:0.188, val_acc:0.934]
Epoch [33/120    avg_loss:0.196, val_acc:0.903]
Epoch [34/120    avg_loss:0.210, val_acc:0.905]
Epoch [35/120    avg_loss:0.215, val_acc:0.911]
Epoch [36/120    avg_loss:0.159, val_acc:0.935]
Epoch [37/120    avg_loss:0.149, val_acc:0.853]
Epoch [38/120    avg_loss:0.223, val_acc:0.872]
Epoch [39/120    avg_loss:0.156, val_acc:0.933]
Epoch [40/120    avg_loss:0.135, val_acc:0.926]
Epoch [41/120    avg_loss:0.140, val_acc:0.920]
Epoch [42/120    avg_loss:0.107, val_acc:0.932]
Epoch [43/120    avg_loss:0.109, val_acc:0.940]
Epoch [44/120    avg_loss:0.090, val_acc:0.942]
Epoch [45/120    avg_loss:0.102, val_acc:0.932]
Epoch [46/120    avg_loss:0.093, val_acc:0.953]
Epoch [47/120    avg_loss:0.072, val_acc:0.940]
Epoch [48/120    avg_loss:0.094, val_acc:0.950]
Epoch [49/120    avg_loss:0.071, val_acc:0.941]
Epoch [50/120    avg_loss:0.061, val_acc:0.954]
Epoch [51/120    avg_loss:0.060, val_acc:0.961]
Epoch [52/120    avg_loss:0.052, val_acc:0.950]
Epoch [53/120    avg_loss:0.085, val_acc:0.946]
Epoch [54/120    avg_loss:0.065, val_acc:0.949]
Epoch [55/120    avg_loss:0.049, val_acc:0.953]
Epoch [56/120    avg_loss:0.056, val_acc:0.939]
Epoch [57/120    avg_loss:0.052, val_acc:0.965]
Epoch [58/120    avg_loss:0.038, val_acc:0.959]
Epoch [59/120    avg_loss:0.039, val_acc:0.948]
Epoch [60/120    avg_loss:0.043, val_acc:0.968]
Epoch [61/120    avg_loss:0.043, val_acc:0.952]
Epoch [62/120    avg_loss:0.040, val_acc:0.958]
Epoch [63/120    avg_loss:0.036, val_acc:0.957]
Epoch [64/120    avg_loss:0.044, val_acc:0.954]
Epoch [65/120    avg_loss:0.041, val_acc:0.952]
Epoch [66/120    avg_loss:0.053, val_acc:0.958]
Epoch [67/120    avg_loss:0.046, val_acc:0.965]
Epoch [68/120    avg_loss:0.086, val_acc:0.920]
Epoch [69/120    avg_loss:0.079, val_acc:0.959]
Epoch [70/120    avg_loss:1.053, val_acc:0.263]
Epoch [71/120    avg_loss:2.217, val_acc:0.438]
Epoch [72/120    avg_loss:1.504, val_acc:0.583]
Epoch [73/120    avg_loss:1.158, val_acc:0.653]
Epoch [74/120    avg_loss:0.857, val_acc:0.690]
Epoch [75/120    avg_loss:0.828, val_acc:0.697]
Epoch [76/120    avg_loss:0.799, val_acc:0.703]
Epoch [77/120    avg_loss:0.738, val_acc:0.710]
Epoch [78/120    avg_loss:0.676, val_acc:0.734]
Epoch [79/120    avg_loss:0.665, val_acc:0.735]
Epoch [80/120    avg_loss:0.637, val_acc:0.759]
Epoch [81/120    avg_loss:0.620, val_acc:0.750]
Epoch [82/120    avg_loss:0.604, val_acc:0.768]
Epoch [83/120    avg_loss:0.603, val_acc:0.776]
Epoch [84/120    avg_loss:0.527, val_acc:0.791]
Epoch [85/120    avg_loss:0.481, val_acc:0.774]
Epoch [86/120    avg_loss:0.484, val_acc:0.799]
Epoch [87/120    avg_loss:0.477, val_acc:0.815]
Epoch [88/120    avg_loss:0.433, val_acc:0.814]
Epoch [89/120    avg_loss:0.422, val_acc:0.812]
Epoch [90/120    avg_loss:0.444, val_acc:0.816]
Epoch [91/120    avg_loss:0.444, val_acc:0.814]
Epoch [92/120    avg_loss:0.440, val_acc:0.816]
Epoch [93/120    avg_loss:0.433, val_acc:0.819]
Epoch [94/120    avg_loss:0.410, val_acc:0.814]
Epoch [95/120    avg_loss:0.452, val_acc:0.817]
Epoch [96/120    avg_loss:0.426, val_acc:0.818]
Epoch [97/120    avg_loss:0.433, val_acc:0.819]
Epoch [98/120    avg_loss:0.461, val_acc:0.819]
Epoch [99/120    avg_loss:0.422, val_acc:0.820]
Epoch [100/120    avg_loss:0.417, val_acc:0.820]
Epoch [101/120    avg_loss:0.442, val_acc:0.819]
Epoch [102/120    avg_loss:0.413, val_acc:0.819]
Epoch [103/120    avg_loss:0.429, val_acc:0.819]
Epoch [104/120    avg_loss:0.415, val_acc:0.819]
Epoch [105/120    avg_loss:0.395, val_acc:0.819]
Epoch [106/120    avg_loss:0.441, val_acc:0.819]
Epoch [107/120    avg_loss:0.425, val_acc:0.818]
Epoch [108/120    avg_loss:0.436, val_acc:0.817]
Epoch [109/120    avg_loss:0.416, val_acc:0.818]
Epoch [110/120    avg_loss:0.436, val_acc:0.818]
Epoch [111/120    avg_loss:0.392, val_acc:0.818]
Epoch [112/120    avg_loss:0.419, val_acc:0.819]
Epoch [113/120    avg_loss:0.425, val_acc:0.819]
Epoch [114/120    avg_loss:0.459, val_acc:0.819]
Epoch [115/120    avg_loss:0.418, val_acc:0.819]
Epoch [116/120    avg_loss:0.413, val_acc:0.819]
Epoch [117/120    avg_loss:0.419, val_acc:0.819]
Epoch [118/120    avg_loss:0.420, val_acc:0.819]
Epoch [119/120    avg_loss:0.418, val_acc:0.818]
Epoch [120/120    avg_loss:0.411, val_acc:0.819]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   32    2    0    0    0    0    0    0    0    0    0    6    0
     0    1    0]
 [   0    0 1036    7   23    3    0    0    0    1   81   79   54    0
     1    0    0]
 [   0    0    9  567   54    0    0    0    0    3   26   76   11    1
     0    0    0]
 [   0    0    0    9  204    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  405    6    4    0    1    0    0    0    0
    16    3    0]
 [   0    0    0    0    2    8  636    0    0    0    7    0    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   35    0    0    0    0    0    9  386    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   63    4    2    5    2    0    0    3  709   78    2    0
     4    3    0]
 [   0    0  206   62    1    2    2   10    0    1  173 1666   81    1
     0    1    4]
 [   0    0    3   10    1    2    0    0    0    0   31    9  467    0
     0    8    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0   18    1    0    0    0
  1086   32    0]
 [   0    0    0    0    0    3    5    1    0    2    0    0    0    0
    38  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
84.59620596205961

F1 scores:
[       nan 0.59259259 0.79569892 0.80654339 0.816      0.93641618
 0.97247706 0.67567568 0.94607843 0.55384615 0.74513925 0.80913065
 0.80865801 0.99462366 0.9493007  0.86002886 0.96      ]

Kappa:
0.8255610981777346
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb5ae840a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.795, val_acc:0.194]
Epoch [2/120    avg_loss:2.597, val_acc:0.335]
Epoch [3/120    avg_loss:2.442, val_acc:0.391]
Epoch [4/120    avg_loss:2.291, val_acc:0.410]
Epoch [5/120    avg_loss:2.201, val_acc:0.431]
Epoch [6/120    avg_loss:2.082, val_acc:0.434]
Epoch [7/120    avg_loss:1.954, val_acc:0.519]
Epoch [8/120    avg_loss:1.830, val_acc:0.553]
Epoch [9/120    avg_loss:1.707, val_acc:0.573]
Epoch [10/120    avg_loss:1.555, val_acc:0.589]
Epoch [11/120    avg_loss:1.446, val_acc:0.637]
Epoch [12/120    avg_loss:1.338, val_acc:0.594]
Epoch [13/120    avg_loss:1.261, val_acc:0.634]
Epoch [14/120    avg_loss:1.099, val_acc:0.684]
Epoch [15/120    avg_loss:1.030, val_acc:0.698]
Epoch [16/120    avg_loss:0.996, val_acc:0.692]
Epoch [17/120    avg_loss:0.921, val_acc:0.756]
Epoch [18/120    avg_loss:0.811, val_acc:0.771]
Epoch [19/120    avg_loss:0.686, val_acc:0.794]
Epoch [20/120    avg_loss:0.651, val_acc:0.752]
Epoch [21/120    avg_loss:0.711, val_acc:0.762]
Epoch [22/120    avg_loss:0.555, val_acc:0.775]
Epoch [23/120    avg_loss:0.546, val_acc:0.826]
Epoch [24/120    avg_loss:0.486, val_acc:0.798]
Epoch [25/120    avg_loss:0.456, val_acc:0.829]
Epoch [26/120    avg_loss:0.408, val_acc:0.832]
Epoch [27/120    avg_loss:0.312, val_acc:0.858]
Epoch [28/120    avg_loss:0.300, val_acc:0.873]
Epoch [29/120    avg_loss:0.244, val_acc:0.891]
Epoch [30/120    avg_loss:0.237, val_acc:0.895]
Epoch [31/120    avg_loss:0.223, val_acc:0.907]
Epoch [32/120    avg_loss:0.219, val_acc:0.895]
Epoch [33/120    avg_loss:0.195, val_acc:0.886]
Epoch [34/120    avg_loss:0.185, val_acc:0.909]
Epoch [35/120    avg_loss:0.196, val_acc:0.889]
Epoch [36/120    avg_loss:0.235, val_acc:0.894]
Epoch [37/120    avg_loss:0.197, val_acc:0.899]
Epoch [38/120    avg_loss:0.188, val_acc:0.907]
Epoch [39/120    avg_loss:0.336, val_acc:0.829]
Epoch [40/120    avg_loss:0.271, val_acc:0.805]
Epoch [41/120    avg_loss:0.274, val_acc:0.885]
Epoch [42/120    avg_loss:0.162, val_acc:0.917]
Epoch [43/120    avg_loss:0.137, val_acc:0.914]
Epoch [44/120    avg_loss:0.182, val_acc:0.924]
Epoch [45/120    avg_loss:0.112, val_acc:0.934]
Epoch [46/120    avg_loss:0.125, val_acc:0.917]
Epoch [47/120    avg_loss:0.137, val_acc:0.919]
Epoch [48/120    avg_loss:0.103, val_acc:0.925]
Epoch [49/120    avg_loss:0.145, val_acc:0.899]
Epoch [50/120    avg_loss:0.098, val_acc:0.938]
Epoch [51/120    avg_loss:0.086, val_acc:0.942]
Epoch [52/120    avg_loss:0.081, val_acc:0.941]
Epoch [53/120    avg_loss:0.079, val_acc:0.928]
Epoch [54/120    avg_loss:0.071, val_acc:0.936]
Epoch [55/120    avg_loss:0.062, val_acc:0.940]
Epoch [56/120    avg_loss:0.088, val_acc:0.950]
Epoch [57/120    avg_loss:0.066, val_acc:0.939]
Epoch [58/120    avg_loss:0.070, val_acc:0.958]
Epoch [59/120    avg_loss:0.049, val_acc:0.948]
Epoch [60/120    avg_loss:0.039, val_acc:0.949]
Epoch [61/120    avg_loss:0.046, val_acc:0.953]
Epoch [62/120    avg_loss:0.046, val_acc:0.956]
Epoch [63/120    avg_loss:0.042, val_acc:0.954]
Epoch [64/120    avg_loss:0.035, val_acc:0.965]
Epoch [65/120    avg_loss:0.044, val_acc:0.944]
Epoch [66/120    avg_loss:0.049, val_acc:0.945]
Epoch [67/120    avg_loss:0.045, val_acc:0.944]
Epoch [68/120    avg_loss:0.055, val_acc:0.918]
Epoch [69/120    avg_loss:0.050, val_acc:0.964]
Epoch [70/120    avg_loss:0.053, val_acc:0.952]
Epoch [71/120    avg_loss:0.060, val_acc:0.918]
Epoch [72/120    avg_loss:0.078, val_acc:0.942]
Epoch [73/120    avg_loss:0.043, val_acc:0.960]
Epoch [74/120    avg_loss:0.245, val_acc:0.930]
Epoch [75/120    avg_loss:0.114, val_acc:0.907]
Epoch [76/120    avg_loss:0.222, val_acc:0.903]
Epoch [77/120    avg_loss:0.181, val_acc:0.893]
Epoch [78/120    avg_loss:0.121, val_acc:0.919]
Epoch [79/120    avg_loss:0.103, val_acc:0.926]
Epoch [80/120    avg_loss:0.077, val_acc:0.931]
Epoch [81/120    avg_loss:0.083, val_acc:0.934]
Epoch [82/120    avg_loss:0.085, val_acc:0.934]
Epoch [83/120    avg_loss:0.066, val_acc:0.934]
Epoch [84/120    avg_loss:0.074, val_acc:0.933]
Epoch [85/120    avg_loss:0.066, val_acc:0.934]
Epoch [86/120    avg_loss:0.061, val_acc:0.934]
Epoch [87/120    avg_loss:0.058, val_acc:0.936]
Epoch [88/120    avg_loss:0.052, val_acc:0.942]
Epoch [89/120    avg_loss:0.052, val_acc:0.936]
Epoch [90/120    avg_loss:0.045, val_acc:0.944]
Epoch [91/120    avg_loss:0.044, val_acc:0.943]
Epoch [92/120    avg_loss:0.051, val_acc:0.942]
Epoch [93/120    avg_loss:0.047, val_acc:0.942]
Epoch [94/120    avg_loss:0.055, val_acc:0.941]
Epoch [95/120    avg_loss:0.063, val_acc:0.942]
Epoch [96/120    avg_loss:0.060, val_acc:0.941]
Epoch [97/120    avg_loss:0.050, val_acc:0.940]
Epoch [98/120    avg_loss:0.044, val_acc:0.940]
Epoch [99/120    avg_loss:0.049, val_acc:0.940]
Epoch [100/120    avg_loss:0.041, val_acc:0.940]
Epoch [101/120    avg_loss:0.048, val_acc:0.940]
Epoch [102/120    avg_loss:0.054, val_acc:0.939]
Epoch [103/120    avg_loss:0.053, val_acc:0.938]
Epoch [104/120    avg_loss:0.050, val_acc:0.936]
Epoch [105/120    avg_loss:0.053, val_acc:0.938]
Epoch [106/120    avg_loss:0.047, val_acc:0.938]
Epoch [107/120    avg_loss:0.054, val_acc:0.936]
Epoch [108/120    avg_loss:0.052, val_acc:0.938]
Epoch [109/120    avg_loss:0.044, val_acc:0.938]
Epoch [110/120    avg_loss:0.049, val_acc:0.938]
Epoch [111/120    avg_loss:0.045, val_acc:0.938]
Epoch [112/120    avg_loss:0.047, val_acc:0.938]
Epoch [113/120    avg_loss:0.053, val_acc:0.936]
Epoch [114/120    avg_loss:0.049, val_acc:0.938]
Epoch [115/120    avg_loss:0.042, val_acc:0.938]
Epoch [116/120    avg_loss:0.046, val_acc:0.938]
Epoch [117/120    avg_loss:0.056, val_acc:0.938]
Epoch [118/120    avg_loss:0.047, val_acc:0.939]
Epoch [119/120    avg_loss:0.057, val_acc:0.939]
Epoch [120/120    avg_loss:0.045, val_acc:0.939]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    2    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0 1229   14    3    0    1    0    0    0    8   30    0    0
     0    0    0]
 [   0    0    4  719    3    0    0    0    0    0    4    4   13    0
     0    0    0]
 [   0    0    0    3  204    0    0    0    0    0    0    0    6    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    1    0    1
     4    0    0]
 [   0    0    0    1    0    0  655    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    2    0    0    3  422    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    9    0    0    0    1    0    0    0  830   23   12    0
     0    0    0]
 [   0    0   20    3    0    2    3    0    0    0   41 2103   20    0
     3    6    9]
 [   0    0    0    4    0    0    0    0    0    0    0    3  524    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0   22    0    0    0    0    0    0    0
    31  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.30352303523036

F1 scores:
[       nan 0.95       0.96505693 0.96445339 0.95550351 0.98961938
 0.97834205 0.92307692 0.99061033 0.97142857 0.94425484 0.96137143
 0.94159928 0.99730458 0.97831743 0.892261   0.93258427]

Kappa:
0.9578949387420197
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f958a98fac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.773, val_acc:0.398]
Epoch [2/120    avg_loss:2.623, val_acc:0.445]
Epoch [3/120    avg_loss:2.466, val_acc:0.446]
Epoch [4/120    avg_loss:2.341, val_acc:0.481]
Epoch [5/120    avg_loss:2.238, val_acc:0.509]
Epoch [6/120    avg_loss:2.117, val_acc:0.508]
Epoch [7/120    avg_loss:1.994, val_acc:0.528]
Epoch [8/120    avg_loss:1.938, val_acc:0.559]
Epoch [9/120    avg_loss:1.838, val_acc:0.535]
Epoch [10/120    avg_loss:1.677, val_acc:0.576]
Epoch [11/120    avg_loss:1.614, val_acc:0.575]
Epoch [12/120    avg_loss:1.496, val_acc:0.605]
Epoch [13/120    avg_loss:1.435, val_acc:0.616]
Epoch [14/120    avg_loss:1.330, val_acc:0.691]
Epoch [15/120    avg_loss:1.151, val_acc:0.704]
Epoch [16/120    avg_loss:1.078, val_acc:0.730]
Epoch [17/120    avg_loss:1.025, val_acc:0.718]
Epoch [18/120    avg_loss:0.878, val_acc:0.760]
Epoch [19/120    avg_loss:0.811, val_acc:0.790]
Epoch [20/120    avg_loss:0.814, val_acc:0.782]
Epoch [21/120    avg_loss:0.703, val_acc:0.778]
Epoch [22/120    avg_loss:0.617, val_acc:0.783]
Epoch [23/120    avg_loss:0.585, val_acc:0.799]
Epoch [24/120    avg_loss:0.598, val_acc:0.826]
Epoch [25/120    avg_loss:0.482, val_acc:0.835]
Epoch [26/120    avg_loss:0.463, val_acc:0.855]
Epoch [27/120    avg_loss:0.398, val_acc:0.855]
Epoch [28/120    avg_loss:0.347, val_acc:0.860]
Epoch [29/120    avg_loss:0.336, val_acc:0.885]
Epoch [30/120    avg_loss:0.338, val_acc:0.867]
Epoch [31/120    avg_loss:0.394, val_acc:0.831]
Epoch [32/120    avg_loss:0.319, val_acc:0.887]
Epoch [33/120    avg_loss:0.240, val_acc:0.908]
Epoch [34/120    avg_loss:0.228, val_acc:0.893]
Epoch [35/120    avg_loss:0.240, val_acc:0.884]
Epoch [36/120    avg_loss:0.255, val_acc:0.883]
Epoch [37/120    avg_loss:0.191, val_acc:0.923]
Epoch [38/120    avg_loss:0.191, val_acc:0.925]
Epoch [39/120    avg_loss:0.167, val_acc:0.928]
Epoch [40/120    avg_loss:0.129, val_acc:0.923]
Epoch [41/120    avg_loss:0.148, val_acc:0.906]
Epoch [42/120    avg_loss:0.126, val_acc:0.916]
Epoch [43/120    avg_loss:0.159, val_acc:0.918]
Epoch [44/120    avg_loss:0.136, val_acc:0.919]
Epoch [45/120    avg_loss:0.113, val_acc:0.923]
Epoch [46/120    avg_loss:0.101, val_acc:0.935]
Epoch [47/120    avg_loss:0.085, val_acc:0.934]
Epoch [48/120    avg_loss:0.108, val_acc:0.930]
Epoch [49/120    avg_loss:0.092, val_acc:0.935]
Epoch [50/120    avg_loss:0.069, val_acc:0.950]
Epoch [51/120    avg_loss:0.081, val_acc:0.943]
Epoch [52/120    avg_loss:0.068, val_acc:0.943]
Epoch [53/120    avg_loss:0.068, val_acc:0.942]
Epoch [54/120    avg_loss:0.072, val_acc:0.944]
Epoch [55/120    avg_loss:0.069, val_acc:0.942]
Epoch [56/120    avg_loss:0.082, val_acc:0.946]
Epoch [57/120    avg_loss:0.059, val_acc:0.942]
Epoch [58/120    avg_loss:0.069, val_acc:0.942]
Epoch [59/120    avg_loss:0.061, val_acc:0.949]
Epoch [60/120    avg_loss:0.082, val_acc:0.952]
Epoch [61/120    avg_loss:0.069, val_acc:0.957]
Epoch [62/120    avg_loss:0.050, val_acc:0.956]
Epoch [63/120    avg_loss:0.064, val_acc:0.947]
Epoch [64/120    avg_loss:0.075, val_acc:0.936]
Epoch [65/120    avg_loss:0.055, val_acc:0.942]
Epoch [66/120    avg_loss:0.059, val_acc:0.956]
Epoch [67/120    avg_loss:0.056, val_acc:0.954]
Epoch [68/120    avg_loss:0.067, val_acc:0.959]
Epoch [69/120    avg_loss:0.044, val_acc:0.957]
Epoch [70/120    avg_loss:0.047, val_acc:0.941]
Epoch [71/120    avg_loss:0.046, val_acc:0.958]
Epoch [72/120    avg_loss:0.050, val_acc:0.944]
Epoch [73/120    avg_loss:0.048, val_acc:0.948]
Epoch [74/120    avg_loss:0.042, val_acc:0.956]
Epoch [75/120    avg_loss:0.042, val_acc:0.958]
Epoch [76/120    avg_loss:0.052, val_acc:0.921]
Epoch [77/120    avg_loss:0.054, val_acc:0.951]
Epoch [78/120    avg_loss:0.035, val_acc:0.955]
Epoch [79/120    avg_loss:0.036, val_acc:0.942]
Epoch [80/120    avg_loss:0.035, val_acc:0.965]
Epoch [81/120    avg_loss:0.033, val_acc:0.960]
Epoch [82/120    avg_loss:0.029, val_acc:0.959]
Epoch [83/120    avg_loss:0.021, val_acc:0.965]
Epoch [84/120    avg_loss:0.021, val_acc:0.965]
Epoch [85/120    avg_loss:0.024, val_acc:0.963]
Epoch [86/120    avg_loss:0.019, val_acc:0.972]
Epoch [87/120    avg_loss:0.017, val_acc:0.977]
Epoch [88/120    avg_loss:0.031, val_acc:0.954]
Epoch [89/120    avg_loss:0.031, val_acc:0.974]
Epoch [90/120    avg_loss:0.027, val_acc:0.965]
Epoch [91/120    avg_loss:0.025, val_acc:0.955]
Epoch [92/120    avg_loss:0.061, val_acc:0.961]
Epoch [93/120    avg_loss:0.129, val_acc:0.939]
Epoch [94/120    avg_loss:0.083, val_acc:0.910]
Epoch [95/120    avg_loss:0.099, val_acc:0.938]
Epoch [96/120    avg_loss:0.108, val_acc:0.933]
Epoch [97/120    avg_loss:0.089, val_acc:0.950]
Epoch [98/120    avg_loss:0.056, val_acc:0.943]
Epoch [99/120    avg_loss:0.058, val_acc:0.952]
Epoch [100/120    avg_loss:0.049, val_acc:0.950]
Epoch [101/120    avg_loss:0.072, val_acc:0.960]
Epoch [102/120    avg_loss:0.032, val_acc:0.965]
Epoch [103/120    avg_loss:0.026, val_acc:0.965]
Epoch [104/120    avg_loss:0.023, val_acc:0.964]
Epoch [105/120    avg_loss:0.023, val_acc:0.963]
Epoch [106/120    avg_loss:0.023, val_acc:0.970]
Epoch [107/120    avg_loss:0.021, val_acc:0.969]
Epoch [108/120    avg_loss:0.024, val_acc:0.970]
Epoch [109/120    avg_loss:0.019, val_acc:0.970]
Epoch [110/120    avg_loss:0.020, val_acc:0.970]
Epoch [111/120    avg_loss:0.020, val_acc:0.970]
Epoch [112/120    avg_loss:0.016, val_acc:0.967]
Epoch [113/120    avg_loss:0.020, val_acc:0.967]
Epoch [114/120    avg_loss:0.022, val_acc:0.967]
Epoch [115/120    avg_loss:0.016, val_acc:0.969]
Epoch [116/120    avg_loss:0.018, val_acc:0.969]
Epoch [117/120    avg_loss:0.022, val_acc:0.969]
Epoch [118/120    avg_loss:0.019, val_acc:0.969]
Epoch [119/120    avg_loss:0.016, val_acc:0.969]
Epoch [120/120    avg_loss:0.018, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    2    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0 1252    3    7    0    1    0    0    0    2   20    0    0
     0    0    0]
 [   0    0    2  725    1    0    3    0    0    2    2    4    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    1    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    1    0    0    0    0  850   18    2    0
     0    0    0]
 [   0    0   10    5    0    1    0    0    0    4   50 2120   17    0
     0    0    3]
 [   0    0    0    1    0    1    0    0    0    0    4    8  519    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
  1124   12    0]
 [   0    0    0    0    0    0    7    0    0    0    4    0    0    0
    66  270    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.80216802168022

F1 scores:
[       nan 0.96202532 0.98080689 0.9790682  0.98156682 0.98383372
 0.98716981 1.         1.         0.8372093  0.95131505 0.96737394
 0.95933457 0.99728997 0.96109448 0.85850556 0.96470588]

Kappa:
0.96355127231524
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f44a3080a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.788, val_acc:0.446]
Epoch [2/120    avg_loss:2.619, val_acc:0.499]
Epoch [3/120    avg_loss:2.442, val_acc:0.492]
Epoch [4/120    avg_loss:2.303, val_acc:0.527]
Epoch [5/120    avg_loss:2.163, val_acc:0.557]
Epoch [6/120    avg_loss:2.055, val_acc:0.531]
Epoch [7/120    avg_loss:1.945, val_acc:0.588]
Epoch [8/120    avg_loss:1.796, val_acc:0.609]
Epoch [9/120    avg_loss:1.661, val_acc:0.662]
Epoch [10/120    avg_loss:1.467, val_acc:0.654]
Epoch [11/120    avg_loss:1.414, val_acc:0.660]
Epoch [12/120    avg_loss:1.346, val_acc:0.649]
Epoch [13/120    avg_loss:1.136, val_acc:0.713]
Epoch [14/120    avg_loss:1.028, val_acc:0.714]
Epoch [15/120    avg_loss:0.889, val_acc:0.745]
Epoch [16/120    avg_loss:0.867, val_acc:0.735]
Epoch [17/120    avg_loss:0.750, val_acc:0.767]
Epoch [18/120    avg_loss:0.767, val_acc:0.782]
Epoch [19/120    avg_loss:0.646, val_acc:0.785]
Epoch [20/120    avg_loss:0.529, val_acc:0.794]
Epoch [21/120    avg_loss:0.664, val_acc:0.790]
Epoch [22/120    avg_loss:0.492, val_acc:0.773]
Epoch [23/120    avg_loss:0.497, val_acc:0.829]
Epoch [24/120    avg_loss:0.502, val_acc:0.805]
Epoch [25/120    avg_loss:0.544, val_acc:0.804]
Epoch [26/120    avg_loss:0.454, val_acc:0.784]
Epoch [27/120    avg_loss:0.367, val_acc:0.858]
Epoch [28/120    avg_loss:0.298, val_acc:0.872]
Epoch [29/120    avg_loss:0.266, val_acc:0.890]
Epoch [30/120    avg_loss:0.241, val_acc:0.870]
Epoch [31/120    avg_loss:0.214, val_acc:0.900]
Epoch [32/120    avg_loss:0.282, val_acc:0.896]
Epoch [33/120    avg_loss:0.174, val_acc:0.923]
Epoch [34/120    avg_loss:0.181, val_acc:0.917]
Epoch [35/120    avg_loss:0.153, val_acc:0.929]
Epoch [36/120    avg_loss:0.132, val_acc:0.898]
Epoch [37/120    avg_loss:0.146, val_acc:0.925]
Epoch [38/120    avg_loss:0.183, val_acc:0.911]
Epoch [39/120    avg_loss:0.179, val_acc:0.921]
Epoch [40/120    avg_loss:0.153, val_acc:0.910]
Epoch [41/120    avg_loss:0.160, val_acc:0.922]
Epoch [42/120    avg_loss:0.126, val_acc:0.922]
Epoch [43/120    avg_loss:0.122, val_acc:0.882]
Epoch [44/120    avg_loss:0.141, val_acc:0.921]
Epoch [45/120    avg_loss:0.111, val_acc:0.941]
Epoch [46/120    avg_loss:0.115, val_acc:0.908]
Epoch [47/120    avg_loss:0.105, val_acc:0.939]
Epoch [48/120    avg_loss:0.145, val_acc:0.917]
Epoch [49/120    avg_loss:0.144, val_acc:0.944]
Epoch [50/120    avg_loss:0.120, val_acc:0.924]
Epoch [51/120    avg_loss:0.094, val_acc:0.940]
Epoch [52/120    avg_loss:0.089, val_acc:0.953]
Epoch [53/120    avg_loss:0.077, val_acc:0.956]
Epoch [54/120    avg_loss:0.077, val_acc:0.956]
Epoch [55/120    avg_loss:0.060, val_acc:0.950]
Epoch [56/120    avg_loss:0.064, val_acc:0.952]
Epoch [57/120    avg_loss:0.046, val_acc:0.942]
Epoch [58/120    avg_loss:0.054, val_acc:0.947]
Epoch [59/120    avg_loss:0.068, val_acc:0.917]
Epoch [60/120    avg_loss:0.058, val_acc:0.945]
Epoch [61/120    avg_loss:0.072, val_acc:0.957]
Epoch [62/120    avg_loss:0.068, val_acc:0.948]
Epoch [63/120    avg_loss:0.134, val_acc:0.923]
Epoch [64/120    avg_loss:0.107, val_acc:0.938]
Epoch [65/120    avg_loss:0.057, val_acc:0.955]
Epoch [66/120    avg_loss:0.067, val_acc:0.924]
Epoch [67/120    avg_loss:0.062, val_acc:0.952]
Epoch [68/120    avg_loss:0.071, val_acc:0.931]
Epoch [69/120    avg_loss:0.068, val_acc:0.959]
Epoch [70/120    avg_loss:0.057, val_acc:0.957]
Epoch [71/120    avg_loss:0.044, val_acc:0.915]
Epoch [72/120    avg_loss:0.057, val_acc:0.960]
Epoch [73/120    avg_loss:0.057, val_acc:0.948]
Epoch [74/120    avg_loss:0.058, val_acc:0.949]
Epoch [75/120    avg_loss:0.045, val_acc:0.956]
Epoch [76/120    avg_loss:0.029, val_acc:0.967]
Epoch [77/120    avg_loss:0.024, val_acc:0.955]
Epoch [78/120    avg_loss:0.030, val_acc:0.960]
Epoch [79/120    avg_loss:0.060, val_acc:0.914]
Epoch [80/120    avg_loss:0.053, val_acc:0.958]
Epoch [81/120    avg_loss:0.035, val_acc:0.968]
Epoch [82/120    avg_loss:0.034, val_acc:0.961]
Epoch [83/120    avg_loss:0.035, val_acc:0.952]
Epoch [84/120    avg_loss:0.033, val_acc:0.950]
Epoch [85/120    avg_loss:0.035, val_acc:0.951]
Epoch [86/120    avg_loss:0.030, val_acc:0.965]
Epoch [87/120    avg_loss:0.020, val_acc:0.968]
Epoch [88/120    avg_loss:0.026, val_acc:0.966]
Epoch [89/120    avg_loss:0.052, val_acc:0.944]
Epoch [90/120    avg_loss:0.041, val_acc:0.964]
Epoch [91/120    avg_loss:0.035, val_acc:0.946]
Epoch [92/120    avg_loss:0.032, val_acc:0.968]
Epoch [93/120    avg_loss:0.029, val_acc:0.969]
Epoch [94/120    avg_loss:0.021, val_acc:0.968]
Epoch [95/120    avg_loss:0.021, val_acc:0.956]
Epoch [96/120    avg_loss:0.017, val_acc:0.972]
Epoch [97/120    avg_loss:0.015, val_acc:0.972]
Epoch [98/120    avg_loss:0.017, val_acc:0.968]
Epoch [99/120    avg_loss:0.012, val_acc:0.974]
Epoch [100/120    avg_loss:0.017, val_acc:0.968]
Epoch [101/120    avg_loss:0.017, val_acc:0.970]
Epoch [102/120    avg_loss:0.012, val_acc:0.975]
Epoch [103/120    avg_loss:0.018, val_acc:0.964]
Epoch [104/120    avg_loss:0.035, val_acc:0.944]
Epoch [105/120    avg_loss:0.029, val_acc:0.959]
Epoch [106/120    avg_loss:0.021, val_acc:0.971]
Epoch [107/120    avg_loss:0.023, val_acc:0.973]
Epoch [108/120    avg_loss:0.020, val_acc:0.974]
Epoch [109/120    avg_loss:0.025, val_acc:0.895]
Epoch [110/120    avg_loss:0.052, val_acc:0.958]
Epoch [111/120    avg_loss:0.032, val_acc:0.975]
Epoch [112/120    avg_loss:0.012, val_acc:0.972]
Epoch [113/120    avg_loss:0.010, val_acc:0.974]
Epoch [114/120    avg_loss:0.022, val_acc:0.967]
Epoch [115/120    avg_loss:0.030, val_acc:0.963]
Epoch [116/120    avg_loss:0.017, val_acc:0.969]
Epoch [117/120    avg_loss:0.014, val_acc:0.971]
Epoch [118/120    avg_loss:0.012, val_acc:0.979]
Epoch [119/120    avg_loss:0.018, val_acc:0.965]
Epoch [120/120    avg_loss:0.024, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1238    4   10    7    0    0    0    1   10    9    6    0
     0    0    0]
 [   0    0    0  719    9    3    0    0    0    3    3    7    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    1    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    1    1    0    3    0    0    0    0  858   11    1    0
     0    0    0]
 [   0    0    8    0    0    2    2    0    0    1   37 2124   33    0
     0    3    0]
 [   0    0    0    5    0    1    0    0    0    0    1    7  518    0
     0    2    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    64  277    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.89972899728997

F1 scores:
[       nan 1.         0.9778831  0.97359513 0.95730337 0.97616345
 0.99167298 1.         1.         0.85       0.96188341 0.97208238
 0.94525547 1.         0.96528075 0.86292835 0.99401198]

Kappa:
0.964686884013931
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5af5298ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.741, val_acc:0.198]
Epoch [2/120    avg_loss:2.573, val_acc:0.299]
Epoch [3/120    avg_loss:2.408, val_acc:0.333]
Epoch [4/120    avg_loss:2.279, val_acc:0.359]
Epoch [5/120    avg_loss:2.209, val_acc:0.383]
Epoch [6/120    avg_loss:2.089, val_acc:0.484]
Epoch [7/120    avg_loss:1.972, val_acc:0.567]
Epoch [8/120    avg_loss:1.846, val_acc:0.555]
Epoch [9/120    avg_loss:1.716, val_acc:0.557]
Epoch [10/120    avg_loss:1.611, val_acc:0.596]
Epoch [11/120    avg_loss:1.444, val_acc:0.615]
Epoch [12/120    avg_loss:1.421, val_acc:0.665]
Epoch [13/120    avg_loss:1.220, val_acc:0.661]
Epoch [14/120    avg_loss:1.117, val_acc:0.714]
Epoch [15/120    avg_loss:1.018, val_acc:0.741]
Epoch [16/120    avg_loss:0.914, val_acc:0.716]
Epoch [17/120    avg_loss:0.798, val_acc:0.718]
Epoch [18/120    avg_loss:0.802, val_acc:0.779]
Epoch [19/120    avg_loss:0.764, val_acc:0.776]
Epoch [20/120    avg_loss:0.640, val_acc:0.761]
Epoch [21/120    avg_loss:0.564, val_acc:0.823]
Epoch [22/120    avg_loss:0.529, val_acc:0.838]
Epoch [23/120    avg_loss:0.431, val_acc:0.833]
Epoch [24/120    avg_loss:0.440, val_acc:0.847]
Epoch [25/120    avg_loss:0.389, val_acc:0.833]
Epoch [26/120    avg_loss:0.394, val_acc:0.848]
Epoch [27/120    avg_loss:0.356, val_acc:0.828]
Epoch [28/120    avg_loss:0.284, val_acc:0.877]
Epoch [29/120    avg_loss:0.249, val_acc:0.836]
Epoch [30/120    avg_loss:0.394, val_acc:0.843]
Epoch [31/120    avg_loss:0.375, val_acc:0.843]
Epoch [32/120    avg_loss:0.335, val_acc:0.845]
Epoch [33/120    avg_loss:0.434, val_acc:0.846]
Epoch [34/120    avg_loss:0.372, val_acc:0.855]
Epoch [35/120    avg_loss:0.246, val_acc:0.884]
Epoch [36/120    avg_loss:0.255, val_acc:0.891]
Epoch [37/120    avg_loss:0.226, val_acc:0.900]
Epoch [38/120    avg_loss:0.181, val_acc:0.892]
Epoch [39/120    avg_loss:0.175, val_acc:0.922]
Epoch [40/120    avg_loss:0.148, val_acc:0.896]
Epoch [41/120    avg_loss:0.169, val_acc:0.920]
Epoch [42/120    avg_loss:0.123, val_acc:0.924]
Epoch [43/120    avg_loss:0.142, val_acc:0.923]
Epoch [44/120    avg_loss:0.149, val_acc:0.915]
Epoch [45/120    avg_loss:0.118, val_acc:0.925]
Epoch [46/120    avg_loss:0.100, val_acc:0.942]
Epoch [47/120    avg_loss:0.085, val_acc:0.927]
Epoch [48/120    avg_loss:0.088, val_acc:0.939]
Epoch [49/120    avg_loss:0.100, val_acc:0.931]
Epoch [50/120    avg_loss:0.081, val_acc:0.957]
Epoch [51/120    avg_loss:0.080, val_acc:0.947]
Epoch [52/120    avg_loss:0.069, val_acc:0.942]
Epoch [53/120    avg_loss:0.076, val_acc:0.945]
Epoch [54/120    avg_loss:0.064, val_acc:0.952]
Epoch [55/120    avg_loss:0.063, val_acc:0.947]
Epoch [56/120    avg_loss:0.075, val_acc:0.936]
Epoch [57/120    avg_loss:0.072, val_acc:0.945]
Epoch [58/120    avg_loss:0.064, val_acc:0.951]
Epoch [59/120    avg_loss:0.080, val_acc:0.952]
Epoch [60/120    avg_loss:0.073, val_acc:0.939]
Epoch [61/120    avg_loss:0.106, val_acc:0.896]
Epoch [62/120    avg_loss:0.078, val_acc:0.941]
Epoch [63/120    avg_loss:0.232, val_acc:0.928]
Epoch [64/120    avg_loss:0.098, val_acc:0.939]
Epoch [65/120    avg_loss:0.076, val_acc:0.951]
Epoch [66/120    avg_loss:0.071, val_acc:0.951]
Epoch [67/120    avg_loss:0.060, val_acc:0.956]
Epoch [68/120    avg_loss:0.052, val_acc:0.953]
Epoch [69/120    avg_loss:0.056, val_acc:0.957]
Epoch [70/120    avg_loss:0.043, val_acc:0.956]
Epoch [71/120    avg_loss:0.059, val_acc:0.954]
Epoch [72/120    avg_loss:0.054, val_acc:0.960]
Epoch [73/120    avg_loss:0.035, val_acc:0.961]
Epoch [74/120    avg_loss:0.041, val_acc:0.960]
Epoch [75/120    avg_loss:0.032, val_acc:0.963]
Epoch [76/120    avg_loss:0.043, val_acc:0.965]
Epoch [77/120    avg_loss:0.032, val_acc:0.968]
Epoch [78/120    avg_loss:0.040, val_acc:0.967]
Epoch [79/120    avg_loss:0.039, val_acc:0.966]
Epoch [80/120    avg_loss:0.032, val_acc:0.967]
Epoch [81/120    avg_loss:0.044, val_acc:0.967]
Epoch [82/120    avg_loss:0.032, val_acc:0.967]
Epoch [83/120    avg_loss:0.030, val_acc:0.964]
Epoch [84/120    avg_loss:0.033, val_acc:0.966]
Epoch [85/120    avg_loss:0.027, val_acc:0.966]
Epoch [86/120    avg_loss:0.034, val_acc:0.967]
Epoch [87/120    avg_loss:0.026, val_acc:0.969]
Epoch [88/120    avg_loss:0.030, val_acc:0.963]
Epoch [89/120    avg_loss:0.030, val_acc:0.963]
Epoch [90/120    avg_loss:0.036, val_acc:0.965]
Epoch [91/120    avg_loss:0.030, val_acc:0.965]
Epoch [92/120    avg_loss:0.032, val_acc:0.971]
Epoch [93/120    avg_loss:0.031, val_acc:0.968]
Epoch [94/120    avg_loss:0.030, val_acc:0.969]
Epoch [95/120    avg_loss:0.027, val_acc:0.970]
Epoch [96/120    avg_loss:0.033, val_acc:0.972]
Epoch [97/120    avg_loss:0.026, val_acc:0.971]
Epoch [98/120    avg_loss:0.029, val_acc:0.971]
Epoch [99/120    avg_loss:0.029, val_acc:0.973]
Epoch [100/120    avg_loss:0.025, val_acc:0.971]
Epoch [101/120    avg_loss:0.028, val_acc:0.975]
Epoch [102/120    avg_loss:0.026, val_acc:0.977]
Epoch [103/120    avg_loss:0.023, val_acc:0.975]
Epoch [104/120    avg_loss:0.027, val_acc:0.973]
Epoch [105/120    avg_loss:0.025, val_acc:0.973]
Epoch [106/120    avg_loss:0.029, val_acc:0.972]
Epoch [107/120    avg_loss:0.028, val_acc:0.970]
Epoch [108/120    avg_loss:0.029, val_acc:0.974]
Epoch [109/120    avg_loss:0.025, val_acc:0.973]
Epoch [110/120    avg_loss:0.026, val_acc:0.973]
Epoch [111/120    avg_loss:0.029, val_acc:0.975]
Epoch [112/120    avg_loss:0.025, val_acc:0.976]
Epoch [113/120    avg_loss:0.023, val_acc:0.972]
Epoch [114/120    avg_loss:0.029, val_acc:0.976]
Epoch [115/120    avg_loss:0.024, val_acc:0.972]
Epoch [116/120    avg_loss:0.024, val_acc:0.973]
Epoch [117/120    avg_loss:0.027, val_acc:0.975]
Epoch [118/120    avg_loss:0.024, val_acc:0.976]
Epoch [119/120    avg_loss:0.027, val_acc:0.976]
Epoch [120/120    avg_loss:0.024, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    1    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1250    4    1    5    0    0    0    0    6   18    1    0
     0    0    0]
 [   0    0    2  716    1    1    1    0    0    5    0    7   14    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    0    1    0
     5    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    1    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0  848   24    0    0
     0    0    0]
 [   0    0   15    0    0    0    2    0    0    2   38 2144    9    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0    0    0    6  519    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    45  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.36585365853658

F1 scores:
[       nan 0.97560976 0.97847358 0.97216565 0.99297424 0.98394495
 0.99544073 1.         0.997669   0.8372093  0.9598189  0.9723356
 0.96200185 1.         0.97369556 0.91212121 0.98224852]

Kappa:
0.9699682052226164
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb4c9f33a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.776, val_acc:0.188]
Epoch [2/120    avg_loss:2.609, val_acc:0.300]
Epoch [3/120    avg_loss:2.477, val_acc:0.424]
Epoch [4/120    avg_loss:2.340, val_acc:0.479]
Epoch [5/120    avg_loss:2.242, val_acc:0.477]
Epoch [6/120    avg_loss:2.144, val_acc:0.510]
Epoch [7/120    avg_loss:2.000, val_acc:0.501]
Epoch [8/120    avg_loss:1.906, val_acc:0.508]
Epoch [9/120    avg_loss:1.771, val_acc:0.529]
Epoch [10/120    avg_loss:1.653, val_acc:0.541]
Epoch [11/120    avg_loss:1.566, val_acc:0.569]
Epoch [12/120    avg_loss:1.488, val_acc:0.597]
Epoch [13/120    avg_loss:1.370, val_acc:0.651]
Epoch [14/120    avg_loss:1.271, val_acc:0.702]
Epoch [15/120    avg_loss:1.130, val_acc:0.721]
Epoch [16/120    avg_loss:1.098, val_acc:0.727]
Epoch [17/120    avg_loss:1.004, val_acc:0.725]
Epoch [18/120    avg_loss:0.910, val_acc:0.738]
Epoch [19/120    avg_loss:0.804, val_acc:0.765]
Epoch [20/120    avg_loss:0.760, val_acc:0.784]
Epoch [21/120    avg_loss:0.707, val_acc:0.785]
Epoch [22/120    avg_loss:0.590, val_acc:0.821]
Epoch [23/120    avg_loss:0.546, val_acc:0.836]
Epoch [24/120    avg_loss:0.539, val_acc:0.831]
Epoch [25/120    avg_loss:0.445, val_acc:0.870]
Epoch [26/120    avg_loss:0.350, val_acc:0.840]
Epoch [27/120    avg_loss:0.393, val_acc:0.878]
Epoch [28/120    avg_loss:0.309, val_acc:0.873]
Epoch [29/120    avg_loss:0.299, val_acc:0.896]
Epoch [30/120    avg_loss:0.244, val_acc:0.885]
Epoch [31/120    avg_loss:0.264, val_acc:0.908]
Epoch [32/120    avg_loss:0.192, val_acc:0.885]
Epoch [33/120    avg_loss:0.191, val_acc:0.904]
Epoch [34/120    avg_loss:0.188, val_acc:0.923]
Epoch [35/120    avg_loss:0.175, val_acc:0.928]
Epoch [36/120    avg_loss:0.179, val_acc:0.877]
Epoch [37/120    avg_loss:0.302, val_acc:0.831]
Epoch [38/120    avg_loss:0.255, val_acc:0.910]
Epoch [39/120    avg_loss:0.177, val_acc:0.921]
Epoch [40/120    avg_loss:0.147, val_acc:0.944]
Epoch [41/120    avg_loss:0.152, val_acc:0.894]
Epoch [42/120    avg_loss:0.189, val_acc:0.931]
Epoch [43/120    avg_loss:0.132, val_acc:0.918]
Epoch [44/120    avg_loss:0.127, val_acc:0.936]
Epoch [45/120    avg_loss:0.129, val_acc:0.948]
Epoch [46/120    avg_loss:0.094, val_acc:0.936]
Epoch [47/120    avg_loss:0.102, val_acc:0.935]
Epoch [48/120    avg_loss:0.071, val_acc:0.946]
Epoch [49/120    avg_loss:0.102, val_acc:0.944]
Epoch [50/120    avg_loss:0.077, val_acc:0.959]
Epoch [51/120    avg_loss:0.072, val_acc:0.947]
Epoch [52/120    avg_loss:0.052, val_acc:0.964]
Epoch [53/120    avg_loss:0.065, val_acc:0.956]
Epoch [54/120    avg_loss:0.069, val_acc:0.943]
Epoch [55/120    avg_loss:0.088, val_acc:0.941]
Epoch [56/120    avg_loss:0.072, val_acc:0.954]
Epoch [57/120    avg_loss:0.096, val_acc:0.946]
Epoch [58/120    avg_loss:0.106, val_acc:0.957]
Epoch [59/120    avg_loss:0.121, val_acc:0.929]
Epoch [60/120    avg_loss:0.107, val_acc:0.943]
Epoch [61/120    avg_loss:0.064, val_acc:0.953]
Epoch [62/120    avg_loss:0.053, val_acc:0.960]
Epoch [63/120    avg_loss:0.048, val_acc:0.967]
Epoch [64/120    avg_loss:0.047, val_acc:0.958]
Epoch [65/120    avg_loss:0.035, val_acc:0.967]
Epoch [66/120    avg_loss:0.040, val_acc:0.958]
Epoch [67/120    avg_loss:0.036, val_acc:0.958]
Epoch [68/120    avg_loss:0.048, val_acc:0.942]
Epoch [69/120    avg_loss:0.048, val_acc:0.969]
Epoch [70/120    avg_loss:0.032, val_acc:0.968]
Epoch [71/120    avg_loss:0.033, val_acc:0.970]
Epoch [72/120    avg_loss:0.035, val_acc:0.975]
Epoch [73/120    avg_loss:0.049, val_acc:0.961]
Epoch [74/120    avg_loss:0.035, val_acc:0.960]
Epoch [75/120    avg_loss:0.028, val_acc:0.968]
Epoch [76/120    avg_loss:0.032, val_acc:0.975]
Epoch [77/120    avg_loss:0.045, val_acc:0.964]
Epoch [78/120    avg_loss:0.035, val_acc:0.968]
Epoch [79/120    avg_loss:0.034, val_acc:0.971]
Epoch [80/120    avg_loss:0.047, val_acc:0.953]
Epoch [81/120    avg_loss:0.034, val_acc:0.966]
Epoch [82/120    avg_loss:0.028, val_acc:0.970]
Epoch [83/120    avg_loss:0.030, val_acc:0.965]
Epoch [84/120    avg_loss:0.026, val_acc:0.969]
Epoch [85/120    avg_loss:0.027, val_acc:0.968]
Epoch [86/120    avg_loss:0.033, val_acc:0.960]
Epoch [87/120    avg_loss:0.041, val_acc:0.973]
Epoch [88/120    avg_loss:0.019, val_acc:0.973]
Epoch [89/120    avg_loss:0.016, val_acc:0.978]
Epoch [90/120    avg_loss:0.022, val_acc:0.969]
Epoch [91/120    avg_loss:0.021, val_acc:0.975]
Epoch [92/120    avg_loss:0.016, val_acc:0.975]
Epoch [93/120    avg_loss:0.035, val_acc:0.950]
Epoch [94/120    avg_loss:0.028, val_acc:0.964]
Epoch [95/120    avg_loss:0.040, val_acc:0.960]
Epoch [96/120    avg_loss:0.026, val_acc:0.970]
Epoch [97/120    avg_loss:0.018, val_acc:0.966]
Epoch [98/120    avg_loss:0.016, val_acc:0.974]
Epoch [99/120    avg_loss:0.014, val_acc:0.969]
Epoch [100/120    avg_loss:0.021, val_acc:0.972]
Epoch [101/120    avg_loss:0.012, val_acc:0.970]
Epoch [102/120    avg_loss:0.016, val_acc:0.966]
Epoch [103/120    avg_loss:0.014, val_acc:0.969]
Epoch [104/120    avg_loss:0.012, val_acc:0.972]
Epoch [105/120    avg_loss:0.012, val_acc:0.973]
Epoch [106/120    avg_loss:0.010, val_acc:0.970]
Epoch [107/120    avg_loss:0.010, val_acc:0.973]
Epoch [108/120    avg_loss:0.009, val_acc:0.974]
Epoch [109/120    avg_loss:0.009, val_acc:0.973]
Epoch [110/120    avg_loss:0.008, val_acc:0.972]
Epoch [111/120    avg_loss:0.008, val_acc:0.975]
Epoch [112/120    avg_loss:0.009, val_acc:0.973]
Epoch [113/120    avg_loss:0.008, val_acc:0.975]
Epoch [114/120    avg_loss:0.009, val_acc:0.976]
Epoch [115/120    avg_loss:0.009, val_acc:0.976]
Epoch [116/120    avg_loss:0.010, val_acc:0.977]
Epoch [117/120    avg_loss:0.008, val_acc:0.977]
Epoch [118/120    avg_loss:0.008, val_acc:0.977]
Epoch [119/120    avg_loss:0.009, val_acc:0.977]
Epoch [120/120    avg_loss:0.011, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1250    2    1    0    2    0    0    0    0   29    0    1
     0    0    0]
 [   0    0    1  737    1    0    0    0    0    1    1    3    0    1
     0    2    0]
 [   0    0    0    0  211    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    0    0    0    0    0
     6    2    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    0    2    0    0    0  846   18    0    0
     1    0    0]
 [   0    0    5    0    0    1    1    0    0    0   10 2159   22    0
     1    4    7]
 [   0    0    0    7    0    1    0    0    0    0    5    1  514    0
     1    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1118   21    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    46  295    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.4959349593496

F1 scores:
[       nan 1.         0.98077678 0.98727395 0.99061033 0.98842593
 0.98942598 1.         1.         0.94444444 0.97409326 0.9767021
 0.95895522 0.99462366 0.9667099  0.87537092 0.94915254]

Kappa:
0.9714501418774446
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbb85843a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.774, val_acc:0.263]
Epoch [2/120    avg_loss:2.617, val_acc:0.347]
Epoch [3/120    avg_loss:2.464, val_acc:0.467]
Epoch [4/120    avg_loss:2.324, val_acc:0.502]
Epoch [5/120    avg_loss:2.259, val_acc:0.511]
Epoch [6/120    avg_loss:2.137, val_acc:0.516]
Epoch [7/120    avg_loss:2.024, val_acc:0.524]
Epoch [8/120    avg_loss:1.924, val_acc:0.544]
Epoch [9/120    avg_loss:1.840, val_acc:0.576]
Epoch [10/120    avg_loss:1.711, val_acc:0.610]
Epoch [11/120    avg_loss:1.638, val_acc:0.635]
Epoch [12/120    avg_loss:1.505, val_acc:0.595]
Epoch [13/120    avg_loss:1.336, val_acc:0.584]
Epoch [14/120    avg_loss:1.244, val_acc:0.642]
Epoch [15/120    avg_loss:1.130, val_acc:0.630]
Epoch [16/120    avg_loss:0.997, val_acc:0.658]
Epoch [17/120    avg_loss:0.910, val_acc:0.660]
Epoch [18/120    avg_loss:0.892, val_acc:0.732]
Epoch [19/120    avg_loss:0.823, val_acc:0.727]
Epoch [20/120    avg_loss:0.659, val_acc:0.767]
Epoch [21/120    avg_loss:0.626, val_acc:0.779]
Epoch [22/120    avg_loss:0.566, val_acc:0.791]
Epoch [23/120    avg_loss:0.508, val_acc:0.808]
Epoch [24/120    avg_loss:0.518, val_acc:0.814]
Epoch [25/120    avg_loss:0.422, val_acc:0.830]
Epoch [26/120    avg_loss:0.388, val_acc:0.852]
Epoch [27/120    avg_loss:0.395, val_acc:0.816]
Epoch [28/120    avg_loss:0.365, val_acc:0.857]
Epoch [29/120    avg_loss:0.320, val_acc:0.826]
Epoch [30/120    avg_loss:0.409, val_acc:0.801]
Epoch [31/120    avg_loss:0.428, val_acc:0.828]
Epoch [32/120    avg_loss:0.362, val_acc:0.854]
Epoch [33/120    avg_loss:0.282, val_acc:0.897]
Epoch [34/120    avg_loss:0.225, val_acc:0.879]
Epoch [35/120    avg_loss:0.209, val_acc:0.870]
Epoch [36/120    avg_loss:0.201, val_acc:0.904]
Epoch [37/120    avg_loss:0.219, val_acc:0.896]
Epoch [38/120    avg_loss:0.177, val_acc:0.919]
Epoch [39/120    avg_loss:1.295, val_acc:0.615]
Epoch [40/120    avg_loss:0.865, val_acc:0.720]
Epoch [41/120    avg_loss:0.533, val_acc:0.809]
Epoch [42/120    avg_loss:0.462, val_acc:0.743]
Epoch [43/120    avg_loss:0.381, val_acc:0.857]
Epoch [44/120    avg_loss:0.374, val_acc:0.778]
Epoch [45/120    avg_loss:0.350, val_acc:0.864]
Epoch [46/120    avg_loss:0.290, val_acc:0.871]
Epoch [47/120    avg_loss:0.252, val_acc:0.894]
Epoch [48/120    avg_loss:0.174, val_acc:0.907]
Epoch [49/120    avg_loss:0.169, val_acc:0.881]
Epoch [50/120    avg_loss:0.179, val_acc:0.912]
Epoch [51/120    avg_loss:0.222, val_acc:0.902]
Epoch [52/120    avg_loss:0.155, val_acc:0.928]
Epoch [53/120    avg_loss:0.118, val_acc:0.933]
Epoch [54/120    avg_loss:0.124, val_acc:0.933]
Epoch [55/120    avg_loss:0.119, val_acc:0.938]
Epoch [56/120    avg_loss:0.116, val_acc:0.940]
Epoch [57/120    avg_loss:0.108, val_acc:0.938]
Epoch [58/120    avg_loss:0.098, val_acc:0.941]
Epoch [59/120    avg_loss:0.115, val_acc:0.940]
Epoch [60/120    avg_loss:0.125, val_acc:0.941]
Epoch [61/120    avg_loss:0.105, val_acc:0.943]
Epoch [62/120    avg_loss:0.101, val_acc:0.941]
Epoch [63/120    avg_loss:0.117, val_acc:0.942]
Epoch [64/120    avg_loss:0.108, val_acc:0.946]
Epoch [65/120    avg_loss:0.090, val_acc:0.946]
Epoch [66/120    avg_loss:0.094, val_acc:0.947]
Epoch [67/120    avg_loss:0.092, val_acc:0.950]
Epoch [68/120    avg_loss:0.090, val_acc:0.946]
Epoch [69/120    avg_loss:0.086, val_acc:0.950]
Epoch [70/120    avg_loss:0.088, val_acc:0.949]
Epoch [71/120    avg_loss:0.099, val_acc:0.948]
Epoch [72/120    avg_loss:0.092, val_acc:0.952]
Epoch [73/120    avg_loss:0.092, val_acc:0.954]
Epoch [74/120    avg_loss:0.090, val_acc:0.953]
Epoch [75/120    avg_loss:0.083, val_acc:0.952]
Epoch [76/120    avg_loss:0.086, val_acc:0.947]
Epoch [77/120    avg_loss:0.081, val_acc:0.950]
Epoch [78/120    avg_loss:0.080, val_acc:0.951]
Epoch [79/120    avg_loss:0.073, val_acc:0.954]
Epoch [80/120    avg_loss:0.089, val_acc:0.955]
Epoch [81/120    avg_loss:0.092, val_acc:0.954]
Epoch [82/120    avg_loss:0.076, val_acc:0.955]
Epoch [83/120    avg_loss:0.087, val_acc:0.958]
Epoch [84/120    avg_loss:0.079, val_acc:0.956]
Epoch [85/120    avg_loss:0.077, val_acc:0.955]
Epoch [86/120    avg_loss:0.076, val_acc:0.954]
Epoch [87/120    avg_loss:0.078, val_acc:0.957]
Epoch [88/120    avg_loss:0.069, val_acc:0.961]
Epoch [89/120    avg_loss:0.080, val_acc:0.958]
Epoch [90/120    avg_loss:0.071, val_acc:0.957]
Epoch [91/120    avg_loss:0.074, val_acc:0.958]
Epoch [92/120    avg_loss:0.059, val_acc:0.959]
Epoch [93/120    avg_loss:0.062, val_acc:0.961]
Epoch [94/120    avg_loss:0.069, val_acc:0.955]
Epoch [95/120    avg_loss:0.060, val_acc:0.961]
Epoch [96/120    avg_loss:0.066, val_acc:0.960]
Epoch [97/120    avg_loss:0.068, val_acc:0.956]
Epoch [98/120    avg_loss:0.065, val_acc:0.964]
Epoch [99/120    avg_loss:0.063, val_acc:0.961]
Epoch [100/120    avg_loss:0.063, val_acc:0.959]
Epoch [101/120    avg_loss:0.073, val_acc:0.957]
Epoch [102/120    avg_loss:0.069, val_acc:0.963]
Epoch [103/120    avg_loss:0.065, val_acc:0.959]
Epoch [104/120    avg_loss:0.068, val_acc:0.957]
Epoch [105/120    avg_loss:0.066, val_acc:0.960]
Epoch [106/120    avg_loss:0.070, val_acc:0.960]
Epoch [107/120    avg_loss:0.063, val_acc:0.963]
Epoch [108/120    avg_loss:0.058, val_acc:0.964]
Epoch [109/120    avg_loss:0.057, val_acc:0.963]
Epoch [110/120    avg_loss:0.061, val_acc:0.965]
Epoch [111/120    avg_loss:0.057, val_acc:0.961]
Epoch [112/120    avg_loss:0.069, val_acc:0.958]
Epoch [113/120    avg_loss:0.069, val_acc:0.966]
Epoch [114/120    avg_loss:0.064, val_acc:0.963]
Epoch [115/120    avg_loss:0.066, val_acc:0.965]
Epoch [116/120    avg_loss:0.067, val_acc:0.966]
Epoch [117/120    avg_loss:0.053, val_acc:0.964]
Epoch [118/120    avg_loss:0.056, val_acc:0.965]
Epoch [119/120    avg_loss:0.059, val_acc:0.965]
Epoch [120/120    avg_loss:0.050, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1214   16    2    4    0    0    0    0   11   38    0    0
     0    0    0]
 [   0    0    2  723    1    6    1    0    0    0    4    4    6    0
     0    0    0]
 [   0    0    0    7  203    0    0    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0  428    1    0    0    1    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    2    0    0    0  836   29    0    0
     0    1    0]
 [   0    0   31    1    5    3    2    0    0    0   58 2095   14    0
     0    1    0]
 [   0    0    0    2    3    0    0    0    0    0    2    0  523    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1123   15    0]
 [   0    0    0    0    0    0   28    0    0    0    0    0    0    0
    20  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.23848238482385

F1 scores:
[       nan 0.98765432 0.956282   0.96593186 0.95081967 0.97716895
 0.97253155 1.         1.         0.94444444 0.93564633 0.95640265
 0.96941613 1.         0.98207258 0.89924812 0.97619048]

Kappa:
0.9571502347713213
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f110e56ea90>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.777, val_acc:0.210]
Epoch [2/120    avg_loss:2.617, val_acc:0.407]
Epoch [3/120    avg_loss:2.466, val_acc:0.436]
Epoch [4/120    avg_loss:2.326, val_acc:0.492]
Epoch [5/120    avg_loss:2.220, val_acc:0.535]
Epoch [6/120    avg_loss:2.078, val_acc:0.593]
Epoch [7/120    avg_loss:1.923, val_acc:0.588]
Epoch [8/120    avg_loss:1.849, val_acc:0.571]
Epoch [9/120    avg_loss:1.712, val_acc:0.584]
Epoch [10/120    avg_loss:1.581, val_acc:0.617]
Epoch [11/120    avg_loss:1.440, val_acc:0.607]
Epoch [12/120    avg_loss:1.375, val_acc:0.640]
Epoch [13/120    avg_loss:1.213, val_acc:0.682]
Epoch [14/120    avg_loss:1.062, val_acc:0.705]
Epoch [15/120    avg_loss:0.965, val_acc:0.695]
Epoch [16/120    avg_loss:0.963, val_acc:0.703]
Epoch [17/120    avg_loss:0.787, val_acc:0.766]
Epoch [18/120    avg_loss:0.745, val_acc:0.746]
Epoch [19/120    avg_loss:0.654, val_acc:0.795]
Epoch [20/120    avg_loss:0.627, val_acc:0.683]
Epoch [21/120    avg_loss:0.670, val_acc:0.778]
Epoch [22/120    avg_loss:0.542, val_acc:0.808]
Epoch [23/120    avg_loss:0.430, val_acc:0.842]
Epoch [24/120    avg_loss:0.360, val_acc:0.849]
Epoch [25/120    avg_loss:0.313, val_acc:0.855]
Epoch [26/120    avg_loss:0.314, val_acc:0.855]
Epoch [27/120    avg_loss:0.338, val_acc:0.873]
Epoch [28/120    avg_loss:0.280, val_acc:0.872]
Epoch [29/120    avg_loss:0.258, val_acc:0.864]
Epoch [30/120    avg_loss:0.234, val_acc:0.860]
Epoch [31/120    avg_loss:0.207, val_acc:0.887]
Epoch [32/120    avg_loss:0.202, val_acc:0.909]
Epoch [33/120    avg_loss:0.184, val_acc:0.892]
Epoch [34/120    avg_loss:0.194, val_acc:0.882]
Epoch [35/120    avg_loss:0.180, val_acc:0.886]
Epoch [36/120    avg_loss:0.159, val_acc:0.932]
Epoch [37/120    avg_loss:0.149, val_acc:0.921]
Epoch [38/120    avg_loss:0.133, val_acc:0.924]
Epoch [39/120    avg_loss:0.147, val_acc:0.939]
Epoch [40/120    avg_loss:0.125, val_acc:0.924]
Epoch [41/120    avg_loss:0.139, val_acc:0.896]
Epoch [42/120    avg_loss:0.115, val_acc:0.951]
Epoch [43/120    avg_loss:0.095, val_acc:0.945]
Epoch [44/120    avg_loss:0.089, val_acc:0.935]
Epoch [45/120    avg_loss:0.105, val_acc:0.928]
Epoch [46/120    avg_loss:0.085, val_acc:0.946]
Epoch [47/120    avg_loss:0.116, val_acc:0.948]
Epoch [48/120    avg_loss:0.099, val_acc:0.927]
Epoch [49/120    avg_loss:0.079, val_acc:0.951]
Epoch [50/120    avg_loss:0.085, val_acc:0.939]
Epoch [51/120    avg_loss:0.056, val_acc:0.959]
Epoch [52/120    avg_loss:0.045, val_acc:0.965]
Epoch [53/120    avg_loss:0.059, val_acc:0.928]
Epoch [54/120    avg_loss:0.065, val_acc:0.966]
Epoch [55/120    avg_loss:0.077, val_acc:0.942]
Epoch [56/120    avg_loss:0.061, val_acc:0.957]
Epoch [57/120    avg_loss:0.073, val_acc:0.957]
Epoch [58/120    avg_loss:0.078, val_acc:0.958]
Epoch [59/120    avg_loss:0.067, val_acc:0.957]
Epoch [60/120    avg_loss:0.057, val_acc:0.952]
Epoch [61/120    avg_loss:0.065, val_acc:0.965]
Epoch [62/120    avg_loss:0.057, val_acc:0.963]
Epoch [63/120    avg_loss:0.092, val_acc:0.935]
Epoch [64/120    avg_loss:0.076, val_acc:0.953]
Epoch [65/120    avg_loss:0.053, val_acc:0.948]
Epoch [66/120    avg_loss:0.051, val_acc:0.959]
Epoch [67/120    avg_loss:0.048, val_acc:0.966]
Epoch [68/120    avg_loss:0.034, val_acc:0.960]
Epoch [69/120    avg_loss:0.034, val_acc:0.969]
Epoch [70/120    avg_loss:0.040, val_acc:0.968]
Epoch [71/120    avg_loss:0.033, val_acc:0.976]
Epoch [72/120    avg_loss:0.043, val_acc:0.959]
Epoch [73/120    avg_loss:0.064, val_acc:0.955]
Epoch [74/120    avg_loss:0.055, val_acc:0.956]
Epoch [75/120    avg_loss:0.039, val_acc:0.965]
Epoch [76/120    avg_loss:0.065, val_acc:0.944]
Epoch [77/120    avg_loss:1.276, val_acc:0.408]
Epoch [78/120    avg_loss:1.473, val_acc:0.562]
Epoch [79/120    avg_loss:1.039, val_acc:0.649]
Epoch [80/120    avg_loss:0.655, val_acc:0.703]
Epoch [81/120    avg_loss:0.483, val_acc:0.840]
Epoch [82/120    avg_loss:0.348, val_acc:0.902]
Epoch [83/120    avg_loss:0.323, val_acc:0.864]
Epoch [84/120    avg_loss:0.256, val_acc:0.914]
Epoch [85/120    avg_loss:0.158, val_acc:0.922]
Epoch [86/120    avg_loss:0.127, val_acc:0.925]
Epoch [87/120    avg_loss:0.106, val_acc:0.932]
Epoch [88/120    avg_loss:0.122, val_acc:0.938]
Epoch [89/120    avg_loss:0.101, val_acc:0.936]
Epoch [90/120    avg_loss:0.119, val_acc:0.940]
Epoch [91/120    avg_loss:0.099, val_acc:0.942]
Epoch [92/120    avg_loss:0.094, val_acc:0.949]
Epoch [93/120    avg_loss:0.082, val_acc:0.947]
Epoch [94/120    avg_loss:0.095, val_acc:0.943]
Epoch [95/120    avg_loss:0.090, val_acc:0.947]
Epoch [96/120    avg_loss:0.066, val_acc:0.946]
Epoch [97/120    avg_loss:0.086, val_acc:0.947]
Epoch [98/120    avg_loss:0.068, val_acc:0.947]
Epoch [99/120    avg_loss:0.069, val_acc:0.945]
Epoch [100/120    avg_loss:0.067, val_acc:0.946]
Epoch [101/120    avg_loss:0.067, val_acc:0.949]
Epoch [102/120    avg_loss:0.083, val_acc:0.948]
Epoch [103/120    avg_loss:0.078, val_acc:0.948]
Epoch [104/120    avg_loss:0.069, val_acc:0.947]
Epoch [105/120    avg_loss:0.073, val_acc:0.950]
Epoch [106/120    avg_loss:0.074, val_acc:0.949]
Epoch [107/120    avg_loss:0.071, val_acc:0.951]
Epoch [108/120    avg_loss:0.069, val_acc:0.948]
Epoch [109/120    avg_loss:0.065, val_acc:0.950]
Epoch [110/120    avg_loss:0.067, val_acc:0.950]
Epoch [111/120    avg_loss:0.075, val_acc:0.950]
Epoch [112/120    avg_loss:0.074, val_acc:0.949]
Epoch [113/120    avg_loss:0.070, val_acc:0.948]
Epoch [114/120    avg_loss:0.064, val_acc:0.948]
Epoch [115/120    avg_loss:0.064, val_acc:0.949]
Epoch [116/120    avg_loss:0.071, val_acc:0.948]
Epoch [117/120    avg_loss:0.068, val_acc:0.948]
Epoch [118/120    avg_loss:0.071, val_acc:0.948]
Epoch [119/120    avg_loss:0.070, val_acc:0.948]
Epoch [120/120    avg_loss:0.063, val_acc:0.948]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    3    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1197    4    8    0    1    0    0    0   27   48    0    0
     0    0    0]
 [   0    0    0  718    2    1    0    0    0    1    1    3   17    4
     0    0    0]
 [   0    0    0    0  208    0    0    0    0    0    0    0    5    0
     0    0    0]
 [   0    0    0    0    0  421    0    0    0    0    1    1    0    3
     9    0    0]
 [   0    0    1    1    0    0  646    0    0    0    0    2    0    0
     7    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0    5    0    0    1    1    0    0    0  836   30    2    0
     0    0    0]
 [   0    0   35    0    0    0    1    0    0    0   44 2099   30    0
     0    0    1]
 [   0    0    0    3    0    0    0    0    0    0    1    0  527    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    2    0    0    0    0
  1120   17    0]
 [   0    0    0    0    0    1    3    0    0    0    0    0    0    0
    70  273    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
95.58807588075881

F1 scores:
[       nan 0.9382716  0.94887039 0.97421981 0.96519722 0.97679814
 0.98701299 1.         0.997669   0.86486486 0.93669468 0.9556112
 0.94191242 0.98143236 0.95522388 0.85579937 0.96428571]

Kappa:
0.9497172726148863
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb884aceac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.756, val_acc:0.218]
Epoch [2/120    avg_loss:2.573, val_acc:0.457]
Epoch [3/120    avg_loss:2.416, val_acc:0.501]
Epoch [4/120    avg_loss:2.333, val_acc:0.505]
Epoch [5/120    avg_loss:2.212, val_acc:0.514]
Epoch [6/120    avg_loss:2.142, val_acc:0.520]
Epoch [7/120    avg_loss:2.016, val_acc:0.515]
Epoch [8/120    avg_loss:1.922, val_acc:0.556]
Epoch [9/120    avg_loss:1.788, val_acc:0.561]
Epoch [10/120    avg_loss:1.740, val_acc:0.544]
Epoch [11/120    avg_loss:1.621, val_acc:0.604]
Epoch [12/120    avg_loss:1.526, val_acc:0.611]
Epoch [13/120    avg_loss:1.454, val_acc:0.623]
Epoch [14/120    avg_loss:1.323, val_acc:0.627]
Epoch [15/120    avg_loss:1.225, val_acc:0.686]
Epoch [16/120    avg_loss:1.127, val_acc:0.699]
Epoch [17/120    avg_loss:1.008, val_acc:0.711]
Epoch [18/120    avg_loss:0.940, val_acc:0.734]
Epoch [19/120    avg_loss:0.863, val_acc:0.796]
Epoch [20/120    avg_loss:0.783, val_acc:0.791]
Epoch [21/120    avg_loss:0.754, val_acc:0.801]
Epoch [22/120    avg_loss:0.680, val_acc:0.768]
Epoch [23/120    avg_loss:0.549, val_acc:0.840]
Epoch [24/120    avg_loss:0.567, val_acc:0.798]
Epoch [25/120    avg_loss:0.500, val_acc:0.802]
Epoch [26/120    avg_loss:0.522, val_acc:0.785]
Epoch [27/120    avg_loss:0.544, val_acc:0.820]
Epoch [28/120    avg_loss:0.435, val_acc:0.822]
Epoch [29/120    avg_loss:0.385, val_acc:0.818]
Epoch [30/120    avg_loss:0.479, val_acc:0.800]
Epoch [31/120    avg_loss:0.441, val_acc:0.852]
Epoch [32/120    avg_loss:0.305, val_acc:0.884]
Epoch [33/120    avg_loss:0.258, val_acc:0.874]
Epoch [34/120    avg_loss:0.247, val_acc:0.917]
Epoch [35/120    avg_loss:0.215, val_acc:0.903]
Epoch [36/120    avg_loss:0.217, val_acc:0.900]
Epoch [37/120    avg_loss:0.212, val_acc:0.914]
Epoch [38/120    avg_loss:0.260, val_acc:0.912]
Epoch [39/120    avg_loss:0.194, val_acc:0.922]
Epoch [40/120    avg_loss:0.146, val_acc:0.934]
Epoch [41/120    avg_loss:0.129, val_acc:0.932]
Epoch [42/120    avg_loss:0.164, val_acc:0.899]
Epoch [43/120    avg_loss:0.186, val_acc:0.936]
Epoch [44/120    avg_loss:0.115, val_acc:0.939]
Epoch [45/120    avg_loss:0.096, val_acc:0.954]
Epoch [46/120    avg_loss:0.092, val_acc:0.940]
Epoch [47/120    avg_loss:0.114, val_acc:0.945]
Epoch [48/120    avg_loss:0.094, val_acc:0.947]
Epoch [49/120    avg_loss:0.089, val_acc:0.947]
Epoch [50/120    avg_loss:0.080, val_acc:0.948]
Epoch [51/120    avg_loss:0.072, val_acc:0.953]
Epoch [52/120    avg_loss:0.088, val_acc:0.954]
Epoch [53/120    avg_loss:0.088, val_acc:0.942]
Epoch [54/120    avg_loss:0.083, val_acc:0.943]
Epoch [55/120    avg_loss:0.080, val_acc:0.965]
Epoch [56/120    avg_loss:0.056, val_acc:0.963]
Epoch [57/120    avg_loss:0.059, val_acc:0.960]
Epoch [58/120    avg_loss:0.052, val_acc:0.953]
Epoch [59/120    avg_loss:0.079, val_acc:0.955]
Epoch [60/120    avg_loss:0.071, val_acc:0.954]
Epoch [61/120    avg_loss:0.046, val_acc:0.958]
Epoch [62/120    avg_loss:0.041, val_acc:0.957]
Epoch [63/120    avg_loss:0.045, val_acc:0.960]
Epoch [64/120    avg_loss:0.064, val_acc:0.946]
Epoch [65/120    avg_loss:0.055, val_acc:0.946]
Epoch [66/120    avg_loss:0.060, val_acc:0.949]
Epoch [67/120    avg_loss:0.049, val_acc:0.955]
Epoch [68/120    avg_loss:0.038, val_acc:0.967]
Epoch [69/120    avg_loss:0.031, val_acc:0.963]
Epoch [70/120    avg_loss:0.035, val_acc:0.960]
Epoch [71/120    avg_loss:0.036, val_acc:0.955]
Epoch [72/120    avg_loss:0.039, val_acc:0.965]
Epoch [73/120    avg_loss:0.040, val_acc:0.960]
Epoch [74/120    avg_loss:0.035, val_acc:0.967]
Epoch [75/120    avg_loss:0.046, val_acc:0.948]
Epoch [76/120    avg_loss:0.066, val_acc:0.959]
Epoch [77/120    avg_loss:0.063, val_acc:0.956]
Epoch [78/120    avg_loss:0.049, val_acc:0.951]
Epoch [79/120    avg_loss:0.037, val_acc:0.963]
Epoch [80/120    avg_loss:0.024, val_acc:0.967]
Epoch [81/120    avg_loss:0.025, val_acc:0.958]
Epoch [82/120    avg_loss:0.025, val_acc:0.970]
Epoch [83/120    avg_loss:0.040, val_acc:0.958]
Epoch [84/120    avg_loss:0.028, val_acc:0.952]
Epoch [85/120    avg_loss:0.060, val_acc:0.949]
Epoch [86/120    avg_loss:0.047, val_acc:0.960]
Epoch [87/120    avg_loss:0.038, val_acc:0.956]
Epoch [88/120    avg_loss:0.031, val_acc:0.969]
Epoch [89/120    avg_loss:0.026, val_acc:0.968]
Epoch [90/120    avg_loss:0.018, val_acc:0.965]
Epoch [91/120    avg_loss:0.020, val_acc:0.966]
Epoch [92/120    avg_loss:0.021, val_acc:0.960]
Epoch [93/120    avg_loss:0.017, val_acc:0.967]
Epoch [94/120    avg_loss:0.037, val_acc:0.961]
Epoch [95/120    avg_loss:0.070, val_acc:0.950]
Epoch [96/120    avg_loss:0.032, val_acc:0.968]
Epoch [97/120    avg_loss:0.025, val_acc:0.969]
Epoch [98/120    avg_loss:0.021, val_acc:0.968]
Epoch [99/120    avg_loss:0.018, val_acc:0.970]
Epoch [100/120    avg_loss:0.015, val_acc:0.970]
Epoch [101/120    avg_loss:0.015, val_acc:0.971]
Epoch [102/120    avg_loss:0.016, val_acc:0.970]
Epoch [103/120    avg_loss:0.017, val_acc:0.971]
Epoch [104/120    avg_loss:0.018, val_acc:0.971]
Epoch [105/120    avg_loss:0.011, val_acc:0.971]
Epoch [106/120    avg_loss:0.017, val_acc:0.972]
Epoch [107/120    avg_loss:0.013, val_acc:0.972]
Epoch [108/120    avg_loss:0.013, val_acc:0.972]
Epoch [109/120    avg_loss:0.014, val_acc:0.972]
Epoch [110/120    avg_loss:0.015, val_acc:0.973]
Epoch [111/120    avg_loss:0.013, val_acc:0.973]
Epoch [112/120    avg_loss:0.011, val_acc:0.972]
Epoch [113/120    avg_loss:0.019, val_acc:0.973]
Epoch [114/120    avg_loss:0.013, val_acc:0.974]
Epoch [115/120    avg_loss:0.011, val_acc:0.972]
Epoch [116/120    avg_loss:0.012, val_acc:0.973]
Epoch [117/120    avg_loss:0.013, val_acc:0.973]
Epoch [118/120    avg_loss:0.012, val_acc:0.974]
Epoch [119/120    avg_loss:0.013, val_acc:0.977]
Epoch [120/120    avg_loss:0.012, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    0    2    0    0
     0    0    0]
 [   0    0 1248    0    0    2    0    0    0    1    2   32    0    0
     0    0    0]
 [   0    0    3  720    4    0    0    0    0    3    1   15    1    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    1    0    0    0  837   30    1    0
     0    0    0]
 [   0    0   15    0    0    0    0    0    0    1   20 2161   12    0
     0    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    5  519    0
     2    6    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    92  255    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
96.91056910569105

F1 scores:
[       nan 0.975      0.97614392 0.98159509 0.99069767 0.99425947
 0.99695122 0.96       1.         0.87804878 0.9648415  0.96971057
 0.96918768 1.         0.95294616 0.81730769 0.96385542]

Kappa:
0.9647298856917612
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe950917a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.779, val_acc:0.194]
Epoch [2/120    avg_loss:2.582, val_acc:0.362]
Epoch [3/120    avg_loss:2.452, val_acc:0.380]
Epoch [4/120    avg_loss:2.312, val_acc:0.411]
Epoch [5/120    avg_loss:2.234, val_acc:0.484]
Epoch [6/120    avg_loss:2.113, val_acc:0.489]
Epoch [7/120    avg_loss:1.983, val_acc:0.540]
Epoch [8/120    avg_loss:1.912, val_acc:0.559]
Epoch [9/120    avg_loss:1.798, val_acc:0.580]
Epoch [10/120    avg_loss:1.728, val_acc:0.575]
Epoch [11/120    avg_loss:1.632, val_acc:0.622]
Epoch [12/120    avg_loss:1.546, val_acc:0.641]
Epoch [13/120    avg_loss:1.407, val_acc:0.661]
Epoch [14/120    avg_loss:1.328, val_acc:0.662]
Epoch [15/120    avg_loss:1.227, val_acc:0.700]
Epoch [16/120    avg_loss:1.134, val_acc:0.672]
Epoch [17/120    avg_loss:1.012, val_acc:0.740]
Epoch [18/120    avg_loss:0.898, val_acc:0.743]
Epoch [19/120    avg_loss:0.805, val_acc:0.785]
Epoch [20/120    avg_loss:0.721, val_acc:0.800]
Epoch [21/120    avg_loss:0.647, val_acc:0.817]
Epoch [22/120    avg_loss:0.599, val_acc:0.818]
Epoch [23/120    avg_loss:0.534, val_acc:0.816]
Epoch [24/120    avg_loss:0.454, val_acc:0.870]
Epoch [25/120    avg_loss:0.486, val_acc:0.829]
Epoch [26/120    avg_loss:0.531, val_acc:0.816]
Epoch [27/120    avg_loss:0.441, val_acc:0.858]
Epoch [28/120    avg_loss:0.347, val_acc:0.875]
Epoch [29/120    avg_loss:0.366, val_acc:0.856]
Epoch [30/120    avg_loss:0.317, val_acc:0.882]
Epoch [31/120    avg_loss:0.252, val_acc:0.908]
Epoch [32/120    avg_loss:0.254, val_acc:0.898]
Epoch [33/120    avg_loss:0.202, val_acc:0.920]
Epoch [34/120    avg_loss:0.210, val_acc:0.893]
Epoch [35/120    avg_loss:0.160, val_acc:0.923]
Epoch [36/120    avg_loss:0.220, val_acc:0.916]
Epoch [37/120    avg_loss:0.207, val_acc:0.909]
Epoch [38/120    avg_loss:0.201, val_acc:0.905]
Epoch [39/120    avg_loss:0.200, val_acc:0.897]
Epoch [40/120    avg_loss:0.207, val_acc:0.891]
Epoch [41/120    avg_loss:0.203, val_acc:0.938]
Epoch [42/120    avg_loss:0.185, val_acc:0.923]
Epoch [43/120    avg_loss:0.212, val_acc:0.908]
Epoch [44/120    avg_loss:0.166, val_acc:0.916]
Epoch [45/120    avg_loss:0.157, val_acc:0.932]
Epoch [46/120    avg_loss:0.142, val_acc:0.923]
Epoch [47/120    avg_loss:0.131, val_acc:0.925]
Epoch [48/120    avg_loss:0.105, val_acc:0.933]
Epoch [49/120    avg_loss:0.086, val_acc:0.949]
Epoch [50/120    avg_loss:0.166, val_acc:0.908]
Epoch [51/120    avg_loss:0.131, val_acc:0.946]
Epoch [52/120    avg_loss:0.085, val_acc:0.938]
Epoch [53/120    avg_loss:0.136, val_acc:0.916]
Epoch [54/120    avg_loss:0.122, val_acc:0.949]
Epoch [55/120    avg_loss:0.093, val_acc:0.947]
Epoch [56/120    avg_loss:0.074, val_acc:0.960]
Epoch [57/120    avg_loss:0.070, val_acc:0.957]
Epoch [58/120    avg_loss:0.045, val_acc:0.967]
Epoch [59/120    avg_loss:0.062, val_acc:0.966]
Epoch [60/120    avg_loss:0.065, val_acc:0.960]
Epoch [61/120    avg_loss:0.039, val_acc:0.967]
Epoch [62/120    avg_loss:0.044, val_acc:0.968]
Epoch [63/120    avg_loss:0.057, val_acc:0.968]
Epoch [64/120    avg_loss:0.044, val_acc:0.970]
Epoch [65/120    avg_loss:0.039, val_acc:0.971]
Epoch [66/120    avg_loss:0.041, val_acc:0.963]
Epoch [67/120    avg_loss:0.035, val_acc:0.969]
Epoch [68/120    avg_loss:0.039, val_acc:0.966]
Epoch [69/120    avg_loss:0.028, val_acc:0.970]
Epoch [70/120    avg_loss:0.028, val_acc:0.973]
Epoch [71/120    avg_loss:0.033, val_acc:0.968]
Epoch [72/120    avg_loss:0.026, val_acc:0.973]
Epoch [73/120    avg_loss:0.044, val_acc:0.958]
Epoch [74/120    avg_loss:0.040, val_acc:0.967]
Epoch [75/120    avg_loss:0.036, val_acc:0.967]
Epoch [76/120    avg_loss:0.024, val_acc:0.972]
Epoch [77/120    avg_loss:0.020, val_acc:0.969]
Epoch [78/120    avg_loss:0.022, val_acc:0.973]
Epoch [79/120    avg_loss:0.024, val_acc:0.976]
Epoch [80/120    avg_loss:0.017, val_acc:0.972]
Epoch [81/120    avg_loss:0.020, val_acc:0.977]
Epoch [82/120    avg_loss:0.023, val_acc:0.969]
Epoch [83/120    avg_loss:0.045, val_acc:0.843]
Epoch [84/120    avg_loss:0.119, val_acc:0.954]
Epoch [85/120    avg_loss:0.054, val_acc:0.964]
Epoch [86/120    avg_loss:0.038, val_acc:0.952]
Epoch [87/120    avg_loss:0.040, val_acc:0.965]
Epoch [88/120    avg_loss:0.056, val_acc:0.882]
Epoch [89/120    avg_loss:0.064, val_acc:0.963]
Epoch [90/120    avg_loss:0.052, val_acc:0.970]
Epoch [91/120    avg_loss:0.034, val_acc:0.972]
Epoch [92/120    avg_loss:0.033, val_acc:0.977]
Epoch [93/120    avg_loss:0.025, val_acc:0.973]
Epoch [94/120    avg_loss:0.020, val_acc:0.976]
Epoch [95/120    avg_loss:0.082, val_acc:0.945]
Epoch [96/120    avg_loss:0.093, val_acc:0.935]
Epoch [97/120    avg_loss:0.067, val_acc:0.955]
Epoch [98/120    avg_loss:0.053, val_acc:0.966]
Epoch [99/120    avg_loss:0.028, val_acc:0.976]
Epoch [100/120    avg_loss:0.017, val_acc:0.970]
Epoch [101/120    avg_loss:0.015, val_acc:0.969]
Epoch [102/120    avg_loss:0.058, val_acc:0.954]
Epoch [103/120    avg_loss:0.084, val_acc:0.956]
Epoch [104/120    avg_loss:0.047, val_acc:0.961]
Epoch [105/120    avg_loss:0.029, val_acc:0.974]
Epoch [106/120    avg_loss:0.017, val_acc:0.977]
Epoch [107/120    avg_loss:0.015, val_acc:0.979]
Epoch [108/120    avg_loss:0.016, val_acc:0.979]
Epoch [109/120    avg_loss:0.010, val_acc:0.980]
Epoch [110/120    avg_loss:0.013, val_acc:0.979]
Epoch [111/120    avg_loss:0.012, val_acc:0.979]
Epoch [112/120    avg_loss:0.014, val_acc:0.978]
Epoch [113/120    avg_loss:0.013, val_acc:0.981]
Epoch [114/120    avg_loss:0.013, val_acc:0.980]
Epoch [115/120    avg_loss:0.012, val_acc:0.980]
Epoch [116/120    avg_loss:0.016, val_acc:0.982]
Epoch [117/120    avg_loss:0.013, val_acc:0.981]
Epoch [118/120    avg_loss:0.011, val_acc:0.981]
Epoch [119/120    avg_loss:0.012, val_acc:0.981]
Epoch [120/120    avg_loss:0.011, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1264    5    4    0    0    0    0    0    1   11    0    0
     0    0    0]
 [   0    0    1  717    5    0    0    0    0    7    1   12    4    0
     0    0    0]
 [   0    0    0    0  210    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0  851   19    0    0
     2    1    0]
 [   0    0   10    1    0    0    0    0    0    0   25 2161   13    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0    1  525    0
     0    4    2]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    46  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.82113821138212

F1 scores:
[       nan 0.95348837 0.98672912 0.97418478 0.97222222 0.99884925
 0.99923839 1.         0.9953271  0.81818182 0.97090702 0.97915723
 0.97222222 0.99728997 0.97367285 0.90662651 0.98224852]

Kappa:
0.9751584172140765
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f72a67e3a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.816, val_acc:0.156]
Epoch [2/120    avg_loss:2.656, val_acc:0.386]
Epoch [3/120    avg_loss:2.487, val_acc:0.468]
Epoch [4/120    avg_loss:2.334, val_acc:0.529]
Epoch [5/120    avg_loss:2.244, val_acc:0.440]
Epoch [6/120    avg_loss:2.147, val_acc:0.468]
Epoch [7/120    avg_loss:2.041, val_acc:0.507]
Epoch [8/120    avg_loss:1.928, val_acc:0.554]
Epoch [9/120    avg_loss:1.835, val_acc:0.559]
Epoch [10/120    avg_loss:1.769, val_acc:0.564]
Epoch [11/120    avg_loss:1.607, val_acc:0.598]
Epoch [12/120    avg_loss:1.591, val_acc:0.601]
Epoch [13/120    avg_loss:1.470, val_acc:0.615]
Epoch [14/120    avg_loss:1.428, val_acc:0.642]
Epoch [15/120    avg_loss:1.341, val_acc:0.656]
Epoch [16/120    avg_loss:1.171, val_acc:0.673]
Epoch [17/120    avg_loss:1.093, val_acc:0.710]
Epoch [18/120    avg_loss:0.985, val_acc:0.735]
Epoch [19/120    avg_loss:0.863, val_acc:0.754]
Epoch [20/120    avg_loss:0.821, val_acc:0.780]
Epoch [21/120    avg_loss:0.727, val_acc:0.717]
Epoch [22/120    avg_loss:0.727, val_acc:0.749]
Epoch [23/120    avg_loss:0.674, val_acc:0.761]
Epoch [24/120    avg_loss:0.646, val_acc:0.791]
Epoch [25/120    avg_loss:0.561, val_acc:0.823]
Epoch [26/120    avg_loss:0.510, val_acc:0.797]
Epoch [27/120    avg_loss:0.540, val_acc:0.802]
Epoch [28/120    avg_loss:0.444, val_acc:0.829]
Epoch [29/120    avg_loss:0.368, val_acc:0.841]
Epoch [30/120    avg_loss:0.320, val_acc:0.868]
Epoch [31/120    avg_loss:0.327, val_acc:0.875]
Epoch [32/120    avg_loss:0.263, val_acc:0.868]
Epoch [33/120    avg_loss:0.247, val_acc:0.859]
Epoch [34/120    avg_loss:0.225, val_acc:0.879]
Epoch [35/120    avg_loss:0.233, val_acc:0.877]
Epoch [36/120    avg_loss:0.191, val_acc:0.900]
Epoch [37/120    avg_loss:0.178, val_acc:0.896]
Epoch [38/120    avg_loss:0.215, val_acc:0.883]
Epoch [39/120    avg_loss:0.189, val_acc:0.892]
Epoch [40/120    avg_loss:0.145, val_acc:0.902]
Epoch [41/120    avg_loss:0.134, val_acc:0.906]
Epoch [42/120    avg_loss:0.129, val_acc:0.919]
Epoch [43/120    avg_loss:0.190, val_acc:0.910]
Epoch [44/120    avg_loss:0.137, val_acc:0.886]
Epoch [45/120    avg_loss:0.137, val_acc:0.912]
Epoch [46/120    avg_loss:0.110, val_acc:0.917]
Epoch [47/120    avg_loss:0.081, val_acc:0.925]
Epoch [48/120    avg_loss:0.080, val_acc:0.938]
Epoch [49/120    avg_loss:0.094, val_acc:0.938]
Epoch [50/120    avg_loss:0.082, val_acc:0.893]
Epoch [51/120    avg_loss:0.099, val_acc:0.935]
Epoch [52/120    avg_loss:0.074, val_acc:0.923]
Epoch [53/120    avg_loss:0.054, val_acc:0.948]
Epoch [54/120    avg_loss:0.061, val_acc:0.938]
Epoch [55/120    avg_loss:0.070, val_acc:0.941]
Epoch [56/120    avg_loss:0.062, val_acc:0.942]
Epoch [57/120    avg_loss:0.077, val_acc:0.936]
Epoch [58/120    avg_loss:0.077, val_acc:0.941]
Epoch [59/120    avg_loss:0.095, val_acc:0.947]
Epoch [60/120    avg_loss:0.067, val_acc:0.944]
Epoch [61/120    avg_loss:0.063, val_acc:0.938]
Epoch [62/120    avg_loss:0.080, val_acc:0.932]
Epoch [63/120    avg_loss:0.091, val_acc:0.924]
Epoch [64/120    avg_loss:0.099, val_acc:0.936]
Epoch [65/120    avg_loss:0.087, val_acc:0.927]
Epoch [66/120    avg_loss:0.106, val_acc:0.898]
Epoch [67/120    avg_loss:0.059, val_acc:0.935]
Epoch [68/120    avg_loss:0.047, val_acc:0.947]
Epoch [69/120    avg_loss:0.043, val_acc:0.945]
Epoch [70/120    avg_loss:0.042, val_acc:0.946]
Epoch [71/120    avg_loss:0.044, val_acc:0.951]
Epoch [72/120    avg_loss:0.048, val_acc:0.952]
Epoch [73/120    avg_loss:0.038, val_acc:0.956]
Epoch [74/120    avg_loss:0.039, val_acc:0.957]
Epoch [75/120    avg_loss:0.035, val_acc:0.956]
Epoch [76/120    avg_loss:0.031, val_acc:0.958]
Epoch [77/120    avg_loss:0.037, val_acc:0.960]
Epoch [78/120    avg_loss:0.040, val_acc:0.961]
Epoch [79/120    avg_loss:0.036, val_acc:0.956]
Epoch [80/120    avg_loss:0.035, val_acc:0.954]
Epoch [81/120    avg_loss:0.028, val_acc:0.956]
Epoch [82/120    avg_loss:0.027, val_acc:0.959]
Epoch [83/120    avg_loss:0.031, val_acc:0.960]
Epoch [84/120    avg_loss:0.030, val_acc:0.960]
Epoch [85/120    avg_loss:0.031, val_acc:0.956]
Epoch [86/120    avg_loss:0.025, val_acc:0.958]
Epoch [87/120    avg_loss:0.027, val_acc:0.961]
Epoch [88/120    avg_loss:0.028, val_acc:0.960]
Epoch [89/120    avg_loss:0.029, val_acc:0.959]
Epoch [90/120    avg_loss:0.025, val_acc:0.963]
Epoch [91/120    avg_loss:0.025, val_acc:0.961]
Epoch [92/120    avg_loss:0.029, val_acc:0.961]
Epoch [93/120    avg_loss:0.034, val_acc:0.963]
Epoch [94/120    avg_loss:0.024, val_acc:0.966]
Epoch [95/120    avg_loss:0.022, val_acc:0.965]
Epoch [96/120    avg_loss:0.022, val_acc:0.961]
Epoch [97/120    avg_loss:0.026, val_acc:0.966]
Epoch [98/120    avg_loss:0.024, val_acc:0.965]
Epoch [99/120    avg_loss:0.024, val_acc:0.964]
Epoch [100/120    avg_loss:0.023, val_acc:0.963]
Epoch [101/120    avg_loss:0.023, val_acc:0.963]
Epoch [102/120    avg_loss:0.024, val_acc:0.961]
Epoch [103/120    avg_loss:0.028, val_acc:0.960]
Epoch [104/120    avg_loss:0.022, val_acc:0.961]
Epoch [105/120    avg_loss:0.025, val_acc:0.963]
Epoch [106/120    avg_loss:0.025, val_acc:0.965]
Epoch [107/120    avg_loss:0.026, val_acc:0.964]
Epoch [108/120    avg_loss:0.020, val_acc:0.965]
Epoch [109/120    avg_loss:0.021, val_acc:0.963]
Epoch [110/120    avg_loss:0.021, val_acc:0.961]
Epoch [111/120    avg_loss:0.024, val_acc:0.961]
Epoch [112/120    avg_loss:0.021, val_acc:0.963]
Epoch [113/120    avg_loss:0.023, val_acc:0.965]
Epoch [114/120    avg_loss:0.023, val_acc:0.963]
Epoch [115/120    avg_loss:0.022, val_acc:0.964]
Epoch [116/120    avg_loss:0.021, val_acc:0.964]
Epoch [117/120    avg_loss:0.017, val_acc:0.965]
Epoch [118/120    avg_loss:0.023, val_acc:0.964]
Epoch [119/120    avg_loss:0.024, val_acc:0.964]
Epoch [120/120    avg_loss:0.025, val_acc:0.963]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1234    7    1    0    4    0    0    1   13   21    4    0
     0    0    0]
 [   0    0    1  704    4    3    0    0    0   16    2    7   10    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    7    0    2    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    2    0    0    1    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    2  853   15    0    0
     0    3    0]
 [   0    0   21    0    0    1    1    0    0    2   17 2154   12    2
     0    0    0]
 [   0    0    0    1    0    2    0    0    0    0    7    7  513    0
     1    1    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0   14    0    0    0    0    0    1    0    0
  1121    3    0]
 [   0    0    0    0    0    0   18    0    0    0    0    0    1    0
    22  306    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
96.99728997289972

F1 scores:
[       nan 0.975      0.97050727 0.96372348 0.98839907 0.96481271
 0.98053892 0.87719298 0.997669   0.53571429 0.96438666 0.97532262
 0.94912118 0.99191375 0.98161121 0.92727273 0.95757576]

Kappa:
0.9657790066484608
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5de8d70a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.802, val_acc:0.252]
Epoch [2/120    avg_loss:2.651, val_acc:0.307]
Epoch [3/120    avg_loss:2.510, val_acc:0.370]
Epoch [4/120    avg_loss:2.396, val_acc:0.428]
Epoch [5/120    avg_loss:2.247, val_acc:0.503]
Epoch [6/120    avg_loss:2.166, val_acc:0.518]
Epoch [7/120    avg_loss:2.066, val_acc:0.524]
Epoch [8/120    avg_loss:1.962, val_acc:0.525]
Epoch [9/120    avg_loss:1.903, val_acc:0.547]
Epoch [10/120    avg_loss:1.762, val_acc:0.566]
Epoch [11/120    avg_loss:1.701, val_acc:0.590]
Epoch [12/120    avg_loss:1.574, val_acc:0.629]
Epoch [13/120    avg_loss:1.463, val_acc:0.620]
Epoch [14/120    avg_loss:1.337, val_acc:0.649]
Epoch [15/120    avg_loss:1.227, val_acc:0.654]
Epoch [16/120    avg_loss:1.191, val_acc:0.698]
Epoch [17/120    avg_loss:1.094, val_acc:0.704]
Epoch [18/120    avg_loss:0.978, val_acc:0.679]
Epoch [19/120    avg_loss:0.923, val_acc:0.722]
Epoch [20/120    avg_loss:0.830, val_acc:0.741]
Epoch [21/120    avg_loss:0.715, val_acc:0.792]
Epoch [22/120    avg_loss:0.650, val_acc:0.816]
Epoch [23/120    avg_loss:0.605, val_acc:0.768]
Epoch [24/120    avg_loss:0.566, val_acc:0.804]
Epoch [25/120    avg_loss:0.519, val_acc:0.797]
Epoch [26/120    avg_loss:0.463, val_acc:0.810]
Epoch [27/120    avg_loss:0.438, val_acc:0.838]
Epoch [28/120    avg_loss:0.470, val_acc:0.823]
Epoch [29/120    avg_loss:0.434, val_acc:0.872]
Epoch [30/120    avg_loss:0.381, val_acc:0.844]
Epoch [31/120    avg_loss:0.339, val_acc:0.839]
Epoch [32/120    avg_loss:0.291, val_acc:0.861]
Epoch [33/120    avg_loss:0.252, val_acc:0.880]
Epoch [34/120    avg_loss:0.246, val_acc:0.872]
Epoch [35/120    avg_loss:0.240, val_acc:0.897]
Epoch [36/120    avg_loss:0.205, val_acc:0.905]
Epoch [37/120    avg_loss:0.183, val_acc:0.914]
Epoch [38/120    avg_loss:0.166, val_acc:0.910]
Epoch [39/120    avg_loss:0.154, val_acc:0.911]
Epoch [40/120    avg_loss:0.170, val_acc:0.911]
Epoch [41/120    avg_loss:0.174, val_acc:0.919]
Epoch [42/120    avg_loss:0.176, val_acc:0.911]
Epoch [43/120    avg_loss:0.174, val_acc:0.917]
Epoch [44/120    avg_loss:0.139, val_acc:0.904]
Epoch [45/120    avg_loss:0.158, val_acc:0.934]
Epoch [46/120    avg_loss:0.122, val_acc:0.904]
Epoch [47/120    avg_loss:0.132, val_acc:0.926]
Epoch [48/120    avg_loss:0.101, val_acc:0.909]
Epoch [49/120    avg_loss:0.114, val_acc:0.924]
Epoch [50/120    avg_loss:0.107, val_acc:0.939]
Epoch [51/120    avg_loss:0.121, val_acc:0.897]
Epoch [52/120    avg_loss:0.172, val_acc:0.912]
Epoch [53/120    avg_loss:0.144, val_acc:0.932]
Epoch [54/120    avg_loss:0.133, val_acc:0.924]
Epoch [55/120    avg_loss:0.098, val_acc:0.919]
Epoch [56/120    avg_loss:0.085, val_acc:0.947]
Epoch [57/120    avg_loss:0.064, val_acc:0.943]
Epoch [58/120    avg_loss:0.071, val_acc:0.928]
Epoch [59/120    avg_loss:0.064, val_acc:0.942]
Epoch [60/120    avg_loss:0.065, val_acc:0.946]
Epoch [61/120    avg_loss:0.086, val_acc:0.933]
Epoch [62/120    avg_loss:0.069, val_acc:0.943]
Epoch [63/120    avg_loss:0.074, val_acc:0.943]
Epoch [64/120    avg_loss:0.054, val_acc:0.948]
Epoch [65/120    avg_loss:0.045, val_acc:0.955]
Epoch [66/120    avg_loss:0.050, val_acc:0.934]
Epoch [67/120    avg_loss:0.049, val_acc:0.949]
Epoch [68/120    avg_loss:0.037, val_acc:0.961]
Epoch [69/120    avg_loss:0.038, val_acc:0.949]
Epoch [70/120    avg_loss:0.043, val_acc:0.947]
Epoch [71/120    avg_loss:0.037, val_acc:0.955]
Epoch [72/120    avg_loss:0.030, val_acc:0.955]
Epoch [73/120    avg_loss:0.028, val_acc:0.955]
Epoch [74/120    avg_loss:0.047, val_acc:0.943]
Epoch [75/120    avg_loss:0.029, val_acc:0.958]
Epoch [76/120    avg_loss:0.026, val_acc:0.958]
Epoch [77/120    avg_loss:0.029, val_acc:0.943]
Epoch [78/120    avg_loss:0.027, val_acc:0.951]
Epoch [79/120    avg_loss:0.039, val_acc:0.956]
Epoch [80/120    avg_loss:0.030, val_acc:0.958]
Epoch [81/120    avg_loss:0.032, val_acc:0.956]
Epoch [82/120    avg_loss:0.033, val_acc:0.956]
Epoch [83/120    avg_loss:0.021, val_acc:0.958]
Epoch [84/120    avg_loss:0.024, val_acc:0.957]
Epoch [85/120    avg_loss:0.020, val_acc:0.957]
Epoch [86/120    avg_loss:0.018, val_acc:0.957]
Epoch [87/120    avg_loss:0.020, val_acc:0.958]
Epoch [88/120    avg_loss:0.020, val_acc:0.958]
Epoch [89/120    avg_loss:0.017, val_acc:0.961]
Epoch [90/120    avg_loss:0.018, val_acc:0.960]
Epoch [91/120    avg_loss:0.017, val_acc:0.960]
Epoch [92/120    avg_loss:0.021, val_acc:0.960]
Epoch [93/120    avg_loss:0.017, val_acc:0.957]
Epoch [94/120    avg_loss:0.017, val_acc:0.958]
Epoch [95/120    avg_loss:0.016, val_acc:0.964]
Epoch [96/120    avg_loss:0.016, val_acc:0.964]
Epoch [97/120    avg_loss:0.016, val_acc:0.961]
Epoch [98/120    avg_loss:0.016, val_acc:0.964]
Epoch [99/120    avg_loss:0.014, val_acc:0.964]
Epoch [100/120    avg_loss:0.017, val_acc:0.966]
Epoch [101/120    avg_loss:0.012, val_acc:0.966]
Epoch [102/120    avg_loss:0.014, val_acc:0.967]
Epoch [103/120    avg_loss:0.013, val_acc:0.969]
Epoch [104/120    avg_loss:0.012, val_acc:0.967]
Epoch [105/120    avg_loss:0.016, val_acc:0.968]
Epoch [106/120    avg_loss:0.014, val_acc:0.967]
Epoch [107/120    avg_loss:0.013, val_acc:0.968]
Epoch [108/120    avg_loss:0.012, val_acc:0.966]
Epoch [109/120    avg_loss:0.014, val_acc:0.967]
Epoch [110/120    avg_loss:0.015, val_acc:0.969]
Epoch [111/120    avg_loss:0.012, val_acc:0.969]
Epoch [112/120    avg_loss:0.017, val_acc:0.967]
Epoch [113/120    avg_loss:0.015, val_acc:0.967]
Epoch [114/120    avg_loss:0.013, val_acc:0.967]
Epoch [115/120    avg_loss:0.013, val_acc:0.967]
Epoch [116/120    avg_loss:0.012, val_acc:0.967]
Epoch [117/120    avg_loss:0.015, val_acc:0.967]
Epoch [118/120    avg_loss:0.013, val_acc:0.964]
Epoch [119/120    avg_loss:0.013, val_acc:0.965]
Epoch [120/120    avg_loss:0.015, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1267    0    3    0    0    0    0    0    2   13    0    0
     0    0    0]
 [   0    0    1  700    5    1    0    0    0    0    2   22   16    0
     0    0    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  432    0    2    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    0    1    0    0    0  858    6    1    0
     0    1    0]
 [   0    0    9    0    0    3    0    2    0    0   11 2169   16    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0    0    4    0  524    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1133    6    0]
 [   0    0    0    0    0    0   29    0    0    0    0    0    0    0
    13  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.91869918699187

F1 scores:
[       nan 0.975      0.98599222 0.96418733 0.97685185 0.99196326
 0.97691735 0.92592593 0.99883586 0.97297297 0.97833523 0.98144796
 0.95707763 1.         0.99125109 0.92564492 0.97619048]

Kappa:
0.9762700465487312
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f61f5f5cac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.789, val_acc:0.253]
Epoch [2/120    avg_loss:2.661, val_acc:0.308]
Epoch [3/120    avg_loss:2.531, val_acc:0.398]
Epoch [4/120    avg_loss:2.384, val_acc:0.454]
Epoch [5/120    avg_loss:2.274, val_acc:0.517]
Epoch [6/120    avg_loss:2.163, val_acc:0.520]
Epoch [7/120    avg_loss:2.050, val_acc:0.445]
Epoch [8/120    avg_loss:1.928, val_acc:0.555]
Epoch [9/120    avg_loss:1.806, val_acc:0.609]
Epoch [10/120    avg_loss:1.674, val_acc:0.621]
Epoch [11/120    avg_loss:1.540, val_acc:0.665]
Epoch [12/120    avg_loss:1.406, val_acc:0.683]
Epoch [13/120    avg_loss:1.326, val_acc:0.664]
Epoch [14/120    avg_loss:1.334, val_acc:0.650]
Epoch [15/120    avg_loss:1.181, val_acc:0.683]
Epoch [16/120    avg_loss:1.048, val_acc:0.691]
Epoch [17/120    avg_loss:0.953, val_acc:0.733]
Epoch [18/120    avg_loss:0.843, val_acc:0.792]
Epoch [19/120    avg_loss:0.698, val_acc:0.778]
Epoch [20/120    avg_loss:0.649, val_acc:0.793]
Epoch [21/120    avg_loss:0.623, val_acc:0.823]
Epoch [22/120    avg_loss:0.516, val_acc:0.809]
Epoch [23/120    avg_loss:0.500, val_acc:0.808]
Epoch [24/120    avg_loss:0.432, val_acc:0.851]
Epoch [25/120    avg_loss:0.367, val_acc:0.834]
Epoch [26/120    avg_loss:0.483, val_acc:0.835]
Epoch [27/120    avg_loss:0.399, val_acc:0.858]
Epoch [28/120    avg_loss:0.347, val_acc:0.851]
Epoch [29/120    avg_loss:0.520, val_acc:0.801]
Epoch [30/120    avg_loss:0.356, val_acc:0.829]
Epoch [31/120    avg_loss:0.282, val_acc:0.864]
Epoch [32/120    avg_loss:0.244, val_acc:0.883]
Epoch [33/120    avg_loss:0.209, val_acc:0.887]
Epoch [34/120    avg_loss:0.309, val_acc:0.861]
Epoch [35/120    avg_loss:0.213, val_acc:0.896]
Epoch [36/120    avg_loss:0.190, val_acc:0.895]
Epoch [37/120    avg_loss:0.200, val_acc:0.896]
Epoch [38/120    avg_loss:0.188, val_acc:0.930]
Epoch [39/120    avg_loss:0.157, val_acc:0.922]
Epoch [40/120    avg_loss:0.157, val_acc:0.930]
Epoch [41/120    avg_loss:0.131, val_acc:0.929]
Epoch [42/120    avg_loss:0.143, val_acc:0.932]
Epoch [43/120    avg_loss:0.109, val_acc:0.921]
Epoch [44/120    avg_loss:0.120, val_acc:0.933]
Epoch [45/120    avg_loss:0.098, val_acc:0.938]
Epoch [46/120    avg_loss:0.099, val_acc:0.933]
Epoch [47/120    avg_loss:0.081, val_acc:0.950]
Epoch [48/120    avg_loss:0.111, val_acc:0.926]
Epoch [49/120    avg_loss:0.100, val_acc:0.948]
Epoch [50/120    avg_loss:0.102, val_acc:0.952]
Epoch [51/120    avg_loss:0.112, val_acc:0.939]
Epoch [52/120    avg_loss:0.131, val_acc:0.940]
Epoch [53/120    avg_loss:0.117, val_acc:0.942]
Epoch [54/120    avg_loss:0.096, val_acc:0.953]
Epoch [55/120    avg_loss:0.065, val_acc:0.949]
Epoch [56/120    avg_loss:0.062, val_acc:0.954]
Epoch [57/120    avg_loss:0.074, val_acc:0.956]
Epoch [58/120    avg_loss:0.104, val_acc:0.946]
Epoch [59/120    avg_loss:0.062, val_acc:0.959]
Epoch [60/120    avg_loss:0.072, val_acc:0.946]
Epoch [61/120    avg_loss:0.120, val_acc:0.950]
Epoch [62/120    avg_loss:0.070, val_acc:0.954]
Epoch [63/120    avg_loss:0.081, val_acc:0.955]
Epoch [64/120    avg_loss:0.081, val_acc:0.943]
Epoch [65/120    avg_loss:0.073, val_acc:0.939]
Epoch [66/120    avg_loss:0.091, val_acc:0.948]
Epoch [67/120    avg_loss:0.049, val_acc:0.968]
Epoch [68/120    avg_loss:0.057, val_acc:0.958]
Epoch [69/120    avg_loss:0.046, val_acc:0.933]
Epoch [70/120    avg_loss:0.086, val_acc:0.952]
Epoch [71/120    avg_loss:0.062, val_acc:0.959]
Epoch [72/120    avg_loss:0.048, val_acc:0.965]
Epoch [73/120    avg_loss:0.046, val_acc:0.965]
Epoch [74/120    avg_loss:0.034, val_acc:0.966]
Epoch [75/120    avg_loss:0.033, val_acc:0.969]
Epoch [76/120    avg_loss:0.028, val_acc:0.967]
Epoch [77/120    avg_loss:0.031, val_acc:0.978]
Epoch [78/120    avg_loss:0.025, val_acc:0.971]
Epoch [79/120    avg_loss:0.024, val_acc:0.968]
Epoch [80/120    avg_loss:0.062, val_acc:0.942]
Epoch [81/120    avg_loss:0.049, val_acc:0.963]
Epoch [82/120    avg_loss:0.040, val_acc:0.972]
Epoch [83/120    avg_loss:0.027, val_acc:0.979]
Epoch [84/120    avg_loss:0.034, val_acc:0.952]
Epoch [85/120    avg_loss:0.044, val_acc:0.973]
Epoch [86/120    avg_loss:0.028, val_acc:0.975]
Epoch [87/120    avg_loss:0.026, val_acc:0.980]
Epoch [88/120    avg_loss:0.018, val_acc:0.969]
Epoch [89/120    avg_loss:0.020, val_acc:0.978]
Epoch [90/120    avg_loss:0.022, val_acc:0.970]
Epoch [91/120    avg_loss:0.025, val_acc:0.973]
Epoch [92/120    avg_loss:0.031, val_acc:0.974]
Epoch [93/120    avg_loss:0.022, val_acc:0.975]
Epoch [94/120    avg_loss:0.027, val_acc:0.966]
Epoch [95/120    avg_loss:0.038, val_acc:0.969]
Epoch [96/120    avg_loss:0.027, val_acc:0.975]
Epoch [97/120    avg_loss:0.021, val_acc:0.964]
Epoch [98/120    avg_loss:0.023, val_acc:0.966]
Epoch [99/120    avg_loss:0.022, val_acc:0.975]
Epoch [100/120    avg_loss:0.013, val_acc:0.981]
Epoch [101/120    avg_loss:0.012, val_acc:0.972]
Epoch [102/120    avg_loss:0.010, val_acc:0.971]
Epoch [103/120    avg_loss:0.014, val_acc:0.975]
Epoch [104/120    avg_loss:0.019, val_acc:0.978]
Epoch [105/120    avg_loss:0.012, val_acc:0.972]
Epoch [106/120    avg_loss:0.014, val_acc:0.973]
Epoch [107/120    avg_loss:0.010, val_acc:0.972]
Epoch [108/120    avg_loss:0.013, val_acc:0.977]
Epoch [109/120    avg_loss:0.015, val_acc:0.972]
Epoch [110/120    avg_loss:0.018, val_acc:0.975]
Epoch [111/120    avg_loss:0.019, val_acc:0.967]
Epoch [112/120    avg_loss:0.018, val_acc:0.970]
Epoch [113/120    avg_loss:0.018, val_acc:0.973]
Epoch [114/120    avg_loss:0.017, val_acc:0.978]
Epoch [115/120    avg_loss:0.014, val_acc:0.983]
Epoch [116/120    avg_loss:0.010, val_acc:0.982]
Epoch [117/120    avg_loss:0.009, val_acc:0.981]
Epoch [118/120    avg_loss:0.008, val_acc:0.981]
Epoch [119/120    avg_loss:0.008, val_acc:0.981]
Epoch [120/120    avg_loss:0.009, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0 1242    3   11    0    0    0    0    0    5   24    0    0
     0    0    0]
 [   0    0    0  728    8    0    0    0    0    4    2    1    1    3
     0    0    0]
 [   0    0    0    3  210    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    1    0    0    0    0    0    0    0
     9    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    2    0    0    2    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    2    0    0    0    0  838   28    0    0
     0    1    0]
 [   0    0    6    0    0    0    6    0    0    0    1 2197    0    0
     0    0    0]
 [   0    0    0    3    0    3    0    0    0    0    7    5  509    0
     3    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    39  295    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.61517615176152

F1 scores:
[       nan 0.975      0.97833793 0.97981157 0.95022624 0.98265896
 0.98277154 1.         0.997669   0.77777778 0.96990741 0.98387819
 0.97044805 0.9919571  0.97457992 0.906298   0.97076023]

Kappa:
0.9727833489400499
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f566a75db38>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.797, val_acc:0.132]
Epoch [2/120    avg_loss:2.650, val_acc:0.270]
Epoch [3/120    avg_loss:2.507, val_acc:0.293]
Epoch [4/120    avg_loss:2.373, val_acc:0.298]
Epoch [5/120    avg_loss:2.250, val_acc:0.368]
Epoch [6/120    avg_loss:2.126, val_acc:0.439]
Epoch [7/120    avg_loss:2.033, val_acc:0.452]
Epoch [8/120    avg_loss:1.941, val_acc:0.531]
Epoch [9/120    avg_loss:1.858, val_acc:0.542]
Epoch [10/120    avg_loss:1.761, val_acc:0.566]
Epoch [11/120    avg_loss:1.666, val_acc:0.626]
Epoch [12/120    avg_loss:1.537, val_acc:0.661]
Epoch [13/120    avg_loss:1.422, val_acc:0.656]
Epoch [14/120    avg_loss:1.323, val_acc:0.665]
Epoch [15/120    avg_loss:1.186, val_acc:0.691]
Epoch [16/120    avg_loss:1.092, val_acc:0.681]
Epoch [17/120    avg_loss:0.993, val_acc:0.738]
Epoch [18/120    avg_loss:0.869, val_acc:0.741]
Epoch [19/120    avg_loss:0.832, val_acc:0.764]
Epoch [20/120    avg_loss:0.708, val_acc:0.776]
Epoch [21/120    avg_loss:0.617, val_acc:0.793]
Epoch [22/120    avg_loss:0.609, val_acc:0.800]
Epoch [23/120    avg_loss:0.554, val_acc:0.800]
Epoch [24/120    avg_loss:0.527, val_acc:0.816]
Epoch [25/120    avg_loss:0.472, val_acc:0.826]
Epoch [26/120    avg_loss:0.406, val_acc:0.850]
Epoch [27/120    avg_loss:0.368, val_acc:0.834]
Epoch [28/120    avg_loss:0.327, val_acc:0.857]
Epoch [29/120    avg_loss:0.313, val_acc:0.874]
Epoch [30/120    avg_loss:0.270, val_acc:0.875]
Epoch [31/120    avg_loss:0.244, val_acc:0.881]
Epoch [32/120    avg_loss:0.265, val_acc:0.880]
Epoch [33/120    avg_loss:0.272, val_acc:0.894]
Epoch [34/120    avg_loss:0.207, val_acc:0.906]
Epoch [35/120    avg_loss:0.186, val_acc:0.908]
Epoch [36/120    avg_loss:0.201, val_acc:0.888]
Epoch [37/120    avg_loss:0.250, val_acc:0.885]
Epoch [38/120    avg_loss:0.230, val_acc:0.915]
Epoch [39/120    avg_loss:0.180, val_acc:0.910]
Epoch [40/120    avg_loss:0.172, val_acc:0.903]
Epoch [41/120    avg_loss:0.168, val_acc:0.916]
Epoch [42/120    avg_loss:0.134, val_acc:0.933]
Epoch [43/120    avg_loss:0.128, val_acc:0.944]
Epoch [44/120    avg_loss:0.103, val_acc:0.936]
Epoch [45/120    avg_loss:0.128, val_acc:0.935]
Epoch [46/120    avg_loss:0.092, val_acc:0.954]
Epoch [47/120    avg_loss:0.070, val_acc:0.940]
Epoch [48/120    avg_loss:0.095, val_acc:0.937]
Epoch [49/120    avg_loss:0.092, val_acc:0.954]
Epoch [50/120    avg_loss:0.075, val_acc:0.949]
Epoch [51/120    avg_loss:0.068, val_acc:0.951]
Epoch [52/120    avg_loss:0.072, val_acc:0.951]
Epoch [53/120    avg_loss:0.078, val_acc:0.940]
Epoch [54/120    avg_loss:0.081, val_acc:0.954]
Epoch [55/120    avg_loss:0.066, val_acc:0.954]
Epoch [56/120    avg_loss:0.079, val_acc:0.938]
Epoch [57/120    avg_loss:0.065, val_acc:0.959]
Epoch [58/120    avg_loss:0.064, val_acc:0.951]
Epoch [59/120    avg_loss:0.046, val_acc:0.955]
Epoch [60/120    avg_loss:0.043, val_acc:0.958]
Epoch [61/120    avg_loss:0.036, val_acc:0.965]
Epoch [62/120    avg_loss:0.038, val_acc:0.970]
Epoch [63/120    avg_loss:0.104, val_acc:0.935]
Epoch [64/120    avg_loss:0.129, val_acc:0.932]
Epoch [65/120    avg_loss:0.097, val_acc:0.934]
Epoch [66/120    avg_loss:0.106, val_acc:0.950]
Epoch [67/120    avg_loss:0.081, val_acc:0.950]
Epoch [68/120    avg_loss:0.061, val_acc:0.951]
Epoch [69/120    avg_loss:0.064, val_acc:0.953]
Epoch [70/120    avg_loss:0.065, val_acc:0.953]
Epoch [71/120    avg_loss:0.062, val_acc:0.953]
Epoch [72/120    avg_loss:0.047, val_acc:0.963]
Epoch [73/120    avg_loss:0.043, val_acc:0.963]
Epoch [74/120    avg_loss:0.032, val_acc:0.968]
Epoch [75/120    avg_loss:0.036, val_acc:0.965]
Epoch [76/120    avg_loss:0.038, val_acc:0.966]
Epoch [77/120    avg_loss:0.029, val_acc:0.968]
Epoch [78/120    avg_loss:0.025, val_acc:0.969]
Epoch [79/120    avg_loss:0.022, val_acc:0.971]
Epoch [80/120    avg_loss:0.022, val_acc:0.974]
Epoch [81/120    avg_loss:0.022, val_acc:0.972]
Epoch [82/120    avg_loss:0.018, val_acc:0.974]
Epoch [83/120    avg_loss:0.019, val_acc:0.974]
Epoch [84/120    avg_loss:0.021, val_acc:0.975]
Epoch [85/120    avg_loss:0.020, val_acc:0.976]
Epoch [86/120    avg_loss:0.021, val_acc:0.974]
Epoch [87/120    avg_loss:0.022, val_acc:0.974]
Epoch [88/120    avg_loss:0.022, val_acc:0.973]
Epoch [89/120    avg_loss:0.021, val_acc:0.974]
Epoch [90/120    avg_loss:0.017, val_acc:0.974]
Epoch [91/120    avg_loss:0.018, val_acc:0.974]
Epoch [92/120    avg_loss:0.020, val_acc:0.973]
Epoch [93/120    avg_loss:0.022, val_acc:0.971]
Epoch [94/120    avg_loss:0.021, val_acc:0.972]
Epoch [95/120    avg_loss:0.016, val_acc:0.972]
Epoch [96/120    avg_loss:0.020, val_acc:0.971]
Epoch [97/120    avg_loss:0.017, val_acc:0.975]
Epoch [98/120    avg_loss:0.017, val_acc:0.978]
Epoch [99/120    avg_loss:0.018, val_acc:0.975]
Epoch [100/120    avg_loss:0.016, val_acc:0.976]
Epoch [101/120    avg_loss:0.020, val_acc:0.978]
Epoch [102/120    avg_loss:0.016, val_acc:0.979]
Epoch [103/120    avg_loss:0.018, val_acc:0.980]
Epoch [104/120    avg_loss:0.016, val_acc:0.976]
Epoch [105/120    avg_loss:0.014, val_acc:0.976]
Epoch [106/120    avg_loss:0.022, val_acc:0.976]
Epoch [107/120    avg_loss:0.016, val_acc:0.978]
Epoch [108/120    avg_loss:0.013, val_acc:0.978]
Epoch [109/120    avg_loss:0.014, val_acc:0.974]
Epoch [110/120    avg_loss:0.017, val_acc:0.975]
Epoch [111/120    avg_loss:0.014, val_acc:0.974]
Epoch [112/120    avg_loss:0.020, val_acc:0.975]
Epoch [113/120    avg_loss:0.020, val_acc:0.976]
Epoch [114/120    avg_loss:0.014, val_acc:0.978]
Epoch [115/120    avg_loss:0.013, val_acc:0.978]
Epoch [116/120    avg_loss:0.016, val_acc:0.975]
Epoch [117/120    avg_loss:0.019, val_acc:0.975]
Epoch [118/120    avg_loss:0.014, val_acc:0.975]
Epoch [119/120    avg_loss:0.017, val_acc:0.975]
Epoch [120/120    avg_loss:0.014, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1254    0    3    4    1    0    0    0    5   18    0    0
     0    0    0]
 [   0    0    0  720    1    0    1    0    0    8    4    3   10    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    3    0    2    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    9    0    0    0    0    0    0  421    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   20    0    0    1    2    0    0    0  838   12    1    0
     0    1    0]
 [   0    0   10    0    0    0    2    0    0    0   14 2179    4    1
     0    0    0]
 [   0    0    1    1    0    6    0    0    0    0    3    5  515    0
     0    2    1]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1137    2    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    20  315    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.7560975609756

F1 scores:
[       nan 0.88888889 0.97549592 0.97959184 0.98598131 0.97477064
 0.98419865 0.94339623 0.98942421 0.7826087  0.96377228 0.98396929
 0.96622889 0.99459459 0.98783666 0.94452774 0.98203593]

Kappa:
0.9744112010082354
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9ae4d72a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.771, val_acc:0.149]
Epoch [2/120    avg_loss:2.607, val_acc:0.287]
Epoch [3/120    avg_loss:2.465, val_acc:0.403]
Epoch [4/120    avg_loss:2.355, val_acc:0.479]
Epoch [5/120    avg_loss:2.244, val_acc:0.499]
Epoch [6/120    avg_loss:2.157, val_acc:0.536]
Epoch [7/120    avg_loss:2.045, val_acc:0.544]
Epoch [8/120    avg_loss:1.952, val_acc:0.568]
Epoch [9/120    avg_loss:1.884, val_acc:0.560]
Epoch [10/120    avg_loss:1.769, val_acc:0.589]
Epoch [11/120    avg_loss:1.626, val_acc:0.619]
Epoch [12/120    avg_loss:1.530, val_acc:0.640]
Epoch [13/120    avg_loss:1.448, val_acc:0.640]
Epoch [14/120    avg_loss:1.383, val_acc:0.691]
Epoch [15/120    avg_loss:1.293, val_acc:0.648]
Epoch [16/120    avg_loss:1.329, val_acc:0.671]
Epoch [17/120    avg_loss:1.193, val_acc:0.728]
Epoch [18/120    avg_loss:1.047, val_acc:0.738]
Epoch [19/120    avg_loss:0.964, val_acc:0.757]
Epoch [20/120    avg_loss:0.879, val_acc:0.760]
Epoch [21/120    avg_loss:0.776, val_acc:0.760]
Epoch [22/120    avg_loss:0.680, val_acc:0.817]
Epoch [23/120    avg_loss:0.637, val_acc:0.789]
Epoch [24/120    avg_loss:0.658, val_acc:0.795]
Epoch [25/120    avg_loss:0.570, val_acc:0.810]
Epoch [26/120    avg_loss:0.563, val_acc:0.802]
Epoch [27/120    avg_loss:0.440, val_acc:0.803]
Epoch [28/120    avg_loss:0.379, val_acc:0.848]
Epoch [29/120    avg_loss:0.328, val_acc:0.840]
Epoch [30/120    avg_loss:0.288, val_acc:0.864]
Epoch [31/120    avg_loss:0.305, val_acc:0.877]
Epoch [32/120    avg_loss:0.300, val_acc:0.865]
Epoch [33/120    avg_loss:0.302, val_acc:0.865]
Epoch [34/120    avg_loss:0.235, val_acc:0.878]
Epoch [35/120    avg_loss:0.219, val_acc:0.835]
Epoch [36/120    avg_loss:0.279, val_acc:0.842]
Epoch [37/120    avg_loss:0.263, val_acc:0.872]
Epoch [38/120    avg_loss:0.207, val_acc:0.876]
Epoch [39/120    avg_loss:0.166, val_acc:0.918]
Epoch [40/120    avg_loss:0.176, val_acc:0.897]
Epoch [41/120    avg_loss:0.158, val_acc:0.909]
Epoch [42/120    avg_loss:0.148, val_acc:0.896]
Epoch [43/120    avg_loss:0.258, val_acc:0.885]
Epoch [44/120    avg_loss:0.189, val_acc:0.917]
Epoch [45/120    avg_loss:0.127, val_acc:0.922]
Epoch [46/120    avg_loss:0.136, val_acc:0.910]
Epoch [47/120    avg_loss:0.112, val_acc:0.942]
Epoch [48/120    avg_loss:0.110, val_acc:0.931]
Epoch [49/120    avg_loss:0.109, val_acc:0.904]
Epoch [50/120    avg_loss:0.229, val_acc:0.893]
Epoch [51/120    avg_loss:0.154, val_acc:0.924]
Epoch [52/120    avg_loss:0.124, val_acc:0.914]
Epoch [53/120    avg_loss:0.112, val_acc:0.929]
Epoch [54/120    avg_loss:0.093, val_acc:0.945]
Epoch [55/120    avg_loss:0.102, val_acc:0.948]
Epoch [56/120    avg_loss:0.074, val_acc:0.948]
Epoch [57/120    avg_loss:0.073, val_acc:0.942]
Epoch [58/120    avg_loss:0.109, val_acc:0.934]
Epoch [59/120    avg_loss:0.070, val_acc:0.949]
Epoch [60/120    avg_loss:0.076, val_acc:0.954]
Epoch [61/120    avg_loss:0.061, val_acc:0.943]
Epoch [62/120    avg_loss:0.069, val_acc:0.951]
Epoch [63/120    avg_loss:0.062, val_acc:0.948]
Epoch [64/120    avg_loss:0.073, val_acc:0.925]
Epoch [65/120    avg_loss:0.092, val_acc:0.953]
Epoch [66/120    avg_loss:0.068, val_acc:0.954]
Epoch [67/120    avg_loss:0.050, val_acc:0.954]
Epoch [68/120    avg_loss:0.054, val_acc:0.948]
Epoch [69/120    avg_loss:0.058, val_acc:0.952]
Epoch [70/120    avg_loss:0.059, val_acc:0.933]
Epoch [71/120    avg_loss:0.049, val_acc:0.943]
Epoch [72/120    avg_loss:0.051, val_acc:0.966]
Epoch [73/120    avg_loss:0.040, val_acc:0.959]
Epoch [74/120    avg_loss:0.038, val_acc:0.963]
Epoch [75/120    avg_loss:0.032, val_acc:0.954]
Epoch [76/120    avg_loss:0.048, val_acc:0.954]
Epoch [77/120    avg_loss:0.045, val_acc:0.957]
Epoch [78/120    avg_loss:0.061, val_acc:0.954]
Epoch [79/120    avg_loss:0.052, val_acc:0.966]
Epoch [80/120    avg_loss:0.049, val_acc:0.946]
Epoch [81/120    avg_loss:0.049, val_acc:0.955]
Epoch [82/120    avg_loss:0.047, val_acc:0.952]
Epoch [83/120    avg_loss:0.039, val_acc:0.961]
Epoch [84/120    avg_loss:0.031, val_acc:0.960]
Epoch [85/120    avg_loss:0.027, val_acc:0.971]
Epoch [86/120    avg_loss:0.024, val_acc:0.972]
Epoch [87/120    avg_loss:0.032, val_acc:0.969]
Epoch [88/120    avg_loss:0.128, val_acc:0.946]
Epoch [89/120    avg_loss:0.081, val_acc:0.954]
Epoch [90/120    avg_loss:0.052, val_acc:0.958]
Epoch [91/120    avg_loss:0.041, val_acc:0.966]
Epoch [92/120    avg_loss:0.030, val_acc:0.963]
Epoch [93/120    avg_loss:0.022, val_acc:0.973]
Epoch [94/120    avg_loss:0.020, val_acc:0.972]
Epoch [95/120    avg_loss:0.022, val_acc:0.967]
Epoch [96/120    avg_loss:0.020, val_acc:0.977]
Epoch [97/120    avg_loss:0.024, val_acc:0.971]
Epoch [98/120    avg_loss:0.037, val_acc:0.975]
Epoch [99/120    avg_loss:0.019, val_acc:0.971]
Epoch [100/120    avg_loss:0.018, val_acc:0.975]
Epoch [101/120    avg_loss:0.018, val_acc:0.970]
Epoch [102/120    avg_loss:0.065, val_acc:0.935]
Epoch [103/120    avg_loss:0.053, val_acc:0.971]
Epoch [104/120    avg_loss:0.030, val_acc:0.965]
Epoch [105/120    avg_loss:0.076, val_acc:0.946]
Epoch [106/120    avg_loss:0.048, val_acc:0.965]
Epoch [107/120    avg_loss:0.047, val_acc:0.969]
Epoch [108/120    avg_loss:0.034, val_acc:0.970]
Epoch [109/120    avg_loss:0.020, val_acc:0.972]
Epoch [110/120    avg_loss:0.019, val_acc:0.980]
Epoch [111/120    avg_loss:0.016, val_acc:0.980]
Epoch [112/120    avg_loss:0.014, val_acc:0.980]
Epoch [113/120    avg_loss:0.015, val_acc:0.981]
Epoch [114/120    avg_loss:0.012, val_acc:0.981]
Epoch [115/120    avg_loss:0.012, val_acc:0.981]
Epoch [116/120    avg_loss:0.013, val_acc:0.981]
Epoch [117/120    avg_loss:0.013, val_acc:0.977]
Epoch [118/120    avg_loss:0.010, val_acc:0.978]
Epoch [119/120    avg_loss:0.015, val_acc:0.978]
Epoch [120/120    avg_loss:0.015, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1253    2    9    0    0    0    0    0    2   19    0    0
     0    0    0]
 [   0    0    2  706    7    0    0    0    0    1    2   17   12    0
     0    0    0]
 [   0    0    0    0  211    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  431    0    2    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    2    0    0
     0    0    0]
 [   0    0    9    1    0    0    0    0    0    0  853   10    0    0
     1    1    0]
 [   0    0    9    0    0    0    1    0    0    0    7 2180   13    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    4   13  512    0
     2    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1137    2    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    32  315    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.90785907859079

F1 scores:
[       nan 0.98765432 0.97928878 0.96978022 0.95909091 0.99538106
 0.99771516 0.96153846 1.         0.91428571 0.97877223 0.9791152
 0.95344507 1.         0.98313878 0.94594595 0.98224852]

Kappa:
0.9761290506219386
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8c4951bb38>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.771, val_acc:0.293]
Epoch [2/120    avg_loss:2.642, val_acc:0.317]
Epoch [3/120    avg_loss:2.486, val_acc:0.412]
Epoch [4/120    avg_loss:2.367, val_acc:0.419]
Epoch [5/120    avg_loss:2.263, val_acc:0.453]
Epoch [6/120    avg_loss:2.132, val_acc:0.480]
Epoch [7/120    avg_loss:2.006, val_acc:0.528]
Epoch [8/120    avg_loss:1.923, val_acc:0.546]
Epoch [9/120    avg_loss:1.789, val_acc:0.578]
Epoch [10/120    avg_loss:1.706, val_acc:0.586]
Epoch [11/120    avg_loss:1.641, val_acc:0.617]
Epoch [12/120    avg_loss:1.507, val_acc:0.653]
Epoch [13/120    avg_loss:1.365, val_acc:0.677]
Epoch [14/120    avg_loss:1.258, val_acc:0.733]
Epoch [15/120    avg_loss:1.133, val_acc:0.714]
Epoch [16/120    avg_loss:1.003, val_acc:0.724]
Epoch [17/120    avg_loss:0.936, val_acc:0.715]
Epoch [18/120    avg_loss:0.842, val_acc:0.762]
Epoch [19/120    avg_loss:0.767, val_acc:0.789]
Epoch [20/120    avg_loss:0.677, val_acc:0.769]
Epoch [21/120    avg_loss:0.655, val_acc:0.798]
Epoch [22/120    avg_loss:0.592, val_acc:0.830]
Epoch [23/120    avg_loss:0.556, val_acc:0.830]
Epoch [24/120    avg_loss:0.471, val_acc:0.829]
Epoch [25/120    avg_loss:0.422, val_acc:0.828]
Epoch [26/120    avg_loss:0.368, val_acc:0.859]
Epoch [27/120    avg_loss:0.322, val_acc:0.875]
Epoch [28/120    avg_loss:0.327, val_acc:0.846]
Epoch [29/120    avg_loss:0.329, val_acc:0.853]
Epoch [30/120    avg_loss:0.298, val_acc:0.857]
Epoch [31/120    avg_loss:0.271, val_acc:0.880]
Epoch [32/120    avg_loss:0.261, val_acc:0.877]
Epoch [33/120    avg_loss:0.213, val_acc:0.909]
Epoch [34/120    avg_loss:0.223, val_acc:0.876]
Epoch [35/120    avg_loss:0.239, val_acc:0.905]
Epoch [36/120    avg_loss:0.200, val_acc:0.909]
Epoch [37/120    avg_loss:0.185, val_acc:0.924]
Epoch [38/120    avg_loss:0.176, val_acc:0.908]
Epoch [39/120    avg_loss:0.202, val_acc:0.917]
Epoch [40/120    avg_loss:0.155, val_acc:0.925]
Epoch [41/120    avg_loss:0.130, val_acc:0.916]
Epoch [42/120    avg_loss:0.126, val_acc:0.934]
Epoch [43/120    avg_loss:0.131, val_acc:0.923]
Epoch [44/120    avg_loss:0.102, val_acc:0.941]
Epoch [45/120    avg_loss:0.104, val_acc:0.941]
Epoch [46/120    avg_loss:0.140, val_acc:0.932]
Epoch [47/120    avg_loss:0.147, val_acc:0.922]
Epoch [48/120    avg_loss:0.128, val_acc:0.941]
Epoch [49/120    avg_loss:0.114, val_acc:0.922]
Epoch [50/120    avg_loss:0.121, val_acc:0.921]
Epoch [51/120    avg_loss:0.109, val_acc:0.929]
Epoch [52/120    avg_loss:0.111, val_acc:0.943]
Epoch [53/120    avg_loss:0.084, val_acc:0.930]
Epoch [54/120    avg_loss:0.082, val_acc:0.953]
Epoch [55/120    avg_loss:0.068, val_acc:0.942]
Epoch [56/120    avg_loss:0.065, val_acc:0.961]
Epoch [57/120    avg_loss:0.078, val_acc:0.933]
Epoch [58/120    avg_loss:0.059, val_acc:0.967]
Epoch [59/120    avg_loss:0.043, val_acc:0.974]
Epoch [60/120    avg_loss:0.070, val_acc:0.949]
Epoch [61/120    avg_loss:0.134, val_acc:0.941]
Epoch [62/120    avg_loss:0.082, val_acc:0.961]
Epoch [63/120    avg_loss:0.053, val_acc:0.963]
Epoch [64/120    avg_loss:0.045, val_acc:0.965]
Epoch [65/120    avg_loss:0.034, val_acc:0.967]
Epoch [66/120    avg_loss:0.045, val_acc:0.966]
Epoch [67/120    avg_loss:0.043, val_acc:0.969]
Epoch [68/120    avg_loss:0.036, val_acc:0.974]
Epoch [69/120    avg_loss:0.047, val_acc:0.975]
Epoch [70/120    avg_loss:0.035, val_acc:0.970]
Epoch [71/120    avg_loss:0.043, val_acc:0.949]
Epoch [72/120    avg_loss:0.053, val_acc:0.960]
Epoch [73/120    avg_loss:0.057, val_acc:0.958]
Epoch [74/120    avg_loss:0.038, val_acc:0.968]
Epoch [75/120    avg_loss:0.032, val_acc:0.971]
Epoch [76/120    avg_loss:0.042, val_acc:0.955]
Epoch [77/120    avg_loss:0.033, val_acc:0.969]
Epoch [78/120    avg_loss:0.032, val_acc:0.961]
Epoch [79/120    avg_loss:0.029, val_acc:0.975]
Epoch [80/120    avg_loss:0.026, val_acc:0.974]
Epoch [81/120    avg_loss:0.021, val_acc:0.972]
Epoch [82/120    avg_loss:0.028, val_acc:0.978]
Epoch [83/120    avg_loss:0.024, val_acc:0.968]
Epoch [84/120    avg_loss:0.026, val_acc:0.972]
Epoch [85/120    avg_loss:0.027, val_acc:0.974]
Epoch [86/120    avg_loss:0.024, val_acc:0.975]
Epoch [87/120    avg_loss:0.021, val_acc:0.961]
Epoch [88/120    avg_loss:0.037, val_acc:0.980]
Epoch [89/120    avg_loss:0.028, val_acc:0.975]
Epoch [90/120    avg_loss:0.017, val_acc:0.981]
Epoch [91/120    avg_loss:0.042, val_acc:0.977]
Epoch [92/120    avg_loss:0.089, val_acc:0.946]
Epoch [93/120    avg_loss:0.053, val_acc:0.963]
Epoch [94/120    avg_loss:0.041, val_acc:0.969]
Epoch [95/120    avg_loss:0.050, val_acc:0.963]
Epoch [96/120    avg_loss:0.028, val_acc:0.969]
Epoch [97/120    avg_loss:0.024, val_acc:0.977]
Epoch [98/120    avg_loss:0.021, val_acc:0.972]
Epoch [99/120    avg_loss:0.021, val_acc:0.975]
Epoch [100/120    avg_loss:0.029, val_acc:0.971]
Epoch [101/120    avg_loss:0.031, val_acc:0.984]
Epoch [102/120    avg_loss:0.022, val_acc:0.975]
Epoch [103/120    avg_loss:0.023, val_acc:0.975]
Epoch [104/120    avg_loss:0.020, val_acc:0.981]
Epoch [105/120    avg_loss:0.011, val_acc:0.981]
Epoch [106/120    avg_loss:0.012, val_acc:0.983]
Epoch [107/120    avg_loss:0.016, val_acc:0.979]
Epoch [108/120    avg_loss:0.015, val_acc:0.981]
Epoch [109/120    avg_loss:0.016, val_acc:0.977]
Epoch [110/120    avg_loss:0.024, val_acc:0.955]
Epoch [111/120    avg_loss:0.028, val_acc:0.975]
Epoch [112/120    avg_loss:0.015, val_acc:0.982]
Epoch [113/120    avg_loss:0.011, val_acc:0.983]
Epoch [114/120    avg_loss:0.024, val_acc:0.974]
Epoch [115/120    avg_loss:0.021, val_acc:0.978]
Epoch [116/120    avg_loss:0.009, val_acc:0.981]
Epoch [117/120    avg_loss:0.011, val_acc:0.980]
Epoch [118/120    avg_loss:0.015, val_acc:0.982]
Epoch [119/120    avg_loss:0.009, val_acc:0.983]
Epoch [120/120    avg_loss:0.011, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1249    0   10    0    0    0    0    0    7   19    0    0
     0    0    0]
 [   0    0    4  717    4    2    0    0    0    4    1    7    7    0
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    2    0    0    0    2  853   10    2    0
     1    0    0]
 [   0    0   15    0    0    3    0    0    0    1   20 2159   12    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0    1    1  525    0
     1    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1122   17    0]
 [   0    0    0    0    0    1   22    0    0    0    0    0    0    0
    31  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.53929539295393

F1 scores:
[       nan 1.         0.97616256 0.97883959 0.96583144 0.98627002
 0.98124531 1.         1.         0.8372093  0.97097325 0.97958258
 0.97222222 1.         0.97565217 0.89193303 0.97109827]

Kappa:
0.9719532586052397
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff8fb189b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.794, val_acc:0.176]
Epoch [2/120    avg_loss:2.651, val_acc:0.393]
Epoch [3/120    avg_loss:2.495, val_acc:0.513]
Epoch [4/120    avg_loss:2.343, val_acc:0.526]
Epoch [5/120    avg_loss:2.221, val_acc:0.534]
Epoch [6/120    avg_loss:2.072, val_acc:0.547]
Epoch [7/120    avg_loss:1.973, val_acc:0.569]
Epoch [8/120    avg_loss:1.837, val_acc:0.598]
Epoch [9/120    avg_loss:1.687, val_acc:0.626]
Epoch [10/120    avg_loss:1.584, val_acc:0.634]
Epoch [11/120    avg_loss:1.451, val_acc:0.692]
Epoch [12/120    avg_loss:1.318, val_acc:0.697]
Epoch [13/120    avg_loss:1.237, val_acc:0.739]
Epoch [14/120    avg_loss:1.133, val_acc:0.777]
Epoch [15/120    avg_loss:0.979, val_acc:0.804]
Epoch [16/120    avg_loss:0.907, val_acc:0.712]
Epoch [17/120    avg_loss:0.841, val_acc:0.767]
Epoch [18/120    avg_loss:0.750, val_acc:0.782]
Epoch [19/120    avg_loss:0.688, val_acc:0.794]
Epoch [20/120    avg_loss:0.624, val_acc:0.831]
Epoch [21/120    avg_loss:0.583, val_acc:0.816]
Epoch [22/120    avg_loss:0.555, val_acc:0.803]
Epoch [23/120    avg_loss:0.506, val_acc:0.795]
Epoch [24/120    avg_loss:0.481, val_acc:0.857]
Epoch [25/120    avg_loss:0.396, val_acc:0.833]
Epoch [26/120    avg_loss:0.345, val_acc:0.847]
Epoch [27/120    avg_loss:0.352, val_acc:0.873]
Epoch [28/120    avg_loss:0.395, val_acc:0.835]
Epoch [29/120    avg_loss:0.403, val_acc:0.859]
Epoch [30/120    avg_loss:0.325, val_acc:0.879]
Epoch [31/120    avg_loss:0.293, val_acc:0.899]
Epoch [32/120    avg_loss:0.260, val_acc:0.861]
Epoch [33/120    avg_loss:0.227, val_acc:0.910]
Epoch [34/120    avg_loss:0.195, val_acc:0.876]
Epoch [35/120    avg_loss:0.207, val_acc:0.929]
Epoch [36/120    avg_loss:0.210, val_acc:0.889]
Epoch [37/120    avg_loss:0.212, val_acc:0.909]
Epoch [38/120    avg_loss:0.200, val_acc:0.920]
Epoch [39/120    avg_loss:0.267, val_acc:0.860]
Epoch [40/120    avg_loss:0.215, val_acc:0.885]
Epoch [41/120    avg_loss:0.168, val_acc:0.927]
Epoch [42/120    avg_loss:0.218, val_acc:0.931]
Epoch [43/120    avg_loss:0.125, val_acc:0.922]
Epoch [44/120    avg_loss:0.131, val_acc:0.931]
Epoch [45/120    avg_loss:0.141, val_acc:0.936]
Epoch [46/120    avg_loss:0.127, val_acc:0.928]
Epoch [47/120    avg_loss:0.134, val_acc:0.932]
Epoch [48/120    avg_loss:0.134, val_acc:0.951]
Epoch [49/120    avg_loss:0.117, val_acc:0.953]
Epoch [50/120    avg_loss:0.140, val_acc:0.943]
Epoch [51/120    avg_loss:0.131, val_acc:0.921]
Epoch [52/120    avg_loss:0.128, val_acc:0.938]
Epoch [53/120    avg_loss:0.126, val_acc:0.931]
Epoch [54/120    avg_loss:0.084, val_acc:0.946]
Epoch [55/120    avg_loss:0.146, val_acc:0.947]
Epoch [56/120    avg_loss:0.097, val_acc:0.954]
Epoch [57/120    avg_loss:0.065, val_acc:0.965]
Epoch [58/120    avg_loss:0.065, val_acc:0.958]
Epoch [59/120    avg_loss:0.054, val_acc:0.955]
Epoch [60/120    avg_loss:0.058, val_acc:0.960]
Epoch [61/120    avg_loss:0.069, val_acc:0.958]
Epoch [62/120    avg_loss:0.047, val_acc:0.964]
Epoch [63/120    avg_loss:0.038, val_acc:0.963]
Epoch [64/120    avg_loss:0.062, val_acc:0.953]
Epoch [65/120    avg_loss:0.063, val_acc:0.966]
Epoch [66/120    avg_loss:0.061, val_acc:0.957]
Epoch [67/120    avg_loss:0.050, val_acc:0.965]
Epoch [68/120    avg_loss:0.040, val_acc:0.967]
Epoch [69/120    avg_loss:0.038, val_acc:0.967]
Epoch [70/120    avg_loss:0.051, val_acc:0.976]
Epoch [71/120    avg_loss:0.045, val_acc:0.970]
Epoch [72/120    avg_loss:0.037, val_acc:0.971]
Epoch [73/120    avg_loss:0.033, val_acc:0.973]
Epoch [74/120    avg_loss:0.030, val_acc:0.975]
Epoch [75/120    avg_loss:0.040, val_acc:0.965]
Epoch [76/120    avg_loss:0.037, val_acc:0.971]
Epoch [77/120    avg_loss:0.036, val_acc:0.965]
Epoch [78/120    avg_loss:0.042, val_acc:0.946]
Epoch [79/120    avg_loss:0.052, val_acc:0.973]
Epoch [80/120    avg_loss:0.040, val_acc:0.960]
Epoch [81/120    avg_loss:0.040, val_acc:0.974]
Epoch [82/120    avg_loss:0.052, val_acc:0.971]
Epoch [83/120    avg_loss:0.030, val_acc:0.978]
Epoch [84/120    avg_loss:0.035, val_acc:0.970]
Epoch [85/120    avg_loss:0.027, val_acc:0.966]
Epoch [86/120    avg_loss:0.021, val_acc:0.982]
Epoch [87/120    avg_loss:0.017, val_acc:0.975]
Epoch [88/120    avg_loss:0.019, val_acc:0.981]
Epoch [89/120    avg_loss:0.026, val_acc:0.971]
Epoch [90/120    avg_loss:0.036, val_acc:0.956]
Epoch [91/120    avg_loss:0.029, val_acc:0.971]
Epoch [92/120    avg_loss:0.079, val_acc:0.946]
Epoch [93/120    avg_loss:0.367, val_acc:0.882]
Epoch [94/120    avg_loss:0.235, val_acc:0.902]
Epoch [95/120    avg_loss:0.155, val_acc:0.931]
Epoch [96/120    avg_loss:0.091, val_acc:0.951]
Epoch [97/120    avg_loss:0.071, val_acc:0.949]
Epoch [98/120    avg_loss:0.051, val_acc:0.906]
Epoch [99/120    avg_loss:0.081, val_acc:0.954]
Epoch [100/120    avg_loss:0.051, val_acc:0.960]
Epoch [101/120    avg_loss:0.040, val_acc:0.966]
Epoch [102/120    avg_loss:0.040, val_acc:0.965]
Epoch [103/120    avg_loss:0.035, val_acc:0.965]
Epoch [104/120    avg_loss:0.023, val_acc:0.967]
Epoch [105/120    avg_loss:0.031, val_acc:0.969]
Epoch [106/120    avg_loss:0.043, val_acc:0.967]
Epoch [107/120    avg_loss:0.031, val_acc:0.969]
Epoch [108/120    avg_loss:0.024, val_acc:0.970]
Epoch [109/120    avg_loss:0.032, val_acc:0.972]
Epoch [110/120    avg_loss:0.029, val_acc:0.972]
Epoch [111/120    avg_loss:0.027, val_acc:0.971]
Epoch [112/120    avg_loss:0.026, val_acc:0.970]
Epoch [113/120    avg_loss:0.023, val_acc:0.970]
Epoch [114/120    avg_loss:0.025, val_acc:0.970]
Epoch [115/120    avg_loss:0.022, val_acc:0.970]
Epoch [116/120    avg_loss:0.024, val_acc:0.971]
Epoch [117/120    avg_loss:0.021, val_acc:0.970]
Epoch [118/120    avg_loss:0.026, val_acc:0.970]
Epoch [119/120    avg_loss:0.025, val_acc:0.970]
Epoch [120/120    avg_loss:0.022, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    2    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1249    6    0    1    1    0    0    0    3   25    0    0
     0    0    0]
 [   0    0    0  717    4    0    3    0    0    8    3    9    3    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  430    0    2    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   14    0    1    0    0
     0    0    0]
 [   0    0   14    0    0    5    5    0    0    0  835   16    0    0
     0    0    0]
 [   0    0   26    5    0    0    0    0    3    0   30 2141    5    0
     0    0    0]
 [   0    0    0    2    2    0    0    0    0    0    0   10  516    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    2    0    0
  1130    7    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    45  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.1490514905149

F1 scores:
[       nan 0.96202532 0.9697205  0.97088693 0.9837587  0.98737084
 0.98866213 0.96153846 0.99652375 0.68292683 0.95592444 0.96987542
 0.97450425 1.         0.974558   0.92073171 0.98245614]

Kappa:
0.9674883163031612
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f534e4c9b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.754, val_acc:0.290]
Epoch [2/120    avg_loss:2.609, val_acc:0.430]
Epoch [3/120    avg_loss:2.476, val_acc:0.440]
Epoch [4/120    avg_loss:2.314, val_acc:0.534]
Epoch [5/120    avg_loss:2.200, val_acc:0.568]
Epoch [6/120    avg_loss:2.073, val_acc:0.565]
Epoch [7/120    avg_loss:1.973, val_acc:0.583]
Epoch [8/120    avg_loss:1.830, val_acc:0.623]
Epoch [9/120    avg_loss:1.730, val_acc:0.601]
Epoch [10/120    avg_loss:1.609, val_acc:0.652]
Epoch [11/120    avg_loss:1.542, val_acc:0.654]
Epoch [12/120    avg_loss:1.360, val_acc:0.637]
Epoch [13/120    avg_loss:1.234, val_acc:0.700]
Epoch [14/120    avg_loss:1.099, val_acc:0.732]
Epoch [15/120    avg_loss:0.983, val_acc:0.694]
Epoch [16/120    avg_loss:0.869, val_acc:0.752]
Epoch [17/120    avg_loss:0.768, val_acc:0.775]
Epoch [18/120    avg_loss:0.681, val_acc:0.792]
Epoch [19/120    avg_loss:0.604, val_acc:0.797]
Epoch [20/120    avg_loss:0.591, val_acc:0.821]
Epoch [21/120    avg_loss:0.553, val_acc:0.831]
Epoch [22/120    avg_loss:0.517, val_acc:0.814]
Epoch [23/120    avg_loss:0.437, val_acc:0.852]
Epoch [24/120    avg_loss:0.363, val_acc:0.842]
Epoch [25/120    avg_loss:0.376, val_acc:0.821]
Epoch [26/120    avg_loss:0.326, val_acc:0.849]
Epoch [27/120    avg_loss:0.277, val_acc:0.884]
Epoch [28/120    avg_loss:0.259, val_acc:0.865]
Epoch [29/120    avg_loss:0.262, val_acc:0.886]
Epoch [30/120    avg_loss:0.212, val_acc:0.904]
Epoch [31/120    avg_loss:0.194, val_acc:0.910]
Epoch [32/120    avg_loss:0.205, val_acc:0.909]
Epoch [33/120    avg_loss:0.172, val_acc:0.919]
Epoch [34/120    avg_loss:0.154, val_acc:0.928]
Epoch [35/120    avg_loss:0.136, val_acc:0.911]
Epoch [36/120    avg_loss:0.164, val_acc:0.889]
Epoch [37/120    avg_loss:0.186, val_acc:0.872]
Epoch [38/120    avg_loss:0.162, val_acc:0.927]
Epoch [39/120    avg_loss:0.113, val_acc:0.930]
Epoch [40/120    avg_loss:0.095, val_acc:0.939]
Epoch [41/120    avg_loss:0.092, val_acc:0.932]
Epoch [42/120    avg_loss:0.118, val_acc:0.917]
Epoch [43/120    avg_loss:0.109, val_acc:0.930]
Epoch [44/120    avg_loss:0.110, val_acc:0.938]
Epoch [45/120    avg_loss:0.137, val_acc:0.938]
Epoch [46/120    avg_loss:0.140, val_acc:0.929]
Epoch [47/120    avg_loss:0.100, val_acc:0.950]
Epoch [48/120    avg_loss:0.095, val_acc:0.933]
Epoch [49/120    avg_loss:0.081, val_acc:0.951]
Epoch [50/120    avg_loss:0.059, val_acc:0.958]
Epoch [51/120    avg_loss:0.072, val_acc:0.952]
Epoch [52/120    avg_loss:0.066, val_acc:0.935]
Epoch [53/120    avg_loss:0.075, val_acc:0.950]
Epoch [54/120    avg_loss:0.060, val_acc:0.961]
Epoch [55/120    avg_loss:0.059, val_acc:0.953]
Epoch [56/120    avg_loss:0.046, val_acc:0.957]
Epoch [57/120    avg_loss:0.046, val_acc:0.971]
Epoch [58/120    avg_loss:0.039, val_acc:0.959]
Epoch [59/120    avg_loss:0.048, val_acc:0.970]
Epoch [60/120    avg_loss:0.049, val_acc:0.950]
Epoch [61/120    avg_loss:0.044, val_acc:0.969]
Epoch [62/120    avg_loss:0.059, val_acc:0.957]
Epoch [63/120    avg_loss:0.038, val_acc:0.968]
Epoch [64/120    avg_loss:0.038, val_acc:0.965]
Epoch [65/120    avg_loss:0.052, val_acc:0.968]
Epoch [66/120    avg_loss:0.030, val_acc:0.966]
Epoch [67/120    avg_loss:0.037, val_acc:0.960]
Epoch [68/120    avg_loss:0.031, val_acc:0.965]
Epoch [69/120    avg_loss:0.032, val_acc:0.967]
Epoch [70/120    avg_loss:0.035, val_acc:0.944]
Epoch [71/120    avg_loss:0.031, val_acc:0.972]
Epoch [72/120    avg_loss:0.020, val_acc:0.969]
Epoch [73/120    avg_loss:0.021, val_acc:0.971]
Epoch [74/120    avg_loss:0.017, val_acc:0.972]
Epoch [75/120    avg_loss:0.018, val_acc:0.971]
Epoch [76/120    avg_loss:0.023, val_acc:0.973]
Epoch [77/120    avg_loss:0.016, val_acc:0.975]
Epoch [78/120    avg_loss:0.021, val_acc:0.975]
Epoch [79/120    avg_loss:0.017, val_acc:0.975]
Epoch [80/120    avg_loss:0.018, val_acc:0.975]
Epoch [81/120    avg_loss:0.016, val_acc:0.971]
Epoch [82/120    avg_loss:0.022, val_acc:0.977]
Epoch [83/120    avg_loss:0.020, val_acc:0.978]
Epoch [84/120    avg_loss:0.021, val_acc:0.975]
Epoch [85/120    avg_loss:0.017, val_acc:0.978]
Epoch [86/120    avg_loss:0.016, val_acc:0.978]
Epoch [87/120    avg_loss:0.015, val_acc:0.976]
Epoch [88/120    avg_loss:0.020, val_acc:0.976]
Epoch [89/120    avg_loss:0.015, val_acc:0.977]
Epoch [90/120    avg_loss:0.015, val_acc:0.977]
Epoch [91/120    avg_loss:0.018, val_acc:0.977]
Epoch [92/120    avg_loss:0.016, val_acc:0.979]
Epoch [93/120    avg_loss:0.014, val_acc:0.978]
Epoch [94/120    avg_loss:0.012, val_acc:0.977]
Epoch [95/120    avg_loss:0.014, val_acc:0.978]
Epoch [96/120    avg_loss:0.014, val_acc:0.978]
Epoch [97/120    avg_loss:0.019, val_acc:0.979]
Epoch [98/120    avg_loss:0.014, val_acc:0.977]
Epoch [99/120    avg_loss:0.014, val_acc:0.978]
Epoch [100/120    avg_loss:0.012, val_acc:0.978]
Epoch [101/120    avg_loss:0.017, val_acc:0.978]
Epoch [102/120    avg_loss:0.012, val_acc:0.979]
Epoch [103/120    avg_loss:0.015, val_acc:0.979]
Epoch [104/120    avg_loss:0.013, val_acc:0.978]
Epoch [105/120    avg_loss:0.020, val_acc:0.976]
Epoch [106/120    avg_loss:0.014, val_acc:0.979]
Epoch [107/120    avg_loss:0.014, val_acc:0.978]
Epoch [108/120    avg_loss:0.012, val_acc:0.978]
Epoch [109/120    avg_loss:0.014, val_acc:0.978]
Epoch [110/120    avg_loss:0.013, val_acc:0.978]
Epoch [111/120    avg_loss:0.012, val_acc:0.978]
Epoch [112/120    avg_loss:0.014, val_acc:0.978]
Epoch [113/120    avg_loss:0.015, val_acc:0.979]
Epoch [114/120    avg_loss:0.013, val_acc:0.977]
Epoch [115/120    avg_loss:0.013, val_acc:0.978]
Epoch [116/120    avg_loss:0.012, val_acc:0.978]
Epoch [117/120    avg_loss:0.015, val_acc:0.978]
Epoch [118/120    avg_loss:0.011, val_acc:0.978]
Epoch [119/120    avg_loss:0.016, val_acc:0.978]
Epoch [120/120    avg_loss:0.016, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1247    1    2    0    0    0    0    1    6   28    0    0
     0    0    0]
 [   0    0    0  702    8   11    0    0    0    2   12    5    7    0
     0    0    0]
 [   0    0    0    0  210    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  425    0    3    0    0    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    1    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    1  858    9    0    0
     0    1    0]
 [   0    0    2    0    0    0    0    0    0    0   12 2184   12    0
     0    0    0]
 [   0    0    1    1    0    0    0    0    0    0   13    0  513    0
     3    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1116   22    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    29  312    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.52845528455285

F1 scores:
[       nan 0.97560976 0.98150335 0.96760855 0.96774194 0.97588978
 0.9939302  0.94339623 0.99650757 0.87179487 0.96567248 0.98467087
 0.95530726 1.         0.97212544 0.91495601 0.95808383]

Kappa:
0.9718170403475923
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f57bfff09e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.793, val_acc:0.295]
Epoch [2/120    avg_loss:2.614, val_acc:0.360]
Epoch [3/120    avg_loss:2.468, val_acc:0.410]
Epoch [4/120    avg_loss:2.343, val_acc:0.451]
Epoch [5/120    avg_loss:2.256, val_acc:0.474]
Epoch [6/120    avg_loss:2.168, val_acc:0.519]
Epoch [7/120    avg_loss:2.050, val_acc:0.558]
Epoch [8/120    avg_loss:1.946, val_acc:0.587]
Epoch [9/120    avg_loss:1.839, val_acc:0.585]
Epoch [10/120    avg_loss:1.729, val_acc:0.624]
Epoch [11/120    avg_loss:1.646, val_acc:0.654]
Epoch [12/120    avg_loss:1.484, val_acc:0.670]
Epoch [13/120    avg_loss:1.378, val_acc:0.718]
Epoch [14/120    avg_loss:1.354, val_acc:0.672]
Epoch [15/120    avg_loss:1.246, val_acc:0.708]
Epoch [16/120    avg_loss:1.124, val_acc:0.701]
Epoch [17/120    avg_loss:1.080, val_acc:0.717]
Epoch [18/120    avg_loss:0.922, val_acc:0.797]
Epoch [19/120    avg_loss:0.827, val_acc:0.809]
Epoch [20/120    avg_loss:0.752, val_acc:0.756]
Epoch [21/120    avg_loss:0.741, val_acc:0.795]
Epoch [22/120    avg_loss:0.615, val_acc:0.834]
Epoch [23/120    avg_loss:0.569, val_acc:0.822]
Epoch [24/120    avg_loss:0.506, val_acc:0.824]
Epoch [25/120    avg_loss:0.438, val_acc:0.849]
Epoch [26/120    avg_loss:0.364, val_acc:0.846]
Epoch [27/120    avg_loss:0.355, val_acc:0.888]
Epoch [28/120    avg_loss:0.324, val_acc:0.871]
Epoch [29/120    avg_loss:0.367, val_acc:0.844]
Epoch [30/120    avg_loss:0.313, val_acc:0.902]
Epoch [31/120    avg_loss:0.338, val_acc:0.857]
Epoch [32/120    avg_loss:0.319, val_acc:0.879]
Epoch [33/120    avg_loss:0.251, val_acc:0.923]
Epoch [34/120    avg_loss:0.198, val_acc:0.882]
Epoch [35/120    avg_loss:0.281, val_acc:0.864]
Epoch [36/120    avg_loss:0.248, val_acc:0.881]
Epoch [37/120    avg_loss:0.219, val_acc:0.869]
Epoch [38/120    avg_loss:0.207, val_acc:0.902]
Epoch [39/120    avg_loss:0.188, val_acc:0.928]
Epoch [40/120    avg_loss:0.182, val_acc:0.920]
Epoch [41/120    avg_loss:0.146, val_acc:0.928]
Epoch [42/120    avg_loss:0.119, val_acc:0.917]
Epoch [43/120    avg_loss:0.121, val_acc:0.937]
Epoch [44/120    avg_loss:0.117, val_acc:0.946]
Epoch [45/120    avg_loss:0.085, val_acc:0.937]
Epoch [46/120    avg_loss:0.127, val_acc:0.917]
Epoch [47/120    avg_loss:0.120, val_acc:0.932]
Epoch [48/120    avg_loss:0.092, val_acc:0.943]
Epoch [49/120    avg_loss:0.101, val_acc:0.931]
Epoch [50/120    avg_loss:0.091, val_acc:0.932]
Epoch [51/120    avg_loss:0.088, val_acc:0.958]
Epoch [52/120    avg_loss:0.068, val_acc:0.953]
Epoch [53/120    avg_loss:0.055, val_acc:0.951]
Epoch [54/120    avg_loss:0.086, val_acc:0.954]
Epoch [55/120    avg_loss:0.096, val_acc:0.947]
Epoch [56/120    avg_loss:0.088, val_acc:0.954]
Epoch [57/120    avg_loss:0.072, val_acc:0.965]
Epoch [58/120    avg_loss:0.079, val_acc:0.957]
Epoch [59/120    avg_loss:0.062, val_acc:0.951]
Epoch [60/120    avg_loss:0.052, val_acc:0.969]
Epoch [61/120    avg_loss:0.048, val_acc:0.952]
Epoch [62/120    avg_loss:0.054, val_acc:0.959]
Epoch [63/120    avg_loss:0.045, val_acc:0.948]
Epoch [64/120    avg_loss:0.039, val_acc:0.968]
Epoch [65/120    avg_loss:0.034, val_acc:0.973]
Epoch [66/120    avg_loss:0.039, val_acc:0.962]
Epoch [67/120    avg_loss:0.049, val_acc:0.941]
Epoch [68/120    avg_loss:0.104, val_acc:0.877]
Epoch [69/120    avg_loss:0.314, val_acc:0.918]
Epoch [70/120    avg_loss:0.157, val_acc:0.927]
Epoch [71/120    avg_loss:0.105, val_acc:0.941]
Epoch [72/120    avg_loss:0.092, val_acc:0.951]
Epoch [73/120    avg_loss:0.068, val_acc:0.958]
Epoch [74/120    avg_loss:0.053, val_acc:0.936]
Epoch [75/120    avg_loss:0.047, val_acc:0.969]
Epoch [76/120    avg_loss:0.047, val_acc:0.959]
Epoch [77/120    avg_loss:0.047, val_acc:0.966]
Epoch [78/120    avg_loss:0.062, val_acc:0.955]
Epoch [79/120    avg_loss:0.043, val_acc:0.960]
Epoch [80/120    avg_loss:0.038, val_acc:0.962]
Epoch [81/120    avg_loss:0.032, val_acc:0.965]
Epoch [82/120    avg_loss:0.027, val_acc:0.967]
Epoch [83/120    avg_loss:0.035, val_acc:0.968]
Epoch [84/120    avg_loss:0.030, val_acc:0.970]
Epoch [85/120    avg_loss:0.025, val_acc:0.966]
Epoch [86/120    avg_loss:0.026, val_acc:0.971]
Epoch [87/120    avg_loss:0.028, val_acc:0.973]
Epoch [88/120    avg_loss:0.032, val_acc:0.976]
Epoch [89/120    avg_loss:0.031, val_acc:0.976]
Epoch [90/120    avg_loss:0.031, val_acc:0.974]
Epoch [91/120    avg_loss:0.025, val_acc:0.971]
Epoch [92/120    avg_loss:0.026, val_acc:0.973]
Epoch [93/120    avg_loss:0.027, val_acc:0.975]
Epoch [94/120    avg_loss:0.031, val_acc:0.981]
Epoch [95/120    avg_loss:0.024, val_acc:0.974]
Epoch [96/120    avg_loss:0.024, val_acc:0.975]
Epoch [97/120    avg_loss:0.026, val_acc:0.975]
Epoch [98/120    avg_loss:0.026, val_acc:0.978]
Epoch [99/120    avg_loss:0.025, val_acc:0.976]
Epoch [100/120    avg_loss:0.028, val_acc:0.978]
Epoch [101/120    avg_loss:0.019, val_acc:0.978]
Epoch [102/120    avg_loss:0.025, val_acc:0.979]
Epoch [103/120    avg_loss:0.019, val_acc:0.979]
Epoch [104/120    avg_loss:0.021, val_acc:0.978]
Epoch [105/120    avg_loss:0.021, val_acc:0.979]
Epoch [106/120    avg_loss:0.021, val_acc:0.980]
Epoch [107/120    avg_loss:0.025, val_acc:0.979]
Epoch [108/120    avg_loss:0.020, val_acc:0.978]
Epoch [109/120    avg_loss:0.019, val_acc:0.978]
Epoch [110/120    avg_loss:0.022, val_acc:0.978]
Epoch [111/120    avg_loss:0.022, val_acc:0.978]
Epoch [112/120    avg_loss:0.024, val_acc:0.978]
Epoch [113/120    avg_loss:0.025, val_acc:0.978]
Epoch [114/120    avg_loss:0.020, val_acc:0.979]
Epoch [115/120    avg_loss:0.019, val_acc:0.979]
Epoch [116/120    avg_loss:0.032, val_acc:0.978]
Epoch [117/120    avg_loss:0.017, val_acc:0.979]
Epoch [118/120    avg_loss:0.017, val_acc:0.979]
Epoch [119/120    avg_loss:0.023, val_acc:0.979]
Epoch [120/120    avg_loss:0.022, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    2 1248    6    9    0    0    0    0    1    5   13    1    0
     0    0    0]
 [   0    0    0  668    4    0    0    0    0   20    3    6   46    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    1    1    0    0    0    2    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    1    1    0    0    0  846   20    2    0
     0    1    0]
 [   0    8    2    0    0    0    0    0    0    0    8 2173   19    0
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    0  527    0
     1    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1116   23    0]
 [   0    0    0    0    0    0   32    0    0    0    0    0    0    0
     4  311    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.17073170731707

F1 scores:
[       nan 0.87234043 0.9830642  0.94018297 0.97038724 0.98959538
 0.97401633 0.98039216 0.997669   0.63157895 0.9735328  0.9823689
 0.93109541 1.         0.98586572 0.90935673 0.97005988]

Kappa:
0.9677692657324094
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f88af301a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.773, val_acc:0.264]
Epoch [2/120    avg_loss:2.642, val_acc:0.416]
Epoch [3/120    avg_loss:2.467, val_acc:0.416]
Epoch [4/120    avg_loss:2.383, val_acc:0.436]
Epoch [5/120    avg_loss:2.277, val_acc:0.451]
Epoch [6/120    avg_loss:2.158, val_acc:0.489]
Epoch [7/120    avg_loss:2.042, val_acc:0.497]
Epoch [8/120    avg_loss:1.968, val_acc:0.529]
Epoch [9/120    avg_loss:1.861, val_acc:0.538]
Epoch [10/120    avg_loss:1.790, val_acc:0.542]
Epoch [11/120    avg_loss:1.678, val_acc:0.563]
Epoch [12/120    avg_loss:1.595, val_acc:0.577]
Epoch [13/120    avg_loss:1.520, val_acc:0.606]
Epoch [14/120    avg_loss:1.392, val_acc:0.661]
Epoch [15/120    avg_loss:1.297, val_acc:0.668]
Epoch [16/120    avg_loss:1.175, val_acc:0.704]
Epoch [17/120    avg_loss:1.067, val_acc:0.666]
Epoch [18/120    avg_loss:0.948, val_acc:0.724]
Epoch [19/120    avg_loss:0.877, val_acc:0.750]
Epoch [20/120    avg_loss:0.755, val_acc:0.689]
Epoch [21/120    avg_loss:0.756, val_acc:0.735]
Epoch [22/120    avg_loss:0.620, val_acc:0.774]
Epoch [23/120    avg_loss:0.511, val_acc:0.775]
Epoch [24/120    avg_loss:0.500, val_acc:0.815]
Epoch [25/120    avg_loss:0.463, val_acc:0.837]
Epoch [26/120    avg_loss:0.415, val_acc:0.850]
Epoch [27/120    avg_loss:0.414, val_acc:0.844]
Epoch [28/120    avg_loss:0.388, val_acc:0.832]
Epoch [29/120    avg_loss:0.346, val_acc:0.869]
Epoch [30/120    avg_loss:0.303, val_acc:0.866]
Epoch [31/120    avg_loss:0.264, val_acc:0.835]
Epoch [32/120    avg_loss:0.470, val_acc:0.875]
Epoch [33/120    avg_loss:0.320, val_acc:0.831]
Epoch [34/120    avg_loss:0.278, val_acc:0.878]
Epoch [35/120    avg_loss:0.260, val_acc:0.889]
Epoch [36/120    avg_loss:0.227, val_acc:0.885]
Epoch [37/120    avg_loss:0.195, val_acc:0.921]
Epoch [38/120    avg_loss:0.306, val_acc:0.851]
Epoch [39/120    avg_loss:0.243, val_acc:0.880]
Epoch [40/120    avg_loss:0.175, val_acc:0.897]
Epoch [41/120    avg_loss:0.233, val_acc:0.903]
Epoch [42/120    avg_loss:0.206, val_acc:0.880]
Epoch [43/120    avg_loss:0.188, val_acc:0.877]
Epoch [44/120    avg_loss:0.151, val_acc:0.920]
Epoch [45/120    avg_loss:0.134, val_acc:0.917]
Epoch [46/120    avg_loss:0.121, val_acc:0.906]
Epoch [47/120    avg_loss:0.115, val_acc:0.921]
Epoch [48/120    avg_loss:0.127, val_acc:0.922]
Epoch [49/120    avg_loss:0.117, val_acc:0.908]
Epoch [50/120    avg_loss:0.115, val_acc:0.941]
Epoch [51/120    avg_loss:0.097, val_acc:0.938]
Epoch [52/120    avg_loss:0.116, val_acc:0.924]
Epoch [53/120    avg_loss:0.090, val_acc:0.948]
Epoch [54/120    avg_loss:0.130, val_acc:0.932]
Epoch [55/120    avg_loss:0.089, val_acc:0.948]
Epoch [56/120    avg_loss:0.078, val_acc:0.929]
Epoch [57/120    avg_loss:0.096, val_acc:0.940]
Epoch [58/120    avg_loss:0.095, val_acc:0.947]
Epoch [59/120    avg_loss:0.080, val_acc:0.943]
Epoch [60/120    avg_loss:0.073, val_acc:0.944]
Epoch [61/120    avg_loss:0.072, val_acc:0.941]
Epoch [62/120    avg_loss:0.067, val_acc:0.952]
Epoch [63/120    avg_loss:0.063, val_acc:0.960]
Epoch [64/120    avg_loss:0.072, val_acc:0.953]
Epoch [65/120    avg_loss:0.054, val_acc:0.941]
Epoch [66/120    avg_loss:0.051, val_acc:0.952]
Epoch [67/120    avg_loss:0.049, val_acc:0.950]
Epoch [68/120    avg_loss:0.052, val_acc:0.954]
Epoch [69/120    avg_loss:0.077, val_acc:0.939]
Epoch [70/120    avg_loss:0.063, val_acc:0.946]
Epoch [71/120    avg_loss:0.056, val_acc:0.960]
Epoch [72/120    avg_loss:0.049, val_acc:0.953]
Epoch [73/120    avg_loss:0.053, val_acc:0.965]
Epoch [74/120    avg_loss:0.052, val_acc:0.963]
Epoch [75/120    avg_loss:0.055, val_acc:0.966]
Epoch [76/120    avg_loss:0.033, val_acc:0.966]
Epoch [77/120    avg_loss:0.034, val_acc:0.968]
Epoch [78/120    avg_loss:0.031, val_acc:0.970]
Epoch [79/120    avg_loss:0.027, val_acc:0.973]
Epoch [80/120    avg_loss:0.033, val_acc:0.975]
Epoch [81/120    avg_loss:0.022, val_acc:0.975]
Epoch [82/120    avg_loss:0.025, val_acc:0.979]
Epoch [83/120    avg_loss:0.032, val_acc:0.978]
Epoch [84/120    avg_loss:0.026, val_acc:0.970]
Epoch [85/120    avg_loss:0.025, val_acc:0.969]
Epoch [86/120    avg_loss:0.018, val_acc:0.980]
Epoch [87/120    avg_loss:0.025, val_acc:0.981]
Epoch [88/120    avg_loss:0.025, val_acc:0.971]
Epoch [89/120    avg_loss:0.028, val_acc:0.969]
Epoch [90/120    avg_loss:0.027, val_acc:0.959]
Epoch [91/120    avg_loss:0.035, val_acc:0.974]
Epoch [92/120    avg_loss:0.023, val_acc:0.973]
Epoch [93/120    avg_loss:0.024, val_acc:0.978]
Epoch [94/120    avg_loss:0.025, val_acc:0.972]
Epoch [95/120    avg_loss:0.023, val_acc:0.971]
Epoch [96/120    avg_loss:0.035, val_acc:0.967]
Epoch [97/120    avg_loss:0.035, val_acc:0.975]
Epoch [98/120    avg_loss:0.019, val_acc:0.972]
Epoch [99/120    avg_loss:0.016, val_acc:0.979]
Epoch [100/120    avg_loss:0.021, val_acc:0.968]
Epoch [101/120    avg_loss:0.019, val_acc:0.978]
Epoch [102/120    avg_loss:0.012, val_acc:0.982]
Epoch [103/120    avg_loss:0.012, val_acc:0.979]
Epoch [104/120    avg_loss:0.012, val_acc:0.978]
Epoch [105/120    avg_loss:0.012, val_acc:0.981]
Epoch [106/120    avg_loss:0.013, val_acc:0.982]
Epoch [107/120    avg_loss:0.015, val_acc:0.984]
Epoch [108/120    avg_loss:0.010, val_acc:0.984]
Epoch [109/120    avg_loss:0.010, val_acc:0.985]
Epoch [110/120    avg_loss:0.012, val_acc:0.983]
Epoch [111/120    avg_loss:0.012, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.983]
Epoch [113/120    avg_loss:0.012, val_acc:0.985]
Epoch [114/120    avg_loss:0.011, val_acc:0.986]
Epoch [115/120    avg_loss:0.012, val_acc:0.984]
Epoch [116/120    avg_loss:0.009, val_acc:0.985]
Epoch [117/120    avg_loss:0.010, val_acc:0.986]
Epoch [118/120    avg_loss:0.010, val_acc:0.985]
Epoch [119/120    avg_loss:0.009, val_acc:0.986]
Epoch [120/120    avg_loss:0.011, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1237    1    4    0    0    0    0    2    8   33    0    0
     0    0    0]
 [   0    0    0  704    3    0    3    0    0   16    3    5   13    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    3    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    4    0    0    0  849   18    0    0
     0    1    0]
 [   0    0    4    0    0    0    2    0    0    1    3 2189   10    1
     0    0    0]
 [   0    0    0    5    2    0    0    0    0    2    5    0  513    0
     2    2    3]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1133    5    0]
 [   0    0    0    0    0    0   40    0    0    2    0    0    0    0
    32  273    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.27913279132791

F1 scores:
[       nan 0.98765432 0.97825227 0.96636925 0.97931034 0.99188876
 0.96182085 0.94339623 1.         0.59016393 0.97362385 0.98271605
 0.95798319 0.99459459 0.98052791 0.86942675 0.97647059]

Kappa:
0.9689633761605836
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff3fc86db00>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.769, val_acc:0.135]
Epoch [2/120    avg_loss:2.619, val_acc:0.216]
Epoch [3/120    avg_loss:2.490, val_acc:0.448]
Epoch [4/120    avg_loss:2.384, val_acc:0.512]
Epoch [5/120    avg_loss:2.258, val_acc:0.567]
Epoch [6/120    avg_loss:2.160, val_acc:0.571]
Epoch [7/120    avg_loss:2.087, val_acc:0.562]
Epoch [8/120    avg_loss:1.995, val_acc:0.565]
Epoch [9/120    avg_loss:1.860, val_acc:0.582]
Epoch [10/120    avg_loss:1.744, val_acc:0.621]
Epoch [11/120    avg_loss:1.627, val_acc:0.665]
Epoch [12/120    avg_loss:1.506, val_acc:0.694]
Epoch [13/120    avg_loss:1.423, val_acc:0.667]
Epoch [14/120    avg_loss:1.264, val_acc:0.730]
Epoch [15/120    avg_loss:1.130, val_acc:0.718]
Epoch [16/120    avg_loss:1.019, val_acc:0.712]
Epoch [17/120    avg_loss:0.950, val_acc:0.774]
Epoch [18/120    avg_loss:0.852, val_acc:0.762]
Epoch [19/120    avg_loss:0.776, val_acc:0.784]
Epoch [20/120    avg_loss:0.718, val_acc:0.796]
Epoch [21/120    avg_loss:0.670, val_acc:0.821]
Epoch [22/120    avg_loss:0.533, val_acc:0.840]
Epoch [23/120    avg_loss:0.552, val_acc:0.833]
Epoch [24/120    avg_loss:0.648, val_acc:0.812]
Epoch [25/120    avg_loss:0.550, val_acc:0.819]
Epoch [26/120    avg_loss:0.428, val_acc:0.854]
Epoch [27/120    avg_loss:0.405, val_acc:0.838]
Epoch [28/120    avg_loss:0.366, val_acc:0.853]
Epoch [29/120    avg_loss:0.368, val_acc:0.873]
Epoch [30/120    avg_loss:0.276, val_acc:0.903]
Epoch [31/120    avg_loss:0.419, val_acc:0.848]
Epoch [32/120    avg_loss:0.282, val_acc:0.863]
Epoch [33/120    avg_loss:0.271, val_acc:0.903]
Epoch [34/120    avg_loss:0.260, val_acc:0.888]
Epoch [35/120    avg_loss:0.238, val_acc:0.900]
Epoch [36/120    avg_loss:0.206, val_acc:0.923]
Epoch [37/120    avg_loss:0.182, val_acc:0.923]
Epoch [38/120    avg_loss:0.197, val_acc:0.927]
Epoch [39/120    avg_loss:0.168, val_acc:0.932]
Epoch [40/120    avg_loss:0.167, val_acc:0.897]
Epoch [41/120    avg_loss:0.227, val_acc:0.903]
Epoch [42/120    avg_loss:0.255, val_acc:0.887]
Epoch [43/120    avg_loss:0.194, val_acc:0.917]
Epoch [44/120    avg_loss:0.163, val_acc:0.937]
Epoch [45/120    avg_loss:0.125, val_acc:0.946]
Epoch [46/120    avg_loss:0.134, val_acc:0.938]
Epoch [47/120    avg_loss:0.116, val_acc:0.958]
Epoch [48/120    avg_loss:0.108, val_acc:0.950]
Epoch [49/120    avg_loss:0.136, val_acc:0.949]
Epoch [50/120    avg_loss:0.113, val_acc:0.948]
Epoch [51/120    avg_loss:0.081, val_acc:0.968]
Epoch [52/120    avg_loss:0.097, val_acc:0.953]
Epoch [53/120    avg_loss:0.088, val_acc:0.934]
Epoch [54/120    avg_loss:0.088, val_acc:0.959]
Epoch [55/120    avg_loss:0.100, val_acc:0.963]
Epoch [56/120    avg_loss:0.078, val_acc:0.939]
Epoch [57/120    avg_loss:0.099, val_acc:0.931]
Epoch [58/120    avg_loss:0.123, val_acc:0.938]
Epoch [59/120    avg_loss:0.081, val_acc:0.963]
Epoch [60/120    avg_loss:0.071, val_acc:0.963]
Epoch [61/120    avg_loss:0.065, val_acc:0.957]
Epoch [62/120    avg_loss:0.058, val_acc:0.955]
Epoch [63/120    avg_loss:0.071, val_acc:0.972]
Epoch [64/120    avg_loss:0.054, val_acc:0.969]
Epoch [65/120    avg_loss:0.060, val_acc:0.962]
Epoch [66/120    avg_loss:0.062, val_acc:0.965]
Epoch [67/120    avg_loss:0.049, val_acc:0.956]
Epoch [68/120    avg_loss:0.044, val_acc:0.963]
Epoch [69/120    avg_loss:0.051, val_acc:0.964]
Epoch [70/120    avg_loss:0.061, val_acc:0.962]
Epoch [71/120    avg_loss:0.050, val_acc:0.973]
Epoch [72/120    avg_loss:0.053, val_acc:0.953]
Epoch [73/120    avg_loss:0.045, val_acc:0.966]
Epoch [74/120    avg_loss:0.053, val_acc:0.965]
Epoch [75/120    avg_loss:0.085, val_acc:0.924]
Epoch [76/120    avg_loss:0.202, val_acc:0.915]
Epoch [77/120    avg_loss:0.109, val_acc:0.954]
Epoch [78/120    avg_loss:0.069, val_acc:0.960]
Epoch [79/120    avg_loss:0.052, val_acc:0.963]
Epoch [80/120    avg_loss:0.059, val_acc:0.968]
Epoch [81/120    avg_loss:0.065, val_acc:0.971]
Epoch [82/120    avg_loss:0.054, val_acc:0.946]
Epoch [83/120    avg_loss:0.071, val_acc:0.904]
Epoch [84/120    avg_loss:0.070, val_acc:0.952]
Epoch [85/120    avg_loss:0.060, val_acc:0.967]
Epoch [86/120    avg_loss:0.042, val_acc:0.972]
Epoch [87/120    avg_loss:0.036, val_acc:0.972]
Epoch [88/120    avg_loss:0.039, val_acc:0.971]
Epoch [89/120    avg_loss:0.034, val_acc:0.974]
Epoch [90/120    avg_loss:0.030, val_acc:0.973]
Epoch [91/120    avg_loss:0.030, val_acc:0.975]
Epoch [92/120    avg_loss:0.030, val_acc:0.975]
Epoch [93/120    avg_loss:0.032, val_acc:0.978]
Epoch [94/120    avg_loss:0.026, val_acc:0.978]
Epoch [95/120    avg_loss:0.027, val_acc:0.978]
Epoch [96/120    avg_loss:0.027, val_acc:0.978]
Epoch [97/120    avg_loss:0.031, val_acc:0.976]
Epoch [98/120    avg_loss:0.027, val_acc:0.975]
Epoch [99/120    avg_loss:0.028, val_acc:0.978]
Epoch [100/120    avg_loss:0.027, val_acc:0.976]
Epoch [101/120    avg_loss:0.027, val_acc:0.976]
Epoch [102/120    avg_loss:0.032, val_acc:0.978]
Epoch [103/120    avg_loss:0.025, val_acc:0.975]
Epoch [104/120    avg_loss:0.026, val_acc:0.975]
Epoch [105/120    avg_loss:0.025, val_acc:0.976]
Epoch [106/120    avg_loss:0.028, val_acc:0.974]
Epoch [107/120    avg_loss:0.025, val_acc:0.976]
Epoch [108/120    avg_loss:0.021, val_acc:0.978]
Epoch [109/120    avg_loss:0.027, val_acc:0.976]
Epoch [110/120    avg_loss:0.026, val_acc:0.980]
Epoch [111/120    avg_loss:0.023, val_acc:0.980]
Epoch [112/120    avg_loss:0.024, val_acc:0.979]
Epoch [113/120    avg_loss:0.022, val_acc:0.979]
Epoch [114/120    avg_loss:0.025, val_acc:0.978]
Epoch [115/120    avg_loss:0.021, val_acc:0.979]
Epoch [116/120    avg_loss:0.022, val_acc:0.978]
Epoch [117/120    avg_loss:0.018, val_acc:0.978]
Epoch [118/120    avg_loss:0.020, val_acc:0.978]
Epoch [119/120    avg_loss:0.023, val_acc:0.975]
Epoch [120/120    avg_loss:0.022, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1237    0    5    0    0    0    0    0   19   24    0    0
     0    0    0]
 [   0    0    0  726    9    0    0    0    0    2    1    0    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  419    0    1    0    1    0    0    0    0
    14    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    2    0
     0    1    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   12    0    0    5    2    0    0    0  846   10    0    0
     0    0    0]
 [   0    0   17    0    0    0    1    0    0    0   14 2174    2    2
     0    0    0]
 [   0    0    5    3    2    2    0    0    0    0    5    8  506    0
     0    0    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    2    0    0    2    0    0    0    0    1    0    0    0
  1131    3    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    1
   114  231    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
96.56368563685636

F1 scores:
[       nan 0.96202532 0.96716185 0.98373984 0.9638009  0.96990741
 0.99391172 0.98039216 0.99649942 0.89473684 0.95918367 0.98237686
 0.95742668 0.98924731 0.94171524 0.79381443 0.96428571]

Kappa:
0.9607740459494662
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f53e3c33b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.774, val_acc:0.178]
Epoch [2/120    avg_loss:2.591, val_acc:0.476]
Epoch [3/120    avg_loss:2.427, val_acc:0.482]
Epoch [4/120    avg_loss:2.332, val_acc:0.486]
Epoch [5/120    avg_loss:2.195, val_acc:0.520]
Epoch [6/120    avg_loss:2.098, val_acc:0.538]
Epoch [7/120    avg_loss:2.021, val_acc:0.552]
Epoch [8/120    avg_loss:1.911, val_acc:0.562]
Epoch [9/120    avg_loss:1.758, val_acc:0.615]
Epoch [10/120    avg_loss:1.624, val_acc:0.628]
Epoch [11/120    avg_loss:1.543, val_acc:0.630]
Epoch [12/120    avg_loss:1.395, val_acc:0.638]
Epoch [13/120    avg_loss:1.279, val_acc:0.655]
Epoch [14/120    avg_loss:1.174, val_acc:0.698]
Epoch [15/120    avg_loss:1.085, val_acc:0.692]
Epoch [16/120    avg_loss:1.011, val_acc:0.708]
Epoch [17/120    avg_loss:0.926, val_acc:0.727]
Epoch [18/120    avg_loss:0.791, val_acc:0.765]
Epoch [19/120    avg_loss:0.803, val_acc:0.779]
Epoch [20/120    avg_loss:0.671, val_acc:0.812]
Epoch [21/120    avg_loss:0.629, val_acc:0.805]
Epoch [22/120    avg_loss:0.612, val_acc:0.794]
Epoch [23/120    avg_loss:0.581, val_acc:0.782]
Epoch [24/120    avg_loss:0.561, val_acc:0.857]
Epoch [25/120    avg_loss:0.454, val_acc:0.850]
Epoch [26/120    avg_loss:0.399, val_acc:0.875]
Epoch [27/120    avg_loss:0.371, val_acc:0.870]
Epoch [28/120    avg_loss:0.452, val_acc:0.865]
Epoch [29/120    avg_loss:0.332, val_acc:0.893]
Epoch [30/120    avg_loss:0.271, val_acc:0.893]
Epoch [31/120    avg_loss:0.275, val_acc:0.866]
Epoch [32/120    avg_loss:0.252, val_acc:0.907]
Epoch [33/120    avg_loss:0.265, val_acc:0.875]
Epoch [34/120    avg_loss:0.237, val_acc:0.898]
Epoch [35/120    avg_loss:0.201, val_acc:0.927]
Epoch [36/120    avg_loss:0.157, val_acc:0.930]
Epoch [37/120    avg_loss:0.166, val_acc:0.923]
Epoch [38/120    avg_loss:0.195, val_acc:0.880]
Epoch [39/120    avg_loss:0.229, val_acc:0.920]
Epoch [40/120    avg_loss:0.209, val_acc:0.846]
Epoch [41/120    avg_loss:0.210, val_acc:0.914]
Epoch [42/120    avg_loss:0.129, val_acc:0.921]
Epoch [43/120    avg_loss:0.125, val_acc:0.913]
Epoch [44/120    avg_loss:0.158, val_acc:0.908]
Epoch [45/120    avg_loss:0.117, val_acc:0.939]
Epoch [46/120    avg_loss:0.106, val_acc:0.940]
Epoch [47/120    avg_loss:0.088, val_acc:0.939]
Epoch [48/120    avg_loss:0.086, val_acc:0.953]
Epoch [49/120    avg_loss:0.083, val_acc:0.942]
Epoch [50/120    avg_loss:0.096, val_acc:0.954]
Epoch [51/120    avg_loss:0.071, val_acc:0.962]
Epoch [52/120    avg_loss:0.096, val_acc:0.939]
Epoch [53/120    avg_loss:0.120, val_acc:0.945]
Epoch [54/120    avg_loss:0.091, val_acc:0.936]
Epoch [55/120    avg_loss:0.059, val_acc:0.958]
Epoch [56/120    avg_loss:0.067, val_acc:0.945]
Epoch [57/120    avg_loss:0.065, val_acc:0.957]
Epoch [58/120    avg_loss:0.064, val_acc:0.956]
Epoch [59/120    avg_loss:0.067, val_acc:0.944]
Epoch [60/120    avg_loss:0.069, val_acc:0.958]
Epoch [61/120    avg_loss:0.066, val_acc:0.958]
Epoch [62/120    avg_loss:0.065, val_acc:0.968]
Epoch [63/120    avg_loss:0.076, val_acc:0.967]
Epoch [64/120    avg_loss:0.065, val_acc:0.961]
Epoch [65/120    avg_loss:0.133, val_acc:0.951]
Epoch [66/120    avg_loss:0.191, val_acc:0.913]
Epoch [67/120    avg_loss:0.114, val_acc:0.922]
Epoch [68/120    avg_loss:0.134, val_acc:0.954]
Epoch [69/120    avg_loss:0.078, val_acc:0.951]
Epoch [70/120    avg_loss:0.068, val_acc:0.958]
Epoch [71/120    avg_loss:0.105, val_acc:0.950]
Epoch [72/120    avg_loss:0.062, val_acc:0.963]
Epoch [73/120    avg_loss:0.040, val_acc:0.973]
Epoch [74/120    avg_loss:0.034, val_acc:0.974]
Epoch [75/120    avg_loss:0.033, val_acc:0.976]
Epoch [76/120    avg_loss:0.041, val_acc:0.971]
Epoch [77/120    avg_loss:0.041, val_acc:0.969]
Epoch [78/120    avg_loss:0.033, val_acc:0.968]
Epoch [79/120    avg_loss:0.039, val_acc:0.976]
Epoch [80/120    avg_loss:0.033, val_acc:0.971]
Epoch [81/120    avg_loss:0.042, val_acc:0.974]
Epoch [82/120    avg_loss:0.034, val_acc:0.963]
Epoch [83/120    avg_loss:0.032, val_acc:0.952]
Epoch [84/120    avg_loss:0.028, val_acc:0.978]
Epoch [85/120    avg_loss:0.033, val_acc:0.954]
Epoch [86/120    avg_loss:0.049, val_acc:0.977]
Epoch [87/120    avg_loss:0.028, val_acc:0.973]
Epoch [88/120    avg_loss:0.036, val_acc:0.970]
Epoch [89/120    avg_loss:0.023, val_acc:0.978]
Epoch [90/120    avg_loss:0.024, val_acc:0.977]
Epoch [91/120    avg_loss:0.019, val_acc:0.975]
Epoch [92/120    avg_loss:0.018, val_acc:0.976]
Epoch [93/120    avg_loss:0.037, val_acc:0.980]
Epoch [94/120    avg_loss:0.021, val_acc:0.984]
Epoch [95/120    avg_loss:0.027, val_acc:0.973]
Epoch [96/120    avg_loss:0.021, val_acc:0.978]
Epoch [97/120    avg_loss:0.025, val_acc:0.973]
Epoch [98/120    avg_loss:0.018, val_acc:0.976]
Epoch [99/120    avg_loss:0.019, val_acc:0.971]
Epoch [100/120    avg_loss:0.019, val_acc:0.978]
Epoch [101/120    avg_loss:0.023, val_acc:0.968]
Epoch [102/120    avg_loss:0.021, val_acc:0.977]
Epoch [103/120    avg_loss:0.016, val_acc:0.976]
Epoch [104/120    avg_loss:0.012, val_acc:0.982]
Epoch [105/120    avg_loss:0.011, val_acc:0.981]
Epoch [106/120    avg_loss:0.014, val_acc:0.981]
Epoch [107/120    avg_loss:0.018, val_acc:0.980]
Epoch [108/120    avg_loss:0.020, val_acc:0.986]
Epoch [109/120    avg_loss:0.012, val_acc:0.986]
Epoch [110/120    avg_loss:0.014, val_acc:0.984]
Epoch [111/120    avg_loss:0.011, val_acc:0.984]
Epoch [112/120    avg_loss:0.010, val_acc:0.981]
Epoch [113/120    avg_loss:0.008, val_acc:0.984]
Epoch [114/120    avg_loss:0.010, val_acc:0.982]
Epoch [115/120    avg_loss:0.010, val_acc:0.982]
Epoch [116/120    avg_loss:0.009, val_acc:0.984]
Epoch [117/120    avg_loss:0.010, val_acc:0.985]
Epoch [118/120    avg_loss:0.009, val_acc:0.982]
Epoch [119/120    avg_loss:0.008, val_acc:0.981]
Epoch [120/120    avg_loss:0.010, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1266    2    1    1    0    0    0    0    0   14    1    0
     0    0    0]
 [   0    0    0  706   24    0    0    0    0    3    1    0   13    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    1    0    0    0    0  854   17    1    0
     0    0    0]
 [   0    0    5    0    0    0    2    0    0    0    8 2189    5    1
     0    0    0]
 [   0    0    0    2    0    1    0    0    0    0    2    1  526    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0    3    0    0    7    0    0    0    0
    14  323    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.34146341463415

F1 scores:
[       nan 0.975      0.989449   0.96911462 0.94456763 0.99656357
 0.99544765 1.         1.         0.7826087  0.98104538 0.98803882
 0.97317299 0.99730458 0.98639754 0.94306569 0.98809524]

Kappa:
0.9810939226409808
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f38b3e05a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.789, val_acc:0.322]
Epoch [2/120    avg_loss:2.638, val_acc:0.404]
Epoch [3/120    avg_loss:2.499, val_acc:0.486]
Epoch [4/120    avg_loss:2.361, val_acc:0.496]
Epoch [5/120    avg_loss:2.263, val_acc:0.542]
Epoch [6/120    avg_loss:2.153, val_acc:0.543]
Epoch [7/120    avg_loss:2.082, val_acc:0.556]
Epoch [8/120    avg_loss:1.986, val_acc:0.558]
Epoch [9/120    avg_loss:1.886, val_acc:0.546]
Epoch [10/120    avg_loss:1.848, val_acc:0.580]
Epoch [11/120    avg_loss:1.762, val_acc:0.595]
Epoch [12/120    avg_loss:1.697, val_acc:0.623]
Epoch [13/120    avg_loss:1.602, val_acc:0.638]
Epoch [14/120    avg_loss:1.556, val_acc:0.640]
Epoch [15/120    avg_loss:1.457, val_acc:0.669]
Epoch [16/120    avg_loss:1.350, val_acc:0.644]
Epoch [17/120    avg_loss:1.238, val_acc:0.692]
Epoch [18/120    avg_loss:1.100, val_acc:0.670]
Epoch [19/120    avg_loss:1.017, val_acc:0.719]
Epoch [20/120    avg_loss:0.987, val_acc:0.725]
Epoch [21/120    avg_loss:0.832, val_acc:0.721]
Epoch [22/120    avg_loss:0.839, val_acc:0.730]
Epoch [23/120    avg_loss:0.692, val_acc:0.759]
Epoch [24/120    avg_loss:0.696, val_acc:0.765]
Epoch [25/120    avg_loss:0.682, val_acc:0.741]
Epoch [26/120    avg_loss:0.642, val_acc:0.798]
Epoch [27/120    avg_loss:0.528, val_acc:0.803]
Epoch [28/120    avg_loss:0.487, val_acc:0.808]
Epoch [29/120    avg_loss:0.461, val_acc:0.839]
Epoch [30/120    avg_loss:0.530, val_acc:0.824]
Epoch [31/120    avg_loss:0.440, val_acc:0.841]
Epoch [32/120    avg_loss:0.359, val_acc:0.869]
Epoch [33/120    avg_loss:0.324, val_acc:0.876]
Epoch [34/120    avg_loss:0.307, val_acc:0.864]
Epoch [35/120    avg_loss:0.318, val_acc:0.887]
Epoch [36/120    avg_loss:0.246, val_acc:0.910]
Epoch [37/120    avg_loss:0.262, val_acc:0.870]
Epoch [38/120    avg_loss:0.219, val_acc:0.900]
Epoch [39/120    avg_loss:0.212, val_acc:0.899]
Epoch [40/120    avg_loss:0.191, val_acc:0.900]
Epoch [41/120    avg_loss:0.167, val_acc:0.917]
Epoch [42/120    avg_loss:0.202, val_acc:0.912]
Epoch [43/120    avg_loss:0.182, val_acc:0.904]
Epoch [44/120    avg_loss:0.166, val_acc:0.915]
Epoch [45/120    avg_loss:0.169, val_acc:0.916]
Epoch [46/120    avg_loss:0.220, val_acc:0.899]
Epoch [47/120    avg_loss:0.222, val_acc:0.912]
Epoch [48/120    avg_loss:0.247, val_acc:0.843]
Epoch [49/120    avg_loss:0.249, val_acc:0.912]
Epoch [50/120    avg_loss:0.184, val_acc:0.896]
Epoch [51/120    avg_loss:0.170, val_acc:0.929]
Epoch [52/120    avg_loss:0.126, val_acc:0.941]
Epoch [53/120    avg_loss:0.162, val_acc:0.934]
Epoch [54/120    avg_loss:0.121, val_acc:0.922]
Epoch [55/120    avg_loss:0.112, val_acc:0.931]
Epoch [56/120    avg_loss:0.079, val_acc:0.942]
Epoch [57/120    avg_loss:0.079, val_acc:0.943]
Epoch [58/120    avg_loss:0.095, val_acc:0.928]
Epoch [59/120    avg_loss:0.109, val_acc:0.933]
Epoch [60/120    avg_loss:0.111, val_acc:0.931]
Epoch [61/120    avg_loss:0.099, val_acc:0.943]
Epoch [62/120    avg_loss:0.084, val_acc:0.937]
Epoch [63/120    avg_loss:0.071, val_acc:0.951]
Epoch [64/120    avg_loss:0.078, val_acc:0.951]
Epoch [65/120    avg_loss:0.059, val_acc:0.936]
Epoch [66/120    avg_loss:0.071, val_acc:0.947]
Epoch [67/120    avg_loss:0.087, val_acc:0.943]
Epoch [68/120    avg_loss:0.066, val_acc:0.953]
Epoch [69/120    avg_loss:0.067, val_acc:0.963]
Epoch [70/120    avg_loss:0.052, val_acc:0.956]
Epoch [71/120    avg_loss:0.058, val_acc:0.948]
Epoch [72/120    avg_loss:0.072, val_acc:0.949]
Epoch [73/120    avg_loss:0.064, val_acc:0.950]
Epoch [74/120    avg_loss:0.043, val_acc:0.957]
Epoch [75/120    avg_loss:0.041, val_acc:0.948]
Epoch [76/120    avg_loss:0.036, val_acc:0.967]
Epoch [77/120    avg_loss:0.034, val_acc:0.964]
Epoch [78/120    avg_loss:0.043, val_acc:0.963]
Epoch [79/120    avg_loss:0.037, val_acc:0.967]
Epoch [80/120    avg_loss:0.044, val_acc:0.964]
Epoch [81/120    avg_loss:0.041, val_acc:0.953]
Epoch [82/120    avg_loss:0.051, val_acc:0.958]
Epoch [83/120    avg_loss:0.055, val_acc:0.955]
Epoch [84/120    avg_loss:0.038, val_acc:0.974]
Epoch [85/120    avg_loss:0.031, val_acc:0.958]
Epoch [86/120    avg_loss:0.033, val_acc:0.966]
Epoch [87/120    avg_loss:0.039, val_acc:0.968]
Epoch [88/120    avg_loss:0.025, val_acc:0.974]
Epoch [89/120    avg_loss:0.031, val_acc:0.970]
Epoch [90/120    avg_loss:0.034, val_acc:0.975]
Epoch [91/120    avg_loss:0.028, val_acc:0.971]
Epoch [92/120    avg_loss:0.039, val_acc:0.973]
Epoch [93/120    avg_loss:0.027, val_acc:0.978]
Epoch [94/120    avg_loss:0.020, val_acc:0.973]
Epoch [95/120    avg_loss:0.021, val_acc:0.971]
Epoch [96/120    avg_loss:0.020, val_acc:0.971]
Epoch [97/120    avg_loss:0.024, val_acc:0.975]
Epoch [98/120    avg_loss:0.027, val_acc:0.969]
Epoch [99/120    avg_loss:0.031, val_acc:0.970]
Epoch [100/120    avg_loss:0.027, val_acc:0.974]
Epoch [101/120    avg_loss:0.024, val_acc:0.982]
Epoch [102/120    avg_loss:0.020, val_acc:0.972]
Epoch [103/120    avg_loss:0.020, val_acc:0.975]
Epoch [104/120    avg_loss:0.020, val_acc:0.969]
Epoch [105/120    avg_loss:0.032, val_acc:0.964]
Epoch [106/120    avg_loss:0.047, val_acc:0.938]
Epoch [107/120    avg_loss:0.061, val_acc:0.962]
Epoch [108/120    avg_loss:0.038, val_acc:0.968]
Epoch [109/120    avg_loss:0.028, val_acc:0.965]
Epoch [110/120    avg_loss:0.023, val_acc:0.964]
Epoch [111/120    avg_loss:0.021, val_acc:0.974]
Epoch [112/120    avg_loss:0.016, val_acc:0.978]
Epoch [113/120    avg_loss:0.028, val_acc:0.970]
Epoch [114/120    avg_loss:0.016, val_acc:0.973]
Epoch [115/120    avg_loss:0.016, val_acc:0.980]
Epoch [116/120    avg_loss:0.015, val_acc:0.982]
Epoch [117/120    avg_loss:0.015, val_acc:0.982]
Epoch [118/120    avg_loss:0.014, val_acc:0.983]
Epoch [119/120    avg_loss:0.013, val_acc:0.980]
Epoch [120/120    avg_loss:0.011, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    2    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1261    0    0    0    0    0    0    0    5   19    0    0
     0    0    0]
 [   0    0    3  710    0    8    0    0    0   13    1    0   11    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   12    0    0    3    5    0    0    0  830   20    5    0
     0    0    0]
 [   0    0    8    0    0    0    0    1    2    1   18 2168    9    0
     1    2    0]
 [   0    0    0    0    0    0    0    0    0    2    0    0  530    0
     0    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    3    1    0    0    0    0    0    0
  1131    4    0]
 [   0    0    0    0    0    0   45    0    0    0    0    0    0    0
    10  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.53929539295393

F1 scores:
[       nan 0.96202532 0.98094127 0.97326936 1.         0.98524404
 0.96046852 0.94339623 0.99300699 0.64       0.95953757 0.98143957
 0.96980787 0.99459459 0.99167032 0.90542636 0.98823529]

Kappa:
0.9719489482176706
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4239190a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.758, val_acc:0.156]
Epoch [2/120    avg_loss:2.597, val_acc:0.296]
Epoch [3/120    avg_loss:2.470, val_acc:0.467]
Epoch [4/120    avg_loss:2.347, val_acc:0.465]
Epoch [5/120    avg_loss:2.250, val_acc:0.495]
Epoch [6/120    avg_loss:2.168, val_acc:0.521]
Epoch [7/120    avg_loss:2.060, val_acc:0.555]
Epoch [8/120    avg_loss:1.958, val_acc:0.578]
Epoch [9/120    avg_loss:1.876, val_acc:0.606]
Epoch [10/120    avg_loss:1.766, val_acc:0.643]
Epoch [11/120    avg_loss:1.676, val_acc:0.634]
Epoch [12/120    avg_loss:1.560, val_acc:0.640]
Epoch [13/120    avg_loss:1.471, val_acc:0.695]
Epoch [14/120    avg_loss:1.340, val_acc:0.680]
Epoch [15/120    avg_loss:1.224, val_acc:0.675]
Epoch [16/120    avg_loss:1.161, val_acc:0.712]
Epoch [17/120    avg_loss:1.088, val_acc:0.692]
Epoch [18/120    avg_loss:0.900, val_acc:0.762]
Epoch [19/120    avg_loss:0.780, val_acc:0.736]
Epoch [20/120    avg_loss:0.734, val_acc:0.718]
Epoch [21/120    avg_loss:0.746, val_acc:0.725]
Epoch [22/120    avg_loss:0.735, val_acc:0.748]
Epoch [23/120    avg_loss:0.642, val_acc:0.800]
Epoch [24/120    avg_loss:0.584, val_acc:0.779]
Epoch [25/120    avg_loss:0.594, val_acc:0.784]
Epoch [26/120    avg_loss:0.535, val_acc:0.826]
Epoch [27/120    avg_loss:0.453, val_acc:0.817]
Epoch [28/120    avg_loss:0.463, val_acc:0.827]
Epoch [29/120    avg_loss:0.453, val_acc:0.828]
Epoch [30/120    avg_loss:0.401, val_acc:0.836]
Epoch [31/120    avg_loss:0.361, val_acc:0.875]
Epoch [32/120    avg_loss:0.325, val_acc:0.845]
Epoch [33/120    avg_loss:0.323, val_acc:0.883]
Epoch [34/120    avg_loss:0.254, val_acc:0.878]
Epoch [35/120    avg_loss:0.277, val_acc:0.871]
Epoch [36/120    avg_loss:0.274, val_acc:0.877]
Epoch [37/120    avg_loss:0.234, val_acc:0.905]
Epoch [38/120    avg_loss:0.219, val_acc:0.898]
Epoch [39/120    avg_loss:0.193, val_acc:0.885]
Epoch [40/120    avg_loss:0.206, val_acc:0.897]
Epoch [41/120    avg_loss:0.201, val_acc:0.871]
Epoch [42/120    avg_loss:0.182, val_acc:0.916]
Epoch [43/120    avg_loss:0.183, val_acc:0.898]
Epoch [44/120    avg_loss:0.185, val_acc:0.908]
Epoch [45/120    avg_loss:0.170, val_acc:0.909]
Epoch [46/120    avg_loss:0.141, val_acc:0.944]
Epoch [47/120    avg_loss:0.124, val_acc:0.943]
Epoch [48/120    avg_loss:0.120, val_acc:0.929]
Epoch [49/120    avg_loss:0.155, val_acc:0.913]
Epoch [50/120    avg_loss:0.139, val_acc:0.913]
Epoch [51/120    avg_loss:0.107, val_acc:0.940]
Epoch [52/120    avg_loss:0.111, val_acc:0.941]
Epoch [53/120    avg_loss:0.106, val_acc:0.946]
Epoch [54/120    avg_loss:0.130, val_acc:0.925]
Epoch [55/120    avg_loss:0.094, val_acc:0.946]
Epoch [56/120    avg_loss:0.093, val_acc:0.951]
Epoch [57/120    avg_loss:0.072, val_acc:0.958]
Epoch [58/120    avg_loss:0.094, val_acc:0.932]
Epoch [59/120    avg_loss:0.122, val_acc:0.927]
Epoch [60/120    avg_loss:0.090, val_acc:0.939]
Epoch [61/120    avg_loss:0.072, val_acc:0.942]
Epoch [62/120    avg_loss:0.075, val_acc:0.939]
Epoch [63/120    avg_loss:0.086, val_acc:0.938]
Epoch [64/120    avg_loss:0.071, val_acc:0.961]
Epoch [65/120    avg_loss:0.055, val_acc:0.955]
Epoch [66/120    avg_loss:0.084, val_acc:0.953]
Epoch [67/120    avg_loss:0.063, val_acc:0.955]
Epoch [68/120    avg_loss:0.066, val_acc:0.962]
Epoch [69/120    avg_loss:0.062, val_acc:0.957]
Epoch [70/120    avg_loss:0.048, val_acc:0.964]
Epoch [71/120    avg_loss:0.034, val_acc:0.961]
Epoch [72/120    avg_loss:0.049, val_acc:0.957]
Epoch [73/120    avg_loss:0.062, val_acc:0.961]
Epoch [74/120    avg_loss:0.042, val_acc:0.963]
Epoch [75/120    avg_loss:0.038, val_acc:0.974]
Epoch [76/120    avg_loss:0.027, val_acc:0.971]
Epoch [77/120    avg_loss:0.044, val_acc:0.946]
Epoch [78/120    avg_loss:0.063, val_acc:0.954]
Epoch [79/120    avg_loss:0.093, val_acc:0.952]
Epoch [80/120    avg_loss:0.104, val_acc:0.952]
Epoch [81/120    avg_loss:0.053, val_acc:0.958]
Epoch [82/120    avg_loss:0.089, val_acc:0.950]
Epoch [83/120    avg_loss:0.058, val_acc:0.966]
Epoch [84/120    avg_loss:0.081, val_acc:0.959]
Epoch [85/120    avg_loss:0.065, val_acc:0.964]
Epoch [86/120    avg_loss:0.051, val_acc:0.955]
Epoch [87/120    avg_loss:0.059, val_acc:0.966]
Epoch [88/120    avg_loss:0.033, val_acc:0.968]
Epoch [89/120    avg_loss:0.030, val_acc:0.977]
Epoch [90/120    avg_loss:0.030, val_acc:0.977]
Epoch [91/120    avg_loss:0.029, val_acc:0.977]
Epoch [92/120    avg_loss:0.021, val_acc:0.976]
Epoch [93/120    avg_loss:0.023, val_acc:0.977]
Epoch [94/120    avg_loss:0.025, val_acc:0.977]
Epoch [95/120    avg_loss:0.025, val_acc:0.979]
Epoch [96/120    avg_loss:0.022, val_acc:0.978]
Epoch [97/120    avg_loss:0.019, val_acc:0.976]
Epoch [98/120    avg_loss:0.018, val_acc:0.977]
Epoch [99/120    avg_loss:0.019, val_acc:0.979]
Epoch [100/120    avg_loss:0.021, val_acc:0.977]
Epoch [101/120    avg_loss:0.020, val_acc:0.977]
Epoch [102/120    avg_loss:0.018, val_acc:0.975]
Epoch [103/120    avg_loss:0.017, val_acc:0.977]
Epoch [104/120    avg_loss:0.019, val_acc:0.976]
Epoch [105/120    avg_loss:0.016, val_acc:0.980]
Epoch [106/120    avg_loss:0.019, val_acc:0.978]
Epoch [107/120    avg_loss:0.015, val_acc:0.979]
Epoch [108/120    avg_loss:0.018, val_acc:0.975]
Epoch [109/120    avg_loss:0.017, val_acc:0.979]
Epoch [110/120    avg_loss:0.018, val_acc:0.978]
Epoch [111/120    avg_loss:0.017, val_acc:0.979]
Epoch [112/120    avg_loss:0.017, val_acc:0.978]
Epoch [113/120    avg_loss:0.021, val_acc:0.982]
Epoch [114/120    avg_loss:0.020, val_acc:0.979]
Epoch [115/120    avg_loss:0.017, val_acc:0.980]
Epoch [116/120    avg_loss:0.020, val_acc:0.979]
Epoch [117/120    avg_loss:0.016, val_acc:0.976]
Epoch [118/120    avg_loss:0.016, val_acc:0.979]
Epoch [119/120    avg_loss:0.020, val_acc:0.980]
Epoch [120/120    avg_loss:0.015, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1251    0    0    0    1    0    0    1    4   28    0    0
     0    0    0]
 [   0    0    0  691    8   11    0    0    0    7    3    0   23    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   19    0    0    5    4    0    0    0  832   12    0    0
     0    3    0]
 [   0    0    3    0    0    0    1    0    1    0   28 2172    4    1
     0    0    0]
 [   0    0    0    0    0   14    0    0    0    1    1    1  512    0
     2    0    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    1    1    0    0    0    0    0    0
  1126   11    0]
 [   0    0    0    0    0    0   15    0    0    4    0    0    0    0
    16  312    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.26829268292683

F1 scores:
[       nan 0.95238095 0.9781079  0.96105702 0.98156682 0.96213808
 0.98124531 0.98039216 0.995338   0.69230769 0.95412844 0.98147311
 0.95433364 0.98395722 0.98642138 0.92719168 0.98245614]

Kappa:
0.9688649283527718
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f68de279ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.804, val_acc:0.123]
Epoch [2/120    avg_loss:2.630, val_acc:0.354]
Epoch [3/120    avg_loss:2.484, val_acc:0.450]
Epoch [4/120    avg_loss:2.375, val_acc:0.536]
Epoch [5/120    avg_loss:2.272, val_acc:0.555]
Epoch [6/120    avg_loss:2.187, val_acc:0.560]
Epoch [7/120    avg_loss:2.049, val_acc:0.573]
Epoch [8/120    avg_loss:2.024, val_acc:0.550]
Epoch [9/120    avg_loss:1.878, val_acc:0.567]
Epoch [10/120    avg_loss:1.804, val_acc:0.609]
Epoch [11/120    avg_loss:1.677, val_acc:0.633]
Epoch [12/120    avg_loss:1.597, val_acc:0.646]
Epoch [13/120    avg_loss:1.491, val_acc:0.678]
Epoch [14/120    avg_loss:1.359, val_acc:0.675]
Epoch [15/120    avg_loss:1.280, val_acc:0.714]
Epoch [16/120    avg_loss:1.154, val_acc:0.768]
Epoch [17/120    avg_loss:1.060, val_acc:0.723]
Epoch [18/120    avg_loss:0.975, val_acc:0.788]
Epoch [19/120    avg_loss:0.938, val_acc:0.763]
Epoch [20/120    avg_loss:0.831, val_acc:0.708]
Epoch [21/120    avg_loss:0.860, val_acc:0.782]
Epoch [22/120    avg_loss:0.735, val_acc:0.805]
Epoch [23/120    avg_loss:0.622, val_acc:0.839]
Epoch [24/120    avg_loss:0.608, val_acc:0.833]
Epoch [25/120    avg_loss:0.566, val_acc:0.825]
Epoch [26/120    avg_loss:0.549, val_acc:0.838]
Epoch [27/120    avg_loss:0.436, val_acc:0.872]
Epoch [28/120    avg_loss:0.405, val_acc:0.864]
Epoch [29/120    avg_loss:0.420, val_acc:0.861]
Epoch [30/120    avg_loss:0.375, val_acc:0.862]
Epoch [31/120    avg_loss:0.330, val_acc:0.885]
Epoch [32/120    avg_loss:0.309, val_acc:0.895]
Epoch [33/120    avg_loss:0.275, val_acc:0.883]
Epoch [34/120    avg_loss:0.353, val_acc:0.867]
Epoch [35/120    avg_loss:0.512, val_acc:0.852]
Epoch [36/120    avg_loss:0.314, val_acc:0.895]
Epoch [37/120    avg_loss:0.283, val_acc:0.897]
Epoch [38/120    avg_loss:0.279, val_acc:0.898]
Epoch [39/120    avg_loss:0.232, val_acc:0.913]
Epoch [40/120    avg_loss:0.239, val_acc:0.923]
Epoch [41/120    avg_loss:0.175, val_acc:0.930]
Epoch [42/120    avg_loss:0.161, val_acc:0.914]
Epoch [43/120    avg_loss:0.185, val_acc:0.920]
Epoch [44/120    avg_loss:0.193, val_acc:0.922]
Epoch [45/120    avg_loss:0.185, val_acc:0.917]
Epoch [46/120    avg_loss:0.174, val_acc:0.936]
Epoch [47/120    avg_loss:0.137, val_acc:0.936]
Epoch [48/120    avg_loss:0.112, val_acc:0.947]
Epoch [49/120    avg_loss:0.129, val_acc:0.946]
Epoch [50/120    avg_loss:0.118, val_acc:0.946]
Epoch [51/120    avg_loss:0.090, val_acc:0.948]
Epoch [52/120    avg_loss:0.159, val_acc:0.851]
Epoch [53/120    avg_loss:0.278, val_acc:0.935]
Epoch [54/120    avg_loss:0.158, val_acc:0.929]
Epoch [55/120    avg_loss:0.130, val_acc:0.931]
Epoch [56/120    avg_loss:0.144, val_acc:0.942]
Epoch [57/120    avg_loss:0.127, val_acc:0.936]
Epoch [58/120    avg_loss:0.160, val_acc:0.924]
Epoch [59/120    avg_loss:0.191, val_acc:0.876]
Epoch [60/120    avg_loss:0.150, val_acc:0.959]
Epoch [61/120    avg_loss:0.195, val_acc:0.905]
Epoch [62/120    avg_loss:0.135, val_acc:0.944]
Epoch [63/120    avg_loss:0.119, val_acc:0.952]
Epoch [64/120    avg_loss:0.085, val_acc:0.955]
Epoch [65/120    avg_loss:0.070, val_acc:0.939]
Epoch [66/120    avg_loss:0.074, val_acc:0.957]
Epoch [67/120    avg_loss:0.067, val_acc:0.959]
Epoch [68/120    avg_loss:0.061, val_acc:0.957]
Epoch [69/120    avg_loss:0.088, val_acc:0.945]
Epoch [70/120    avg_loss:0.067, val_acc:0.967]
Epoch [71/120    avg_loss:0.059, val_acc:0.959]
Epoch [72/120    avg_loss:0.063, val_acc:0.955]
Epoch [73/120    avg_loss:0.068, val_acc:0.957]
Epoch [74/120    avg_loss:0.064, val_acc:0.958]
Epoch [75/120    avg_loss:0.060, val_acc:0.957]
Epoch [76/120    avg_loss:0.049, val_acc:0.957]
Epoch [77/120    avg_loss:0.047, val_acc:0.967]
Epoch [78/120    avg_loss:0.046, val_acc:0.969]
Epoch [79/120    avg_loss:0.045, val_acc:0.974]
Epoch [80/120    avg_loss:0.031, val_acc:0.967]
Epoch [81/120    avg_loss:0.047, val_acc:0.962]
Epoch [82/120    avg_loss:0.037, val_acc:0.977]
Epoch [83/120    avg_loss:0.044, val_acc:0.977]
Epoch [84/120    avg_loss:0.035, val_acc:0.976]
Epoch [85/120    avg_loss:0.032, val_acc:0.964]
Epoch [86/120    avg_loss:0.037, val_acc:0.964]
Epoch [87/120    avg_loss:0.028, val_acc:0.975]
Epoch [88/120    avg_loss:0.021, val_acc:0.970]
Epoch [89/120    avg_loss:0.047, val_acc:0.971]
Epoch [90/120    avg_loss:0.035, val_acc:0.976]
Epoch [91/120    avg_loss:0.031, val_acc:0.976]
Epoch [92/120    avg_loss:0.022, val_acc:0.980]
Epoch [93/120    avg_loss:0.025, val_acc:0.974]
Epoch [94/120    avg_loss:0.069, val_acc:0.955]
Epoch [95/120    avg_loss:0.071, val_acc:0.958]
Epoch [96/120    avg_loss:0.038, val_acc:0.977]
Epoch [97/120    avg_loss:0.026, val_acc:0.978]
Epoch [98/120    avg_loss:0.030, val_acc:0.973]
Epoch [99/120    avg_loss:0.031, val_acc:0.980]
Epoch [100/120    avg_loss:0.032, val_acc:0.971]
Epoch [101/120    avg_loss:0.027, val_acc:0.984]
Epoch [102/120    avg_loss:0.022, val_acc:0.981]
Epoch [103/120    avg_loss:0.029, val_acc:0.969]
Epoch [104/120    avg_loss:0.037, val_acc:0.980]
Epoch [105/120    avg_loss:0.026, val_acc:0.976]
Epoch [106/120    avg_loss:0.046, val_acc:0.975]
Epoch [107/120    avg_loss:0.023, val_acc:0.974]
Epoch [108/120    avg_loss:0.019, val_acc:0.981]
Epoch [109/120    avg_loss:0.014, val_acc:0.985]
Epoch [110/120    avg_loss:0.014, val_acc:0.982]
Epoch [111/120    avg_loss:0.022, val_acc:0.982]
Epoch [112/120    avg_loss:0.020, val_acc:0.979]
Epoch [113/120    avg_loss:0.032, val_acc:0.982]
Epoch [114/120    avg_loss:0.030, val_acc:0.973]
Epoch [115/120    avg_loss:0.017, val_acc:0.978]
Epoch [116/120    avg_loss:0.022, val_acc:0.957]
Epoch [117/120    avg_loss:0.018, val_acc:0.980]
Epoch [118/120    avg_loss:0.018, val_acc:0.976]
Epoch [119/120    avg_loss:0.018, val_acc:0.981]
Epoch [120/120    avg_loss:0.024, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    2    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1258    2    0    0    0    0    0    4    1   20    0    0
     0    0    0]
 [   0    0    0  683    9    7    0    0    0    1    3    0   44    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    4    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    9    2    0    0    0  845   11    5    0
     0    2    0]
 [   0    0    0    0    0    0    2    0    0    0   14 2177   17    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    1    0    1  527    0
     2    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    1    0    0    0
  1128    8    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    30  303    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.50677506775068

F1 scores:
[       nan 0.94871795 0.98821681 0.95324494 0.97931034 0.97168743
 0.9850075  0.92592593 1.         0.8        0.97070649 0.98506787
 0.93439716 0.99728997 0.98086957 0.91818182 0.98224852]

Kappa:
0.9715819631031218
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb642259a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.780, val_acc:0.376]
Epoch [2/120    avg_loss:2.606, val_acc:0.412]
Epoch [3/120    avg_loss:2.470, val_acc:0.467]
Epoch [4/120    avg_loss:2.346, val_acc:0.489]
Epoch [5/120    avg_loss:2.239, val_acc:0.505]
Epoch [6/120    avg_loss:2.103, val_acc:0.534]
Epoch [7/120    avg_loss:2.050, val_acc:0.538]
Epoch [8/120    avg_loss:1.941, val_acc:0.572]
Epoch [9/120    avg_loss:1.824, val_acc:0.584]
Epoch [10/120    avg_loss:1.748, val_acc:0.611]
Epoch [11/120    avg_loss:1.654, val_acc:0.616]
Epoch [12/120    avg_loss:1.552, val_acc:0.586]
Epoch [13/120    avg_loss:1.419, val_acc:0.625]
Epoch [14/120    avg_loss:1.318, val_acc:0.688]
Epoch [15/120    avg_loss:1.250, val_acc:0.721]
Epoch [16/120    avg_loss:1.157, val_acc:0.693]
Epoch [17/120    avg_loss:1.039, val_acc:0.740]
Epoch [18/120    avg_loss:0.896, val_acc:0.759]
Epoch [19/120    avg_loss:0.870, val_acc:0.762]
Epoch [20/120    avg_loss:0.763, val_acc:0.769]
Epoch [21/120    avg_loss:0.748, val_acc:0.786]
Epoch [22/120    avg_loss:0.687, val_acc:0.807]
Epoch [23/120    avg_loss:0.599, val_acc:0.761]
Epoch [24/120    avg_loss:0.575, val_acc:0.815]
Epoch [25/120    avg_loss:0.519, val_acc:0.829]
Epoch [26/120    avg_loss:0.459, val_acc:0.855]
Epoch [27/120    avg_loss:0.512, val_acc:0.844]
Epoch [28/120    avg_loss:0.412, val_acc:0.851]
Epoch [29/120    avg_loss:0.363, val_acc:0.853]
Epoch [30/120    avg_loss:0.385, val_acc:0.857]
Epoch [31/120    avg_loss:0.308, val_acc:0.868]
Epoch [32/120    avg_loss:0.304, val_acc:0.884]
Epoch [33/120    avg_loss:0.298, val_acc:0.893]
Epoch [34/120    avg_loss:0.321, val_acc:0.894]
Epoch [35/120    avg_loss:0.277, val_acc:0.875]
Epoch [36/120    avg_loss:0.281, val_acc:0.900]
Epoch [37/120    avg_loss:0.328, val_acc:0.895]
Epoch [38/120    avg_loss:0.250, val_acc:0.919]
Epoch [39/120    avg_loss:0.197, val_acc:0.919]
Epoch [40/120    avg_loss:0.176, val_acc:0.922]
Epoch [41/120    avg_loss:0.175, val_acc:0.923]
Epoch [42/120    avg_loss:0.168, val_acc:0.933]
Epoch [43/120    avg_loss:0.164, val_acc:0.917]
Epoch [44/120    avg_loss:0.141, val_acc:0.939]
Epoch [45/120    avg_loss:0.141, val_acc:0.935]
Epoch [46/120    avg_loss:0.161, val_acc:0.900]
Epoch [47/120    avg_loss:0.140, val_acc:0.940]
Epoch [48/120    avg_loss:0.129, val_acc:0.936]
Epoch [49/120    avg_loss:0.124, val_acc:0.943]
Epoch [50/120    avg_loss:0.090, val_acc:0.945]
Epoch [51/120    avg_loss:0.087, val_acc:0.956]
Epoch [52/120    avg_loss:0.109, val_acc:0.929]
Epoch [53/120    avg_loss:0.141, val_acc:0.930]
Epoch [54/120    avg_loss:0.163, val_acc:0.947]
Epoch [55/120    avg_loss:0.123, val_acc:0.950]
Epoch [56/120    avg_loss:0.104, val_acc:0.962]
Epoch [57/120    avg_loss:0.143, val_acc:0.920]
Epoch [58/120    avg_loss:0.112, val_acc:0.954]
Epoch [59/120    avg_loss:0.077, val_acc:0.957]
Epoch [60/120    avg_loss:0.071, val_acc:0.962]
Epoch [61/120    avg_loss:0.070, val_acc:0.957]
Epoch [62/120    avg_loss:0.068, val_acc:0.963]
Epoch [63/120    avg_loss:0.069, val_acc:0.956]
Epoch [64/120    avg_loss:0.084, val_acc:0.936]
Epoch [65/120    avg_loss:0.062, val_acc:0.961]
Epoch [66/120    avg_loss:0.044, val_acc:0.966]
Epoch [67/120    avg_loss:0.051, val_acc:0.963]
Epoch [68/120    avg_loss:0.050, val_acc:0.961]
Epoch [69/120    avg_loss:0.051, val_acc:0.963]
Epoch [70/120    avg_loss:0.064, val_acc:0.979]
Epoch [71/120    avg_loss:0.053, val_acc:0.971]
Epoch [72/120    avg_loss:0.061, val_acc:0.963]
Epoch [73/120    avg_loss:0.051, val_acc:0.975]
Epoch [74/120    avg_loss:0.043, val_acc:0.974]
Epoch [75/120    avg_loss:0.039, val_acc:0.975]
Epoch [76/120    avg_loss:0.055, val_acc:0.971]
Epoch [77/120    avg_loss:0.051, val_acc:0.964]
Epoch [78/120    avg_loss:0.070, val_acc:0.967]
Epoch [79/120    avg_loss:0.066, val_acc:0.965]
Epoch [80/120    avg_loss:0.061, val_acc:0.952]
Epoch [81/120    avg_loss:0.042, val_acc:0.974]
Epoch [82/120    avg_loss:0.084, val_acc:0.952]
Epoch [83/120    avg_loss:0.078, val_acc:0.971]
Epoch [84/120    avg_loss:0.044, val_acc:0.974]
Epoch [85/120    avg_loss:0.037, val_acc:0.977]
Epoch [86/120    avg_loss:0.028, val_acc:0.978]
Epoch [87/120    avg_loss:0.031, val_acc:0.979]
Epoch [88/120    avg_loss:0.029, val_acc:0.978]
Epoch [89/120    avg_loss:0.031, val_acc:0.979]
Epoch [90/120    avg_loss:0.027, val_acc:0.980]
Epoch [91/120    avg_loss:0.030, val_acc:0.982]
Epoch [92/120    avg_loss:0.024, val_acc:0.981]
Epoch [93/120    avg_loss:0.026, val_acc:0.981]
Epoch [94/120    avg_loss:0.027, val_acc:0.980]
Epoch [95/120    avg_loss:0.028, val_acc:0.985]
Epoch [96/120    avg_loss:0.023, val_acc:0.982]
Epoch [97/120    avg_loss:0.025, val_acc:0.984]
Epoch [98/120    avg_loss:0.023, val_acc:0.985]
Epoch [99/120    avg_loss:0.021, val_acc:0.985]
Epoch [100/120    avg_loss:0.030, val_acc:0.985]
Epoch [101/120    avg_loss:0.022, val_acc:0.986]
Epoch [102/120    avg_loss:0.023, val_acc:0.984]
Epoch [103/120    avg_loss:0.028, val_acc:0.987]
Epoch [104/120    avg_loss:0.022, val_acc:0.986]
Epoch [105/120    avg_loss:0.021, val_acc:0.984]
Epoch [106/120    avg_loss:0.021, val_acc:0.986]
Epoch [107/120    avg_loss:0.026, val_acc:0.982]
Epoch [108/120    avg_loss:0.024, val_acc:0.987]
Epoch [109/120    avg_loss:0.022, val_acc:0.982]
Epoch [110/120    avg_loss:0.021, val_acc:0.984]
Epoch [111/120    avg_loss:0.022, val_acc:0.988]
Epoch [112/120    avg_loss:0.023, val_acc:0.988]
Epoch [113/120    avg_loss:0.022, val_acc:0.982]
Epoch [114/120    avg_loss:0.019, val_acc:0.986]
Epoch [115/120    avg_loss:0.019, val_acc:0.986]
Epoch [116/120    avg_loss:0.020, val_acc:0.985]
Epoch [117/120    avg_loss:0.022, val_acc:0.989]
Epoch [118/120    avg_loss:0.018, val_acc:0.988]
Epoch [119/120    avg_loss:0.017, val_acc:0.987]
Epoch [120/120    avg_loss:0.018, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1241    7    4    0    1    0    0    0    6   26    0    0
     0    0    0]
 [   0    0    1  705    7   18    1    0    0   10    1    0    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    6    0    0    0    0  853    5    2    0
     1    0    0]
 [   0    0   12    0    0    0    0    0    0    0   15 2177    6    0
     0    0    0]
 [   0    0    0    1    0   13    0    0    0    0    6    0  510    0
     0    2    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0    0
  1136    2    0]
 [   0    0    0    0    0    0   47    0    0    2    0    0    0    0
    14  284    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.37669376693766

F1 scores:
[       nan 0.975      0.97447978 0.96575342 0.97482838 0.95469613
 0.96176471 1.         0.997669   0.73469388 0.97042093 0.98506787
 0.96408318 0.99728997 0.99040976 0.89448819 0.98823529]

Kappa:
0.9700953232418339
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9011d92a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.791, val_acc:0.283]
Epoch [2/120    avg_loss:2.621, val_acc:0.443]
Epoch [3/120    avg_loss:2.459, val_acc:0.456]
Epoch [4/120    avg_loss:2.354, val_acc:0.512]
Epoch [5/120    avg_loss:2.219, val_acc:0.534]
Epoch [6/120    avg_loss:2.131, val_acc:0.550]
Epoch [7/120    avg_loss:2.031, val_acc:0.577]
Epoch [8/120    avg_loss:1.952, val_acc:0.594]
Epoch [9/120    avg_loss:1.848, val_acc:0.589]
Epoch [10/120    avg_loss:1.762, val_acc:0.626]
Epoch [11/120    avg_loss:1.652, val_acc:0.626]
Epoch [12/120    avg_loss:1.528, val_acc:0.622]
Epoch [13/120    avg_loss:1.536, val_acc:0.657]
Epoch [14/120    avg_loss:1.395, val_acc:0.681]
Epoch [15/120    avg_loss:1.295, val_acc:0.698]
Epoch [16/120    avg_loss:1.227, val_acc:0.713]
Epoch [17/120    avg_loss:1.149, val_acc:0.723]
Epoch [18/120    avg_loss:1.018, val_acc:0.717]
Epoch [19/120    avg_loss:0.967, val_acc:0.745]
Epoch [20/120    avg_loss:0.851, val_acc:0.774]
Epoch [21/120    avg_loss:0.821, val_acc:0.764]
Epoch [22/120    avg_loss:0.785, val_acc:0.755]
Epoch [23/120    avg_loss:0.796, val_acc:0.754]
Epoch [24/120    avg_loss:0.815, val_acc:0.739]
Epoch [25/120    avg_loss:0.628, val_acc:0.832]
Epoch [26/120    avg_loss:0.566, val_acc:0.826]
Epoch [27/120    avg_loss:0.498, val_acc:0.860]
Epoch [28/120    avg_loss:0.438, val_acc:0.868]
Epoch [29/120    avg_loss:0.447, val_acc:0.870]
Epoch [30/120    avg_loss:0.384, val_acc:0.855]
Epoch [31/120    avg_loss:0.480, val_acc:0.830]
Epoch [32/120    avg_loss:0.426, val_acc:0.874]
Epoch [33/120    avg_loss:0.299, val_acc:0.885]
Epoch [34/120    avg_loss:0.286, val_acc:0.902]
Epoch [35/120    avg_loss:0.345, val_acc:0.832]
Epoch [36/120    avg_loss:0.397, val_acc:0.842]
Epoch [37/120    avg_loss:0.310, val_acc:0.881]
Epoch [38/120    avg_loss:0.246, val_acc:0.885]
Epoch [39/120    avg_loss:0.254, val_acc:0.895]
Epoch [40/120    avg_loss:0.208, val_acc:0.907]
Epoch [41/120    avg_loss:0.218, val_acc:0.910]
Epoch [42/120    avg_loss:0.206, val_acc:0.917]
Epoch [43/120    avg_loss:0.173, val_acc:0.913]
Epoch [44/120    avg_loss:0.152, val_acc:0.918]
Epoch [45/120    avg_loss:0.143, val_acc:0.928]
Epoch [46/120    avg_loss:0.124, val_acc:0.941]
Epoch [47/120    avg_loss:0.160, val_acc:0.913]
Epoch [48/120    avg_loss:0.167, val_acc:0.915]
Epoch [49/120    avg_loss:0.142, val_acc:0.924]
Epoch [50/120    avg_loss:0.115, val_acc:0.929]
Epoch [51/120    avg_loss:0.129, val_acc:0.946]
Epoch [52/120    avg_loss:0.107, val_acc:0.946]
Epoch [53/120    avg_loss:0.088, val_acc:0.953]
Epoch [54/120    avg_loss:0.096, val_acc:0.947]
Epoch [55/120    avg_loss:0.078, val_acc:0.939]
Epoch [56/120    avg_loss:0.100, val_acc:0.953]
Epoch [57/120    avg_loss:0.096, val_acc:0.960]
Epoch [58/120    avg_loss:0.095, val_acc:0.944]
Epoch [59/120    avg_loss:0.083, val_acc:0.957]
Epoch [60/120    avg_loss:0.089, val_acc:0.960]
Epoch [61/120    avg_loss:0.090, val_acc:0.939]
Epoch [62/120    avg_loss:0.090, val_acc:0.916]
Epoch [63/120    avg_loss:0.097, val_acc:0.934]
Epoch [64/120    avg_loss:0.085, val_acc:0.951]
Epoch [65/120    avg_loss:0.073, val_acc:0.965]
Epoch [66/120    avg_loss:0.074, val_acc:0.953]
Epoch [67/120    avg_loss:0.060, val_acc:0.948]
Epoch [68/120    avg_loss:0.061, val_acc:0.962]
Epoch [69/120    avg_loss:0.064, val_acc:0.956]
Epoch [70/120    avg_loss:0.057, val_acc:0.952]
Epoch [71/120    avg_loss:0.072, val_acc:0.949]
Epoch [72/120    avg_loss:0.159, val_acc:0.943]
Epoch [73/120    avg_loss:0.111, val_acc:0.953]
Epoch [74/120    avg_loss:0.074, val_acc:0.957]
Epoch [75/120    avg_loss:0.065, val_acc:0.959]
Epoch [76/120    avg_loss:0.066, val_acc:0.951]
Epoch [77/120    avg_loss:0.058, val_acc:0.951]
Epoch [78/120    avg_loss:0.060, val_acc:0.967]
Epoch [79/120    avg_loss:0.043, val_acc:0.970]
Epoch [80/120    avg_loss:0.033, val_acc:0.966]
Epoch [81/120    avg_loss:0.048, val_acc:0.949]
Epoch [82/120    avg_loss:0.053, val_acc:0.955]
Epoch [83/120    avg_loss:0.057, val_acc:0.957]
Epoch [84/120    avg_loss:0.036, val_acc:0.968]
Epoch [85/120    avg_loss:0.042, val_acc:0.970]
Epoch [86/120    avg_loss:0.041, val_acc:0.965]
Epoch [87/120    avg_loss:0.039, val_acc:0.968]
Epoch [88/120    avg_loss:0.026, val_acc:0.969]
Epoch [89/120    avg_loss:0.032, val_acc:0.967]
Epoch [90/120    avg_loss:0.036, val_acc:0.970]
Epoch [91/120    avg_loss:0.030, val_acc:0.974]
Epoch [92/120    avg_loss:0.021, val_acc:0.968]
Epoch [93/120    avg_loss:0.029, val_acc:0.956]
Epoch [94/120    avg_loss:0.059, val_acc:0.963]
Epoch [95/120    avg_loss:0.048, val_acc:0.979]
Epoch [96/120    avg_loss:0.035, val_acc:0.975]
Epoch [97/120    avg_loss:0.050, val_acc:0.940]
Epoch [98/120    avg_loss:0.049, val_acc:0.971]
Epoch [99/120    avg_loss:0.049, val_acc:0.972]
Epoch [100/120    avg_loss:0.029, val_acc:0.976]
Epoch [101/120    avg_loss:0.033, val_acc:0.974]
Epoch [102/120    avg_loss:0.024, val_acc:0.978]
Epoch [103/120    avg_loss:0.019, val_acc:0.982]
Epoch [104/120    avg_loss:0.025, val_acc:0.979]
Epoch [105/120    avg_loss:0.035, val_acc:0.972]
Epoch [106/120    avg_loss:0.044, val_acc:0.974]
Epoch [107/120    avg_loss:0.036, val_acc:0.972]
Epoch [108/120    avg_loss:0.050, val_acc:0.963]
Epoch [109/120    avg_loss:0.034, val_acc:0.972]
Epoch [110/120    avg_loss:0.025, val_acc:0.971]
Epoch [111/120    avg_loss:0.023, val_acc:0.979]
Epoch [112/120    avg_loss:0.023, val_acc:0.970]
Epoch [113/120    avg_loss:0.020, val_acc:0.978]
Epoch [114/120    avg_loss:0.021, val_acc:0.971]
Epoch [115/120    avg_loss:0.019, val_acc:0.974]
Epoch [116/120    avg_loss:0.022, val_acc:0.971]
Epoch [117/120    avg_loss:0.018, val_acc:0.981]
Epoch [118/120    avg_loss:0.011, val_acc:0.979]
Epoch [119/120    avg_loss:0.012, val_acc:0.980]
Epoch [120/120    avg_loss:0.014, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1243    0   11    0    1    0    0    0    6   22    2    0
     0    0    0]
 [   0    0    1  697    0    0    0    0    0    9   10   15   13    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    1    0    4    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    5    0    0    0    0  847   17    1    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0    4 2194   10    0
     0    0    0]
 [   0    0    4    2    0    1    0    0    0    0    0   12  513    0
     0    1    1]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    0   22    0    0    0    0    0    0    0
    57  268    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.22493224932249

F1 scores:
[       nan 1.         0.97874016 0.96403873 0.97482838 0.98156682
 0.98203593 0.98039216 1.         0.73469388 0.97244546 0.98143592
 0.95619758 0.99191375 0.96995708 0.8576     0.99408284]

Kappa:
0.9683253357130821
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1562410ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.782, val_acc:0.223]
Epoch [2/120    avg_loss:2.629, val_acc:0.370]
Epoch [3/120    avg_loss:2.509, val_acc:0.438]
Epoch [4/120    avg_loss:2.386, val_acc:0.487]
Epoch [5/120    avg_loss:2.274, val_acc:0.502]
Epoch [6/120    avg_loss:2.183, val_acc:0.518]
Epoch [7/120    avg_loss:2.090, val_acc:0.525]
Epoch [8/120    avg_loss:1.996, val_acc:0.546]
Epoch [9/120    avg_loss:1.907, val_acc:0.578]
Epoch [10/120    avg_loss:1.789, val_acc:0.584]
Epoch [11/120    avg_loss:1.724, val_acc:0.608]
Epoch [12/120    avg_loss:1.608, val_acc:0.628]
Epoch [13/120    avg_loss:1.475, val_acc:0.630]
Epoch [14/120    avg_loss:1.403, val_acc:0.671]
Epoch [15/120    avg_loss:1.260, val_acc:0.669]
Epoch [16/120    avg_loss:1.184, val_acc:0.691]
Epoch [17/120    avg_loss:1.109, val_acc:0.694]
Epoch [18/120    avg_loss:1.013, val_acc:0.735]
Epoch [19/120    avg_loss:0.882, val_acc:0.724]
Epoch [20/120    avg_loss:0.877, val_acc:0.759]
Epoch [21/120    avg_loss:0.797, val_acc:0.759]
Epoch [22/120    avg_loss:0.701, val_acc:0.787]
Epoch [23/120    avg_loss:0.732, val_acc:0.755]
Epoch [24/120    avg_loss:0.628, val_acc:0.781]
Epoch [25/120    avg_loss:0.524, val_acc:0.764]
Epoch [26/120    avg_loss:0.537, val_acc:0.814]
Epoch [27/120    avg_loss:0.518, val_acc:0.800]
Epoch [28/120    avg_loss:0.423, val_acc:0.835]
Epoch [29/120    avg_loss:0.346, val_acc:0.864]
Epoch [30/120    avg_loss:0.401, val_acc:0.844]
Epoch [31/120    avg_loss:0.339, val_acc:0.848]
Epoch [32/120    avg_loss:0.327, val_acc:0.824]
Epoch [33/120    avg_loss:0.322, val_acc:0.854]
Epoch [34/120    avg_loss:0.368, val_acc:0.850]
Epoch [35/120    avg_loss:0.334, val_acc:0.853]
Epoch [36/120    avg_loss:0.289, val_acc:0.866]
Epoch [37/120    avg_loss:0.231, val_acc:0.897]
Epoch [38/120    avg_loss:0.204, val_acc:0.876]
Epoch [39/120    avg_loss:0.199, val_acc:0.884]
Epoch [40/120    avg_loss:0.194, val_acc:0.887]
Epoch [41/120    avg_loss:0.897, val_acc:0.809]
Epoch [42/120    avg_loss:0.375, val_acc:0.845]
Epoch [43/120    avg_loss:0.281, val_acc:0.877]
Epoch [44/120    avg_loss:0.226, val_acc:0.880]
Epoch [45/120    avg_loss:0.254, val_acc:0.871]
Epoch [46/120    avg_loss:0.183, val_acc:0.902]
Epoch [47/120    avg_loss:0.160, val_acc:0.911]
Epoch [48/120    avg_loss:0.167, val_acc:0.907]
Epoch [49/120    avg_loss:0.130, val_acc:0.897]
Epoch [50/120    avg_loss:0.130, val_acc:0.905]
Epoch [51/120    avg_loss:0.101, val_acc:0.938]
Epoch [52/120    avg_loss:0.098, val_acc:0.935]
Epoch [53/120    avg_loss:0.087, val_acc:0.925]
Epoch [54/120    avg_loss:0.127, val_acc:0.932]
Epoch [55/120    avg_loss:0.123, val_acc:0.928]
Epoch [56/120    avg_loss:0.096, val_acc:0.944]
Epoch [57/120    avg_loss:0.077, val_acc:0.943]
Epoch [58/120    avg_loss:0.080, val_acc:0.940]
Epoch [59/120    avg_loss:0.087, val_acc:0.936]
Epoch [60/120    avg_loss:0.098, val_acc:0.940]
Epoch [61/120    avg_loss:0.104, val_acc:0.936]
Epoch [62/120    avg_loss:0.075, val_acc:0.942]
Epoch [63/120    avg_loss:0.056, val_acc:0.951]
Epoch [64/120    avg_loss:0.058, val_acc:0.951]
Epoch [65/120    avg_loss:0.066, val_acc:0.946]
Epoch [66/120    avg_loss:0.070, val_acc:0.948]
Epoch [67/120    avg_loss:0.067, val_acc:0.941]
Epoch [68/120    avg_loss:0.065, val_acc:0.938]
Epoch [69/120    avg_loss:0.067, val_acc:0.952]
Epoch [70/120    avg_loss:0.056, val_acc:0.946]
Epoch [71/120    avg_loss:0.070, val_acc:0.947]
Epoch [72/120    avg_loss:0.075, val_acc:0.949]
Epoch [73/120    avg_loss:0.059, val_acc:0.933]
Epoch [74/120    avg_loss:0.057, val_acc:0.956]
Epoch [75/120    avg_loss:0.039, val_acc:0.965]
Epoch [76/120    avg_loss:0.039, val_acc:0.956]
Epoch [77/120    avg_loss:0.036, val_acc:0.958]
Epoch [78/120    avg_loss:0.038, val_acc:0.957]
Epoch [79/120    avg_loss:0.039, val_acc:0.962]
Epoch [80/120    avg_loss:0.055, val_acc:0.955]
Epoch [81/120    avg_loss:0.047, val_acc:0.958]
Epoch [82/120    avg_loss:0.038, val_acc:0.959]
Epoch [83/120    avg_loss:0.054, val_acc:0.939]
Epoch [84/120    avg_loss:0.047, val_acc:0.950]
Epoch [85/120    avg_loss:0.047, val_acc:0.958]
Epoch [86/120    avg_loss:0.043, val_acc:0.941]
Epoch [87/120    avg_loss:0.044, val_acc:0.966]
Epoch [88/120    avg_loss:0.055, val_acc:0.923]
Epoch [89/120    avg_loss:0.072, val_acc:0.944]
Epoch [90/120    avg_loss:0.062, val_acc:0.963]
Epoch [91/120    avg_loss:0.037, val_acc:0.965]
Epoch [92/120    avg_loss:0.037, val_acc:0.956]
Epoch [93/120    avg_loss:0.034, val_acc:0.952]
Epoch [94/120    avg_loss:0.031, val_acc:0.970]
Epoch [95/120    avg_loss:0.022, val_acc:0.963]
Epoch [96/120    avg_loss:0.026, val_acc:0.963]
Epoch [97/120    avg_loss:0.017, val_acc:0.964]
Epoch [98/120    avg_loss:0.025, val_acc:0.937]
Epoch [99/120    avg_loss:0.046, val_acc:0.960]
Epoch [100/120    avg_loss:0.023, val_acc:0.965]
Epoch [101/120    avg_loss:0.017, val_acc:0.963]
Epoch [102/120    avg_loss:0.018, val_acc:0.968]
Epoch [103/120    avg_loss:0.021, val_acc:0.962]
Epoch [104/120    avg_loss:0.023, val_acc:0.947]
Epoch [105/120    avg_loss:0.028, val_acc:0.966]
Epoch [106/120    avg_loss:0.033, val_acc:0.954]
Epoch [107/120    avg_loss:0.019, val_acc:0.967]
Epoch [108/120    avg_loss:0.018, val_acc:0.971]
Epoch [109/120    avg_loss:0.020, val_acc:0.970]
Epoch [110/120    avg_loss:0.015, val_acc:0.969]
Epoch [111/120    avg_loss:0.017, val_acc:0.968]
Epoch [112/120    avg_loss:0.012, val_acc:0.971]
Epoch [113/120    avg_loss:0.015, val_acc:0.969]
Epoch [114/120    avg_loss:0.014, val_acc:0.968]
Epoch [115/120    avg_loss:0.012, val_acc:0.970]
Epoch [116/120    avg_loss:0.012, val_acc:0.969]
Epoch [117/120    avg_loss:0.012, val_acc:0.971]
Epoch [118/120    avg_loss:0.011, val_acc:0.974]
Epoch [119/120    avg_loss:0.017, val_acc:0.973]
Epoch [120/120    avg_loss:0.014, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1222    4   14    0    1    0    0    0    6   38    0    0
     0    0    0]
 [   0    0    8  715    4    0    2    0    0    8    3    0    7    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    5    0    3    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    6    0    0    2    1    0    0    0  847   13    4    0
     0    2    0]
 [   0    0    2    3    0    0    7    0    0    0   12 2180    6    0
     0    0    0]
 [   0    0    0    3    2    3    0    0    0    0    4    6  510    0
     1    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    1    0    0    7    1    0    1    0    1    0    0    0
  1124    4    0]
 [   0    0    0    1    0    0   43    0    0    0    0    0    0    0
     5  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.19241192411924

F1 scores:
[       nan 0.97560976 0.96830428 0.97014925 0.95280899 0.97359357
 0.95830285 0.90909091 0.99767442 0.73913043 0.96855346 0.98021583
 0.95954845 1.         0.98900132 0.91551459 0.96511628]

Kappa:
0.9679893973365232
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbc0c2f5a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.806, val_acc:0.249]
Epoch [2/120    avg_loss:2.683, val_acc:0.353]
Epoch [3/120    avg_loss:2.567, val_acc:0.504]
Epoch [4/120    avg_loss:2.434, val_acc:0.525]
Epoch [5/120    avg_loss:2.308, val_acc:0.534]
Epoch [6/120    avg_loss:2.195, val_acc:0.548]
Epoch [7/120    avg_loss:2.079, val_acc:0.552]
Epoch [8/120    avg_loss:2.015, val_acc:0.571]
Epoch [9/120    avg_loss:1.886, val_acc:0.609]
Epoch [10/120    avg_loss:1.748, val_acc:0.595]
Epoch [11/120    avg_loss:1.702, val_acc:0.604]
Epoch [12/120    avg_loss:1.594, val_acc:0.642]
Epoch [13/120    avg_loss:1.452, val_acc:0.642]
Epoch [14/120    avg_loss:1.328, val_acc:0.699]
Epoch [15/120    avg_loss:1.259, val_acc:0.674]
Epoch [16/120    avg_loss:1.183, val_acc:0.709]
Epoch [17/120    avg_loss:1.032, val_acc:0.743]
Epoch [18/120    avg_loss:0.940, val_acc:0.747]
Epoch [19/120    avg_loss:0.886, val_acc:0.744]
Epoch [20/120    avg_loss:0.775, val_acc:0.781]
Epoch [21/120    avg_loss:0.776, val_acc:0.801]
Epoch [22/120    avg_loss:0.761, val_acc:0.760]
Epoch [23/120    avg_loss:0.627, val_acc:0.807]
Epoch [24/120    avg_loss:0.579, val_acc:0.782]
Epoch [25/120    avg_loss:0.524, val_acc:0.815]
Epoch [26/120    avg_loss:0.522, val_acc:0.806]
Epoch [27/120    avg_loss:0.515, val_acc:0.816]
Epoch [28/120    avg_loss:0.459, val_acc:0.841]
Epoch [29/120    avg_loss:0.371, val_acc:0.869]
Epoch [30/120    avg_loss:0.386, val_acc:0.842]
Epoch [31/120    avg_loss:0.363, val_acc:0.882]
Epoch [32/120    avg_loss:0.337, val_acc:0.832]
Epoch [33/120    avg_loss:0.353, val_acc:0.853]
Epoch [34/120    avg_loss:0.295, val_acc:0.882]
Epoch [35/120    avg_loss:0.284, val_acc:0.862]
Epoch [36/120    avg_loss:0.234, val_acc:0.891]
Epoch [37/120    avg_loss:0.220, val_acc:0.886]
Epoch [38/120    avg_loss:0.219, val_acc:0.899]
Epoch [39/120    avg_loss:0.203, val_acc:0.925]
Epoch [40/120    avg_loss:0.170, val_acc:0.887]
Epoch [41/120    avg_loss:0.189, val_acc:0.889]
Epoch [42/120    avg_loss:0.179, val_acc:0.900]
Epoch [43/120    avg_loss:0.175, val_acc:0.927]
Epoch [44/120    avg_loss:0.146, val_acc:0.915]
Epoch [45/120    avg_loss:0.169, val_acc:0.931]
Epoch [46/120    avg_loss:0.144, val_acc:0.921]
Epoch [47/120    avg_loss:0.111, val_acc:0.934]
Epoch [48/120    avg_loss:0.124, val_acc:0.912]
Epoch [49/120    avg_loss:0.099, val_acc:0.949]
Epoch [50/120    avg_loss:0.075, val_acc:0.953]
Epoch [51/120    avg_loss:0.116, val_acc:0.941]
Epoch [52/120    avg_loss:0.098, val_acc:0.949]
Epoch [53/120    avg_loss:0.098, val_acc:0.940]
Epoch [54/120    avg_loss:0.089, val_acc:0.960]
Epoch [55/120    avg_loss:0.086, val_acc:0.952]
Epoch [56/120    avg_loss:0.108, val_acc:0.920]
Epoch [57/120    avg_loss:0.120, val_acc:0.946]
Epoch [58/120    avg_loss:0.098, val_acc:0.952]
Epoch [59/120    avg_loss:0.091, val_acc:0.950]
Epoch [60/120    avg_loss:0.073, val_acc:0.949]
Epoch [61/120    avg_loss:0.078, val_acc:0.935]
Epoch [62/120    avg_loss:0.066, val_acc:0.964]
Epoch [63/120    avg_loss:0.062, val_acc:0.956]
Epoch [64/120    avg_loss:0.062, val_acc:0.946]
Epoch [65/120    avg_loss:0.050, val_acc:0.966]
Epoch [66/120    avg_loss:0.052, val_acc:0.957]
Epoch [67/120    avg_loss:0.046, val_acc:0.964]
Epoch [68/120    avg_loss:0.049, val_acc:0.954]
Epoch [69/120    avg_loss:0.054, val_acc:0.968]
Epoch [70/120    avg_loss:0.039, val_acc:0.970]
Epoch [71/120    avg_loss:0.051, val_acc:0.959]
Epoch [72/120    avg_loss:0.038, val_acc:0.964]
Epoch [73/120    avg_loss:0.032, val_acc:0.963]
Epoch [74/120    avg_loss:0.038, val_acc:0.960]
Epoch [75/120    avg_loss:0.053, val_acc:0.959]
Epoch [76/120    avg_loss:0.071, val_acc:0.966]
Epoch [77/120    avg_loss:0.079, val_acc:0.962]
Epoch [78/120    avg_loss:0.067, val_acc:0.972]
Epoch [79/120    avg_loss:0.049, val_acc:0.965]
Epoch [80/120    avg_loss:0.060, val_acc:0.962]
Epoch [81/120    avg_loss:0.047, val_acc:0.971]
Epoch [82/120    avg_loss:0.043, val_acc:0.967]
Epoch [83/120    avg_loss:0.044, val_acc:0.951]
Epoch [84/120    avg_loss:0.028, val_acc:0.966]
Epoch [85/120    avg_loss:0.029, val_acc:0.975]
Epoch [86/120    avg_loss:0.040, val_acc:0.972]
Epoch [87/120    avg_loss:0.039, val_acc:0.973]
Epoch [88/120    avg_loss:0.032, val_acc:0.969]
Epoch [89/120    avg_loss:0.044, val_acc:0.975]
Epoch [90/120    avg_loss:0.034, val_acc:0.969]
Epoch [91/120    avg_loss:0.038, val_acc:0.964]
Epoch [92/120    avg_loss:0.039, val_acc:0.971]
Epoch [93/120    avg_loss:0.030, val_acc:0.965]
Epoch [94/120    avg_loss:0.031, val_acc:0.975]
Epoch [95/120    avg_loss:0.031, val_acc:0.965]
Epoch [96/120    avg_loss:0.026, val_acc:0.973]
Epoch [97/120    avg_loss:0.016, val_acc:0.980]
Epoch [98/120    avg_loss:0.016, val_acc:0.969]
Epoch [99/120    avg_loss:0.022, val_acc:0.964]
Epoch [100/120    avg_loss:0.028, val_acc:0.966]
Epoch [101/120    avg_loss:0.026, val_acc:0.971]
Epoch [102/120    avg_loss:0.027, val_acc:0.979]
Epoch [103/120    avg_loss:0.018, val_acc:0.979]
Epoch [104/120    avg_loss:0.022, val_acc:0.976]
Epoch [105/120    avg_loss:0.015, val_acc:0.979]
Epoch [106/120    avg_loss:0.028, val_acc:0.973]
Epoch [107/120    avg_loss:0.027, val_acc:0.960]
Epoch [108/120    avg_loss:0.027, val_acc:0.979]
Epoch [109/120    avg_loss:0.020, val_acc:0.973]
Epoch [110/120    avg_loss:0.026, val_acc:0.968]
Epoch [111/120    avg_loss:0.018, val_acc:0.969]
Epoch [112/120    avg_loss:0.013, val_acc:0.972]
Epoch [113/120    avg_loss:0.015, val_acc:0.974]
Epoch [114/120    avg_loss:0.012, val_acc:0.975]
Epoch [115/120    avg_loss:0.012, val_acc:0.978]
Epoch [116/120    avg_loss:0.012, val_acc:0.980]
Epoch [117/120    avg_loss:0.011, val_acc:0.981]
Epoch [118/120    avg_loss:0.011, val_acc:0.982]
Epoch [119/120    avg_loss:0.013, val_acc:0.984]
Epoch [120/120    avg_loss:0.013, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1252    5    4    0    0    0    0    0    8   16    0    0
     0    0    0]
 [   0    0    0  719    7    7    2    0    0    2    0    1    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    2    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    5    0    0    0    0  850    5    1    0
     0    5    0]
 [   0    0    6    0    0    0    1    0    0    0    4 2198    1    0
     0    0    0]
 [   0    0    1    0    3    7    0    0    0    0    3    0  517    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1138    1    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    59  287    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.01626016260163

F1 scores:
[       nan 0.975      0.98080689 0.97756628 0.96818182 0.97632469
 0.99620349 0.96153846 1.         0.94736842 0.97588978 0.99210111
 0.97180451 1.         0.97431507 0.896875   0.9704142 ]

Kappa:
0.9773726796299015
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f063d689b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.798, val_acc:0.247]
Epoch [2/120    avg_loss:2.657, val_acc:0.382]
Epoch [3/120    avg_loss:2.494, val_acc:0.485]
Epoch [4/120    avg_loss:2.367, val_acc:0.520]
Epoch [5/120    avg_loss:2.249, val_acc:0.537]
Epoch [6/120    avg_loss:2.165, val_acc:0.534]
Epoch [7/120    avg_loss:2.065, val_acc:0.592]
Epoch [8/120    avg_loss:1.957, val_acc:0.579]
Epoch [9/120    avg_loss:1.889, val_acc:0.589]
Epoch [10/120    avg_loss:1.754, val_acc:0.611]
Epoch [11/120    avg_loss:1.637, val_acc:0.630]
Epoch [12/120    avg_loss:1.523, val_acc:0.653]
Epoch [13/120    avg_loss:1.436, val_acc:0.665]
Epoch [14/120    avg_loss:1.244, val_acc:0.675]
Epoch [15/120    avg_loss:1.211, val_acc:0.686]
Epoch [16/120    avg_loss:1.109, val_acc:0.713]
Epoch [17/120    avg_loss:0.987, val_acc:0.709]
Epoch [18/120    avg_loss:0.919, val_acc:0.733]
Epoch [19/120    avg_loss:0.846, val_acc:0.769]
Epoch [20/120    avg_loss:0.734, val_acc:0.815]
Epoch [21/120    avg_loss:0.633, val_acc:0.802]
Epoch [22/120    avg_loss:0.609, val_acc:0.814]
Epoch [23/120    avg_loss:0.505, val_acc:0.850]
Epoch [24/120    avg_loss:0.492, val_acc:0.859]
Epoch [25/120    avg_loss:0.448, val_acc:0.849]
Epoch [26/120    avg_loss:0.396, val_acc:0.862]
Epoch [27/120    avg_loss:0.336, val_acc:0.858]
Epoch [28/120    avg_loss:0.352, val_acc:0.862]
Epoch [29/120    avg_loss:0.320, val_acc:0.857]
Epoch [30/120    avg_loss:0.317, val_acc:0.858]
Epoch [31/120    avg_loss:0.322, val_acc:0.850]
Epoch [32/120    avg_loss:0.297, val_acc:0.859]
Epoch [33/120    avg_loss:0.275, val_acc:0.901]
Epoch [34/120    avg_loss:0.213, val_acc:0.907]
Epoch [35/120    avg_loss:0.191, val_acc:0.909]
Epoch [36/120    avg_loss:0.194, val_acc:0.900]
Epoch [37/120    avg_loss:0.197, val_acc:0.916]
Epoch [38/120    avg_loss:0.174, val_acc:0.907]
Epoch [39/120    avg_loss:0.164, val_acc:0.901]
Epoch [40/120    avg_loss:0.142, val_acc:0.926]
Epoch [41/120    avg_loss:0.127, val_acc:0.927]
Epoch [42/120    avg_loss:0.149, val_acc:0.912]
Epoch [43/120    avg_loss:0.172, val_acc:0.915]
Epoch [44/120    avg_loss:0.133, val_acc:0.925]
Epoch [45/120    avg_loss:0.135, val_acc:0.911]
Epoch [46/120    avg_loss:0.132, val_acc:0.920]
Epoch [47/120    avg_loss:0.135, val_acc:0.941]
Epoch [48/120    avg_loss:0.094, val_acc:0.949]
Epoch [49/120    avg_loss:0.080, val_acc:0.941]
Epoch [50/120    avg_loss:0.088, val_acc:0.943]
Epoch [51/120    avg_loss:0.164, val_acc:0.895]
Epoch [52/120    avg_loss:0.239, val_acc:0.882]
Epoch [53/120    avg_loss:0.203, val_acc:0.895]
Epoch [54/120    avg_loss:0.296, val_acc:0.849]
Epoch [55/120    avg_loss:0.221, val_acc:0.904]
Epoch [56/120    avg_loss:0.143, val_acc:0.917]
Epoch [57/120    avg_loss:0.114, val_acc:0.933]
Epoch [58/120    avg_loss:0.089, val_acc:0.949]
Epoch [59/120    avg_loss:0.100, val_acc:0.923]
Epoch [60/120    avg_loss:0.117, val_acc:0.943]
Epoch [61/120    avg_loss:0.081, val_acc:0.936]
Epoch [62/120    avg_loss:0.077, val_acc:0.944]
Epoch [63/120    avg_loss:0.061, val_acc:0.943]
Epoch [64/120    avg_loss:0.069, val_acc:0.925]
Epoch [65/120    avg_loss:0.181, val_acc:0.909]
Epoch [66/120    avg_loss:0.126, val_acc:0.922]
Epoch [67/120    avg_loss:0.105, val_acc:0.934]
Epoch [68/120    avg_loss:0.056, val_acc:0.931]
Epoch [69/120    avg_loss:0.072, val_acc:0.944]
Epoch [70/120    avg_loss:0.064, val_acc:0.942]
Epoch [71/120    avg_loss:0.052, val_acc:0.952]
Epoch [72/120    avg_loss:0.055, val_acc:0.940]
Epoch [73/120    avg_loss:0.043, val_acc:0.959]
Epoch [74/120    avg_loss:0.046, val_acc:0.947]
Epoch [75/120    avg_loss:0.080, val_acc:0.952]
Epoch [76/120    avg_loss:0.055, val_acc:0.958]
Epoch [77/120    avg_loss:0.045, val_acc:0.948]
Epoch [78/120    avg_loss:0.064, val_acc:0.953]
Epoch [79/120    avg_loss:0.061, val_acc:0.942]
Epoch [80/120    avg_loss:0.049, val_acc:0.960]
Epoch [81/120    avg_loss:0.032, val_acc:0.942]
Epoch [82/120    avg_loss:0.041, val_acc:0.968]
Epoch [83/120    avg_loss:0.037, val_acc:0.956]
Epoch [84/120    avg_loss:0.031, val_acc:0.957]
Epoch [85/120    avg_loss:0.047, val_acc:0.964]
Epoch [86/120    avg_loss:0.028, val_acc:0.966]
Epoch [87/120    avg_loss:0.031, val_acc:0.959]
Epoch [88/120    avg_loss:0.027, val_acc:0.966]
Epoch [89/120    avg_loss:0.029, val_acc:0.964]
Epoch [90/120    avg_loss:0.034, val_acc:0.965]
Epoch [91/120    avg_loss:0.026, val_acc:0.960]
Epoch [92/120    avg_loss:0.026, val_acc:0.970]
Epoch [93/120    avg_loss:0.037, val_acc:0.963]
Epoch [94/120    avg_loss:0.023, val_acc:0.970]
Epoch [95/120    avg_loss:0.026, val_acc:0.964]
Epoch [96/120    avg_loss:0.022, val_acc:0.962]
Epoch [97/120    avg_loss:0.053, val_acc:0.949]
Epoch [98/120    avg_loss:0.074, val_acc:0.955]
Epoch [99/120    avg_loss:0.065, val_acc:0.966]
Epoch [100/120    avg_loss:0.042, val_acc:0.960]
Epoch [101/120    avg_loss:0.034, val_acc:0.969]
Epoch [102/120    avg_loss:0.064, val_acc:0.964]
Epoch [103/120    avg_loss:0.028, val_acc:0.968]
Epoch [104/120    avg_loss:0.021, val_acc:0.968]
Epoch [105/120    avg_loss:0.029, val_acc:0.967]
Epoch [106/120    avg_loss:0.022, val_acc:0.971]
Epoch [107/120    avg_loss:0.045, val_acc:0.964]
Epoch [108/120    avg_loss:0.032, val_acc:0.947]
Epoch [109/120    avg_loss:0.029, val_acc:0.960]
Epoch [110/120    avg_loss:0.037, val_acc:0.971]
Epoch [111/120    avg_loss:0.040, val_acc:0.964]
Epoch [112/120    avg_loss:0.023, val_acc:0.964]
Epoch [113/120    avg_loss:0.024, val_acc:0.965]
Epoch [114/120    avg_loss:0.021, val_acc:0.971]
Epoch [115/120    avg_loss:0.015, val_acc:0.964]
Epoch [116/120    avg_loss:0.015, val_acc:0.973]
Epoch [117/120    avg_loss:0.012, val_acc:0.975]
Epoch [118/120    avg_loss:0.014, val_acc:0.969]
Epoch [119/120    avg_loss:0.013, val_acc:0.973]
Epoch [120/120    avg_loss:0.013, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1237    1   10    0    0    0    0    0   12   25    0    0
     0    0    0]
 [   0    0    0  691    9    2    0    0    0    2    3    0   40    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   30    1    0    2    5    0    0    0  829    7    0    0
     0    1    0]
 [   0    0    4    0    0    1    3    2    0    0   18 2154   26    0
     2    0    0]
 [   0    0    0    3    0    2    0    0    0    0    6    1  518    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1135    3    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    26  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.00813008130082

F1 scores:
[       nan 0.96202532 0.96754009 0.95706371 0.95495495 0.98739977
 0.98277154 0.96153846 1.         0.91891892 0.94959908 0.97953615
 0.92582663 1.         0.98438855 0.93171472 0.98245614]

Kappa:
0.9659082633289605
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff345665a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.736, val_acc:0.331]
Epoch [2/120    avg_loss:2.546, val_acc:0.367]
Epoch [3/120    avg_loss:2.410, val_acc:0.395]
Epoch [4/120    avg_loss:2.300, val_acc:0.383]
Epoch [5/120    avg_loss:2.217, val_acc:0.470]
Epoch [6/120    avg_loss:2.118, val_acc:0.511]
Epoch [7/120    avg_loss:2.040, val_acc:0.566]
Epoch [8/120    avg_loss:1.948, val_acc:0.592]
Epoch [9/120    avg_loss:1.892, val_acc:0.636]
Epoch [10/120    avg_loss:1.809, val_acc:0.614]
Epoch [11/120    avg_loss:1.687, val_acc:0.615]
Epoch [12/120    avg_loss:1.614, val_acc:0.643]
Epoch [13/120    avg_loss:1.533, val_acc:0.657]
Epoch [14/120    avg_loss:1.485, val_acc:0.651]
Epoch [15/120    avg_loss:1.377, val_acc:0.658]
Epoch [16/120    avg_loss:1.266, val_acc:0.652]
Epoch [17/120    avg_loss:1.129, val_acc:0.704]
Epoch [18/120    avg_loss:1.028, val_acc:0.709]
Epoch [19/120    avg_loss:0.976, val_acc:0.762]
Epoch [20/120    avg_loss:0.852, val_acc:0.749]
Epoch [21/120    avg_loss:0.824, val_acc:0.713]
Epoch [22/120    avg_loss:0.753, val_acc:0.764]
Epoch [23/120    avg_loss:0.657, val_acc:0.788]
Epoch [24/120    avg_loss:0.662, val_acc:0.774]
Epoch [25/120    avg_loss:0.670, val_acc:0.822]
Epoch [26/120    avg_loss:0.538, val_acc:0.828]
Epoch [27/120    avg_loss:0.557, val_acc:0.805]
Epoch [28/120    avg_loss:0.475, val_acc:0.850]
Epoch [29/120    avg_loss:0.404, val_acc:0.849]
Epoch [30/120    avg_loss:0.343, val_acc:0.874]
Epoch [31/120    avg_loss:0.341, val_acc:0.864]
Epoch [32/120    avg_loss:0.337, val_acc:0.882]
Epoch [33/120    avg_loss:0.282, val_acc:0.885]
Epoch [34/120    avg_loss:0.351, val_acc:0.870]
Epoch [35/120    avg_loss:0.348, val_acc:0.874]
Epoch [36/120    avg_loss:0.338, val_acc:0.871]
Epoch [37/120    avg_loss:0.289, val_acc:0.843]
Epoch [38/120    avg_loss:0.249, val_acc:0.901]
Epoch [39/120    avg_loss:0.210, val_acc:0.890]
Epoch [40/120    avg_loss:0.246, val_acc:0.908]
Epoch [41/120    avg_loss:0.272, val_acc:0.872]
Epoch [42/120    avg_loss:0.209, val_acc:0.898]
Epoch [43/120    avg_loss:0.213, val_acc:0.870]
Epoch [44/120    avg_loss:0.203, val_acc:0.886]
Epoch [45/120    avg_loss:0.153, val_acc:0.891]
Epoch [46/120    avg_loss:0.139, val_acc:0.920]
Epoch [47/120    avg_loss:0.183, val_acc:0.865]
Epoch [48/120    avg_loss:0.170, val_acc:0.919]
Epoch [49/120    avg_loss:0.134, val_acc:0.923]
Epoch [50/120    avg_loss:0.122, val_acc:0.924]
Epoch [51/120    avg_loss:0.125, val_acc:0.922]
Epoch [52/120    avg_loss:0.129, val_acc:0.922]
Epoch [53/120    avg_loss:0.116, val_acc:0.905]
Epoch [54/120    avg_loss:0.123, val_acc:0.907]
Epoch [55/120    avg_loss:0.120, val_acc:0.906]
Epoch [56/120    avg_loss:0.133, val_acc:0.907]
Epoch [57/120    avg_loss:0.126, val_acc:0.907]
Epoch [58/120    avg_loss:0.111, val_acc:0.910]
Epoch [59/120    avg_loss:0.140, val_acc:0.875]
Epoch [60/120    avg_loss:0.110, val_acc:0.922]
Epoch [61/120    avg_loss:0.105, val_acc:0.931]
Epoch [62/120    avg_loss:0.149, val_acc:0.885]
Epoch [63/120    avg_loss:0.175, val_acc:0.784]
Epoch [64/120    avg_loss:0.355, val_acc:0.867]
Epoch [65/120    avg_loss:0.310, val_acc:0.886]
Epoch [66/120    avg_loss:0.220, val_acc:0.880]
Epoch [67/120    avg_loss:0.170, val_acc:0.925]
Epoch [68/120    avg_loss:0.112, val_acc:0.918]
Epoch [69/120    avg_loss:0.125, val_acc:0.921]
Epoch [70/120    avg_loss:0.090, val_acc:0.932]
Epoch [71/120    avg_loss:0.084, val_acc:0.928]
Epoch [72/120    avg_loss:0.082, val_acc:0.930]
Epoch [73/120    avg_loss:0.058, val_acc:0.936]
Epoch [74/120    avg_loss:0.091, val_acc:0.946]
Epoch [75/120    avg_loss:0.071, val_acc:0.934]
Epoch [76/120    avg_loss:0.056, val_acc:0.939]
Epoch [77/120    avg_loss:0.054, val_acc:0.952]
Epoch [78/120    avg_loss:0.063, val_acc:0.935]
Epoch [79/120    avg_loss:0.063, val_acc:0.952]
Epoch [80/120    avg_loss:0.038, val_acc:0.943]
Epoch [81/120    avg_loss:0.052, val_acc:0.935]
Epoch [82/120    avg_loss:0.048, val_acc:0.947]
Epoch [83/120    avg_loss:0.042, val_acc:0.950]
Epoch [84/120    avg_loss:0.034, val_acc:0.946]
Epoch [85/120    avg_loss:0.049, val_acc:0.950]
Epoch [86/120    avg_loss:0.052, val_acc:0.948]
Epoch [87/120    avg_loss:0.042, val_acc:0.946]
Epoch [88/120    avg_loss:0.047, val_acc:0.942]
Epoch [89/120    avg_loss:0.038, val_acc:0.951]
Epoch [90/120    avg_loss:0.032, val_acc:0.944]
Epoch [91/120    avg_loss:0.027, val_acc:0.948]
Epoch [92/120    avg_loss:0.039, val_acc:0.950]
Epoch [93/120    avg_loss:0.030, val_acc:0.955]
Epoch [94/120    avg_loss:0.022, val_acc:0.957]
Epoch [95/120    avg_loss:0.023, val_acc:0.954]
Epoch [96/120    avg_loss:0.026, val_acc:0.956]
Epoch [97/120    avg_loss:0.019, val_acc:0.956]
Epoch [98/120    avg_loss:0.021, val_acc:0.958]
Epoch [99/120    avg_loss:0.020, val_acc:0.958]
Epoch [100/120    avg_loss:0.019, val_acc:0.959]
Epoch [101/120    avg_loss:0.018, val_acc:0.958]
Epoch [102/120    avg_loss:0.019, val_acc:0.958]
Epoch [103/120    avg_loss:0.017, val_acc:0.956]
Epoch [104/120    avg_loss:0.017, val_acc:0.956]
Epoch [105/120    avg_loss:0.022, val_acc:0.957]
Epoch [106/120    avg_loss:0.020, val_acc:0.957]
Epoch [107/120    avg_loss:0.021, val_acc:0.958]
Epoch [108/120    avg_loss:0.018, val_acc:0.957]
Epoch [109/120    avg_loss:0.016, val_acc:0.957]
Epoch [110/120    avg_loss:0.021, val_acc:0.957]
Epoch [111/120    avg_loss:0.018, val_acc:0.955]
Epoch [112/120    avg_loss:0.022, val_acc:0.959]
Epoch [113/120    avg_loss:0.021, val_acc:0.959]
Epoch [114/120    avg_loss:0.018, val_acc:0.961]
Epoch [115/120    avg_loss:0.029, val_acc:0.959]
Epoch [116/120    avg_loss:0.022, val_acc:0.962]
Epoch [117/120    avg_loss:0.018, val_acc:0.961]
Epoch [118/120    avg_loss:0.019, val_acc:0.959]
Epoch [119/120    avg_loss:0.017, val_acc:0.959]
Epoch [120/120    avg_loss:0.015, val_acc:0.959]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1247    0    2    0    4    0    0    0    5   25    2    0
     0    0    0]
 [   0    0    4  668    1   13    0    0    0   14    0    0   39    8
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    5    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    7    0    0   10    0    0    0    0
     0    0    0]
 [   0    0   16   23    0    4    0    0    0    0  822    6    0    0
     0    4    0]
 [   0    0   15    0    0    0    4    0    0    0   17 2168    3    3
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    1   17    2  510    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    1    1    0    0
  1130    2    0]
 [   0    0    0    0    0    0   32    0    0    0    0    0    0    0
    60  255    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
96.08672086720867

F1 scores:
[       nan 1.         0.97156213 0.92842252 0.99300699 0.96730552
 0.9624171  1.         1.         0.41666667 0.94645941 0.98277425
 0.93235832 0.97112861 0.96829477 0.83881579 0.94545455]

Kappa:
0.9553778224082555
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4737303a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.764, val_acc:0.277]
Epoch [2/120    avg_loss:2.611, val_acc:0.304]
Epoch [3/120    avg_loss:2.449, val_acc:0.371]
Epoch [4/120    avg_loss:2.353, val_acc:0.456]
Epoch [5/120    avg_loss:2.269, val_acc:0.514]
Epoch [6/120    avg_loss:2.228, val_acc:0.529]
Epoch [7/120    avg_loss:2.125, val_acc:0.543]
Epoch [8/120    avg_loss:2.004, val_acc:0.569]
Epoch [9/120    avg_loss:1.922, val_acc:0.595]
Epoch [10/120    avg_loss:1.795, val_acc:0.627]
Epoch [11/120    avg_loss:1.694, val_acc:0.640]
Epoch [12/120    avg_loss:1.592, val_acc:0.630]
Epoch [13/120    avg_loss:1.551, val_acc:0.616]
Epoch [14/120    avg_loss:1.489, val_acc:0.678]
Epoch [15/120    avg_loss:1.357, val_acc:0.675]
Epoch [16/120    avg_loss:1.236, val_acc:0.681]
Epoch [17/120    avg_loss:1.145, val_acc:0.704]
Epoch [18/120    avg_loss:1.059, val_acc:0.718]
Epoch [19/120    avg_loss:0.949, val_acc:0.727]
Epoch [20/120    avg_loss:0.875, val_acc:0.745]
Epoch [21/120    avg_loss:0.882, val_acc:0.737]
Epoch [22/120    avg_loss:0.813, val_acc:0.734]
Epoch [23/120    avg_loss:0.740, val_acc:0.752]
Epoch [24/120    avg_loss:0.650, val_acc:0.781]
Epoch [25/120    avg_loss:0.598, val_acc:0.799]
Epoch [26/120    avg_loss:0.603, val_acc:0.769]
Epoch [27/120    avg_loss:0.561, val_acc:0.819]
Epoch [28/120    avg_loss:0.530, val_acc:0.789]
Epoch [29/120    avg_loss:0.518, val_acc:0.828]
Epoch [30/120    avg_loss:0.489, val_acc:0.810]
Epoch [31/120    avg_loss:0.403, val_acc:0.834]
Epoch [32/120    avg_loss:0.374, val_acc:0.855]
Epoch [33/120    avg_loss:0.387, val_acc:0.867]
Epoch [34/120    avg_loss:0.294, val_acc:0.876]
Epoch [35/120    avg_loss:0.287, val_acc:0.856]
Epoch [36/120    avg_loss:0.279, val_acc:0.865]
Epoch [37/120    avg_loss:0.245, val_acc:0.894]
Epoch [38/120    avg_loss:0.295, val_acc:0.876]
Epoch [39/120    avg_loss:0.309, val_acc:0.844]
Epoch [40/120    avg_loss:0.308, val_acc:0.840]
Epoch [41/120    avg_loss:0.301, val_acc:0.863]
Epoch [42/120    avg_loss:0.563, val_acc:0.792]
Epoch [43/120    avg_loss:0.353, val_acc:0.854]
Epoch [44/120    avg_loss:0.260, val_acc:0.901]
Epoch [45/120    avg_loss:0.254, val_acc:0.876]
Epoch [46/120    avg_loss:0.225, val_acc:0.890]
Epoch [47/120    avg_loss:0.189, val_acc:0.908]
Epoch [48/120    avg_loss:0.198, val_acc:0.871]
Epoch [49/120    avg_loss:0.198, val_acc:0.920]
Epoch [50/120    avg_loss:0.160, val_acc:0.931]
Epoch [51/120    avg_loss:0.134, val_acc:0.918]
Epoch [52/120    avg_loss:0.155, val_acc:0.922]
Epoch [53/120    avg_loss:0.140, val_acc:0.940]
Epoch [54/120    avg_loss:0.132, val_acc:0.894]
Epoch [55/120    avg_loss:0.127, val_acc:0.928]
Epoch [56/120    avg_loss:0.133, val_acc:0.925]
Epoch [57/120    avg_loss:0.131, val_acc:0.911]
Epoch [58/120    avg_loss:0.105, val_acc:0.942]
Epoch [59/120    avg_loss:0.086, val_acc:0.938]
Epoch [60/120    avg_loss:0.091, val_acc:0.936]
Epoch [61/120    avg_loss:0.116, val_acc:0.885]
Epoch [62/120    avg_loss:0.104, val_acc:0.938]
Epoch [63/120    avg_loss:0.102, val_acc:0.944]
Epoch [64/120    avg_loss:0.102, val_acc:0.939]
Epoch [65/120    avg_loss:0.081, val_acc:0.946]
Epoch [66/120    avg_loss:0.065, val_acc:0.945]
Epoch [67/120    avg_loss:0.057, val_acc:0.934]
Epoch [68/120    avg_loss:0.085, val_acc:0.948]
Epoch [69/120    avg_loss:0.088, val_acc:0.938]
Epoch [70/120    avg_loss:0.066, val_acc:0.943]
Epoch [71/120    avg_loss:0.068, val_acc:0.938]
Epoch [72/120    avg_loss:0.059, val_acc:0.946]
Epoch [73/120    avg_loss:0.059, val_acc:0.950]
Epoch [74/120    avg_loss:0.073, val_acc:0.914]
Epoch [75/120    avg_loss:0.061, val_acc:0.956]
Epoch [76/120    avg_loss:0.051, val_acc:0.953]
Epoch [77/120    avg_loss:0.069, val_acc:0.955]
Epoch [78/120    avg_loss:0.045, val_acc:0.950]
Epoch [79/120    avg_loss:0.048, val_acc:0.957]
Epoch [80/120    avg_loss:0.055, val_acc:0.938]
Epoch [81/120    avg_loss:0.051, val_acc:0.957]
Epoch [82/120    avg_loss:0.045, val_acc:0.952]
Epoch [83/120    avg_loss:0.037, val_acc:0.955]
Epoch [84/120    avg_loss:0.038, val_acc:0.957]
Epoch [85/120    avg_loss:0.040, val_acc:0.965]
Epoch [86/120    avg_loss:0.041, val_acc:0.958]
Epoch [87/120    avg_loss:0.050, val_acc:0.953]
Epoch [88/120    avg_loss:0.045, val_acc:0.969]
Epoch [89/120    avg_loss:0.048, val_acc:0.950]
Epoch [90/120    avg_loss:0.054, val_acc:0.941]
Epoch [91/120    avg_loss:0.068, val_acc:0.951]
Epoch [92/120    avg_loss:0.035, val_acc:0.969]
Epoch [93/120    avg_loss:0.042, val_acc:0.961]
Epoch [94/120    avg_loss:0.033, val_acc:0.969]
Epoch [95/120    avg_loss:0.028, val_acc:0.968]
Epoch [96/120    avg_loss:0.026, val_acc:0.963]
Epoch [97/120    avg_loss:0.027, val_acc:0.969]
Epoch [98/120    avg_loss:0.026, val_acc:0.970]
Epoch [99/120    avg_loss:0.040, val_acc:0.967]
Epoch [100/120    avg_loss:0.081, val_acc:0.953]
Epoch [101/120    avg_loss:0.045, val_acc:0.957]
Epoch [102/120    avg_loss:0.042, val_acc:0.958]
Epoch [103/120    avg_loss:0.036, val_acc:0.962]
Epoch [104/120    avg_loss:0.029, val_acc:0.965]
Epoch [105/120    avg_loss:0.028, val_acc:0.958]
Epoch [106/120    avg_loss:0.038, val_acc:0.963]
Epoch [107/120    avg_loss:0.052, val_acc:0.919]
Epoch [108/120    avg_loss:0.039, val_acc:0.956]
Epoch [109/120    avg_loss:0.032, val_acc:0.958]
Epoch [110/120    avg_loss:0.020, val_acc:0.964]
Epoch [111/120    avg_loss:0.017, val_acc:0.967]
Epoch [112/120    avg_loss:0.022, val_acc:0.969]
Epoch [113/120    avg_loss:0.017, val_acc:0.968]
Epoch [114/120    avg_loss:0.017, val_acc:0.967]
Epoch [115/120    avg_loss:0.016, val_acc:0.970]
Epoch [116/120    avg_loss:0.017, val_acc:0.969]
Epoch [117/120    avg_loss:0.018, val_acc:0.970]
Epoch [118/120    avg_loss:0.012, val_acc:0.969]
Epoch [119/120    avg_loss:0.017, val_acc:0.968]
Epoch [120/120    avg_loss:0.019, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1268    1    1    0    0    0    0    0    4   11    0    0
     0    0    0]
 [   0    0    1  714   10    8    0    0    0    9    1    0    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    1    0    0    3    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  425    0    0    0    5    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    9   61    0    0    0    0    0    0  761   37    7    0
     0    0    0]
 [   0    0   13    0    0    0    0    0    1    0   26 2156   14    0
     0    0    0]
 [   0    0    7    3   13    1    0    0    0    0    6    0  502    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   16    0    0    0    0    1    0    0
    73  257    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.10840108401084

F1 scores:
[       nan 0.98765432 0.98142415 0.93394375 0.94666667 0.98165138
 0.98646617 1.         0.99299065 0.66666667 0.90919952 0.97644928
 0.94183865 0.99728997 0.96686491 0.85099338 0.98823529]

Kappa:
0.9556096767667634
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f00d3ecea58>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.802, val_acc:0.249]
Epoch [2/120    avg_loss:2.647, val_acc:0.355]
Epoch [3/120    avg_loss:2.528, val_acc:0.411]
Epoch [4/120    avg_loss:2.412, val_acc:0.457]
Epoch [5/120    avg_loss:2.325, val_acc:0.497]
Epoch [6/120    avg_loss:2.187, val_acc:0.553]
Epoch [7/120    avg_loss:2.152, val_acc:0.534]
Epoch [8/120    avg_loss:2.061, val_acc:0.572]
Epoch [9/120    avg_loss:1.935, val_acc:0.601]
Epoch [10/120    avg_loss:1.832, val_acc:0.602]
Epoch [11/120    avg_loss:1.783, val_acc:0.634]
Epoch [12/120    avg_loss:1.677, val_acc:0.628]
Epoch [13/120    avg_loss:1.568, val_acc:0.667]
Epoch [14/120    avg_loss:1.521, val_acc:0.694]
Epoch [15/120    avg_loss:1.344, val_acc:0.695]
Epoch [16/120    avg_loss:1.278, val_acc:0.671]
Epoch [17/120    avg_loss:1.187, val_acc:0.686]
Epoch [18/120    avg_loss:1.044, val_acc:0.727]
Epoch [19/120    avg_loss:0.961, val_acc:0.725]
Epoch [20/120    avg_loss:0.818, val_acc:0.776]
Epoch [21/120    avg_loss:0.776, val_acc:0.807]
Epoch [22/120    avg_loss:0.795, val_acc:0.793]
Epoch [23/120    avg_loss:0.777, val_acc:0.799]
Epoch [24/120    avg_loss:0.688, val_acc:0.815]
Epoch [25/120    avg_loss:0.586, val_acc:0.849]
Epoch [26/120    avg_loss:0.561, val_acc:0.819]
Epoch [27/120    avg_loss:0.500, val_acc:0.843]
Epoch [28/120    avg_loss:0.484, val_acc:0.751]
Epoch [29/120    avg_loss:0.568, val_acc:0.853]
Epoch [30/120    avg_loss:0.399, val_acc:0.866]
Epoch [31/120    avg_loss:0.430, val_acc:0.853]
Epoch [32/120    avg_loss:0.417, val_acc:0.846]
Epoch [33/120    avg_loss:0.382, val_acc:0.845]
Epoch [34/120    avg_loss:0.378, val_acc:0.875]
Epoch [35/120    avg_loss:0.324, val_acc:0.864]
Epoch [36/120    avg_loss:0.288, val_acc:0.875]
Epoch [37/120    avg_loss:0.291, val_acc:0.826]
Epoch [38/120    avg_loss:0.267, val_acc:0.885]
Epoch [39/120    avg_loss:0.199, val_acc:0.904]
Epoch [40/120    avg_loss:0.209, val_acc:0.890]
Epoch [41/120    avg_loss:0.202, val_acc:0.882]
Epoch [42/120    avg_loss:0.219, val_acc:0.913]
Epoch [43/120    avg_loss:0.248, val_acc:0.871]
Epoch [44/120    avg_loss:0.178, val_acc:0.900]
Epoch [45/120    avg_loss:0.184, val_acc:0.898]
Epoch [46/120    avg_loss:0.222, val_acc:0.876]
Epoch [47/120    avg_loss:0.175, val_acc:0.914]
Epoch [48/120    avg_loss:0.150, val_acc:0.911]
Epoch [49/120    avg_loss:0.117, val_acc:0.932]
Epoch [50/120    avg_loss:0.155, val_acc:0.917]
Epoch [51/120    avg_loss:0.140, val_acc:0.927]
Epoch [52/120    avg_loss:0.115, val_acc:0.920]
Epoch [53/120    avg_loss:0.125, val_acc:0.934]
Epoch [54/120    avg_loss:0.115, val_acc:0.923]
Epoch [55/120    avg_loss:0.095, val_acc:0.921]
Epoch [56/120    avg_loss:0.120, val_acc:0.923]
Epoch [57/120    avg_loss:0.177, val_acc:0.890]
Epoch [58/120    avg_loss:0.172, val_acc:0.890]
Epoch [59/120    avg_loss:0.223, val_acc:0.880]
Epoch [60/120    avg_loss:0.231, val_acc:0.918]
Epoch [61/120    avg_loss:0.134, val_acc:0.930]
Epoch [62/120    avg_loss:0.098, val_acc:0.923]
Epoch [63/120    avg_loss:0.111, val_acc:0.928]
Epoch [64/120    avg_loss:0.085, val_acc:0.939]
Epoch [65/120    avg_loss:0.090, val_acc:0.940]
Epoch [66/120    avg_loss:0.072, val_acc:0.942]
Epoch [67/120    avg_loss:0.110, val_acc:0.935]
Epoch [68/120    avg_loss:0.118, val_acc:0.908]
Epoch [69/120    avg_loss:0.095, val_acc:0.928]
Epoch [70/120    avg_loss:0.070, val_acc:0.936]
Epoch [71/120    avg_loss:0.074, val_acc:0.939]
Epoch [72/120    avg_loss:0.073, val_acc:0.924]
Epoch [73/120    avg_loss:0.056, val_acc:0.961]
Epoch [74/120    avg_loss:0.067, val_acc:0.932]
Epoch [75/120    avg_loss:0.062, val_acc:0.946]
Epoch [76/120    avg_loss:0.043, val_acc:0.961]
Epoch [77/120    avg_loss:0.050, val_acc:0.946]
Epoch [78/120    avg_loss:0.057, val_acc:0.951]
Epoch [79/120    avg_loss:0.052, val_acc:0.953]
Epoch [80/120    avg_loss:0.047, val_acc:0.957]
Epoch [81/120    avg_loss:0.050, val_acc:0.941]
Epoch [82/120    avg_loss:0.046, val_acc:0.955]
Epoch [83/120    avg_loss:0.047, val_acc:0.954]
Epoch [84/120    avg_loss:0.040, val_acc:0.934]
Epoch [85/120    avg_loss:0.053, val_acc:0.948]
Epoch [86/120    avg_loss:0.060, val_acc:0.936]
Epoch [87/120    avg_loss:0.070, val_acc:0.963]
Epoch [88/120    avg_loss:0.050, val_acc:0.948]
Epoch [89/120    avg_loss:0.043, val_acc:0.955]
Epoch [90/120    avg_loss:0.036, val_acc:0.952]
Epoch [91/120    avg_loss:0.047, val_acc:0.956]
Epoch [92/120    avg_loss:0.034, val_acc:0.966]
Epoch [93/120    avg_loss:0.059, val_acc:0.950]
Epoch [94/120    avg_loss:0.034, val_acc:0.957]
Epoch [95/120    avg_loss:0.044, val_acc:0.951]
Epoch [96/120    avg_loss:0.040, val_acc:0.964]
Epoch [97/120    avg_loss:0.051, val_acc:0.953]
Epoch [98/120    avg_loss:0.044, val_acc:0.950]
Epoch [99/120    avg_loss:0.038, val_acc:0.950]
Epoch [100/120    avg_loss:0.041, val_acc:0.961]
Epoch [101/120    avg_loss:0.035, val_acc:0.963]
Epoch [102/120    avg_loss:0.026, val_acc:0.966]
Epoch [103/120    avg_loss:0.022, val_acc:0.967]
Epoch [104/120    avg_loss:0.021, val_acc:0.963]
Epoch [105/120    avg_loss:0.021, val_acc:0.964]
Epoch [106/120    avg_loss:0.026, val_acc:0.952]
Epoch [107/120    avg_loss:0.030, val_acc:0.961]
Epoch [108/120    avg_loss:0.026, val_acc:0.963]
Epoch [109/120    avg_loss:0.023, val_acc:0.971]
Epoch [110/120    avg_loss:0.021, val_acc:0.965]
Epoch [111/120    avg_loss:0.033, val_acc:0.968]
Epoch [112/120    avg_loss:0.040, val_acc:0.963]
Epoch [113/120    avg_loss:0.026, val_acc:0.962]
Epoch [114/120    avg_loss:0.026, val_acc:0.963]
Epoch [115/120    avg_loss:0.031, val_acc:0.967]
Epoch [116/120    avg_loss:0.024, val_acc:0.964]
Epoch [117/120    avg_loss:0.017, val_acc:0.965]
Epoch [118/120    avg_loss:0.022, val_acc:0.955]
Epoch [119/120    avg_loss:0.028, val_acc:0.963]
Epoch [120/120    avg_loss:0.025, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1218    2    0    0    0    0    0    0   14   51    0    0
     0    0    0]
 [   0    0    1  714    1    5    0    0    0   20    0    1    4    0
     1    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    1    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   23   28    0    6    0    0    0    0  783   31    0    0
     1    3    0]
 [   0    0    1    0    0    0    3    0    1    0    6 2199    0    0
     0    0    0]
 [   0    0    3    8    6    4    0    0    0    2   13    3  492    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   13    0    0    0    0    3    0    0    0
  1122    0    1]
 [   0    0    0    0    0    0   42    0    0    0    0    0    0    0
    95  210    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.49051490514906

F1 scores:
[       nan 0.96202532 0.96246543 0.95136576 0.97911833 0.96420582
 0.96307238 1.         0.99883856 0.5862069  0.92280495 0.97798532
 0.95256534 1.         0.94923858 0.75       0.96470588]

Kappa:
0.9484840233150872
creating ./logs/logs-2022-01-17IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-17:23:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8921b09a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.752, val_acc:0.233]
Epoch [2/120    avg_loss:2.606, val_acc:0.295]
Epoch [3/120    avg_loss:2.482, val_acc:0.330]
Epoch [4/120    avg_loss:2.380, val_acc:0.380]
Epoch [5/120    avg_loss:2.309, val_acc:0.370]
Epoch [6/120    avg_loss:2.254, val_acc:0.408]
Epoch [7/120    avg_loss:2.191, val_acc:0.495]
Epoch [8/120    avg_loss:2.041, val_acc:0.544]
Epoch [9/120    avg_loss:2.009, val_acc:0.547]
Epoch [10/120    avg_loss:1.915, val_acc:0.578]
Epoch [11/120    avg_loss:1.795, val_acc:0.577]
Epoch [12/120    avg_loss:1.747, val_acc:0.604]
Epoch [13/120    avg_loss:1.613, val_acc:0.612]
Epoch [14/120    avg_loss:1.532, val_acc:0.657]
Epoch [15/120    avg_loss:1.368, val_acc:0.700]
Epoch [16/120    avg_loss:1.293, val_acc:0.675]
Epoch [17/120    avg_loss:1.153, val_acc:0.713]
Epoch [18/120    avg_loss:1.055, val_acc:0.720]
Epoch [19/120    avg_loss:0.965, val_acc:0.760]
Epoch [20/120    avg_loss:0.914, val_acc:0.760]
Epoch [21/120    avg_loss:0.830, val_acc:0.792]
Epoch [22/120    avg_loss:0.763, val_acc:0.777]
Epoch [23/120    avg_loss:0.713, val_acc:0.775]
Epoch [24/120    avg_loss:0.745, val_acc:0.818]
Epoch [25/120    avg_loss:0.624, val_acc:0.870]
Epoch [26/120    avg_loss:0.514, val_acc:0.854]
Epoch [27/120    avg_loss:0.508, val_acc:0.860]
Epoch [28/120    avg_loss:0.525, val_acc:0.882]
Epoch [29/120    avg_loss:0.492, val_acc:0.850]
Epoch [30/120    avg_loss:0.400, val_acc:0.882]
Epoch [31/120    avg_loss:0.401, val_acc:0.842]
Epoch [32/120    avg_loss:0.413, val_acc:0.890]
Epoch [33/120    avg_loss:0.352, val_acc:0.851]
Epoch [34/120    avg_loss:0.500, val_acc:0.872]
Epoch [35/120    avg_loss:0.333, val_acc:0.900]
Epoch [36/120    avg_loss:0.302, val_acc:0.873]
Epoch [37/120    avg_loss:0.252, val_acc:0.891]
Epoch [38/120    avg_loss:0.263, val_acc:0.923]
Epoch [39/120    avg_loss:0.276, val_acc:0.912]
Epoch [40/120    avg_loss:0.237, val_acc:0.910]
Epoch [41/120    avg_loss:0.350, val_acc:0.899]
Epoch [42/120    avg_loss:0.260, val_acc:0.881]
Epoch [43/120    avg_loss:0.189, val_acc:0.948]
Epoch [44/120    avg_loss:0.141, val_acc:0.936]
Epoch [45/120    avg_loss:0.186, val_acc:0.935]
Epoch [46/120    avg_loss:0.161, val_acc:0.947]
Epoch [47/120    avg_loss:0.143, val_acc:0.929]
Epoch [48/120    avg_loss:0.150, val_acc:0.940]
Epoch [49/120    avg_loss:0.130, val_acc:0.943]
Epoch [50/120    avg_loss:0.147, val_acc:0.932]
Epoch [51/120    avg_loss:0.126, val_acc:0.954]
Epoch [52/120    avg_loss:0.129, val_acc:0.944]
Epoch [53/120    avg_loss:0.097, val_acc:0.935]
Epoch [54/120    avg_loss:0.084, val_acc:0.964]
Epoch [55/120    avg_loss:0.081, val_acc:0.940]
Epoch [56/120    avg_loss:0.100, val_acc:0.951]
Epoch [57/120    avg_loss:0.090, val_acc:0.954]
Epoch [58/120    avg_loss:0.079, val_acc:0.941]
Epoch [59/120    avg_loss:0.084, val_acc:0.957]
Epoch [60/120    avg_loss:0.086, val_acc:0.958]
Epoch [61/120    avg_loss:0.063, val_acc:0.964]
Epoch [62/120    avg_loss:0.061, val_acc:0.963]
Epoch [63/120    avg_loss:0.063, val_acc:0.962]
Epoch [64/120    avg_loss:0.068, val_acc:0.941]
Epoch [65/120    avg_loss:0.053, val_acc:0.960]
Epoch [66/120    avg_loss:0.047, val_acc:0.960]
Epoch [67/120    avg_loss:0.072, val_acc:0.960]
Epoch [68/120    avg_loss:0.058, val_acc:0.957]
Epoch [69/120    avg_loss:0.040, val_acc:0.968]
Epoch [70/120    avg_loss:0.047, val_acc:0.964]
Epoch [71/120    avg_loss:0.048, val_acc:0.962]
Epoch [72/120    avg_loss:0.053, val_acc:0.957]
Epoch [73/120    avg_loss:0.042, val_acc:0.966]
Epoch [74/120    avg_loss:0.041, val_acc:0.964]
Epoch [75/120    avg_loss:0.049, val_acc:0.966]
Epoch [76/120    avg_loss:0.044, val_acc:0.966]
Epoch [77/120    avg_loss:0.040, val_acc:0.964]
Epoch [78/120    avg_loss:0.057, val_acc:0.969]
Epoch [79/120    avg_loss:0.049, val_acc:0.962]
Epoch [80/120    avg_loss:0.031, val_acc:0.978]
Epoch [81/120    avg_loss:0.044, val_acc:0.969]
Epoch [82/120    avg_loss:0.028, val_acc:0.944]
Epoch [83/120    avg_loss:0.048, val_acc:0.955]
Epoch [84/120    avg_loss:0.044, val_acc:0.967]
Epoch [85/120    avg_loss:0.027, val_acc:0.970]
Epoch [86/120    avg_loss:0.028, val_acc:0.964]
Epoch [87/120    avg_loss:0.030, val_acc:0.967]
Epoch [88/120    avg_loss:0.029, val_acc:0.970]
Epoch [89/120    avg_loss:0.023, val_acc:0.970]
Epoch [90/120    avg_loss:0.027, val_acc:0.973]
Epoch [91/120    avg_loss:0.039, val_acc:0.959]
Epoch [92/120    avg_loss:0.050, val_acc:0.962]
Epoch [93/120    avg_loss:0.027, val_acc:0.971]
Epoch [94/120    avg_loss:0.029, val_acc:0.971]
Epoch [95/120    avg_loss:0.017, val_acc:0.972]
Epoch [96/120    avg_loss:0.018, val_acc:0.968]
Epoch [97/120    avg_loss:0.019, val_acc:0.969]
Epoch [98/120    avg_loss:0.017, val_acc:0.969]
Epoch [99/120    avg_loss:0.016, val_acc:0.969]
Epoch [100/120    avg_loss:0.015, val_acc:0.969]
Epoch [101/120    avg_loss:0.015, val_acc:0.972]
Epoch [102/120    avg_loss:0.019, val_acc:0.972]
Epoch [103/120    avg_loss:0.015, val_acc:0.973]
Epoch [104/120    avg_loss:0.016, val_acc:0.972]
Epoch [105/120    avg_loss:0.017, val_acc:0.974]
Epoch [106/120    avg_loss:0.014, val_acc:0.974]
Epoch [107/120    avg_loss:0.018, val_acc:0.974]
Epoch [108/120    avg_loss:0.019, val_acc:0.974]
Epoch [109/120    avg_loss:0.016, val_acc:0.974]
Epoch [110/120    avg_loss:0.016, val_acc:0.973]
Epoch [111/120    avg_loss:0.013, val_acc:0.973]
Epoch [112/120    avg_loss:0.017, val_acc:0.973]
Epoch [113/120    avg_loss:0.015, val_acc:0.973]
Epoch [114/120    avg_loss:0.014, val_acc:0.973]
Epoch [115/120    avg_loss:0.017, val_acc:0.973]
Epoch [116/120    avg_loss:0.020, val_acc:0.973]
Epoch [117/120    avg_loss:0.021, val_acc:0.973]
Epoch [118/120    avg_loss:0.014, val_acc:0.973]
Epoch [119/120    avg_loss:0.014, val_acc:0.973]
Epoch [120/120    avg_loss:0.013, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1247    1    0    0    1    0    0    0    4   30    2    0
     0    0    0]
 [   0    0    1  699    0    3    0    0    0   20    0    0   24    0
     0    0    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    3    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    6    0    0   11    0    0    1    0
     0    0    0]
 [   0    0   17   27    0    5    0    0    0    0  802   14    0    0
     0   10    0]
 [   0    0    6    0    0    1    4    0    0    0   11 2184    4    0
     0    0    0]
 [   0    0    0    0    0    8    0    0    0    0    8   27  483    0
     2    1    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    2    0    0    0
  1133    0    0]
 [   0    0    0    0    0    0   22    0    0    5    0    0    0    0
    41  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.42276422764228

F1 scores:
[       nan 0.975      0.97574335 0.94779661 0.99528302 0.96832579
 0.97473997 1.         1.         0.38596491 0.94131455 0.97805643
 0.92       1.         0.97714532 0.87598116 0.96511628]

Kappa:
0.9591758714454528
