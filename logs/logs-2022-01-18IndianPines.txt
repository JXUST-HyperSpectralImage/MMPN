creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6a7a747ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.773, val_acc:0.137]
Epoch [2/120    avg_loss:2.630, val_acc:0.287]
Epoch [3/120    avg_loss:2.495, val_acc:0.418]
Epoch [4/120    avg_loss:2.356, val_acc:0.467]
Epoch [5/120    avg_loss:2.290, val_acc:0.532]
Epoch [6/120    avg_loss:2.184, val_acc:0.499]
Epoch [7/120    avg_loss:2.087, val_acc:0.477]
Epoch [8/120    avg_loss:2.002, val_acc:0.488]
Epoch [9/120    avg_loss:1.938, val_acc:0.514]
Epoch [10/120    avg_loss:1.832, val_acc:0.541]
Epoch [11/120    avg_loss:1.728, val_acc:0.572]
Epoch [12/120    avg_loss:1.598, val_acc:0.630]
Epoch [13/120    avg_loss:1.529, val_acc:0.650]
Epoch [14/120    avg_loss:1.383, val_acc:0.670]
Epoch [15/120    avg_loss:1.272, val_acc:0.671]
Epoch [16/120    avg_loss:1.172, val_acc:0.695]
Epoch [17/120    avg_loss:1.028, val_acc:0.714]
Epoch [18/120    avg_loss:0.943, val_acc:0.759]
Epoch [19/120    avg_loss:0.894, val_acc:0.730]
Epoch [20/120    avg_loss:0.890, val_acc:0.757]
Epoch [21/120    avg_loss:0.789, val_acc:0.774]
Epoch [22/120    avg_loss:0.666, val_acc:0.791]
Epoch [23/120    avg_loss:0.618, val_acc:0.769]
Epoch [24/120    avg_loss:0.572, val_acc:0.786]
Epoch [25/120    avg_loss:0.565, val_acc:0.822]
Epoch [26/120    avg_loss:0.470, val_acc:0.837]
Epoch [27/120    avg_loss:0.466, val_acc:0.844]
Epoch [28/120    avg_loss:0.468, val_acc:0.809]
Epoch [29/120    avg_loss:0.458, val_acc:0.815]
Epoch [30/120    avg_loss:0.439, val_acc:0.854]
Epoch [31/120    avg_loss:0.335, val_acc:0.885]
Epoch [32/120    avg_loss:0.309, val_acc:0.850]
Epoch [33/120    avg_loss:0.315, val_acc:0.851]
Epoch [34/120    avg_loss:0.354, val_acc:0.856]
Epoch [35/120    avg_loss:0.281, val_acc:0.871]
Epoch [36/120    avg_loss:0.258, val_acc:0.852]
Epoch [37/120    avg_loss:0.299, val_acc:0.836]
Epoch [38/120    avg_loss:0.282, val_acc:0.884]
Epoch [39/120    avg_loss:0.237, val_acc:0.856]
Epoch [40/120    avg_loss:0.242, val_acc:0.857]
Epoch [41/120    avg_loss:0.267, val_acc:0.876]
Epoch [42/120    avg_loss:0.277, val_acc:0.889]
Epoch [43/120    avg_loss:0.230, val_acc:0.899]
Epoch [44/120    avg_loss:0.178, val_acc:0.925]
Epoch [45/120    avg_loss:0.180, val_acc:0.918]
Epoch [46/120    avg_loss:0.173, val_acc:0.916]
Epoch [47/120    avg_loss:0.130, val_acc:0.919]
Epoch [48/120    avg_loss:0.118, val_acc:0.921]
Epoch [49/120    avg_loss:0.108, val_acc:0.928]
Epoch [50/120    avg_loss:0.111, val_acc:0.890]
Epoch [51/120    avg_loss:0.130, val_acc:0.936]
Epoch [52/120    avg_loss:0.127, val_acc:0.925]
Epoch [53/120    avg_loss:0.102, val_acc:0.924]
Epoch [54/120    avg_loss:0.103, val_acc:0.908]
Epoch [55/120    avg_loss:0.096, val_acc:0.916]
Epoch [56/120    avg_loss:0.082, val_acc:0.941]
Epoch [57/120    avg_loss:0.088, val_acc:0.945]
Epoch [58/120    avg_loss:0.084, val_acc:0.935]
Epoch [59/120    avg_loss:0.098, val_acc:0.935]
Epoch [60/120    avg_loss:0.079, val_acc:0.953]
Epoch [61/120    avg_loss:0.057, val_acc:0.953]
Epoch [62/120    avg_loss:0.059, val_acc:0.944]
Epoch [63/120    avg_loss:0.070, val_acc:0.953]
Epoch [64/120    avg_loss:0.064, val_acc:0.935]
Epoch [65/120    avg_loss:0.058, val_acc:0.953]
Epoch [66/120    avg_loss:0.054, val_acc:0.939]
Epoch [67/120    avg_loss:0.066, val_acc:0.946]
Epoch [68/120    avg_loss:0.063, val_acc:0.940]
Epoch [69/120    avg_loss:0.049, val_acc:0.938]
Epoch [70/120    avg_loss:0.044, val_acc:0.951]
Epoch [71/120    avg_loss:0.075, val_acc:0.934]
Epoch [72/120    avg_loss:0.077, val_acc:0.948]
Epoch [73/120    avg_loss:0.051, val_acc:0.957]
Epoch [74/120    avg_loss:0.052, val_acc:0.946]
Epoch [75/120    avg_loss:0.063, val_acc:0.952]
Epoch [76/120    avg_loss:0.058, val_acc:0.959]
Epoch [77/120    avg_loss:0.041, val_acc:0.957]
Epoch [78/120    avg_loss:0.046, val_acc:0.953]
Epoch [79/120    avg_loss:0.070, val_acc:0.946]
Epoch [80/120    avg_loss:0.052, val_acc:0.956]
Epoch [81/120    avg_loss:0.045, val_acc:0.941]
Epoch [82/120    avg_loss:0.045, val_acc:0.953]
Epoch [83/120    avg_loss:0.044, val_acc:0.954]
Epoch [84/120    avg_loss:0.059, val_acc:0.954]
Epoch [85/120    avg_loss:0.045, val_acc:0.947]
Epoch [86/120    avg_loss:0.039, val_acc:0.948]
Epoch [87/120    avg_loss:0.047, val_acc:0.956]
Epoch [88/120    avg_loss:0.031, val_acc:0.959]
Epoch [89/120    avg_loss:0.035, val_acc:0.957]
Epoch [90/120    avg_loss:0.031, val_acc:0.956]
Epoch [91/120    avg_loss:0.032, val_acc:0.965]
Epoch [92/120    avg_loss:0.031, val_acc:0.961]
Epoch [93/120    avg_loss:0.036, val_acc:0.953]
Epoch [94/120    avg_loss:0.051, val_acc:0.961]
Epoch [95/120    avg_loss:0.071, val_acc:0.953]
Epoch [96/120    avg_loss:0.039, val_acc:0.963]
Epoch [97/120    avg_loss:0.033, val_acc:0.962]
Epoch [98/120    avg_loss:0.044, val_acc:0.952]
Epoch [99/120    avg_loss:0.047, val_acc:0.966]
Epoch [100/120    avg_loss:0.040, val_acc:0.967]
Epoch [101/120    avg_loss:0.030, val_acc:0.968]
Epoch [102/120    avg_loss:0.047, val_acc:0.958]
Epoch [103/120    avg_loss:0.032, val_acc:0.955]
Epoch [104/120    avg_loss:0.040, val_acc:0.966]
Epoch [105/120    avg_loss:0.027, val_acc:0.970]
Epoch [106/120    avg_loss:0.019, val_acc:0.965]
Epoch [107/120    avg_loss:0.029, val_acc:0.958]
Epoch [108/120    avg_loss:0.031, val_acc:0.961]
Epoch [109/120    avg_loss:0.023, val_acc:0.966]
Epoch [110/120    avg_loss:0.018, val_acc:0.969]
Epoch [111/120    avg_loss:0.017, val_acc:0.968]
Epoch [112/120    avg_loss:0.015, val_acc:0.964]
Epoch [113/120    avg_loss:0.024, val_acc:0.955]
Epoch [114/120    avg_loss:0.021, val_acc:0.966]
Epoch [115/120    avg_loss:0.019, val_acc:0.967]
Epoch [116/120    avg_loss:0.021, val_acc:0.971]
Epoch [117/120    avg_loss:0.026, val_acc:0.967]
Epoch [118/120    avg_loss:0.029, val_acc:0.965]
Epoch [119/120    avg_loss:0.021, val_acc:0.966]
Epoch [120/120    avg_loss:0.018, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1182    5    0    0    1    0    0    0   40   55    1    1
     0    0    0]
 [   0    0    5  713    0    8    0    0    0   11    1    0    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    3    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    3    0    0   11    0    2    1    0
     0    0    0]
 [   0    0    0   18    0    4    0    0    0    0  849    0    3    0
     0    1    0]
 [   0    0    0    0    0    0    1    0    0    0   23 2180    4    2
     0    0    0]
 [   0    0    0   23    6    8    0    0    0    0   13    6  476    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    0    0    1    0    1    0    0    0
  1128    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    42  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.46612466124661

F1 scores:
[       nan 0.93506494 0.95631068 0.94625083 0.98611111 0.95964126
 0.99468489 1.         0.99883856 0.51162791 0.93967903 0.97867565
 0.92517007 0.9919571  0.97535668 0.93415008 0.98224852]

Kappa:
0.9596946148207192
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faa79928ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.792, val_acc:0.224]
Epoch [2/120    avg_loss:2.653, val_acc:0.303]
Epoch [3/120    avg_loss:2.488, val_acc:0.345]
Epoch [4/120    avg_loss:2.362, val_acc:0.382]
Epoch [5/120    avg_loss:2.247, val_acc:0.405]
Epoch [6/120    avg_loss:2.166, val_acc:0.422]
Epoch [7/120    avg_loss:2.123, val_acc:0.429]
Epoch [8/120    avg_loss:2.070, val_acc:0.463]
Epoch [9/120    avg_loss:1.989, val_acc:0.555]
Epoch [10/120    avg_loss:1.900, val_acc:0.590]
Epoch [11/120    avg_loss:1.828, val_acc:0.607]
Epoch [12/120    avg_loss:1.725, val_acc:0.624]
Epoch [13/120    avg_loss:1.695, val_acc:0.645]
Epoch [14/120    avg_loss:1.550, val_acc:0.675]
Epoch [15/120    avg_loss:1.468, val_acc:0.690]
Epoch [16/120    avg_loss:1.383, val_acc:0.694]
Epoch [17/120    avg_loss:1.291, val_acc:0.724]
Epoch [18/120    avg_loss:1.251, val_acc:0.736]
Epoch [19/120    avg_loss:1.141, val_acc:0.688]
Epoch [20/120    avg_loss:1.098, val_acc:0.748]
Epoch [21/120    avg_loss:0.978, val_acc:0.775]
Epoch [22/120    avg_loss:0.888, val_acc:0.764]
Epoch [23/120    avg_loss:0.794, val_acc:0.785]
Epoch [24/120    avg_loss:0.756, val_acc:0.750]
Epoch [25/120    avg_loss:0.773, val_acc:0.764]
Epoch [26/120    avg_loss:0.716, val_acc:0.809]
Epoch [27/120    avg_loss:0.618, val_acc:0.771]
Epoch [28/120    avg_loss:0.594, val_acc:0.797]
Epoch [29/120    avg_loss:0.541, val_acc:0.851]
Epoch [30/120    avg_loss:0.449, val_acc:0.867]
Epoch [31/120    avg_loss:0.402, val_acc:0.861]
Epoch [32/120    avg_loss:0.375, val_acc:0.867]
Epoch [33/120    avg_loss:0.430, val_acc:0.866]
Epoch [34/120    avg_loss:0.371, val_acc:0.876]
Epoch [35/120    avg_loss:0.342, val_acc:0.885]
Epoch [36/120    avg_loss:0.293, val_acc:0.899]
Epoch [37/120    avg_loss:0.255, val_acc:0.885]
Epoch [38/120    avg_loss:0.271, val_acc:0.896]
Epoch [39/120    avg_loss:0.296, val_acc:0.887]
Epoch [40/120    avg_loss:0.278, val_acc:0.867]
Epoch [41/120    avg_loss:0.235, val_acc:0.908]
Epoch [42/120    avg_loss:0.210, val_acc:0.908]
Epoch [43/120    avg_loss:0.187, val_acc:0.910]
Epoch [44/120    avg_loss:0.213, val_acc:0.901]
Epoch [45/120    avg_loss:0.224, val_acc:0.871]
Epoch [46/120    avg_loss:0.239, val_acc:0.883]
Epoch [47/120    avg_loss:0.283, val_acc:0.888]
Epoch [48/120    avg_loss:0.218, val_acc:0.906]
Epoch [49/120    avg_loss:0.157, val_acc:0.902]
Epoch [50/120    avg_loss:0.141, val_acc:0.928]
Epoch [51/120    avg_loss:0.155, val_acc:0.882]
Epoch [52/120    avg_loss:0.169, val_acc:0.902]
Epoch [53/120    avg_loss:0.140, val_acc:0.924]
Epoch [54/120    avg_loss:0.121, val_acc:0.922]
Epoch [55/120    avg_loss:0.109, val_acc:0.941]
Epoch [56/120    avg_loss:0.152, val_acc:0.917]
Epoch [57/120    avg_loss:0.112, val_acc:0.920]
Epoch [58/120    avg_loss:0.094, val_acc:0.924]
Epoch [59/120    avg_loss:0.108, val_acc:0.921]
Epoch [60/120    avg_loss:0.119, val_acc:0.934]
Epoch [61/120    avg_loss:0.100, val_acc:0.935]
Epoch [62/120    avg_loss:0.103, val_acc:0.934]
Epoch [63/120    avg_loss:0.107, val_acc:0.833]
Epoch [64/120    avg_loss:0.155, val_acc:0.938]
Epoch [65/120    avg_loss:0.102, val_acc:0.941]
Epoch [66/120    avg_loss:0.076, val_acc:0.932]
Epoch [67/120    avg_loss:0.087, val_acc:0.932]
Epoch [68/120    avg_loss:0.075, val_acc:0.939]
Epoch [69/120    avg_loss:0.082, val_acc:0.940]
Epoch [70/120    avg_loss:0.084, val_acc:0.939]
Epoch [71/120    avg_loss:0.069, val_acc:0.942]
Epoch [72/120    avg_loss:0.128, val_acc:0.908]
Epoch [73/120    avg_loss:0.119, val_acc:0.924]
Epoch [74/120    avg_loss:0.084, val_acc:0.941]
Epoch [75/120    avg_loss:0.100, val_acc:0.936]
Epoch [76/120    avg_loss:0.063, val_acc:0.943]
Epoch [77/120    avg_loss:0.063, val_acc:0.954]
Epoch [78/120    avg_loss:0.054, val_acc:0.930]
Epoch [79/120    avg_loss:0.090, val_acc:0.943]
Epoch [80/120    avg_loss:0.063, val_acc:0.938]
Epoch [81/120    avg_loss:0.045, val_acc:0.945]
Epoch [82/120    avg_loss:0.050, val_acc:0.947]
Epoch [83/120    avg_loss:0.076, val_acc:0.929]
Epoch [84/120    avg_loss:0.066, val_acc:0.948]
Epoch [85/120    avg_loss:0.066, val_acc:0.944]
Epoch [86/120    avg_loss:0.072, val_acc:0.956]
Epoch [87/120    avg_loss:0.052, val_acc:0.957]
Epoch [88/120    avg_loss:0.056, val_acc:0.957]
Epoch [89/120    avg_loss:0.058, val_acc:0.951]
Epoch [90/120    avg_loss:0.044, val_acc:0.958]
Epoch [91/120    avg_loss:0.041, val_acc:0.951]
Epoch [92/120    avg_loss:0.034, val_acc:0.944]
Epoch [93/120    avg_loss:0.047, val_acc:0.950]
Epoch [94/120    avg_loss:0.039, val_acc:0.938]
Epoch [95/120    avg_loss:0.037, val_acc:0.953]
Epoch [96/120    avg_loss:0.031, val_acc:0.953]
Epoch [97/120    avg_loss:0.029, val_acc:0.963]
Epoch [98/120    avg_loss:0.025, val_acc:0.963]
Epoch [99/120    avg_loss:0.031, val_acc:0.969]
Epoch [100/120    avg_loss:0.035, val_acc:0.950]
Epoch [101/120    avg_loss:0.037, val_acc:0.964]
Epoch [102/120    avg_loss:0.062, val_acc:0.958]
Epoch [103/120    avg_loss:0.036, val_acc:0.951]
Epoch [104/120    avg_loss:0.024, val_acc:0.959]
Epoch [105/120    avg_loss:0.039, val_acc:0.947]
Epoch [106/120    avg_loss:0.037, val_acc:0.958]
Epoch [107/120    avg_loss:0.033, val_acc:0.956]
Epoch [108/120    avg_loss:0.025, val_acc:0.966]
Epoch [109/120    avg_loss:0.024, val_acc:0.957]
Epoch [110/120    avg_loss:0.027, val_acc:0.964]
Epoch [111/120    avg_loss:0.024, val_acc:0.955]
Epoch [112/120    avg_loss:0.059, val_acc:0.948]
Epoch [113/120    avg_loss:0.037, val_acc:0.959]
Epoch [114/120    avg_loss:0.030, val_acc:0.963]
Epoch [115/120    avg_loss:0.021, val_acc:0.963]
Epoch [116/120    avg_loss:0.022, val_acc:0.964]
Epoch [117/120    avg_loss:0.020, val_acc:0.965]
Epoch [118/120    avg_loss:0.020, val_acc:0.967]
Epoch [119/120    avg_loss:0.019, val_acc:0.965]
Epoch [120/120    avg_loss:0.017, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1238    1   13    0    1    0    0    0    9   21    1    0
     0    1    0]
 [   0    0    1  727    0   14    0    0    0    3    0    0    0    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    7    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   15    0    0    2    0
     0    0    0]
 [   0    0   12   35    0    7    0    0    0    2  813    2    0    0
     0    4    0]
 [   0    0   16    0    0    0    3    0    1    0   17 2164    8    1
     0    0    0]
 [   0    0    0    7    0    9    0    0    0    0    6   29  481    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   10    0    0    9    0    0    0    0
   119  209    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
95.71815718157181

F1 scores:
[       nan 0.96202532 0.97021944 0.95847067 0.97038724 0.95622896
 0.98795181 0.87719298 0.99300699 0.6122449  0.9426087  0.97763723
 0.92857143 0.9919571  0.94903926 0.74509804 0.95121951]

Kappa:
0.951132512038122
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcafbcc4a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.798, val_acc:0.155]
Epoch [2/120    avg_loss:2.656, val_acc:0.239]
Epoch [3/120    avg_loss:2.556, val_acc:0.424]
Epoch [4/120    avg_loss:2.409, val_acc:0.495]
Epoch [5/120    avg_loss:2.330, val_acc:0.484]
Epoch [6/120    avg_loss:2.205, val_acc:0.523]
Epoch [7/120    avg_loss:2.129, val_acc:0.535]
Epoch [8/120    avg_loss:2.034, val_acc:0.553]
Epoch [9/120    avg_loss:1.978, val_acc:0.565]
Epoch [10/120    avg_loss:1.907, val_acc:0.589]
Epoch [11/120    avg_loss:1.809, val_acc:0.594]
Epoch [12/120    avg_loss:1.750, val_acc:0.610]
Epoch [13/120    avg_loss:1.633, val_acc:0.625]
Epoch [14/120    avg_loss:1.591, val_acc:0.646]
Epoch [15/120    avg_loss:1.487, val_acc:0.664]
Epoch [16/120    avg_loss:1.350, val_acc:0.688]
Epoch [17/120    avg_loss:1.229, val_acc:0.703]
Epoch [18/120    avg_loss:1.167, val_acc:0.695]
Epoch [19/120    avg_loss:1.062, val_acc:0.729]
Epoch [20/120    avg_loss:0.933, val_acc:0.773]
Epoch [21/120    avg_loss:0.876, val_acc:0.730]
Epoch [22/120    avg_loss:0.830, val_acc:0.755]
Epoch [23/120    avg_loss:0.730, val_acc:0.764]
Epoch [24/120    avg_loss:0.774, val_acc:0.734]
Epoch [25/120    avg_loss:0.662, val_acc:0.781]
Epoch [26/120    avg_loss:0.630, val_acc:0.827]
Epoch [27/120    avg_loss:0.582, val_acc:0.797]
Epoch [28/120    avg_loss:0.491, val_acc:0.840]
Epoch [29/120    avg_loss:0.539, val_acc:0.826]
Epoch [30/120    avg_loss:0.470, val_acc:0.816]
Epoch [31/120    avg_loss:0.482, val_acc:0.846]
Epoch [32/120    avg_loss:0.450, val_acc:0.850]
Epoch [33/120    avg_loss:0.453, val_acc:0.776]
Epoch [34/120    avg_loss:0.421, val_acc:0.855]
Epoch [35/120    avg_loss:0.324, val_acc:0.842]
Epoch [36/120    avg_loss:0.284, val_acc:0.872]
Epoch [37/120    avg_loss:0.266, val_acc:0.871]
Epoch [38/120    avg_loss:0.243, val_acc:0.908]
Epoch [39/120    avg_loss:0.240, val_acc:0.874]
Epoch [40/120    avg_loss:0.184, val_acc:0.898]
Epoch [41/120    avg_loss:0.192, val_acc:0.896]
Epoch [42/120    avg_loss:0.174, val_acc:0.901]
Epoch [43/120    avg_loss:0.173, val_acc:0.907]
Epoch [44/120    avg_loss:0.163, val_acc:0.906]
Epoch [45/120    avg_loss:0.183, val_acc:0.913]
Epoch [46/120    avg_loss:0.163, val_acc:0.911]
Epoch [47/120    avg_loss:0.166, val_acc:0.907]
Epoch [48/120    avg_loss:0.168, val_acc:0.912]
Epoch [49/120    avg_loss:0.169, val_acc:0.898]
Epoch [50/120    avg_loss:0.186, val_acc:0.883]
Epoch [51/120    avg_loss:0.367, val_acc:0.875]
Epoch [52/120    avg_loss:0.193, val_acc:0.891]
Epoch [53/120    avg_loss:0.173, val_acc:0.905]
Epoch [54/120    avg_loss:0.155, val_acc:0.887]
Epoch [55/120    avg_loss:0.138, val_acc:0.907]
Epoch [56/120    avg_loss:0.172, val_acc:0.900]
Epoch [57/120    avg_loss:0.112, val_acc:0.918]
Epoch [58/120    avg_loss:0.119, val_acc:0.921]
Epoch [59/120    avg_loss:0.127, val_acc:0.925]
Epoch [60/120    avg_loss:0.110, val_acc:0.931]
Epoch [61/120    avg_loss:0.095, val_acc:0.938]
Epoch [62/120    avg_loss:0.106, val_acc:0.917]
Epoch [63/120    avg_loss:0.079, val_acc:0.936]
Epoch [64/120    avg_loss:0.103, val_acc:0.936]
Epoch [65/120    avg_loss:0.096, val_acc:0.939]
Epoch [66/120    avg_loss:0.077, val_acc:0.931]
Epoch [67/120    avg_loss:0.066, val_acc:0.952]
Epoch [68/120    avg_loss:0.064, val_acc:0.947]
Epoch [69/120    avg_loss:0.063, val_acc:0.938]
Epoch [70/120    avg_loss:0.062, val_acc:0.950]
Epoch [71/120    avg_loss:0.070, val_acc:0.948]
Epoch [72/120    avg_loss:0.077, val_acc:0.930]
Epoch [73/120    avg_loss:0.061, val_acc:0.948]
Epoch [74/120    avg_loss:0.049, val_acc:0.940]
Epoch [75/120    avg_loss:0.072, val_acc:0.940]
Epoch [76/120    avg_loss:0.058, val_acc:0.954]
Epoch [77/120    avg_loss:0.042, val_acc:0.951]
Epoch [78/120    avg_loss:0.048, val_acc:0.961]
Epoch [79/120    avg_loss:0.041, val_acc:0.958]
Epoch [80/120    avg_loss:0.038, val_acc:0.962]
Epoch [81/120    avg_loss:0.034, val_acc:0.952]
Epoch [82/120    avg_loss:0.039, val_acc:0.956]
Epoch [83/120    avg_loss:0.035, val_acc:0.954]
Epoch [84/120    avg_loss:0.039, val_acc:0.952]
Epoch [85/120    avg_loss:0.051, val_acc:0.953]
Epoch [86/120    avg_loss:0.040, val_acc:0.961]
Epoch [87/120    avg_loss:0.041, val_acc:0.962]
Epoch [88/120    avg_loss:0.045, val_acc:0.945]
Epoch [89/120    avg_loss:0.042, val_acc:0.939]
Epoch [90/120    avg_loss:0.112, val_acc:0.931]
Epoch [91/120    avg_loss:0.055, val_acc:0.943]
Epoch [92/120    avg_loss:0.065, val_acc:0.931]
Epoch [93/120    avg_loss:0.180, val_acc:0.932]
Epoch [94/120    avg_loss:0.116, val_acc:0.934]
Epoch [95/120    avg_loss:0.068, val_acc:0.945]
Epoch [96/120    avg_loss:0.046, val_acc:0.954]
Epoch [97/120    avg_loss:0.048, val_acc:0.951]
Epoch [98/120    avg_loss:0.051, val_acc:0.944]
Epoch [99/120    avg_loss:0.062, val_acc:0.951]
Epoch [100/120    avg_loss:0.046, val_acc:0.952]
Epoch [101/120    avg_loss:0.032, val_acc:0.959]
Epoch [102/120    avg_loss:0.023, val_acc:0.957]
Epoch [103/120    avg_loss:0.028, val_acc:0.961]
Epoch [104/120    avg_loss:0.030, val_acc:0.961]
Epoch [105/120    avg_loss:0.026, val_acc:0.962]
Epoch [106/120    avg_loss:0.025, val_acc:0.964]
Epoch [107/120    avg_loss:0.027, val_acc:0.965]
Epoch [108/120    avg_loss:0.022, val_acc:0.963]
Epoch [109/120    avg_loss:0.026, val_acc:0.964]
Epoch [110/120    avg_loss:0.022, val_acc:0.965]
Epoch [111/120    avg_loss:0.027, val_acc:0.964]
Epoch [112/120    avg_loss:0.023, val_acc:0.964]
Epoch [113/120    avg_loss:0.022, val_acc:0.966]
Epoch [114/120    avg_loss:0.020, val_acc:0.965]
Epoch [115/120    avg_loss:0.020, val_acc:0.968]
Epoch [116/120    avg_loss:0.022, val_acc:0.966]
Epoch [117/120    avg_loss:0.023, val_acc:0.967]
Epoch [118/120    avg_loss:0.025, val_acc:0.967]
Epoch [119/120    avg_loss:0.021, val_acc:0.970]
Epoch [120/120    avg_loss:0.020, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1226    0    0    0    1    0    0    0   12   45    1    0
     0    0    0]
 [   0    0    0  706    3   27    1    0    0    5    2    0    2    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   13   29    0    5    2    0    0    0  818    7    1    0
     0    0    0]
 [   0    0    9    0    0    0    3    0    0    0   20 2175    1    2
     0    0    0]
 [   0    0    0   10    0   12    0    0    0    0    5   11  493    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    1    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0    4    0    0    3    0    0    0    0
    42  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
96.82384823848238

F1 scores:
[       nan 0.975      0.96802211 0.9463807  0.99300699 0.94967177
 0.99093656 1.         0.99883856 0.77272727 0.94293948 0.97796763
 0.95173745 0.9919571  0.97973264 0.92403101 0.97005988]

Kappa:
0.9637697477443931
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f69121d5b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.799, val_acc:0.148]
Epoch [2/120    avg_loss:2.609, val_acc:0.248]
Epoch [3/120    avg_loss:2.450, val_acc:0.283]
Epoch [4/120    avg_loss:2.355, val_acc:0.364]
Epoch [5/120    avg_loss:2.246, val_acc:0.374]
Epoch [6/120    avg_loss:2.163, val_acc:0.521]
Epoch [7/120    avg_loss:2.074, val_acc:0.566]
Epoch [8/120    avg_loss:1.967, val_acc:0.573]
Epoch [9/120    avg_loss:1.886, val_acc:0.596]
Epoch [10/120    avg_loss:1.786, val_acc:0.604]
Epoch [11/120    avg_loss:1.677, val_acc:0.643]
Epoch [12/120    avg_loss:1.596, val_acc:0.662]
Epoch [13/120    avg_loss:1.537, val_acc:0.672]
Epoch [14/120    avg_loss:1.411, val_acc:0.696]
Epoch [15/120    avg_loss:1.396, val_acc:0.661]
Epoch [16/120    avg_loss:1.427, val_acc:0.670]
Epoch [17/120    avg_loss:1.234, val_acc:0.718]
Epoch [18/120    avg_loss:1.167, val_acc:0.736]
Epoch [19/120    avg_loss:1.017, val_acc:0.771]
Epoch [20/120    avg_loss:0.935, val_acc:0.802]
Epoch [21/120    avg_loss:0.853, val_acc:0.800]
Epoch [22/120    avg_loss:0.776, val_acc:0.818]
Epoch [23/120    avg_loss:0.676, val_acc:0.831]
Epoch [24/120    avg_loss:0.612, val_acc:0.849]
Epoch [25/120    avg_loss:0.586, val_acc:0.842]
Epoch [26/120    avg_loss:0.605, val_acc:0.831]
Epoch [27/120    avg_loss:0.548, val_acc:0.856]
Epoch [28/120    avg_loss:0.467, val_acc:0.830]
Epoch [29/120    avg_loss:0.409, val_acc:0.888]
Epoch [30/120    avg_loss:0.543, val_acc:0.856]
Epoch [31/120    avg_loss:0.417, val_acc:0.900]
Epoch [32/120    avg_loss:0.399, val_acc:0.874]
Epoch [33/120    avg_loss:0.417, val_acc:0.878]
Epoch [34/120    avg_loss:0.335, val_acc:0.896]
Epoch [35/120    avg_loss:0.250, val_acc:0.920]
Epoch [36/120    avg_loss:0.269, val_acc:0.922]
Epoch [37/120    avg_loss:0.217, val_acc:0.934]
Epoch [38/120    avg_loss:0.227, val_acc:0.922]
Epoch [39/120    avg_loss:0.276, val_acc:0.900]
Epoch [40/120    avg_loss:0.256, val_acc:0.900]
Epoch [41/120    avg_loss:0.212, val_acc:0.897]
Epoch [42/120    avg_loss:0.218, val_acc:0.925]
Epoch [43/120    avg_loss:0.269, val_acc:0.901]
Epoch [44/120    avg_loss:0.169, val_acc:0.931]
Epoch [45/120    avg_loss:0.134, val_acc:0.939]
Epoch [46/120    avg_loss:0.165, val_acc:0.920]
Epoch [47/120    avg_loss:0.158, val_acc:0.939]
Epoch [48/120    avg_loss:0.197, val_acc:0.920]
Epoch [49/120    avg_loss:0.151, val_acc:0.946]
Epoch [50/120    avg_loss:0.122, val_acc:0.943]
Epoch [51/120    avg_loss:0.133, val_acc:0.936]
Epoch [52/120    avg_loss:0.179, val_acc:0.939]
Epoch [53/120    avg_loss:0.152, val_acc:0.933]
Epoch [54/120    avg_loss:0.115, val_acc:0.951]
Epoch [55/120    avg_loss:0.110, val_acc:0.938]
Epoch [56/120    avg_loss:0.095, val_acc:0.954]
Epoch [57/120    avg_loss:0.106, val_acc:0.929]
Epoch [58/120    avg_loss:0.077, val_acc:0.955]
Epoch [59/120    avg_loss:0.059, val_acc:0.959]
Epoch [60/120    avg_loss:0.045, val_acc:0.962]
Epoch [61/120    avg_loss:0.063, val_acc:0.961]
Epoch [62/120    avg_loss:0.083, val_acc:0.947]
Epoch [63/120    avg_loss:0.079, val_acc:0.947]
Epoch [64/120    avg_loss:0.088, val_acc:0.957]
Epoch [65/120    avg_loss:0.078, val_acc:0.963]
Epoch [66/120    avg_loss:0.250, val_acc:0.845]
Epoch [67/120    avg_loss:1.395, val_acc:0.697]
Epoch [68/120    avg_loss:0.771, val_acc:0.797]
Epoch [69/120    avg_loss:0.393, val_acc:0.885]
Epoch [70/120    avg_loss:0.385, val_acc:0.862]
Epoch [71/120    avg_loss:0.280, val_acc:0.900]
Epoch [72/120    avg_loss:0.175, val_acc:0.894]
Epoch [73/120    avg_loss:0.151, val_acc:0.921]
Epoch [74/120    avg_loss:0.134, val_acc:0.941]
Epoch [75/120    avg_loss:0.095, val_acc:0.945]
Epoch [76/120    avg_loss:0.106, val_acc:0.936]
Epoch [77/120    avg_loss:0.087, val_acc:0.931]
Epoch [78/120    avg_loss:0.135, val_acc:0.904]
Epoch [79/120    avg_loss:0.111, val_acc:0.940]
Epoch [80/120    avg_loss:0.089, val_acc:0.951]
Epoch [81/120    avg_loss:0.087, val_acc:0.950]
Epoch [82/120    avg_loss:0.069, val_acc:0.955]
Epoch [83/120    avg_loss:0.068, val_acc:0.955]
Epoch [84/120    avg_loss:0.063, val_acc:0.956]
Epoch [85/120    avg_loss:0.056, val_acc:0.957]
Epoch [86/120    avg_loss:0.054, val_acc:0.958]
Epoch [87/120    avg_loss:0.056, val_acc:0.959]
Epoch [88/120    avg_loss:0.062, val_acc:0.958]
Epoch [89/120    avg_loss:0.062, val_acc:0.959]
Epoch [90/120    avg_loss:0.057, val_acc:0.955]
Epoch [91/120    avg_loss:0.051, val_acc:0.955]
Epoch [92/120    avg_loss:0.052, val_acc:0.955]
Epoch [93/120    avg_loss:0.060, val_acc:0.957]
Epoch [94/120    avg_loss:0.050, val_acc:0.957]
Epoch [95/120    avg_loss:0.051, val_acc:0.957]
Epoch [96/120    avg_loss:0.055, val_acc:0.957]
Epoch [97/120    avg_loss:0.054, val_acc:0.957]
Epoch [98/120    avg_loss:0.058, val_acc:0.957]
Epoch [99/120    avg_loss:0.048, val_acc:0.956]
Epoch [100/120    avg_loss:0.051, val_acc:0.957]
Epoch [101/120    avg_loss:0.047, val_acc:0.957]
Epoch [102/120    avg_loss:0.054, val_acc:0.958]
Epoch [103/120    avg_loss:0.044, val_acc:0.959]
Epoch [104/120    avg_loss:0.049, val_acc:0.959]
Epoch [105/120    avg_loss:0.048, val_acc:0.959]
Epoch [106/120    avg_loss:0.055, val_acc:0.959]
Epoch [107/120    avg_loss:0.057, val_acc:0.959]
Epoch [108/120    avg_loss:0.050, val_acc:0.959]
Epoch [109/120    avg_loss:0.051, val_acc:0.959]
Epoch [110/120    avg_loss:0.049, val_acc:0.959]
Epoch [111/120    avg_loss:0.047, val_acc:0.959]
Epoch [112/120    avg_loss:0.048, val_acc:0.959]
Epoch [113/120    avg_loss:0.064, val_acc:0.959]
Epoch [114/120    avg_loss:0.048, val_acc:0.959]
Epoch [115/120    avg_loss:0.058, val_acc:0.959]
Epoch [116/120    avg_loss:0.051, val_acc:0.959]
Epoch [117/120    avg_loss:0.050, val_acc:0.959]
Epoch [118/120    avg_loss:0.051, val_acc:0.959]
Epoch [119/120    avg_loss:0.056, val_acc:0.959]
Epoch [120/120    avg_loss:0.061, val_acc:0.959]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1222    8    0    0    1    0    0    0    2   51    0    0
     1    0    0]
 [   0    0    4  720    4    1    0    0    0    9    0    2    6    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    2    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   38   31    0    8    1    0    0    0  794    0    0    0
     0    3    0]
 [   0    0   37    0    0    5   12    0    1    0   23 2122    8    2
     0    0    0]
 [   0    0    5   32    0   10    0    0    0    0    2    1  480    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   22    0    0    0    0    0    0    0    0
  1115    2    0]
 [   0    0    0    0    0    0   12    0    0    7    0    0    0    0
    51  277    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.44715447154472

F1 scores:
[       nan 0.975      0.94326515 0.93567251 0.99069767 0.94285714
 0.97830965 0.96153846 0.99767442 0.61818182 0.9352179  0.96740369
 0.93294461 0.9919571  0.96620451 0.87797147 0.98823529]

Kappa:
0.9481156360992884
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8001001ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.792, val_acc:0.271]
Epoch [2/120    avg_loss:2.654, val_acc:0.316]
Epoch [3/120    avg_loss:2.518, val_acc:0.359]
Epoch [4/120    avg_loss:2.403, val_acc:0.463]
Epoch [5/120    avg_loss:2.313, val_acc:0.484]
Epoch [6/120    avg_loss:2.140, val_acc:0.508]
Epoch [7/120    avg_loss:2.110, val_acc:0.536]
Epoch [8/120    avg_loss:2.014, val_acc:0.555]
Epoch [9/120    avg_loss:1.935, val_acc:0.580]
Epoch [10/120    avg_loss:1.844, val_acc:0.610]
Epoch [11/120    avg_loss:1.782, val_acc:0.618]
Epoch [12/120    avg_loss:1.693, val_acc:0.628]
Epoch [13/120    avg_loss:1.588, val_acc:0.632]
Epoch [14/120    avg_loss:1.492, val_acc:0.621]
Epoch [15/120    avg_loss:1.446, val_acc:0.666]
Epoch [16/120    avg_loss:1.335, val_acc:0.677]
Epoch [17/120    avg_loss:1.228, val_acc:0.686]
Epoch [18/120    avg_loss:1.124, val_acc:0.721]
Epoch [19/120    avg_loss:1.049, val_acc:0.705]
Epoch [20/120    avg_loss:0.970, val_acc:0.758]
Epoch [21/120    avg_loss:0.919, val_acc:0.740]
Epoch [22/120    avg_loss:0.786, val_acc:0.774]
Epoch [23/120    avg_loss:0.679, val_acc:0.798]
Epoch [24/120    avg_loss:0.736, val_acc:0.785]
Epoch [25/120    avg_loss:0.673, val_acc:0.716]
Epoch [26/120    avg_loss:0.731, val_acc:0.830]
Epoch [27/120    avg_loss:0.604, val_acc:0.836]
Epoch [28/120    avg_loss:0.525, val_acc:0.852]
Epoch [29/120    avg_loss:0.450, val_acc:0.794]
Epoch [30/120    avg_loss:0.486, val_acc:0.857]
Epoch [31/120    avg_loss:0.382, val_acc:0.849]
Epoch [32/120    avg_loss:0.375, val_acc:0.877]
Epoch [33/120    avg_loss:0.331, val_acc:0.884]
Epoch [34/120    avg_loss:0.271, val_acc:0.896]
Epoch [35/120    avg_loss:0.247, val_acc:0.888]
Epoch [36/120    avg_loss:0.250, val_acc:0.906]
Epoch [37/120    avg_loss:0.239, val_acc:0.916]
Epoch [38/120    avg_loss:0.191, val_acc:0.896]
Epoch [39/120    avg_loss:0.204, val_acc:0.924]
Epoch [40/120    avg_loss:0.163, val_acc:0.923]
Epoch [41/120    avg_loss:0.181, val_acc:0.910]
Epoch [42/120    avg_loss:0.209, val_acc:0.922]
Epoch [43/120    avg_loss:0.194, val_acc:0.923]
Epoch [44/120    avg_loss:0.155, val_acc:0.927]
Epoch [45/120    avg_loss:0.157, val_acc:0.924]
Epoch [46/120    avg_loss:0.133, val_acc:0.932]
Epoch [47/120    avg_loss:0.113, val_acc:0.936]
Epoch [48/120    avg_loss:0.099, val_acc:0.936]
Epoch [49/120    avg_loss:0.370, val_acc:0.886]
Epoch [50/120    avg_loss:0.249, val_acc:0.890]
Epoch [51/120    avg_loss:0.172, val_acc:0.929]
Epoch [52/120    avg_loss:0.139, val_acc:0.950]
Epoch [53/120    avg_loss:0.119, val_acc:0.917]
Epoch [54/120    avg_loss:0.134, val_acc:0.919]
Epoch [55/120    avg_loss:0.112, val_acc:0.943]
Epoch [56/120    avg_loss:0.084, val_acc:0.914]
Epoch [57/120    avg_loss:0.079, val_acc:0.958]
Epoch [58/120    avg_loss:0.068, val_acc:0.963]
Epoch [59/120    avg_loss:0.061, val_acc:0.944]
Epoch [60/120    avg_loss:0.089, val_acc:0.933]
Epoch [61/120    avg_loss:0.091, val_acc:0.961]
Epoch [62/120    avg_loss:0.073, val_acc:0.947]
Epoch [63/120    avg_loss:0.066, val_acc:0.964]
Epoch [64/120    avg_loss:0.131, val_acc:0.941]
Epoch [65/120    avg_loss:0.128, val_acc:0.922]
Epoch [66/120    avg_loss:0.132, val_acc:0.945]
Epoch [67/120    avg_loss:0.112, val_acc:0.946]
Epoch [68/120    avg_loss:0.083, val_acc:0.958]
Epoch [69/120    avg_loss:0.048, val_acc:0.962]
Epoch [70/120    avg_loss:0.119, val_acc:0.919]
Epoch [71/120    avg_loss:0.090, val_acc:0.951]
Epoch [72/120    avg_loss:0.072, val_acc:0.965]
Epoch [73/120    avg_loss:0.057, val_acc:0.959]
Epoch [74/120    avg_loss:0.066, val_acc:0.957]
Epoch [75/120    avg_loss:0.054, val_acc:0.952]
Epoch [76/120    avg_loss:0.069, val_acc:0.944]
Epoch [77/120    avg_loss:0.047, val_acc:0.967]
Epoch [78/120    avg_loss:0.037, val_acc:0.964]
Epoch [79/120    avg_loss:0.088, val_acc:0.952]
Epoch [80/120    avg_loss:0.072, val_acc:0.963]
Epoch [81/120    avg_loss:0.069, val_acc:0.962]
Epoch [82/120    avg_loss:0.050, val_acc:0.966]
Epoch [83/120    avg_loss:0.038, val_acc:0.975]
Epoch [84/120    avg_loss:0.033, val_acc:0.967]
Epoch [85/120    avg_loss:0.054, val_acc:0.977]
Epoch [86/120    avg_loss:0.033, val_acc:0.965]
Epoch [87/120    avg_loss:0.024, val_acc:0.970]
Epoch [88/120    avg_loss:0.029, val_acc:0.968]
Epoch [89/120    avg_loss:0.039, val_acc:0.962]
Epoch [90/120    avg_loss:0.051, val_acc:0.968]
Epoch [91/120    avg_loss:0.041, val_acc:0.971]
Epoch [92/120    avg_loss:0.034, val_acc:0.977]
Epoch [93/120    avg_loss:0.023, val_acc:0.975]
Epoch [94/120    avg_loss:0.020, val_acc:0.973]
Epoch [95/120    avg_loss:0.019, val_acc:0.970]
Epoch [96/120    avg_loss:0.025, val_acc:0.976]
Epoch [97/120    avg_loss:0.029, val_acc:0.958]
Epoch [98/120    avg_loss:0.029, val_acc:0.970]
Epoch [99/120    avg_loss:0.023, val_acc:0.973]
Epoch [100/120    avg_loss:0.024, val_acc:0.965]
Epoch [101/120    avg_loss:0.017, val_acc:0.975]
Epoch [102/120    avg_loss:0.024, val_acc:0.976]
Epoch [103/120    avg_loss:0.015, val_acc:0.984]
Epoch [104/120    avg_loss:0.015, val_acc:0.982]
Epoch [105/120    avg_loss:0.017, val_acc:0.975]
Epoch [106/120    avg_loss:0.026, val_acc:0.984]
Epoch [107/120    avg_loss:0.015, val_acc:0.979]
Epoch [108/120    avg_loss:0.018, val_acc:0.978]
Epoch [109/120    avg_loss:0.020, val_acc:0.977]
Epoch [110/120    avg_loss:0.026, val_acc:0.953]
Epoch [111/120    avg_loss:0.045, val_acc:0.981]
Epoch [112/120    avg_loss:0.040, val_acc:0.981]
Epoch [113/120    avg_loss:0.017, val_acc:0.975]
Epoch [114/120    avg_loss:0.024, val_acc:0.973]
Epoch [115/120    avg_loss:0.030, val_acc:0.970]
Epoch [116/120    avg_loss:0.023, val_acc:0.980]
Epoch [117/120    avg_loss:0.024, val_acc:0.978]
Epoch [118/120    avg_loss:0.013, val_acc:0.984]
Epoch [119/120    avg_loss:0.011, val_acc:0.980]
Epoch [120/120    avg_loss:0.022, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1271    0    0    0    2    0    0    1    0    9    2    0
     0    0    0]
 [   0    0    4  703   10   15    0    0    0   12    0    0    2    1
     0    0    0]
 [   0    0    0    0  211    0    0    0    0    0    0    0    1    0
     1    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1   36    0    5    0    0    0    0  817   16    0    0
     0    0    0]
 [   0    0   11    0    0    0    6    0    0    0    4 2180    8    1
     0    0    0]
 [   0    0    5    2    0    1    0    0    0    1    9    9  505    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   24    0    0    0    0    0    0    0
    86  237    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.84552845528455

F1 scores:
[       nan 1.         0.98641832 0.94489247 0.97235023 0.97412823
 0.97622585 1.         1.         0.69230769 0.95835777 0.98553345
 0.95825427 0.99462366 0.96280642 0.81164384 0.98203593]

Kappa:
0.9640071653826824
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd7eecc5a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.796, val_acc:0.201]
Epoch [2/120    avg_loss:2.631, val_acc:0.363]
Epoch [3/120    avg_loss:2.450, val_acc:0.419]
Epoch [4/120    avg_loss:2.341, val_acc:0.490]
Epoch [5/120    avg_loss:2.243, val_acc:0.490]
Epoch [6/120    avg_loss:2.181, val_acc:0.520]
Epoch [7/120    avg_loss:2.047, val_acc:0.582]
Epoch [8/120    avg_loss:2.001, val_acc:0.584]
Epoch [9/120    avg_loss:1.884, val_acc:0.602]
Epoch [10/120    avg_loss:1.789, val_acc:0.617]
Epoch [11/120    avg_loss:1.673, val_acc:0.639]
Epoch [12/120    avg_loss:1.611, val_acc:0.674]
Epoch [13/120    avg_loss:1.506, val_acc:0.694]
Epoch [14/120    avg_loss:1.383, val_acc:0.752]
Epoch [15/120    avg_loss:1.274, val_acc:0.749]
Epoch [16/120    avg_loss:1.161, val_acc:0.766]
Epoch [17/120    avg_loss:1.009, val_acc:0.761]
Epoch [18/120    avg_loss:0.915, val_acc:0.797]
Epoch [19/120    avg_loss:0.859, val_acc:0.783]
Epoch [20/120    avg_loss:0.798, val_acc:0.811]
Epoch [21/120    avg_loss:0.732, val_acc:0.812]
Epoch [22/120    avg_loss:0.661, val_acc:0.838]
Epoch [23/120    avg_loss:0.631, val_acc:0.849]
Epoch [24/120    avg_loss:0.523, val_acc:0.841]
Epoch [25/120    avg_loss:0.585, val_acc:0.819]
Epoch [26/120    avg_loss:0.451, val_acc:0.873]
Epoch [27/120    avg_loss:0.410, val_acc:0.848]
Epoch [28/120    avg_loss:0.443, val_acc:0.882]
Epoch [29/120    avg_loss:0.382, val_acc:0.849]
Epoch [30/120    avg_loss:0.349, val_acc:0.871]
Epoch [31/120    avg_loss:0.305, val_acc:0.895]
Epoch [32/120    avg_loss:0.360, val_acc:0.890]
Epoch [33/120    avg_loss:0.386, val_acc:0.868]
Epoch [34/120    avg_loss:0.278, val_acc:0.870]
Epoch [35/120    avg_loss:0.264, val_acc:0.888]
Epoch [36/120    avg_loss:0.287, val_acc:0.832]
Epoch [37/120    avg_loss:0.347, val_acc:0.875]
Epoch [38/120    avg_loss:0.266, val_acc:0.897]
Epoch [39/120    avg_loss:0.239, val_acc:0.909]
Epoch [40/120    avg_loss:0.327, val_acc:0.886]
Epoch [41/120    avg_loss:0.246, val_acc:0.886]
Epoch [42/120    avg_loss:0.193, val_acc:0.920]
Epoch [43/120    avg_loss:0.148, val_acc:0.910]
Epoch [44/120    avg_loss:0.167, val_acc:0.919]
Epoch [45/120    avg_loss:0.167, val_acc:0.944]
Epoch [46/120    avg_loss:0.151, val_acc:0.902]
Epoch [47/120    avg_loss:0.109, val_acc:0.932]
Epoch [48/120    avg_loss:0.137, val_acc:0.934]
Epoch [49/120    avg_loss:0.162, val_acc:0.907]
Epoch [50/120    avg_loss:0.141, val_acc:0.924]
Epoch [51/120    avg_loss:0.155, val_acc:0.905]
Epoch [52/120    avg_loss:0.117, val_acc:0.947]
Epoch [53/120    avg_loss:0.137, val_acc:0.925]
Epoch [54/120    avg_loss:0.124, val_acc:0.940]
Epoch [55/120    avg_loss:0.124, val_acc:0.925]
Epoch [56/120    avg_loss:0.129, val_acc:0.930]
Epoch [57/120    avg_loss:0.110, val_acc:0.938]
Epoch [58/120    avg_loss:0.121, val_acc:0.940]
Epoch [59/120    avg_loss:0.110, val_acc:0.929]
Epoch [60/120    avg_loss:0.102, val_acc:0.954]
Epoch [61/120    avg_loss:0.086, val_acc:0.952]
Epoch [62/120    avg_loss:0.079, val_acc:0.943]
Epoch [63/120    avg_loss:0.102, val_acc:0.957]
Epoch [64/120    avg_loss:0.072, val_acc:0.963]
Epoch [65/120    avg_loss:0.087, val_acc:0.951]
Epoch [66/120    avg_loss:0.077, val_acc:0.956]
Epoch [67/120    avg_loss:0.060, val_acc:0.959]
Epoch [68/120    avg_loss:0.120, val_acc:0.945]
Epoch [69/120    avg_loss:0.100, val_acc:0.947]
Epoch [70/120    avg_loss:0.078, val_acc:0.951]
Epoch [71/120    avg_loss:0.079, val_acc:0.962]
Epoch [72/120    avg_loss:0.054, val_acc:0.962]
Epoch [73/120    avg_loss:0.044, val_acc:0.963]
Epoch [74/120    avg_loss:0.045, val_acc:0.964]
Epoch [75/120    avg_loss:0.054, val_acc:0.956]
Epoch [76/120    avg_loss:0.065, val_acc:0.928]
Epoch [77/120    avg_loss:0.080, val_acc:0.964]
Epoch [78/120    avg_loss:0.050, val_acc:0.952]
Epoch [79/120    avg_loss:0.067, val_acc:0.952]
Epoch [80/120    avg_loss:0.065, val_acc:0.951]
Epoch [81/120    avg_loss:0.070, val_acc:0.963]
Epoch [82/120    avg_loss:0.081, val_acc:0.951]
Epoch [83/120    avg_loss:0.074, val_acc:0.957]
Epoch [84/120    avg_loss:0.073, val_acc:0.950]
Epoch [85/120    avg_loss:0.050, val_acc:0.962]
Epoch [86/120    avg_loss:0.048, val_acc:0.965]
Epoch [87/120    avg_loss:0.032, val_acc:0.969]
Epoch [88/120    avg_loss:0.049, val_acc:0.971]
Epoch [89/120    avg_loss:0.074, val_acc:0.963]
Epoch [90/120    avg_loss:0.067, val_acc:0.946]
Epoch [91/120    avg_loss:0.074, val_acc:0.952]
Epoch [92/120    avg_loss:0.044, val_acc:0.973]
Epoch [93/120    avg_loss:0.046, val_acc:0.961]
Epoch [94/120    avg_loss:0.032, val_acc:0.970]
Epoch [95/120    avg_loss:0.037, val_acc:0.950]
Epoch [96/120    avg_loss:0.057, val_acc:0.963]
Epoch [97/120    avg_loss:0.035, val_acc:0.971]
Epoch [98/120    avg_loss:0.034, val_acc:0.975]
Epoch [99/120    avg_loss:0.038, val_acc:0.970]
Epoch [100/120    avg_loss:0.031, val_acc:0.962]
Epoch [101/120    avg_loss:0.030, val_acc:0.967]
Epoch [102/120    avg_loss:0.056, val_acc:0.963]
Epoch [103/120    avg_loss:0.039, val_acc:0.954]
Epoch [104/120    avg_loss:0.036, val_acc:0.971]
Epoch [105/120    avg_loss:0.030, val_acc:0.963]
Epoch [106/120    avg_loss:0.026, val_acc:0.970]
Epoch [107/120    avg_loss:0.023, val_acc:0.976]
Epoch [108/120    avg_loss:0.021, val_acc:0.970]
Epoch [109/120    avg_loss:0.023, val_acc:0.976]
Epoch [110/120    avg_loss:0.032, val_acc:0.974]
Epoch [111/120    avg_loss:0.024, val_acc:0.980]
Epoch [112/120    avg_loss:0.019, val_acc:0.979]
Epoch [113/120    avg_loss:0.018, val_acc:0.978]
Epoch [114/120    avg_loss:0.026, val_acc:0.974]
Epoch [115/120    avg_loss:0.022, val_acc:0.980]
Epoch [116/120    avg_loss:0.023, val_acc:0.961]
Epoch [117/120    avg_loss:0.025, val_acc:0.979]
Epoch [118/120    avg_loss:0.031, val_acc:0.969]
Epoch [119/120    avg_loss:0.021, val_acc:0.981]
Epoch [120/120    avg_loss:0.024, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1239    0    4    0    4    0    0    1   15   21    1    0
     0    0    0]
 [   0    0    2  736    0    6    0    0    0    2    0    0    1    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  421    0    6    0    5    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   13    0    0    5    0
     0    0    0]
 [   0    0   31   25    0   14    0    0    0    0  798    3    3    0
     1    0    0]
 [   0    0   24    1    0    0    1    0   13    0    7 2161    2    1
     0    0    0]
 [   0    0    0   19    7   13    0    0    0    0    5    0  482    0
     1    1    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   11    0    0    0    0    2    0    0    0
  1126    0    0]
 [   0    0    0    0    0    0   31    0    0    0    0    0    0    0
    62  254    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.08672086720867

F1 scores:
[       nan 0.98765432 0.95972115 0.96335079 0.97482838 0.93555556
 0.97333333 0.89285714 0.98510882 0.66666667 0.93772033 0.98339022
 0.93774319 0.99730458 0.96569468 0.84385382 0.96551724]

Kappa:
0.9553866922725286
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc20609aac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.751, val_acc:0.204]
Epoch [2/120    avg_loss:2.618, val_acc:0.285]
Epoch [3/120    avg_loss:2.517, val_acc:0.405]
Epoch [4/120    avg_loss:2.390, val_acc:0.438]
Epoch [5/120    avg_loss:2.277, val_acc:0.472]
Epoch [6/120    avg_loss:2.190, val_acc:0.512]
Epoch [7/120    avg_loss:2.069, val_acc:0.581]
Epoch [8/120    avg_loss:2.039, val_acc:0.592]
Epoch [9/120    avg_loss:1.883, val_acc:0.637]
Epoch [10/120    avg_loss:1.812, val_acc:0.620]
Epoch [11/120    avg_loss:1.709, val_acc:0.649]
Epoch [12/120    avg_loss:1.585, val_acc:0.667]
Epoch [13/120    avg_loss:1.491, val_acc:0.671]
Epoch [14/120    avg_loss:1.348, val_acc:0.690]
Epoch [15/120    avg_loss:1.287, val_acc:0.698]
Epoch [16/120    avg_loss:1.147, val_acc:0.724]
Epoch [17/120    avg_loss:1.037, val_acc:0.712]
Epoch [18/120    avg_loss:0.989, val_acc:0.725]
Epoch [19/120    avg_loss:0.882, val_acc:0.752]
Epoch [20/120    avg_loss:0.801, val_acc:0.779]
Epoch [21/120    avg_loss:0.741, val_acc:0.807]
Epoch [22/120    avg_loss:0.684, val_acc:0.792]
Epoch [23/120    avg_loss:0.643, val_acc:0.811]
Epoch [24/120    avg_loss:0.616, val_acc:0.839]
Epoch [25/120    avg_loss:0.596, val_acc:0.839]
Epoch [26/120    avg_loss:0.596, val_acc:0.821]
Epoch [27/120    avg_loss:0.558, val_acc:0.810]
Epoch [28/120    avg_loss:0.624, val_acc:0.788]
Epoch [29/120    avg_loss:0.588, val_acc:0.798]
Epoch [30/120    avg_loss:0.543, val_acc:0.845]
Epoch [31/120    avg_loss:0.449, val_acc:0.827]
Epoch [32/120    avg_loss:0.404, val_acc:0.863]
Epoch [33/120    avg_loss:0.403, val_acc:0.857]
Epoch [34/120    avg_loss:0.347, val_acc:0.881]
Epoch [35/120    avg_loss:0.415, val_acc:0.836]
Epoch [36/120    avg_loss:0.432, val_acc:0.858]
Epoch [37/120    avg_loss:0.360, val_acc:0.876]
Epoch [38/120    avg_loss:0.306, val_acc:0.863]
Epoch [39/120    avg_loss:0.304, val_acc:0.887]
Epoch [40/120    avg_loss:0.306, val_acc:0.859]
Epoch [41/120    avg_loss:0.290, val_acc:0.896]
Epoch [42/120    avg_loss:0.265, val_acc:0.921]
Epoch [43/120    avg_loss:0.228, val_acc:0.910]
Epoch [44/120    avg_loss:0.220, val_acc:0.896]
Epoch [45/120    avg_loss:0.252, val_acc:0.912]
Epoch [46/120    avg_loss:0.208, val_acc:0.937]
Epoch [47/120    avg_loss:0.167, val_acc:0.941]
Epoch [48/120    avg_loss:0.144, val_acc:0.927]
Epoch [49/120    avg_loss:0.148, val_acc:0.929]
Epoch [50/120    avg_loss:0.144, val_acc:0.936]
Epoch [51/120    avg_loss:0.128, val_acc:0.927]
Epoch [52/120    avg_loss:0.149, val_acc:0.932]
Epoch [53/120    avg_loss:0.175, val_acc:0.894]
Epoch [54/120    avg_loss:0.122, val_acc:0.930]
Epoch [55/120    avg_loss:0.143, val_acc:0.920]
Epoch [56/120    avg_loss:0.130, val_acc:0.925]
Epoch [57/120    avg_loss:0.139, val_acc:0.929]
Epoch [58/120    avg_loss:0.167, val_acc:0.920]
Epoch [59/120    avg_loss:0.123, val_acc:0.935]
Epoch [60/120    avg_loss:0.099, val_acc:0.940]
Epoch [61/120    avg_loss:0.090, val_acc:0.948]
Epoch [62/120    avg_loss:0.072, val_acc:0.950]
Epoch [63/120    avg_loss:0.072, val_acc:0.955]
Epoch [64/120    avg_loss:0.071, val_acc:0.949]
Epoch [65/120    avg_loss:0.059, val_acc:0.953]
Epoch [66/120    avg_loss:0.065, val_acc:0.953]
Epoch [67/120    avg_loss:0.066, val_acc:0.954]
Epoch [68/120    avg_loss:0.062, val_acc:0.954]
Epoch [69/120    avg_loss:0.066, val_acc:0.954]
Epoch [70/120    avg_loss:0.060, val_acc:0.955]
Epoch [71/120    avg_loss:0.064, val_acc:0.954]
Epoch [72/120    avg_loss:0.066, val_acc:0.955]
Epoch [73/120    avg_loss:0.063, val_acc:0.952]
Epoch [74/120    avg_loss:0.064, val_acc:0.950]
Epoch [75/120    avg_loss:0.058, val_acc:0.952]
Epoch [76/120    avg_loss:0.059, val_acc:0.954]
Epoch [77/120    avg_loss:0.058, val_acc:0.954]
Epoch [78/120    avg_loss:0.061, val_acc:0.954]
Epoch [79/120    avg_loss:0.058, val_acc:0.953]
Epoch [80/120    avg_loss:0.063, val_acc:0.953]
Epoch [81/120    avg_loss:0.055, val_acc:0.954]
Epoch [82/120    avg_loss:0.066, val_acc:0.955]
Epoch [83/120    avg_loss:0.061, val_acc:0.957]
Epoch [84/120    avg_loss:0.054, val_acc:0.956]
Epoch [85/120    avg_loss:0.058, val_acc:0.955]
Epoch [86/120    avg_loss:0.057, val_acc:0.955]
Epoch [87/120    avg_loss:0.048, val_acc:0.955]
Epoch [88/120    avg_loss:0.051, val_acc:0.958]
Epoch [89/120    avg_loss:0.052, val_acc:0.957]
Epoch [90/120    avg_loss:0.046, val_acc:0.957]
Epoch [91/120    avg_loss:0.058, val_acc:0.957]
Epoch [92/120    avg_loss:0.054, val_acc:0.956]
Epoch [93/120    avg_loss:0.049, val_acc:0.955]
Epoch [94/120    avg_loss:0.054, val_acc:0.957]
Epoch [95/120    avg_loss:0.047, val_acc:0.956]
Epoch [96/120    avg_loss:0.053, val_acc:0.958]
Epoch [97/120    avg_loss:0.056, val_acc:0.957]
Epoch [98/120    avg_loss:0.060, val_acc:0.958]
Epoch [99/120    avg_loss:0.052, val_acc:0.958]
Epoch [100/120    avg_loss:0.054, val_acc:0.956]
Epoch [101/120    avg_loss:0.058, val_acc:0.957]
Epoch [102/120    avg_loss:0.053, val_acc:0.958]
Epoch [103/120    avg_loss:0.067, val_acc:0.959]
Epoch [104/120    avg_loss:0.052, val_acc:0.959]
Epoch [105/120    avg_loss:0.053, val_acc:0.959]
Epoch [106/120    avg_loss:0.052, val_acc:0.957]
Epoch [107/120    avg_loss:0.051, val_acc:0.961]
Epoch [108/120    avg_loss:0.044, val_acc:0.959]
Epoch [109/120    avg_loss:0.046, val_acc:0.957]
Epoch [110/120    avg_loss:0.046, val_acc:0.962]
Epoch [111/120    avg_loss:0.046, val_acc:0.959]
Epoch [112/120    avg_loss:0.045, val_acc:0.959]
Epoch [113/120    avg_loss:0.052, val_acc:0.957]
Epoch [114/120    avg_loss:0.045, val_acc:0.959]
Epoch [115/120    avg_loss:0.048, val_acc:0.956]
Epoch [116/120    avg_loss:0.043, val_acc:0.957]
Epoch [117/120    avg_loss:0.048, val_acc:0.958]
Epoch [118/120    avg_loss:0.043, val_acc:0.959]
Epoch [119/120    avg_loss:0.050, val_acc:0.961]
Epoch [120/120    avg_loss:0.044, val_acc:0.963]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    0    0    0    0    0    0    0    0    6    0    0    0
     0    0    0]
 [   0    0 1224    1    0    0    6    0    0    0    4   45    4    0
     0    1    0]
 [   0    0    1  720    1    0    0    0    0   12    0    0   12    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  410    0    4    0    5    0    0    0    0
    16    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   53   89    0   10    1    0    0    0  700   10    9    0
     2    1    0]
 [   0    1   17    0    0    0    6    0    1    0    1 2176    1    2
     5    0    0]
 [   0    0    0   35   14   11    0    0    0    3   11    0  455    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    4    2    0    0
  1132    0    0]
 [   0    0    0    0    0    0   46    0    0    0    0    0    0    1
    72  228    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
94.13550135501355

F1 scores:
[       nan 0.90909091 0.94883721 0.90452261 0.96598639 0.94688222
 0.95474453 0.92592593 0.99767981 0.59259259 0.87445347 0.9788574
 0.89215686 0.98930481 0.95688926 0.79029463 0.95294118]

Kappa:
0.9330610337013547
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbdf2055b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.764, val_acc:0.319]
Epoch [2/120    avg_loss:2.630, val_acc:0.287]
Epoch [3/120    avg_loss:2.496, val_acc:0.291]
Epoch [4/120    avg_loss:2.370, val_acc:0.285]
Epoch [5/120    avg_loss:2.269, val_acc:0.302]
Epoch [6/120    avg_loss:2.158, val_acc:0.402]
Epoch [7/120    avg_loss:2.069, val_acc:0.435]
Epoch [8/120    avg_loss:2.022, val_acc:0.471]
Epoch [9/120    avg_loss:1.947, val_acc:0.560]
Epoch [10/120    avg_loss:1.847, val_acc:0.552]
Epoch [11/120    avg_loss:1.761, val_acc:0.559]
Epoch [12/120    avg_loss:1.695, val_acc:0.578]
Epoch [13/120    avg_loss:1.589, val_acc:0.589]
Epoch [14/120    avg_loss:1.474, val_acc:0.611]
Epoch [15/120    avg_loss:1.345, val_acc:0.659]
Epoch [16/120    avg_loss:1.293, val_acc:0.675]
Epoch [17/120    avg_loss:1.209, val_acc:0.690]
Epoch [18/120    avg_loss:1.117, val_acc:0.673]
Epoch [19/120    avg_loss:1.080, val_acc:0.718]
Epoch [20/120    avg_loss:0.978, val_acc:0.732]
Epoch [21/120    avg_loss:0.930, val_acc:0.737]
Epoch [22/120    avg_loss:0.807, val_acc:0.773]
Epoch [23/120    avg_loss:0.790, val_acc:0.766]
Epoch [24/120    avg_loss:0.740, val_acc:0.761]
Epoch [25/120    avg_loss:0.668, val_acc:0.788]
Epoch [26/120    avg_loss:0.627, val_acc:0.820]
Epoch [27/120    avg_loss:0.655, val_acc:0.783]
Epoch [28/120    avg_loss:0.645, val_acc:0.817]
Epoch [29/120    avg_loss:0.753, val_acc:0.751]
Epoch [30/120    avg_loss:0.635, val_acc:0.797]
Epoch [31/120    avg_loss:0.657, val_acc:0.836]
Epoch [32/120    avg_loss:0.543, val_acc:0.837]
Epoch [33/120    avg_loss:0.464, val_acc:0.825]
Epoch [34/120    avg_loss:0.396, val_acc:0.865]
Epoch [35/120    avg_loss:0.370, val_acc:0.854]
Epoch [36/120    avg_loss:0.344, val_acc:0.863]
Epoch [37/120    avg_loss:0.294, val_acc:0.896]
Epoch [38/120    avg_loss:0.280, val_acc:0.899]
Epoch [39/120    avg_loss:0.285, val_acc:0.919]
Epoch [40/120    avg_loss:0.245, val_acc:0.897]
Epoch [41/120    avg_loss:0.261, val_acc:0.908]
Epoch [42/120    avg_loss:0.250, val_acc:0.908]
Epoch [43/120    avg_loss:0.218, val_acc:0.918]
Epoch [44/120    avg_loss:0.211, val_acc:0.908]
Epoch [45/120    avg_loss:0.278, val_acc:0.849]
Epoch [46/120    avg_loss:0.305, val_acc:0.899]
Epoch [47/120    avg_loss:0.248, val_acc:0.921]
Epoch [48/120    avg_loss:0.201, val_acc:0.917]
Epoch [49/120    avg_loss:0.199, val_acc:0.908]
Epoch [50/120    avg_loss:0.171, val_acc:0.927]
Epoch [51/120    avg_loss:0.148, val_acc:0.914]
Epoch [52/120    avg_loss:0.149, val_acc:0.935]
Epoch [53/120    avg_loss:0.160, val_acc:0.930]
Epoch [54/120    avg_loss:0.120, val_acc:0.935]
Epoch [55/120    avg_loss:0.120, val_acc:0.939]
Epoch [56/120    avg_loss:0.115, val_acc:0.939]
Epoch [57/120    avg_loss:0.134, val_acc:0.918]
Epoch [58/120    avg_loss:0.122, val_acc:0.945]
Epoch [59/120    avg_loss:0.101, val_acc:0.948]
Epoch [60/120    avg_loss:0.109, val_acc:0.950]
Epoch [61/120    avg_loss:0.139, val_acc:0.932]
Epoch [62/120    avg_loss:0.120, val_acc:0.914]
Epoch [63/120    avg_loss:0.125, val_acc:0.945]
Epoch [64/120    avg_loss:0.113, val_acc:0.935]
Epoch [65/120    avg_loss:0.107, val_acc:0.931]
Epoch [66/120    avg_loss:0.119, val_acc:0.946]
Epoch [67/120    avg_loss:0.071, val_acc:0.959]
Epoch [68/120    avg_loss:0.067, val_acc:0.946]
Epoch [69/120    avg_loss:0.077, val_acc:0.956]
Epoch [70/120    avg_loss:0.072, val_acc:0.958]
Epoch [71/120    avg_loss:0.063, val_acc:0.958]
Epoch [72/120    avg_loss:0.058, val_acc:0.957]
Epoch [73/120    avg_loss:0.066, val_acc:0.948]
Epoch [74/120    avg_loss:0.063, val_acc:0.943]
Epoch [75/120    avg_loss:0.063, val_acc:0.959]
Epoch [76/120    avg_loss:0.050, val_acc:0.957]
Epoch [77/120    avg_loss:0.044, val_acc:0.957]
Epoch [78/120    avg_loss:0.163, val_acc:0.857]
Epoch [79/120    avg_loss:0.436, val_acc:0.893]
Epoch [80/120    avg_loss:0.284, val_acc:0.939]
Epoch [81/120    avg_loss:0.159, val_acc:0.901]
Epoch [82/120    avg_loss:0.152, val_acc:0.931]
Epoch [83/120    avg_loss:0.180, val_acc:0.938]
Epoch [84/120    avg_loss:0.132, val_acc:0.946]
Epoch [85/120    avg_loss:0.131, val_acc:0.941]
Epoch [86/120    avg_loss:0.105, val_acc:0.954]
Epoch [87/120    avg_loss:0.084, val_acc:0.954]
Epoch [88/120    avg_loss:0.094, val_acc:0.952]
Epoch [89/120    avg_loss:0.072, val_acc:0.955]
Epoch [90/120    avg_loss:0.059, val_acc:0.962]
Epoch [91/120    avg_loss:0.044, val_acc:0.965]
Epoch [92/120    avg_loss:0.049, val_acc:0.963]
Epoch [93/120    avg_loss:0.048, val_acc:0.966]
Epoch [94/120    avg_loss:0.045, val_acc:0.966]
Epoch [95/120    avg_loss:0.049, val_acc:0.964]
Epoch [96/120    avg_loss:0.050, val_acc:0.965]
Epoch [97/120    avg_loss:0.043, val_acc:0.965]
Epoch [98/120    avg_loss:0.044, val_acc:0.964]
Epoch [99/120    avg_loss:0.046, val_acc:0.964]
Epoch [100/120    avg_loss:0.046, val_acc:0.967]
Epoch [101/120    avg_loss:0.044, val_acc:0.966]
Epoch [102/120    avg_loss:0.039, val_acc:0.965]
Epoch [103/120    avg_loss:0.047, val_acc:0.965]
Epoch [104/120    avg_loss:0.042, val_acc:0.964]
Epoch [105/120    avg_loss:0.039, val_acc:0.965]
Epoch [106/120    avg_loss:0.034, val_acc:0.965]
Epoch [107/120    avg_loss:0.039, val_acc:0.966]
Epoch [108/120    avg_loss:0.039, val_acc:0.968]
Epoch [109/120    avg_loss:0.041, val_acc:0.968]
Epoch [110/120    avg_loss:0.034, val_acc:0.964]
Epoch [111/120    avg_loss:0.037, val_acc:0.966]
Epoch [112/120    avg_loss:0.036, val_acc:0.967]
Epoch [113/120    avg_loss:0.036, val_acc:0.965]
Epoch [114/120    avg_loss:0.035, val_acc:0.969]
Epoch [115/120    avg_loss:0.043, val_acc:0.969]
Epoch [116/120    avg_loss:0.037, val_acc:0.973]
Epoch [117/120    avg_loss:0.030, val_acc:0.969]
Epoch [118/120    avg_loss:0.036, val_acc:0.969]
Epoch [119/120    avg_loss:0.035, val_acc:0.970]
Epoch [120/120    avg_loss:0.033, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1188    0    6    3    2    0    0    0    9   74    0    0
     0    3    0]
 [   0    0    3  723    0    8    0    0    0    3    0    0   10    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    1    2    0    3    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  653    0    0    1    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   14    0    0    0    0    0    0  416    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    1    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   24   85    0    1    5    0    0    0  749    4    2    0
     0    5    0]
 [   0    0    4    0    0    0    5    0    0    0    6 2190    3    2
     0    0    0]
 [   0    0    0   38    0    5    0    0    0    0   11    0  474    0
     0    1    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    2    0    0    0
  1135    0    0]
 [   0    0    1    0    0    0   44    0    0    6    0    0    0    0
    92  204    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.48238482384824

F1 scores:
[       nan 0.80434783 0.94850299 0.90715182 0.98611111 0.96810934
 0.95467836 0.96153846 0.98229044 0.68085106 0.90458937 0.97767857
 0.92578125 0.99462366 0.95740194 0.72857143 0.96511628]

Kappa:
0.936998406741058
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd8ababaac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.764, val_acc:0.295]
Epoch [2/120    avg_loss:2.637, val_acc:0.349]
Epoch [3/120    avg_loss:2.510, val_acc:0.466]
Epoch [4/120    avg_loss:2.409, val_acc:0.514]
Epoch [5/120    avg_loss:2.297, val_acc:0.550]
Epoch [6/120    avg_loss:2.213, val_acc:0.519]
Epoch [7/120    avg_loss:2.108, val_acc:0.552]
Epoch [8/120    avg_loss:1.999, val_acc:0.572]
Epoch [9/120    avg_loss:1.926, val_acc:0.569]
Epoch [10/120    avg_loss:1.830, val_acc:0.592]
Epoch [11/120    avg_loss:1.738, val_acc:0.589]
Epoch [12/120    avg_loss:1.630, val_acc:0.620]
Epoch [13/120    avg_loss:1.505, val_acc:0.640]
Epoch [14/120    avg_loss:1.402, val_acc:0.651]
Epoch [15/120    avg_loss:1.272, val_acc:0.678]
Epoch [16/120    avg_loss:1.147, val_acc:0.693]
Epoch [17/120    avg_loss:1.087, val_acc:0.706]
Epoch [18/120    avg_loss:0.991, val_acc:0.715]
Epoch [19/120    avg_loss:0.998, val_acc:0.738]
Epoch [20/120    avg_loss:0.871, val_acc:0.748]
Epoch [21/120    avg_loss:0.784, val_acc:0.742]
Epoch [22/120    avg_loss:0.756, val_acc:0.775]
Epoch [23/120    avg_loss:0.699, val_acc:0.767]
Epoch [24/120    avg_loss:0.636, val_acc:0.776]
Epoch [25/120    avg_loss:0.637, val_acc:0.789]
Epoch [26/120    avg_loss:0.537, val_acc:0.801]
Epoch [27/120    avg_loss:0.530, val_acc:0.814]
Epoch [28/120    avg_loss:0.596, val_acc:0.748]
Epoch [29/120    avg_loss:0.506, val_acc:0.816]
Epoch [30/120    avg_loss:0.416, val_acc:0.831]
Epoch [31/120    avg_loss:0.416, val_acc:0.812]
Epoch [32/120    avg_loss:0.383, val_acc:0.820]
Epoch [33/120    avg_loss:0.435, val_acc:0.818]
Epoch [34/120    avg_loss:0.390, val_acc:0.831]
Epoch [35/120    avg_loss:0.322, val_acc:0.867]
Epoch [36/120    avg_loss:0.339, val_acc:0.849]
Epoch [37/120    avg_loss:0.299, val_acc:0.864]
Epoch [38/120    avg_loss:0.303, val_acc:0.861]
Epoch [39/120    avg_loss:0.281, val_acc:0.880]
Epoch [40/120    avg_loss:0.247, val_acc:0.878]
Epoch [41/120    avg_loss:0.233, val_acc:0.890]
Epoch [42/120    avg_loss:0.229, val_acc:0.875]
Epoch [43/120    avg_loss:0.280, val_acc:0.875]
Epoch [44/120    avg_loss:0.244, val_acc:0.896]
Epoch [45/120    avg_loss:0.239, val_acc:0.885]
Epoch [46/120    avg_loss:0.195, val_acc:0.896]
Epoch [47/120    avg_loss:0.217, val_acc:0.886]
Epoch [48/120    avg_loss:0.256, val_acc:0.896]
Epoch [49/120    avg_loss:0.191, val_acc:0.904]
Epoch [50/120    avg_loss:0.242, val_acc:0.884]
Epoch [51/120    avg_loss:0.212, val_acc:0.896]
Epoch [52/120    avg_loss:0.169, val_acc:0.910]
Epoch [53/120    avg_loss:0.132, val_acc:0.919]
Epoch [54/120    avg_loss:0.151, val_acc:0.905]
Epoch [55/120    avg_loss:0.162, val_acc:0.912]
Epoch [56/120    avg_loss:0.165, val_acc:0.902]
Epoch [57/120    avg_loss:0.154, val_acc:0.923]
Epoch [58/120    avg_loss:0.126, val_acc:0.925]
Epoch [59/120    avg_loss:0.120, val_acc:0.909]
Epoch [60/120    avg_loss:0.165, val_acc:0.899]
Epoch [61/120    avg_loss:0.114, val_acc:0.932]
Epoch [62/120    avg_loss:0.120, val_acc:0.909]
Epoch [63/120    avg_loss:0.171, val_acc:0.918]
Epoch [64/120    avg_loss:0.151, val_acc:0.892]
Epoch [65/120    avg_loss:0.179, val_acc:0.896]
Epoch [66/120    avg_loss:0.149, val_acc:0.916]
Epoch [67/120    avg_loss:0.139, val_acc:0.925]
Epoch [68/120    avg_loss:0.118, val_acc:0.928]
Epoch [69/120    avg_loss:0.088, val_acc:0.946]
Epoch [70/120    avg_loss:0.095, val_acc:0.909]
Epoch [71/120    avg_loss:0.107, val_acc:0.931]
Epoch [72/120    avg_loss:0.102, val_acc:0.922]
Epoch [73/120    avg_loss:0.065, val_acc:0.889]
Epoch [74/120    avg_loss:0.096, val_acc:0.914]
Epoch [75/120    avg_loss:0.126, val_acc:0.907]
Epoch [76/120    avg_loss:0.181, val_acc:0.896]
Epoch [77/120    avg_loss:0.149, val_acc:0.902]
Epoch [78/120    avg_loss:0.120, val_acc:0.928]
Epoch [79/120    avg_loss:0.097, val_acc:0.932]
Epoch [80/120    avg_loss:0.095, val_acc:0.930]
Epoch [81/120    avg_loss:0.086, val_acc:0.943]
Epoch [82/120    avg_loss:0.116, val_acc:0.936]
Epoch [83/120    avg_loss:0.079, val_acc:0.941]
Epoch [84/120    avg_loss:0.067, val_acc:0.946]
Epoch [85/120    avg_loss:0.060, val_acc:0.947]
Epoch [86/120    avg_loss:0.044, val_acc:0.950]
Epoch [87/120    avg_loss:0.051, val_acc:0.952]
Epoch [88/120    avg_loss:0.050, val_acc:0.955]
Epoch [89/120    avg_loss:0.049, val_acc:0.947]
Epoch [90/120    avg_loss:0.050, val_acc:0.948]
Epoch [91/120    avg_loss:0.048, val_acc:0.953]
Epoch [92/120    avg_loss:0.046, val_acc:0.950]
Epoch [93/120    avg_loss:0.042, val_acc:0.952]
Epoch [94/120    avg_loss:0.037, val_acc:0.956]
Epoch [95/120    avg_loss:0.047, val_acc:0.953]
Epoch [96/120    avg_loss:0.048, val_acc:0.955]
Epoch [97/120    avg_loss:0.046, val_acc:0.950]
Epoch [98/120    avg_loss:0.042, val_acc:0.954]
Epoch [99/120    avg_loss:0.037, val_acc:0.955]
Epoch [100/120    avg_loss:0.046, val_acc:0.956]
Epoch [101/120    avg_loss:0.046, val_acc:0.953]
Epoch [102/120    avg_loss:0.043, val_acc:0.957]
Epoch [103/120    avg_loss:0.038, val_acc:0.957]
Epoch [104/120    avg_loss:0.040, val_acc:0.957]
Epoch [105/120    avg_loss:0.045, val_acc:0.958]
Epoch [106/120    avg_loss:0.039, val_acc:0.957]
Epoch [107/120    avg_loss:0.041, val_acc:0.962]
Epoch [108/120    avg_loss:0.038, val_acc:0.958]
Epoch [109/120    avg_loss:0.043, val_acc:0.957]
Epoch [110/120    avg_loss:0.044, val_acc:0.958]
Epoch [111/120    avg_loss:0.041, val_acc:0.962]
Epoch [112/120    avg_loss:0.041, val_acc:0.961]
Epoch [113/120    avg_loss:0.040, val_acc:0.958]
Epoch [114/120    avg_loss:0.039, val_acc:0.958]
Epoch [115/120    avg_loss:0.036, val_acc:0.958]
Epoch [116/120    avg_loss:0.036, val_acc:0.958]
Epoch [117/120    avg_loss:0.035, val_acc:0.958]
Epoch [118/120    avg_loss:0.037, val_acc:0.957]
Epoch [119/120    avg_loss:0.036, val_acc:0.957]
Epoch [120/120    avg_loss:0.035, val_acc:0.957]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1191    3    0    3    6    0    0    0    7   74    0    0
     0    1    0]
 [   0    0    3  720    0   10    0    0    0   11    0    0    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  419    0    0    0    7    0    0    0    0
     9    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    1    0    0
     6    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    4    0    0   11    0    0    0    0
     0    0    0]
 [   0    0   45   90    0    7    0    0    0    0  720   12    0    0
     0    1    0]
 [   0    0   15    0    0    1   13    0    0    0    3 2168    5    3
     2    0    0]
 [   0    0    0   22    1    8    0    0    0    0    9    6  480    0
     0    1    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    1    0    0    0    0    0
  1134    2    0]
 [   0    0    0    0    0    0    0    0    0    3    0    0    0    0
   105  239    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.39566395663957

F1 scores:
[       nan 0.98765432 0.93816463 0.90851735 0.99765808 0.94689266
 0.97744361 1.         0.99883856 0.44       0.89164087 0.96980541
 0.93841642 0.9919571  0.94697286 0.80879865 0.95402299]

Kappa:
0.9360067443677998
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faeb9878b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.799, val_acc:0.355]
Epoch [2/120    avg_loss:2.664, val_acc:0.470]
Epoch [3/120    avg_loss:2.547, val_acc:0.467]
Epoch [4/120    avg_loss:2.430, val_acc:0.498]
Epoch [5/120    avg_loss:2.301, val_acc:0.532]
Epoch [6/120    avg_loss:2.206, val_acc:0.551]
Epoch [7/120    avg_loss:2.115, val_acc:0.568]
Epoch [8/120    avg_loss:2.078, val_acc:0.562]
Epoch [9/120    avg_loss:1.971, val_acc:0.616]
Epoch [10/120    avg_loss:1.932, val_acc:0.640]
Epoch [11/120    avg_loss:1.830, val_acc:0.649]
Epoch [12/120    avg_loss:1.750, val_acc:0.651]
Epoch [13/120    avg_loss:1.635, val_acc:0.671]
Epoch [14/120    avg_loss:1.512, val_acc:0.682]
Epoch [15/120    avg_loss:1.401, val_acc:0.695]
Epoch [16/120    avg_loss:1.348, val_acc:0.699]
Epoch [17/120    avg_loss:1.299, val_acc:0.677]
Epoch [18/120    avg_loss:1.158, val_acc:0.738]
Epoch [19/120    avg_loss:1.063, val_acc:0.743]
Epoch [20/120    avg_loss:0.949, val_acc:0.762]
Epoch [21/120    avg_loss:0.900, val_acc:0.778]
Epoch [22/120    avg_loss:0.772, val_acc:0.807]
Epoch [23/120    avg_loss:0.745, val_acc:0.800]
Epoch [24/120    avg_loss:0.679, val_acc:0.779]
Epoch [25/120    avg_loss:0.654, val_acc:0.804]
Epoch [26/120    avg_loss:0.625, val_acc:0.830]
Epoch [27/120    avg_loss:0.543, val_acc:0.793]
Epoch [28/120    avg_loss:0.606, val_acc:0.816]
Epoch [29/120    avg_loss:0.516, val_acc:0.829]
Epoch [30/120    avg_loss:0.443, val_acc:0.869]
Epoch [31/120    avg_loss:0.511, val_acc:0.838]
Epoch [32/120    avg_loss:0.470, val_acc:0.845]
Epoch [33/120    avg_loss:0.400, val_acc:0.865]
Epoch [34/120    avg_loss:0.347, val_acc:0.866]
Epoch [35/120    avg_loss:0.304, val_acc:0.866]
Epoch [36/120    avg_loss:0.302, val_acc:0.887]
Epoch [37/120    avg_loss:0.303, val_acc:0.889]
Epoch [38/120    avg_loss:0.275, val_acc:0.861]
Epoch [39/120    avg_loss:0.293, val_acc:0.907]
Epoch [40/120    avg_loss:0.284, val_acc:0.909]
Epoch [41/120    avg_loss:0.232, val_acc:0.905]
Epoch [42/120    avg_loss:0.246, val_acc:0.899]
Epoch [43/120    avg_loss:0.236, val_acc:0.901]
Epoch [44/120    avg_loss:0.214, val_acc:0.874]
Epoch [45/120    avg_loss:0.264, val_acc:0.918]
Epoch [46/120    avg_loss:0.254, val_acc:0.898]
Epoch [47/120    avg_loss:0.251, val_acc:0.890]
Epoch [48/120    avg_loss:0.243, val_acc:0.908]
Epoch [49/120    avg_loss:0.188, val_acc:0.909]
Epoch [50/120    avg_loss:0.185, val_acc:0.923]
Epoch [51/120    avg_loss:0.230, val_acc:0.887]
Epoch [52/120    avg_loss:0.225, val_acc:0.908]
Epoch [53/120    avg_loss:0.196, val_acc:0.896]
Epoch [54/120    avg_loss:0.179, val_acc:0.908]
Epoch [55/120    avg_loss:0.157, val_acc:0.909]
Epoch [56/120    avg_loss:0.144, val_acc:0.928]
Epoch [57/120    avg_loss:0.136, val_acc:0.918]
Epoch [58/120    avg_loss:0.164, val_acc:0.925]
Epoch [59/120    avg_loss:0.166, val_acc:0.903]
Epoch [60/120    avg_loss:0.174, val_acc:0.922]
Epoch [61/120    avg_loss:0.147, val_acc:0.932]
Epoch [62/120    avg_loss:0.156, val_acc:0.941]
Epoch [63/120    avg_loss:0.146, val_acc:0.914]
Epoch [64/120    avg_loss:0.123, val_acc:0.934]
Epoch [65/120    avg_loss:0.110, val_acc:0.948]
Epoch [66/120    avg_loss:0.118, val_acc:0.934]
Epoch [67/120    avg_loss:0.126, val_acc:0.941]
Epoch [68/120    avg_loss:0.102, val_acc:0.935]
Epoch [69/120    avg_loss:0.094, val_acc:0.945]
Epoch [70/120    avg_loss:0.124, val_acc:0.931]
Epoch [71/120    avg_loss:0.139, val_acc:0.945]
Epoch [72/120    avg_loss:0.120, val_acc:0.935]
Epoch [73/120    avg_loss:0.104, val_acc:0.935]
Epoch [74/120    avg_loss:0.096, val_acc:0.930]
Epoch [75/120    avg_loss:0.083, val_acc:0.943]
Epoch [76/120    avg_loss:0.094, val_acc:0.946]
Epoch [77/120    avg_loss:0.076, val_acc:0.957]
Epoch [78/120    avg_loss:0.069, val_acc:0.950]
Epoch [79/120    avg_loss:0.066, val_acc:0.950]
Epoch [80/120    avg_loss:0.067, val_acc:0.959]
Epoch [81/120    avg_loss:0.055, val_acc:0.952]
Epoch [82/120    avg_loss:0.088, val_acc:0.944]
Epoch [83/120    avg_loss:0.082, val_acc:0.946]
Epoch [84/120    avg_loss:0.062, val_acc:0.932]
Epoch [85/120    avg_loss:0.074, val_acc:0.957]
Epoch [86/120    avg_loss:0.082, val_acc:0.946]
Epoch [87/120    avg_loss:0.064, val_acc:0.939]
Epoch [88/120    avg_loss:0.075, val_acc:0.931]
Epoch [89/120    avg_loss:0.062, val_acc:0.940]
Epoch [90/120    avg_loss:0.060, val_acc:0.963]
Epoch [91/120    avg_loss:0.070, val_acc:0.957]
Epoch [92/120    avg_loss:0.056, val_acc:0.961]
Epoch [93/120    avg_loss:0.046, val_acc:0.950]
Epoch [94/120    avg_loss:0.061, val_acc:0.955]
Epoch [95/120    avg_loss:0.048, val_acc:0.959]
Epoch [96/120    avg_loss:0.062, val_acc:0.955]
Epoch [97/120    avg_loss:0.049, val_acc:0.957]
Epoch [98/120    avg_loss:0.056, val_acc:0.952]
Epoch [99/120    avg_loss:0.035, val_acc:0.956]
Epoch [100/120    avg_loss:0.034, val_acc:0.961]
Epoch [101/120    avg_loss:0.041, val_acc:0.962]
Epoch [102/120    avg_loss:0.039, val_acc:0.948]
Epoch [103/120    avg_loss:0.046, val_acc:0.956]
Epoch [104/120    avg_loss:0.034, val_acc:0.966]
Epoch [105/120    avg_loss:0.034, val_acc:0.966]
Epoch [106/120    avg_loss:0.024, val_acc:0.971]
Epoch [107/120    avg_loss:0.023, val_acc:0.974]
Epoch [108/120    avg_loss:0.025, val_acc:0.977]
Epoch [109/120    avg_loss:0.028, val_acc:0.974]
Epoch [110/120    avg_loss:0.022, val_acc:0.974]
Epoch [111/120    avg_loss:0.023, val_acc:0.976]
Epoch [112/120    avg_loss:0.021, val_acc:0.977]
Epoch [113/120    avg_loss:0.026, val_acc:0.976]
Epoch [114/120    avg_loss:0.022, val_acc:0.974]
Epoch [115/120    avg_loss:0.026, val_acc:0.976]
Epoch [116/120    avg_loss:0.024, val_acc:0.975]
Epoch [117/120    avg_loss:0.021, val_acc:0.974]
Epoch [118/120    avg_loss:0.021, val_acc:0.975]
Epoch [119/120    avg_loss:0.021, val_acc:0.975]
Epoch [120/120    avg_loss:0.022, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1240    2    0    0    4    0    0    0    2   33    1    0
     0    3    0]
 [   0    0    1  709    2   22    0    0    0    5    0    0    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    0    0    4    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   15    0    0    1    0
     0    0    0]
 [   0    0   13   89    0    4    1    0    0    0  746   11    0    0
     0   11    0]
 [   0    0   30    0    0    0   12    0    0    0    2 2160    1    2
     3    0    0]
 [   0    0    0   26    6    8    0    0    0    0   11    0  478    0
     1    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    2    0    0
  1135    0    0]
 [   0    0    0    0    0    0   37    0    0    0    0    0    0    0
   111  199    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.67750677506776

F1 scores:
[       nan 0.94871795 0.96535617 0.90031746 0.98156682 0.94960806
 0.95900439 1.         0.99883856 0.71428571 0.90920171 0.97803939
 0.93359375 0.99462366 0.9470171  0.70944742 0.97647059]

Kappa:
0.9392618697470682
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9db9347ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.785, val_acc:0.154]
Epoch [2/120    avg_loss:2.650, val_acc:0.215]
Epoch [3/120    avg_loss:2.493, val_acc:0.309]
Epoch [4/120    avg_loss:2.390, val_acc:0.338]
Epoch [5/120    avg_loss:2.309, val_acc:0.354]
Epoch [6/120    avg_loss:2.230, val_acc:0.378]
Epoch [7/120    avg_loss:2.134, val_acc:0.438]
Epoch [8/120    avg_loss:2.048, val_acc:0.490]
Epoch [9/120    avg_loss:1.957, val_acc:0.517]
Epoch [10/120    avg_loss:1.891, val_acc:0.551]
Epoch [11/120    avg_loss:1.771, val_acc:0.582]
Epoch [12/120    avg_loss:1.708, val_acc:0.627]
Epoch [13/120    avg_loss:1.575, val_acc:0.635]
Epoch [14/120    avg_loss:1.474, val_acc:0.657]
Epoch [15/120    avg_loss:1.395, val_acc:0.697]
Epoch [16/120    avg_loss:1.224, val_acc:0.711]
Epoch [17/120    avg_loss:1.161, val_acc:0.714]
Epoch [18/120    avg_loss:1.126, val_acc:0.734]
Epoch [19/120    avg_loss:0.957, val_acc:0.767]
Epoch [20/120    avg_loss:0.875, val_acc:0.750]
Epoch [21/120    avg_loss:0.857, val_acc:0.778]
Epoch [22/120    avg_loss:0.759, val_acc:0.797]
Epoch [23/120    avg_loss:0.756, val_acc:0.807]
Epoch [24/120    avg_loss:0.684, val_acc:0.834]
Epoch [25/120    avg_loss:0.600, val_acc:0.824]
Epoch [26/120    avg_loss:0.546, val_acc:0.867]
Epoch [27/120    avg_loss:0.568, val_acc:0.860]
Epoch [28/120    avg_loss:0.544, val_acc:0.866]
Epoch [29/120    avg_loss:0.558, val_acc:0.841]
Epoch [30/120    avg_loss:0.486, val_acc:0.829]
Epoch [31/120    avg_loss:0.478, val_acc:0.854]
Epoch [32/120    avg_loss:0.533, val_acc:0.825]
Epoch [33/120    avg_loss:0.510, val_acc:0.845]
Epoch [34/120    avg_loss:0.406, val_acc:0.887]
Epoch [35/120    avg_loss:0.356, val_acc:0.843]
Epoch [36/120    avg_loss:0.367, val_acc:0.863]
Epoch [37/120    avg_loss:0.396, val_acc:0.892]
Epoch [38/120    avg_loss:0.341, val_acc:0.911]
Epoch [39/120    avg_loss:0.292, val_acc:0.899]
Epoch [40/120    avg_loss:0.298, val_acc:0.906]
Epoch [41/120    avg_loss:0.263, val_acc:0.919]
Epoch [42/120    avg_loss:0.262, val_acc:0.902]
Epoch [43/120    avg_loss:0.220, val_acc:0.924]
Epoch [44/120    avg_loss:0.214, val_acc:0.902]
Epoch [45/120    avg_loss:0.187, val_acc:0.929]
Epoch [46/120    avg_loss:0.205, val_acc:0.931]
Epoch [47/120    avg_loss:0.193, val_acc:0.929]
Epoch [48/120    avg_loss:0.227, val_acc:0.917]
Epoch [49/120    avg_loss:0.212, val_acc:0.918]
Epoch [50/120    avg_loss:0.176, val_acc:0.940]
Epoch [51/120    avg_loss:0.179, val_acc:0.936]
Epoch [52/120    avg_loss:0.157, val_acc:0.943]
Epoch [53/120    avg_loss:0.219, val_acc:0.932]
Epoch [54/120    avg_loss:0.149, val_acc:0.925]
Epoch [55/120    avg_loss:0.133, val_acc:0.925]
Epoch [56/120    avg_loss:0.157, val_acc:0.943]
Epoch [57/120    avg_loss:0.130, val_acc:0.946]
Epoch [58/120    avg_loss:0.111, val_acc:0.955]
Epoch [59/120    avg_loss:0.223, val_acc:0.878]
Epoch [60/120    avg_loss:0.193, val_acc:0.927]
Epoch [61/120    avg_loss:0.202, val_acc:0.943]
Epoch [62/120    avg_loss:0.131, val_acc:0.943]
Epoch [63/120    avg_loss:0.139, val_acc:0.950]
Epoch [64/120    avg_loss:0.154, val_acc:0.931]
Epoch [65/120    avg_loss:0.137, val_acc:0.943]
Epoch [66/120    avg_loss:0.107, val_acc:0.964]
Epoch [67/120    avg_loss:0.124, val_acc:0.947]
Epoch [68/120    avg_loss:0.114, val_acc:0.944]
Epoch [69/120    avg_loss:0.114, val_acc:0.950]
Epoch [70/120    avg_loss:0.113, val_acc:0.954]
Epoch [71/120    avg_loss:0.089, val_acc:0.957]
Epoch [72/120    avg_loss:0.072, val_acc:0.969]
Epoch [73/120    avg_loss:0.065, val_acc:0.955]
Epoch [74/120    avg_loss:0.058, val_acc:0.959]
Epoch [75/120    avg_loss:0.083, val_acc:0.958]
Epoch [76/120    avg_loss:0.086, val_acc:0.965]
Epoch [77/120    avg_loss:0.094, val_acc:0.957]
Epoch [78/120    avg_loss:0.107, val_acc:0.964]
Epoch [79/120    avg_loss:0.066, val_acc:0.953]
Epoch [80/120    avg_loss:0.061, val_acc:0.944]
Epoch [81/120    avg_loss:0.061, val_acc:0.959]
Epoch [82/120    avg_loss:0.077, val_acc:0.963]
Epoch [83/120    avg_loss:0.055, val_acc:0.966]
Epoch [84/120    avg_loss:0.080, val_acc:0.971]
Epoch [85/120    avg_loss:0.063, val_acc:0.966]
Epoch [86/120    avg_loss:0.061, val_acc:0.968]
Epoch [87/120    avg_loss:0.061, val_acc:0.959]
Epoch [88/120    avg_loss:0.079, val_acc:0.959]
Epoch [89/120    avg_loss:0.069, val_acc:0.963]
Epoch [90/120    avg_loss:0.057, val_acc:0.964]
Epoch [91/120    avg_loss:0.064, val_acc:0.961]
Epoch [92/120    avg_loss:0.054, val_acc:0.972]
Epoch [93/120    avg_loss:0.057, val_acc:0.972]
Epoch [94/120    avg_loss:0.062, val_acc:0.972]
Epoch [95/120    avg_loss:0.038, val_acc:0.968]
Epoch [96/120    avg_loss:0.049, val_acc:0.972]
Epoch [97/120    avg_loss:0.044, val_acc:0.975]
Epoch [98/120    avg_loss:0.049, val_acc:0.965]
Epoch [99/120    avg_loss:0.040, val_acc:0.968]
Epoch [100/120    avg_loss:0.034, val_acc:0.980]
Epoch [101/120    avg_loss:0.029, val_acc:0.977]
Epoch [102/120    avg_loss:0.032, val_acc:0.981]
Epoch [103/120    avg_loss:0.039, val_acc:0.978]
Epoch [104/120    avg_loss:0.038, val_acc:0.980]
Epoch [105/120    avg_loss:0.045, val_acc:0.969]
Epoch [106/120    avg_loss:0.027, val_acc:0.963]
Epoch [107/120    avg_loss:0.034, val_acc:0.970]
Epoch [108/120    avg_loss:0.032, val_acc:0.972]
Epoch [109/120    avg_loss:0.029, val_acc:0.979]
Epoch [110/120    avg_loss:0.030, val_acc:0.978]
Epoch [111/120    avg_loss:0.025, val_acc:0.980]
Epoch [112/120    avg_loss:0.029, val_acc:0.976]
Epoch [113/120    avg_loss:0.029, val_acc:0.968]
Epoch [114/120    avg_loss:0.031, val_acc:0.976]
Epoch [115/120    avg_loss:0.037, val_acc:0.979]
Epoch [116/120    avg_loss:0.026, val_acc:0.981]
Epoch [117/120    avg_loss:0.019, val_acc:0.983]
Epoch [118/120    avg_loss:0.019, val_acc:0.984]
Epoch [119/120    avg_loss:0.020, val_acc:0.985]
Epoch [120/120    avg_loss:0.021, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1232    0    0    0    0    0    0    0    7   37    1    0
     0    8    0]
 [   0    0    2  704    9    7    0    0    0   16    0    0    5    3
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  419    0    0    0    3    0    0    0    0
    13    0    0]
 [   0    0    0    0    0    0  652    0    0    2    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   17   89    0    5    0    0    0    0  752    8    0    0
     1    3    0]
 [   0    0   14    0    0    0    3    0    2    0    2 2183    2    2
     2    0    0]
 [   0    0    0    0    2    8    0    0    0    0   13    2  499    0
     0    0   10]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    3    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0    0    0    0    9    0    0    0    0
   123  215    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.20867208672087

F1 scores:
[       nan 0.98765432 0.96589573 0.91369241 0.97247706 0.956621
 0.99390244 1.         0.99767981 0.52307692 0.91041162 0.98266937
 0.95777351 0.98666667 0.93990883 0.7504363  0.94382022]

Kappa:
0.9453157566609712
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f19f7266b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.812, val_acc:0.193]
Epoch [2/120    avg_loss:2.677, val_acc:0.307]
Epoch [3/120    avg_loss:2.548, val_acc:0.416]
Epoch [4/120    avg_loss:2.463, val_acc:0.458]
Epoch [5/120    avg_loss:2.370, val_acc:0.471]
Epoch [6/120    avg_loss:2.279, val_acc:0.557]
Epoch [7/120    avg_loss:2.149, val_acc:0.583]
Epoch [8/120    avg_loss:2.085, val_acc:0.591]
Epoch [9/120    avg_loss:2.023, val_acc:0.595]
Epoch [10/120    avg_loss:1.929, val_acc:0.601]
Epoch [11/120    avg_loss:1.769, val_acc:0.636]
Epoch [12/120    avg_loss:1.694, val_acc:0.631]
Epoch [13/120    avg_loss:1.604, val_acc:0.654]
Epoch [14/120    avg_loss:1.459, val_acc:0.676]
Epoch [15/120    avg_loss:1.403, val_acc:0.657]
Epoch [16/120    avg_loss:1.276, val_acc:0.681]
Epoch [17/120    avg_loss:1.136, val_acc:0.698]
Epoch [18/120    avg_loss:1.076, val_acc:0.718]
Epoch [19/120    avg_loss:1.006, val_acc:0.730]
Epoch [20/120    avg_loss:0.970, val_acc:0.696]
Epoch [21/120    avg_loss:0.915, val_acc:0.766]
Epoch [22/120    avg_loss:0.773, val_acc:0.765]
Epoch [23/120    avg_loss:0.744, val_acc:0.775]
Epoch [24/120    avg_loss:0.663, val_acc:0.816]
Epoch [25/120    avg_loss:0.613, val_acc:0.789]
Epoch [26/120    avg_loss:0.552, val_acc:0.836]
Epoch [27/120    avg_loss:0.483, val_acc:0.831]
Epoch [28/120    avg_loss:0.470, val_acc:0.851]
Epoch [29/120    avg_loss:0.441, val_acc:0.868]
Epoch [30/120    avg_loss:0.426, val_acc:0.857]
Epoch [31/120    avg_loss:0.443, val_acc:0.823]
Epoch [32/120    avg_loss:0.392, val_acc:0.857]
Epoch [33/120    avg_loss:0.388, val_acc:0.824]
Epoch [34/120    avg_loss:0.367, val_acc:0.852]
Epoch [35/120    avg_loss:0.329, val_acc:0.864]
Epoch [36/120    avg_loss:0.333, val_acc:0.883]
Epoch [37/120    avg_loss:0.291, val_acc:0.882]
Epoch [38/120    avg_loss:0.268, val_acc:0.874]
Epoch [39/120    avg_loss:0.319, val_acc:0.883]
Epoch [40/120    avg_loss:0.297, val_acc:0.874]
Epoch [41/120    avg_loss:0.296, val_acc:0.898]
Epoch [42/120    avg_loss:0.322, val_acc:0.898]
Epoch [43/120    avg_loss:0.241, val_acc:0.891]
Epoch [44/120    avg_loss:0.255, val_acc:0.904]
Epoch [45/120    avg_loss:0.282, val_acc:0.911]
Epoch [46/120    avg_loss:0.184, val_acc:0.900]
Epoch [47/120    avg_loss:0.220, val_acc:0.902]
Epoch [48/120    avg_loss:0.194, val_acc:0.922]
Epoch [49/120    avg_loss:0.183, val_acc:0.895]
Epoch [50/120    avg_loss:0.170, val_acc:0.917]
Epoch [51/120    avg_loss:0.159, val_acc:0.920]
Epoch [52/120    avg_loss:0.147, val_acc:0.926]
Epoch [53/120    avg_loss:0.156, val_acc:0.921]
Epoch [54/120    avg_loss:0.154, val_acc:0.941]
Epoch [55/120    avg_loss:0.153, val_acc:0.927]
Epoch [56/120    avg_loss:0.135, val_acc:0.925]
Epoch [57/120    avg_loss:0.129, val_acc:0.937]
Epoch [58/120    avg_loss:0.119, val_acc:0.934]
Epoch [59/120    avg_loss:0.095, val_acc:0.941]
Epoch [60/120    avg_loss:0.138, val_acc:0.938]
Epoch [61/120    avg_loss:0.090, val_acc:0.935]
Epoch [62/120    avg_loss:0.089, val_acc:0.947]
Epoch [63/120    avg_loss:0.120, val_acc:0.932]
Epoch [64/120    avg_loss:0.132, val_acc:0.891]
Epoch [65/120    avg_loss:0.168, val_acc:0.931]
Epoch [66/120    avg_loss:0.166, val_acc:0.921]
Epoch [67/120    avg_loss:0.172, val_acc:0.925]
Epoch [68/120    avg_loss:0.234, val_acc:0.905]
Epoch [69/120    avg_loss:0.158, val_acc:0.901]
Epoch [70/120    avg_loss:0.161, val_acc:0.912]
Epoch [71/120    avg_loss:0.138, val_acc:0.939]
Epoch [72/120    avg_loss:0.091, val_acc:0.946]
Epoch [73/120    avg_loss:0.113, val_acc:0.936]
Epoch [74/120    avg_loss:0.102, val_acc:0.936]
Epoch [75/120    avg_loss:0.074, val_acc:0.949]
Epoch [76/120    avg_loss:0.067, val_acc:0.946]
Epoch [77/120    avg_loss:0.090, val_acc:0.949]
Epoch [78/120    avg_loss:0.079, val_acc:0.945]
Epoch [79/120    avg_loss:0.086, val_acc:0.958]
Epoch [80/120    avg_loss:0.091, val_acc:0.946]
Epoch [81/120    avg_loss:0.075, val_acc:0.945]
Epoch [82/120    avg_loss:0.069, val_acc:0.956]
Epoch [83/120    avg_loss:0.054, val_acc:0.950]
Epoch [84/120    avg_loss:0.052, val_acc:0.936]
Epoch [85/120    avg_loss:0.070, val_acc:0.943]
Epoch [86/120    avg_loss:0.057, val_acc:0.953]
Epoch [87/120    avg_loss:0.065, val_acc:0.961]
Epoch [88/120    avg_loss:0.081, val_acc:0.944]
Epoch [89/120    avg_loss:0.074, val_acc:0.950]
Epoch [90/120    avg_loss:0.057, val_acc:0.939]
Epoch [91/120    avg_loss:0.052, val_acc:0.959]
Epoch [92/120    avg_loss:0.040, val_acc:0.958]
Epoch [93/120    avg_loss:0.083, val_acc:0.943]
Epoch [94/120    avg_loss:0.084, val_acc:0.938]
Epoch [95/120    avg_loss:0.056, val_acc:0.956]
Epoch [96/120    avg_loss:0.069, val_acc:0.944]
Epoch [97/120    avg_loss:0.047, val_acc:0.953]
Epoch [98/120    avg_loss:0.057, val_acc:0.955]
Epoch [99/120    avg_loss:0.076, val_acc:0.948]
Epoch [100/120    avg_loss:0.062, val_acc:0.954]
Epoch [101/120    avg_loss:0.037, val_acc:0.959]
Epoch [102/120    avg_loss:0.035, val_acc:0.963]
Epoch [103/120    avg_loss:0.030, val_acc:0.963]
Epoch [104/120    avg_loss:0.031, val_acc:0.963]
Epoch [105/120    avg_loss:0.026, val_acc:0.964]
Epoch [106/120    avg_loss:0.036, val_acc:0.965]
Epoch [107/120    avg_loss:0.029, val_acc:0.966]
Epoch [108/120    avg_loss:0.024, val_acc:0.967]
Epoch [109/120    avg_loss:0.025, val_acc:0.964]
Epoch [110/120    avg_loss:0.029, val_acc:0.964]
Epoch [111/120    avg_loss:0.024, val_acc:0.965]
Epoch [112/120    avg_loss:0.027, val_acc:0.967]
Epoch [113/120    avg_loss:0.029, val_acc:0.965]
Epoch [114/120    avg_loss:0.024, val_acc:0.965]
Epoch [115/120    avg_loss:0.034, val_acc:0.965]
Epoch [116/120    avg_loss:0.024, val_acc:0.964]
Epoch [117/120    avg_loss:0.027, val_acc:0.967]
Epoch [118/120    avg_loss:0.026, val_acc:0.967]
Epoch [119/120    avg_loss:0.027, val_acc:0.964]
Epoch [120/120    avg_loss:0.024, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1216   12    1    0    3    0    0    0    3   49    1    0
     0    0    0]
 [   0    0    0  721    2    7    0    0    0    5    0    0   10    1
     0    1    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  420    0    0    0    3    0    0    0    0
    12    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    3    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0   31   90    0    6    0    0    0    0  737    3    0    0
     2    6    0]
 [   0    0   14    0    0    0   10    0    0    0    3 2172    2    4
     5    0    0]
 [   0    0    0   31    3   10    0    0    0    0    9    0  469    0
     0    0   12]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    3    3    0    0
  1127    0    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
   104  241    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.77506775067751

F1 scores:
[       nan 0.96202532 0.95522388 0.90068707 0.9837587  0.95130238
 0.98412698 1.         0.99883856 0.73170732 0.90263319 0.97837838
 0.91960784 0.98666667 0.94230769 0.81008403 0.93333333]

Kappa:
0.9403705246814491
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb211b42a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.834, val_acc:0.178]
Epoch [2/120    avg_loss:2.712, val_acc:0.247]
Epoch [3/120    avg_loss:2.563, val_acc:0.352]
Epoch [4/120    avg_loss:2.445, val_acc:0.404]
Epoch [5/120    avg_loss:2.286, val_acc:0.449]
Epoch [6/120    avg_loss:2.196, val_acc:0.521]
Epoch [7/120    avg_loss:2.082, val_acc:0.581]
Epoch [8/120    avg_loss:1.982, val_acc:0.582]
Epoch [9/120    avg_loss:1.930, val_acc:0.542]
Epoch [10/120    avg_loss:1.836, val_acc:0.610]
Epoch [11/120    avg_loss:1.713, val_acc:0.611]
Epoch [12/120    avg_loss:1.598, val_acc:0.631]
Epoch [13/120    avg_loss:1.479, val_acc:0.667]
Epoch [14/120    avg_loss:1.427, val_acc:0.639]
Epoch [15/120    avg_loss:1.446, val_acc:0.641]
Epoch [16/120    avg_loss:1.349, val_acc:0.666]
Epoch [17/120    avg_loss:1.267, val_acc:0.707]
Epoch [18/120    avg_loss:1.137, val_acc:0.713]
Epoch [19/120    avg_loss:1.112, val_acc:0.734]
Epoch [20/120    avg_loss:1.017, val_acc:0.737]
Epoch [21/120    avg_loss:0.982, val_acc:0.730]
Epoch [22/120    avg_loss:0.908, val_acc:0.772]
Epoch [23/120    avg_loss:0.832, val_acc:0.777]
Epoch [24/120    avg_loss:0.791, val_acc:0.794]
Epoch [25/120    avg_loss:0.778, val_acc:0.748]
Epoch [26/120    avg_loss:0.778, val_acc:0.807]
Epoch [27/120    avg_loss:0.741, val_acc:0.806]
Epoch [28/120    avg_loss:0.630, val_acc:0.766]
Epoch [29/120    avg_loss:0.614, val_acc:0.828]
Epoch [30/120    avg_loss:0.624, val_acc:0.825]
Epoch [31/120    avg_loss:0.545, val_acc:0.843]
Epoch [32/120    avg_loss:0.520, val_acc:0.833]
Epoch [33/120    avg_loss:0.473, val_acc:0.865]
Epoch [34/120    avg_loss:0.431, val_acc:0.816]
Epoch [35/120    avg_loss:0.452, val_acc:0.847]
Epoch [36/120    avg_loss:0.414, val_acc:0.866]
Epoch [37/120    avg_loss:0.394, val_acc:0.866]
Epoch [38/120    avg_loss:0.405, val_acc:0.868]
Epoch [39/120    avg_loss:0.376, val_acc:0.883]
Epoch [40/120    avg_loss:0.427, val_acc:0.823]
Epoch [41/120    avg_loss:0.400, val_acc:0.876]
Epoch [42/120    avg_loss:0.347, val_acc:0.870]
Epoch [43/120    avg_loss:0.286, val_acc:0.867]
Epoch [44/120    avg_loss:0.303, val_acc:0.905]
Epoch [45/120    avg_loss:0.238, val_acc:0.913]
Epoch [46/120    avg_loss:0.249, val_acc:0.896]
Epoch [47/120    avg_loss:0.272, val_acc:0.906]
Epoch [48/120    avg_loss:0.229, val_acc:0.925]
Epoch [49/120    avg_loss:0.198, val_acc:0.920]
Epoch [50/120    avg_loss:0.180, val_acc:0.914]
Epoch [51/120    avg_loss:0.176, val_acc:0.919]
Epoch [52/120    avg_loss:0.255, val_acc:0.924]
Epoch [53/120    avg_loss:0.173, val_acc:0.932]
Epoch [54/120    avg_loss:0.150, val_acc:0.933]
Epoch [55/120    avg_loss:0.140, val_acc:0.921]
Epoch [56/120    avg_loss:0.184, val_acc:0.906]
Epoch [57/120    avg_loss:0.185, val_acc:0.932]
Epoch [58/120    avg_loss:0.155, val_acc:0.943]
Epoch [59/120    avg_loss:0.153, val_acc:0.927]
Epoch [60/120    avg_loss:0.123, val_acc:0.940]
Epoch [61/120    avg_loss:0.147, val_acc:0.935]
Epoch [62/120    avg_loss:0.116, val_acc:0.947]
Epoch [63/120    avg_loss:0.115, val_acc:0.947]
Epoch [64/120    avg_loss:0.133, val_acc:0.955]
Epoch [65/120    avg_loss:0.121, val_acc:0.942]
Epoch [66/120    avg_loss:0.125, val_acc:0.948]
Epoch [67/120    avg_loss:0.099, val_acc:0.950]
Epoch [68/120    avg_loss:0.084, val_acc:0.948]
Epoch [69/120    avg_loss:0.101, val_acc:0.955]
Epoch [70/120    avg_loss:0.090, val_acc:0.946]
Epoch [71/120    avg_loss:0.083, val_acc:0.958]
Epoch [72/120    avg_loss:0.072, val_acc:0.959]
Epoch [73/120    avg_loss:0.068, val_acc:0.935]
Epoch [74/120    avg_loss:0.102, val_acc:0.956]
Epoch [75/120    avg_loss:0.078, val_acc:0.946]
Epoch [76/120    avg_loss:0.063, val_acc:0.964]
Epoch [77/120    avg_loss:0.058, val_acc:0.959]
Epoch [78/120    avg_loss:0.061, val_acc:0.957]
Epoch [79/120    avg_loss:0.063, val_acc:0.955]
Epoch [80/120    avg_loss:0.125, val_acc:0.925]
Epoch [81/120    avg_loss:0.115, val_acc:0.954]
Epoch [82/120    avg_loss:0.075, val_acc:0.951]
Epoch [83/120    avg_loss:0.072, val_acc:0.954]
Epoch [84/120    avg_loss:0.062, val_acc:0.959]
Epoch [85/120    avg_loss:0.083, val_acc:0.950]
Epoch [86/120    avg_loss:0.086, val_acc:0.961]
Epoch [87/120    avg_loss:0.077, val_acc:0.968]
Epoch [88/120    avg_loss:0.060, val_acc:0.971]
Epoch [89/120    avg_loss:0.066, val_acc:0.969]
Epoch [90/120    avg_loss:0.051, val_acc:0.976]
Epoch [91/120    avg_loss:0.045, val_acc:0.972]
Epoch [92/120    avg_loss:0.051, val_acc:0.957]
Epoch [93/120    avg_loss:0.063, val_acc:0.955]
Epoch [94/120    avg_loss:0.064, val_acc:0.963]
Epoch [95/120    avg_loss:0.063, val_acc:0.969]
Epoch [96/120    avg_loss:0.048, val_acc:0.965]
Epoch [97/120    avg_loss:0.039, val_acc:0.968]
Epoch [98/120    avg_loss:0.066, val_acc:0.942]
Epoch [99/120    avg_loss:0.103, val_acc:0.949]
Epoch [100/120    avg_loss:0.066, val_acc:0.965]
Epoch [101/120    avg_loss:0.061, val_acc:0.956]
Epoch [102/120    avg_loss:0.071, val_acc:0.944]
Epoch [103/120    avg_loss:0.076, val_acc:0.946]
Epoch [104/120    avg_loss:0.068, val_acc:0.959]
Epoch [105/120    avg_loss:0.046, val_acc:0.964]
Epoch [106/120    avg_loss:0.037, val_acc:0.968]
Epoch [107/120    avg_loss:0.044, val_acc:0.964]
Epoch [108/120    avg_loss:0.041, val_acc:0.966]
Epoch [109/120    avg_loss:0.050, val_acc:0.969]
Epoch [110/120    avg_loss:0.038, val_acc:0.970]
Epoch [111/120    avg_loss:0.033, val_acc:0.969]
Epoch [112/120    avg_loss:0.032, val_acc:0.972]
Epoch [113/120    avg_loss:0.035, val_acc:0.968]
Epoch [114/120    avg_loss:0.031, val_acc:0.970]
Epoch [115/120    avg_loss:0.028, val_acc:0.969]
Epoch [116/120    avg_loss:0.030, val_acc:0.968]
Epoch [117/120    avg_loss:0.030, val_acc:0.968]
Epoch [118/120    avg_loss:0.028, val_acc:0.968]
Epoch [119/120    avg_loss:0.030, val_acc:0.969]
Epoch [120/120    avg_loss:0.032, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    6 1229    3    0    0    2    0    0    0    8   32    1    0
     0    4    0]
 [   0    0    1  721    4    5    3    0    0   12    0    0    1    0
     0    0    0]
 [   0    0    0    3  210    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    4    0    6    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0   10    0    0    7    0    0    0    0
     0    0    0]
 [   0    0   28   90    0    5    0    0    0    0  741    8    1    0
     0    2    0]
 [   0    0   22    0    0    1   16    0    0    0   12 2150    5    3
     1    0    0]
 [   0    0    0    6    0    4    0    0    0    0   15   11  493    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0    0    1    0    0
  1135    0    0]
 [   0    0    2    0    0    0   29    0    0    0    0    0    0    0
    89  227    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.89430894308943

F1 scores:
[       nan 0.91111111 0.95642023 0.9178867  0.98360656 0.96907216
 0.95557174 0.92592593 0.997669   0.3255814  0.8976378  0.97439384
 0.95173745 0.9919571  0.95942519 0.78275862 0.96511628]

Kappa:
0.9417593730082295
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1cff283ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.792, val_acc:0.157]
Epoch [2/120    avg_loss:2.655, val_acc:0.352]
Epoch [3/120    avg_loss:2.511, val_acc:0.402]
Epoch [4/120    avg_loss:2.401, val_acc:0.410]
Epoch [5/120    avg_loss:2.298, val_acc:0.419]
Epoch [6/120    avg_loss:2.189, val_acc:0.454]
Epoch [7/120    avg_loss:2.115, val_acc:0.476]
Epoch [8/120    avg_loss:1.988, val_acc:0.534]
Epoch [9/120    avg_loss:1.984, val_acc:0.610]
Epoch [10/120    avg_loss:1.832, val_acc:0.582]
Epoch [11/120    avg_loss:1.749, val_acc:0.602]
Epoch [12/120    avg_loss:1.701, val_acc:0.606]
Epoch [13/120    avg_loss:1.551, val_acc:0.623]
Epoch [14/120    avg_loss:1.466, val_acc:0.660]
Epoch [15/120    avg_loss:1.283, val_acc:0.695]
Epoch [16/120    avg_loss:1.196, val_acc:0.708]
Epoch [17/120    avg_loss:1.133, val_acc:0.737]
Epoch [18/120    avg_loss:0.978, val_acc:0.737]
Epoch [19/120    avg_loss:0.962, val_acc:0.752]
Epoch [20/120    avg_loss:0.858, val_acc:0.780]
Epoch [21/120    avg_loss:0.792, val_acc:0.779]
Epoch [22/120    avg_loss:0.748, val_acc:0.770]
Epoch [23/120    avg_loss:0.707, val_acc:0.788]
Epoch [24/120    avg_loss:0.677, val_acc:0.776]
Epoch [25/120    avg_loss:0.641, val_acc:0.809]
Epoch [26/120    avg_loss:0.572, val_acc:0.812]
Epoch [27/120    avg_loss:0.586, val_acc:0.803]
Epoch [28/120    avg_loss:0.541, val_acc:0.817]
Epoch [29/120    avg_loss:0.510, val_acc:0.832]
Epoch [30/120    avg_loss:0.488, val_acc:0.834]
Epoch [31/120    avg_loss:0.445, val_acc:0.830]
Epoch [32/120    avg_loss:0.429, val_acc:0.861]
Epoch [33/120    avg_loss:0.437, val_acc:0.861]
Epoch [34/120    avg_loss:0.367, val_acc:0.863]
Epoch [35/120    avg_loss:0.317, val_acc:0.859]
Epoch [36/120    avg_loss:0.330, val_acc:0.877]
Epoch [37/120    avg_loss:0.315, val_acc:0.852]
Epoch [38/120    avg_loss:0.325, val_acc:0.885]
Epoch [39/120    avg_loss:0.269, val_acc:0.873]
Epoch [40/120    avg_loss:0.288, val_acc:0.871]
Epoch [41/120    avg_loss:0.283, val_acc:0.852]
Epoch [42/120    avg_loss:0.245, val_acc:0.900]
Epoch [43/120    avg_loss:0.237, val_acc:0.880]
Epoch [44/120    avg_loss:0.291, val_acc:0.874]
Epoch [45/120    avg_loss:0.259, val_acc:0.874]
Epoch [46/120    avg_loss:0.228, val_acc:0.906]
Epoch [47/120    avg_loss:0.205, val_acc:0.901]
Epoch [48/120    avg_loss:0.201, val_acc:0.897]
Epoch [49/120    avg_loss:0.207, val_acc:0.902]
Epoch [50/120    avg_loss:0.174, val_acc:0.930]
Epoch [51/120    avg_loss:0.165, val_acc:0.910]
Epoch [52/120    avg_loss:0.211, val_acc:0.906]
Epoch [53/120    avg_loss:0.170, val_acc:0.918]
Epoch [54/120    avg_loss:0.133, val_acc:0.921]
Epoch [55/120    avg_loss:0.137, val_acc:0.931]
Epoch [56/120    avg_loss:0.141, val_acc:0.927]
Epoch [57/120    avg_loss:0.141, val_acc:0.933]
Epoch [58/120    avg_loss:0.132, val_acc:0.938]
Epoch [59/120    avg_loss:0.147, val_acc:0.929]
Epoch [60/120    avg_loss:0.159, val_acc:0.929]
Epoch [61/120    avg_loss:0.144, val_acc:0.923]
Epoch [62/120    avg_loss:0.210, val_acc:0.907]
Epoch [63/120    avg_loss:0.139, val_acc:0.927]
Epoch [64/120    avg_loss:0.111, val_acc:0.936]
Epoch [65/120    avg_loss:0.115, val_acc:0.934]
Epoch [66/120    avg_loss:0.106, val_acc:0.921]
Epoch [67/120    avg_loss:0.130, val_acc:0.942]
Epoch [68/120    avg_loss:0.100, val_acc:0.945]
Epoch [69/120    avg_loss:0.077, val_acc:0.959]
Epoch [70/120    avg_loss:0.114, val_acc:0.935]
Epoch [71/120    avg_loss:0.116, val_acc:0.953]
Epoch [72/120    avg_loss:0.076, val_acc:0.959]
Epoch [73/120    avg_loss:0.083, val_acc:0.961]
Epoch [74/120    avg_loss:0.076, val_acc:0.957]
Epoch [75/120    avg_loss:0.059, val_acc:0.958]
Epoch [76/120    avg_loss:0.056, val_acc:0.968]
Epoch [77/120    avg_loss:0.046, val_acc:0.964]
Epoch [78/120    avg_loss:0.050, val_acc:0.955]
Epoch [79/120    avg_loss:0.056, val_acc:0.961]
Epoch [80/120    avg_loss:0.062, val_acc:0.953]
Epoch [81/120    avg_loss:0.076, val_acc:0.948]
Epoch [82/120    avg_loss:0.069, val_acc:0.961]
Epoch [83/120    avg_loss:0.050, val_acc:0.962]
Epoch [84/120    avg_loss:0.059, val_acc:0.959]
Epoch [85/120    avg_loss:0.110, val_acc:0.922]
Epoch [86/120    avg_loss:0.103, val_acc:0.957]
Epoch [87/120    avg_loss:0.077, val_acc:0.959]
Epoch [88/120    avg_loss:0.071, val_acc:0.966]
Epoch [89/120    avg_loss:0.040, val_acc:0.965]
Epoch [90/120    avg_loss:0.041, val_acc:0.969]
Epoch [91/120    avg_loss:0.038, val_acc:0.969]
Epoch [92/120    avg_loss:0.034, val_acc:0.971]
Epoch [93/120    avg_loss:0.037, val_acc:0.971]
Epoch [94/120    avg_loss:0.030, val_acc:0.971]
Epoch [95/120    avg_loss:0.027, val_acc:0.971]
Epoch [96/120    avg_loss:0.029, val_acc:0.971]
Epoch [97/120    avg_loss:0.032, val_acc:0.971]
Epoch [98/120    avg_loss:0.027, val_acc:0.971]
Epoch [99/120    avg_loss:0.030, val_acc:0.971]
Epoch [100/120    avg_loss:0.028, val_acc:0.971]
Epoch [101/120    avg_loss:0.028, val_acc:0.971]
Epoch [102/120    avg_loss:0.024, val_acc:0.971]
Epoch [103/120    avg_loss:0.025, val_acc:0.973]
Epoch [104/120    avg_loss:0.025, val_acc:0.971]
Epoch [105/120    avg_loss:0.023, val_acc:0.970]
Epoch [106/120    avg_loss:0.026, val_acc:0.969]
Epoch [107/120    avg_loss:0.024, val_acc:0.970]
Epoch [108/120    avg_loss:0.027, val_acc:0.973]
Epoch [109/120    avg_loss:0.025, val_acc:0.971]
Epoch [110/120    avg_loss:0.028, val_acc:0.971]
Epoch [111/120    avg_loss:0.025, val_acc:0.973]
Epoch [112/120    avg_loss:0.023, val_acc:0.971]
Epoch [113/120    avg_loss:0.021, val_acc:0.975]
Epoch [114/120    avg_loss:0.021, val_acc:0.973]
Epoch [115/120    avg_loss:0.021, val_acc:0.973]
Epoch [116/120    avg_loss:0.028, val_acc:0.974]
Epoch [117/120    avg_loss:0.025, val_acc:0.974]
Epoch [118/120    avg_loss:0.026, val_acc:0.976]
Epoch [119/120    avg_loss:0.026, val_acc:0.977]
Epoch [120/120    avg_loss:0.024, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1210    0    0    0    5    0    0    0    8   58    4    0
     0    0    0]
 [   0    0    7  715    0    6    0    0    0    5    0    4    9    1
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0  413    0    3    0    3    0    0    0    0
    16    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   13    0    0    3    0
     0    0    0]
 [   0    0   18   89    0    2    0    0    0    0  759    1    0    0
     1    5    0]
 [   0    0   11    0    0    0    5    0    0    0   19 2173    0    2
     0    0    0]
 [   0    0    0   26    1    1    0    0    0    0    6   14  480    0
     0    2    4]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    1    2    0    0
  1128    2    0]
 [   0    0    0    0    0    0   39    0    0    9    0    0    0    0
    93  206    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.59078590785907

F1 scores:
[       nan 0.98765432 0.95614382 0.90678503 0.99530516 0.95601852
 0.96187683 0.94339623 1.         0.54166667 0.90952666 0.97356631
 0.93023256 0.98924731 0.9490955  0.73309609 0.96470588]

Kappa:
0.9382332219826571
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f14c965bac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.842, val_acc:0.336]
Epoch [2/120    avg_loss:2.706, val_acc:0.432]
Epoch [3/120    avg_loss:2.566, val_acc:0.502]
Epoch [4/120    avg_loss:2.432, val_acc:0.530]
Epoch [5/120    avg_loss:2.331, val_acc:0.533]
Epoch [6/120    avg_loss:2.236, val_acc:0.538]
Epoch [7/120    avg_loss:2.150, val_acc:0.551]
Epoch [8/120    avg_loss:2.072, val_acc:0.570]
Epoch [9/120    avg_loss:1.970, val_acc:0.598]
Epoch [10/120    avg_loss:1.868, val_acc:0.614]
Epoch [11/120    avg_loss:1.773, val_acc:0.639]
Epoch [12/120    avg_loss:1.700, val_acc:0.626]
Epoch [13/120    avg_loss:1.580, val_acc:0.649]
Epoch [14/120    avg_loss:1.423, val_acc:0.661]
Epoch [15/120    avg_loss:1.318, val_acc:0.666]
Epoch [16/120    avg_loss:1.239, val_acc:0.690]
Epoch [17/120    avg_loss:1.185, val_acc:0.694]
Epoch [18/120    avg_loss:1.038, val_acc:0.733]
Epoch [19/120    avg_loss:0.928, val_acc:0.720]
Epoch [20/120    avg_loss:0.906, val_acc:0.722]
Epoch [21/120    avg_loss:0.810, val_acc:0.774]
Epoch [22/120    avg_loss:0.827, val_acc:0.724]
Epoch [23/120    avg_loss:0.812, val_acc:0.777]
Epoch [24/120    avg_loss:0.741, val_acc:0.777]
Epoch [25/120    avg_loss:0.681, val_acc:0.761]
Epoch [26/120    avg_loss:0.566, val_acc:0.833]
Epoch [27/120    avg_loss:0.562, val_acc:0.816]
Epoch [28/120    avg_loss:0.520, val_acc:0.800]
Epoch [29/120    avg_loss:0.450, val_acc:0.847]
Epoch [30/120    avg_loss:0.436, val_acc:0.856]
Epoch [31/120    avg_loss:0.446, val_acc:0.842]
Epoch [32/120    avg_loss:0.569, val_acc:0.795]
Epoch [33/120    avg_loss:0.487, val_acc:0.847]
Epoch [34/120    avg_loss:0.410, val_acc:0.848]
Epoch [35/120    avg_loss:0.339, val_acc:0.878]
Epoch [36/120    avg_loss:0.298, val_acc:0.876]
Epoch [37/120    avg_loss:0.390, val_acc:0.820]
Epoch [38/120    avg_loss:0.356, val_acc:0.880]
Epoch [39/120    avg_loss:0.298, val_acc:0.861]
Epoch [40/120    avg_loss:0.299, val_acc:0.880]
Epoch [41/120    avg_loss:0.325, val_acc:0.856]
Epoch [42/120    avg_loss:0.264, val_acc:0.892]
Epoch [43/120    avg_loss:0.359, val_acc:0.872]
Epoch [44/120    avg_loss:0.260, val_acc:0.886]
Epoch [45/120    avg_loss:0.293, val_acc:0.863]
Epoch [46/120    avg_loss:0.242, val_acc:0.895]
Epoch [47/120    avg_loss:0.244, val_acc:0.874]
Epoch [48/120    avg_loss:0.243, val_acc:0.893]
Epoch [49/120    avg_loss:0.177, val_acc:0.900]
Epoch [50/120    avg_loss:0.183, val_acc:0.912]
Epoch [51/120    avg_loss:0.141, val_acc:0.919]
Epoch [52/120    avg_loss:0.169, val_acc:0.929]
Epoch [53/120    avg_loss:0.153, val_acc:0.925]
Epoch [54/120    avg_loss:0.172, val_acc:0.903]
Epoch [55/120    avg_loss:0.200, val_acc:0.920]
Epoch [56/120    avg_loss:0.156, val_acc:0.910]
Epoch [57/120    avg_loss:0.182, val_acc:0.912]
Epoch [58/120    avg_loss:0.163, val_acc:0.907]
Epoch [59/120    avg_loss:0.151, val_acc:0.922]
Epoch [60/120    avg_loss:0.140, val_acc:0.935]
Epoch [61/120    avg_loss:0.107, val_acc:0.934]
Epoch [62/120    avg_loss:0.103, val_acc:0.931]
Epoch [63/120    avg_loss:0.098, val_acc:0.934]
Epoch [64/120    avg_loss:0.100, val_acc:0.941]
Epoch [65/120    avg_loss:0.088, val_acc:0.922]
Epoch [66/120    avg_loss:0.106, val_acc:0.925]
Epoch [67/120    avg_loss:0.135, val_acc:0.931]
Epoch [68/120    avg_loss:0.154, val_acc:0.910]
Epoch [69/120    avg_loss:0.129, val_acc:0.919]
Epoch [70/120    avg_loss:0.127, val_acc:0.919]
Epoch [71/120    avg_loss:0.119, val_acc:0.927]
Epoch [72/120    avg_loss:0.108, val_acc:0.918]
Epoch [73/120    avg_loss:0.146, val_acc:0.931]
Epoch [74/120    avg_loss:0.111, val_acc:0.927]
Epoch [75/120    avg_loss:0.122, val_acc:0.917]
Epoch [76/120    avg_loss:0.094, val_acc:0.926]
Epoch [77/120    avg_loss:0.083, val_acc:0.948]
Epoch [78/120    avg_loss:0.064, val_acc:0.947]
Epoch [79/120    avg_loss:0.068, val_acc:0.940]
Epoch [80/120    avg_loss:0.055, val_acc:0.941]
Epoch [81/120    avg_loss:0.058, val_acc:0.946]
Epoch [82/120    avg_loss:0.065, val_acc:0.946]
Epoch [83/120    avg_loss:0.063, val_acc:0.957]
Epoch [84/120    avg_loss:0.065, val_acc:0.941]
Epoch [85/120    avg_loss:0.078, val_acc:0.944]
Epoch [86/120    avg_loss:0.090, val_acc:0.935]
Epoch [87/120    avg_loss:0.087, val_acc:0.935]
Epoch [88/120    avg_loss:0.089, val_acc:0.946]
Epoch [89/120    avg_loss:0.060, val_acc:0.948]
Epoch [90/120    avg_loss:0.049, val_acc:0.938]
Epoch [91/120    avg_loss:0.063, val_acc:0.949]
Epoch [92/120    avg_loss:0.043, val_acc:0.954]
Epoch [93/120    avg_loss:0.040, val_acc:0.941]
Epoch [94/120    avg_loss:0.050, val_acc:0.941]
Epoch [95/120    avg_loss:0.070, val_acc:0.954]
Epoch [96/120    avg_loss:0.080, val_acc:0.910]
Epoch [97/120    avg_loss:0.073, val_acc:0.932]
Epoch [98/120    avg_loss:0.048, val_acc:0.947]
Epoch [99/120    avg_loss:0.043, val_acc:0.948]
Epoch [100/120    avg_loss:0.040, val_acc:0.952]
Epoch [101/120    avg_loss:0.029, val_acc:0.954]
Epoch [102/120    avg_loss:0.035, val_acc:0.954]
Epoch [103/120    avg_loss:0.036, val_acc:0.949]
Epoch [104/120    avg_loss:0.032, val_acc:0.954]
Epoch [105/120    avg_loss:0.035, val_acc:0.953]
Epoch [106/120    avg_loss:0.043, val_acc:0.956]
Epoch [107/120    avg_loss:0.034, val_acc:0.955]
Epoch [108/120    avg_loss:0.024, val_acc:0.956]
Epoch [109/120    avg_loss:0.030, val_acc:0.958]
Epoch [110/120    avg_loss:0.030, val_acc:0.961]
Epoch [111/120    avg_loss:0.030, val_acc:0.954]
Epoch [112/120    avg_loss:0.030, val_acc:0.955]
Epoch [113/120    avg_loss:0.032, val_acc:0.962]
Epoch [114/120    avg_loss:0.023, val_acc:0.962]
Epoch [115/120    avg_loss:0.031, val_acc:0.961]
Epoch [116/120    avg_loss:0.026, val_acc:0.961]
Epoch [117/120    avg_loss:0.032, val_acc:0.962]
Epoch [118/120    avg_loss:0.025, val_acc:0.962]
Epoch [119/120    avg_loss:0.037, val_acc:0.961]
Epoch [120/120    avg_loss:0.024, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1215    3    0    0    6    0    0    0    8   41    3    0
     0    9    0]
 [   0    0    1  679    1    8    0    0    0    7    0    0   51    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  420    0    4    0    4    0    1    0    0
     6    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0   12    0    0    1    0
     0    0    0]
 [   0    0   24   90    0    5    0    0    0    0  743    3    0    0
     0   10    0]
 [   0    0   14    1    0    0   11    0    2    0    5 2164    1    3
     9    0    0]
 [   0    0    1    1    5    9    0    0    0    0   13    6  495    0
     0    0    4]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0   11    0    0    2    0    4    2    0    0
  1120    0    0]
 [   0    0    0    0    0    0   30    0    0    0    0    0    0    0
   107  210    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.14634146341463

F1 scores:
[       nan 0.94871795 0.95669291 0.88990826 0.98611111 0.94488189
 0.96318115 0.92592593 0.99537037 0.58536585 0.89951574 0.97697517
 0.9124424  0.98924731 0.94078118 0.72916667 0.97674419]

Kappa:
0.9332199844843541
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc56192aac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.767, val_acc:0.245]
Epoch [2/120    avg_loss:2.623, val_acc:0.341]
Epoch [3/120    avg_loss:2.487, val_acc:0.339]
Epoch [4/120    avg_loss:2.346, val_acc:0.420]
Epoch [5/120    avg_loss:2.269, val_acc:0.468]
Epoch [6/120    avg_loss:2.171, val_acc:0.514]
Epoch [7/120    avg_loss:2.097, val_acc:0.555]
Epoch [8/120    avg_loss:2.016, val_acc:0.520]
Epoch [9/120    avg_loss:1.913, val_acc:0.622]
Epoch [10/120    avg_loss:1.787, val_acc:0.623]
Epoch [11/120    avg_loss:1.656, val_acc:0.610]
Epoch [12/120    avg_loss:1.579, val_acc:0.632]
Epoch [13/120    avg_loss:1.465, val_acc:0.668]
Epoch [14/120    avg_loss:1.336, val_acc:0.671]
Epoch [15/120    avg_loss:1.256, val_acc:0.679]
Epoch [16/120    avg_loss:1.189, val_acc:0.700]
Epoch [17/120    avg_loss:1.065, val_acc:0.717]
Epoch [18/120    avg_loss:1.081, val_acc:0.722]
Epoch [19/120    avg_loss:0.940, val_acc:0.738]
Epoch [20/120    avg_loss:0.901, val_acc:0.767]
Epoch [21/120    avg_loss:0.862, val_acc:0.752]
Epoch [22/120    avg_loss:0.771, val_acc:0.800]
Epoch [23/120    avg_loss:0.693, val_acc:0.805]
Epoch [24/120    avg_loss:0.622, val_acc:0.825]
Epoch [25/120    avg_loss:0.561, val_acc:0.816]
Epoch [26/120    avg_loss:0.669, val_acc:0.785]
Epoch [27/120    avg_loss:0.584, val_acc:0.792]
Epoch [28/120    avg_loss:0.521, val_acc:0.831]
Epoch [29/120    avg_loss:0.561, val_acc:0.812]
Epoch [30/120    avg_loss:0.491, val_acc:0.823]
Epoch [31/120    avg_loss:0.454, val_acc:0.851]
Epoch [32/120    avg_loss:0.431, val_acc:0.849]
Epoch [33/120    avg_loss:0.384, val_acc:0.838]
Epoch [34/120    avg_loss:0.360, val_acc:0.882]
Epoch [35/120    avg_loss:0.333, val_acc:0.876]
Epoch [36/120    avg_loss:0.321, val_acc:0.875]
Epoch [37/120    avg_loss:0.350, val_acc:0.881]
Epoch [38/120    avg_loss:0.301, val_acc:0.901]
Epoch [39/120    avg_loss:0.261, val_acc:0.889]
Epoch [40/120    avg_loss:0.280, val_acc:0.870]
Epoch [41/120    avg_loss:0.243, val_acc:0.902]
Epoch [42/120    avg_loss:0.267, val_acc:0.903]
Epoch [43/120    avg_loss:0.270, val_acc:0.894]
Epoch [44/120    avg_loss:0.186, val_acc:0.919]
Epoch [45/120    avg_loss:0.326, val_acc:0.880]
Epoch [46/120    avg_loss:0.236, val_acc:0.894]
Epoch [47/120    avg_loss:0.217, val_acc:0.912]
Epoch [48/120    avg_loss:0.196, val_acc:0.918]
Epoch [49/120    avg_loss:0.166, val_acc:0.917]
Epoch [50/120    avg_loss:0.299, val_acc:0.864]
Epoch [51/120    avg_loss:0.299, val_acc:0.885]
Epoch [52/120    avg_loss:0.240, val_acc:0.898]
Epoch [53/120    avg_loss:0.230, val_acc:0.900]
Epoch [54/120    avg_loss:0.193, val_acc:0.912]
Epoch [55/120    avg_loss:0.218, val_acc:0.904]
Epoch [56/120    avg_loss:0.165, val_acc:0.929]
Epoch [57/120    avg_loss:0.213, val_acc:0.918]
Epoch [58/120    avg_loss:0.163, val_acc:0.932]
Epoch [59/120    avg_loss:0.157, val_acc:0.930]
Epoch [60/120    avg_loss:0.144, val_acc:0.945]
Epoch [61/120    avg_loss:0.135, val_acc:0.944]
Epoch [62/120    avg_loss:0.137, val_acc:0.922]
Epoch [63/120    avg_loss:0.150, val_acc:0.935]
Epoch [64/120    avg_loss:0.120, val_acc:0.944]
Epoch [65/120    avg_loss:0.150, val_acc:0.939]
Epoch [66/120    avg_loss:0.124, val_acc:0.935]
Epoch [67/120    avg_loss:0.129, val_acc:0.937]
Epoch [68/120    avg_loss:0.100, val_acc:0.932]
Epoch [69/120    avg_loss:0.100, val_acc:0.946]
Epoch [70/120    avg_loss:0.103, val_acc:0.954]
Epoch [71/120    avg_loss:0.092, val_acc:0.953]
Epoch [72/120    avg_loss:0.127, val_acc:0.936]
Epoch [73/120    avg_loss:0.111, val_acc:0.934]
Epoch [74/120    avg_loss:0.103, val_acc:0.949]
Epoch [75/120    avg_loss:0.075, val_acc:0.947]
Epoch [76/120    avg_loss:0.079, val_acc:0.957]
Epoch [77/120    avg_loss:0.079, val_acc:0.959]
Epoch [78/120    avg_loss:0.077, val_acc:0.937]
Epoch [79/120    avg_loss:0.108, val_acc:0.946]
Epoch [80/120    avg_loss:0.090, val_acc:0.956]
Epoch [81/120    avg_loss:0.084, val_acc:0.953]
Epoch [82/120    avg_loss:0.081, val_acc:0.950]
Epoch [83/120    avg_loss:0.068, val_acc:0.948]
Epoch [84/120    avg_loss:0.067, val_acc:0.953]
Epoch [85/120    avg_loss:0.066, val_acc:0.954]
Epoch [86/120    avg_loss:0.085, val_acc:0.946]
Epoch [87/120    avg_loss:0.098, val_acc:0.945]
Epoch [88/120    avg_loss:0.062, val_acc:0.954]
Epoch [89/120    avg_loss:0.066, val_acc:0.948]
Epoch [90/120    avg_loss:0.104, val_acc:0.910]
Epoch [91/120    avg_loss:0.094, val_acc:0.949]
Epoch [92/120    avg_loss:0.054, val_acc:0.957]
Epoch [93/120    avg_loss:0.061, val_acc:0.959]
Epoch [94/120    avg_loss:0.056, val_acc:0.961]
Epoch [95/120    avg_loss:0.043, val_acc:0.965]
Epoch [96/120    avg_loss:0.045, val_acc:0.962]
Epoch [97/120    avg_loss:0.040, val_acc:0.962]
Epoch [98/120    avg_loss:0.041, val_acc:0.963]
Epoch [99/120    avg_loss:0.044, val_acc:0.965]
Epoch [100/120    avg_loss:0.037, val_acc:0.964]
Epoch [101/120    avg_loss:0.045, val_acc:0.967]
Epoch [102/120    avg_loss:0.049, val_acc:0.965]
Epoch [103/120    avg_loss:0.037, val_acc:0.966]
Epoch [104/120    avg_loss:0.033, val_acc:0.964]
Epoch [105/120    avg_loss:0.036, val_acc:0.967]
Epoch [106/120    avg_loss:0.034, val_acc:0.967]
Epoch [107/120    avg_loss:0.036, val_acc:0.963]
Epoch [108/120    avg_loss:0.038, val_acc:0.966]
Epoch [109/120    avg_loss:0.040, val_acc:0.966]
Epoch [110/120    avg_loss:0.032, val_acc:0.966]
Epoch [111/120    avg_loss:0.036, val_acc:0.968]
Epoch [112/120    avg_loss:0.034, val_acc:0.968]
Epoch [113/120    avg_loss:0.028, val_acc:0.967]
Epoch [114/120    avg_loss:0.037, val_acc:0.967]
Epoch [115/120    avg_loss:0.044, val_acc:0.966]
Epoch [116/120    avg_loss:0.032, val_acc:0.970]
Epoch [117/120    avg_loss:0.042, val_acc:0.970]
Epoch [118/120    avg_loss:0.033, val_acc:0.972]
Epoch [119/120    avg_loss:0.036, val_acc:0.970]
Epoch [120/120    avg_loss:0.035, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1227    4    3    0    3    0    0    0    5   35    2    0
     0    6    0]
 [   0    0    1  713    0    0    0    0    0   11    0    0   20    2
     0    0    0]
 [   0    0    0    3  210    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  415    0    5    0    5    0    1    0    0
     9    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    2    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   18    0    0    0    0    0    0  412    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   18   90    0    4    0    0    0    0  748    7    0    0
     0    8    0]
 [   0    0   27    0    0    0    9    0    0    0   15 2153    4    2
     0    0    0]
 [   0    0    1   31    1    8    0    0    1    0   10    1  473    0
     0    0    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    3    0    0    0
  1125    3    0]
 [   0    0    0    0    0    0    1    0    0    4    0    0    0    0
   114  228    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.33062330623306

F1 scores:
[       nan 0.80808081 0.95896835 0.89629164 0.98360656 0.95512083
 0.98561696 0.90909091 0.97630332 0.56603774 0.90283645 0.97663869
 0.91400966 0.98930481 0.94102886 0.77027027 0.94252874]

Kappa:
0.9353395306046689
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ee0e37a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.806, val_acc:0.123]
Epoch [2/120    avg_loss:2.699, val_acc:0.292]
Epoch [3/120    avg_loss:2.573, val_acc:0.352]
Epoch [4/120    avg_loss:2.448, val_acc:0.422]
Epoch [5/120    avg_loss:2.313, val_acc:0.484]
Epoch [6/120    avg_loss:2.235, val_acc:0.511]
Epoch [7/120    avg_loss:2.162, val_acc:0.537]
Epoch [8/120    avg_loss:2.064, val_acc:0.570]
Epoch [9/120    avg_loss:1.974, val_acc:0.507]
Epoch [10/120    avg_loss:1.851, val_acc:0.579]
Epoch [11/120    avg_loss:1.763, val_acc:0.580]
Epoch [12/120    avg_loss:1.698, val_acc:0.577]
Epoch [13/120    avg_loss:1.568, val_acc:0.591]
Epoch [14/120    avg_loss:1.459, val_acc:0.593]
Epoch [15/120    avg_loss:1.389, val_acc:0.616]
Epoch [16/120    avg_loss:1.284, val_acc:0.602]
Epoch [17/120    avg_loss:1.172, val_acc:0.674]
Epoch [18/120    avg_loss:1.068, val_acc:0.686]
Epoch [19/120    avg_loss:0.957, val_acc:0.728]
Epoch [20/120    avg_loss:0.934, val_acc:0.736]
Epoch [21/120    avg_loss:0.876, val_acc:0.751]
Epoch [22/120    avg_loss:0.773, val_acc:0.726]
Epoch [23/120    avg_loss:0.736, val_acc:0.794]
Epoch [24/120    avg_loss:0.715, val_acc:0.793]
Epoch [25/120    avg_loss:0.611, val_acc:0.844]
Epoch [26/120    avg_loss:0.585, val_acc:0.830]
Epoch [27/120    avg_loss:0.533, val_acc:0.835]
Epoch [28/120    avg_loss:0.476, val_acc:0.868]
Epoch [29/120    avg_loss:0.433, val_acc:0.842]
Epoch [30/120    avg_loss:0.359, val_acc:0.899]
Epoch [31/120    avg_loss:0.319, val_acc:0.889]
Epoch [32/120    avg_loss:0.299, val_acc:0.870]
Epoch [33/120    avg_loss:0.288, val_acc:0.886]
Epoch [34/120    avg_loss:0.245, val_acc:0.892]
Epoch [35/120    avg_loss:0.255, val_acc:0.919]
Epoch [36/120    avg_loss:0.214, val_acc:0.912]
Epoch [37/120    avg_loss:0.194, val_acc:0.935]
Epoch [38/120    avg_loss:0.176, val_acc:0.928]
Epoch [39/120    avg_loss:0.181, val_acc:0.948]
Epoch [40/120    avg_loss:0.161, val_acc:0.917]
Epoch [41/120    avg_loss:0.155, val_acc:0.918]
Epoch [42/120    avg_loss:0.154, val_acc:0.951]
Epoch [43/120    avg_loss:0.119, val_acc:0.950]
Epoch [44/120    avg_loss:0.141, val_acc:0.941]
Epoch [45/120    avg_loss:0.128, val_acc:0.946]
Epoch [46/120    avg_loss:0.104, val_acc:0.945]
Epoch [47/120    avg_loss:0.089, val_acc:0.963]
Epoch [48/120    avg_loss:0.114, val_acc:0.922]
Epoch [49/120    avg_loss:0.156, val_acc:0.931]
Epoch [50/120    avg_loss:0.162, val_acc:0.942]
Epoch [51/120    avg_loss:0.111, val_acc:0.935]
Epoch [52/120    avg_loss:0.118, val_acc:0.944]
Epoch [53/120    avg_loss:0.106, val_acc:0.953]
Epoch [54/120    avg_loss:0.079, val_acc:0.936]
Epoch [55/120    avg_loss:0.133, val_acc:0.950]
Epoch [56/120    avg_loss:0.094, val_acc:0.934]
Epoch [57/120    avg_loss:0.104, val_acc:0.953]
Epoch [58/120    avg_loss:0.078, val_acc:0.942]
Epoch [59/120    avg_loss:0.094, val_acc:0.966]
Epoch [60/120    avg_loss:0.090, val_acc:0.942]
Epoch [61/120    avg_loss:0.111, val_acc:0.960]
Epoch [62/120    avg_loss:0.072, val_acc:0.958]
Epoch [63/120    avg_loss:0.099, val_acc:0.941]
Epoch [64/120    avg_loss:0.077, val_acc:0.955]
Epoch [65/120    avg_loss:0.059, val_acc:0.961]
Epoch [66/120    avg_loss:0.038, val_acc:0.969]
Epoch [67/120    avg_loss:0.034, val_acc:0.957]
Epoch [68/120    avg_loss:0.035, val_acc:0.967]
Epoch [69/120    avg_loss:0.036, val_acc:0.961]
Epoch [70/120    avg_loss:0.040, val_acc:0.974]
Epoch [71/120    avg_loss:0.027, val_acc:0.961]
Epoch [72/120    avg_loss:0.060, val_acc:0.964]
Epoch [73/120    avg_loss:0.071, val_acc:0.969]
Epoch [74/120    avg_loss:0.062, val_acc:0.967]
Epoch [75/120    avg_loss:0.035, val_acc:0.971]
Epoch [76/120    avg_loss:0.045, val_acc:0.967]
Epoch [77/120    avg_loss:0.033, val_acc:0.961]
Epoch [78/120    avg_loss:0.027, val_acc:0.967]
Epoch [79/120    avg_loss:0.026, val_acc:0.969]
Epoch [80/120    avg_loss:0.024, val_acc:0.978]
Epoch [81/120    avg_loss:0.028, val_acc:0.973]
Epoch [82/120    avg_loss:0.024, val_acc:0.971]
Epoch [83/120    avg_loss:0.020, val_acc:0.979]
Epoch [84/120    avg_loss:0.019, val_acc:0.970]
Epoch [85/120    avg_loss:0.018, val_acc:0.968]
Epoch [86/120    avg_loss:0.016, val_acc:0.978]
Epoch [87/120    avg_loss:0.016, val_acc:0.981]
Epoch [88/120    avg_loss:0.017, val_acc:0.960]
Epoch [89/120    avg_loss:0.016, val_acc:0.972]
Epoch [90/120    avg_loss:0.022, val_acc:0.967]
Epoch [91/120    avg_loss:0.017, val_acc:0.978]
Epoch [92/120    avg_loss:0.014, val_acc:0.982]
Epoch [93/120    avg_loss:0.013, val_acc:0.977]
Epoch [94/120    avg_loss:0.013, val_acc:0.978]
Epoch [95/120    avg_loss:0.012, val_acc:0.977]
Epoch [96/120    avg_loss:0.018, val_acc:0.980]
Epoch [97/120    avg_loss:0.015, val_acc:0.984]
Epoch [98/120    avg_loss:0.015, val_acc:0.981]
Epoch [99/120    avg_loss:0.010, val_acc:0.980]
Epoch [100/120    avg_loss:0.009, val_acc:0.981]
Epoch [101/120    avg_loss:0.009, val_acc:0.983]
Epoch [102/120    avg_loss:0.008, val_acc:0.983]
Epoch [103/120    avg_loss:0.011, val_acc:0.979]
Epoch [104/120    avg_loss:0.012, val_acc:0.971]
Epoch [105/120    avg_loss:0.022, val_acc:0.982]
Epoch [106/120    avg_loss:0.012, val_acc:0.977]
Epoch [107/120    avg_loss:0.013, val_acc:0.983]
Epoch [108/120    avg_loss:0.014, val_acc:0.980]
Epoch [109/120    avg_loss:0.024, val_acc:0.976]
Epoch [110/120    avg_loss:0.029, val_acc:0.976]
Epoch [111/120    avg_loss:0.017, val_acc:0.979]
Epoch [112/120    avg_loss:0.013, val_acc:0.977]
Epoch [113/120    avg_loss:0.013, val_acc:0.980]
Epoch [114/120    avg_loss:0.010, val_acc:0.980]
Epoch [115/120    avg_loss:0.011, val_acc:0.980]
Epoch [116/120    avg_loss:0.010, val_acc:0.981]
Epoch [117/120    avg_loss:0.008, val_acc:0.981]
Epoch [118/120    avg_loss:0.009, val_acc:0.982]
Epoch [119/120    avg_loss:0.009, val_acc:0.981]
Epoch [120/120    avg_loss:0.008, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1239    3    4    1    0    0    0    0   10   27    1    0
     0    0    0]
 [   0    0    0  728    2    0    0    0    0    5    1    9    2    0
     0    0    0]
 [   0    0    0    2  210    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  430    1    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   16    0    0    0    0    0    0    0  842   17    0    0
     0    0    0]
 [   0    0   29    0    0    0    2    0    0    0    4 2161    4    0
     2    8    0]
 [   0    0    0    5    0    0    0    0    0    0    0    2  521    0
     3    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1118   20    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    79  266    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.02981029810299

F1 scores:
[       nan 0.975      0.96420233 0.98047138 0.97902098 0.99307159
 0.99544765 1.         1.         0.85714286 0.97172533 0.97628191
 0.97932331 1.         0.95351812 0.82866044 0.98224852]

Kappa:
0.96611369646141
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f85b55ffa58>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.819, val_acc:0.237]
Epoch [2/120    avg_loss:2.676, val_acc:0.306]
Epoch [3/120    avg_loss:2.527, val_acc:0.368]
Epoch [4/120    avg_loss:2.419, val_acc:0.408]
Epoch [5/120    avg_loss:2.349, val_acc:0.510]
Epoch [6/120    avg_loss:2.260, val_acc:0.531]
Epoch [7/120    avg_loss:2.166, val_acc:0.529]
Epoch [8/120    avg_loss:2.066, val_acc:0.531]
Epoch [9/120    avg_loss:2.010, val_acc:0.532]
Epoch [10/120    avg_loss:1.912, val_acc:0.571]
Epoch [11/120    avg_loss:1.793, val_acc:0.566]
Epoch [12/120    avg_loss:1.754, val_acc:0.578]
Epoch [13/120    avg_loss:1.647, val_acc:0.589]
Epoch [14/120    avg_loss:1.578, val_acc:0.608]
Epoch [15/120    avg_loss:1.526, val_acc:0.623]
Epoch [16/120    avg_loss:1.394, val_acc:0.642]
Epoch [17/120    avg_loss:1.263, val_acc:0.647]
Epoch [18/120    avg_loss:1.193, val_acc:0.646]
Epoch [19/120    avg_loss:1.192, val_acc:0.664]
Epoch [20/120    avg_loss:1.105, val_acc:0.683]
Epoch [21/120    avg_loss:0.991, val_acc:0.742]
Epoch [22/120    avg_loss:0.862, val_acc:0.777]
Epoch [23/120    avg_loss:0.900, val_acc:0.736]
Epoch [24/120    avg_loss:0.779, val_acc:0.765]
Epoch [25/120    avg_loss:0.690, val_acc:0.783]
Epoch [26/120    avg_loss:0.621, val_acc:0.782]
Epoch [27/120    avg_loss:0.621, val_acc:0.798]
Epoch [28/120    avg_loss:0.521, val_acc:0.821]
Epoch [29/120    avg_loss:0.481, val_acc:0.829]
Epoch [30/120    avg_loss:0.401, val_acc:0.833]
Epoch [31/120    avg_loss:0.432, val_acc:0.836]
Epoch [32/120    avg_loss:0.374, val_acc:0.832]
Epoch [33/120    avg_loss:0.383, val_acc:0.820]
Epoch [34/120    avg_loss:0.424, val_acc:0.820]
Epoch [35/120    avg_loss:0.351, val_acc:0.838]
Epoch [36/120    avg_loss:0.291, val_acc:0.863]
Epoch [37/120    avg_loss:0.277, val_acc:0.868]
Epoch [38/120    avg_loss:0.265, val_acc:0.855]
Epoch [39/120    avg_loss:0.225, val_acc:0.891]
Epoch [40/120    avg_loss:0.195, val_acc:0.884]
Epoch [41/120    avg_loss:0.219, val_acc:0.903]
Epoch [42/120    avg_loss:0.180, val_acc:0.903]
Epoch [43/120    avg_loss:0.187, val_acc:0.860]
Epoch [44/120    avg_loss:0.173, val_acc:0.893]
Epoch [45/120    avg_loss:0.161, val_acc:0.902]
Epoch [46/120    avg_loss:0.134, val_acc:0.932]
Epoch [47/120    avg_loss:0.141, val_acc:0.899]
Epoch [48/120    avg_loss:0.133, val_acc:0.896]
Epoch [49/120    avg_loss:0.157, val_acc:0.924]
Epoch [50/120    avg_loss:0.165, val_acc:0.884]
Epoch [51/120    avg_loss:0.175, val_acc:0.908]
Epoch [52/120    avg_loss:0.142, val_acc:0.935]
Epoch [53/120    avg_loss:0.182, val_acc:0.910]
Epoch [54/120    avg_loss:0.128, val_acc:0.897]
Epoch [55/120    avg_loss:0.125, val_acc:0.927]
Epoch [56/120    avg_loss:0.103, val_acc:0.929]
Epoch [57/120    avg_loss:0.096, val_acc:0.932]
Epoch [58/120    avg_loss:0.086, val_acc:0.945]
Epoch [59/120    avg_loss:0.106, val_acc:0.947]
Epoch [60/120    avg_loss:0.076, val_acc:0.926]
Epoch [61/120    avg_loss:0.084, val_acc:0.923]
Epoch [62/120    avg_loss:0.065, val_acc:0.948]
Epoch [63/120    avg_loss:0.057, val_acc:0.950]
Epoch [64/120    avg_loss:0.058, val_acc:0.939]
Epoch [65/120    avg_loss:0.063, val_acc:0.955]
Epoch [66/120    avg_loss:0.058, val_acc:0.951]
Epoch [67/120    avg_loss:0.058, val_acc:0.932]
Epoch [68/120    avg_loss:0.069, val_acc:0.947]
Epoch [69/120    avg_loss:0.057, val_acc:0.943]
Epoch [70/120    avg_loss:0.056, val_acc:0.960]
Epoch [71/120    avg_loss:0.058, val_acc:0.948]
Epoch [72/120    avg_loss:0.044, val_acc:0.952]
Epoch [73/120    avg_loss:0.054, val_acc:0.958]
Epoch [74/120    avg_loss:0.056, val_acc:0.950]
Epoch [75/120    avg_loss:0.044, val_acc:0.948]
Epoch [76/120    avg_loss:0.041, val_acc:0.955]
Epoch [77/120    avg_loss:0.044, val_acc:0.949]
Epoch [78/120    avg_loss:0.036, val_acc:0.958]
Epoch [79/120    avg_loss:0.081, val_acc:0.934]
Epoch [80/120    avg_loss:0.054, val_acc:0.946]
Epoch [81/120    avg_loss:0.046, val_acc:0.936]
Epoch [82/120    avg_loss:0.049, val_acc:0.958]
Epoch [83/120    avg_loss:0.046, val_acc:0.948]
Epoch [84/120    avg_loss:0.035, val_acc:0.960]
Epoch [85/120    avg_loss:0.028, val_acc:0.968]
Epoch [86/120    avg_loss:0.027, val_acc:0.968]
Epoch [87/120    avg_loss:0.025, val_acc:0.970]
Epoch [88/120    avg_loss:0.021, val_acc:0.968]
Epoch [89/120    avg_loss:0.024, val_acc:0.964]
Epoch [90/120    avg_loss:0.020, val_acc:0.968]
Epoch [91/120    avg_loss:0.020, val_acc:0.968]
Epoch [92/120    avg_loss:0.019, val_acc:0.968]
Epoch [93/120    avg_loss:0.024, val_acc:0.969]
Epoch [94/120    avg_loss:0.019, val_acc:0.970]
Epoch [95/120    avg_loss:0.021, val_acc:0.966]
Epoch [96/120    avg_loss:0.022, val_acc:0.968]
Epoch [97/120    avg_loss:0.022, val_acc:0.967]
Epoch [98/120    avg_loss:0.018, val_acc:0.967]
Epoch [99/120    avg_loss:0.018, val_acc:0.968]
Epoch [100/120    avg_loss:0.020, val_acc:0.965]
Epoch [101/120    avg_loss:0.022, val_acc:0.966]
Epoch [102/120    avg_loss:0.018, val_acc:0.964]
Epoch [103/120    avg_loss:0.022, val_acc:0.965]
Epoch [104/120    avg_loss:0.018, val_acc:0.965]
Epoch [105/120    avg_loss:0.020, val_acc:0.968]
Epoch [106/120    avg_loss:0.020, val_acc:0.971]
Epoch [107/120    avg_loss:0.019, val_acc:0.970]
Epoch [108/120    avg_loss:0.016, val_acc:0.967]
Epoch [109/120    avg_loss:0.016, val_acc:0.970]
Epoch [110/120    avg_loss:0.022, val_acc:0.972]
Epoch [111/120    avg_loss:0.019, val_acc:0.973]
Epoch [112/120    avg_loss:0.015, val_acc:0.972]
Epoch [113/120    avg_loss:0.017, val_acc:0.966]
Epoch [114/120    avg_loss:0.017, val_acc:0.970]
Epoch [115/120    avg_loss:0.017, val_acc:0.967]
Epoch [116/120    avg_loss:0.016, val_acc:0.965]
Epoch [117/120    avg_loss:0.016, val_acc:0.970]
Epoch [118/120    avg_loss:0.017, val_acc:0.969]
Epoch [119/120    avg_loss:0.021, val_acc:0.966]
Epoch [120/120    avg_loss:0.016, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0 1248    2    7    1    3    0    0    0    4   20    0    0
     0    0    0]
 [   0    0    0  725    4    1    0    0    0    2    1   10    4    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    0    0    0
     5    1    0]
 [   0    0    0    0    0    0  651    0    0    0    0    5    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    1    0    0    0  833   33    3    0
     0    0    0]
 [   0    0    3    0    0    0    8    0    0    2   30 2161    6    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    7    3  520    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    1    1    0    0    0
  1122   14    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    58  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.18157181571816

F1 scores:
[       nan 0.975      0.98190401 0.98039216 0.97247706 0.99076212
 0.98487141 1.         1.         0.85       0.95145631 0.97298514
 0.97014925 1.         0.96516129 0.88615385 0.96969697]

Kappa:
0.9678466423857988
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2164850a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.811, val_acc:0.216]
Epoch [2/120    avg_loss:2.688, val_acc:0.348]
Epoch [3/120    avg_loss:2.528, val_acc:0.444]
Epoch [4/120    avg_loss:2.435, val_acc:0.480]
Epoch [5/120    avg_loss:2.326, val_acc:0.544]
Epoch [6/120    avg_loss:2.210, val_acc:0.570]
Epoch [7/120    avg_loss:2.114, val_acc:0.571]
Epoch [8/120    avg_loss:2.013, val_acc:0.552]
Epoch [9/120    avg_loss:1.872, val_acc:0.523]
Epoch [10/120    avg_loss:1.714, val_acc:0.575]
Epoch [11/120    avg_loss:1.597, val_acc:0.559]
Epoch [12/120    avg_loss:1.509, val_acc:0.572]
Epoch [13/120    avg_loss:1.363, val_acc:0.630]
Epoch [14/120    avg_loss:1.283, val_acc:0.658]
Epoch [15/120    avg_loss:1.200, val_acc:0.573]
Epoch [16/120    avg_loss:1.112, val_acc:0.676]
Epoch [17/120    avg_loss:1.014, val_acc:0.697]
Epoch [18/120    avg_loss:0.948, val_acc:0.719]
Epoch [19/120    avg_loss:0.806, val_acc:0.739]
Epoch [20/120    avg_loss:0.816, val_acc:0.640]
Epoch [21/120    avg_loss:0.728, val_acc:0.747]
Epoch [22/120    avg_loss:0.644, val_acc:0.766]
Epoch [23/120    avg_loss:0.565, val_acc:0.816]
Epoch [24/120    avg_loss:0.507, val_acc:0.803]
Epoch [25/120    avg_loss:0.459, val_acc:0.833]
Epoch [26/120    avg_loss:0.439, val_acc:0.829]
Epoch [27/120    avg_loss:0.505, val_acc:0.806]
Epoch [28/120    avg_loss:0.534, val_acc:0.797]
Epoch [29/120    avg_loss:0.409, val_acc:0.840]
Epoch [30/120    avg_loss:0.336, val_acc:0.846]
Epoch [31/120    avg_loss:0.294, val_acc:0.867]
Epoch [32/120    avg_loss:0.311, val_acc:0.847]
Epoch [33/120    avg_loss:0.383, val_acc:0.824]
Epoch [34/120    avg_loss:0.315, val_acc:0.874]
Epoch [35/120    avg_loss:0.323, val_acc:0.875]
Epoch [36/120    avg_loss:0.249, val_acc:0.903]
Epoch [37/120    avg_loss:0.213, val_acc:0.918]
Epoch [38/120    avg_loss:0.184, val_acc:0.911]
Epoch [39/120    avg_loss:0.166, val_acc:0.917]
Epoch [40/120    avg_loss:0.158, val_acc:0.928]
Epoch [41/120    avg_loss:0.122, val_acc:0.933]
Epoch [42/120    avg_loss:0.117, val_acc:0.928]
Epoch [43/120    avg_loss:0.117, val_acc:0.915]
Epoch [44/120    avg_loss:0.144, val_acc:0.884]
Epoch [45/120    avg_loss:0.131, val_acc:0.924]
Epoch [46/120    avg_loss:0.102, val_acc:0.941]
Epoch [47/120    avg_loss:0.091, val_acc:0.941]
Epoch [48/120    avg_loss:0.097, val_acc:0.936]
Epoch [49/120    avg_loss:0.080, val_acc:0.947]
Epoch [50/120    avg_loss:0.065, val_acc:0.945]
Epoch [51/120    avg_loss:0.077, val_acc:0.940]
Epoch [52/120    avg_loss:0.122, val_acc:0.930]
Epoch [53/120    avg_loss:0.080, val_acc:0.931]
Epoch [54/120    avg_loss:0.075, val_acc:0.956]
Epoch [55/120    avg_loss:0.082, val_acc:0.950]
Epoch [56/120    avg_loss:0.060, val_acc:0.954]
Epoch [57/120    avg_loss:0.068, val_acc:0.967]
Epoch [58/120    avg_loss:0.053, val_acc:0.944]
Epoch [59/120    avg_loss:0.063, val_acc:0.945]
Epoch [60/120    avg_loss:0.088, val_acc:0.948]
Epoch [61/120    avg_loss:0.067, val_acc:0.946]
Epoch [62/120    avg_loss:0.058, val_acc:0.951]
Epoch [63/120    avg_loss:0.048, val_acc:0.964]
Epoch [64/120    avg_loss:0.042, val_acc:0.972]
Epoch [65/120    avg_loss:0.043, val_acc:0.955]
Epoch [66/120    avg_loss:0.039, val_acc:0.972]
Epoch [67/120    avg_loss:0.030, val_acc:0.971]
Epoch [68/120    avg_loss:0.028, val_acc:0.967]
Epoch [69/120    avg_loss:0.045, val_acc:0.963]
Epoch [70/120    avg_loss:0.041, val_acc:0.967]
Epoch [71/120    avg_loss:0.033, val_acc:0.968]
Epoch [72/120    avg_loss:0.032, val_acc:0.969]
Epoch [73/120    avg_loss:0.050, val_acc:0.959]
Epoch [74/120    avg_loss:0.038, val_acc:0.965]
Epoch [75/120    avg_loss:0.043, val_acc:0.954]
Epoch [76/120    avg_loss:0.031, val_acc:0.967]
Epoch [77/120    avg_loss:0.030, val_acc:0.973]
Epoch [78/120    avg_loss:0.046, val_acc:0.960]
Epoch [79/120    avg_loss:0.028, val_acc:0.967]
Epoch [80/120    avg_loss:0.024, val_acc:0.976]
Epoch [81/120    avg_loss:0.024, val_acc:0.959]
Epoch [82/120    avg_loss:0.028, val_acc:0.970]
Epoch [83/120    avg_loss:0.019, val_acc:0.970]
Epoch [84/120    avg_loss:0.028, val_acc:0.969]
Epoch [85/120    avg_loss:0.022, val_acc:0.970]
Epoch [86/120    avg_loss:0.021, val_acc:0.966]
Epoch [87/120    avg_loss:0.049, val_acc:0.961]
Epoch [88/120    avg_loss:0.050, val_acc:0.959]
Epoch [89/120    avg_loss:0.051, val_acc:0.947]
Epoch [90/120    avg_loss:0.030, val_acc:0.974]
Epoch [91/120    avg_loss:0.020, val_acc:0.974]
Epoch [92/120    avg_loss:0.019, val_acc:0.973]
Epoch [93/120    avg_loss:0.023, val_acc:0.970]
Epoch [94/120    avg_loss:0.021, val_acc:0.971]
Epoch [95/120    avg_loss:0.015, val_acc:0.973]
Epoch [96/120    avg_loss:0.019, val_acc:0.974]
Epoch [97/120    avg_loss:0.014, val_acc:0.976]
Epoch [98/120    avg_loss:0.015, val_acc:0.978]
Epoch [99/120    avg_loss:0.013, val_acc:0.979]
Epoch [100/120    avg_loss:0.012, val_acc:0.978]
Epoch [101/120    avg_loss:0.012, val_acc:0.978]
Epoch [102/120    avg_loss:0.013, val_acc:0.979]
Epoch [103/120    avg_loss:0.010, val_acc:0.980]
Epoch [104/120    avg_loss:0.011, val_acc:0.980]
Epoch [105/120    avg_loss:0.012, val_acc:0.982]
Epoch [106/120    avg_loss:0.012, val_acc:0.981]
Epoch [107/120    avg_loss:0.011, val_acc:0.980]
Epoch [108/120    avg_loss:0.012, val_acc:0.980]
Epoch [109/120    avg_loss:0.013, val_acc:0.981]
Epoch [110/120    avg_loss:0.013, val_acc:0.981]
Epoch [111/120    avg_loss:0.010, val_acc:0.979]
Epoch [112/120    avg_loss:0.012, val_acc:0.980]
Epoch [113/120    avg_loss:0.011, val_acc:0.980]
Epoch [114/120    avg_loss:0.012, val_acc:0.979]
Epoch [115/120    avg_loss:0.010, val_acc:0.982]
Epoch [116/120    avg_loss:0.010, val_acc:0.980]
Epoch [117/120    avg_loss:0.009, val_acc:0.980]
Epoch [118/120    avg_loss:0.011, val_acc:0.979]
Epoch [119/120    avg_loss:0.010, val_acc:0.979]
Epoch [120/120    avg_loss:0.010, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1256    1    3    0    2    0    0    0    4   19    0    0
     0    0    0]
 [   0    0    0  728    7    0    0    0    0    6    0    1    4    0
     0    1    0]
 [   0    0    0    1  209    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    1    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  846   24    0    0
     0    0    0]
 [   0    0   12    0    0    0    0    0    0    1   10 2167   20    0
     0    0    0]
 [   0    0    0   11    0    0    0    0    0    0    8    0  511    0
     2    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1135    4    0]
 [   0    0    0    0    0    1    2    0    0    3    0    0    0    0
    82  259    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.25745257452574

F1 scores:
[       nan 0.95238095 0.98163345 0.97783747 0.96759259 0.99190751
 0.99620349 1.         0.99649942 0.72727273 0.9707401  0.97965642
 0.95246971 1.         0.96064325 0.84640523 0.98809524]

Kappa:
0.9687090415810405
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3eca0ccac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.803, val_acc:0.334]
Epoch [2/120    avg_loss:2.660, val_acc:0.430]
Epoch [3/120    avg_loss:2.521, val_acc:0.483]
Epoch [4/120    avg_loss:2.419, val_acc:0.516]
Epoch [5/120    avg_loss:2.342, val_acc:0.535]
Epoch [6/120    avg_loss:2.204, val_acc:0.547]
Epoch [7/120    avg_loss:2.092, val_acc:0.560]
Epoch [8/120    avg_loss:2.019, val_acc:0.571]
Epoch [9/120    avg_loss:1.917, val_acc:0.553]
Epoch [10/120    avg_loss:1.798, val_acc:0.566]
Epoch [11/120    avg_loss:1.701, val_acc:0.555]
Epoch [12/120    avg_loss:1.570, val_acc:0.535]
Epoch [13/120    avg_loss:1.501, val_acc:0.595]
Epoch [14/120    avg_loss:1.332, val_acc:0.668]
Epoch [15/120    avg_loss:1.239, val_acc:0.644]
Epoch [16/120    avg_loss:1.140, val_acc:0.698]
Epoch [17/120    avg_loss:1.067, val_acc:0.650]
Epoch [18/120    avg_loss:0.975, val_acc:0.683]
Epoch [19/120    avg_loss:0.883, val_acc:0.695]
Epoch [20/120    avg_loss:0.860, val_acc:0.730]
Epoch [21/120    avg_loss:0.779, val_acc:0.755]
Epoch [22/120    avg_loss:0.719, val_acc:0.740]
Epoch [23/120    avg_loss:0.665, val_acc:0.747]
Epoch [24/120    avg_loss:0.698, val_acc:0.771]
Epoch [25/120    avg_loss:0.575, val_acc:0.784]
Epoch [26/120    avg_loss:0.493, val_acc:0.822]
Epoch [27/120    avg_loss:0.471, val_acc:0.777]
Epoch [28/120    avg_loss:0.480, val_acc:0.820]
Epoch [29/120    avg_loss:0.410, val_acc:0.836]
Epoch [30/120    avg_loss:0.440, val_acc:0.834]
Epoch [31/120    avg_loss:0.383, val_acc:0.867]
Epoch [32/120    avg_loss:0.383, val_acc:0.852]
Epoch [33/120    avg_loss:0.323, val_acc:0.880]
Epoch [34/120    avg_loss:0.289, val_acc:0.867]
Epoch [35/120    avg_loss:0.282, val_acc:0.900]
Epoch [36/120    avg_loss:0.216, val_acc:0.885]
Epoch [37/120    avg_loss:0.226, val_acc:0.893]
Epoch [38/120    avg_loss:0.178, val_acc:0.925]
Epoch [39/120    avg_loss:0.198, val_acc:0.901]
Epoch [40/120    avg_loss:0.164, val_acc:0.926]
Epoch [41/120    avg_loss:0.147, val_acc:0.903]
Epoch [42/120    avg_loss:0.145, val_acc:0.909]
Epoch [43/120    avg_loss:0.172, val_acc:0.934]
Epoch [44/120    avg_loss:0.169, val_acc:0.912]
Epoch [45/120    avg_loss:0.140, val_acc:0.892]
Epoch [46/120    avg_loss:0.147, val_acc:0.928]
Epoch [47/120    avg_loss:0.106, val_acc:0.942]
Epoch [48/120    avg_loss:0.122, val_acc:0.941]
Epoch [49/120    avg_loss:0.103, val_acc:0.957]
Epoch [50/120    avg_loss:0.084, val_acc:0.947]
Epoch [51/120    avg_loss:0.099, val_acc:0.942]
Epoch [52/120    avg_loss:0.130, val_acc:0.936]
Epoch [53/120    avg_loss:0.129, val_acc:0.936]
Epoch [54/120    avg_loss:0.151, val_acc:0.936]
Epoch [55/120    avg_loss:0.106, val_acc:0.926]
Epoch [56/120    avg_loss:0.088, val_acc:0.944]
Epoch [57/120    avg_loss:0.087, val_acc:0.945]
Epoch [58/120    avg_loss:0.081, val_acc:0.961]
Epoch [59/120    avg_loss:0.061, val_acc:0.952]
Epoch [60/120    avg_loss:0.060, val_acc:0.952]
Epoch [61/120    avg_loss:0.052, val_acc:0.954]
Epoch [62/120    avg_loss:0.052, val_acc:0.959]
Epoch [63/120    avg_loss:0.047, val_acc:0.957]
Epoch [64/120    avg_loss:0.049, val_acc:0.964]
Epoch [65/120    avg_loss:0.039, val_acc:0.955]
Epoch [66/120    avg_loss:0.043, val_acc:0.954]
Epoch [67/120    avg_loss:0.038, val_acc:0.971]
Epoch [68/120    avg_loss:0.043, val_acc:0.964]
Epoch [69/120    avg_loss:0.035, val_acc:0.970]
Epoch [70/120    avg_loss:0.035, val_acc:0.967]
Epoch [71/120    avg_loss:0.037, val_acc:0.964]
Epoch [72/120    avg_loss:0.039, val_acc:0.972]
Epoch [73/120    avg_loss:0.029, val_acc:0.971]
Epoch [74/120    avg_loss:0.035, val_acc:0.968]
Epoch [75/120    avg_loss:0.024, val_acc:0.961]
Epoch [76/120    avg_loss:0.046, val_acc:0.967]
Epoch [77/120    avg_loss:0.037, val_acc:0.951]
Epoch [78/120    avg_loss:0.039, val_acc:0.960]
Epoch [79/120    avg_loss:0.034, val_acc:0.960]
Epoch [80/120    avg_loss:0.045, val_acc:0.965]
Epoch [81/120    avg_loss:0.029, val_acc:0.956]
Epoch [82/120    avg_loss:0.038, val_acc:0.958]
Epoch [83/120    avg_loss:0.027, val_acc:0.956]
Epoch [84/120    avg_loss:0.033, val_acc:0.960]
Epoch [85/120    avg_loss:0.033, val_acc:0.965]
Epoch [86/120    avg_loss:0.024, val_acc:0.965]
Epoch [87/120    avg_loss:0.021, val_acc:0.967]
Epoch [88/120    avg_loss:0.023, val_acc:0.969]
Epoch [89/120    avg_loss:0.017, val_acc:0.973]
Epoch [90/120    avg_loss:0.020, val_acc:0.972]
Epoch [91/120    avg_loss:0.016, val_acc:0.972]
Epoch [92/120    avg_loss:0.016, val_acc:0.970]
Epoch [93/120    avg_loss:0.019, val_acc:0.971]
Epoch [94/120    avg_loss:0.015, val_acc:0.971]
Epoch [95/120    avg_loss:0.019, val_acc:0.971]
Epoch [96/120    avg_loss:0.018, val_acc:0.972]
Epoch [97/120    avg_loss:0.018, val_acc:0.969]
Epoch [98/120    avg_loss:0.019, val_acc:0.969]
Epoch [99/120    avg_loss:0.015, val_acc:0.970]
Epoch [100/120    avg_loss:0.016, val_acc:0.968]
Epoch [101/120    avg_loss:0.015, val_acc:0.967]
Epoch [102/120    avg_loss:0.016, val_acc:0.971]
Epoch [103/120    avg_loss:0.012, val_acc:0.971]
Epoch [104/120    avg_loss:0.015, val_acc:0.971]
Epoch [105/120    avg_loss:0.013, val_acc:0.971]
Epoch [106/120    avg_loss:0.014, val_acc:0.971]
Epoch [107/120    avg_loss:0.014, val_acc:0.971]
Epoch [108/120    avg_loss:0.013, val_acc:0.971]
Epoch [109/120    avg_loss:0.015, val_acc:0.971]
Epoch [110/120    avg_loss:0.015, val_acc:0.971]
Epoch [111/120    avg_loss:0.017, val_acc:0.971]
Epoch [112/120    avg_loss:0.016, val_acc:0.971]
Epoch [113/120    avg_loss:0.013, val_acc:0.971]
Epoch [114/120    avg_loss:0.015, val_acc:0.971]
Epoch [115/120    avg_loss:0.014, val_acc:0.971]
Epoch [116/120    avg_loss:0.015, val_acc:0.971]
Epoch [117/120    avg_loss:0.013, val_acc:0.971]
Epoch [118/120    avg_loss:0.017, val_acc:0.971]
Epoch [119/120    avg_loss:0.016, val_acc:0.971]
Epoch [120/120    avg_loss:0.013, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    2    0    0    0    0    0    0    0    0    0    0    1
     0    0    0]
 [   0    0 1247    3    6    0    1    0    0    2    0   25    1    0
     0    0    0]
 [   0    0    7  720    8    0    0    0    0    6    2    1    1    0
     0    2    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    1    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    2    0    0    1  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    1    1    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    1    0    0    0    0  839   24    1    0
     0    0    0]
 [   0    0   20    0    0    0    1    1    0    2   25 2148   12    0
     0    1    0]
 [   0    0    0    8    2    0    0    0    0    0    4    2  513    0
     1    2    2]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1116   23    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    45  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.96476964769647

F1 scores:
[       nan 0.96202532 0.97005056 0.97297297 0.95475113 0.98959538
 0.99167298 0.94339623 0.99649942 0.69565217 0.96160458 0.97370807
 0.96428571 0.99459459 0.9679098  0.88226528 0.97619048]

Kappa:
0.9653978334864175
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc7d455da58>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.781, val_acc:0.175]
Epoch [2/120    avg_loss:2.632, val_acc:0.322]
Epoch [3/120    avg_loss:2.518, val_acc:0.388]
Epoch [4/120    avg_loss:2.388, val_acc:0.408]
Epoch [5/120    avg_loss:2.333, val_acc:0.458]
Epoch [6/120    avg_loss:2.253, val_acc:0.482]
Epoch [7/120    avg_loss:2.153, val_acc:0.460]
Epoch [8/120    avg_loss:2.100, val_acc:0.497]
Epoch [9/120    avg_loss:2.026, val_acc:0.516]
Epoch [10/120    avg_loss:1.950, val_acc:0.559]
Epoch [11/120    avg_loss:1.826, val_acc:0.577]
Epoch [12/120    avg_loss:1.787, val_acc:0.594]
Epoch [13/120    avg_loss:1.653, val_acc:0.619]
Epoch [14/120    avg_loss:1.542, val_acc:0.601]
Epoch [15/120    avg_loss:1.427, val_acc:0.650]
Epoch [16/120    avg_loss:1.308, val_acc:0.702]
Epoch [17/120    avg_loss:1.254, val_acc:0.647]
Epoch [18/120    avg_loss:1.149, val_acc:0.694]
Epoch [19/120    avg_loss:1.037, val_acc:0.716]
Epoch [20/120    avg_loss:0.960, val_acc:0.714]
Epoch [21/120    avg_loss:0.825, val_acc:0.740]
Epoch [22/120    avg_loss:0.785, val_acc:0.714]
Epoch [23/120    avg_loss:0.677, val_acc:0.797]
Epoch [24/120    avg_loss:0.785, val_acc:0.746]
Epoch [25/120    avg_loss:0.675, val_acc:0.771]
Epoch [26/120    avg_loss:0.623, val_acc:0.764]
Epoch [27/120    avg_loss:0.499, val_acc:0.817]
Epoch [28/120    avg_loss:0.424, val_acc:0.826]
Epoch [29/120    avg_loss:0.413, val_acc:0.795]
Epoch [30/120    avg_loss:0.434, val_acc:0.803]
Epoch [31/120    avg_loss:0.399, val_acc:0.816]
Epoch [32/120    avg_loss:0.368, val_acc:0.853]
Epoch [33/120    avg_loss:0.319, val_acc:0.866]
Epoch [34/120    avg_loss:0.321, val_acc:0.852]
Epoch [35/120    avg_loss:0.338, val_acc:0.866]
Epoch [36/120    avg_loss:0.265, val_acc:0.854]
Epoch [37/120    avg_loss:0.243, val_acc:0.884]
Epoch [38/120    avg_loss:0.214, val_acc:0.893]
Epoch [39/120    avg_loss:0.201, val_acc:0.866]
Epoch [40/120    avg_loss:0.188, val_acc:0.889]
Epoch [41/120    avg_loss:0.191, val_acc:0.882]
Epoch [42/120    avg_loss:0.182, val_acc:0.842]
Epoch [43/120    avg_loss:0.173, val_acc:0.914]
Epoch [44/120    avg_loss:0.176, val_acc:0.905]
Epoch [45/120    avg_loss:0.126, val_acc:0.922]
Epoch [46/120    avg_loss:0.132, val_acc:0.919]
Epoch [47/120    avg_loss:0.115, val_acc:0.918]
Epoch [48/120    avg_loss:0.164, val_acc:0.895]
Epoch [49/120    avg_loss:0.166, val_acc:0.916]
Epoch [50/120    avg_loss:0.144, val_acc:0.912]
Epoch [51/120    avg_loss:0.128, val_acc:0.910]
Epoch [52/120    avg_loss:0.122, val_acc:0.908]
Epoch [53/120    avg_loss:0.105, val_acc:0.940]
Epoch [54/120    avg_loss:0.092, val_acc:0.934]
Epoch [55/120    avg_loss:0.077, val_acc:0.943]
Epoch [56/120    avg_loss:0.089, val_acc:0.932]
Epoch [57/120    avg_loss:0.061, val_acc:0.934]
Epoch [58/120    avg_loss:0.073, val_acc:0.944]
Epoch [59/120    avg_loss:0.073, val_acc:0.950]
Epoch [60/120    avg_loss:0.066, val_acc:0.923]
Epoch [61/120    avg_loss:0.066, val_acc:0.943]
Epoch [62/120    avg_loss:0.060, val_acc:0.953]
Epoch [63/120    avg_loss:0.058, val_acc:0.946]
Epoch [64/120    avg_loss:0.070, val_acc:0.943]
Epoch [65/120    avg_loss:0.078, val_acc:0.940]
Epoch [66/120    avg_loss:0.065, val_acc:0.953]
Epoch [67/120    avg_loss:0.063, val_acc:0.945]
Epoch [68/120    avg_loss:0.063, val_acc:0.953]
Epoch [69/120    avg_loss:0.047, val_acc:0.952]
Epoch [70/120    avg_loss:0.042, val_acc:0.956]
Epoch [71/120    avg_loss:0.038, val_acc:0.952]
Epoch [72/120    avg_loss:0.037, val_acc:0.954]
Epoch [73/120    avg_loss:0.032, val_acc:0.951]
Epoch [74/120    avg_loss:0.047, val_acc:0.942]
Epoch [75/120    avg_loss:0.070, val_acc:0.945]
Epoch [76/120    avg_loss:0.051, val_acc:0.944]
Epoch [77/120    avg_loss:0.049, val_acc:0.956]
Epoch [78/120    avg_loss:0.043, val_acc:0.957]
Epoch [79/120    avg_loss:0.033, val_acc:0.961]
Epoch [80/120    avg_loss:0.038, val_acc:0.961]
Epoch [81/120    avg_loss:0.039, val_acc:0.951]
Epoch [82/120    avg_loss:0.046, val_acc:0.965]
Epoch [83/120    avg_loss:0.034, val_acc:0.960]
Epoch [84/120    avg_loss:0.037, val_acc:0.958]
Epoch [85/120    avg_loss:0.028, val_acc:0.956]
Epoch [86/120    avg_loss:0.034, val_acc:0.960]
Epoch [87/120    avg_loss:0.026, val_acc:0.960]
Epoch [88/120    avg_loss:0.031, val_acc:0.966]
Epoch [89/120    avg_loss:0.020, val_acc:0.963]
Epoch [90/120    avg_loss:0.021, val_acc:0.967]
Epoch [91/120    avg_loss:0.021, val_acc:0.966]
Epoch [92/120    avg_loss:0.020, val_acc:0.966]
Epoch [93/120    avg_loss:0.020, val_acc:0.964]
Epoch [94/120    avg_loss:0.016, val_acc:0.967]
Epoch [95/120    avg_loss:0.018, val_acc:0.964]
Epoch [96/120    avg_loss:0.016, val_acc:0.963]
Epoch [97/120    avg_loss:0.025, val_acc:0.933]
Epoch [98/120    avg_loss:0.032, val_acc:0.960]
Epoch [99/120    avg_loss:0.027, val_acc:0.938]
Epoch [100/120    avg_loss:0.027, val_acc:0.957]
Epoch [101/120    avg_loss:0.033, val_acc:0.964]
Epoch [102/120    avg_loss:0.017, val_acc:0.966]
Epoch [103/120    avg_loss:0.016, val_acc:0.961]
Epoch [104/120    avg_loss:0.021, val_acc:0.960]
Epoch [105/120    avg_loss:0.024, val_acc:0.963]
Epoch [106/120    avg_loss:0.047, val_acc:0.945]
Epoch [107/120    avg_loss:0.094, val_acc:0.961]
Epoch [108/120    avg_loss:0.038, val_acc:0.964]
Epoch [109/120    avg_loss:0.042, val_acc:0.963]
Epoch [110/120    avg_loss:0.059, val_acc:0.957]
Epoch [111/120    avg_loss:0.030, val_acc:0.966]
Epoch [112/120    avg_loss:0.024, val_acc:0.965]
Epoch [113/120    avg_loss:0.020, val_acc:0.966]
Epoch [114/120    avg_loss:0.022, val_acc:0.966]
Epoch [115/120    avg_loss:0.023, val_acc:0.966]
Epoch [116/120    avg_loss:0.017, val_acc:0.963]
Epoch [117/120    avg_loss:0.019, val_acc:0.965]
Epoch [118/120    avg_loss:0.017, val_acc:0.965]
Epoch [119/120    avg_loss:0.018, val_acc:0.963]
Epoch [120/120    avg_loss:0.018, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0 1262    5    0    0    0    0    0    0    2   15    1    0
     0    0    0]
 [   0    0    1  727    0    0    0    0    0    1    2    4    7    3
     0    2    0]
 [   0    0    0    4  209    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    2    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    4    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    0    2    0    0    0  844   15    1    0
     3    1    0]
 [   0    0   31    0    0    0    0    0    0    1   34 2120   23    0
     1    0    0]
 [   0    0    0    3    0    0    0    0    0    0    4    6  519    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    3    0    0    0    0
  1128    8    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    80  266    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.86720867208672

F1 scores:
[       nan 0.98765432 0.97527048 0.97846568 0.99052133 0.99071926
 0.99238965 1.         1.         0.87804878 0.95854628 0.96914286
 0.9558011  0.9919571  0.95714892 0.8525641  0.98823529]

Kappa:
0.9642837784471477
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7fd7cd8a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.803, val_acc:0.122]
Epoch [2/120    avg_loss:2.702, val_acc:0.251]
Epoch [3/120    avg_loss:2.587, val_acc:0.289]
Epoch [4/120    avg_loss:2.485, val_acc:0.449]
Epoch [5/120    avg_loss:2.385, val_acc:0.510]
Epoch [6/120    avg_loss:2.273, val_acc:0.552]
Epoch [7/120    avg_loss:2.197, val_acc:0.544]
Epoch [8/120    avg_loss:2.082, val_acc:0.570]
Epoch [9/120    avg_loss:2.022, val_acc:0.553]
Epoch [10/120    avg_loss:1.937, val_acc:0.579]
Epoch [11/120    avg_loss:1.818, val_acc:0.598]
Epoch [12/120    avg_loss:1.710, val_acc:0.613]
Epoch [13/120    avg_loss:1.600, val_acc:0.622]
Epoch [14/120    avg_loss:1.500, val_acc:0.624]
Epoch [15/120    avg_loss:1.345, val_acc:0.622]
Epoch [16/120    avg_loss:1.294, val_acc:0.652]
Epoch [17/120    avg_loss:1.182, val_acc:0.686]
Epoch [18/120    avg_loss:1.077, val_acc:0.676]
Epoch [19/120    avg_loss:1.027, val_acc:0.700]
Epoch [20/120    avg_loss:0.932, val_acc:0.733]
Epoch [21/120    avg_loss:0.794, val_acc:0.725]
Epoch [22/120    avg_loss:0.769, val_acc:0.761]
Epoch [23/120    avg_loss:0.701, val_acc:0.815]
Epoch [24/120    avg_loss:0.623, val_acc:0.808]
Epoch [25/120    avg_loss:0.529, val_acc:0.822]
Epoch [26/120    avg_loss:0.516, val_acc:0.806]
Epoch [27/120    avg_loss:0.469, val_acc:0.819]
Epoch [28/120    avg_loss:0.416, val_acc:0.806]
Epoch [29/120    avg_loss:0.365, val_acc:0.857]
Epoch [30/120    avg_loss:0.322, val_acc:0.867]
Epoch [31/120    avg_loss:0.316, val_acc:0.875]
Epoch [32/120    avg_loss:0.277, val_acc:0.891]
Epoch [33/120    avg_loss:0.248, val_acc:0.893]
Epoch [34/120    avg_loss:0.229, val_acc:0.909]
Epoch [35/120    avg_loss:0.226, val_acc:0.911]
Epoch [36/120    avg_loss:0.182, val_acc:0.893]
Epoch [37/120    avg_loss:0.263, val_acc:0.865]
Epoch [38/120    avg_loss:0.310, val_acc:0.898]
Epoch [39/120    avg_loss:0.218, val_acc:0.885]
Epoch [40/120    avg_loss:0.247, val_acc:0.880]
Epoch [41/120    avg_loss:0.259, val_acc:0.891]
Epoch [42/120    avg_loss:0.211, val_acc:0.897]
Epoch [43/120    avg_loss:0.167, val_acc:0.899]
Epoch [44/120    avg_loss:0.153, val_acc:0.936]
Epoch [45/120    avg_loss:0.141, val_acc:0.909]
Epoch [46/120    avg_loss:0.163, val_acc:0.869]
Epoch [47/120    avg_loss:0.152, val_acc:0.910]
Epoch [48/120    avg_loss:0.132, val_acc:0.945]
Epoch [49/120    avg_loss:0.118, val_acc:0.952]
Epoch [50/120    avg_loss:0.111, val_acc:0.940]
Epoch [51/120    avg_loss:0.114, val_acc:0.944]
Epoch [52/120    avg_loss:0.100, val_acc:0.935]
Epoch [53/120    avg_loss:0.078, val_acc:0.949]
Epoch [54/120    avg_loss:0.072, val_acc:0.949]
Epoch [55/120    avg_loss:0.078, val_acc:0.960]
Epoch [56/120    avg_loss:0.066, val_acc:0.942]
Epoch [57/120    avg_loss:0.090, val_acc:0.947]
Epoch [58/120    avg_loss:0.071, val_acc:0.957]
Epoch [59/120    avg_loss:0.089, val_acc:0.959]
Epoch [60/120    avg_loss:0.078, val_acc:0.949]
Epoch [61/120    avg_loss:0.069, val_acc:0.950]
Epoch [62/120    avg_loss:0.058, val_acc:0.948]
Epoch [63/120    avg_loss:0.049, val_acc:0.961]
Epoch [64/120    avg_loss:0.050, val_acc:0.947]
Epoch [65/120    avg_loss:0.053, val_acc:0.961]
Epoch [66/120    avg_loss:0.065, val_acc:0.958]
Epoch [67/120    avg_loss:0.074, val_acc:0.957]
Epoch [68/120    avg_loss:0.058, val_acc:0.955]
Epoch [69/120    avg_loss:0.051, val_acc:0.965]
Epoch [70/120    avg_loss:0.042, val_acc:0.966]
Epoch [71/120    avg_loss:0.046, val_acc:0.965]
Epoch [72/120    avg_loss:0.044, val_acc:0.960]
Epoch [73/120    avg_loss:0.036, val_acc:0.959]
Epoch [74/120    avg_loss:0.080, val_acc:0.931]
Epoch [75/120    avg_loss:0.198, val_acc:0.909]
Epoch [76/120    avg_loss:0.203, val_acc:0.902]
Epoch [77/120    avg_loss:0.169, val_acc:0.923]
Epoch [78/120    avg_loss:0.131, val_acc:0.933]
Epoch [79/120    avg_loss:0.106, val_acc:0.904]
Epoch [80/120    avg_loss:0.079, val_acc:0.956]
Epoch [81/120    avg_loss:0.070, val_acc:0.944]
Epoch [82/120    avg_loss:0.071, val_acc:0.950]
Epoch [83/120    avg_loss:0.049, val_acc:0.953]
Epoch [84/120    avg_loss:0.051, val_acc:0.957]
Epoch [85/120    avg_loss:0.038, val_acc:0.956]
Epoch [86/120    avg_loss:0.037, val_acc:0.960]
Epoch [87/120    avg_loss:0.031, val_acc:0.954]
Epoch [88/120    avg_loss:0.033, val_acc:0.953]
Epoch [89/120    avg_loss:0.029, val_acc:0.957]
Epoch [90/120    avg_loss:0.029, val_acc:0.957]
Epoch [91/120    avg_loss:0.030, val_acc:0.956]
Epoch [92/120    avg_loss:0.029, val_acc:0.961]
Epoch [93/120    avg_loss:0.028, val_acc:0.961]
Epoch [94/120    avg_loss:0.036, val_acc:0.963]
Epoch [95/120    avg_loss:0.026, val_acc:0.957]
Epoch [96/120    avg_loss:0.032, val_acc:0.961]
Epoch [97/120    avg_loss:0.024, val_acc:0.961]
Epoch [98/120    avg_loss:0.027, val_acc:0.959]
Epoch [99/120    avg_loss:0.027, val_acc:0.959]
Epoch [100/120    avg_loss:0.031, val_acc:0.959]
Epoch [101/120    avg_loss:0.037, val_acc:0.959]
Epoch [102/120    avg_loss:0.031, val_acc:0.959]
Epoch [103/120    avg_loss:0.029, val_acc:0.960]
Epoch [104/120    avg_loss:0.030, val_acc:0.960]
Epoch [105/120    avg_loss:0.033, val_acc:0.961]
Epoch [106/120    avg_loss:0.025, val_acc:0.960]
Epoch [107/120    avg_loss:0.025, val_acc:0.960]
Epoch [108/120    avg_loss:0.026, val_acc:0.959]
Epoch [109/120    avg_loss:0.031, val_acc:0.959]
Epoch [110/120    avg_loss:0.024, val_acc:0.959]
Epoch [111/120    avg_loss:0.026, val_acc:0.959]
Epoch [112/120    avg_loss:0.028, val_acc:0.959]
Epoch [113/120    avg_loss:0.031, val_acc:0.959]
Epoch [114/120    avg_loss:0.031, val_acc:0.959]
Epoch [115/120    avg_loss:0.027, val_acc:0.959]
Epoch [116/120    avg_loss:0.028, val_acc:0.959]
Epoch [117/120    avg_loss:0.028, val_acc:0.959]
Epoch [118/120    avg_loss:0.029, val_acc:0.959]
Epoch [119/120    avg_loss:0.024, val_acc:0.959]
Epoch [120/120    avg_loss:0.030, val_acc:0.959]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1241    8    9    2    0    0    0    0    6   19    0    0
     0    0    0]
 [   0    0    5  705   13    0    0    0    0    8    0    5    9    2
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    2    0    0    0    1    0    0    0
     7    1    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   15    0    1    0    0
     0    0    0]
 [   0    0   18    0    0    3    3    0    0    0  831   17    3    0
     0    0    0]
 [   0    0   17    1    0    1    0    0    0    0   35 2133   23    0
     0    0    0]
 [   0    0    1    6    1    1    0    0    0    2    1    0  520    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    69  276    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.4769647696477

F1 scores:
[       nan 0.98765432 0.9665109  0.95983662 0.94407159 0.97921478
 0.9924357  1.         1.         0.69767442 0.94971429 0.97286203
 0.95412844 0.99191375 0.96157131 0.86656201 0.98224852]

Kappa:
0.9598478556990494
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f04e19789e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.789, val_acc:0.234]
Epoch [2/120    avg_loss:2.650, val_acc:0.353]
Epoch [3/120    avg_loss:2.509, val_acc:0.418]
Epoch [4/120    avg_loss:2.455, val_acc:0.444]
Epoch [5/120    avg_loss:2.366, val_acc:0.474]
Epoch [6/120    avg_loss:2.266, val_acc:0.554]
Epoch [7/120    avg_loss:2.179, val_acc:0.596]
Epoch [8/120    avg_loss:2.091, val_acc:0.594]
Epoch [9/120    avg_loss:1.967, val_acc:0.602]
Epoch [10/120    avg_loss:1.870, val_acc:0.599]
Epoch [11/120    avg_loss:1.747, val_acc:0.614]
Epoch [12/120    avg_loss:1.686, val_acc:0.632]
Epoch [13/120    avg_loss:1.526, val_acc:0.644]
Epoch [14/120    avg_loss:1.370, val_acc:0.636]
Epoch [15/120    avg_loss:1.273, val_acc:0.679]
Epoch [16/120    avg_loss:1.142, val_acc:0.683]
Epoch [17/120    avg_loss:1.054, val_acc:0.649]
Epoch [18/120    avg_loss:1.001, val_acc:0.711]
Epoch [19/120    avg_loss:0.912, val_acc:0.735]
Epoch [20/120    avg_loss:0.859, val_acc:0.641]
Epoch [21/120    avg_loss:0.785, val_acc:0.758]
Epoch [22/120    avg_loss:0.767, val_acc:0.805]
Epoch [23/120    avg_loss:0.664, val_acc:0.781]
Epoch [24/120    avg_loss:1.184, val_acc:0.368]
Epoch [25/120    avg_loss:2.080, val_acc:0.452]
Epoch [26/120    avg_loss:1.904, val_acc:0.510]
Epoch [27/120    avg_loss:1.786, val_acc:0.564]
Epoch [28/120    avg_loss:1.679, val_acc:0.632]
Epoch [29/120    avg_loss:1.530, val_acc:0.613]
Epoch [30/120    avg_loss:1.424, val_acc:0.639]
Epoch [31/120    avg_loss:1.335, val_acc:0.645]
Epoch [32/120    avg_loss:1.263, val_acc:0.669]
Epoch [33/120    avg_loss:1.138, val_acc:0.669]
Epoch [34/120    avg_loss:1.104, val_acc:0.695]
Epoch [35/120    avg_loss:1.084, val_acc:0.698]
Epoch [36/120    avg_loss:0.983, val_acc:0.729]
Epoch [37/120    avg_loss:0.920, val_acc:0.741]
Epoch [38/120    avg_loss:0.879, val_acc:0.753]
Epoch [39/120    avg_loss:0.889, val_acc:0.754]
Epoch [40/120    avg_loss:0.828, val_acc:0.751]
Epoch [41/120    avg_loss:0.831, val_acc:0.753]
Epoch [42/120    avg_loss:0.792, val_acc:0.759]
Epoch [43/120    avg_loss:0.810, val_acc:0.759]
Epoch [44/120    avg_loss:0.781, val_acc:0.766]
Epoch [45/120    avg_loss:0.761, val_acc:0.766]
Epoch [46/120    avg_loss:0.764, val_acc:0.759]
Epoch [47/120    avg_loss:0.756, val_acc:0.774]
Epoch [48/120    avg_loss:0.714, val_acc:0.771]
Epoch [49/120    avg_loss:0.701, val_acc:0.771]
Epoch [50/120    avg_loss:0.731, val_acc:0.773]
Epoch [51/120    avg_loss:0.691, val_acc:0.773]
Epoch [52/120    avg_loss:0.700, val_acc:0.776]
Epoch [53/120    avg_loss:0.721, val_acc:0.775]
Epoch [54/120    avg_loss:0.691, val_acc:0.775]
Epoch [55/120    avg_loss:0.703, val_acc:0.775]
Epoch [56/120    avg_loss:0.689, val_acc:0.777]
Epoch [57/120    avg_loss:0.703, val_acc:0.775]
Epoch [58/120    avg_loss:0.692, val_acc:0.774]
Epoch [59/120    avg_loss:0.721, val_acc:0.776]
Epoch [60/120    avg_loss:0.701, val_acc:0.776]
Epoch [61/120    avg_loss:0.685, val_acc:0.777]
Epoch [62/120    avg_loss:0.683, val_acc:0.777]
Epoch [63/120    avg_loss:0.691, val_acc:0.777]
Epoch [64/120    avg_loss:0.701, val_acc:0.776]
Epoch [65/120    avg_loss:0.693, val_acc:0.776]
Epoch [66/120    avg_loss:0.675, val_acc:0.776]
Epoch [67/120    avg_loss:0.712, val_acc:0.777]
Epoch [68/120    avg_loss:0.683, val_acc:0.778]
Epoch [69/120    avg_loss:0.705, val_acc:0.779]
Epoch [70/120    avg_loss:0.663, val_acc:0.777]
Epoch [71/120    avg_loss:0.667, val_acc:0.777]
Epoch [72/120    avg_loss:0.695, val_acc:0.777]
Epoch [73/120    avg_loss:0.680, val_acc:0.778]
Epoch [74/120    avg_loss:0.675, val_acc:0.776]
Epoch [75/120    avg_loss:0.675, val_acc:0.775]
Epoch [76/120    avg_loss:0.692, val_acc:0.776]
Epoch [77/120    avg_loss:0.682, val_acc:0.776]
Epoch [78/120    avg_loss:0.683, val_acc:0.776]
Epoch [79/120    avg_loss:0.711, val_acc:0.777]
Epoch [80/120    avg_loss:0.688, val_acc:0.777]
Epoch [81/120    avg_loss:0.699, val_acc:0.777]
Epoch [82/120    avg_loss:0.695, val_acc:0.777]
Epoch [83/120    avg_loss:0.709, val_acc:0.777]
Epoch [84/120    avg_loss:0.688, val_acc:0.777]
Epoch [85/120    avg_loss:0.702, val_acc:0.777]
Epoch [86/120    avg_loss:0.717, val_acc:0.778]
Epoch [87/120    avg_loss:0.651, val_acc:0.778]
Epoch [88/120    avg_loss:0.696, val_acc:0.778]
Epoch [89/120    avg_loss:0.690, val_acc:0.778]
Epoch [90/120    avg_loss:0.706, val_acc:0.778]
Epoch [91/120    avg_loss:0.668, val_acc:0.778]
Epoch [92/120    avg_loss:0.701, val_acc:0.778]
Epoch [93/120    avg_loss:0.705, val_acc:0.778]
Epoch [94/120    avg_loss:0.711, val_acc:0.778]
Epoch [95/120    avg_loss:0.727, val_acc:0.778]
Epoch [96/120    avg_loss:0.691, val_acc:0.778]
Epoch [97/120    avg_loss:0.697, val_acc:0.778]
Epoch [98/120    avg_loss:0.714, val_acc:0.778]
Epoch [99/120    avg_loss:0.688, val_acc:0.778]
Epoch [100/120    avg_loss:0.697, val_acc:0.778]
Epoch [101/120    avg_loss:0.708, val_acc:0.778]
Epoch [102/120    avg_loss:0.697, val_acc:0.778]
Epoch [103/120    avg_loss:0.665, val_acc:0.778]
Epoch [104/120    avg_loss:0.678, val_acc:0.778]
Epoch [105/120    avg_loss:0.667, val_acc:0.778]
Epoch [106/120    avg_loss:0.669, val_acc:0.778]
Epoch [107/120    avg_loss:0.699, val_acc:0.778]
Epoch [108/120    avg_loss:0.729, val_acc:0.778]
Epoch [109/120    avg_loss:0.723, val_acc:0.778]
Epoch [110/120    avg_loss:0.716, val_acc:0.778]
Epoch [111/120    avg_loss:0.673, val_acc:0.778]
Epoch [112/120    avg_loss:0.687, val_acc:0.778]
Epoch [113/120    avg_loss:0.720, val_acc:0.778]
Epoch [114/120    avg_loss:0.751, val_acc:0.778]
Epoch [115/120    avg_loss:0.727, val_acc:0.778]
Epoch [116/120    avg_loss:0.668, val_acc:0.778]
Epoch [117/120    avg_loss:0.682, val_acc:0.778]
Epoch [118/120    avg_loss:0.706, val_acc:0.778]
Epoch [119/120    avg_loss:0.667, val_acc:0.778]
Epoch [120/120    avg_loss:0.676, val_acc:0.778]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   34    1    0    0    0    2    0    0    0    1    0    0    0
     3    0    0]
 [   0    0  661   55   37   14    0    0    3    1  197  241   55    0
    11   10    0]
 [   0    0   11  565  107    2   10    0    0    6    1   21   20    2
     0    2    0]
 [   0    0    0    2  205    0    0    0    0    3    0    0    3    0
     0    0    0]
 [   0    0    0    1    1  367    0   39    0    0    0    0    2    0
    21    4    0]
 [   0    0    1    0    0    0  636    5    0    1    0    0    0    0
    10    4    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   25    0    0    0    0    0    0  399    0    0    0    0    1
     0    5    0]
 [   0    0    0    0    1    0    8    0    0    6    0    0    1    0
     0    2    0]
 [   0    0    5    1    1   17    5    0    5   25  630  166    5    3
     0   12    0]
 [   0    0   51  101   18   11   14    4    4   10  265 1509  152    0
    15   18   38]
 [   0    0    0   18   22    1    5    0    0    9    1   10  463    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    1    0    0    0    5    0    1
  1052   79    0]
 [   0    0    0    0    0    5    5    0    0    0    0    0    0    0
    66  271    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
76.86720867208672

F1 scores:
[       nan 0.68       0.6560794  0.75838926 0.67768595 0.86049238
 0.94783905 0.50505051 0.94887039 0.15189873 0.63959391 0.72513215
 0.74919094 0.98143236 0.90807078 0.71883289 0.79047619]

Kappa:
0.7393739976877727
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa63461ab38>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.783, val_acc:0.204]
Epoch [2/120    avg_loss:2.663, val_acc:0.263]
Epoch [3/120    avg_loss:2.560, val_acc:0.373]
Epoch [4/120    avg_loss:2.441, val_acc:0.438]
Epoch [5/120    avg_loss:2.370, val_acc:0.477]
Epoch [6/120    avg_loss:2.301, val_acc:0.485]
Epoch [7/120    avg_loss:2.221, val_acc:0.498]
Epoch [8/120    avg_loss:2.125, val_acc:0.492]
Epoch [9/120    avg_loss:2.036, val_acc:0.526]
Epoch [10/120    avg_loss:1.938, val_acc:0.546]
Epoch [11/120    avg_loss:1.894, val_acc:0.579]
Epoch [12/120    avg_loss:1.785, val_acc:0.584]
Epoch [13/120    avg_loss:1.698, val_acc:0.593]
Epoch [14/120    avg_loss:1.597, val_acc:0.630]
Epoch [15/120    avg_loss:1.468, val_acc:0.645]
Epoch [16/120    avg_loss:1.350, val_acc:0.679]
Epoch [17/120    avg_loss:1.270, val_acc:0.682]
Epoch [18/120    avg_loss:1.148, val_acc:0.696]
Epoch [19/120    avg_loss:1.109, val_acc:0.705]
Epoch [20/120    avg_loss:1.100, val_acc:0.716]
Epoch [21/120    avg_loss:0.958, val_acc:0.730]
Epoch [22/120    avg_loss:0.864, val_acc:0.750]
Epoch [23/120    avg_loss:0.797, val_acc:0.764]
Epoch [24/120    avg_loss:0.712, val_acc:0.784]
Epoch [25/120    avg_loss:0.671, val_acc:0.732]
Epoch [26/120    avg_loss:0.618, val_acc:0.794]
Epoch [27/120    avg_loss:0.565, val_acc:0.804]
Epoch [28/120    avg_loss:0.462, val_acc:0.821]
Epoch [29/120    avg_loss:0.487, val_acc:0.828]
Epoch [30/120    avg_loss:0.416, val_acc:0.845]
Epoch [31/120    avg_loss:0.400, val_acc:0.850]
Epoch [32/120    avg_loss:0.422, val_acc:0.845]
Epoch [33/120    avg_loss:0.325, val_acc:0.859]
Epoch [34/120    avg_loss:0.312, val_acc:0.867]
Epoch [35/120    avg_loss:0.268, val_acc:0.900]
Epoch [36/120    avg_loss:0.254, val_acc:0.880]
Epoch [37/120    avg_loss:0.255, val_acc:0.889]
Epoch [38/120    avg_loss:0.220, val_acc:0.894]
Epoch [39/120    avg_loss:0.224, val_acc:0.891]
Epoch [40/120    avg_loss:0.236, val_acc:0.904]
Epoch [41/120    avg_loss:0.233, val_acc:0.912]
Epoch [42/120    avg_loss:0.192, val_acc:0.912]
Epoch [43/120    avg_loss:0.185, val_acc:0.912]
Epoch [44/120    avg_loss:0.191, val_acc:0.894]
Epoch [45/120    avg_loss:0.177, val_acc:0.909]
Epoch [46/120    avg_loss:0.179, val_acc:0.903]
Epoch [47/120    avg_loss:0.149, val_acc:0.935]
Epoch [48/120    avg_loss:0.161, val_acc:0.921]
Epoch [49/120    avg_loss:0.168, val_acc:0.925]
Epoch [50/120    avg_loss:0.134, val_acc:0.931]
Epoch [51/120    avg_loss:0.099, val_acc:0.940]
Epoch [52/120    avg_loss:0.086, val_acc:0.943]
Epoch [53/120    avg_loss:0.094, val_acc:0.938]
Epoch [54/120    avg_loss:0.070, val_acc:0.948]
Epoch [55/120    avg_loss:0.066, val_acc:0.959]
Epoch [56/120    avg_loss:0.061, val_acc:0.950]
Epoch [57/120    avg_loss:0.056, val_acc:0.955]
Epoch [58/120    avg_loss:0.074, val_acc:0.946]
Epoch [59/120    avg_loss:0.067, val_acc:0.957]
Epoch [60/120    avg_loss:0.055, val_acc:0.958]
Epoch [61/120    avg_loss:0.056, val_acc:0.954]
Epoch [62/120    avg_loss:0.061, val_acc:0.942]
Epoch [63/120    avg_loss:0.086, val_acc:0.962]
Epoch [64/120    avg_loss:0.061, val_acc:0.958]
Epoch [65/120    avg_loss:0.101, val_acc:0.938]
Epoch [66/120    avg_loss:0.354, val_acc:0.873]
Epoch [67/120    avg_loss:0.187, val_acc:0.920]
Epoch [68/120    avg_loss:0.117, val_acc:0.930]
Epoch [69/120    avg_loss:0.085, val_acc:0.944]
Epoch [70/120    avg_loss:0.076, val_acc:0.945]
Epoch [71/120    avg_loss:0.073, val_acc:0.941]
Epoch [72/120    avg_loss:0.071, val_acc:0.957]
Epoch [73/120    avg_loss:0.068, val_acc:0.945]
Epoch [74/120    avg_loss:0.093, val_acc:0.952]
Epoch [75/120    avg_loss:0.061, val_acc:0.943]
Epoch [76/120    avg_loss:0.046, val_acc:0.955]
Epoch [77/120    avg_loss:0.041, val_acc:0.966]
Epoch [78/120    avg_loss:0.028, val_acc:0.969]
Epoch [79/120    avg_loss:0.035, val_acc:0.970]
Epoch [80/120    avg_loss:0.032, val_acc:0.971]
Epoch [81/120    avg_loss:0.029, val_acc:0.972]
Epoch [82/120    avg_loss:0.030, val_acc:0.973]
Epoch [83/120    avg_loss:0.026, val_acc:0.971]
Epoch [84/120    avg_loss:0.026, val_acc:0.972]
Epoch [85/120    avg_loss:0.026, val_acc:0.972]
Epoch [86/120    avg_loss:0.020, val_acc:0.973]
Epoch [87/120    avg_loss:0.028, val_acc:0.971]
Epoch [88/120    avg_loss:0.029, val_acc:0.974]
Epoch [89/120    avg_loss:0.028, val_acc:0.970]
Epoch [90/120    avg_loss:0.025, val_acc:0.970]
Epoch [91/120    avg_loss:0.027, val_acc:0.972]
Epoch [92/120    avg_loss:0.020, val_acc:0.968]
Epoch [93/120    avg_loss:0.024, val_acc:0.971]
Epoch [94/120    avg_loss:0.031, val_acc:0.973]
Epoch [95/120    avg_loss:0.022, val_acc:0.973]
Epoch [96/120    avg_loss:0.023, val_acc:0.973]
Epoch [97/120    avg_loss:0.025, val_acc:0.972]
Epoch [98/120    avg_loss:0.025, val_acc:0.972]
Epoch [99/120    avg_loss:0.024, val_acc:0.974]
Epoch [100/120    avg_loss:0.027, val_acc:0.974]
Epoch [101/120    avg_loss:0.023, val_acc:0.975]
Epoch [102/120    avg_loss:0.023, val_acc:0.977]
Epoch [103/120    avg_loss:0.022, val_acc:0.973]
Epoch [104/120    avg_loss:0.022, val_acc:0.975]
Epoch [105/120    avg_loss:0.021, val_acc:0.976]
Epoch [106/120    avg_loss:0.020, val_acc:0.973]
Epoch [107/120    avg_loss:0.022, val_acc:0.972]
Epoch [108/120    avg_loss:0.021, val_acc:0.975]
Epoch [109/120    avg_loss:0.021, val_acc:0.974]
Epoch [110/120    avg_loss:0.020, val_acc:0.977]
Epoch [111/120    avg_loss:0.018, val_acc:0.974]
Epoch [112/120    avg_loss:0.024, val_acc:0.976]
Epoch [113/120    avg_loss:0.024, val_acc:0.977]
Epoch [114/120    avg_loss:0.020, val_acc:0.975]
Epoch [115/120    avg_loss:0.019, val_acc:0.975]
Epoch [116/120    avg_loss:0.020, val_acc:0.972]
Epoch [117/120    avg_loss:0.022, val_acc:0.976]
Epoch [118/120    avg_loss:0.020, val_acc:0.978]
Epoch [119/120    avg_loss:0.017, val_acc:0.974]
Epoch [120/120    avg_loss:0.018, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    3 1228    6    4    8    1    0    0    0    3   32    0    0
     0    0    0]
 [   0    0    3  722    4    3    0    0    0    1    2    4    8    0
     0    0    0]
 [   0    0    0    2  210    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    0    0    1    0    0
     7    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    0    0
     0    1    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    0  833   33    2    0
     0    0    0]
 [   0    0    1    0    2    0    0    0    0    0   27 2160   20    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    3    1  525    0
     0    2    2]
 [   0    0    0    1    0    0    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    1    0    1    0    0
  1123   12    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    61  267    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.80216802168022

F1 scores:
[       nan 0.96470588 0.97305864 0.97633536 0.96997691 0.976
 0.98271976 1.         0.99883586 0.94736842 0.95582329 0.97209721
 0.96330275 0.99728997 0.96353496 0.84896661 0.98823529]

Kappa:
0.9635219185968867
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f886785ca58>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.807, val_acc:0.140]
Epoch [2/120    avg_loss:2.669, val_acc:0.235]
Epoch [3/120    avg_loss:2.546, val_acc:0.254]
Epoch [4/120    avg_loss:2.441, val_acc:0.333]
Epoch [5/120    avg_loss:2.365, val_acc:0.361]
Epoch [6/120    avg_loss:2.267, val_acc:0.383]
Epoch [7/120    avg_loss:2.186, val_acc:0.414]
Epoch [8/120    avg_loss:2.110, val_acc:0.467]
Epoch [9/120    avg_loss:1.985, val_acc:0.485]
Epoch [10/120    avg_loss:1.888, val_acc:0.542]
Epoch [11/120    avg_loss:1.837, val_acc:0.533]
Epoch [12/120    avg_loss:1.720, val_acc:0.541]
Epoch [13/120    avg_loss:1.631, val_acc:0.603]
Epoch [14/120    avg_loss:1.567, val_acc:0.633]
Epoch [15/120    avg_loss:1.466, val_acc:0.645]
Epoch [16/120    avg_loss:1.339, val_acc:0.658]
Epoch [17/120    avg_loss:1.215, val_acc:0.692]
Epoch [18/120    avg_loss:1.174, val_acc:0.709]
Epoch [19/120    avg_loss:1.111, val_acc:0.689]
Epoch [20/120    avg_loss:1.021, val_acc:0.715]
Epoch [21/120    avg_loss:0.952, val_acc:0.688]
Epoch [22/120    avg_loss:0.872, val_acc:0.764]
Epoch [23/120    avg_loss:0.758, val_acc:0.767]
Epoch [24/120    avg_loss:0.680, val_acc:0.752]
Epoch [25/120    avg_loss:0.615, val_acc:0.803]
Epoch [26/120    avg_loss:0.524, val_acc:0.829]
Epoch [27/120    avg_loss:0.510, val_acc:0.833]
Epoch [28/120    avg_loss:0.437, val_acc:0.863]
Epoch [29/120    avg_loss:0.452, val_acc:0.856]
Epoch [30/120    avg_loss:0.450, val_acc:0.839]
Epoch [31/120    avg_loss:0.369, val_acc:0.835]
Epoch [32/120    avg_loss:0.320, val_acc:0.860]
Epoch [33/120    avg_loss:0.257, val_acc:0.890]
Epoch [34/120    avg_loss:0.248, val_acc:0.860]
Epoch [35/120    avg_loss:0.265, val_acc:0.893]
Epoch [36/120    avg_loss:0.218, val_acc:0.891]
Epoch [37/120    avg_loss:0.214, val_acc:0.897]
Epoch [38/120    avg_loss:0.169, val_acc:0.916]
Epoch [39/120    avg_loss:0.161, val_acc:0.915]
Epoch [40/120    avg_loss:0.154, val_acc:0.917]
Epoch [41/120    avg_loss:0.139, val_acc:0.930]
Epoch [42/120    avg_loss:0.115, val_acc:0.941]
Epoch [43/120    avg_loss:0.118, val_acc:0.922]
Epoch [44/120    avg_loss:0.136, val_acc:0.914]
Epoch [45/120    avg_loss:0.108, val_acc:0.940]
Epoch [46/120    avg_loss:0.100, val_acc:0.921]
Epoch [47/120    avg_loss:0.107, val_acc:0.917]
Epoch [48/120    avg_loss:0.128, val_acc:0.938]
Epoch [49/120    avg_loss:0.136, val_acc:0.930]
Epoch [50/120    avg_loss:0.107, val_acc:0.928]
Epoch [51/120    avg_loss:0.133, val_acc:0.933]
Epoch [52/120    avg_loss:0.130, val_acc:0.930]
Epoch [53/120    avg_loss:0.106, val_acc:0.942]
Epoch [54/120    avg_loss:0.077, val_acc:0.943]
Epoch [55/120    avg_loss:0.156, val_acc:0.940]
Epoch [56/120    avg_loss:0.098, val_acc:0.951]
Epoch [57/120    avg_loss:0.076, val_acc:0.944]
Epoch [58/120    avg_loss:0.090, val_acc:0.949]
Epoch [59/120    avg_loss:0.056, val_acc:0.941]
Epoch [60/120    avg_loss:0.073, val_acc:0.953]
Epoch [61/120    avg_loss:0.053, val_acc:0.950]
Epoch [62/120    avg_loss:0.054, val_acc:0.957]
Epoch [63/120    avg_loss:0.053, val_acc:0.950]
Epoch [64/120    avg_loss:0.044, val_acc:0.966]
Epoch [65/120    avg_loss:0.035, val_acc:0.961]
Epoch [66/120    avg_loss:0.032, val_acc:0.970]
Epoch [67/120    avg_loss:0.030, val_acc:0.967]
Epoch [68/120    avg_loss:0.045, val_acc:0.958]
Epoch [69/120    avg_loss:0.031, val_acc:0.964]
Epoch [70/120    avg_loss:0.030, val_acc:0.961]
Epoch [71/120    avg_loss:0.035, val_acc:0.965]
Epoch [72/120    avg_loss:0.051, val_acc:0.924]
Epoch [73/120    avg_loss:0.090, val_acc:0.939]
Epoch [74/120    avg_loss:0.138, val_acc:0.880]
Epoch [75/120    avg_loss:0.407, val_acc:0.895]
Epoch [76/120    avg_loss:0.160, val_acc:0.929]
Epoch [77/120    avg_loss:0.095, val_acc:0.941]
Epoch [78/120    avg_loss:0.111, val_acc:0.941]
Epoch [79/120    avg_loss:0.062, val_acc:0.928]
Epoch [80/120    avg_loss:0.067, val_acc:0.958]
Epoch [81/120    avg_loss:0.051, val_acc:0.959]
Epoch [82/120    avg_loss:0.041, val_acc:0.961]
Epoch [83/120    avg_loss:0.036, val_acc:0.966]
Epoch [84/120    avg_loss:0.045, val_acc:0.961]
Epoch [85/120    avg_loss:0.036, val_acc:0.964]
Epoch [86/120    avg_loss:0.041, val_acc:0.964]
Epoch [87/120    avg_loss:0.034, val_acc:0.963]
Epoch [88/120    avg_loss:0.039, val_acc:0.965]
Epoch [89/120    avg_loss:0.036, val_acc:0.971]
Epoch [90/120    avg_loss:0.032, val_acc:0.966]
Epoch [91/120    avg_loss:0.034, val_acc:0.970]
Epoch [92/120    avg_loss:0.030, val_acc:0.969]
Epoch [93/120    avg_loss:0.028, val_acc:0.966]
Epoch [94/120    avg_loss:0.029, val_acc:0.969]
Epoch [95/120    avg_loss:0.030, val_acc:0.970]
Epoch [96/120    avg_loss:0.026, val_acc:0.971]
Epoch [97/120    avg_loss:0.032, val_acc:0.971]
Epoch [98/120    avg_loss:0.031, val_acc:0.969]
Epoch [99/120    avg_loss:0.027, val_acc:0.970]
Epoch [100/120    avg_loss:0.030, val_acc:0.971]
Epoch [101/120    avg_loss:0.032, val_acc:0.969]
Epoch [102/120    avg_loss:0.029, val_acc:0.972]
Epoch [103/120    avg_loss:0.026, val_acc:0.971]
Epoch [104/120    avg_loss:0.026, val_acc:0.969]
Epoch [105/120    avg_loss:0.028, val_acc:0.971]
Epoch [106/120    avg_loss:0.028, val_acc:0.970]
Epoch [107/120    avg_loss:0.026, val_acc:0.969]
Epoch [108/120    avg_loss:0.028, val_acc:0.970]
Epoch [109/120    avg_loss:0.025, val_acc:0.970]
Epoch [110/120    avg_loss:0.030, val_acc:0.971]
Epoch [111/120    avg_loss:0.023, val_acc:0.970]
Epoch [112/120    avg_loss:0.025, val_acc:0.971]
Epoch [113/120    avg_loss:0.026, val_acc:0.972]
Epoch [114/120    avg_loss:0.023, val_acc:0.973]
Epoch [115/120    avg_loss:0.023, val_acc:0.971]
Epoch [116/120    avg_loss:0.027, val_acc:0.974]
Epoch [117/120    avg_loss:0.025, val_acc:0.973]
Epoch [118/120    avg_loss:0.022, val_acc:0.970]
Epoch [119/120    avg_loss:0.034, val_acc:0.969]
Epoch [120/120    avg_loss:0.024, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    1    0    2    0
     0    0    0]
 [   0    0 1241    7    6    0    0    0    0    0    1   27    3    0
     0    0    0]
 [   0    0    2  721    3    2    0    0    0    4    9    2    3    1
     0    0    0]
 [   0    0    0    6  207    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    1    2    0    0    0  853   13    0    0
     0    0    0]
 [   0    0    3    0    0    0    1    0    0    0   34 2128   32    4
     0    8    0]
 [   0    0    3    1    0    0    1    0    0    0    3    5  517    1
     0    1    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    0    1    0    0    1    0    0    0    0
    66  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.8780487804878

F1 scores:
[       nan 0.96202532 0.97716535 0.97300945 0.96503497 0.98847926
 0.99468489 1.         1.         0.87804878 0.96058559 0.97036024
 0.94688645 0.98133333 0.96454507 0.86511628 0.98224852]

Kappa:
0.9644210389185195
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f30b08b6a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.797, val_acc:0.111]
Epoch [2/120    avg_loss:2.646, val_acc:0.200]
Epoch [3/120    avg_loss:2.470, val_acc:0.343]
Epoch [4/120    avg_loss:2.403, val_acc:0.384]
Epoch [5/120    avg_loss:2.310, val_acc:0.453]
Epoch [6/120    avg_loss:2.188, val_acc:0.504]
Epoch [7/120    avg_loss:2.097, val_acc:0.524]
Epoch [8/120    avg_loss:2.056, val_acc:0.531]
Epoch [9/120    avg_loss:1.924, val_acc:0.556]
Epoch [10/120    avg_loss:1.869, val_acc:0.577]
Epoch [11/120    avg_loss:1.766, val_acc:0.525]
Epoch [12/120    avg_loss:1.699, val_acc:0.572]
Epoch [13/120    avg_loss:1.596, val_acc:0.596]
Epoch [14/120    avg_loss:1.492, val_acc:0.606]
Epoch [15/120    avg_loss:1.452, val_acc:0.615]
Epoch [16/120    avg_loss:1.329, val_acc:0.665]
Epoch [17/120    avg_loss:1.262, val_acc:0.625]
Epoch [18/120    avg_loss:1.158, val_acc:0.675]
Epoch [19/120    avg_loss:1.087, val_acc:0.680]
Epoch [20/120    avg_loss:1.023, val_acc:0.686]
Epoch [21/120    avg_loss:0.945, val_acc:0.705]
Epoch [22/120    avg_loss:0.832, val_acc:0.716]
Epoch [23/120    avg_loss:0.775, val_acc:0.764]
Epoch [24/120    avg_loss:0.685, val_acc:0.778]
Epoch [25/120    avg_loss:0.676, val_acc:0.790]
Epoch [26/120    avg_loss:0.631, val_acc:0.794]
Epoch [27/120    avg_loss:0.519, val_acc:0.810]
Epoch [28/120    avg_loss:0.478, val_acc:0.807]
Epoch [29/120    avg_loss:0.538, val_acc:0.829]
Epoch [30/120    avg_loss:0.447, val_acc:0.847]
Epoch [31/120    avg_loss:0.413, val_acc:0.844]
Epoch [32/120    avg_loss:0.435, val_acc:0.829]
Epoch [33/120    avg_loss:0.354, val_acc:0.850]
Epoch [34/120    avg_loss:0.328, val_acc:0.863]
Epoch [35/120    avg_loss:0.316, val_acc:0.865]
Epoch [36/120    avg_loss:0.392, val_acc:0.869]
Epoch [37/120    avg_loss:0.317, val_acc:0.895]
Epoch [38/120    avg_loss:0.252, val_acc:0.905]
Epoch [39/120    avg_loss:0.241, val_acc:0.875]
Epoch [40/120    avg_loss:0.240, val_acc:0.889]
Epoch [41/120    avg_loss:0.224, val_acc:0.878]
Epoch [42/120    avg_loss:0.297, val_acc:0.885]
Epoch [43/120    avg_loss:0.276, val_acc:0.892]
Epoch [44/120    avg_loss:0.181, val_acc:0.915]
Epoch [45/120    avg_loss:0.203, val_acc:0.918]
Epoch [46/120    avg_loss:0.176, val_acc:0.925]
Epoch [47/120    avg_loss:0.170, val_acc:0.909]
Epoch [48/120    avg_loss:0.209, val_acc:0.907]
Epoch [49/120    avg_loss:0.164, val_acc:0.910]
Epoch [50/120    avg_loss:0.165, val_acc:0.894]
Epoch [51/120    avg_loss:0.146, val_acc:0.923]
Epoch [52/120    avg_loss:0.122, val_acc:0.943]
Epoch [53/120    avg_loss:0.117, val_acc:0.909]
Epoch [54/120    avg_loss:0.116, val_acc:0.942]
Epoch [55/120    avg_loss:0.087, val_acc:0.935]
Epoch [56/120    avg_loss:0.081, val_acc:0.949]
Epoch [57/120    avg_loss:0.090, val_acc:0.942]
Epoch [58/120    avg_loss:0.093, val_acc:0.941]
Epoch [59/120    avg_loss:0.096, val_acc:0.944]
Epoch [60/120    avg_loss:0.101, val_acc:0.933]
Epoch [61/120    avg_loss:0.084, val_acc:0.941]
Epoch [62/120    avg_loss:0.139, val_acc:0.877]
Epoch [63/120    avg_loss:0.192, val_acc:0.896]
Epoch [64/120    avg_loss:0.121, val_acc:0.891]
Epoch [65/120    avg_loss:0.094, val_acc:0.953]
Epoch [66/120    avg_loss:0.097, val_acc:0.938]
Epoch [67/120    avg_loss:0.082, val_acc:0.947]
Epoch [68/120    avg_loss:0.075, val_acc:0.958]
Epoch [69/120    avg_loss:0.058, val_acc:0.960]
Epoch [70/120    avg_loss:0.056, val_acc:0.966]
Epoch [71/120    avg_loss:0.062, val_acc:0.953]
Epoch [72/120    avg_loss:0.052, val_acc:0.958]
Epoch [73/120    avg_loss:0.060, val_acc:0.947]
Epoch [74/120    avg_loss:0.047, val_acc:0.963]
Epoch [75/120    avg_loss:0.051, val_acc:0.949]
Epoch [76/120    avg_loss:0.054, val_acc:0.954]
Epoch [77/120    avg_loss:0.046, val_acc:0.952]
Epoch [78/120    avg_loss:0.034, val_acc:0.933]
Epoch [79/120    avg_loss:0.053, val_acc:0.940]
Epoch [80/120    avg_loss:0.061, val_acc:0.959]
Epoch [81/120    avg_loss:0.046, val_acc:0.947]
Epoch [82/120    avg_loss:0.039, val_acc:0.965]
Epoch [83/120    avg_loss:0.037, val_acc:0.946]
Epoch [84/120    avg_loss:0.036, val_acc:0.957]
Epoch [85/120    avg_loss:0.030, val_acc:0.965]
Epoch [86/120    avg_loss:0.032, val_acc:0.961]
Epoch [87/120    avg_loss:0.025, val_acc:0.966]
Epoch [88/120    avg_loss:0.026, val_acc:0.969]
Epoch [89/120    avg_loss:0.024, val_acc:0.967]
Epoch [90/120    avg_loss:0.021, val_acc:0.968]
Epoch [91/120    avg_loss:0.022, val_acc:0.968]
Epoch [92/120    avg_loss:0.023, val_acc:0.969]
Epoch [93/120    avg_loss:0.021, val_acc:0.969]
Epoch [94/120    avg_loss:0.024, val_acc:0.968]
Epoch [95/120    avg_loss:0.022, val_acc:0.969]
Epoch [96/120    avg_loss:0.025, val_acc:0.969]
Epoch [97/120    avg_loss:0.019, val_acc:0.968]
Epoch [98/120    avg_loss:0.024, val_acc:0.969]
Epoch [99/120    avg_loss:0.021, val_acc:0.969]
Epoch [100/120    avg_loss:0.020, val_acc:0.972]
Epoch [101/120    avg_loss:0.030, val_acc:0.971]
Epoch [102/120    avg_loss:0.028, val_acc:0.969]
Epoch [103/120    avg_loss:0.022, val_acc:0.971]
Epoch [104/120    avg_loss:0.019, val_acc:0.971]
Epoch [105/120    avg_loss:0.022, val_acc:0.973]
Epoch [106/120    avg_loss:0.019, val_acc:0.972]
Epoch [107/120    avg_loss:0.021, val_acc:0.969]
Epoch [108/120    avg_loss:0.020, val_acc:0.969]
Epoch [109/120    avg_loss:0.020, val_acc:0.970]
Epoch [110/120    avg_loss:0.029, val_acc:0.970]
Epoch [111/120    avg_loss:0.021, val_acc:0.970]
Epoch [112/120    avg_loss:0.019, val_acc:0.969]
Epoch [113/120    avg_loss:0.018, val_acc:0.969]
Epoch [114/120    avg_loss:0.018, val_acc:0.970]
Epoch [115/120    avg_loss:0.022, val_acc:0.971]
Epoch [116/120    avg_loss:0.020, val_acc:0.972]
Epoch [117/120    avg_loss:0.020, val_acc:0.969]
Epoch [118/120    avg_loss:0.023, val_acc:0.970]
Epoch [119/120    avg_loss:0.018, val_acc:0.970]
Epoch [120/120    avg_loss:0.018, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1249    7    4    0    0    0    0    0    1   24    0    0
     0    0    0]
 [   0    0    2  717   20    0    2    0    0    0    0    1    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    0    0    1    0    0
     8    0    0]
 [   0    0    0    0    0    0  653    0    0    3    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    2    0    0
     0    0    0]
 [   0    0    8    0    0    0    0    0    0    1  833   31    2    0
     0    0    0]
 [   0    0   25    0    0    0    1    0    0    0   16 2153   12    0
     3    0    0]
 [   0    0    1    1    0    0    0    0    0    0    0    0  528    0
     1    2    1]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1120   19    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    99  248    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.67208672086721

F1 scores:
[       nan 0.98765432 0.97198444 0.97418478 0.94666667 0.98954704
 0.9946687  1.         1.         0.82051282 0.96523754 0.97376753
 0.97597043 0.99728997 0.94474905 0.80519481 0.98809524]

Kappa:
0.9620314563466823
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7afd4a2a20>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.811, val_acc:0.169]
Epoch [2/120    avg_loss:2.673, val_acc:0.263]
Epoch [3/120    avg_loss:2.551, val_acc:0.334]
Epoch [4/120    avg_loss:2.423, val_acc:0.418]
Epoch [5/120    avg_loss:2.324, val_acc:0.530]
Epoch [6/120    avg_loss:2.227, val_acc:0.579]
Epoch [7/120    avg_loss:2.151, val_acc:0.601]
Epoch [8/120    avg_loss:2.040, val_acc:0.614]
Epoch [9/120    avg_loss:1.974, val_acc:0.620]
Epoch [10/120    avg_loss:1.895, val_acc:0.594]
Epoch [11/120    avg_loss:1.842, val_acc:0.625]
Epoch [12/120    avg_loss:1.660, val_acc:0.672]
Epoch [13/120    avg_loss:1.544, val_acc:0.647]
Epoch [14/120    avg_loss:1.495, val_acc:0.662]
Epoch [15/120    avg_loss:1.332, val_acc:0.696]
Epoch [16/120    avg_loss:1.250, val_acc:0.705]
Epoch [17/120    avg_loss:1.172, val_acc:0.696]
Epoch [18/120    avg_loss:1.067, val_acc:0.722]
Epoch [19/120    avg_loss:0.973, val_acc:0.742]
Epoch [20/120    avg_loss:0.831, val_acc:0.771]
Epoch [21/120    avg_loss:0.781, val_acc:0.755]
Epoch [22/120    avg_loss:0.718, val_acc:0.793]
Epoch [23/120    avg_loss:0.718, val_acc:0.803]
Epoch [24/120    avg_loss:0.667, val_acc:0.808]
Epoch [25/120    avg_loss:0.593, val_acc:0.786]
Epoch [26/120    avg_loss:0.513, val_acc:0.821]
Epoch [27/120    avg_loss:0.579, val_acc:0.792]
Epoch [28/120    avg_loss:0.527, val_acc:0.818]
Epoch [29/120    avg_loss:0.446, val_acc:0.811]
Epoch [30/120    avg_loss:0.419, val_acc:0.845]
Epoch [31/120    avg_loss:0.425, val_acc:0.840]
Epoch [32/120    avg_loss:0.411, val_acc:0.826]
Epoch [33/120    avg_loss:0.352, val_acc:0.863]
Epoch [34/120    avg_loss:0.331, val_acc:0.872]
Epoch [35/120    avg_loss:0.297, val_acc:0.844]
Epoch [36/120    avg_loss:0.293, val_acc:0.871]
Epoch [37/120    avg_loss:0.227, val_acc:0.893]
Epoch [38/120    avg_loss:0.385, val_acc:0.846]
Epoch [39/120    avg_loss:0.342, val_acc:0.826]
Epoch [40/120    avg_loss:0.275, val_acc:0.883]
Epoch [41/120    avg_loss:0.254, val_acc:0.897]
Epoch [42/120    avg_loss:0.243, val_acc:0.881]
Epoch [43/120    avg_loss:0.224, val_acc:0.890]
Epoch [44/120    avg_loss:0.152, val_acc:0.922]
Epoch [45/120    avg_loss:0.145, val_acc:0.919]
Epoch [46/120    avg_loss:0.181, val_acc:0.911]
Epoch [47/120    avg_loss:0.177, val_acc:0.899]
Epoch [48/120    avg_loss:0.150, val_acc:0.922]
Epoch [49/120    avg_loss:0.150, val_acc:0.914]
Epoch [50/120    avg_loss:0.163, val_acc:0.925]
Epoch [51/120    avg_loss:0.159, val_acc:0.921]
Epoch [52/120    avg_loss:0.128, val_acc:0.929]
Epoch [53/120    avg_loss:0.096, val_acc:0.939]
Epoch [54/120    avg_loss:0.099, val_acc:0.930]
Epoch [55/120    avg_loss:0.109, val_acc:0.933]
Epoch [56/120    avg_loss:0.089, val_acc:0.943]
Epoch [57/120    avg_loss:0.081, val_acc:0.936]
Epoch [58/120    avg_loss:0.080, val_acc:0.942]
Epoch [59/120    avg_loss:0.088, val_acc:0.951]
Epoch [60/120    avg_loss:0.070, val_acc:0.919]
Epoch [61/120    avg_loss:0.094, val_acc:0.954]
Epoch [62/120    avg_loss:0.080, val_acc:0.952]
Epoch [63/120    avg_loss:0.092, val_acc:0.923]
Epoch [64/120    avg_loss:0.183, val_acc:0.939]
Epoch [65/120    avg_loss:0.104, val_acc:0.943]
Epoch [66/120    avg_loss:0.071, val_acc:0.951]
Epoch [67/120    avg_loss:0.073, val_acc:0.946]
Epoch [68/120    avg_loss:0.078, val_acc:0.931]
Epoch [69/120    avg_loss:0.095, val_acc:0.935]
Epoch [70/120    avg_loss:0.068, val_acc:0.947]
Epoch [71/120    avg_loss:0.151, val_acc:0.936]
Epoch [72/120    avg_loss:0.099, val_acc:0.927]
Epoch [73/120    avg_loss:0.071, val_acc:0.953]
Epoch [74/120    avg_loss:0.061, val_acc:0.952]
Epoch [75/120    avg_loss:0.053, val_acc:0.959]
Epoch [76/120    avg_loss:0.046, val_acc:0.961]
Epoch [77/120    avg_loss:0.043, val_acc:0.965]
Epoch [78/120    avg_loss:0.039, val_acc:0.966]
Epoch [79/120    avg_loss:0.035, val_acc:0.969]
Epoch [80/120    avg_loss:0.037, val_acc:0.967]
Epoch [81/120    avg_loss:0.040, val_acc:0.969]
Epoch [82/120    avg_loss:0.036, val_acc:0.971]
Epoch [83/120    avg_loss:0.038, val_acc:0.971]
Epoch [84/120    avg_loss:0.033, val_acc:0.969]
Epoch [85/120    avg_loss:0.034, val_acc:0.969]
Epoch [86/120    avg_loss:0.034, val_acc:0.969]
Epoch [87/120    avg_loss:0.033, val_acc:0.969]
Epoch [88/120    avg_loss:0.028, val_acc:0.970]
Epoch [89/120    avg_loss:0.035, val_acc:0.969]
Epoch [90/120    avg_loss:0.026, val_acc:0.972]
Epoch [91/120    avg_loss:0.035, val_acc:0.971]
Epoch [92/120    avg_loss:0.031, val_acc:0.971]
Epoch [93/120    avg_loss:0.034, val_acc:0.970]
Epoch [94/120    avg_loss:0.037, val_acc:0.971]
Epoch [95/120    avg_loss:0.031, val_acc:0.973]
Epoch [96/120    avg_loss:0.028, val_acc:0.972]
Epoch [97/120    avg_loss:0.029, val_acc:0.970]
Epoch [98/120    avg_loss:0.029, val_acc:0.971]
Epoch [99/120    avg_loss:0.025, val_acc:0.968]
Epoch [100/120    avg_loss:0.030, val_acc:0.969]
Epoch [101/120    avg_loss:0.030, val_acc:0.971]
Epoch [102/120    avg_loss:0.026, val_acc:0.973]
Epoch [103/120    avg_loss:0.027, val_acc:0.972]
Epoch [104/120    avg_loss:0.033, val_acc:0.972]
Epoch [105/120    avg_loss:0.028, val_acc:0.969]
Epoch [106/120    avg_loss:0.032, val_acc:0.974]
Epoch [107/120    avg_loss:0.032, val_acc:0.975]
Epoch [108/120    avg_loss:0.026, val_acc:0.972]
Epoch [109/120    avg_loss:0.033, val_acc:0.970]
Epoch [110/120    avg_loss:0.029, val_acc:0.975]
Epoch [111/120    avg_loss:0.022, val_acc:0.974]
Epoch [112/120    avg_loss:0.025, val_acc:0.975]
Epoch [113/120    avg_loss:0.035, val_acc:0.972]
Epoch [114/120    avg_loss:0.028, val_acc:0.972]
Epoch [115/120    avg_loss:0.028, val_acc:0.972]
Epoch [116/120    avg_loss:0.026, val_acc:0.972]
Epoch [117/120    avg_loss:0.023, val_acc:0.974]
Epoch [118/120    avg_loss:0.027, val_acc:0.974]
Epoch [119/120    avg_loss:0.025, val_acc:0.973]
Epoch [120/120    avg_loss:0.026, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0 1250    1    4    0    1    0    0    0    2   27    0    0
     0    0    0]
 [   0    0    1  717    1    2    0    0    0   15    5    1    5    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   14    0    0    3    1    0    0    0  841   11    0    0
     2    3    0]
 [   0    0    4    0    0    0    2    0    0    0   22 2166   16    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    1    0  526    0
     0    2    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    23  320    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.84281842818429

F1 scores:
[       nan 0.97560976 0.9788567  0.9755102  0.98604651 0.99315068
 0.99242424 1.         0.99883586 0.68       0.96334479 0.98075617
 0.9704797  0.99728997 0.98386393 0.93704246 0.97005988]

Kappa:
0.9754127096110788
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8c492f4ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.810, val_acc:0.158]
Epoch [2/120    avg_loss:2.685, val_acc:0.253]
Epoch [3/120    avg_loss:2.570, val_acc:0.322]
Epoch [4/120    avg_loss:2.439, val_acc:0.434]
Epoch [5/120    avg_loss:2.337, val_acc:0.429]
Epoch [6/120    avg_loss:2.233, val_acc:0.479]
Epoch [7/120    avg_loss:2.149, val_acc:0.445]
Epoch [8/120    avg_loss:2.025, val_acc:0.491]
Epoch [9/120    avg_loss:1.959, val_acc:0.500]
Epoch [10/120    avg_loss:1.858, val_acc:0.520]
Epoch [11/120    avg_loss:1.747, val_acc:0.566]
Epoch [12/120    avg_loss:1.635, val_acc:0.551]
Epoch [13/120    avg_loss:1.551, val_acc:0.586]
Epoch [14/120    avg_loss:1.463, val_acc:0.606]
Epoch [15/120    avg_loss:1.337, val_acc:0.654]
Epoch [16/120    avg_loss:1.291, val_acc:0.630]
Epoch [17/120    avg_loss:1.174, val_acc:0.745]
Epoch [18/120    avg_loss:1.072, val_acc:0.741]
Epoch [19/120    avg_loss:0.965, val_acc:0.738]
Epoch [20/120    avg_loss:0.929, val_acc:0.780]
Epoch [21/120    avg_loss:0.830, val_acc:0.784]
Epoch [22/120    avg_loss:0.733, val_acc:0.793]
Epoch [23/120    avg_loss:0.703, val_acc:0.795]
Epoch [24/120    avg_loss:0.673, val_acc:0.770]
Epoch [25/120    avg_loss:0.658, val_acc:0.774]
Epoch [26/120    avg_loss:0.625, val_acc:0.803]
Epoch [27/120    avg_loss:0.543, val_acc:0.818]
Epoch [28/120    avg_loss:0.492, val_acc:0.868]
Epoch [29/120    avg_loss:0.468, val_acc:0.863]
Epoch [30/120    avg_loss:0.395, val_acc:0.883]
Epoch [31/120    avg_loss:0.348, val_acc:0.879]
Epoch [32/120    avg_loss:0.282, val_acc:0.853]
Epoch [33/120    avg_loss:0.325, val_acc:0.838]
Epoch [34/120    avg_loss:0.310, val_acc:0.916]
Epoch [35/120    avg_loss:0.250, val_acc:0.895]
Epoch [36/120    avg_loss:0.242, val_acc:0.905]
Epoch [37/120    avg_loss:0.202, val_acc:0.907]
Epoch [38/120    avg_loss:0.246, val_acc:0.882]
Epoch [39/120    avg_loss:0.207, val_acc:0.886]
Epoch [40/120    avg_loss:0.179, val_acc:0.918]
Epoch [41/120    avg_loss:0.179, val_acc:0.939]
Epoch [42/120    avg_loss:0.152, val_acc:0.924]
Epoch [43/120    avg_loss:0.158, val_acc:0.933]
Epoch [44/120    avg_loss:0.157, val_acc:0.934]
Epoch [45/120    avg_loss:0.126, val_acc:0.919]
Epoch [46/120    avg_loss:0.113, val_acc:0.932]
Epoch [47/120    avg_loss:0.126, val_acc:0.927]
Epoch [48/120    avg_loss:0.111, val_acc:0.950]
Epoch [49/120    avg_loss:0.086, val_acc:0.941]
Epoch [50/120    avg_loss:0.095, val_acc:0.946]
Epoch [51/120    avg_loss:0.125, val_acc:0.936]
Epoch [52/120    avg_loss:0.093, val_acc:0.930]
Epoch [53/120    avg_loss:0.111, val_acc:0.930]
Epoch [54/120    avg_loss:0.110, val_acc:0.948]
Epoch [55/120    avg_loss:0.083, val_acc:0.940]
Epoch [56/120    avg_loss:0.074, val_acc:0.945]
Epoch [57/120    avg_loss:0.062, val_acc:0.952]
Epoch [58/120    avg_loss:0.068, val_acc:0.938]
Epoch [59/120    avg_loss:0.060, val_acc:0.948]
Epoch [60/120    avg_loss:0.058, val_acc:0.919]
Epoch [61/120    avg_loss:0.054, val_acc:0.955]
Epoch [62/120    avg_loss:0.084, val_acc:0.916]
Epoch [63/120    avg_loss:0.074, val_acc:0.956]
Epoch [64/120    avg_loss:0.063, val_acc:0.948]
Epoch [65/120    avg_loss:0.055, val_acc:0.950]
Epoch [66/120    avg_loss:0.051, val_acc:0.957]
Epoch [67/120    avg_loss:0.042, val_acc:0.963]
Epoch [68/120    avg_loss:0.037, val_acc:0.942]
Epoch [69/120    avg_loss:0.043, val_acc:0.961]
Epoch [70/120    avg_loss:0.036, val_acc:0.965]
Epoch [71/120    avg_loss:0.063, val_acc:0.887]
Epoch [72/120    avg_loss:0.350, val_acc:0.876]
Epoch [73/120    avg_loss:0.207, val_acc:0.921]
Epoch [74/120    avg_loss:0.206, val_acc:0.879]
Epoch [75/120    avg_loss:0.132, val_acc:0.955]
Epoch [76/120    avg_loss:0.087, val_acc:0.933]
Epoch [77/120    avg_loss:0.101, val_acc:0.936]
Epoch [78/120    avg_loss:0.126, val_acc:0.909]
Epoch [79/120    avg_loss:0.123, val_acc:0.958]
Epoch [80/120    avg_loss:0.061, val_acc:0.959]
Epoch [81/120    avg_loss:0.061, val_acc:0.960]
Epoch [82/120    avg_loss:0.039, val_acc:0.967]
Epoch [83/120    avg_loss:0.044, val_acc:0.967]
Epoch [84/120    avg_loss:0.034, val_acc:0.963]
Epoch [85/120    avg_loss:0.039, val_acc:0.964]
Epoch [86/120    avg_loss:0.040, val_acc:0.953]
Epoch [87/120    avg_loss:0.040, val_acc:0.946]
Epoch [88/120    avg_loss:0.039, val_acc:0.955]
Epoch [89/120    avg_loss:0.049, val_acc:0.959]
Epoch [90/120    avg_loss:0.040, val_acc:0.968]
Epoch [91/120    avg_loss:0.039, val_acc:0.963]
Epoch [92/120    avg_loss:0.034, val_acc:0.977]
Epoch [93/120    avg_loss:0.026, val_acc:0.969]
Epoch [94/120    avg_loss:0.039, val_acc:0.967]
Epoch [95/120    avg_loss:0.038, val_acc:0.959]
Epoch [96/120    avg_loss:0.029, val_acc:0.963]
Epoch [97/120    avg_loss:0.028, val_acc:0.969]
Epoch [98/120    avg_loss:0.025, val_acc:0.966]
Epoch [99/120    avg_loss:0.027, val_acc:0.973]
Epoch [100/120    avg_loss:0.030, val_acc:0.961]
Epoch [101/120    avg_loss:0.024, val_acc:0.968]
Epoch [102/120    avg_loss:0.029, val_acc:0.964]
Epoch [103/120    avg_loss:0.030, val_acc:0.964]
Epoch [104/120    avg_loss:0.025, val_acc:0.963]
Epoch [105/120    avg_loss:0.035, val_acc:0.969]
Epoch [106/120    avg_loss:0.026, val_acc:0.970]
Epoch [107/120    avg_loss:0.021, val_acc:0.972]
Epoch [108/120    avg_loss:0.015, val_acc:0.971]
Epoch [109/120    avg_loss:0.012, val_acc:0.971]
Epoch [110/120    avg_loss:0.014, val_acc:0.971]
Epoch [111/120    avg_loss:0.017, val_acc:0.973]
Epoch [112/120    avg_loss:0.013, val_acc:0.974]
Epoch [113/120    avg_loss:0.013, val_acc:0.973]
Epoch [114/120    avg_loss:0.012, val_acc:0.975]
Epoch [115/120    avg_loss:0.012, val_acc:0.974]
Epoch [116/120    avg_loss:0.015, val_acc:0.974]
Epoch [117/120    avg_loss:0.012, val_acc:0.974]
Epoch [118/120    avg_loss:0.013, val_acc:0.974]
Epoch [119/120    avg_loss:0.013, val_acc:0.974]
Epoch [120/120    avg_loss:0.015, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    0    2    0    0
     0    0    0]
 [   0    0 1251    8    0    1    2    0    0    0    2   21    0    0
     0    0    0]
 [   0    0    0  714    5    8    1    0    0    2    4    0   13    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    3    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    1    2    0    0    1  842   21    2    0
     0    0    0]
 [   0    0    6    0    0    0    1    0    0    1   11 2183    5    0
     3    0    0]
 [   0    0    0    3    1    2    0    0    0    0    9    0  517    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1128   10    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    41  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.58265582655827

F1 scores:
[       nan 0.975      0.98194662 0.9701087  0.98611111 0.98527746
 0.98118886 1.         0.9953271  0.9        0.96615032 0.98333333
 0.96007428 1.         0.97535668 0.89846154 0.98203593]

Kappa:
0.9724261372191918
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f93766f1b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.804, val_acc:0.059]
Epoch [2/120    avg_loss:2.687, val_acc:0.267]
Epoch [3/120    avg_loss:2.570, val_acc:0.372]
Epoch [4/120    avg_loss:2.482, val_acc:0.393]
Epoch [5/120    avg_loss:2.386, val_acc:0.423]
Epoch [6/120    avg_loss:2.283, val_acc:0.455]
Epoch [7/120    avg_loss:2.162, val_acc:0.508]
Epoch [8/120    avg_loss:2.112, val_acc:0.519]
Epoch [9/120    avg_loss:2.009, val_acc:0.554]
Epoch [10/120    avg_loss:1.912, val_acc:0.579]
Epoch [11/120    avg_loss:1.832, val_acc:0.601]
Epoch [12/120    avg_loss:1.751, val_acc:0.631]
Epoch [13/120    avg_loss:1.645, val_acc:0.645]
Epoch [14/120    avg_loss:1.538, val_acc:0.650]
Epoch [15/120    avg_loss:1.483, val_acc:0.658]
Epoch [16/120    avg_loss:1.340, val_acc:0.694]
Epoch [17/120    avg_loss:1.197, val_acc:0.733]
Epoch [18/120    avg_loss:1.098, val_acc:0.741]
Epoch [19/120    avg_loss:1.016, val_acc:0.761]
Epoch [20/120    avg_loss:0.908, val_acc:0.755]
Epoch [21/120    avg_loss:0.809, val_acc:0.814]
Epoch [22/120    avg_loss:0.702, val_acc:0.781]
Epoch [23/120    avg_loss:0.712, val_acc:0.777]
Epoch [24/120    avg_loss:0.620, val_acc:0.824]
Epoch [25/120    avg_loss:0.535, val_acc:0.846]
Epoch [26/120    avg_loss:0.457, val_acc:0.868]
Epoch [27/120    avg_loss:0.523, val_acc:0.842]
Epoch [28/120    avg_loss:0.482, val_acc:0.863]
Epoch [29/120    avg_loss:0.468, val_acc:0.875]
Epoch [30/120    avg_loss:0.410, val_acc:0.864]
Epoch [31/120    avg_loss:0.337, val_acc:0.882]
Epoch [32/120    avg_loss:0.299, val_acc:0.901]
Epoch [33/120    avg_loss:0.260, val_acc:0.893]
Epoch [34/120    avg_loss:0.385, val_acc:0.908]
Epoch [35/120    avg_loss:0.297, val_acc:0.863]
Epoch [36/120    avg_loss:0.260, val_acc:0.910]
Epoch [37/120    avg_loss:0.322, val_acc:0.883]
Epoch [38/120    avg_loss:0.235, val_acc:0.925]
Epoch [39/120    avg_loss:0.216, val_acc:0.909]
Epoch [40/120    avg_loss:0.196, val_acc:0.909]
Epoch [41/120    avg_loss:0.189, val_acc:0.890]
Epoch [42/120    avg_loss:0.172, val_acc:0.927]
Epoch [43/120    avg_loss:0.167, val_acc:0.904]
Epoch [44/120    avg_loss:0.176, val_acc:0.926]
Epoch [45/120    avg_loss:0.132, val_acc:0.922]
Epoch [46/120    avg_loss:0.178, val_acc:0.919]
Epoch [47/120    avg_loss:0.140, val_acc:0.934]
Epoch [48/120    avg_loss:0.126, val_acc:0.940]
Epoch [49/120    avg_loss:0.133, val_acc:0.931]
Epoch [50/120    avg_loss:0.161, val_acc:0.935]
Epoch [51/120    avg_loss:0.130, val_acc:0.924]
Epoch [52/120    avg_loss:0.107, val_acc:0.936]
Epoch [53/120    avg_loss:0.110, val_acc:0.940]
Epoch [54/120    avg_loss:0.119, val_acc:0.933]
Epoch [55/120    avg_loss:0.112, val_acc:0.923]
Epoch [56/120    avg_loss:0.130, val_acc:0.940]
Epoch [57/120    avg_loss:0.140, val_acc:0.941]
Epoch [58/120    avg_loss:0.089, val_acc:0.948]
Epoch [59/120    avg_loss:0.108, val_acc:0.933]
Epoch [60/120    avg_loss:0.100, val_acc:0.926]
Epoch [61/120    avg_loss:0.097, val_acc:0.938]
Epoch [62/120    avg_loss:0.079, val_acc:0.932]
Epoch [63/120    avg_loss:0.083, val_acc:0.929]
Epoch [64/120    avg_loss:0.052, val_acc:0.954]
Epoch [65/120    avg_loss:0.102, val_acc:0.959]
Epoch [66/120    avg_loss:0.097, val_acc:0.953]
Epoch [67/120    avg_loss:0.056, val_acc:0.964]
Epoch [68/120    avg_loss:0.063, val_acc:0.946]
Epoch [69/120    avg_loss:0.072, val_acc:0.967]
Epoch [70/120    avg_loss:0.059, val_acc:0.960]
Epoch [71/120    avg_loss:0.076, val_acc:0.935]
Epoch [72/120    avg_loss:0.094, val_acc:0.945]
Epoch [73/120    avg_loss:0.055, val_acc:0.958]
Epoch [74/120    avg_loss:0.045, val_acc:0.956]
Epoch [75/120    avg_loss:0.033, val_acc:0.957]
Epoch [76/120    avg_loss:0.033, val_acc:0.960]
Epoch [77/120    avg_loss:0.043, val_acc:0.967]
Epoch [78/120    avg_loss:0.027, val_acc:0.961]
Epoch [79/120    avg_loss:0.038, val_acc:0.970]
Epoch [80/120    avg_loss:0.047, val_acc:0.965]
Epoch [81/120    avg_loss:0.041, val_acc:0.964]
Epoch [82/120    avg_loss:0.038, val_acc:0.961]
Epoch [83/120    avg_loss:0.036, val_acc:0.969]
Epoch [84/120    avg_loss:0.030, val_acc:0.976]
Epoch [85/120    avg_loss:0.034, val_acc:0.976]
Epoch [86/120    avg_loss:0.037, val_acc:0.968]
Epoch [87/120    avg_loss:0.038, val_acc:0.973]
Epoch [88/120    avg_loss:0.055, val_acc:0.967]
Epoch [89/120    avg_loss:0.034, val_acc:0.969]
Epoch [90/120    avg_loss:0.030, val_acc:0.973]
Epoch [91/120    avg_loss:0.054, val_acc:0.950]
Epoch [92/120    avg_loss:0.069, val_acc:0.959]
Epoch [93/120    avg_loss:0.047, val_acc:0.969]
Epoch [94/120    avg_loss:0.057, val_acc:0.949]
Epoch [95/120    avg_loss:0.039, val_acc:0.975]
Epoch [96/120    avg_loss:0.040, val_acc:0.968]
Epoch [97/120    avg_loss:0.032, val_acc:0.971]
Epoch [98/120    avg_loss:0.024, val_acc:0.972]
Epoch [99/120    avg_loss:0.023, val_acc:0.975]
Epoch [100/120    avg_loss:0.018, val_acc:0.981]
Epoch [101/120    avg_loss:0.015, val_acc:0.978]
Epoch [102/120    avg_loss:0.018, val_acc:0.979]
Epoch [103/120    avg_loss:0.016, val_acc:0.982]
Epoch [104/120    avg_loss:0.017, val_acc:0.981]
Epoch [105/120    avg_loss:0.018, val_acc:0.980]
Epoch [106/120    avg_loss:0.014, val_acc:0.980]
Epoch [107/120    avg_loss:0.018, val_acc:0.982]
Epoch [108/120    avg_loss:0.015, val_acc:0.979]
Epoch [109/120    avg_loss:0.014, val_acc:0.979]
Epoch [110/120    avg_loss:0.013, val_acc:0.979]
Epoch [111/120    avg_loss:0.015, val_acc:0.979]
Epoch [112/120    avg_loss:0.013, val_acc:0.981]
Epoch [113/120    avg_loss:0.016, val_acc:0.981]
Epoch [114/120    avg_loss:0.014, val_acc:0.978]
Epoch [115/120    avg_loss:0.014, val_acc:0.979]
Epoch [116/120    avg_loss:0.013, val_acc:0.979]
Epoch [117/120    avg_loss:0.015, val_acc:0.982]
Epoch [118/120    avg_loss:0.012, val_acc:0.979]
Epoch [119/120    avg_loss:0.013, val_acc:0.980]
Epoch [120/120    avg_loss:0.014, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1244    2    5    0    1    0    0    0    4   29    0    0
     0    0    0]
 [   0    0    1  709    6    0    0    0    0   12    2    5   12    0
     0    0    0]
 [   0    0    0    0  209    0    0    0    0    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    1    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    4    0    0    2    0    0    0    0  858   10    0    0
     0    1    0]
 [   0    0   12    0    0    0    1    0    0    1    8 2187    1    0
     0    0    0]
 [   0    0    0    1    0    1    0    0    0    0    7    3  517    0
     0    3    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    0    2    0    0    2    0    0    0    0
    12  331    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.03794037940379

F1 scores:
[       nan 0.95348837 0.97721917 0.97189856 0.96535797 0.98847926
 0.99544073 1.         0.9953271  0.66666667 0.97833523 0.984027
 0.96635514 0.99728997 0.98775153 0.9566474  0.98224852]

Kappa:
0.9776272023634068
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1685816ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.804, val_acc:0.126]
Epoch [2/120    avg_loss:2.659, val_acc:0.261]
Epoch [3/120    avg_loss:2.528, val_acc:0.368]
Epoch [4/120    avg_loss:2.439, val_acc:0.378]
Epoch [5/120    avg_loss:2.328, val_acc:0.453]
Epoch [6/120    avg_loss:2.246, val_acc:0.547]
Epoch [7/120    avg_loss:2.143, val_acc:0.525]
Epoch [8/120    avg_loss:2.124, val_acc:0.514]
Epoch [9/120    avg_loss:2.011, val_acc:0.574]
Epoch [10/120    avg_loss:1.902, val_acc:0.594]
Epoch [11/120    avg_loss:1.824, val_acc:0.628]
Epoch [12/120    avg_loss:1.669, val_acc:0.652]
Epoch [13/120    avg_loss:1.579, val_acc:0.623]
Epoch [14/120    avg_loss:1.479, val_acc:0.643]
Epoch [15/120    avg_loss:1.321, val_acc:0.647]
Epoch [16/120    avg_loss:1.226, val_acc:0.644]
Epoch [17/120    avg_loss:1.175, val_acc:0.652]
Epoch [18/120    avg_loss:1.069, val_acc:0.674]
Epoch [19/120    avg_loss:0.961, val_acc:0.707]
Epoch [20/120    avg_loss:0.911, val_acc:0.711]
Epoch [21/120    avg_loss:0.918, val_acc:0.713]
Epoch [22/120    avg_loss:0.840, val_acc:0.743]
Epoch [23/120    avg_loss:0.736, val_acc:0.760]
Epoch [24/120    avg_loss:0.698, val_acc:0.756]
Epoch [25/120    avg_loss:0.667, val_acc:0.770]
Epoch [26/120    avg_loss:0.631, val_acc:0.778]
Epoch [27/120    avg_loss:0.582, val_acc:0.768]
Epoch [28/120    avg_loss:0.606, val_acc:0.803]
Epoch [29/120    avg_loss:0.576, val_acc:0.793]
Epoch [30/120    avg_loss:0.463, val_acc:0.832]
Epoch [31/120    avg_loss:0.405, val_acc:0.826]
Epoch [32/120    avg_loss:0.413, val_acc:0.819]
Epoch [33/120    avg_loss:0.409, val_acc:0.851]
Epoch [34/120    avg_loss:0.350, val_acc:0.867]
Epoch [35/120    avg_loss:0.361, val_acc:0.866]
Epoch [36/120    avg_loss:0.332, val_acc:0.887]
Epoch [37/120    avg_loss:0.288, val_acc:0.882]
Epoch [38/120    avg_loss:0.318, val_acc:0.871]
Epoch [39/120    avg_loss:0.255, val_acc:0.890]
Epoch [40/120    avg_loss:0.237, val_acc:0.881]
Epoch [41/120    avg_loss:0.212, val_acc:0.908]
Epoch [42/120    avg_loss:0.161, val_acc:0.920]
Epoch [43/120    avg_loss:0.165, val_acc:0.907]
Epoch [44/120    avg_loss:0.195, val_acc:0.866]
Epoch [45/120    avg_loss:0.187, val_acc:0.902]
Epoch [46/120    avg_loss:0.190, val_acc:0.910]
Epoch [47/120    avg_loss:0.177, val_acc:0.901]
Epoch [48/120    avg_loss:0.166, val_acc:0.905]
Epoch [49/120    avg_loss:0.133, val_acc:0.916]
Epoch [50/120    avg_loss:0.130, val_acc:0.922]
Epoch [51/120    avg_loss:0.107, val_acc:0.934]
Epoch [52/120    avg_loss:0.104, val_acc:0.935]
Epoch [53/120    avg_loss:0.104, val_acc:0.940]
Epoch [54/120    avg_loss:0.124, val_acc:0.907]
Epoch [55/120    avg_loss:0.113, val_acc:0.931]
Epoch [56/120    avg_loss:0.101, val_acc:0.922]
Epoch [57/120    avg_loss:0.142, val_acc:0.892]
Epoch [58/120    avg_loss:0.156, val_acc:0.911]
Epoch [59/120    avg_loss:0.111, val_acc:0.943]
Epoch [60/120    avg_loss:0.096, val_acc:0.934]
Epoch [61/120    avg_loss:0.119, val_acc:0.935]
Epoch [62/120    avg_loss:0.069, val_acc:0.942]
Epoch [63/120    avg_loss:0.069, val_acc:0.943]
Epoch [64/120    avg_loss:0.066, val_acc:0.932]
Epoch [65/120    avg_loss:0.075, val_acc:0.931]
Epoch [66/120    avg_loss:0.060, val_acc:0.941]
Epoch [67/120    avg_loss:0.073, val_acc:0.941]
Epoch [68/120    avg_loss:0.073, val_acc:0.930]
Epoch [69/120    avg_loss:0.097, val_acc:0.933]
Epoch [70/120    avg_loss:0.081, val_acc:0.933]
Epoch [71/120    avg_loss:0.080, val_acc:0.936]
Epoch [72/120    avg_loss:0.069, val_acc:0.935]
Epoch [73/120    avg_loss:0.065, val_acc:0.955]
Epoch [74/120    avg_loss:0.057, val_acc:0.941]
Epoch [75/120    avg_loss:0.050, val_acc:0.934]
Epoch [76/120    avg_loss:0.045, val_acc:0.956]
Epoch [77/120    avg_loss:0.044, val_acc:0.951]
Epoch [78/120    avg_loss:0.038, val_acc:0.963]
Epoch [79/120    avg_loss:0.038, val_acc:0.954]
Epoch [80/120    avg_loss:0.033, val_acc:0.961]
Epoch [81/120    avg_loss:0.040, val_acc:0.961]
Epoch [82/120    avg_loss:0.032, val_acc:0.959]
Epoch [83/120    avg_loss:0.042, val_acc:0.947]
Epoch [84/120    avg_loss:0.041, val_acc:0.965]
Epoch [85/120    avg_loss:0.031, val_acc:0.941]
Epoch [86/120    avg_loss:0.056, val_acc:0.963]
Epoch [87/120    avg_loss:0.049, val_acc:0.935]
Epoch [88/120    avg_loss:0.047, val_acc:0.964]
Epoch [89/120    avg_loss:0.033, val_acc:0.965]
Epoch [90/120    avg_loss:0.037, val_acc:0.966]
Epoch [91/120    avg_loss:0.035, val_acc:0.966]
Epoch [92/120    avg_loss:0.032, val_acc:0.964]
Epoch [93/120    avg_loss:0.026, val_acc:0.966]
Epoch [94/120    avg_loss:0.023, val_acc:0.966]
Epoch [95/120    avg_loss:0.022, val_acc:0.961]
Epoch [96/120    avg_loss:0.019, val_acc:0.958]
Epoch [97/120    avg_loss:0.020, val_acc:0.969]
Epoch [98/120    avg_loss:0.026, val_acc:0.961]
Epoch [99/120    avg_loss:0.038, val_acc:0.942]
Epoch [100/120    avg_loss:0.046, val_acc:0.958]
Epoch [101/120    avg_loss:0.028, val_acc:0.963]
Epoch [102/120    avg_loss:0.022, val_acc:0.965]
Epoch [103/120    avg_loss:0.034, val_acc:0.953]
Epoch [104/120    avg_loss:0.022, val_acc:0.959]
Epoch [105/120    avg_loss:0.021, val_acc:0.958]
Epoch [106/120    avg_loss:0.020, val_acc:0.967]
Epoch [107/120    avg_loss:0.017, val_acc:0.965]
Epoch [108/120    avg_loss:0.021, val_acc:0.963]
Epoch [109/120    avg_loss:0.018, val_acc:0.957]
Epoch [110/120    avg_loss:0.018, val_acc:0.963]
Epoch [111/120    avg_loss:0.015, val_acc:0.966]
Epoch [112/120    avg_loss:0.014, val_acc:0.967]
Epoch [113/120    avg_loss:0.014, val_acc:0.968]
Epoch [114/120    avg_loss:0.012, val_acc:0.966]
Epoch [115/120    avg_loss:0.012, val_acc:0.966]
Epoch [116/120    avg_loss:0.014, val_acc:0.964]
Epoch [117/120    avg_loss:0.011, val_acc:0.964]
Epoch [118/120    avg_loss:0.010, val_acc:0.966]
Epoch [119/120    avg_loss:0.014, val_acc:0.966]
Epoch [120/120    avg_loss:0.014, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1242    0    4    0    0    0    0    0    3   36    0    0
     0    0    0]
 [   0    0    0  721    0    0    0    0    0    2    3    0   20    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    3    1    0    0    0  846   19    0    0
     0    1    0]
 [   0    0    3    0    0    0    0    0    0    0   11 2182   14    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0    0    0    0  525    0
     0    4    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1138    1    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    20  322    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.11382113821138

F1 scores:
[       nan 0.95121951 0.97988166 0.97961957 0.99069767 0.99310345
 0.99242424 1.         0.997669   0.88888889 0.97241379 0.98111511
 0.95890411 0.99730458 0.98913516 0.95407407 0.98203593]

Kappa:
0.9784872662146223
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1c247e6ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.821, val_acc:0.246]
Epoch [2/120    avg_loss:2.672, val_acc:0.293]
Epoch [3/120    avg_loss:2.520, val_acc:0.469]
Epoch [4/120    avg_loss:2.417, val_acc:0.524]
Epoch [5/120    avg_loss:2.295, val_acc:0.544]
Epoch [6/120    avg_loss:2.199, val_acc:0.588]
Epoch [7/120    avg_loss:2.118, val_acc:0.584]
Epoch [8/120    avg_loss:2.017, val_acc:0.606]
Epoch [9/120    avg_loss:1.940, val_acc:0.619]
Epoch [10/120    avg_loss:1.836, val_acc:0.601]
Epoch [11/120    avg_loss:1.740, val_acc:0.629]
Epoch [12/120    avg_loss:1.632, val_acc:0.647]
Epoch [13/120    avg_loss:1.559, val_acc:0.637]
Epoch [14/120    avg_loss:1.429, val_acc:0.666]
Epoch [15/120    avg_loss:1.315, val_acc:0.709]
Epoch [16/120    avg_loss:1.253, val_acc:0.713]
Epoch [17/120    avg_loss:1.092, val_acc:0.774]
Epoch [18/120    avg_loss:0.980, val_acc:0.745]
Epoch [19/120    avg_loss:0.966, val_acc:0.762]
Epoch [20/120    avg_loss:0.893, val_acc:0.760]
Epoch [21/120    avg_loss:0.765, val_acc:0.824]
Epoch [22/120    avg_loss:0.673, val_acc:0.817]
Epoch [23/120    avg_loss:0.591, val_acc:0.822]
Epoch [24/120    avg_loss:0.574, val_acc:0.835]
Epoch [25/120    avg_loss:0.498, val_acc:0.830]
Epoch [26/120    avg_loss:0.434, val_acc:0.812]
Epoch [27/120    avg_loss:0.442, val_acc:0.783]
Epoch [28/120    avg_loss:0.398, val_acc:0.880]
Epoch [29/120    avg_loss:0.367, val_acc:0.860]
Epoch [30/120    avg_loss:0.321, val_acc:0.883]
Epoch [31/120    avg_loss:0.267, val_acc:0.901]
Epoch [32/120    avg_loss:0.270, val_acc:0.877]
Epoch [33/120    avg_loss:0.265, val_acc:0.883]
Epoch [34/120    avg_loss:0.245, val_acc:0.912]
Epoch [35/120    avg_loss:0.203, val_acc:0.921]
Epoch [36/120    avg_loss:0.184, val_acc:0.921]
Epoch [37/120    avg_loss:0.155, val_acc:0.929]
Epoch [38/120    avg_loss:0.130, val_acc:0.923]
Epoch [39/120    avg_loss:0.143, val_acc:0.921]
Epoch [40/120    avg_loss:0.140, val_acc:0.930]
Epoch [41/120    avg_loss:0.119, val_acc:0.934]
Epoch [42/120    avg_loss:0.110, val_acc:0.923]
Epoch [43/120    avg_loss:0.122, val_acc:0.939]
Epoch [44/120    avg_loss:0.125, val_acc:0.921]
Epoch [45/120    avg_loss:0.139, val_acc:0.932]
Epoch [46/120    avg_loss:0.124, val_acc:0.908]
Epoch [47/120    avg_loss:0.243, val_acc:0.905]
Epoch [48/120    avg_loss:0.145, val_acc:0.925]
Epoch [49/120    avg_loss:0.100, val_acc:0.931]
Epoch [50/120    avg_loss:0.083, val_acc:0.935]
Epoch [51/120    avg_loss:0.121, val_acc:0.934]
Epoch [52/120    avg_loss:0.101, val_acc:0.959]
Epoch [53/120    avg_loss:0.070, val_acc:0.950]
Epoch [54/120    avg_loss:0.085, val_acc:0.938]
Epoch [55/120    avg_loss:0.120, val_acc:0.922]
Epoch [56/120    avg_loss:0.107, val_acc:0.941]
Epoch [57/120    avg_loss:0.069, val_acc:0.954]
Epoch [58/120    avg_loss:0.046, val_acc:0.957]
Epoch [59/120    avg_loss:0.048, val_acc:0.948]
Epoch [60/120    avg_loss:0.056, val_acc:0.957]
Epoch [61/120    avg_loss:0.041, val_acc:0.964]
Epoch [62/120    avg_loss:0.041, val_acc:0.969]
Epoch [63/120    avg_loss:0.043, val_acc:0.963]
Epoch [64/120    avg_loss:0.042, val_acc:0.954]
Epoch [65/120    avg_loss:0.047, val_acc:0.954]
Epoch [66/120    avg_loss:0.036, val_acc:0.963]
Epoch [67/120    avg_loss:0.033, val_acc:0.960]
Epoch [68/120    avg_loss:0.026, val_acc:0.961]
Epoch [69/120    avg_loss:0.030, val_acc:0.973]
Epoch [70/120    avg_loss:0.036, val_acc:0.963]
Epoch [71/120    avg_loss:0.049, val_acc:0.960]
Epoch [72/120    avg_loss:0.049, val_acc:0.949]
Epoch [73/120    avg_loss:0.029, val_acc:0.964]
Epoch [74/120    avg_loss:0.032, val_acc:0.968]
Epoch [75/120    avg_loss:0.032, val_acc:0.971]
Epoch [76/120    avg_loss:0.033, val_acc:0.966]
Epoch [77/120    avg_loss:0.024, val_acc:0.969]
Epoch [78/120    avg_loss:0.027, val_acc:0.969]
Epoch [79/120    avg_loss:0.018, val_acc:0.971]
Epoch [80/120    avg_loss:0.021, val_acc:0.969]
Epoch [81/120    avg_loss:0.023, val_acc:0.970]
Epoch [82/120    avg_loss:0.019, val_acc:0.975]
Epoch [83/120    avg_loss:0.017, val_acc:0.970]
Epoch [84/120    avg_loss:0.014, val_acc:0.978]
Epoch [85/120    avg_loss:0.020, val_acc:0.934]
Epoch [86/120    avg_loss:0.030, val_acc:0.973]
Epoch [87/120    avg_loss:0.018, val_acc:0.975]
Epoch [88/120    avg_loss:0.021, val_acc:0.975]
Epoch [89/120    avg_loss:0.018, val_acc:0.972]
Epoch [90/120    avg_loss:0.024, val_acc:0.969]
Epoch [91/120    avg_loss:0.028, val_acc:0.952]
Epoch [92/120    avg_loss:0.037, val_acc:0.960]
Epoch [93/120    avg_loss:0.029, val_acc:0.970]
Epoch [94/120    avg_loss:0.023, val_acc:0.977]
Epoch [95/120    avg_loss:0.014, val_acc:0.977]
Epoch [96/120    avg_loss:0.016, val_acc:0.974]
Epoch [97/120    avg_loss:0.011, val_acc:0.975]
Epoch [98/120    avg_loss:0.011, val_acc:0.978]
Epoch [99/120    avg_loss:0.011, val_acc:0.980]
Epoch [100/120    avg_loss:0.011, val_acc:0.982]
Epoch [101/120    avg_loss:0.008, val_acc:0.982]
Epoch [102/120    avg_loss:0.010, val_acc:0.982]
Epoch [103/120    avg_loss:0.011, val_acc:0.981]
Epoch [104/120    avg_loss:0.010, val_acc:0.982]
Epoch [105/120    avg_loss:0.010, val_acc:0.983]
Epoch [106/120    avg_loss:0.009, val_acc:0.983]
Epoch [107/120    avg_loss:0.010, val_acc:0.983]
Epoch [108/120    avg_loss:0.008, val_acc:0.983]
Epoch [109/120    avg_loss:0.010, val_acc:0.982]
Epoch [110/120    avg_loss:0.008, val_acc:0.983]
Epoch [111/120    avg_loss:0.010, val_acc:0.983]
Epoch [112/120    avg_loss:0.011, val_acc:0.982]
Epoch [113/120    avg_loss:0.008, val_acc:0.983]
Epoch [114/120    avg_loss:0.009, val_acc:0.982]
Epoch [115/120    avg_loss:0.008, val_acc:0.982]
Epoch [116/120    avg_loss:0.008, val_acc:0.982]
Epoch [117/120    avg_loss:0.008, val_acc:0.982]
Epoch [118/120    avg_loss:0.006, val_acc:0.982]
Epoch [119/120    avg_loss:0.008, val_acc:0.982]
Epoch [120/120    avg_loss:0.009, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1246    0    5    0    0    0    0    2    7   25    0    0
     0    0    0]
 [   0    0    0  716    6    7    0    0    0   11    2    2    2    1
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0   11    0    0    2    0    0    0    0  844   18    0    0
     0    0    0]
 [   0    0    6    0    0    2    1    0    0    1    8 2179   13    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0    0    3  528    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1135    4    0]
 [   0    0    1    0    0    0   26    0    0    0    0    0    0    0
    17  303    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.88617886178862

F1 scores:
[       nan 0.95348837 0.97687181 0.97814208 0.97247706 0.98751419
 0.97834205 1.         0.9953271  0.69387755 0.97235023 0.98153153
 0.97959184 0.99730458 0.9908337  0.9266055  0.98809524]

Kappa:
0.9758961296744877
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f629fe739e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.814, val_acc:0.095]
Epoch [2/120    avg_loss:2.689, val_acc:0.261]
Epoch [3/120    avg_loss:2.559, val_acc:0.433]
Epoch [4/120    avg_loss:2.458, val_acc:0.468]
Epoch [5/120    avg_loss:2.380, val_acc:0.474]
Epoch [6/120    avg_loss:2.269, val_acc:0.489]
Epoch [7/120    avg_loss:2.173, val_acc:0.511]
Epoch [8/120    avg_loss:2.076, val_acc:0.557]
Epoch [9/120    avg_loss:1.982, val_acc:0.540]
Epoch [10/120    avg_loss:1.879, val_acc:0.555]
Epoch [11/120    avg_loss:1.817, val_acc:0.570]
Epoch [12/120    avg_loss:1.718, val_acc:0.598]
Epoch [13/120    avg_loss:1.615, val_acc:0.620]
Epoch [14/120    avg_loss:1.442, val_acc:0.648]
Epoch [15/120    avg_loss:1.391, val_acc:0.641]
Epoch [16/120    avg_loss:1.290, val_acc:0.647]
Epoch [17/120    avg_loss:1.137, val_acc:0.682]
Epoch [18/120    avg_loss:1.078, val_acc:0.684]
Epoch [19/120    avg_loss:0.977, val_acc:0.681]
Epoch [20/120    avg_loss:0.867, val_acc:0.722]
Epoch [21/120    avg_loss:0.815, val_acc:0.729]
Epoch [22/120    avg_loss:0.721, val_acc:0.790]
Epoch [23/120    avg_loss:0.621, val_acc:0.817]
Epoch [24/120    avg_loss:0.586, val_acc:0.806]
Epoch [25/120    avg_loss:0.583, val_acc:0.825]
Epoch [26/120    avg_loss:0.485, val_acc:0.846]
Epoch [27/120    avg_loss:0.465, val_acc:0.849]
Epoch [28/120    avg_loss:0.426, val_acc:0.839]
Epoch [29/120    avg_loss:0.365, val_acc:0.861]
Epoch [30/120    avg_loss:0.324, val_acc:0.892]
Epoch [31/120    avg_loss:0.287, val_acc:0.900]
Epoch [32/120    avg_loss:0.258, val_acc:0.893]
Epoch [33/120    avg_loss:0.260, val_acc:0.846]
Epoch [34/120    avg_loss:0.286, val_acc:0.884]
Epoch [35/120    avg_loss:0.271, val_acc:0.885]
Epoch [36/120    avg_loss:0.262, val_acc:0.909]
Epoch [37/120    avg_loss:0.224, val_acc:0.878]
Epoch [38/120    avg_loss:0.274, val_acc:0.893]
Epoch [39/120    avg_loss:0.223, val_acc:0.895]
Epoch [40/120    avg_loss:0.208, val_acc:0.903]
Epoch [41/120    avg_loss:0.203, val_acc:0.905]
Epoch [42/120    avg_loss:0.165, val_acc:0.923]
Epoch [43/120    avg_loss:0.155, val_acc:0.927]
Epoch [44/120    avg_loss:0.170, val_acc:0.916]
Epoch [45/120    avg_loss:0.162, val_acc:0.929]
Epoch [46/120    avg_loss:0.156, val_acc:0.912]
Epoch [47/120    avg_loss:0.148, val_acc:0.910]
Epoch [48/120    avg_loss:0.124, val_acc:0.925]
Epoch [49/120    avg_loss:0.110, val_acc:0.931]
Epoch [50/120    avg_loss:0.109, val_acc:0.940]
Epoch [51/120    avg_loss:0.098, val_acc:0.929]
Epoch [52/120    avg_loss:0.121, val_acc:0.923]
Epoch [53/120    avg_loss:0.103, val_acc:0.946]
Epoch [54/120    avg_loss:0.149, val_acc:0.926]
Epoch [55/120    avg_loss:0.117, val_acc:0.925]
Epoch [56/120    avg_loss:0.101, val_acc:0.921]
Epoch [57/120    avg_loss:0.098, val_acc:0.931]
Epoch [58/120    avg_loss:0.090, val_acc:0.945]
Epoch [59/120    avg_loss:0.083, val_acc:0.949]
Epoch [60/120    avg_loss:0.070, val_acc:0.951]
Epoch [61/120    avg_loss:0.064, val_acc:0.958]
Epoch [62/120    avg_loss:0.058, val_acc:0.959]
Epoch [63/120    avg_loss:0.051, val_acc:0.964]
Epoch [64/120    avg_loss:0.053, val_acc:0.951]
Epoch [65/120    avg_loss:0.055, val_acc:0.959]
Epoch [66/120    avg_loss:0.066, val_acc:0.942]
Epoch [67/120    avg_loss:0.054, val_acc:0.956]
Epoch [68/120    avg_loss:0.040, val_acc:0.955]
Epoch [69/120    avg_loss:0.051, val_acc:0.952]
Epoch [70/120    avg_loss:0.056, val_acc:0.961]
Epoch [71/120    avg_loss:0.042, val_acc:0.959]
Epoch [72/120    avg_loss:0.043, val_acc:0.963]
Epoch [73/120    avg_loss:0.042, val_acc:0.960]
Epoch [74/120    avg_loss:0.039, val_acc:0.953]
Epoch [75/120    avg_loss:0.038, val_acc:0.957]
Epoch [76/120    avg_loss:0.033, val_acc:0.972]
Epoch [77/120    avg_loss:0.028, val_acc:0.968]
Epoch [78/120    avg_loss:0.031, val_acc:0.966]
Epoch [79/120    avg_loss:0.025, val_acc:0.963]
Epoch [80/120    avg_loss:0.029, val_acc:0.972]
Epoch [81/120    avg_loss:0.026, val_acc:0.967]
Epoch [82/120    avg_loss:0.028, val_acc:0.976]
Epoch [83/120    avg_loss:0.022, val_acc:0.976]
Epoch [84/120    avg_loss:0.023, val_acc:0.958]
Epoch [85/120    avg_loss:0.036, val_acc:0.966]
Epoch [86/120    avg_loss:0.034, val_acc:0.967]
Epoch [87/120    avg_loss:0.028, val_acc:0.970]
Epoch [88/120    avg_loss:0.031, val_acc:0.976]
Epoch [89/120    avg_loss:0.035, val_acc:0.966]
Epoch [90/120    avg_loss:0.030, val_acc:0.955]
Epoch [91/120    avg_loss:0.023, val_acc:0.961]
Epoch [92/120    avg_loss:0.024, val_acc:0.971]
Epoch [93/120    avg_loss:0.024, val_acc:0.965]
Epoch [94/120    avg_loss:0.022, val_acc:0.970]
Epoch [95/120    avg_loss:0.017, val_acc:0.976]
Epoch [96/120    avg_loss:0.020, val_acc:0.974]
Epoch [97/120    avg_loss:0.034, val_acc:0.953]
Epoch [98/120    avg_loss:0.044, val_acc:0.958]
Epoch [99/120    avg_loss:0.040, val_acc:0.968]
Epoch [100/120    avg_loss:0.040, val_acc:0.961]
Epoch [101/120    avg_loss:0.031, val_acc:0.974]
Epoch [102/120    avg_loss:0.027, val_acc:0.955]
Epoch [103/120    avg_loss:0.023, val_acc:0.971]
Epoch [104/120    avg_loss:0.030, val_acc:0.968]
Epoch [105/120    avg_loss:0.024, val_acc:0.954]
Epoch [106/120    avg_loss:0.032, val_acc:0.966]
Epoch [107/120    avg_loss:0.024, val_acc:0.964]
Epoch [108/120    avg_loss:0.027, val_acc:0.970]
Epoch [109/120    avg_loss:0.019, val_acc:0.975]
Epoch [110/120    avg_loss:0.013, val_acc:0.981]
Epoch [111/120    avg_loss:0.017, val_acc:0.980]
Epoch [112/120    avg_loss:0.011, val_acc:0.982]
Epoch [113/120    avg_loss:0.013, val_acc:0.982]
Epoch [114/120    avg_loss:0.014, val_acc:0.981]
Epoch [115/120    avg_loss:0.013, val_acc:0.981]
Epoch [116/120    avg_loss:0.012, val_acc:0.981]
Epoch [117/120    avg_loss:0.012, val_acc:0.981]
Epoch [118/120    avg_loss:0.011, val_acc:0.981]
Epoch [119/120    avg_loss:0.013, val_acc:0.979]
Epoch [120/120    avg_loss:0.016, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1253    3    2    0    0    0    0    0    4   23    0    0
     0    0    0]
 [   0    0    4  714    5    1    1    0    0    8    6    1    6    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    4    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    3    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   10    0    0    0    1    0    0    0  843   18    0    0
     0    3    0]
 [   0    0    1    0    0    2    0    0    0    0    6 2199    2    0
     0    0    0]
 [   0    0    0    2    5    0    0    0    0    0    1    2  519    0
     4    0    1]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    30  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.88617886178862

F1 scores:
[       nan 0.96202532 0.98159029 0.97407913 0.97260274 0.98964327
 0.98187311 0.92592593 0.99883586 0.77272727 0.97008055 0.98698384
 0.97556391 0.99459459 0.98006932 0.91515152 0.98809524]

Kappa:
0.9758827761950188
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0749b5aa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.803, val_acc:0.085]
Epoch [2/120    avg_loss:2.677, val_acc:0.154]
Epoch [3/120    avg_loss:2.542, val_acc:0.406]
Epoch [4/120    avg_loss:2.426, val_acc:0.495]
Epoch [5/120    avg_loss:2.337, val_acc:0.525]
Epoch [6/120    avg_loss:2.240, val_acc:0.544]
Epoch [7/120    avg_loss:2.190, val_acc:0.532]
Epoch [8/120    avg_loss:2.107, val_acc:0.518]
Epoch [9/120    avg_loss:2.031, val_acc:0.569]
Epoch [10/120    avg_loss:1.928, val_acc:0.594]
Epoch [11/120    avg_loss:1.854, val_acc:0.604]
Epoch [12/120    avg_loss:1.765, val_acc:0.620]
Epoch [13/120    avg_loss:1.687, val_acc:0.607]
Epoch [14/120    avg_loss:1.612, val_acc:0.619]
Epoch [15/120    avg_loss:1.521, val_acc:0.631]
Epoch [16/120    avg_loss:1.412, val_acc:0.624]
Epoch [17/120    avg_loss:1.298, val_acc:0.669]
Epoch [18/120    avg_loss:1.193, val_acc:0.679]
Epoch [19/120    avg_loss:1.141, val_acc:0.677]
Epoch [20/120    avg_loss:1.018, val_acc:0.713]
Epoch [21/120    avg_loss:0.945, val_acc:0.742]
Epoch [22/120    avg_loss:0.873, val_acc:0.742]
Epoch [23/120    avg_loss:0.771, val_acc:0.801]
Epoch [24/120    avg_loss:0.684, val_acc:0.779]
Epoch [25/120    avg_loss:0.663, val_acc:0.812]
Epoch [26/120    avg_loss:0.548, val_acc:0.808]
Epoch [27/120    avg_loss:0.657, val_acc:0.806]
Epoch [28/120    avg_loss:0.593, val_acc:0.796]
Epoch [29/120    avg_loss:0.516, val_acc:0.768]
Epoch [30/120    avg_loss:0.413, val_acc:0.838]
Epoch [31/120    avg_loss:0.383, val_acc:0.843]
Epoch [32/120    avg_loss:0.352, val_acc:0.850]
Epoch [33/120    avg_loss:0.309, val_acc:0.861]
Epoch [34/120    avg_loss:0.305, val_acc:0.872]
Epoch [35/120    avg_loss:0.262, val_acc:0.911]
Epoch [36/120    avg_loss:0.242, val_acc:0.914]
Epoch [37/120    avg_loss:0.217, val_acc:0.906]
Epoch [38/120    avg_loss:0.246, val_acc:0.912]
Epoch [39/120    avg_loss:0.203, val_acc:0.912]
Epoch [40/120    avg_loss:0.213, val_acc:0.920]
Epoch [41/120    avg_loss:0.160, val_acc:0.928]
Epoch [42/120    avg_loss:0.159, val_acc:0.939]
Epoch [43/120    avg_loss:0.151, val_acc:0.930]
Epoch [44/120    avg_loss:0.135, val_acc:0.935]
Epoch [45/120    avg_loss:0.150, val_acc:0.935]
Epoch [46/120    avg_loss:0.150, val_acc:0.946]
Epoch [47/120    avg_loss:0.120, val_acc:0.941]
Epoch [48/120    avg_loss:0.104, val_acc:0.956]
Epoch [49/120    avg_loss:0.090, val_acc:0.945]
Epoch [50/120    avg_loss:0.105, val_acc:0.944]
Epoch [51/120    avg_loss:0.115, val_acc:0.944]
Epoch [52/120    avg_loss:0.091, val_acc:0.950]
Epoch [53/120    avg_loss:0.078, val_acc:0.955]
Epoch [54/120    avg_loss:0.079, val_acc:0.947]
Epoch [55/120    avg_loss:0.097, val_acc:0.950]
Epoch [56/120    avg_loss:0.096, val_acc:0.961]
Epoch [57/120    avg_loss:0.099, val_acc:0.946]
Epoch [58/120    avg_loss:0.067, val_acc:0.945]
Epoch [59/120    avg_loss:0.066, val_acc:0.961]
Epoch [60/120    avg_loss:0.065, val_acc:0.965]
Epoch [61/120    avg_loss:0.056, val_acc:0.959]
Epoch [62/120    avg_loss:0.049, val_acc:0.967]
Epoch [63/120    avg_loss:0.051, val_acc:0.959]
Epoch [64/120    avg_loss:0.059, val_acc:0.960]
Epoch [65/120    avg_loss:0.073, val_acc:0.944]
Epoch [66/120    avg_loss:0.096, val_acc:0.956]
Epoch [67/120    avg_loss:0.070, val_acc:0.965]
Epoch [68/120    avg_loss:0.060, val_acc:0.966]
Epoch [69/120    avg_loss:0.051, val_acc:0.965]
Epoch [70/120    avg_loss:0.057, val_acc:0.959]
Epoch [71/120    avg_loss:0.066, val_acc:0.947]
Epoch [72/120    avg_loss:0.050, val_acc:0.965]
Epoch [73/120    avg_loss:0.055, val_acc:0.958]
Epoch [74/120    avg_loss:0.043, val_acc:0.961]
Epoch [75/120    avg_loss:0.068, val_acc:0.963]
Epoch [76/120    avg_loss:0.036, val_acc:0.970]
Epoch [77/120    avg_loss:0.040, val_acc:0.972]
Epoch [78/120    avg_loss:0.029, val_acc:0.974]
Epoch [79/120    avg_loss:0.030, val_acc:0.977]
Epoch [80/120    avg_loss:0.024, val_acc:0.977]
Epoch [81/120    avg_loss:0.033, val_acc:0.979]
Epoch [82/120    avg_loss:0.028, val_acc:0.976]
Epoch [83/120    avg_loss:0.023, val_acc:0.978]
Epoch [84/120    avg_loss:0.029, val_acc:0.976]
Epoch [85/120    avg_loss:0.025, val_acc:0.977]
Epoch [86/120    avg_loss:0.021, val_acc:0.977]
Epoch [87/120    avg_loss:0.025, val_acc:0.977]
Epoch [88/120    avg_loss:0.022, val_acc:0.976]
Epoch [89/120    avg_loss:0.025, val_acc:0.978]
Epoch [90/120    avg_loss:0.025, val_acc:0.979]
Epoch [91/120    avg_loss:0.023, val_acc:0.980]
Epoch [92/120    avg_loss:0.025, val_acc:0.976]
Epoch [93/120    avg_loss:0.022, val_acc:0.976]
Epoch [94/120    avg_loss:0.023, val_acc:0.975]
Epoch [95/120    avg_loss:0.019, val_acc:0.978]
Epoch [96/120    avg_loss:0.022, val_acc:0.975]
Epoch [97/120    avg_loss:0.022, val_acc:0.977]
Epoch [98/120    avg_loss:0.021, val_acc:0.975]
Epoch [99/120    avg_loss:0.018, val_acc:0.977]
Epoch [100/120    avg_loss:0.023, val_acc:0.975]
Epoch [101/120    avg_loss:0.019, val_acc:0.977]
Epoch [102/120    avg_loss:0.023, val_acc:0.974]
Epoch [103/120    avg_loss:0.019, val_acc:0.976]
Epoch [104/120    avg_loss:0.022, val_acc:0.974]
Epoch [105/120    avg_loss:0.019, val_acc:0.974]
Epoch [106/120    avg_loss:0.028, val_acc:0.974]
Epoch [107/120    avg_loss:0.018, val_acc:0.975]
Epoch [108/120    avg_loss:0.023, val_acc:0.975]
Epoch [109/120    avg_loss:0.022, val_acc:0.975]
Epoch [110/120    avg_loss:0.023, val_acc:0.975]
Epoch [111/120    avg_loss:0.020, val_acc:0.975]
Epoch [112/120    avg_loss:0.018, val_acc:0.975]
Epoch [113/120    avg_loss:0.018, val_acc:0.975]
Epoch [114/120    avg_loss:0.019, val_acc:0.975]
Epoch [115/120    avg_loss:0.020, val_acc:0.975]
Epoch [116/120    avg_loss:0.020, val_acc:0.977]
Epoch [117/120    avg_loss:0.021, val_acc:0.976]
Epoch [118/120    avg_loss:0.022, val_acc:0.976]
Epoch [119/120    avg_loss:0.019, val_acc:0.976]
Epoch [120/120    avg_loss:0.020, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1250    1    3    0    0    0    0    4    2   25    0    0
     0    0    0]
 [   0    0    0  720    3    0    0    0    0    8    1    2   12    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    1    1    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    1  857   14    0    0
     0    0    0]
 [   0    0   16    0    0    1    0    0    0    1   16 2170    4    0
     0    2    0]
 [   0    0    0    0    2    0    0    0    0    0   10    0  516    0
     2    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1125   14    0]
 [   0    0    1    0    0    0   10    0    0    0    0    0    0    0
    10  326    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.97289972899729

F1 scores:
[       nan 0.92857143 0.97847358 0.98092643 0.98156682 0.99307159
 0.99017385 0.98039216 0.9953271  0.72       0.97220647 0.98123446
 0.96719775 0.99730458 0.98727512 0.94492754 0.97647059]

Kappa:
0.976894356698012
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff243c18b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.773, val_acc:0.203]
Epoch [2/120    avg_loss:2.650, val_acc:0.342]
Epoch [3/120    avg_loss:2.535, val_acc:0.330]
Epoch [4/120    avg_loss:2.447, val_acc:0.397]
Epoch [5/120    avg_loss:2.354, val_acc:0.438]
Epoch [6/120    avg_loss:2.282, val_acc:0.472]
Epoch [7/120    avg_loss:2.193, val_acc:0.533]
Epoch [8/120    avg_loss:2.098, val_acc:0.541]
Epoch [9/120    avg_loss:2.031, val_acc:0.554]
Epoch [10/120    avg_loss:1.949, val_acc:0.551]
Epoch [11/120    avg_loss:1.863, val_acc:0.550]
Epoch [12/120    avg_loss:1.775, val_acc:0.593]
Epoch [13/120    avg_loss:1.659, val_acc:0.611]
Epoch [14/120    avg_loss:1.560, val_acc:0.629]
Epoch [15/120    avg_loss:1.492, val_acc:0.645]
Epoch [16/120    avg_loss:1.351, val_acc:0.666]
Epoch [17/120    avg_loss:1.281, val_acc:0.677]
Epoch [18/120    avg_loss:1.163, val_acc:0.678]
Epoch [19/120    avg_loss:1.080, val_acc:0.710]
Epoch [20/120    avg_loss:0.970, val_acc:0.716]
Epoch [21/120    avg_loss:0.955, val_acc:0.720]
Epoch [22/120    avg_loss:0.851, val_acc:0.755]
Epoch [23/120    avg_loss:0.756, val_acc:0.739]
Epoch [24/120    avg_loss:0.685, val_acc:0.805]
Epoch [25/120    avg_loss:0.580, val_acc:0.802]
Epoch [26/120    avg_loss:0.492, val_acc:0.834]
Epoch [27/120    avg_loss:0.454, val_acc:0.819]
Epoch [28/120    avg_loss:0.372, val_acc:0.866]
Epoch [29/120    avg_loss:0.388, val_acc:0.811]
Epoch [30/120    avg_loss:0.438, val_acc:0.852]
Epoch [31/120    avg_loss:0.339, val_acc:0.873]
Epoch [32/120    avg_loss:0.295, val_acc:0.886]
Epoch [33/120    avg_loss:0.243, val_acc:0.883]
Epoch [34/120    avg_loss:0.230, val_acc:0.905]
Epoch [35/120    avg_loss:0.252, val_acc:0.872]
Epoch [36/120    avg_loss:0.268, val_acc:0.909]
Epoch [37/120    avg_loss:0.243, val_acc:0.904]
Epoch [38/120    avg_loss:0.187, val_acc:0.890]
Epoch [39/120    avg_loss:0.174, val_acc:0.931]
Epoch [40/120    avg_loss:0.154, val_acc:0.886]
Epoch [41/120    avg_loss:0.160, val_acc:0.920]
Epoch [42/120    avg_loss:0.137, val_acc:0.913]
Epoch [43/120    avg_loss:0.130, val_acc:0.929]
Epoch [44/120    avg_loss:0.141, val_acc:0.920]
Epoch [45/120    avg_loss:0.136, val_acc:0.928]
Epoch [46/120    avg_loss:0.125, val_acc:0.921]
Epoch [47/120    avg_loss:0.123, val_acc:0.913]
Epoch [48/120    avg_loss:0.137, val_acc:0.926]
Epoch [49/120    avg_loss:0.123, val_acc:0.938]
Epoch [50/120    avg_loss:0.110, val_acc:0.944]
Epoch [51/120    avg_loss:0.091, val_acc:0.930]
Epoch [52/120    avg_loss:0.087, val_acc:0.934]
Epoch [53/120    avg_loss:0.088, val_acc:0.930]
Epoch [54/120    avg_loss:0.066, val_acc:0.945]
Epoch [55/120    avg_loss:0.094, val_acc:0.936]
Epoch [56/120    avg_loss:0.075, val_acc:0.932]
Epoch [57/120    avg_loss:0.089, val_acc:0.945]
Epoch [58/120    avg_loss:0.086, val_acc:0.941]
Epoch [59/120    avg_loss:0.074, val_acc:0.941]
Epoch [60/120    avg_loss:0.094, val_acc:0.933]
Epoch [61/120    avg_loss:0.093, val_acc:0.939]
Epoch [62/120    avg_loss:0.207, val_acc:0.921]
Epoch [63/120    avg_loss:0.176, val_acc:0.913]
Epoch [64/120    avg_loss:0.111, val_acc:0.945]
Epoch [65/120    avg_loss:0.087, val_acc:0.947]
Epoch [66/120    avg_loss:0.073, val_acc:0.948]
Epoch [67/120    avg_loss:0.083, val_acc:0.929]
Epoch [68/120    avg_loss:0.063, val_acc:0.935]
Epoch [69/120    avg_loss:0.051, val_acc:0.953]
Epoch [70/120    avg_loss:0.048, val_acc:0.954]
Epoch [71/120    avg_loss:0.043, val_acc:0.954]
Epoch [72/120    avg_loss:0.053, val_acc:0.945]
Epoch [73/120    avg_loss:0.061, val_acc:0.944]
Epoch [74/120    avg_loss:0.058, val_acc:0.936]
Epoch [75/120    avg_loss:0.053, val_acc:0.946]
Epoch [76/120    avg_loss:0.036, val_acc:0.958]
Epoch [77/120    avg_loss:0.040, val_acc:0.945]
Epoch [78/120    avg_loss:0.083, val_acc:0.956]
Epoch [79/120    avg_loss:0.047, val_acc:0.948]
Epoch [80/120    avg_loss:0.038, val_acc:0.959]
Epoch [81/120    avg_loss:0.037, val_acc:0.960]
Epoch [82/120    avg_loss:0.030, val_acc:0.948]
Epoch [83/120    avg_loss:0.044, val_acc:0.940]
Epoch [84/120    avg_loss:0.052, val_acc:0.956]
Epoch [85/120    avg_loss:0.036, val_acc:0.957]
Epoch [86/120    avg_loss:0.033, val_acc:0.949]
Epoch [87/120    avg_loss:0.025, val_acc:0.958]
Epoch [88/120    avg_loss:0.023, val_acc:0.961]
Epoch [89/120    avg_loss:0.028, val_acc:0.958]
Epoch [90/120    avg_loss:0.116, val_acc:0.916]
Epoch [91/120    avg_loss:0.126, val_acc:0.927]
Epoch [92/120    avg_loss:0.074, val_acc:0.935]
Epoch [93/120    avg_loss:0.070, val_acc:0.944]
Epoch [94/120    avg_loss:0.065, val_acc:0.941]
Epoch [95/120    avg_loss:0.044, val_acc:0.948]
Epoch [96/120    avg_loss:0.035, val_acc:0.955]
Epoch [97/120    avg_loss:0.028, val_acc:0.956]
Epoch [98/120    avg_loss:0.039, val_acc:0.950]
Epoch [99/120    avg_loss:0.036, val_acc:0.947]
Epoch [100/120    avg_loss:0.028, val_acc:0.932]
Epoch [101/120    avg_loss:0.054, val_acc:0.949]
Epoch [102/120    avg_loss:0.027, val_acc:0.959]
Epoch [103/120    avg_loss:0.024, val_acc:0.962]
Epoch [104/120    avg_loss:0.023, val_acc:0.962]
Epoch [105/120    avg_loss:0.022, val_acc:0.963]
Epoch [106/120    avg_loss:0.018, val_acc:0.964]
Epoch [107/120    avg_loss:0.021, val_acc:0.961]
Epoch [108/120    avg_loss:0.026, val_acc:0.962]
Epoch [109/120    avg_loss:0.021, val_acc:0.967]
Epoch [110/120    avg_loss:0.018, val_acc:0.968]
Epoch [111/120    avg_loss:0.020, val_acc:0.966]
Epoch [112/120    avg_loss:0.017, val_acc:0.967]
Epoch [113/120    avg_loss:0.015, val_acc:0.966]
Epoch [114/120    avg_loss:0.015, val_acc:0.967]
Epoch [115/120    avg_loss:0.019, val_acc:0.967]
Epoch [116/120    avg_loss:0.016, val_acc:0.964]
Epoch [117/120    avg_loss:0.017, val_acc:0.967]
Epoch [118/120    avg_loss:0.020, val_acc:0.966]
Epoch [119/120    avg_loss:0.017, val_acc:0.963]
Epoch [120/120    avg_loss:0.018, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1242    2    8    0    0    0    0    0    3   30    0    0
     0    0    0]
 [   0    0    2  721    1    0    1    0    0   11    7    0    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    1    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    9    0    0    0    0    0    0  421    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    1    0    0    0  857   10    0    0
     0    0    0]
 [   0    0    7    0    0    0    2    0    0    1   14 2164   22    0
     0    0    0]
 [   0    0    0    3    1    1    0    0    0    0    4    3  517    0
     2    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    1    0    0
  1122   15    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
    11  335    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.81029810298104

F1 scores:
[       nan 0.86363636 0.97679906 0.97895451 0.97706422 0.99192618
 0.99544073 1.         0.98826291 0.73469388 0.97220647 0.97940711
 0.95652174 1.         0.98464239 0.95988539 0.96385542]

Kappa:
0.975046354297739
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb754b37b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.790, val_acc:0.208]
Epoch [2/120    avg_loss:2.664, val_acc:0.260]
Epoch [3/120    avg_loss:2.541, val_acc:0.283]
Epoch [4/120    avg_loss:2.420, val_acc:0.363]
Epoch [5/120    avg_loss:2.340, val_acc:0.385]
Epoch [6/120    avg_loss:2.229, val_acc:0.450]
Epoch [7/120    avg_loss:2.148, val_acc:0.537]
Epoch [8/120    avg_loss:2.019, val_acc:0.572]
Epoch [9/120    avg_loss:1.945, val_acc:0.564]
Epoch [10/120    avg_loss:1.840, val_acc:0.617]
Epoch [11/120    avg_loss:1.784, val_acc:0.633]
Epoch [12/120    avg_loss:1.630, val_acc:0.662]
Epoch [13/120    avg_loss:1.520, val_acc:0.678]
Epoch [14/120    avg_loss:1.428, val_acc:0.676]
Epoch [15/120    avg_loss:1.321, val_acc:0.713]
Epoch [16/120    avg_loss:1.196, val_acc:0.718]
Epoch [17/120    avg_loss:1.102, val_acc:0.749]
Epoch [18/120    avg_loss:0.978, val_acc:0.760]
Epoch [19/120    avg_loss:0.909, val_acc:0.796]
Epoch [20/120    avg_loss:0.807, val_acc:0.810]
Epoch [21/120    avg_loss:0.778, val_acc:0.831]
Epoch [22/120    avg_loss:0.737, val_acc:0.812]
Epoch [23/120    avg_loss:0.609, val_acc:0.850]
Epoch [24/120    avg_loss:0.561, val_acc:0.855]
Epoch [25/120    avg_loss:0.493, val_acc:0.853]
Epoch [26/120    avg_loss:0.473, val_acc:0.858]
Epoch [27/120    avg_loss:0.465, val_acc:0.817]
Epoch [28/120    avg_loss:0.386, val_acc:0.885]
Epoch [29/120    avg_loss:0.355, val_acc:0.888]
Epoch [30/120    avg_loss:0.361, val_acc:0.883]
Epoch [31/120    avg_loss:0.312, val_acc:0.904]
Epoch [32/120    avg_loss:0.251, val_acc:0.895]
Epoch [33/120    avg_loss:0.248, val_acc:0.904]
Epoch [34/120    avg_loss:0.275, val_acc:0.914]
Epoch [35/120    avg_loss:0.202, val_acc:0.922]
Epoch [36/120    avg_loss:0.178, val_acc:0.932]
Epoch [37/120    avg_loss:0.169, val_acc:0.943]
Epoch [38/120    avg_loss:0.166, val_acc:0.942]
Epoch [39/120    avg_loss:0.177, val_acc:0.919]
Epoch [40/120    avg_loss:0.159, val_acc:0.931]
Epoch [41/120    avg_loss:0.141, val_acc:0.930]
Epoch [42/120    avg_loss:0.132, val_acc:0.947]
Epoch [43/120    avg_loss:0.113, val_acc:0.940]
Epoch [44/120    avg_loss:0.107, val_acc:0.946]
Epoch [45/120    avg_loss:0.094, val_acc:0.952]
Epoch [46/120    avg_loss:0.103, val_acc:0.954]
Epoch [47/120    avg_loss:0.120, val_acc:0.912]
Epoch [48/120    avg_loss:0.160, val_acc:0.946]
Epoch [49/120    avg_loss:0.109, val_acc:0.925]
Epoch [50/120    avg_loss:0.118, val_acc:0.946]
Epoch [51/120    avg_loss:0.091, val_acc:0.960]
Epoch [52/120    avg_loss:0.085, val_acc:0.912]
Epoch [53/120    avg_loss:0.084, val_acc:0.950]
Epoch [54/120    avg_loss:0.089, val_acc:0.945]
Epoch [55/120    avg_loss:0.073, val_acc:0.967]
Epoch [56/120    avg_loss:0.076, val_acc:0.942]
Epoch [57/120    avg_loss:0.073, val_acc:0.972]
Epoch [58/120    avg_loss:0.077, val_acc:0.963]
Epoch [59/120    avg_loss:0.072, val_acc:0.962]
Epoch [60/120    avg_loss:0.071, val_acc:0.949]
Epoch [61/120    avg_loss:0.075, val_acc:0.969]
Epoch [62/120    avg_loss:0.070, val_acc:0.964]
Epoch [63/120    avg_loss:0.051, val_acc:0.967]
Epoch [64/120    avg_loss:0.053, val_acc:0.972]
Epoch [65/120    avg_loss:0.038, val_acc:0.964]
Epoch [66/120    avg_loss:0.045, val_acc:0.969]
Epoch [67/120    avg_loss:0.041, val_acc:0.964]
Epoch [68/120    avg_loss:0.043, val_acc:0.971]
Epoch [69/120    avg_loss:0.038, val_acc:0.973]
Epoch [70/120    avg_loss:0.038, val_acc:0.961]
Epoch [71/120    avg_loss:0.042, val_acc:0.960]
Epoch [72/120    avg_loss:0.035, val_acc:0.968]
Epoch [73/120    avg_loss:0.042, val_acc:0.983]
Epoch [74/120    avg_loss:0.042, val_acc:0.967]
Epoch [75/120    avg_loss:0.034, val_acc:0.970]
Epoch [76/120    avg_loss:0.036, val_acc:0.969]
Epoch [77/120    avg_loss:0.039, val_acc:0.967]
Epoch [78/120    avg_loss:0.053, val_acc:0.969]
Epoch [79/120    avg_loss:0.037, val_acc:0.961]
Epoch [80/120    avg_loss:0.024, val_acc:0.975]
Epoch [81/120    avg_loss:0.030, val_acc:0.975]
Epoch [82/120    avg_loss:0.040, val_acc:0.975]
Epoch [83/120    avg_loss:0.040, val_acc:0.970]
Epoch [84/120    avg_loss:0.041, val_acc:0.974]
Epoch [85/120    avg_loss:0.044, val_acc:0.970]
Epoch [86/120    avg_loss:0.041, val_acc:0.974]
Epoch [87/120    avg_loss:0.025, val_acc:0.974]
Epoch [88/120    avg_loss:0.029, val_acc:0.980]
Epoch [89/120    avg_loss:0.018, val_acc:0.980]
Epoch [90/120    avg_loss:0.019, val_acc:0.978]
Epoch [91/120    avg_loss:0.020, val_acc:0.980]
Epoch [92/120    avg_loss:0.020, val_acc:0.981]
Epoch [93/120    avg_loss:0.016, val_acc:0.980]
Epoch [94/120    avg_loss:0.014, val_acc:0.980]
Epoch [95/120    avg_loss:0.015, val_acc:0.978]
Epoch [96/120    avg_loss:0.019, val_acc:0.980]
Epoch [97/120    avg_loss:0.020, val_acc:0.981]
Epoch [98/120    avg_loss:0.015, val_acc:0.981]
Epoch [99/120    avg_loss:0.015, val_acc:0.981]
Epoch [100/120    avg_loss:0.015, val_acc:0.981]
Epoch [101/120    avg_loss:0.016, val_acc:0.981]
Epoch [102/120    avg_loss:0.014, val_acc:0.981]
Epoch [103/120    avg_loss:0.013, val_acc:0.981]
Epoch [104/120    avg_loss:0.014, val_acc:0.981]
Epoch [105/120    avg_loss:0.013, val_acc:0.981]
Epoch [106/120    avg_loss:0.015, val_acc:0.981]
Epoch [107/120    avg_loss:0.015, val_acc:0.981]
Epoch [108/120    avg_loss:0.013, val_acc:0.981]
Epoch [109/120    avg_loss:0.014, val_acc:0.981]
Epoch [110/120    avg_loss:0.016, val_acc:0.981]
Epoch [111/120    avg_loss:0.015, val_acc:0.981]
Epoch [112/120    avg_loss:0.013, val_acc:0.981]
Epoch [113/120    avg_loss:0.015, val_acc:0.981]
Epoch [114/120    avg_loss:0.016, val_acc:0.981]
Epoch [115/120    avg_loss:0.012, val_acc:0.981]
Epoch [116/120    avg_loss:0.015, val_acc:0.981]
Epoch [117/120    avg_loss:0.015, val_acc:0.981]
Epoch [118/120    avg_loss:0.014, val_acc:0.981]
Epoch [119/120    avg_loss:0.016, val_acc:0.981]
Epoch [120/120    avg_loss:0.016, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0 1256    1    2    0    2    0    0    0    2   22    0    0
     0    0    0]
 [   0    0    0  713    3    0    0    0    0   15    1    5    9    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    2    3    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   16    0    1    0    0
     0    0    0]
 [   0    0    7    0    0    0    1    0    0    0  848   12    2    0
     1    4    0]
 [   0    0    3    0    0    0    1    0    0    0   16 2162   17    0
     0    0   11]
 [   0    0    0    0    0    0    0    0    0    1    5    0  525    0
     1    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    21  317    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.6910569105691

F1 scores:
[       nan 0.92857143 0.98471188 0.97604381 0.98839907 0.99071926
 0.98491704 0.94339623 0.9953271  0.64       0.9708071  0.97961033
 0.96153846 0.99730458 0.98211949 0.92961877 0.91525424]

Kappa:
0.9736879779371233
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffa84487ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.784, val_acc:0.272]
Epoch [2/120    avg_loss:2.667, val_acc:0.286]
Epoch [3/120    avg_loss:2.515, val_acc:0.386]
Epoch [4/120    avg_loss:2.410, val_acc:0.434]
Epoch [5/120    avg_loss:2.355, val_acc:0.422]
Epoch [6/120    avg_loss:2.251, val_acc:0.452]
Epoch [7/120    avg_loss:2.163, val_acc:0.574]
Epoch [8/120    avg_loss:2.111, val_acc:0.587]
Epoch [9/120    avg_loss:2.035, val_acc:0.585]
Epoch [10/120    avg_loss:1.942, val_acc:0.589]
Epoch [11/120    avg_loss:1.853, val_acc:0.619]
Epoch [12/120    avg_loss:1.740, val_acc:0.628]
Epoch [13/120    avg_loss:1.634, val_acc:0.651]
Epoch [14/120    avg_loss:1.475, val_acc:0.684]
Epoch [15/120    avg_loss:1.367, val_acc:0.706]
Epoch [16/120    avg_loss:1.203, val_acc:0.699]
Epoch [17/120    avg_loss:1.089, val_acc:0.712]
Epoch [18/120    avg_loss:1.027, val_acc:0.735]
Epoch [19/120    avg_loss:0.965, val_acc:0.746]
Epoch [20/120    avg_loss:0.928, val_acc:0.734]
Epoch [21/120    avg_loss:0.849, val_acc:0.747]
Epoch [22/120    avg_loss:0.762, val_acc:0.753]
Epoch [23/120    avg_loss:0.695, val_acc:0.819]
Epoch [24/120    avg_loss:0.616, val_acc:0.779]
Epoch [25/120    avg_loss:0.561, val_acc:0.839]
Epoch [26/120    avg_loss:0.570, val_acc:0.841]
Epoch [27/120    avg_loss:0.620, val_acc:0.807]
Epoch [28/120    avg_loss:0.521, val_acc:0.807]
Epoch [29/120    avg_loss:0.450, val_acc:0.849]
Epoch [30/120    avg_loss:0.451, val_acc:0.835]
Epoch [31/120    avg_loss:0.529, val_acc:0.792]
Epoch [32/120    avg_loss:0.478, val_acc:0.807]
Epoch [33/120    avg_loss:0.444, val_acc:0.843]
Epoch [34/120    avg_loss:0.433, val_acc:0.817]
Epoch [35/120    avg_loss:0.393, val_acc:0.859]
Epoch [36/120    avg_loss:0.322, val_acc:0.856]
Epoch [37/120    avg_loss:0.262, val_acc:0.861]
Epoch [38/120    avg_loss:0.237, val_acc:0.901]
Epoch [39/120    avg_loss:0.217, val_acc:0.904]
Epoch [40/120    avg_loss:0.191, val_acc:0.876]
Epoch [41/120    avg_loss:0.179, val_acc:0.889]
Epoch [42/120    avg_loss:0.143, val_acc:0.919]
Epoch [43/120    avg_loss:0.142, val_acc:0.932]
Epoch [44/120    avg_loss:0.155, val_acc:0.916]
Epoch [45/120    avg_loss:0.161, val_acc:0.903]
Epoch [46/120    avg_loss:0.138, val_acc:0.919]
Epoch [47/120    avg_loss:0.102, val_acc:0.934]
Epoch [48/120    avg_loss:0.107, val_acc:0.930]
Epoch [49/120    avg_loss:0.124, val_acc:0.925]
Epoch [50/120    avg_loss:0.112, val_acc:0.938]
Epoch [51/120    avg_loss:0.097, val_acc:0.932]
Epoch [52/120    avg_loss:0.090, val_acc:0.946]
Epoch [53/120    avg_loss:0.103, val_acc:0.932]
Epoch [54/120    avg_loss:0.089, val_acc:0.936]
Epoch [55/120    avg_loss:0.082, val_acc:0.916]
Epoch [56/120    avg_loss:0.115, val_acc:0.941]
Epoch [57/120    avg_loss:0.086, val_acc:0.955]
Epoch [58/120    avg_loss:0.090, val_acc:0.932]
Epoch [59/120    avg_loss:0.074, val_acc:0.932]
Epoch [60/120    avg_loss:0.107, val_acc:0.932]
Epoch [61/120    avg_loss:0.119, val_acc:0.936]
Epoch [62/120    avg_loss:0.107, val_acc:0.934]
Epoch [63/120    avg_loss:0.056, val_acc:0.952]
Epoch [64/120    avg_loss:0.053, val_acc:0.960]
Epoch [65/120    avg_loss:0.054, val_acc:0.956]
Epoch [66/120    avg_loss:0.052, val_acc:0.956]
Epoch [67/120    avg_loss:0.040, val_acc:0.958]
Epoch [68/120    avg_loss:0.050, val_acc:0.953]
Epoch [69/120    avg_loss:0.049, val_acc:0.952]
Epoch [70/120    avg_loss:0.048, val_acc:0.957]
Epoch [71/120    avg_loss:0.047, val_acc:0.960]
Epoch [72/120    avg_loss:0.039, val_acc:0.961]
Epoch [73/120    avg_loss:0.031, val_acc:0.963]
Epoch [74/120    avg_loss:0.038, val_acc:0.963]
Epoch [75/120    avg_loss:0.038, val_acc:0.961]
Epoch [76/120    avg_loss:0.034, val_acc:0.960]
Epoch [77/120    avg_loss:0.042, val_acc:0.963]
Epoch [78/120    avg_loss:0.027, val_acc:0.962]
Epoch [79/120    avg_loss:0.030, val_acc:0.972]
Epoch [80/120    avg_loss:0.041, val_acc:0.955]
Epoch [81/120    avg_loss:0.031, val_acc:0.966]
Epoch [82/120    avg_loss:0.044, val_acc:0.955]
Epoch [83/120    avg_loss:0.052, val_acc:0.958]
Epoch [84/120    avg_loss:0.034, val_acc:0.963]
Epoch [85/120    avg_loss:0.059, val_acc:0.960]
Epoch [86/120    avg_loss:0.069, val_acc:0.952]
Epoch [87/120    avg_loss:0.047, val_acc:0.968]
Epoch [88/120    avg_loss:0.067, val_acc:0.952]
Epoch [89/120    avg_loss:0.046, val_acc:0.968]
Epoch [90/120    avg_loss:0.034, val_acc:0.960]
Epoch [91/120    avg_loss:0.028, val_acc:0.967]
Epoch [92/120    avg_loss:0.026, val_acc:0.971]
Epoch [93/120    avg_loss:0.027, val_acc:0.969]
Epoch [94/120    avg_loss:0.022, val_acc:0.971]
Epoch [95/120    avg_loss:0.017, val_acc:0.974]
Epoch [96/120    avg_loss:0.017, val_acc:0.972]
Epoch [97/120    avg_loss:0.015, val_acc:0.973]
Epoch [98/120    avg_loss:0.015, val_acc:0.972]
Epoch [99/120    avg_loss:0.015, val_acc:0.970]
Epoch [100/120    avg_loss:0.015, val_acc:0.971]
Epoch [101/120    avg_loss:0.017, val_acc:0.969]
Epoch [102/120    avg_loss:0.017, val_acc:0.969]
Epoch [103/120    avg_loss:0.018, val_acc:0.971]
Epoch [104/120    avg_loss:0.014, val_acc:0.972]
Epoch [105/120    avg_loss:0.012, val_acc:0.973]
Epoch [106/120    avg_loss:0.012, val_acc:0.972]
Epoch [107/120    avg_loss:0.014, val_acc:0.970]
Epoch [108/120    avg_loss:0.013, val_acc:0.971]
Epoch [109/120    avg_loss:0.016, val_acc:0.971]
Epoch [110/120    avg_loss:0.011, val_acc:0.971]
Epoch [111/120    avg_loss:0.013, val_acc:0.971]
Epoch [112/120    avg_loss:0.013, val_acc:0.971]
Epoch [113/120    avg_loss:0.015, val_acc:0.971]
Epoch [114/120    avg_loss:0.014, val_acc:0.971]
Epoch [115/120    avg_loss:0.012, val_acc:0.970]
Epoch [116/120    avg_loss:0.013, val_acc:0.970]
Epoch [117/120    avg_loss:0.013, val_acc:0.970]
Epoch [118/120    avg_loss:0.013, val_acc:0.970]
Epoch [119/120    avg_loss:0.013, val_acc:0.970]
Epoch [120/120    avg_loss:0.016, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1249    3    9    0    1    0    0    0    2   21    0    0
     0    0    0]
 [   0    0    0  713    7    0    1    0    0    4    3    5   14    0
     0    0    0]
 [   0    0    0    5  207    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    4    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   10    0    0    0    0    0    0    3  849   13    0    0
     0    0    0]
 [   0    0   10    0    0    0    3    0    0    2    7 2187    1    0
     0    0    0]
 [   0    0    6    4    0    1    0    0    0    0   12    2  504    0
     1    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    5    0    1    0    0
  1124    8    0]
 [   0    0    0    0    0    0    1    0    0   13    0    0    0    0
    39  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.42005420054201

F1 scores:
[       nan 0.98765432 0.97578125 0.96875    0.94954128 0.99424626
 0.99086758 1.         1.         0.53125    0.97084048 0.98446995
 0.95184136 1.         0.97484822 0.90461538 0.95808383]

Kappa:
0.9705726493674778
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd9cd54f9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.761, val_acc:0.178]
Epoch [2/120    avg_loss:2.651, val_acc:0.392]
Epoch [3/120    avg_loss:2.531, val_acc:0.409]
Epoch [4/120    avg_loss:2.433, val_acc:0.416]
Epoch [5/120    avg_loss:2.340, val_acc:0.464]
Epoch [6/120    avg_loss:2.243, val_acc:0.443]
Epoch [7/120    avg_loss:2.165, val_acc:0.448]
Epoch [8/120    avg_loss:2.090, val_acc:0.502]
Epoch [9/120    avg_loss:2.049, val_acc:0.531]
Epoch [10/120    avg_loss:1.935, val_acc:0.555]
Epoch [11/120    avg_loss:1.861, val_acc:0.583]
Epoch [12/120    avg_loss:1.788, val_acc:0.605]
Epoch [13/120    avg_loss:1.715, val_acc:0.588]
Epoch [14/120    avg_loss:1.590, val_acc:0.635]
Epoch [15/120    avg_loss:1.527, val_acc:0.650]
Epoch [16/120    avg_loss:1.410, val_acc:0.635]
Epoch [17/120    avg_loss:1.309, val_acc:0.655]
Epoch [18/120    avg_loss:1.255, val_acc:0.675]
Epoch [19/120    avg_loss:1.166, val_acc:0.699]
Epoch [20/120    avg_loss:1.075, val_acc:0.692]
Epoch [21/120    avg_loss:0.969, val_acc:0.703]
Epoch [22/120    avg_loss:0.915, val_acc:0.696]
Epoch [23/120    avg_loss:0.848, val_acc:0.753]
Epoch [24/120    avg_loss:0.844, val_acc:0.703]
Epoch [25/120    avg_loss:0.827, val_acc:0.723]
Epoch [26/120    avg_loss:0.664, val_acc:0.762]
Epoch [27/120    avg_loss:0.610, val_acc:0.808]
Epoch [28/120    avg_loss:0.561, val_acc:0.815]
Epoch [29/120    avg_loss:0.503, val_acc:0.829]
Epoch [30/120    avg_loss:0.483, val_acc:0.849]
Epoch [31/120    avg_loss:0.396, val_acc:0.862]
Epoch [32/120    avg_loss:0.354, val_acc:0.857]
Epoch [33/120    avg_loss:0.414, val_acc:0.864]
Epoch [34/120    avg_loss:0.372, val_acc:0.879]
Epoch [35/120    avg_loss:0.323, val_acc:0.864]
Epoch [36/120    avg_loss:0.297, val_acc:0.865]
Epoch [37/120    avg_loss:0.274, val_acc:0.887]
Epoch [38/120    avg_loss:0.278, val_acc:0.878]
Epoch [39/120    avg_loss:0.263, val_acc:0.895]
Epoch [40/120    avg_loss:0.285, val_acc:0.915]
Epoch [41/120    avg_loss:0.267, val_acc:0.883]
Epoch [42/120    avg_loss:0.261, val_acc:0.883]
Epoch [43/120    avg_loss:0.264, val_acc:0.884]
Epoch [44/120    avg_loss:0.230, val_acc:0.913]
Epoch [45/120    avg_loss:0.190, val_acc:0.921]
Epoch [46/120    avg_loss:0.187, val_acc:0.912]
Epoch [47/120    avg_loss:0.200, val_acc:0.923]
Epoch [48/120    avg_loss:0.179, val_acc:0.906]
Epoch [49/120    avg_loss:0.168, val_acc:0.914]
Epoch [50/120    avg_loss:0.167, val_acc:0.925]
Epoch [51/120    avg_loss:0.141, val_acc:0.927]
Epoch [52/120    avg_loss:0.136, val_acc:0.941]
Epoch [53/120    avg_loss:0.135, val_acc:0.925]
Epoch [54/120    avg_loss:0.135, val_acc:0.936]
Epoch [55/120    avg_loss:0.107, val_acc:0.945]
Epoch [56/120    avg_loss:0.094, val_acc:0.943]
Epoch [57/120    avg_loss:0.103, val_acc:0.932]
Epoch [58/120    avg_loss:0.162, val_acc:0.921]
Epoch [59/120    avg_loss:0.145, val_acc:0.943]
Epoch [60/120    avg_loss:0.193, val_acc:0.891]
Epoch [61/120    avg_loss:0.202, val_acc:0.905]
Epoch [62/120    avg_loss:0.154, val_acc:0.928]
Epoch [63/120    avg_loss:0.140, val_acc:0.927]
Epoch [64/120    avg_loss:0.131, val_acc:0.946]
Epoch [65/120    avg_loss:0.113, val_acc:0.938]
Epoch [66/120    avg_loss:0.093, val_acc:0.955]
Epoch [67/120    avg_loss:0.078, val_acc:0.952]
Epoch [68/120    avg_loss:0.075, val_acc:0.960]
Epoch [69/120    avg_loss:0.074, val_acc:0.946]
Epoch [70/120    avg_loss:0.075, val_acc:0.954]
Epoch [71/120    avg_loss:0.096, val_acc:0.939]
Epoch [72/120    avg_loss:0.089, val_acc:0.950]
Epoch [73/120    avg_loss:0.063, val_acc:0.964]
Epoch [74/120    avg_loss:0.059, val_acc:0.962]
Epoch [75/120    avg_loss:0.048, val_acc:0.964]
Epoch [76/120    avg_loss:0.049, val_acc:0.954]
Epoch [77/120    avg_loss:0.050, val_acc:0.962]
Epoch [78/120    avg_loss:0.055, val_acc:0.966]
Epoch [79/120    avg_loss:0.043, val_acc:0.976]
Epoch [80/120    avg_loss:0.041, val_acc:0.967]
Epoch [81/120    avg_loss:0.039, val_acc:0.968]
Epoch [82/120    avg_loss:0.060, val_acc:0.939]
Epoch [83/120    avg_loss:0.059, val_acc:0.968]
Epoch [84/120    avg_loss:0.046, val_acc:0.968]
Epoch [85/120    avg_loss:0.057, val_acc:0.959]
Epoch [86/120    avg_loss:0.082, val_acc:0.948]
Epoch [87/120    avg_loss:0.058, val_acc:0.971]
Epoch [88/120    avg_loss:0.047, val_acc:0.961]
Epoch [89/120    avg_loss:0.046, val_acc:0.971]
Epoch [90/120    avg_loss:0.041, val_acc:0.976]
Epoch [91/120    avg_loss:0.028, val_acc:0.980]
Epoch [92/120    avg_loss:0.040, val_acc:0.964]
Epoch [93/120    avg_loss:0.044, val_acc:0.967]
Epoch [94/120    avg_loss:0.048, val_acc:0.953]
Epoch [95/120    avg_loss:0.037, val_acc:0.967]
Epoch [96/120    avg_loss:0.030, val_acc:0.976]
Epoch [97/120    avg_loss:0.039, val_acc:0.959]
Epoch [98/120    avg_loss:0.038, val_acc:0.977]
Epoch [99/120    avg_loss:0.031, val_acc:0.975]
Epoch [100/120    avg_loss:0.042, val_acc:0.962]
Epoch [101/120    avg_loss:0.039, val_acc:0.972]
Epoch [102/120    avg_loss:0.041, val_acc:0.968]
Epoch [103/120    avg_loss:0.044, val_acc:0.974]
Epoch [104/120    avg_loss:0.033, val_acc:0.978]
Epoch [105/120    avg_loss:0.032, val_acc:0.978]
Epoch [106/120    avg_loss:0.020, val_acc:0.976]
Epoch [107/120    avg_loss:0.023, val_acc:0.977]
Epoch [108/120    avg_loss:0.020, val_acc:0.977]
Epoch [109/120    avg_loss:0.026, val_acc:0.980]
Epoch [110/120    avg_loss:0.023, val_acc:0.977]
Epoch [111/120    avg_loss:0.022, val_acc:0.980]
Epoch [112/120    avg_loss:0.023, val_acc:0.980]
Epoch [113/120    avg_loss:0.022, val_acc:0.981]
Epoch [114/120    avg_loss:0.022, val_acc:0.983]
Epoch [115/120    avg_loss:0.017, val_acc:0.984]
Epoch [116/120    avg_loss:0.019, val_acc:0.983]
Epoch [117/120    avg_loss:0.019, val_acc:0.983]
Epoch [118/120    avg_loss:0.019, val_acc:0.983]
Epoch [119/120    avg_loss:0.022, val_acc:0.984]
Epoch [120/120    avg_loss:0.020, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    1    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1246    0    5    0    2    0    0    1    4   27    0    0
     0    0    0]
 [   0    0    1  699    9    8    0    1    0    8    7    1   13    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  421    1    6    0    0    0    1    0    0
     6    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   12    0    0    8    1    0    0    0  845    6    0    0
     0    3    0]
 [   0    0   12    0    0    0    4    0    0    0   24 2156   12    2
     0    0    0]
 [   0    0    2    0    0   11    0    0    0    2    7    5  504    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1131    5    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    48  287    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
96.73712737127371

F1 scores:
[       nan 0.93506494 0.9738179  0.96680498 0.96583144 0.9503386
 0.98271976 0.87719298 0.997669   0.73913043 0.95696489 0.97799955
 0.93942218 0.99462366 0.97332186 0.89269051 0.95121951]

Kappa:
0.9628016409838605
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff3037c4ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.797, val_acc:0.179]
Epoch [2/120    avg_loss:2.643, val_acc:0.388]
Epoch [3/120    avg_loss:2.531, val_acc:0.486]
Epoch [4/120    avg_loss:2.439, val_acc:0.524]
Epoch [5/120    avg_loss:2.342, val_acc:0.524]
Epoch [6/120    avg_loss:2.273, val_acc:0.541]
Epoch [7/120    avg_loss:2.199, val_acc:0.560]
Epoch [8/120    avg_loss:2.114, val_acc:0.594]
Epoch [9/120    avg_loss:2.037, val_acc:0.633]
Epoch [10/120    avg_loss:1.968, val_acc:0.648]
Epoch [11/120    avg_loss:1.916, val_acc:0.628]
Epoch [12/120    avg_loss:1.865, val_acc:0.640]
Epoch [13/120    avg_loss:1.789, val_acc:0.654]
Epoch [14/120    avg_loss:1.717, val_acc:0.663]
Epoch [15/120    avg_loss:1.591, val_acc:0.680]
Epoch [16/120    avg_loss:1.555, val_acc:0.675]
Epoch [17/120    avg_loss:1.507, val_acc:0.716]
Epoch [18/120    avg_loss:1.358, val_acc:0.711]
Epoch [19/120    avg_loss:1.231, val_acc:0.735]
Epoch [20/120    avg_loss:1.160, val_acc:0.699]
Epoch [21/120    avg_loss:1.048, val_acc:0.768]
Epoch [22/120    avg_loss:0.960, val_acc:0.776]
Epoch [23/120    avg_loss:0.874, val_acc:0.797]
Epoch [24/120    avg_loss:0.801, val_acc:0.815]
Epoch [25/120    avg_loss:0.770, val_acc:0.818]
Epoch [26/120    avg_loss:0.757, val_acc:0.793]
Epoch [27/120    avg_loss:0.753, val_acc:0.800]
Epoch [28/120    avg_loss:0.657, val_acc:0.852]
Epoch [29/120    avg_loss:0.577, val_acc:0.847]
Epoch [30/120    avg_loss:0.518, val_acc:0.851]
Epoch [31/120    avg_loss:0.476, val_acc:0.866]
Epoch [32/120    avg_loss:0.404, val_acc:0.881]
Epoch [33/120    avg_loss:0.396, val_acc:0.881]
Epoch [34/120    avg_loss:0.381, val_acc:0.883]
Epoch [35/120    avg_loss:0.341, val_acc:0.900]
Epoch [36/120    avg_loss:0.325, val_acc:0.891]
Epoch [37/120    avg_loss:0.318, val_acc:0.902]
Epoch [38/120    avg_loss:0.304, val_acc:0.875]
Epoch [39/120    avg_loss:0.312, val_acc:0.895]
Epoch [40/120    avg_loss:0.242, val_acc:0.905]
Epoch [41/120    avg_loss:0.233, val_acc:0.915]
Epoch [42/120    avg_loss:0.186, val_acc:0.920]
Epoch [43/120    avg_loss:0.197, val_acc:0.912]
Epoch [44/120    avg_loss:0.176, val_acc:0.925]
Epoch [45/120    avg_loss:0.160, val_acc:0.933]
Epoch [46/120    avg_loss:0.139, val_acc:0.947]
Epoch [47/120    avg_loss:0.124, val_acc:0.932]
Epoch [48/120    avg_loss:0.139, val_acc:0.913]
Epoch [49/120    avg_loss:0.149, val_acc:0.942]
Epoch [50/120    avg_loss:0.167, val_acc:0.904]
Epoch [51/120    avg_loss:0.178, val_acc:0.914]
Epoch [52/120    avg_loss:0.126, val_acc:0.942]
Epoch [53/120    avg_loss:0.115, val_acc:0.941]
Epoch [54/120    avg_loss:0.110, val_acc:0.947]
Epoch [55/120    avg_loss:0.088, val_acc:0.952]
Epoch [56/120    avg_loss:0.083, val_acc:0.942]
Epoch [57/120    avg_loss:0.081, val_acc:0.941]
Epoch [58/120    avg_loss:0.079, val_acc:0.949]
Epoch [59/120    avg_loss:0.073, val_acc:0.944]
Epoch [60/120    avg_loss:0.067, val_acc:0.945]
Epoch [61/120    avg_loss:0.072, val_acc:0.957]
Epoch [62/120    avg_loss:0.059, val_acc:0.957]
Epoch [63/120    avg_loss:0.058, val_acc:0.960]
Epoch [64/120    avg_loss:0.064, val_acc:0.955]
Epoch [65/120    avg_loss:0.059, val_acc:0.944]
Epoch [66/120    avg_loss:0.054, val_acc:0.964]
Epoch [67/120    avg_loss:0.067, val_acc:0.950]
Epoch [68/120    avg_loss:0.053, val_acc:0.966]
Epoch [69/120    avg_loss:0.053, val_acc:0.961]
Epoch [70/120    avg_loss:0.047, val_acc:0.963]
Epoch [71/120    avg_loss:0.054, val_acc:0.958]
Epoch [72/120    avg_loss:0.040, val_acc:0.961]
Epoch [73/120    avg_loss:0.040, val_acc:0.964]
Epoch [74/120    avg_loss:0.048, val_acc:0.943]
Epoch [75/120    avg_loss:0.056, val_acc:0.960]
Epoch [76/120    avg_loss:0.045, val_acc:0.962]
Epoch [77/120    avg_loss:0.039, val_acc:0.963]
Epoch [78/120    avg_loss:0.043, val_acc:0.962]
Epoch [79/120    avg_loss:0.032, val_acc:0.964]
Epoch [80/120    avg_loss:0.037, val_acc:0.963]
Epoch [81/120    avg_loss:0.029, val_acc:0.961]
Epoch [82/120    avg_loss:0.028, val_acc:0.960]
Epoch [83/120    avg_loss:0.025, val_acc:0.964]
Epoch [84/120    avg_loss:0.019, val_acc:0.970]
Epoch [85/120    avg_loss:0.019, val_acc:0.970]
Epoch [86/120    avg_loss:0.022, val_acc:0.972]
Epoch [87/120    avg_loss:0.024, val_acc:0.969]
Epoch [88/120    avg_loss:0.019, val_acc:0.972]
Epoch [89/120    avg_loss:0.021, val_acc:0.970]
Epoch [90/120    avg_loss:0.026, val_acc:0.972]
Epoch [91/120    avg_loss:0.022, val_acc:0.971]
Epoch [92/120    avg_loss:0.020, val_acc:0.971]
Epoch [93/120    avg_loss:0.022, val_acc:0.973]
Epoch [94/120    avg_loss:0.019, val_acc:0.973]
Epoch [95/120    avg_loss:0.020, val_acc:0.970]
Epoch [96/120    avg_loss:0.016, val_acc:0.968]
Epoch [97/120    avg_loss:0.023, val_acc:0.970]
Epoch [98/120    avg_loss:0.020, val_acc:0.972]
Epoch [99/120    avg_loss:0.021, val_acc:0.973]
Epoch [100/120    avg_loss:0.019, val_acc:0.971]
Epoch [101/120    avg_loss:0.022, val_acc:0.971]
Epoch [102/120    avg_loss:0.021, val_acc:0.972]
Epoch [103/120    avg_loss:0.018, val_acc:0.970]
Epoch [104/120    avg_loss:0.020, val_acc:0.973]
Epoch [105/120    avg_loss:0.019, val_acc:0.972]
Epoch [106/120    avg_loss:0.019, val_acc:0.973]
Epoch [107/120    avg_loss:0.019, val_acc:0.973]
Epoch [108/120    avg_loss:0.017, val_acc:0.972]
Epoch [109/120    avg_loss:0.018, val_acc:0.974]
Epoch [110/120    avg_loss:0.016, val_acc:0.973]
Epoch [111/120    avg_loss:0.019, val_acc:0.972]
Epoch [112/120    avg_loss:0.020, val_acc:0.972]
Epoch [113/120    avg_loss:0.017, val_acc:0.974]
Epoch [114/120    avg_loss:0.021, val_acc:0.972]
Epoch [115/120    avg_loss:0.020, val_acc:0.973]
Epoch [116/120    avg_loss:0.016, val_acc:0.973]
Epoch [117/120    avg_loss:0.014, val_acc:0.974]
Epoch [118/120    avg_loss:0.015, val_acc:0.973]
Epoch [119/120    avg_loss:0.017, val_acc:0.974]
Epoch [120/120    avg_loss:0.022, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1256    1    0    0    1    0    0    1    6   15    1    0
     0    4    0]
 [   0    0    0  695    0   12    0    0    0    7    0    1   29    2
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    9    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   36    0    0    5    1    0    0    0  826    0    0    0
     0    7    0]
 [   0    0    9    0    0    1    0    1    0    0   20 2174    1    1
     3    0    0]
 [   0    0    0    0    2    1    0    0    0    0    2    7  513    0
     5    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    2    0    0    0
  1128    5    0]
 [   0    0    0    0    0    0   18    0    0    3    0    0    0    0
    14  312    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.27913279132791

F1 scores:
[       nan 0.96202532 0.97138438 0.96327096 0.9953271  0.96371882
 0.98348348 0.83333333 0.99883856 0.72340426 0.9527105  0.98616466
 0.94912118 0.9919571  0.98558322 0.92035398 0.97619048]

Kappa:
0.9689892593866796
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f551af51b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.822, val_acc:0.206]
Epoch [2/120    avg_loss:2.730, val_acc:0.264]
Epoch [3/120    avg_loss:2.618, val_acc:0.321]
Epoch [4/120    avg_loss:2.513, val_acc:0.322]
Epoch [5/120    avg_loss:2.419, val_acc:0.403]
Epoch [6/120    avg_loss:2.317, val_acc:0.454]
Epoch [7/120    avg_loss:2.275, val_acc:0.492]
Epoch [8/120    avg_loss:2.173, val_acc:0.510]
Epoch [9/120    avg_loss:2.106, val_acc:0.483]
Epoch [10/120    avg_loss:2.066, val_acc:0.515]
Epoch [11/120    avg_loss:1.948, val_acc:0.533]
Epoch [12/120    avg_loss:1.879, val_acc:0.598]
Epoch [13/120    avg_loss:1.793, val_acc:0.605]
Epoch [14/120    avg_loss:1.711, val_acc:0.614]
Epoch [15/120    avg_loss:1.620, val_acc:0.647]
Epoch [16/120    avg_loss:1.561, val_acc:0.683]
Epoch [17/120    avg_loss:1.439, val_acc:0.698]
Epoch [18/120    avg_loss:1.345, val_acc:0.689]
Epoch [19/120    avg_loss:1.268, val_acc:0.688]
Epoch [20/120    avg_loss:1.178, val_acc:0.718]
Epoch [21/120    avg_loss:1.181, val_acc:0.664]
Epoch [22/120    avg_loss:1.041, val_acc:0.736]
Epoch [23/120    avg_loss:0.965, val_acc:0.740]
Epoch [24/120    avg_loss:0.871, val_acc:0.739]
Epoch [25/120    avg_loss:0.897, val_acc:0.733]
Epoch [26/120    avg_loss:0.743, val_acc:0.756]
Epoch [27/120    avg_loss:0.694, val_acc:0.774]
Epoch [28/120    avg_loss:0.663, val_acc:0.766]
Epoch [29/120    avg_loss:0.667, val_acc:0.764]
Epoch [30/120    avg_loss:0.604, val_acc:0.804]
Epoch [31/120    avg_loss:0.520, val_acc:0.797]
Epoch [32/120    avg_loss:0.463, val_acc:0.815]
Epoch [33/120    avg_loss:0.445, val_acc:0.837]
Epoch [34/120    avg_loss:0.409, val_acc:0.843]
Epoch [35/120    avg_loss:0.404, val_acc:0.837]
Epoch [36/120    avg_loss:0.392, val_acc:0.836]
Epoch [37/120    avg_loss:0.334, val_acc:0.860]
Epoch [38/120    avg_loss:0.320, val_acc:0.859]
Epoch [39/120    avg_loss:0.336, val_acc:0.875]
Epoch [40/120    avg_loss:0.250, val_acc:0.892]
Epoch [41/120    avg_loss:0.265, val_acc:0.888]
Epoch [42/120    avg_loss:0.254, val_acc:0.887]
Epoch [43/120    avg_loss:0.241, val_acc:0.879]
Epoch [44/120    avg_loss:0.222, val_acc:0.894]
Epoch [45/120    avg_loss:0.208, val_acc:0.876]
Epoch [46/120    avg_loss:0.253, val_acc:0.885]
Epoch [47/120    avg_loss:0.272, val_acc:0.905]
Epoch [48/120    avg_loss:0.185, val_acc:0.905]
Epoch [49/120    avg_loss:0.171, val_acc:0.922]
Epoch [50/120    avg_loss:0.161, val_acc:0.909]
Epoch [51/120    avg_loss:0.125, val_acc:0.921]
Epoch [52/120    avg_loss:0.132, val_acc:0.927]
Epoch [53/120    avg_loss:0.121, val_acc:0.929]
Epoch [54/120    avg_loss:0.118, val_acc:0.922]
Epoch [55/120    avg_loss:0.122, val_acc:0.916]
Epoch [56/120    avg_loss:0.112, val_acc:0.944]
Epoch [57/120    avg_loss:0.098, val_acc:0.943]
Epoch [58/120    avg_loss:0.082, val_acc:0.940]
Epoch [59/120    avg_loss:0.102, val_acc:0.927]
Epoch [60/120    avg_loss:0.108, val_acc:0.926]
Epoch [61/120    avg_loss:0.095, val_acc:0.934]
Epoch [62/120    avg_loss:0.109, val_acc:0.916]
Epoch [63/120    avg_loss:0.109, val_acc:0.935]
Epoch [64/120    avg_loss:0.130, val_acc:0.939]
Epoch [65/120    avg_loss:0.108, val_acc:0.941]
Epoch [66/120    avg_loss:0.110, val_acc:0.949]
Epoch [67/120    avg_loss:0.072, val_acc:0.939]
Epoch [68/120    avg_loss:0.071, val_acc:0.947]
Epoch [69/120    avg_loss:0.064, val_acc:0.949]
Epoch [70/120    avg_loss:0.073, val_acc:0.949]
Epoch [71/120    avg_loss:0.059, val_acc:0.929]
Epoch [72/120    avg_loss:0.222, val_acc:0.765]
Epoch [73/120    avg_loss:0.302, val_acc:0.892]
Epoch [74/120    avg_loss:0.266, val_acc:0.913]
Epoch [75/120    avg_loss:0.161, val_acc:0.914]
Epoch [76/120    avg_loss:0.132, val_acc:0.911]
Epoch [77/120    avg_loss:0.109, val_acc:0.945]
Epoch [78/120    avg_loss:0.087, val_acc:0.929]
Epoch [79/120    avg_loss:0.090, val_acc:0.934]
Epoch [80/120    avg_loss:0.093, val_acc:0.942]
Epoch [81/120    avg_loss:0.069, val_acc:0.952]
Epoch [82/120    avg_loss:0.108, val_acc:0.943]
Epoch [83/120    avg_loss:0.095, val_acc:0.943]
Epoch [84/120    avg_loss:0.075, val_acc:0.941]
Epoch [85/120    avg_loss:0.067, val_acc:0.953]
Epoch [86/120    avg_loss:0.045, val_acc:0.945]
Epoch [87/120    avg_loss:0.052, val_acc:0.955]
Epoch [88/120    avg_loss:0.055, val_acc:0.955]
Epoch [89/120    avg_loss:0.056, val_acc:0.954]
Epoch [90/120    avg_loss:0.046, val_acc:0.956]
Epoch [91/120    avg_loss:0.045, val_acc:0.949]
Epoch [92/120    avg_loss:0.042, val_acc:0.947]
Epoch [93/120    avg_loss:0.052, val_acc:0.954]
Epoch [94/120    avg_loss:0.038, val_acc:0.953]
Epoch [95/120    avg_loss:0.046, val_acc:0.953]
Epoch [96/120    avg_loss:0.050, val_acc:0.940]
Epoch [97/120    avg_loss:0.053, val_acc:0.949]
Epoch [98/120    avg_loss:0.035, val_acc:0.947]
Epoch [99/120    avg_loss:0.035, val_acc:0.960]
Epoch [100/120    avg_loss:0.029, val_acc:0.964]
Epoch [101/120    avg_loss:0.027, val_acc:0.966]
Epoch [102/120    avg_loss:0.022, val_acc:0.964]
Epoch [103/120    avg_loss:0.027, val_acc:0.960]
Epoch [104/120    avg_loss:0.025, val_acc:0.966]
Epoch [105/120    avg_loss:0.020, val_acc:0.963]
Epoch [106/120    avg_loss:0.026, val_acc:0.928]
Epoch [107/120    avg_loss:0.032, val_acc:0.959]
Epoch [108/120    avg_loss:0.028, val_acc:0.961]
Epoch [109/120    avg_loss:0.033, val_acc:0.961]
Epoch [110/120    avg_loss:0.031, val_acc:0.960]
Epoch [111/120    avg_loss:0.031, val_acc:0.960]
Epoch [112/120    avg_loss:0.025, val_acc:0.956]
Epoch [113/120    avg_loss:0.032, val_acc:0.962]
Epoch [114/120    avg_loss:0.025, val_acc:0.957]
Epoch [115/120    avg_loss:0.029, val_acc:0.956]
Epoch [116/120    avg_loss:0.024, val_acc:0.963]
Epoch [117/120    avg_loss:0.023, val_acc:0.961]
Epoch [118/120    avg_loss:0.017, val_acc:0.964]
Epoch [119/120    avg_loss:0.015, val_acc:0.964]
Epoch [120/120    avg_loss:0.014, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    1 1259    0    1    0    0    0    0    0    7   17    0    0
     0    0    0]
 [   0    0    2  705   12    3    0    0    0   14    0    0   10    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    2    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   14    0    0    1    0
     0    0    0]
 [   0    0   11    0    0    6    1    0    0    0  838   16    1    0
     0    2    0]
 [   0    0   11    0    0    0    1    0    1    0   16 2161   20    0
     0    0    0]
 [   0    0    0    8    1   13    0    0    0    0    1    2  506    0
     0    0    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    2    0    0    0
  1118   18    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
     9  324    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.37669376693766

F1 scores:
[       nan 0.93670886 0.9805296  0.96377307 0.96818182 0.97078652
 0.98644578 0.96153846 0.99883856 0.59574468 0.96156053 0.98049002
 0.94227188 0.99459459 0.98632554 0.93777135 0.9704142 ]

Kappa:
0.9701108347203266
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb976771a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.789, val_acc:0.144]
Epoch [2/120    avg_loss:2.648, val_acc:0.414]
Epoch [3/120    avg_loss:2.524, val_acc:0.473]
Epoch [4/120    avg_loss:2.429, val_acc:0.495]
Epoch [5/120    avg_loss:2.337, val_acc:0.524]
Epoch [6/120    avg_loss:2.235, val_acc:0.522]
Epoch [7/120    avg_loss:2.193, val_acc:0.579]
Epoch [8/120    avg_loss:2.121, val_acc:0.601]
Epoch [9/120    avg_loss:2.017, val_acc:0.601]
Epoch [10/120    avg_loss:1.967, val_acc:0.631]
Epoch [11/120    avg_loss:1.882, val_acc:0.626]
Epoch [12/120    avg_loss:1.802, val_acc:0.637]
Epoch [13/120    avg_loss:1.723, val_acc:0.655]
Epoch [14/120    avg_loss:1.615, val_acc:0.673]
Epoch [15/120    avg_loss:1.546, val_acc:0.668]
Epoch [16/120    avg_loss:1.492, val_acc:0.653]
Epoch [17/120    avg_loss:1.383, val_acc:0.699]
Epoch [18/120    avg_loss:1.259, val_acc:0.679]
Epoch [19/120    avg_loss:1.138, val_acc:0.725]
Epoch [20/120    avg_loss:1.057, val_acc:0.731]
Epoch [21/120    avg_loss:1.029, val_acc:0.722]
Epoch [22/120    avg_loss:0.906, val_acc:0.748]
Epoch [23/120    avg_loss:0.866, val_acc:0.786]
Epoch [24/120    avg_loss:0.762, val_acc:0.777]
Epoch [25/120    avg_loss:0.750, val_acc:0.705]
Epoch [26/120    avg_loss:0.691, val_acc:0.809]
Epoch [27/120    avg_loss:0.618, val_acc:0.766]
Epoch [28/120    avg_loss:0.581, val_acc:0.822]
Epoch [29/120    avg_loss:0.549, val_acc:0.829]
Epoch [30/120    avg_loss:0.499, val_acc:0.855]
Epoch [31/120    avg_loss:0.460, val_acc:0.832]
Epoch [32/120    avg_loss:0.481, val_acc:0.824]
Epoch [33/120    avg_loss:0.372, val_acc:0.859]
Epoch [34/120    avg_loss:0.367, val_acc:0.886]
Epoch [35/120    avg_loss:0.353, val_acc:0.875]
Epoch [36/120    avg_loss:0.289, val_acc:0.903]
Epoch [37/120    avg_loss:0.295, val_acc:0.901]
Epoch [38/120    avg_loss:0.917, val_acc:0.709]
Epoch [39/120    avg_loss:0.551, val_acc:0.869]
Epoch [40/120    avg_loss:0.414, val_acc:0.836]
Epoch [41/120    avg_loss:0.417, val_acc:0.795]
Epoch [42/120    avg_loss:0.314, val_acc:0.866]
Epoch [43/120    avg_loss:0.298, val_acc:0.892]
Epoch [44/120    avg_loss:0.231, val_acc:0.894]
Epoch [45/120    avg_loss:0.215, val_acc:0.919]
Epoch [46/120    avg_loss:0.222, val_acc:0.908]
Epoch [47/120    avg_loss:0.342, val_acc:0.855]
Epoch [48/120    avg_loss:0.259, val_acc:0.886]
Epoch [49/120    avg_loss:0.239, val_acc:0.917]
Epoch [50/120    avg_loss:0.192, val_acc:0.930]
Epoch [51/120    avg_loss:0.179, val_acc:0.903]
Epoch [52/120    avg_loss:0.214, val_acc:0.907]
Epoch [53/120    avg_loss:0.155, val_acc:0.927]
Epoch [54/120    avg_loss:0.227, val_acc:0.912]
Epoch [55/120    avg_loss:0.194, val_acc:0.915]
Epoch [56/120    avg_loss:0.141, val_acc:0.953]
Epoch [57/120    avg_loss:0.132, val_acc:0.933]
Epoch [58/120    avg_loss:0.125, val_acc:0.932]
Epoch [59/120    avg_loss:0.110, val_acc:0.941]
Epoch [60/120    avg_loss:0.104, val_acc:0.950]
Epoch [61/120    avg_loss:0.095, val_acc:0.953]
Epoch [62/120    avg_loss:0.095, val_acc:0.953]
Epoch [63/120    avg_loss:0.093, val_acc:0.948]
Epoch [64/120    avg_loss:0.135, val_acc:0.925]
Epoch [65/120    avg_loss:0.131, val_acc:0.946]
Epoch [66/120    avg_loss:0.084, val_acc:0.949]
Epoch [67/120    avg_loss:0.079, val_acc:0.949]
Epoch [68/120    avg_loss:0.076, val_acc:0.962]
Epoch [69/120    avg_loss:0.062, val_acc:0.942]
Epoch [70/120    avg_loss:0.088, val_acc:0.943]
Epoch [71/120    avg_loss:0.075, val_acc:0.953]
Epoch [72/120    avg_loss:0.069, val_acc:0.962]
Epoch [73/120    avg_loss:0.071, val_acc:0.956]
Epoch [74/120    avg_loss:0.065, val_acc:0.954]
Epoch [75/120    avg_loss:0.056, val_acc:0.956]
Epoch [76/120    avg_loss:0.055, val_acc:0.958]
Epoch [77/120    avg_loss:0.053, val_acc:0.958]
Epoch [78/120    avg_loss:0.054, val_acc:0.966]
Epoch [79/120    avg_loss:0.056, val_acc:0.967]
Epoch [80/120    avg_loss:0.076, val_acc:0.946]
Epoch [81/120    avg_loss:0.060, val_acc:0.953]
Epoch [82/120    avg_loss:0.046, val_acc:0.966]
Epoch [83/120    avg_loss:0.042, val_acc:0.964]
Epoch [84/120    avg_loss:0.055, val_acc:0.945]
Epoch [85/120    avg_loss:0.049, val_acc:0.961]
Epoch [86/120    avg_loss:0.049, val_acc:0.967]
Epoch [87/120    avg_loss:0.036, val_acc:0.974]
Epoch [88/120    avg_loss:0.039, val_acc:0.968]
Epoch [89/120    avg_loss:0.040, val_acc:0.964]
Epoch [90/120    avg_loss:0.042, val_acc:0.971]
Epoch [91/120    avg_loss:0.038, val_acc:0.964]
Epoch [92/120    avg_loss:0.040, val_acc:0.962]
Epoch [93/120    avg_loss:0.035, val_acc:0.969]
Epoch [94/120    avg_loss:0.026, val_acc:0.973]
Epoch [95/120    avg_loss:0.034, val_acc:0.960]
Epoch [96/120    avg_loss:0.033, val_acc:0.972]
Epoch [97/120    avg_loss:0.031, val_acc:0.968]
Epoch [98/120    avg_loss:0.024, val_acc:0.976]
Epoch [99/120    avg_loss:0.026, val_acc:0.973]
Epoch [100/120    avg_loss:0.028, val_acc:0.972]
Epoch [101/120    avg_loss:0.027, val_acc:0.966]
Epoch [102/120    avg_loss:0.028, val_acc:0.967]
Epoch [103/120    avg_loss:0.032, val_acc:0.954]
Epoch [104/120    avg_loss:0.054, val_acc:0.958]
Epoch [105/120    avg_loss:0.030, val_acc:0.968]
Epoch [106/120    avg_loss:0.044, val_acc:0.960]
Epoch [107/120    avg_loss:0.033, val_acc:0.966]
Epoch [108/120    avg_loss:0.028, val_acc:0.966]
Epoch [109/120    avg_loss:0.027, val_acc:0.974]
Epoch [110/120    avg_loss:0.032, val_acc:0.943]
Epoch [111/120    avg_loss:0.046, val_acc:0.975]
Epoch [112/120    avg_loss:0.022, val_acc:0.978]
Epoch [113/120    avg_loss:0.021, val_acc:0.978]
Epoch [114/120    avg_loss:0.026, val_acc:0.976]
Epoch [115/120    avg_loss:0.019, val_acc:0.974]
Epoch [116/120    avg_loss:0.017, val_acc:0.977]
Epoch [117/120    avg_loss:0.018, val_acc:0.978]
Epoch [118/120    avg_loss:0.020, val_acc:0.977]
Epoch [119/120    avg_loss:0.017, val_acc:0.977]
Epoch [120/120    avg_loss:0.018, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1250    0    3    0    1    0    0    0    9   22    0    0
     0    0    0]
 [   0    0    1  688    2   17    0    0    0   14    4    3   18    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    5    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    6    0    0   10    0    0    2    0
     0    0    0]
 [   0    0   15    0    0    8    4    0    0    0  838    4    6    0
     0    0    0]
 [   0    0    6    0    0    0    2    0    0    0   14 2180    6    1
     1    0    0]
 [   0    0    0    1    0    6    0    0    0    0   10    2  511    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    0    0    1    0    0    0
  1128    0    0]
 [   0    0    0    0    0    0   31    0    0    0    0    0    0    0
     9  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.18157181571816

F1 scores:
[       nan 0.96385542 0.97770825 0.95821727 0.98839907 0.94456763
 0.96531365 0.90909091 0.997669   0.47619048 0.956621   0.98575627
 0.94717331 0.99730458 0.98817346 0.93883792 0.9704142 ]

Kappa:
0.967872678238423
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9b3160aa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.787, val_acc:0.230]
Epoch [2/120    avg_loss:2.675, val_acc:0.239]
Epoch [3/120    avg_loss:2.568, val_acc:0.255]
Epoch [4/120    avg_loss:2.467, val_acc:0.264]
Epoch [5/120    avg_loss:2.410, val_acc:0.324]
Epoch [6/120    avg_loss:2.322, val_acc:0.374]
Epoch [7/120    avg_loss:2.241, val_acc:0.397]
Epoch [8/120    avg_loss:2.151, val_acc:0.428]
Epoch [9/120    avg_loss:2.096, val_acc:0.484]
Epoch [10/120    avg_loss:2.041, val_acc:0.486]
Epoch [11/120    avg_loss:1.923, val_acc:0.526]
Epoch [12/120    avg_loss:1.874, val_acc:0.537]
Epoch [13/120    avg_loss:1.788, val_acc:0.616]
Epoch [14/120    avg_loss:1.669, val_acc:0.598]
Epoch [15/120    avg_loss:1.580, val_acc:0.652]
Epoch [16/120    avg_loss:1.468, val_acc:0.685]
Epoch [17/120    avg_loss:1.389, val_acc:0.662]
Epoch [18/120    avg_loss:1.340, val_acc:0.662]
Epoch [19/120    avg_loss:1.232, val_acc:0.722]
Epoch [20/120    avg_loss:1.139, val_acc:0.686]
Epoch [21/120    avg_loss:1.103, val_acc:0.676]
Epoch [22/120    avg_loss:0.988, val_acc:0.741]
Epoch [23/120    avg_loss:0.893, val_acc:0.744]
Epoch [24/120    avg_loss:0.906, val_acc:0.759]
Epoch [25/120    avg_loss:0.796, val_acc:0.798]
Epoch [26/120    avg_loss:0.696, val_acc:0.827]
Epoch [27/120    avg_loss:0.653, val_acc:0.824]
Epoch [28/120    avg_loss:0.670, val_acc:0.828]
Epoch [29/120    avg_loss:0.588, val_acc:0.848]
Epoch [30/120    avg_loss:0.486, val_acc:0.814]
Epoch [31/120    avg_loss:0.459, val_acc:0.856]
Epoch [32/120    avg_loss:0.409, val_acc:0.846]
Epoch [33/120    avg_loss:0.476, val_acc:0.810]
Epoch [34/120    avg_loss:0.388, val_acc:0.885]
Epoch [35/120    avg_loss:0.359, val_acc:0.874]
Epoch [36/120    avg_loss:0.384, val_acc:0.837]
Epoch [37/120    avg_loss:0.355, val_acc:0.888]
Epoch [38/120    avg_loss:0.308, val_acc:0.906]
Epoch [39/120    avg_loss:0.280, val_acc:0.888]
Epoch [40/120    avg_loss:0.309, val_acc:0.874]
Epoch [41/120    avg_loss:0.396, val_acc:0.864]
Epoch [42/120    avg_loss:0.245, val_acc:0.903]
Epoch [43/120    avg_loss:0.215, val_acc:0.903]
Epoch [44/120    avg_loss:0.205, val_acc:0.925]
Epoch [45/120    avg_loss:0.180, val_acc:0.925]
Epoch [46/120    avg_loss:0.221, val_acc:0.916]
Epoch [47/120    avg_loss:0.168, val_acc:0.919]
Epoch [48/120    avg_loss:0.211, val_acc:0.876]
Epoch [49/120    avg_loss:0.204, val_acc:0.919]
Epoch [50/120    avg_loss:0.188, val_acc:0.921]
Epoch [51/120    avg_loss:0.176, val_acc:0.936]
Epoch [52/120    avg_loss:0.141, val_acc:0.921]
Epoch [53/120    avg_loss:0.175, val_acc:0.867]
Epoch [54/120    avg_loss:0.166, val_acc:0.911]
Epoch [55/120    avg_loss:0.131, val_acc:0.940]
Epoch [56/120    avg_loss:0.102, val_acc:0.931]
Epoch [57/120    avg_loss:0.114, val_acc:0.939]
Epoch [58/120    avg_loss:0.082, val_acc:0.932]
Epoch [59/120    avg_loss:0.070, val_acc:0.950]
Epoch [60/120    avg_loss:0.082, val_acc:0.948]
Epoch [61/120    avg_loss:0.087, val_acc:0.952]
Epoch [62/120    avg_loss:0.082, val_acc:0.928]
Epoch [63/120    avg_loss:0.080, val_acc:0.930]
Epoch [64/120    avg_loss:0.102, val_acc:0.942]
Epoch [65/120    avg_loss:0.092, val_acc:0.947]
Epoch [66/120    avg_loss:0.243, val_acc:0.879]
Epoch [67/120    avg_loss:0.161, val_acc:0.931]
Epoch [68/120    avg_loss:0.097, val_acc:0.945]
Epoch [69/120    avg_loss:0.104, val_acc:0.940]
Epoch [70/120    avg_loss:0.092, val_acc:0.935]
Epoch [71/120    avg_loss:0.077, val_acc:0.946]
Epoch [72/120    avg_loss:0.070, val_acc:0.950]
Epoch [73/120    avg_loss:0.065, val_acc:0.934]
Epoch [74/120    avg_loss:0.071, val_acc:0.940]
Epoch [75/120    avg_loss:0.070, val_acc:0.961]
Epoch [76/120    avg_loss:0.050, val_acc:0.964]
Epoch [77/120    avg_loss:0.053, val_acc:0.964]
Epoch [78/120    avg_loss:0.043, val_acc:0.967]
Epoch [79/120    avg_loss:0.044, val_acc:0.967]
Epoch [80/120    avg_loss:0.043, val_acc:0.968]
Epoch [81/120    avg_loss:0.039, val_acc:0.971]
Epoch [82/120    avg_loss:0.040, val_acc:0.969]
Epoch [83/120    avg_loss:0.039, val_acc:0.970]
Epoch [84/120    avg_loss:0.036, val_acc:0.969]
Epoch [85/120    avg_loss:0.035, val_acc:0.971]
Epoch [86/120    avg_loss:0.033, val_acc:0.970]
Epoch [87/120    avg_loss:0.032, val_acc:0.971]
Epoch [88/120    avg_loss:0.038, val_acc:0.970]
Epoch [89/120    avg_loss:0.041, val_acc:0.970]
Epoch [90/120    avg_loss:0.034, val_acc:0.969]
Epoch [91/120    avg_loss:0.031, val_acc:0.969]
Epoch [92/120    avg_loss:0.037, val_acc:0.970]
Epoch [93/120    avg_loss:0.031, val_acc:0.966]
Epoch [94/120    avg_loss:0.035, val_acc:0.968]
Epoch [95/120    avg_loss:0.032, val_acc:0.970]
Epoch [96/120    avg_loss:0.034, val_acc:0.971]
Epoch [97/120    avg_loss:0.035, val_acc:0.970]
Epoch [98/120    avg_loss:0.030, val_acc:0.967]
Epoch [99/120    avg_loss:0.033, val_acc:0.968]
Epoch [100/120    avg_loss:0.029, val_acc:0.969]
Epoch [101/120    avg_loss:0.025, val_acc:0.970]
Epoch [102/120    avg_loss:0.027, val_acc:0.970]
Epoch [103/120    avg_loss:0.030, val_acc:0.970]
Epoch [104/120    avg_loss:0.026, val_acc:0.969]
Epoch [105/120    avg_loss:0.030, val_acc:0.968]
Epoch [106/120    avg_loss:0.029, val_acc:0.968]
Epoch [107/120    avg_loss:0.028, val_acc:0.970]
Epoch [108/120    avg_loss:0.029, val_acc:0.969]
Epoch [109/120    avg_loss:0.026, val_acc:0.969]
Epoch [110/120    avg_loss:0.029, val_acc:0.969]
Epoch [111/120    avg_loss:0.034, val_acc:0.969]
Epoch [112/120    avg_loss:0.027, val_acc:0.969]
Epoch [113/120    avg_loss:0.030, val_acc:0.969]
Epoch [114/120    avg_loss:0.030, val_acc:0.969]
Epoch [115/120    avg_loss:0.027, val_acc:0.969]
Epoch [116/120    avg_loss:0.027, val_acc:0.969]
Epoch [117/120    avg_loss:0.028, val_acc:0.970]
Epoch [118/120    avg_loss:0.029, val_acc:0.970]
Epoch [119/120    avg_loss:0.027, val_acc:0.971]
Epoch [120/120    avg_loss:0.026, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1249    6    0    0    0    0    0    0   10   16    4    0
     0    0    0]
 [   0    0    2  710    2    1    0    0    0   13    2    3   14    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    4    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    4    0    0    4    1    0    0    0  851   12    0    0
     0    3    0]
 [   0    0    7    0    0    0    1    0    2    0   25 2167    7    1
     0    0    0]
 [   0    0    0    0    1    4    0    0    0    0    3    6  512    0
     3    0    5]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    3    0    0    0
  1130    0    0]
 [   0    0    0    0    0    0   39    0    0    0    0    0    0    0
    13  295    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.42005420054201

F1 scores:
[       nan 0.98765432 0.98076168 0.96928328 0.9882904  0.97610922
 0.9697417  1.         0.99300699 0.64150943 0.96158192 0.98165345
 0.95167286 0.99459459 0.98862642 0.91472868 0.97109827]

Kappa:
0.9705931685581952
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5278ac2b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.744, val_acc:0.304]
Epoch [2/120    avg_loss:2.584, val_acc:0.356]
Epoch [3/120    avg_loss:2.500, val_acc:0.481]
Epoch [4/120    avg_loss:2.384, val_acc:0.499]
Epoch [5/120    avg_loss:2.298, val_acc:0.508]
Epoch [6/120    avg_loss:2.257, val_acc:0.505]
Epoch [7/120    avg_loss:2.156, val_acc:0.520]
Epoch [8/120    avg_loss:2.077, val_acc:0.538]
Epoch [9/120    avg_loss:2.023, val_acc:0.557]
Epoch [10/120    avg_loss:1.927, val_acc:0.565]
Epoch [11/120    avg_loss:1.868, val_acc:0.570]
Epoch [12/120    avg_loss:1.824, val_acc:0.570]
Epoch [13/120    avg_loss:1.730, val_acc:0.581]
Epoch [14/120    avg_loss:1.604, val_acc:0.610]
Epoch [15/120    avg_loss:1.527, val_acc:0.602]
Epoch [16/120    avg_loss:1.426, val_acc:0.670]
Epoch [17/120    avg_loss:1.350, val_acc:0.659]
Epoch [18/120    avg_loss:1.299, val_acc:0.694]
Epoch [19/120    avg_loss:1.217, val_acc:0.684]
Epoch [20/120    avg_loss:1.169, val_acc:0.711]
Epoch [21/120    avg_loss:1.046, val_acc:0.737]
Epoch [22/120    avg_loss:0.966, val_acc:0.736]
Epoch [23/120    avg_loss:0.917, val_acc:0.750]
Epoch [24/120    avg_loss:0.846, val_acc:0.783]
Epoch [25/120    avg_loss:0.742, val_acc:0.775]
Epoch [26/120    avg_loss:0.722, val_acc:0.794]
Epoch [27/120    avg_loss:0.699, val_acc:0.829]
Epoch [28/120    avg_loss:0.687, val_acc:0.831]
Epoch [29/120    avg_loss:0.621, val_acc:0.815]
Epoch [30/120    avg_loss:0.586, val_acc:0.773]
Epoch [31/120    avg_loss:0.583, val_acc:0.825]
Epoch [32/120    avg_loss:0.543, val_acc:0.869]
Epoch [33/120    avg_loss:0.470, val_acc:0.851]
Epoch [34/120    avg_loss:0.452, val_acc:0.883]
Epoch [35/120    avg_loss:0.396, val_acc:0.892]
Epoch [36/120    avg_loss:0.364, val_acc:0.883]
Epoch [37/120    avg_loss:0.330, val_acc:0.883]
Epoch [38/120    avg_loss:0.320, val_acc:0.893]
Epoch [39/120    avg_loss:0.312, val_acc:0.903]
Epoch [40/120    avg_loss:0.305, val_acc:0.888]
Epoch [41/120    avg_loss:0.248, val_acc:0.905]
Epoch [42/120    avg_loss:0.268, val_acc:0.912]
Epoch [43/120    avg_loss:0.245, val_acc:0.923]
Epoch [44/120    avg_loss:0.229, val_acc:0.927]
Epoch [45/120    avg_loss:0.223, val_acc:0.906]
Epoch [46/120    avg_loss:0.295, val_acc:0.917]
Epoch [47/120    avg_loss:0.242, val_acc:0.922]
Epoch [48/120    avg_loss:0.193, val_acc:0.927]
Epoch [49/120    avg_loss:0.206, val_acc:0.905]
Epoch [50/120    avg_loss:0.171, val_acc:0.925]
Epoch [51/120    avg_loss:0.137, val_acc:0.930]
Epoch [52/120    avg_loss:0.141, val_acc:0.925]
Epoch [53/120    avg_loss:0.142, val_acc:0.909]
Epoch [54/120    avg_loss:0.149, val_acc:0.936]
Epoch [55/120    avg_loss:0.140, val_acc:0.925]
Epoch [56/120    avg_loss:0.129, val_acc:0.940]
Epoch [57/120    avg_loss:0.111, val_acc:0.939]
Epoch [58/120    avg_loss:0.118, val_acc:0.954]
Epoch [59/120    avg_loss:0.087, val_acc:0.941]
Epoch [60/120    avg_loss:0.115, val_acc:0.938]
Epoch [61/120    avg_loss:0.127, val_acc:0.939]
Epoch [62/120    avg_loss:0.105, val_acc:0.945]
Epoch [63/120    avg_loss:0.084, val_acc:0.952]
Epoch [64/120    avg_loss:0.078, val_acc:0.922]
Epoch [65/120    avg_loss:0.069, val_acc:0.952]
Epoch [66/120    avg_loss:0.073, val_acc:0.956]
Epoch [67/120    avg_loss:0.069, val_acc:0.957]
Epoch [68/120    avg_loss:0.072, val_acc:0.960]
Epoch [69/120    avg_loss:0.072, val_acc:0.954]
Epoch [70/120    avg_loss:0.076, val_acc:0.953]
Epoch [71/120    avg_loss:0.104, val_acc:0.947]
Epoch [72/120    avg_loss:0.088, val_acc:0.962]
Epoch [73/120    avg_loss:0.066, val_acc:0.961]
Epoch [74/120    avg_loss:0.069, val_acc:0.952]
Epoch [75/120    avg_loss:0.066, val_acc:0.962]
Epoch [76/120    avg_loss:0.052, val_acc:0.964]
Epoch [77/120    avg_loss:0.052, val_acc:0.966]
Epoch [78/120    avg_loss:0.038, val_acc:0.960]
Epoch [79/120    avg_loss:0.040, val_acc:0.971]
Epoch [80/120    avg_loss:0.044, val_acc:0.973]
Epoch [81/120    avg_loss:0.048, val_acc:0.952]
Epoch [82/120    avg_loss:0.072, val_acc:0.961]
Epoch [83/120    avg_loss:0.053, val_acc:0.958]
Epoch [84/120    avg_loss:0.056, val_acc:0.967]
Epoch [85/120    avg_loss:0.063, val_acc:0.956]
Epoch [86/120    avg_loss:0.053, val_acc:0.972]
Epoch [87/120    avg_loss:0.049, val_acc:0.973]
Epoch [88/120    avg_loss:0.042, val_acc:0.963]
Epoch [89/120    avg_loss:0.037, val_acc:0.972]
Epoch [90/120    avg_loss:0.027, val_acc:0.968]
Epoch [91/120    avg_loss:0.038, val_acc:0.963]
Epoch [92/120    avg_loss:0.048, val_acc:0.957]
Epoch [93/120    avg_loss:0.041, val_acc:0.970]
Epoch [94/120    avg_loss:0.045, val_acc:0.969]
Epoch [95/120    avg_loss:0.035, val_acc:0.969]
Epoch [96/120    avg_loss:0.056, val_acc:0.964]
Epoch [97/120    avg_loss:0.029, val_acc:0.966]
Epoch [98/120    avg_loss:0.029, val_acc:0.969]
Epoch [99/120    avg_loss:0.034, val_acc:0.976]
Epoch [100/120    avg_loss:0.024, val_acc:0.971]
Epoch [101/120    avg_loss:0.022, val_acc:0.971]
Epoch [102/120    avg_loss:0.021, val_acc:0.964]
Epoch [103/120    avg_loss:0.027, val_acc:0.969]
Epoch [104/120    avg_loss:0.025, val_acc:0.971]
Epoch [105/120    avg_loss:0.032, val_acc:0.968]
Epoch [106/120    avg_loss:0.031, val_acc:0.972]
Epoch [107/120    avg_loss:0.038, val_acc:0.970]
Epoch [108/120    avg_loss:0.029, val_acc:0.970]
Epoch [109/120    avg_loss:0.034, val_acc:0.967]
Epoch [110/120    avg_loss:0.035, val_acc:0.980]
Epoch [111/120    avg_loss:0.028, val_acc:0.963]
Epoch [112/120    avg_loss:0.028, val_acc:0.970]
Epoch [113/120    avg_loss:0.032, val_acc:0.963]
Epoch [114/120    avg_loss:0.062, val_acc:0.912]
Epoch [115/120    avg_loss:0.063, val_acc:0.966]
Epoch [116/120    avg_loss:0.034, val_acc:0.971]
Epoch [117/120    avg_loss:0.027, val_acc:0.973]
Epoch [118/120    avg_loss:0.021, val_acc:0.969]
Epoch [119/120    avg_loss:0.017, val_acc:0.974]
Epoch [120/120    avg_loss:0.018, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1203    1    1    0    2    0    0    0    6   70    2    0
     0    0    0]
 [   0    0    2  681    6   13    2    0    0   15   10    0   18    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    1    4    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    2    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    2    0    0    8    3    0    0    0  847   14    1    0
     0    0    0]
 [   0    0    2    0    0    1    4    4    0    0   17 2176    5    1
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    1   13    1  509    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    1    0    0    0
  1134    1    0]
 [   0    0    0    0    0    0   40    0    0    2    0    0    0    0
    11  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.6178861788618

F1 scores:
[       nan 0.96202532 0.96471532 0.95311407 0.98383372 0.95856663
 0.95811903 0.86206897 0.99883586 0.61818182 0.95598194 0.9729488
 0.94874185 0.99730458 0.99169217 0.91588785 0.95906433]

Kappa:
0.9614264432145095
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f6bb29a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.821, val_acc:0.112]
Epoch [2/120    avg_loss:2.731, val_acc:0.139]
Epoch [3/120    avg_loss:2.613, val_acc:0.131]
Epoch [4/120    avg_loss:2.503, val_acc:0.182]
Epoch [5/120    avg_loss:2.412, val_acc:0.289]
Epoch [6/120    avg_loss:2.326, val_acc:0.347]
Epoch [7/120    avg_loss:2.215, val_acc:0.359]
Epoch [8/120    avg_loss:2.160, val_acc:0.440]
Epoch [9/120    avg_loss:2.075, val_acc:0.497]
Epoch [10/120    avg_loss:1.999, val_acc:0.494]
Epoch [11/120    avg_loss:1.914, val_acc:0.537]
Epoch [12/120    avg_loss:1.846, val_acc:0.561]
Epoch [13/120    avg_loss:1.745, val_acc:0.587]
Epoch [14/120    avg_loss:1.661, val_acc:0.619]
Epoch [15/120    avg_loss:1.556, val_acc:0.653]
Epoch [16/120    avg_loss:1.493, val_acc:0.672]
Epoch [17/120    avg_loss:1.425, val_acc:0.710]
Epoch [18/120    avg_loss:1.349, val_acc:0.711]
Epoch [19/120    avg_loss:1.246, val_acc:0.702]
Epoch [20/120    avg_loss:1.163, val_acc:0.724]
Epoch [21/120    avg_loss:1.053, val_acc:0.685]
Epoch [22/120    avg_loss:0.981, val_acc:0.745]
Epoch [23/120    avg_loss:0.879, val_acc:0.741]
Epoch [24/120    avg_loss:0.861, val_acc:0.746]
Epoch [25/120    avg_loss:0.808, val_acc:0.750]
Epoch [26/120    avg_loss:0.790, val_acc:0.764]
Epoch [27/120    avg_loss:0.672, val_acc:0.769]
Epoch [28/120    avg_loss:0.602, val_acc:0.821]
Epoch [29/120    avg_loss:0.543, val_acc:0.811]
Epoch [30/120    avg_loss:0.508, val_acc:0.816]
Epoch [31/120    avg_loss:0.437, val_acc:0.836]
Epoch [32/120    avg_loss:0.418, val_acc:0.850]
Epoch [33/120    avg_loss:0.432, val_acc:0.809]
Epoch [34/120    avg_loss:0.384, val_acc:0.845]
Epoch [35/120    avg_loss:0.306, val_acc:0.867]
Epoch [36/120    avg_loss:0.315, val_acc:0.876]
Epoch [37/120    avg_loss:0.276, val_acc:0.873]
Epoch [38/120    avg_loss:0.301, val_acc:0.869]
Epoch [39/120    avg_loss:0.257, val_acc:0.897]
Epoch [40/120    avg_loss:0.251, val_acc:0.890]
Epoch [41/120    avg_loss:0.246, val_acc:0.877]
Epoch [42/120    avg_loss:0.217, val_acc:0.904]
Epoch [43/120    avg_loss:0.175, val_acc:0.893]
Epoch [44/120    avg_loss:0.181, val_acc:0.906]
Epoch [45/120    avg_loss:0.209, val_acc:0.894]
Epoch [46/120    avg_loss:0.180, val_acc:0.927]
Epoch [47/120    avg_loss:0.186, val_acc:0.890]
Epoch [48/120    avg_loss:0.168, val_acc:0.918]
Epoch [49/120    avg_loss:0.142, val_acc:0.925]
Epoch [50/120    avg_loss:0.133, val_acc:0.914]
Epoch [51/120    avg_loss:0.146, val_acc:0.930]
Epoch [52/120    avg_loss:0.149, val_acc:0.926]
Epoch [53/120    avg_loss:0.107, val_acc:0.931]
Epoch [54/120    avg_loss:0.103, val_acc:0.918]
Epoch [55/120    avg_loss:0.106, val_acc:0.939]
Epoch [56/120    avg_loss:0.094, val_acc:0.938]
Epoch [57/120    avg_loss:0.097, val_acc:0.934]
Epoch [58/120    avg_loss:0.075, val_acc:0.942]
Epoch [59/120    avg_loss:0.078, val_acc:0.944]
Epoch [60/120    avg_loss:0.079, val_acc:0.928]
Epoch [61/120    avg_loss:0.073, val_acc:0.934]
Epoch [62/120    avg_loss:0.078, val_acc:0.933]
Epoch [63/120    avg_loss:0.071, val_acc:0.944]
Epoch [64/120    avg_loss:0.083, val_acc:0.941]
Epoch [65/120    avg_loss:0.067, val_acc:0.949]
Epoch [66/120    avg_loss:0.052, val_acc:0.945]
Epoch [67/120    avg_loss:0.054, val_acc:0.939]
Epoch [68/120    avg_loss:0.054, val_acc:0.950]
Epoch [69/120    avg_loss:0.066, val_acc:0.940]
Epoch [70/120    avg_loss:0.101, val_acc:0.936]
Epoch [71/120    avg_loss:0.074, val_acc:0.938]
Epoch [72/120    avg_loss:0.063, val_acc:0.932]
Epoch [73/120    avg_loss:0.058, val_acc:0.948]
Epoch [74/120    avg_loss:0.078, val_acc:0.929]
Epoch [75/120    avg_loss:0.059, val_acc:0.949]
Epoch [76/120    avg_loss:0.043, val_acc:0.939]
Epoch [77/120    avg_loss:0.060, val_acc:0.945]
Epoch [78/120    avg_loss:0.047, val_acc:0.949]
Epoch [79/120    avg_loss:0.045, val_acc:0.946]
Epoch [80/120    avg_loss:0.040, val_acc:0.947]
Epoch [81/120    avg_loss:0.049, val_acc:0.941]
Epoch [82/120    avg_loss:0.052, val_acc:0.963]
Epoch [83/120    avg_loss:0.031, val_acc:0.968]
Epoch [84/120    avg_loss:0.031, val_acc:0.967]
Epoch [85/120    avg_loss:0.022, val_acc:0.967]
Epoch [86/120    avg_loss:0.025, val_acc:0.969]
Epoch [87/120    avg_loss:0.026, val_acc:0.969]
Epoch [88/120    avg_loss:0.026, val_acc:0.962]
Epoch [89/120    avg_loss:0.021, val_acc:0.963]
Epoch [90/120    avg_loss:0.026, val_acc:0.966]
Epoch [91/120    avg_loss:0.019, val_acc:0.963]
Epoch [92/120    avg_loss:0.024, val_acc:0.967]
Epoch [93/120    avg_loss:0.021, val_acc:0.967]
Epoch [94/120    avg_loss:0.029, val_acc:0.960]
Epoch [95/120    avg_loss:0.019, val_acc:0.962]
Epoch [96/120    avg_loss:0.027, val_acc:0.964]
Epoch [97/120    avg_loss:0.027, val_acc:0.961]
Epoch [98/120    avg_loss:0.020, val_acc:0.962]
Epoch [99/120    avg_loss:0.022, val_acc:0.967]
Epoch [100/120    avg_loss:0.020, val_acc:0.961]
Epoch [101/120    avg_loss:0.019, val_acc:0.961]
Epoch [102/120    avg_loss:0.021, val_acc:0.962]
Epoch [103/120    avg_loss:0.021, val_acc:0.962]
Epoch [104/120    avg_loss:0.025, val_acc:0.962]
Epoch [105/120    avg_loss:0.022, val_acc:0.961]
Epoch [106/120    avg_loss:0.019, val_acc:0.961]
Epoch [107/120    avg_loss:0.018, val_acc:0.961]
Epoch [108/120    avg_loss:0.021, val_acc:0.961]
Epoch [109/120    avg_loss:0.019, val_acc:0.961]
Epoch [110/120    avg_loss:0.019, val_acc:0.961]
Epoch [111/120    avg_loss:0.020, val_acc:0.961]
Epoch [112/120    avg_loss:0.018, val_acc:0.961]
Epoch [113/120    avg_loss:0.021, val_acc:0.962]
Epoch [114/120    avg_loss:0.025, val_acc:0.962]
Epoch [115/120    avg_loss:0.018, val_acc:0.963]
Epoch [116/120    avg_loss:0.020, val_acc:0.963]
Epoch [117/120    avg_loss:0.019, val_acc:0.963]
Epoch [118/120    avg_loss:0.022, val_acc:0.963]
Epoch [119/120    avg_loss:0.020, val_acc:0.963]
Epoch [120/120    avg_loss:0.022, val_acc:0.963]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1258    1    4    0    0    0    0    0    1   21    0    0
     0    0    0]
 [   0    0    1  707    1    3    0    0    0    4    5    2   24    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    4    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   19    0    0    7    1    0    0    0  832    7    9    0
     0    0    0]
 [   0    1   12    2    0    0    2    2    1    1   18 2158   12    0
     1    0    0]
 [   0    0    0    4    0    6    0    0    0    0    0    9  509    0
     0    4    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1133    5    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
     1  321    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.44173441734418

F1 scores:
[       nan 0.97560976 0.97708738 0.96584699 0.98839907 0.9738933
 0.97393894 0.94117647 0.99883856 0.76923077 0.96018465 0.97912886
 0.93308891 1.         0.99516908 0.94830133 0.97005988]

Kappa:
0.9708439208429926
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcbe64b2ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.810, val_acc:0.182]
Epoch [2/120    avg_loss:2.655, val_acc:0.295]
Epoch [3/120    avg_loss:2.526, val_acc:0.310]
Epoch [4/120    avg_loss:2.400, val_acc:0.339]
Epoch [5/120    avg_loss:2.304, val_acc:0.381]
Epoch [6/120    avg_loss:2.253, val_acc:0.385]
Epoch [7/120    avg_loss:2.144, val_acc:0.434]
Epoch [8/120    avg_loss:2.051, val_acc:0.506]
Epoch [9/120    avg_loss:1.995, val_acc:0.494]
Epoch [10/120    avg_loss:1.881, val_acc:0.533]
Epoch [11/120    avg_loss:1.787, val_acc:0.545]
Epoch [12/120    avg_loss:1.648, val_acc:0.567]
Epoch [13/120    avg_loss:1.594, val_acc:0.611]
Epoch [14/120    avg_loss:1.461, val_acc:0.561]
Epoch [15/120    avg_loss:1.363, val_acc:0.655]
Epoch [16/120    avg_loss:1.270, val_acc:0.677]
Epoch [17/120    avg_loss:1.203, val_acc:0.671]
Epoch [18/120    avg_loss:1.119, val_acc:0.705]
Epoch [19/120    avg_loss:1.034, val_acc:0.705]
Epoch [20/120    avg_loss:0.938, val_acc:0.724]
Epoch [21/120    avg_loss:0.870, val_acc:0.762]
Epoch [22/120    avg_loss:0.771, val_acc:0.770]
Epoch [23/120    avg_loss:0.781, val_acc:0.773]
Epoch [24/120    avg_loss:0.708, val_acc:0.796]
Epoch [25/120    avg_loss:0.658, val_acc:0.803]
Epoch [26/120    avg_loss:0.586, val_acc:0.820]
Epoch [27/120    avg_loss:0.523, val_acc:0.817]
Epoch [28/120    avg_loss:0.547, val_acc:0.846]
Epoch [29/120    avg_loss:0.509, val_acc:0.842]
Epoch [30/120    avg_loss:0.438, val_acc:0.860]
Epoch [31/120    avg_loss:0.376, val_acc:0.874]
Epoch [32/120    avg_loss:0.356, val_acc:0.858]
Epoch [33/120    avg_loss:0.308, val_acc:0.889]
Epoch [34/120    avg_loss:0.330, val_acc:0.886]
Epoch [35/120    avg_loss:0.283, val_acc:0.879]
Epoch [36/120    avg_loss:0.270, val_acc:0.893]
Epoch [37/120    avg_loss:0.273, val_acc:0.838]
Epoch [38/120    avg_loss:0.274, val_acc:0.881]
Epoch [39/120    avg_loss:0.203, val_acc:0.898]
Epoch [40/120    avg_loss:0.243, val_acc:0.894]
Epoch [41/120    avg_loss:0.205, val_acc:0.897]
Epoch [42/120    avg_loss:0.201, val_acc:0.911]
Epoch [43/120    avg_loss:0.164, val_acc:0.912]
Epoch [44/120    avg_loss:0.164, val_acc:0.895]
Epoch [45/120    avg_loss:0.158, val_acc:0.893]
Epoch [46/120    avg_loss:0.142, val_acc:0.915]
Epoch [47/120    avg_loss:0.115, val_acc:0.920]
Epoch [48/120    avg_loss:0.132, val_acc:0.926]
Epoch [49/120    avg_loss:0.146, val_acc:0.904]
Epoch [50/120    avg_loss:0.132, val_acc:0.919]
Epoch [51/120    avg_loss:0.146, val_acc:0.904]
Epoch [52/120    avg_loss:0.116, val_acc:0.922]
Epoch [53/120    avg_loss:0.150, val_acc:0.906]
Epoch [54/120    avg_loss:0.114, val_acc:0.928]
Epoch [55/120    avg_loss:0.107, val_acc:0.931]
Epoch [56/120    avg_loss:0.093, val_acc:0.933]
Epoch [57/120    avg_loss:0.097, val_acc:0.938]
Epoch [58/120    avg_loss:0.080, val_acc:0.939]
Epoch [59/120    avg_loss:0.084, val_acc:0.936]
Epoch [60/120    avg_loss:0.079, val_acc:0.939]
Epoch [61/120    avg_loss:0.064, val_acc:0.949]
Epoch [62/120    avg_loss:0.077, val_acc:0.947]
Epoch [63/120    avg_loss:0.063, val_acc:0.940]
Epoch [64/120    avg_loss:0.070, val_acc:0.948]
Epoch [65/120    avg_loss:0.066, val_acc:0.949]
Epoch [66/120    avg_loss:0.061, val_acc:0.945]
Epoch [67/120    avg_loss:0.054, val_acc:0.949]
Epoch [68/120    avg_loss:0.065, val_acc:0.948]
Epoch [69/120    avg_loss:0.062, val_acc:0.949]
Epoch [70/120    avg_loss:0.108, val_acc:0.899]
Epoch [71/120    avg_loss:0.093, val_acc:0.925]
Epoch [72/120    avg_loss:0.070, val_acc:0.949]
Epoch [73/120    avg_loss:0.062, val_acc:0.950]
Epoch [74/120    avg_loss:0.063, val_acc:0.933]
Epoch [75/120    avg_loss:0.065, val_acc:0.944]
Epoch [76/120    avg_loss:0.051, val_acc:0.960]
Epoch [77/120    avg_loss:0.045, val_acc:0.952]
Epoch [78/120    avg_loss:0.045, val_acc:0.957]
Epoch [79/120    avg_loss:0.034, val_acc:0.959]
Epoch [80/120    avg_loss:0.049, val_acc:0.956]
Epoch [81/120    avg_loss:0.045, val_acc:0.956]
Epoch [82/120    avg_loss:0.042, val_acc:0.955]
Epoch [83/120    avg_loss:0.039, val_acc:0.955]
Epoch [84/120    avg_loss:0.033, val_acc:0.946]
Epoch [85/120    avg_loss:0.045, val_acc:0.938]
Epoch [86/120    avg_loss:0.043, val_acc:0.947]
Epoch [87/120    avg_loss:0.036, val_acc:0.957]
Epoch [88/120    avg_loss:0.046, val_acc:0.939]
Epoch [89/120    avg_loss:0.036, val_acc:0.945]
Epoch [90/120    avg_loss:0.033, val_acc:0.962]
Epoch [91/120    avg_loss:0.026, val_acc:0.961]
Epoch [92/120    avg_loss:0.023, val_acc:0.964]
Epoch [93/120    avg_loss:0.020, val_acc:0.967]
Epoch [94/120    avg_loss:0.024, val_acc:0.967]
Epoch [95/120    avg_loss:0.021, val_acc:0.967]
Epoch [96/120    avg_loss:0.022, val_acc:0.970]
Epoch [97/120    avg_loss:0.021, val_acc:0.967]
Epoch [98/120    avg_loss:0.028, val_acc:0.966]
Epoch [99/120    avg_loss:0.020, val_acc:0.966]
Epoch [100/120    avg_loss:0.022, val_acc:0.967]
Epoch [101/120    avg_loss:0.023, val_acc:0.967]
Epoch [102/120    avg_loss:0.016, val_acc:0.966]
Epoch [103/120    avg_loss:0.018, val_acc:0.968]
Epoch [104/120    avg_loss:0.018, val_acc:0.969]
Epoch [105/120    avg_loss:0.016, val_acc:0.968]
Epoch [106/120    avg_loss:0.019, val_acc:0.968]
Epoch [107/120    avg_loss:0.018, val_acc:0.969]
Epoch [108/120    avg_loss:0.018, val_acc:0.968]
Epoch [109/120    avg_loss:0.023, val_acc:0.969]
Epoch [110/120    avg_loss:0.018, val_acc:0.969]
Epoch [111/120    avg_loss:0.019, val_acc:0.969]
Epoch [112/120    avg_loss:0.021, val_acc:0.969]
Epoch [113/120    avg_loss:0.019, val_acc:0.970]
Epoch [114/120    avg_loss:0.016, val_acc:0.969]
Epoch [115/120    avg_loss:0.018, val_acc:0.969]
Epoch [116/120    avg_loss:0.017, val_acc:0.969]
Epoch [117/120    avg_loss:0.019, val_acc:0.969]
Epoch [118/120    avg_loss:0.016, val_acc:0.969]
Epoch [119/120    avg_loss:0.019, val_acc:0.968]
Epoch [120/120    avg_loss:0.018, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1259    0    0    0    0    0    0    1    6   18    1    0
     0    0    0]
 [   0    0    0  692    1   24    0    0    0   12    0    0   17    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    2    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    3    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  422    0    0    0    8    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   24    0    0    7    0    0    0    0  829   10    0    0
     0    5    0]
 [   0    0    9    0    0    1    2    0    0    0   19 2171    8    0
     0    0    0]
 [   0    0    0    1    0    4    0    0    0    0    0    5  519    0
     0    0    5]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    4    1    0    0
  1119   14    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    17  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.30081300813008

F1 scores:
[       nan 0.975      0.97710516 0.96111111 0.99765808 0.95690608
 0.98415094 1.         0.989449   0.70833333 0.9556196  0.98279765
 0.95404412 0.99459459 0.9828722  0.92982456 0.97109827]

Kappa:
0.969236135257424
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f666bf0db00>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.807, val_acc:0.194]
Epoch [2/120    avg_loss:2.692, val_acc:0.332]
Epoch [3/120    avg_loss:2.572, val_acc:0.350]
Epoch [4/120    avg_loss:2.463, val_acc:0.472]
Epoch [5/120    avg_loss:2.352, val_acc:0.550]
Epoch [6/120    avg_loss:2.262, val_acc:0.550]
Epoch [7/120    avg_loss:2.188, val_acc:0.559]
Epoch [8/120    avg_loss:2.083, val_acc:0.595]
Epoch [9/120    avg_loss:2.025, val_acc:0.606]
Epoch [10/120    avg_loss:1.936, val_acc:0.627]
Epoch [11/120    avg_loss:1.860, val_acc:0.633]
Epoch [12/120    avg_loss:1.809, val_acc:0.638]
Epoch [13/120    avg_loss:1.745, val_acc:0.671]
Epoch [14/120    avg_loss:1.651, val_acc:0.667]
Epoch [15/120    avg_loss:1.553, val_acc:0.661]
Epoch [16/120    avg_loss:1.450, val_acc:0.683]
Epoch [17/120    avg_loss:1.387, val_acc:0.669]
Epoch [18/120    avg_loss:1.324, val_acc:0.659]
Epoch [19/120    avg_loss:1.196, val_acc:0.742]
Epoch [20/120    avg_loss:1.100, val_acc:0.763]
Epoch [21/120    avg_loss:0.989, val_acc:0.765]
Epoch [22/120    avg_loss:0.948, val_acc:0.788]
Epoch [23/120    avg_loss:0.855, val_acc:0.788]
Epoch [24/120    avg_loss:0.817, val_acc:0.724]
Epoch [25/120    avg_loss:0.854, val_acc:0.792]
Epoch [26/120    avg_loss:0.796, val_acc:0.793]
Epoch [27/120    avg_loss:0.710, val_acc:0.814]
Epoch [28/120    avg_loss:0.672, val_acc:0.845]
Epoch [29/120    avg_loss:0.599, val_acc:0.807]
Epoch [30/120    avg_loss:0.524, val_acc:0.857]
Epoch [31/120    avg_loss:0.439, val_acc:0.867]
Epoch [32/120    avg_loss:0.388, val_acc:0.895]
Epoch [33/120    avg_loss:0.376, val_acc:0.875]
Epoch [34/120    avg_loss:0.412, val_acc:0.892]
Epoch [35/120    avg_loss:0.331, val_acc:0.898]
Epoch [36/120    avg_loss:0.336, val_acc:0.908]
Epoch [37/120    avg_loss:0.336, val_acc:0.872]
Epoch [38/120    avg_loss:0.326, val_acc:0.903]
Epoch [39/120    avg_loss:0.323, val_acc:0.832]
Epoch [40/120    avg_loss:0.242, val_acc:0.914]
Epoch [41/120    avg_loss:0.226, val_acc:0.897]
Epoch [42/120    avg_loss:0.201, val_acc:0.918]
Epoch [43/120    avg_loss:0.215, val_acc:0.926]
Epoch [44/120    avg_loss:0.243, val_acc:0.928]
Epoch [45/120    avg_loss:0.211, val_acc:0.915]
Epoch [46/120    avg_loss:0.199, val_acc:0.913]
Epoch [47/120    avg_loss:0.200, val_acc:0.913]
Epoch [48/120    avg_loss:0.180, val_acc:0.944]
Epoch [49/120    avg_loss:0.159, val_acc:0.932]
Epoch [50/120    avg_loss:0.125, val_acc:0.944]
Epoch [51/120    avg_loss:0.153, val_acc:0.942]
Epoch [52/120    avg_loss:0.124, val_acc:0.945]
Epoch [53/120    avg_loss:0.116, val_acc:0.946]
Epoch [54/120    avg_loss:0.097, val_acc:0.958]
Epoch [55/120    avg_loss:0.113, val_acc:0.946]
Epoch [56/120    avg_loss:0.109, val_acc:0.948]
Epoch [57/120    avg_loss:0.113, val_acc:0.939]
Epoch [58/120    avg_loss:0.151, val_acc:0.898]
Epoch [59/120    avg_loss:0.138, val_acc:0.935]
Epoch [60/120    avg_loss:0.262, val_acc:0.885]
Epoch [61/120    avg_loss:0.206, val_acc:0.928]
Epoch [62/120    avg_loss:0.164, val_acc:0.950]
Epoch [63/120    avg_loss:0.126, val_acc:0.928]
Epoch [64/120    avg_loss:0.122, val_acc:0.931]
Epoch [65/120    avg_loss:0.100, val_acc:0.956]
Epoch [66/120    avg_loss:0.116, val_acc:0.958]
Epoch [67/120    avg_loss:0.141, val_acc:0.953]
Epoch [68/120    avg_loss:0.100, val_acc:0.908]
Epoch [69/120    avg_loss:0.110, val_acc:0.950]
Epoch [70/120    avg_loss:0.091, val_acc:0.943]
Epoch [71/120    avg_loss:0.071, val_acc:0.950]
Epoch [72/120    avg_loss:0.067, val_acc:0.961]
Epoch [73/120    avg_loss:0.049, val_acc:0.967]
Epoch [74/120    avg_loss:0.052, val_acc:0.963]
Epoch [75/120    avg_loss:0.056, val_acc:0.966]
Epoch [76/120    avg_loss:0.051, val_acc:0.946]
Epoch [77/120    avg_loss:0.093, val_acc:0.961]
Epoch [78/120    avg_loss:0.057, val_acc:0.953]
Epoch [79/120    avg_loss:0.058, val_acc:0.959]
Epoch [80/120    avg_loss:0.047, val_acc:0.962]
Epoch [81/120    avg_loss:0.067, val_acc:0.950]
Epoch [82/120    avg_loss:0.062, val_acc:0.966]
Epoch [83/120    avg_loss:0.037, val_acc:0.964]
Epoch [84/120    avg_loss:0.037, val_acc:0.966]
Epoch [85/120    avg_loss:0.040, val_acc:0.967]
Epoch [86/120    avg_loss:0.047, val_acc:0.974]
Epoch [87/120    avg_loss:0.033, val_acc:0.967]
Epoch [88/120    avg_loss:0.042, val_acc:0.968]
Epoch [89/120    avg_loss:0.045, val_acc:0.956]
Epoch [90/120    avg_loss:0.040, val_acc:0.950]
Epoch [91/120    avg_loss:0.058, val_acc:0.966]
Epoch [92/120    avg_loss:0.041, val_acc:0.959]
Epoch [93/120    avg_loss:0.039, val_acc:0.963]
Epoch [94/120    avg_loss:0.038, val_acc:0.967]
Epoch [95/120    avg_loss:0.034, val_acc:0.966]
Epoch [96/120    avg_loss:0.033, val_acc:0.963]
Epoch [97/120    avg_loss:0.030, val_acc:0.970]
Epoch [98/120    avg_loss:0.038, val_acc:0.959]
Epoch [99/120    avg_loss:0.031, val_acc:0.971]
Epoch [100/120    avg_loss:0.024, val_acc:0.974]
Epoch [101/120    avg_loss:0.019, val_acc:0.975]
Epoch [102/120    avg_loss:0.023, val_acc:0.972]
Epoch [103/120    avg_loss:0.021, val_acc:0.976]
Epoch [104/120    avg_loss:0.021, val_acc:0.975]
Epoch [105/120    avg_loss:0.018, val_acc:0.976]
Epoch [106/120    avg_loss:0.018, val_acc:0.976]
Epoch [107/120    avg_loss:0.020, val_acc:0.975]
Epoch [108/120    avg_loss:0.017, val_acc:0.975]
Epoch [109/120    avg_loss:0.017, val_acc:0.976]
Epoch [110/120    avg_loss:0.024, val_acc:0.977]
Epoch [111/120    avg_loss:0.017, val_acc:0.977]
Epoch [112/120    avg_loss:0.022, val_acc:0.977]
Epoch [113/120    avg_loss:0.017, val_acc:0.978]
Epoch [114/120    avg_loss:0.018, val_acc:0.977]
Epoch [115/120    avg_loss:0.017, val_acc:0.976]
Epoch [116/120    avg_loss:0.018, val_acc:0.975]
Epoch [117/120    avg_loss:0.022, val_acc:0.975]
Epoch [118/120    avg_loss:0.018, val_acc:0.974]
Epoch [119/120    avg_loss:0.015, val_acc:0.975]
Epoch [120/120    avg_loss:0.018, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1257    0    1    0    5    0    0    0    3   17    0    0
     0    2    0]
 [   0    0    0  686   19    3    0    0    0    6    5    2   26    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    3    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   11    0    0    5    0    0    0    0  839   18    1    0
     1    0    0]
 [   0    0   11    0    0    1    0    0    0    0   15 2173   10    0
     0    0    0]
 [   0    0    0    1    1    2    0    0    0    0    3    0  523    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    1    0    0
  1135    2    0]
 [   0    0    0    0    0    0   18    0    0    0    0    0    0    0
    24  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.50677506775068

F1 scores:
[       nan 0.975      0.98049922 0.9567643  0.95302013 0.98751419
 0.97821187 1.         1.         0.82926829 0.96270797 0.9823689
 0.95350957 1.         0.98524306 0.92987805 0.97619048]

Kappa:
0.9715740070174655
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0a953b9b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.813, val_acc:0.254]
Epoch [2/120    avg_loss:2.701, val_acc:0.309]
Epoch [3/120    avg_loss:2.579, val_acc:0.428]
Epoch [4/120    avg_loss:2.466, val_acc:0.484]
Epoch [5/120    avg_loss:2.383, val_acc:0.518]
Epoch [6/120    avg_loss:2.291, val_acc:0.510]
Epoch [7/120    avg_loss:2.225, val_acc:0.501]
Epoch [8/120    avg_loss:2.123, val_acc:0.529]
Epoch [9/120    avg_loss:2.046, val_acc:0.556]
Epoch [10/120    avg_loss:1.969, val_acc:0.548]
Epoch [11/120    avg_loss:1.913, val_acc:0.575]
Epoch [12/120    avg_loss:1.830, val_acc:0.603]
Epoch [13/120    avg_loss:1.750, val_acc:0.606]
Epoch [14/120    avg_loss:1.671, val_acc:0.637]
Epoch [15/120    avg_loss:1.558, val_acc:0.634]
Epoch [16/120    avg_loss:1.480, val_acc:0.665]
Epoch [17/120    avg_loss:1.409, val_acc:0.666]
Epoch [18/120    avg_loss:1.242, val_acc:0.711]
Epoch [19/120    avg_loss:1.203, val_acc:0.683]
Epoch [20/120    avg_loss:1.152, val_acc:0.732]
Epoch [21/120    avg_loss:1.009, val_acc:0.766]
Epoch [22/120    avg_loss:0.899, val_acc:0.792]
Epoch [23/120    avg_loss:0.877, val_acc:0.769]
Epoch [24/120    avg_loss:0.832, val_acc:0.789]
Epoch [25/120    avg_loss:0.796, val_acc:0.777]
Epoch [26/120    avg_loss:0.736, val_acc:0.784]
Epoch [27/120    avg_loss:0.604, val_acc:0.832]
Epoch [28/120    avg_loss:0.514, val_acc:0.828]
Epoch [29/120    avg_loss:0.483, val_acc:0.845]
Epoch [30/120    avg_loss:0.435, val_acc:0.853]
Epoch [31/120    avg_loss:0.497, val_acc:0.855]
Epoch [32/120    avg_loss:0.409, val_acc:0.860]
Epoch [33/120    avg_loss:0.337, val_acc:0.861]
Epoch [34/120    avg_loss:0.325, val_acc:0.892]
Epoch [35/120    avg_loss:0.292, val_acc:0.870]
Epoch [36/120    avg_loss:0.309, val_acc:0.897]
Epoch [37/120    avg_loss:0.256, val_acc:0.900]
Epoch [38/120    avg_loss:0.267, val_acc:0.892]
Epoch [39/120    avg_loss:0.281, val_acc:0.891]
Epoch [40/120    avg_loss:0.230, val_acc:0.886]
Epoch [41/120    avg_loss:0.211, val_acc:0.879]
Epoch [42/120    avg_loss:0.227, val_acc:0.901]
Epoch [43/120    avg_loss:0.200, val_acc:0.898]
Epoch [44/120    avg_loss:0.161, val_acc:0.925]
Epoch [45/120    avg_loss:0.148, val_acc:0.902]
Epoch [46/120    avg_loss:0.146, val_acc:0.907]
Epoch [47/120    avg_loss:0.138, val_acc:0.920]
Epoch [48/120    avg_loss:0.124, val_acc:0.927]
Epoch [49/120    avg_loss:0.132, val_acc:0.935]
Epoch [50/120    avg_loss:0.118, val_acc:0.933]
Epoch [51/120    avg_loss:0.103, val_acc:0.929]
Epoch [52/120    avg_loss:0.161, val_acc:0.923]
Epoch [53/120    avg_loss:0.217, val_acc:0.901]
Epoch [54/120    avg_loss:0.136, val_acc:0.928]
Epoch [55/120    avg_loss:0.129, val_acc:0.926]
Epoch [56/120    avg_loss:0.118, val_acc:0.934]
Epoch [57/120    avg_loss:0.129, val_acc:0.917]
Epoch [58/120    avg_loss:0.103, val_acc:0.939]
Epoch [59/120    avg_loss:0.122, val_acc:0.934]
Epoch [60/120    avg_loss:0.134, val_acc:0.936]
Epoch [61/120    avg_loss:0.139, val_acc:0.915]
Epoch [62/120    avg_loss:0.120, val_acc:0.939]
Epoch [63/120    avg_loss:0.123, val_acc:0.933]
Epoch [64/120    avg_loss:0.078, val_acc:0.949]
Epoch [65/120    avg_loss:0.072, val_acc:0.935]
Epoch [66/120    avg_loss:0.061, val_acc:0.957]
Epoch [67/120    avg_loss:0.052, val_acc:0.954]
Epoch [68/120    avg_loss:0.080, val_acc:0.961]
Epoch [69/120    avg_loss:0.067, val_acc:0.932]
Epoch [70/120    avg_loss:0.058, val_acc:0.963]
Epoch [71/120    avg_loss:0.063, val_acc:0.962]
Epoch [72/120    avg_loss:0.087, val_acc:0.947]
Epoch [73/120    avg_loss:0.063, val_acc:0.945]
Epoch [74/120    avg_loss:0.093, val_acc:0.947]
Epoch [75/120    avg_loss:0.062, val_acc:0.955]
Epoch [76/120    avg_loss:0.057, val_acc:0.961]
Epoch [77/120    avg_loss:0.050, val_acc:0.957]
Epoch [78/120    avg_loss:0.046, val_acc:0.959]
Epoch [79/120    avg_loss:0.044, val_acc:0.964]
Epoch [80/120    avg_loss:0.043, val_acc:0.964]
Epoch [81/120    avg_loss:0.046, val_acc:0.964]
Epoch [82/120    avg_loss:0.037, val_acc:0.961]
Epoch [83/120    avg_loss:0.037, val_acc:0.966]
Epoch [84/120    avg_loss:0.041, val_acc:0.960]
Epoch [85/120    avg_loss:0.040, val_acc:0.962]
Epoch [86/120    avg_loss:0.035, val_acc:0.967]
Epoch [87/120    avg_loss:0.031, val_acc:0.967]
Epoch [88/120    avg_loss:0.041, val_acc:0.966]
Epoch [89/120    avg_loss:0.036, val_acc:0.961]
Epoch [90/120    avg_loss:0.024, val_acc:0.962]
Epoch [91/120    avg_loss:0.027, val_acc:0.971]
Epoch [92/120    avg_loss:0.025, val_acc:0.971]
Epoch [93/120    avg_loss:0.036, val_acc:0.969]
Epoch [94/120    avg_loss:0.028, val_acc:0.967]
Epoch [95/120    avg_loss:0.025, val_acc:0.967]
Epoch [96/120    avg_loss:0.040, val_acc:0.954]
Epoch [97/120    avg_loss:0.039, val_acc:0.963]
Epoch [98/120    avg_loss:0.033, val_acc:0.962]
Epoch [99/120    avg_loss:0.028, val_acc:0.964]
Epoch [100/120    avg_loss:0.036, val_acc:0.963]
Epoch [101/120    avg_loss:0.029, val_acc:0.971]
Epoch [102/120    avg_loss:0.025, val_acc:0.969]
Epoch [103/120    avg_loss:0.021, val_acc:0.974]
Epoch [104/120    avg_loss:0.028, val_acc:0.967]
Epoch [105/120    avg_loss:0.034, val_acc:0.960]
Epoch [106/120    avg_loss:0.044, val_acc:0.950]
Epoch [107/120    avg_loss:0.032, val_acc:0.957]
Epoch [108/120    avg_loss:0.040, val_acc:0.969]
Epoch [109/120    avg_loss:0.027, val_acc:0.969]
Epoch [110/120    avg_loss:0.024, val_acc:0.960]
Epoch [111/120    avg_loss:0.025, val_acc:0.968]
Epoch [112/120    avg_loss:0.015, val_acc:0.974]
Epoch [113/120    avg_loss:0.017, val_acc:0.970]
Epoch [114/120    avg_loss:0.020, val_acc:0.949]
Epoch [115/120    avg_loss:0.025, val_acc:0.967]
Epoch [116/120    avg_loss:0.035, val_acc:0.963]
Epoch [117/120    avg_loss:0.049, val_acc:0.934]
Epoch [118/120    avg_loss:0.040, val_acc:0.966]
Epoch [119/120    avg_loss:0.021, val_acc:0.971]
Epoch [120/120    avg_loss:0.019, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1230    5   12    0    0    0    0    0   15   23    0    0
     0    0    0]
 [   0    0    4  713    2   12    0    0    0    6    6    2    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   13    0    0    0    0    0    0    0  844   17    0    0
     1    0    0]
 [   0    0   40    0    0    0    0    0    0    0   22 2142    6    0
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0   11    6  502    0
     4    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    2    2    0    1
  1124    6    0]
 [   0    0    1    0    0    0   25    0    0    0    0    0    0    0
     7  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.07317073170732

F1 scores:
[       nan 1.         0.95608239 0.97337884 0.96818182 0.97309417
 0.98056801 1.         1.         0.85714286 0.95098592 0.97297297
 0.96168582 0.99730458 0.98769772 0.94152924 0.97674419]

Kappa:
0.9666444555277687
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f10caa75a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.811, val_acc:0.172]
Epoch [2/120    avg_loss:2.720, val_acc:0.343]
Epoch [3/120    avg_loss:2.606, val_acc:0.385]
Epoch [4/120    avg_loss:2.506, val_acc:0.456]
Epoch [5/120    avg_loss:2.408, val_acc:0.519]
Epoch [6/120    avg_loss:2.344, val_acc:0.515]
Epoch [7/120    avg_loss:2.311, val_acc:0.539]
Epoch [8/120    avg_loss:2.230, val_acc:0.540]
Epoch [9/120    avg_loss:2.111, val_acc:0.565]
Epoch [10/120    avg_loss:2.063, val_acc:0.565]
Epoch [11/120    avg_loss:2.002, val_acc:0.581]
Epoch [12/120    avg_loss:1.899, val_acc:0.604]
Epoch [13/120    avg_loss:1.839, val_acc:0.638]
Epoch [14/120    avg_loss:1.716, val_acc:0.622]
Epoch [15/120    avg_loss:1.678, val_acc:0.648]
Epoch [16/120    avg_loss:1.611, val_acc:0.633]
Epoch [17/120    avg_loss:1.517, val_acc:0.650]
Epoch [18/120    avg_loss:1.412, val_acc:0.675]
Epoch [19/120    avg_loss:1.334, val_acc:0.657]
Epoch [20/120    avg_loss:1.290, val_acc:0.686]
Epoch [21/120    avg_loss:1.189, val_acc:0.724]
Epoch [22/120    avg_loss:1.177, val_acc:0.695]
Epoch [23/120    avg_loss:1.109, val_acc:0.734]
Epoch [24/120    avg_loss:1.027, val_acc:0.753]
Epoch [25/120    avg_loss:0.954, val_acc:0.742]
Epoch [26/120    avg_loss:0.873, val_acc:0.792]
Epoch [27/120    avg_loss:0.808, val_acc:0.783]
Epoch [28/120    avg_loss:0.708, val_acc:0.792]
Epoch [29/120    avg_loss:0.700, val_acc:0.805]
Epoch [30/120    avg_loss:0.637, val_acc:0.812]
Epoch [31/120    avg_loss:0.550, val_acc:0.847]
Epoch [32/120    avg_loss:0.512, val_acc:0.856]
Epoch [33/120    avg_loss:0.517, val_acc:0.845]
Epoch [34/120    avg_loss:0.456, val_acc:0.855]
Epoch [35/120    avg_loss:0.412, val_acc:0.854]
Epoch [36/120    avg_loss:0.385, val_acc:0.886]
Epoch [37/120    avg_loss:0.368, val_acc:0.846]
Epoch [38/120    avg_loss:0.357, val_acc:0.886]
Epoch [39/120    avg_loss:0.293, val_acc:0.903]
Epoch [40/120    avg_loss:0.315, val_acc:0.898]
Epoch [41/120    avg_loss:0.325, val_acc:0.887]
Epoch [42/120    avg_loss:0.319, val_acc:0.883]
Epoch [43/120    avg_loss:0.309, val_acc:0.879]
Epoch [44/120    avg_loss:0.316, val_acc:0.884]
Epoch [45/120    avg_loss:0.263, val_acc:0.904]
Epoch [46/120    avg_loss:0.205, val_acc:0.907]
Epoch [47/120    avg_loss:0.200, val_acc:0.908]
Epoch [48/120    avg_loss:0.317, val_acc:0.831]
Epoch [49/120    avg_loss:0.391, val_acc:0.860]
Epoch [50/120    avg_loss:0.320, val_acc:0.892]
Epoch [51/120    avg_loss:0.218, val_acc:0.917]
Epoch [52/120    avg_loss:0.175, val_acc:0.920]
Epoch [53/120    avg_loss:0.151, val_acc:0.904]
Epoch [54/120    avg_loss:0.143, val_acc:0.933]
Epoch [55/120    avg_loss:0.149, val_acc:0.915]
Epoch [56/120    avg_loss:0.149, val_acc:0.915]
Epoch [57/120    avg_loss:0.159, val_acc:0.924]
Epoch [58/120    avg_loss:0.126, val_acc:0.939]
Epoch [59/120    avg_loss:0.129, val_acc:0.934]
Epoch [60/120    avg_loss:0.116, val_acc:0.930]
Epoch [61/120    avg_loss:0.115, val_acc:0.933]
Epoch [62/120    avg_loss:0.124, val_acc:0.925]
Epoch [63/120    avg_loss:0.122, val_acc:0.931]
Epoch [64/120    avg_loss:0.111, val_acc:0.943]
Epoch [65/120    avg_loss:0.092, val_acc:0.922]
Epoch [66/120    avg_loss:0.096, val_acc:0.941]
Epoch [67/120    avg_loss:0.092, val_acc:0.933]
Epoch [68/120    avg_loss:0.085, val_acc:0.942]
Epoch [69/120    avg_loss:0.073, val_acc:0.949]
Epoch [70/120    avg_loss:0.059, val_acc:0.942]
Epoch [71/120    avg_loss:0.063, val_acc:0.953]
Epoch [72/120    avg_loss:0.077, val_acc:0.940]
Epoch [73/120    avg_loss:0.088, val_acc:0.950]
Epoch [74/120    avg_loss:0.080, val_acc:0.953]
Epoch [75/120    avg_loss:0.073, val_acc:0.952]
Epoch [76/120    avg_loss:0.076, val_acc:0.950]
Epoch [77/120    avg_loss:0.068, val_acc:0.949]
Epoch [78/120    avg_loss:0.071, val_acc:0.936]
Epoch [79/120    avg_loss:0.075, val_acc:0.949]
Epoch [80/120    avg_loss:0.073, val_acc:0.946]
Epoch [81/120    avg_loss:0.068, val_acc:0.952]
Epoch [82/120    avg_loss:0.051, val_acc:0.960]
Epoch [83/120    avg_loss:0.050, val_acc:0.961]
Epoch [84/120    avg_loss:0.045, val_acc:0.954]
Epoch [85/120    avg_loss:0.067, val_acc:0.943]
Epoch [86/120    avg_loss:0.086, val_acc:0.950]
Epoch [87/120    avg_loss:0.059, val_acc:0.954]
Epoch [88/120    avg_loss:0.070, val_acc:0.960]
Epoch [89/120    avg_loss:0.077, val_acc:0.945]
Epoch [90/120    avg_loss:0.045, val_acc:0.959]
Epoch [91/120    avg_loss:0.055, val_acc:0.963]
Epoch [92/120    avg_loss:0.048, val_acc:0.959]
Epoch [93/120    avg_loss:0.042, val_acc:0.954]
Epoch [94/120    avg_loss:0.042, val_acc:0.965]
Epoch [95/120    avg_loss:0.042, val_acc:0.949]
Epoch [96/120    avg_loss:0.048, val_acc:0.955]
Epoch [97/120    avg_loss:0.045, val_acc:0.964]
Epoch [98/120    avg_loss:0.028, val_acc:0.963]
Epoch [99/120    avg_loss:0.046, val_acc:0.962]
Epoch [100/120    avg_loss:0.039, val_acc:0.968]
Epoch [101/120    avg_loss:0.031, val_acc:0.970]
Epoch [102/120    avg_loss:0.035, val_acc:0.967]
Epoch [103/120    avg_loss:0.033, val_acc:0.972]
Epoch [104/120    avg_loss:0.027, val_acc:0.970]
Epoch [105/120    avg_loss:0.030, val_acc:0.969]
Epoch [106/120    avg_loss:0.028, val_acc:0.967]
Epoch [107/120    avg_loss:0.039, val_acc:0.967]
Epoch [108/120    avg_loss:0.026, val_acc:0.964]
Epoch [109/120    avg_loss:0.025, val_acc:0.968]
Epoch [110/120    avg_loss:0.035, val_acc:0.970]
Epoch [111/120    avg_loss:0.043, val_acc:0.962]
Epoch [112/120    avg_loss:0.080, val_acc:0.956]
Epoch [113/120    avg_loss:0.099, val_acc:0.941]
Epoch [114/120    avg_loss:0.066, val_acc:0.959]
Epoch [115/120    avg_loss:0.045, val_acc:0.964]
Epoch [116/120    avg_loss:0.053, val_acc:0.955]
Epoch [117/120    avg_loss:0.047, val_acc:0.968]
Epoch [118/120    avg_loss:0.030, val_acc:0.968]
Epoch [119/120    avg_loss:0.031, val_acc:0.969]
Epoch [120/120    avg_loss:0.028, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1241   18    0    0    2    0    0    0    2   21    1    0
     0    0    0]
 [   0    0    0  691    6   13    0    0    0   16    0    2   19    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    4    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   14    0    1    1    0
     0    0    0]
 [   0    0   15   40    0    5    1    0    0    0  808    3    0    0
     0    3    0]
 [   0    0    6    0    0    0    2    0    0    0    6 2181   13    2
     0    0    0]
 [   0    0    0    0    4    7    0    0    0    1    9    7  500    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    1    0    0    3    0    0
  1127    4    0]
 [   0    0    0    0    0    0   37    0    0    0    0    0    0    0
    58  252    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.18428184281842

F1 scores:
[       nan 0.94871795 0.97447978 0.92379679 0.97706422 0.95964126
 0.96759941 0.92592593 0.99883856 0.53846154 0.94835681 0.98509485
 0.93632959 0.99462366 0.96987952 0.83168317 0.96551724]

Kappa:
0.9564895590820053
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fce146deac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.837, val_acc:0.121]
Epoch [2/120    avg_loss:2.759, val_acc:0.134]
Epoch [3/120    avg_loss:2.673, val_acc:0.260]
Epoch [4/120    avg_loss:2.588, val_acc:0.371]
Epoch [5/120    avg_loss:2.503, val_acc:0.424]
Epoch [6/120    avg_loss:2.423, val_acc:0.461]
Epoch [7/120    avg_loss:2.327, val_acc:0.460]
Epoch [8/120    avg_loss:2.245, val_acc:0.480]
Epoch [9/120    avg_loss:2.189, val_acc:0.511]
Epoch [10/120    avg_loss:2.090, val_acc:0.511]
Epoch [11/120    avg_loss:1.981, val_acc:0.527]
Epoch [12/120    avg_loss:1.916, val_acc:0.536]
Epoch [13/120    avg_loss:1.813, val_acc:0.562]
Epoch [14/120    avg_loss:1.727, val_acc:0.587]
Epoch [15/120    avg_loss:1.586, val_acc:0.580]
Epoch [16/120    avg_loss:1.473, val_acc:0.617]
Epoch [17/120    avg_loss:1.327, val_acc:0.613]
Epoch [18/120    avg_loss:1.267, val_acc:0.643]
Epoch [19/120    avg_loss:1.213, val_acc:0.647]
Epoch [20/120    avg_loss:1.116, val_acc:0.628]
Epoch [21/120    avg_loss:1.030, val_acc:0.701]
Epoch [22/120    avg_loss:0.978, val_acc:0.693]
Epoch [23/120    avg_loss:0.989, val_acc:0.722]
Epoch [24/120    avg_loss:0.831, val_acc:0.775]
Epoch [25/120    avg_loss:0.817, val_acc:0.799]
Epoch [26/120    avg_loss:0.822, val_acc:0.753]
Epoch [27/120    avg_loss:0.699, val_acc:0.781]
Epoch [28/120    avg_loss:0.647, val_acc:0.797]
Epoch [29/120    avg_loss:0.634, val_acc:0.800]
Epoch [30/120    avg_loss:0.565, val_acc:0.826]
Epoch [31/120    avg_loss:0.483, val_acc:0.815]
Epoch [32/120    avg_loss:0.500, val_acc:0.846]
Epoch [33/120    avg_loss:0.445, val_acc:0.814]
Epoch [34/120    avg_loss:0.408, val_acc:0.829]
Epoch [35/120    avg_loss:0.377, val_acc:0.874]
Epoch [36/120    avg_loss:0.354, val_acc:0.865]
Epoch [37/120    avg_loss:0.344, val_acc:0.810]
Epoch [38/120    avg_loss:0.345, val_acc:0.860]
Epoch [39/120    avg_loss:0.302, val_acc:0.864]
Epoch [40/120    avg_loss:0.273, val_acc:0.865]
Epoch [41/120    avg_loss:0.286, val_acc:0.868]
Epoch [42/120    avg_loss:0.389, val_acc:0.818]
Epoch [43/120    avg_loss:0.468, val_acc:0.797]
Epoch [44/120    avg_loss:0.337, val_acc:0.874]
Epoch [45/120    avg_loss:0.236, val_acc:0.867]
Epoch [46/120    avg_loss:0.250, val_acc:0.887]
Epoch [47/120    avg_loss:0.216, val_acc:0.900]
Epoch [48/120    avg_loss:0.199, val_acc:0.902]
Epoch [49/120    avg_loss:0.242, val_acc:0.876]
Epoch [50/120    avg_loss:0.209, val_acc:0.907]
Epoch [51/120    avg_loss:0.206, val_acc:0.897]
Epoch [52/120    avg_loss:0.178, val_acc:0.906]
Epoch [53/120    avg_loss:0.167, val_acc:0.896]
Epoch [54/120    avg_loss:0.134, val_acc:0.919]
Epoch [55/120    avg_loss:0.119, val_acc:0.919]
Epoch [56/120    avg_loss:0.121, val_acc:0.920]
Epoch [57/120    avg_loss:0.131, val_acc:0.933]
Epoch [58/120    avg_loss:0.116, val_acc:0.921]
Epoch [59/120    avg_loss:0.127, val_acc:0.926]
Epoch [60/120    avg_loss:1.092, val_acc:0.546]
Epoch [61/120    avg_loss:1.051, val_acc:0.719]
Epoch [62/120    avg_loss:0.633, val_acc:0.816]
Epoch [63/120    avg_loss:0.358, val_acc:0.862]
Epoch [64/120    avg_loss:0.294, val_acc:0.838]
Epoch [65/120    avg_loss:0.286, val_acc:0.883]
Epoch [66/120    avg_loss:0.243, val_acc:0.892]
Epoch [67/120    avg_loss:0.185, val_acc:0.904]
Epoch [68/120    avg_loss:0.161, val_acc:0.901]
Epoch [69/120    avg_loss:0.141, val_acc:0.916]
Epoch [70/120    avg_loss:0.151, val_acc:0.912]
Epoch [71/120    avg_loss:0.112, val_acc:0.917]
Epoch [72/120    avg_loss:0.101, val_acc:0.922]
Epoch [73/120    avg_loss:0.111, val_acc:0.922]
Epoch [74/120    avg_loss:0.103, val_acc:0.929]
Epoch [75/120    avg_loss:0.094, val_acc:0.929]
Epoch [76/120    avg_loss:0.099, val_acc:0.930]
Epoch [77/120    avg_loss:0.085, val_acc:0.938]
Epoch [78/120    avg_loss:0.088, val_acc:0.935]
Epoch [79/120    avg_loss:0.092, val_acc:0.940]
Epoch [80/120    avg_loss:0.088, val_acc:0.939]
Epoch [81/120    avg_loss:0.090, val_acc:0.943]
Epoch [82/120    avg_loss:0.081, val_acc:0.942]
Epoch [83/120    avg_loss:0.085, val_acc:0.945]
Epoch [84/120    avg_loss:0.088, val_acc:0.945]
Epoch [85/120    avg_loss:0.083, val_acc:0.944]
Epoch [86/120    avg_loss:0.073, val_acc:0.945]
Epoch [87/120    avg_loss:0.072, val_acc:0.950]
Epoch [88/120    avg_loss:0.074, val_acc:0.946]
Epoch [89/120    avg_loss:0.077, val_acc:0.943]
Epoch [90/120    avg_loss:0.082, val_acc:0.948]
Epoch [91/120    avg_loss:0.073, val_acc:0.943]
Epoch [92/120    avg_loss:0.075, val_acc:0.944]
Epoch [93/120    avg_loss:0.080, val_acc:0.946]
Epoch [94/120    avg_loss:0.086, val_acc:0.941]
Epoch [95/120    avg_loss:0.076, val_acc:0.940]
Epoch [96/120    avg_loss:0.066, val_acc:0.944]
Epoch [97/120    avg_loss:0.075, val_acc:0.948]
Epoch [98/120    avg_loss:0.081, val_acc:0.949]
Epoch [99/120    avg_loss:0.074, val_acc:0.949]
Epoch [100/120    avg_loss:0.076, val_acc:0.943]
Epoch [101/120    avg_loss:0.083, val_acc:0.945]
Epoch [102/120    avg_loss:0.068, val_acc:0.945]
Epoch [103/120    avg_loss:0.074, val_acc:0.945]
Epoch [104/120    avg_loss:0.068, val_acc:0.945]
Epoch [105/120    avg_loss:0.073, val_acc:0.946]
Epoch [106/120    avg_loss:0.070, val_acc:0.946]
Epoch [107/120    avg_loss:0.066, val_acc:0.948]
Epoch [108/120    avg_loss:0.063, val_acc:0.948]
Epoch [109/120    avg_loss:0.065, val_acc:0.948]
Epoch [110/120    avg_loss:0.066, val_acc:0.949]
Epoch [111/120    avg_loss:0.076, val_acc:0.949]
Epoch [112/120    avg_loss:0.063, val_acc:0.949]
Epoch [113/120    avg_loss:0.071, val_acc:0.949]
Epoch [114/120    avg_loss:0.067, val_acc:0.949]
Epoch [115/120    avg_loss:0.066, val_acc:0.949]
Epoch [116/120    avg_loss:0.075, val_acc:0.949]
Epoch [117/120    avg_loss:0.071, val_acc:0.949]
Epoch [118/120    avg_loss:0.064, val_acc:0.949]
Epoch [119/120    avg_loss:0.070, val_acc:0.949]
Epoch [120/120    avg_loss:0.067, val_acc:0.949]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1175    3   14    0    3    0    0    0   17   73    0    0
     0    0    0]
 [   0    0    7  682    6    0    1    0    0   31    1    0   19    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    2    0    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  647    0    0    0    0    5    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    2    0    0   11    0    1    0    0
     0    0    0]
 [   0    0   28   57    0    5    1    0    0    0  728   46    1    0
     1    8    0]
 [   0    0   34    0    0    0   12    0    6    0   24 2117   12    4
     1    0    0]
 [   0    0    0   19    1    2    0    0    0    1   13   10  480    0
     1    1    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    1    0    0    8    0    0    2    0    1    0    0    0
  1124    3    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    81  261    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
93.51761517615176

F1 scores:
[       nan 0.96202532 0.92885375 0.9021164  0.95302013 0.97610922
 0.97293233 1.         0.99078341 0.33846154 0.87605295 0.94890184
 0.91603053 0.98930481 0.95578231 0.84193548 0.95348837]

Kappa:
0.9260457765054045
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc483951b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.771, val_acc:0.289]
Epoch [2/120    avg_loss:2.653, val_acc:0.349]
Epoch [3/120    avg_loss:2.549, val_acc:0.375]
Epoch [4/120    avg_loss:2.472, val_acc:0.410]
Epoch [5/120    avg_loss:2.396, val_acc:0.433]
Epoch [6/120    avg_loss:2.340, val_acc:0.461]
Epoch [7/120    avg_loss:2.271, val_acc:0.470]
Epoch [8/120    avg_loss:2.195, val_acc:0.474]
Epoch [9/120    avg_loss:2.144, val_acc:0.493]
Epoch [10/120    avg_loss:2.084, val_acc:0.547]
Epoch [11/120    avg_loss:2.012, val_acc:0.539]
Epoch [12/120    avg_loss:1.925, val_acc:0.558]
Epoch [13/120    avg_loss:1.875, val_acc:0.560]
Epoch [14/120    avg_loss:1.775, val_acc:0.558]
Epoch [15/120    avg_loss:1.706, val_acc:0.581]
Epoch [16/120    avg_loss:1.664, val_acc:0.614]
Epoch [17/120    avg_loss:1.561, val_acc:0.651]
Epoch [18/120    avg_loss:1.481, val_acc:0.671]
Epoch [19/120    avg_loss:1.384, val_acc:0.689]
Epoch [20/120    avg_loss:1.326, val_acc:0.704]
Epoch [21/120    avg_loss:1.234, val_acc:0.724]
Epoch [22/120    avg_loss:1.150, val_acc:0.689]
Epoch [23/120    avg_loss:1.124, val_acc:0.731]
Epoch [24/120    avg_loss:1.036, val_acc:0.754]
Epoch [25/120    avg_loss:0.960, val_acc:0.751]
Epoch [26/120    avg_loss:0.844, val_acc:0.735]
Epoch [27/120    avg_loss:0.760, val_acc:0.785]
Epoch [28/120    avg_loss:0.747, val_acc:0.776]
Epoch [29/120    avg_loss:0.705, val_acc:0.761]
Epoch [30/120    avg_loss:0.688, val_acc:0.794]
Epoch [31/120    avg_loss:0.616, val_acc:0.812]
Epoch [32/120    avg_loss:0.558, val_acc:0.846]
Epoch [33/120    avg_loss:0.495, val_acc:0.833]
Epoch [34/120    avg_loss:0.485, val_acc:0.840]
Epoch [35/120    avg_loss:0.438, val_acc:0.881]
Epoch [36/120    avg_loss:0.454, val_acc:0.848]
Epoch [37/120    avg_loss:0.423, val_acc:0.855]
Epoch [38/120    avg_loss:0.415, val_acc:0.856]
Epoch [39/120    avg_loss:0.325, val_acc:0.895]
Epoch [40/120    avg_loss:0.333, val_acc:0.898]
Epoch [41/120    avg_loss:0.277, val_acc:0.886]
Epoch [42/120    avg_loss:0.277, val_acc:0.902]
Epoch [43/120    avg_loss:0.308, val_acc:0.884]
Epoch [44/120    avg_loss:0.319, val_acc:0.873]
Epoch [45/120    avg_loss:0.282, val_acc:0.863]
Epoch [46/120    avg_loss:0.264, val_acc:0.904]
Epoch [47/120    avg_loss:0.217, val_acc:0.914]
Epoch [48/120    avg_loss:0.216, val_acc:0.890]
Epoch [49/120    avg_loss:0.239, val_acc:0.919]
Epoch [50/120    avg_loss:0.267, val_acc:0.912]
Epoch [51/120    avg_loss:0.181, val_acc:0.916]
Epoch [52/120    avg_loss:0.180, val_acc:0.929]
Epoch [53/120    avg_loss:0.197, val_acc:0.927]
Epoch [54/120    avg_loss:0.176, val_acc:0.933]
Epoch [55/120    avg_loss:0.150, val_acc:0.913]
Epoch [56/120    avg_loss:0.153, val_acc:0.935]
Epoch [57/120    avg_loss:0.137, val_acc:0.942]
Epoch [58/120    avg_loss:0.124, val_acc:0.926]
Epoch [59/120    avg_loss:0.123, val_acc:0.939]
Epoch [60/120    avg_loss:0.104, val_acc:0.940]
Epoch [61/120    avg_loss:0.104, val_acc:0.936]
Epoch [62/120    avg_loss:0.125, val_acc:0.946]
Epoch [63/120    avg_loss:0.107, val_acc:0.943]
Epoch [64/120    avg_loss:0.106, val_acc:0.948]
Epoch [65/120    avg_loss:0.121, val_acc:0.950]
Epoch [66/120    avg_loss:0.116, val_acc:0.952]
Epoch [67/120    avg_loss:0.087, val_acc:0.943]
Epoch [68/120    avg_loss:0.081, val_acc:0.941]
Epoch [69/120    avg_loss:0.101, val_acc:0.935]
Epoch [70/120    avg_loss:0.120, val_acc:0.938]
Epoch [71/120    avg_loss:0.102, val_acc:0.938]
Epoch [72/120    avg_loss:0.094, val_acc:0.939]
Epoch [73/120    avg_loss:0.086, val_acc:0.943]
Epoch [74/120    avg_loss:0.074, val_acc:0.941]
Epoch [75/120    avg_loss:0.073, val_acc:0.942]
Epoch [76/120    avg_loss:0.068, val_acc:0.953]
Epoch [77/120    avg_loss:0.068, val_acc:0.952]
Epoch [78/120    avg_loss:0.074, val_acc:0.948]
Epoch [79/120    avg_loss:0.075, val_acc:0.950]
Epoch [80/120    avg_loss:0.106, val_acc:0.926]
Epoch [81/120    avg_loss:0.112, val_acc:0.952]
Epoch [82/120    avg_loss:0.087, val_acc:0.953]
Epoch [83/120    avg_loss:0.079, val_acc:0.955]
Epoch [84/120    avg_loss:0.058, val_acc:0.953]
Epoch [85/120    avg_loss:0.051, val_acc:0.954]
Epoch [86/120    avg_loss:0.056, val_acc:0.953]
Epoch [87/120    avg_loss:0.055, val_acc:0.959]
Epoch [88/120    avg_loss:0.049, val_acc:0.962]
Epoch [89/120    avg_loss:0.050, val_acc:0.955]
Epoch [90/120    avg_loss:0.040, val_acc:0.961]
Epoch [91/120    avg_loss:0.040, val_acc:0.958]
Epoch [92/120    avg_loss:0.040, val_acc:0.962]
Epoch [93/120    avg_loss:0.044, val_acc:0.960]
Epoch [94/120    avg_loss:0.080, val_acc:0.951]
Epoch [95/120    avg_loss:0.085, val_acc:0.956]
Epoch [96/120    avg_loss:0.053, val_acc:0.963]
Epoch [97/120    avg_loss:0.063, val_acc:0.942]
Epoch [98/120    avg_loss:0.068, val_acc:0.940]
Epoch [99/120    avg_loss:0.054, val_acc:0.962]
Epoch [100/120    avg_loss:0.040, val_acc:0.961]
Epoch [101/120    avg_loss:0.035, val_acc:0.965]
Epoch [102/120    avg_loss:0.035, val_acc:0.959]
Epoch [103/120    avg_loss:0.052, val_acc:0.952]
Epoch [104/120    avg_loss:0.046, val_acc:0.962]
Epoch [105/120    avg_loss:0.054, val_acc:0.962]
Epoch [106/120    avg_loss:0.040, val_acc:0.965]
Epoch [107/120    avg_loss:0.037, val_acc:0.960]
Epoch [108/120    avg_loss:0.033, val_acc:0.964]
Epoch [109/120    avg_loss:0.043, val_acc:0.965]
Epoch [110/120    avg_loss:0.036, val_acc:0.963]
Epoch [111/120    avg_loss:0.035, val_acc:0.962]
Epoch [112/120    avg_loss:0.042, val_acc:0.960]
Epoch [113/120    avg_loss:0.038, val_acc:0.968]
Epoch [114/120    avg_loss:0.038, val_acc:0.969]
Epoch [115/120    avg_loss:0.033, val_acc:0.968]
Epoch [116/120    avg_loss:0.035, val_acc:0.968]
Epoch [117/120    avg_loss:0.026, val_acc:0.972]
Epoch [118/120    avg_loss:0.034, val_acc:0.964]
Epoch [119/120    avg_loss:0.042, val_acc:0.962]
Epoch [120/120    avg_loss:0.040, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    2 1195    4    0    1    0    0    0    0    3   78    2    0
     0    0    0]
 [   0    0    5  600    1   36    0    0    0    7    0    0   98    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    2    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   31   57    0    4    0    0    0    0  762   17    1    0
     1    2    0]
 [   0    0    6    0    0    2    1    0    0    0   20 2178    1    2
     0    0    0]
 [   0    0    0    0   10   11    0    0    0    0    6    1  502    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    1    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0   12    0    0    3    0    0    0    0
    39  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
94.710027100271

F1 scores:
[       nan 0.97619048 0.94766059 0.85227273 0.97482838 0.93275488
 0.98715042 0.96153846 1.         0.68085106 0.91421716 0.97080455
 0.8768559  0.99462366 0.98056156 0.91277259 0.94610778]

Kappa:
0.9396521282605017
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fde5335aac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.792, val_acc:0.152]
Epoch [2/120    avg_loss:2.696, val_acc:0.239]
Epoch [3/120    avg_loss:2.597, val_acc:0.262]
Epoch [4/120    avg_loss:2.488, val_acc:0.301]
Epoch [5/120    avg_loss:2.393, val_acc:0.338]
Epoch [6/120    avg_loss:2.325, val_acc:0.368]
Epoch [7/120    avg_loss:2.273, val_acc:0.390]
Epoch [8/120    avg_loss:2.176, val_acc:0.433]
Epoch [9/120    avg_loss:2.136, val_acc:0.475]
Epoch [10/120    avg_loss:2.052, val_acc:0.481]
Epoch [11/120    avg_loss:1.961, val_acc:0.475]
Epoch [12/120    avg_loss:1.893, val_acc:0.506]
Epoch [13/120    avg_loss:1.768, val_acc:0.522]
Epoch [14/120    avg_loss:1.724, val_acc:0.566]
Epoch [15/120    avg_loss:1.656, val_acc:0.576]
Epoch [16/120    avg_loss:1.557, val_acc:0.599]
Epoch [17/120    avg_loss:1.465, val_acc:0.617]
Epoch [18/120    avg_loss:1.366, val_acc:0.652]
Epoch [19/120    avg_loss:1.356, val_acc:0.666]
Epoch [20/120    avg_loss:1.223, val_acc:0.706]
Epoch [21/120    avg_loss:1.121, val_acc:0.714]
Epoch [22/120    avg_loss:1.079, val_acc:0.713]
Epoch [23/120    avg_loss:0.948, val_acc:0.710]
Epoch [24/120    avg_loss:0.921, val_acc:0.702]
Epoch [25/120    avg_loss:0.899, val_acc:0.734]
Epoch [26/120    avg_loss:0.834, val_acc:0.778]
Epoch [27/120    avg_loss:0.812, val_acc:0.754]
Epoch [28/120    avg_loss:0.723, val_acc:0.766]
Epoch [29/120    avg_loss:0.672, val_acc:0.798]
Epoch [30/120    avg_loss:0.633, val_acc:0.809]
Epoch [31/120    avg_loss:0.541, val_acc:0.837]
Epoch [32/120    avg_loss:0.485, val_acc:0.800]
Epoch [33/120    avg_loss:0.519, val_acc:0.829]
Epoch [34/120    avg_loss:0.471, val_acc:0.836]
Epoch [35/120    avg_loss:0.463, val_acc:0.842]
Epoch [36/120    avg_loss:0.375, val_acc:0.858]
Epoch [37/120    avg_loss:0.359, val_acc:0.862]
Epoch [38/120    avg_loss:0.378, val_acc:0.862]
Epoch [39/120    avg_loss:0.319, val_acc:0.874]
Epoch [40/120    avg_loss:0.309, val_acc:0.844]
Epoch [41/120    avg_loss:0.278, val_acc:0.893]
Epoch [42/120    avg_loss:0.308, val_acc:0.860]
Epoch [43/120    avg_loss:0.311, val_acc:0.872]
Epoch [44/120    avg_loss:0.297, val_acc:0.903]
Epoch [45/120    avg_loss:0.258, val_acc:0.864]
Epoch [46/120    avg_loss:0.243, val_acc:0.911]
Epoch [47/120    avg_loss:0.228, val_acc:0.893]
Epoch [48/120    avg_loss:0.229, val_acc:0.881]
Epoch [49/120    avg_loss:0.235, val_acc:0.890]
Epoch [50/120    avg_loss:0.229, val_acc:0.906]
Epoch [51/120    avg_loss:0.172, val_acc:0.912]
Epoch [52/120    avg_loss:0.149, val_acc:0.905]
Epoch [53/120    avg_loss:0.149, val_acc:0.905]
Epoch [54/120    avg_loss:0.157, val_acc:0.922]
Epoch [55/120    avg_loss:0.163, val_acc:0.923]
Epoch [56/120    avg_loss:0.146, val_acc:0.919]
Epoch [57/120    avg_loss:0.129, val_acc:0.939]
Epoch [58/120    avg_loss:0.113, val_acc:0.917]
Epoch [59/120    avg_loss:0.106, val_acc:0.932]
Epoch [60/120    avg_loss:0.139, val_acc:0.896]
Epoch [61/120    avg_loss:0.210, val_acc:0.905]
Epoch [62/120    avg_loss:0.176, val_acc:0.904]
Epoch [63/120    avg_loss:0.163, val_acc:0.907]
Epoch [64/120    avg_loss:0.116, val_acc:0.936]
Epoch [65/120    avg_loss:0.116, val_acc:0.936]
Epoch [66/120    avg_loss:0.118, val_acc:0.938]
Epoch [67/120    avg_loss:0.092, val_acc:0.941]
Epoch [68/120    avg_loss:0.108, val_acc:0.948]
Epoch [69/120    avg_loss:0.102, val_acc:0.912]
Epoch [70/120    avg_loss:0.085, val_acc:0.941]
Epoch [71/120    avg_loss:0.079, val_acc:0.949]
Epoch [72/120    avg_loss:0.100, val_acc:0.939]
Epoch [73/120    avg_loss:0.097, val_acc:0.920]
Epoch [74/120    avg_loss:0.098, val_acc:0.906]
Epoch [75/120    avg_loss:0.082, val_acc:0.949]
Epoch [76/120    avg_loss:0.077, val_acc:0.929]
Epoch [77/120    avg_loss:0.072, val_acc:0.933]
Epoch [78/120    avg_loss:0.090, val_acc:0.939]
Epoch [79/120    avg_loss:0.055, val_acc:0.940]
Epoch [80/120    avg_loss:0.058, val_acc:0.951]
Epoch [81/120    avg_loss:0.067, val_acc:0.948]
Epoch [82/120    avg_loss:0.051, val_acc:0.943]
Epoch [83/120    avg_loss:0.062, val_acc:0.952]
Epoch [84/120    avg_loss:0.051, val_acc:0.959]
Epoch [85/120    avg_loss:0.052, val_acc:0.954]
Epoch [86/120    avg_loss:0.044, val_acc:0.952]
Epoch [87/120    avg_loss:0.048, val_acc:0.958]
Epoch [88/120    avg_loss:0.050, val_acc:0.956]
Epoch [89/120    avg_loss:0.049, val_acc:0.965]
Epoch [90/120    avg_loss:0.048, val_acc:0.961]
Epoch [91/120    avg_loss:0.050, val_acc:0.960]
Epoch [92/120    avg_loss:0.039, val_acc:0.960]
Epoch [93/120    avg_loss:0.033, val_acc:0.960]
Epoch [94/120    avg_loss:0.049, val_acc:0.956]
Epoch [95/120    avg_loss:0.056, val_acc:0.944]
Epoch [96/120    avg_loss:0.056, val_acc:0.961]
Epoch [97/120    avg_loss:0.050, val_acc:0.954]
Epoch [98/120    avg_loss:0.056, val_acc:0.958]
Epoch [99/120    avg_loss:0.044, val_acc:0.956]
Epoch [100/120    avg_loss:0.031, val_acc:0.970]
Epoch [101/120    avg_loss:0.030, val_acc:0.965]
Epoch [102/120    avg_loss:0.030, val_acc:0.969]
Epoch [103/120    avg_loss:0.032, val_acc:0.970]
Epoch [104/120    avg_loss:0.025, val_acc:0.970]
Epoch [105/120    avg_loss:0.034, val_acc:0.959]
Epoch [106/120    avg_loss:0.036, val_acc:0.973]
Epoch [107/120    avg_loss:0.034, val_acc:0.964]
Epoch [108/120    avg_loss:0.030, val_acc:0.972]
Epoch [109/120    avg_loss:0.043, val_acc:0.967]
Epoch [110/120    avg_loss:0.042, val_acc:0.898]
Epoch [111/120    avg_loss:0.072, val_acc:0.960]
Epoch [112/120    avg_loss:0.047, val_acc:0.955]
Epoch [113/120    avg_loss:0.029, val_acc:0.962]
Epoch [114/120    avg_loss:0.032, val_acc:0.960]
Epoch [115/120    avg_loss:0.034, val_acc:0.958]
Epoch [116/120    avg_loss:0.025, val_acc:0.968]
Epoch [117/120    avg_loss:0.044, val_acc:0.962]
Epoch [118/120    avg_loss:0.049, val_acc:0.958]
Epoch [119/120    avg_loss:0.035, val_acc:0.965]
Epoch [120/120    avg_loss:0.030, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1243    3    0    2    0    0    0    0    6   29    2    0
     0    0    0]
 [   0    0    4  694    1   14    0    0    0   19    0    0   14    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    1    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  654    0    0    1    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   52   72    0    8    0    0    0    0  737    1    0    0
     0    5    0]
 [   0    0   24    0    0    0    3    0    0    0   20 2157    4    1
     1    0    0]
 [   0    0    0    3    8   11    0    0    0    0    6    6  491    0
     0    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    3    0    4    0    0    0
  1124    0    0]
 [   0    0    0    0    0    0   22    0    0   13    0    0    0    0
    66  246    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.02439024390245

F1 scores:
[       nan 0.98765432 0.95322086 0.91375905 0.97931034 0.94247788
 0.97904192 1.         0.99535963 0.51428571 0.89387508 0.97934166
 0.93881453 0.99730458 0.96109448 0.82274247 0.94915254]

Kappa:
0.9432705427884754
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4b09ce9ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.811, val_acc:0.176]
Epoch [2/120    avg_loss:2.716, val_acc:0.215]
Epoch [3/120    avg_loss:2.616, val_acc:0.282]
Epoch [4/120    avg_loss:2.515, val_acc:0.390]
Epoch [5/120    avg_loss:2.421, val_acc:0.414]
Epoch [6/120    avg_loss:2.328, val_acc:0.439]
Epoch [7/120    avg_loss:2.256, val_acc:0.472]
Epoch [8/120    avg_loss:2.156, val_acc:0.522]
Epoch [9/120    avg_loss:2.108, val_acc:0.540]
Epoch [10/120    avg_loss:2.039, val_acc:0.546]
Epoch [11/120    avg_loss:1.976, val_acc:0.567]
Epoch [12/120    avg_loss:1.937, val_acc:0.577]
Epoch [13/120    avg_loss:1.902, val_acc:0.589]
Epoch [14/120    avg_loss:1.767, val_acc:0.600]
Epoch [15/120    avg_loss:1.730, val_acc:0.613]
Epoch [16/120    avg_loss:1.587, val_acc:0.602]
Epoch [17/120    avg_loss:1.589, val_acc:0.637]
Epoch [18/120    avg_loss:1.450, val_acc:0.635]
Epoch [19/120    avg_loss:1.348, val_acc:0.656]
Epoch [20/120    avg_loss:1.300, val_acc:0.664]
Epoch [21/120    avg_loss:1.287, val_acc:0.671]
Epoch [22/120    avg_loss:1.230, val_acc:0.671]
Epoch [23/120    avg_loss:1.135, val_acc:0.693]
Epoch [24/120    avg_loss:1.085, val_acc:0.709]
Epoch [25/120    avg_loss:0.968, val_acc:0.720]
Epoch [26/120    avg_loss:0.935, val_acc:0.728]
Epoch [27/120    avg_loss:0.927, val_acc:0.740]
Epoch [28/120    avg_loss:0.823, val_acc:0.768]
Epoch [29/120    avg_loss:0.753, val_acc:0.791]
Epoch [30/120    avg_loss:0.766, val_acc:0.759]
Epoch [31/120    avg_loss:0.714, val_acc:0.779]
Epoch [32/120    avg_loss:0.661, val_acc:0.772]
Epoch [33/120    avg_loss:0.651, val_acc:0.805]
Epoch [34/120    avg_loss:0.578, val_acc:0.850]
Epoch [35/120    avg_loss:0.488, val_acc:0.815]
Epoch [36/120    avg_loss:0.512, val_acc:0.852]
Epoch [37/120    avg_loss:0.399, val_acc:0.833]
Epoch [38/120    avg_loss:0.408, val_acc:0.862]
Epoch [39/120    avg_loss:0.397, val_acc:0.860]
Epoch [40/120    avg_loss:0.392, val_acc:0.828]
Epoch [41/120    avg_loss:0.357, val_acc:0.843]
Epoch [42/120    avg_loss:0.347, val_acc:0.881]
Epoch [43/120    avg_loss:0.306, val_acc:0.894]
Epoch [44/120    avg_loss:0.240, val_acc:0.895]
Epoch [45/120    avg_loss:0.243, val_acc:0.867]
Epoch [46/120    avg_loss:0.239, val_acc:0.900]
Epoch [47/120    avg_loss:0.275, val_acc:0.890]
Epoch [48/120    avg_loss:0.230, val_acc:0.901]
Epoch [49/120    avg_loss:0.181, val_acc:0.906]
Epoch [50/120    avg_loss:0.176, val_acc:0.919]
Epoch [51/120    avg_loss:0.168, val_acc:0.904]
Epoch [52/120    avg_loss:0.191, val_acc:0.910]
Epoch [53/120    avg_loss:0.163, val_acc:0.931]
Epoch [54/120    avg_loss:0.159, val_acc:0.929]
Epoch [55/120    avg_loss:0.169, val_acc:0.921]
Epoch [56/120    avg_loss:0.166, val_acc:0.923]
Epoch [57/120    avg_loss:0.133, val_acc:0.913]
Epoch [58/120    avg_loss:0.121, val_acc:0.926]
Epoch [59/120    avg_loss:0.116, val_acc:0.931]
Epoch [60/120    avg_loss:0.129, val_acc:0.924]
Epoch [61/120    avg_loss:0.133, val_acc:0.944]
Epoch [62/120    avg_loss:0.125, val_acc:0.938]
Epoch [63/120    avg_loss:0.121, val_acc:0.939]
Epoch [64/120    avg_loss:0.101, val_acc:0.952]
Epoch [65/120    avg_loss:0.109, val_acc:0.934]
Epoch [66/120    avg_loss:0.093, val_acc:0.930]
Epoch [67/120    avg_loss:0.099, val_acc:0.955]
Epoch [68/120    avg_loss:0.102, val_acc:0.945]
Epoch [69/120    avg_loss:0.085, val_acc:0.943]
Epoch [70/120    avg_loss:0.072, val_acc:0.958]
Epoch [71/120    avg_loss:0.103, val_acc:0.953]
Epoch [72/120    avg_loss:0.080, val_acc:0.960]
Epoch [73/120    avg_loss:0.065, val_acc:0.958]
Epoch [74/120    avg_loss:0.062, val_acc:0.962]
Epoch [75/120    avg_loss:0.058, val_acc:0.965]
Epoch [76/120    avg_loss:0.064, val_acc:0.950]
Epoch [77/120    avg_loss:0.067, val_acc:0.970]
Epoch [78/120    avg_loss:0.046, val_acc:0.964]
Epoch [79/120    avg_loss:0.061, val_acc:0.952]
Epoch [80/120    avg_loss:0.078, val_acc:0.954]
Epoch [81/120    avg_loss:0.052, val_acc:0.967]
Epoch [82/120    avg_loss:0.045, val_acc:0.955]
Epoch [83/120    avg_loss:0.049, val_acc:0.973]
Epoch [84/120    avg_loss:0.049, val_acc:0.960]
Epoch [85/120    avg_loss:0.056, val_acc:0.958]
Epoch [86/120    avg_loss:0.063, val_acc:0.964]
Epoch [87/120    avg_loss:0.041, val_acc:0.970]
Epoch [88/120    avg_loss:0.057, val_acc:0.971]
Epoch [89/120    avg_loss:0.047, val_acc:0.968]
Epoch [90/120    avg_loss:0.058, val_acc:0.965]
Epoch [91/120    avg_loss:0.045, val_acc:0.952]
Epoch [92/120    avg_loss:0.043, val_acc:0.971]
Epoch [93/120    avg_loss:0.046, val_acc:0.953]
Epoch [94/120    avg_loss:0.045, val_acc:0.964]
Epoch [95/120    avg_loss:0.043, val_acc:0.962]
Epoch [96/120    avg_loss:0.040, val_acc:0.968]
Epoch [97/120    avg_loss:0.035, val_acc:0.974]
Epoch [98/120    avg_loss:0.025, val_acc:0.974]
Epoch [99/120    avg_loss:0.022, val_acc:0.973]
Epoch [100/120    avg_loss:0.022, val_acc:0.971]
Epoch [101/120    avg_loss:0.020, val_acc:0.973]
Epoch [102/120    avg_loss:0.019, val_acc:0.972]
Epoch [103/120    avg_loss:0.021, val_acc:0.973]
Epoch [104/120    avg_loss:0.020, val_acc:0.973]
Epoch [105/120    avg_loss:0.022, val_acc:0.973]
Epoch [106/120    avg_loss:0.021, val_acc:0.973]
Epoch [107/120    avg_loss:0.017, val_acc:0.973]
Epoch [108/120    avg_loss:0.020, val_acc:0.972]
Epoch [109/120    avg_loss:0.021, val_acc:0.973]
Epoch [110/120    avg_loss:0.017, val_acc:0.974]
Epoch [111/120    avg_loss:0.019, val_acc:0.974]
Epoch [112/120    avg_loss:0.016, val_acc:0.973]
Epoch [113/120    avg_loss:0.018, val_acc:0.973]
Epoch [114/120    avg_loss:0.020, val_acc:0.974]
Epoch [115/120    avg_loss:0.016, val_acc:0.974]
Epoch [116/120    avg_loss:0.020, val_acc:0.975]
Epoch [117/120    avg_loss:0.019, val_acc:0.974]
Epoch [118/120    avg_loss:0.018, val_acc:0.973]
Epoch [119/120    avg_loss:0.019, val_acc:0.974]
Epoch [120/120    avg_loss:0.027, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1252    5    5    0    0    0    0    0    2   20    1    0
     0    0    0]
 [   0    0    6  697    3   10    0    0    0   16    0    0   12    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    6    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   18   29    0    8    1    0    0    0  810    6    0    0
     0    3    0]
 [   0    0   10    0    0    1    6    1    0    0   14 2173    4    1
     0    0    0]
 [   0    0    0   23   14   11    0    0    0    2   12    0  465    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0   40    0    0    0    0    0    0    0
    76  231    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
95.69647696476964

F1 scores:
[       nan 0.96202532 0.97356143 0.92809587 0.95089286 0.9527027
 0.96470588 0.87719298 0.99883856 0.55172414 0.94295693 0.98548753
 0.90998043 0.98930481 0.96595745 0.79518072 0.92941176]

Kappa:
0.9509219890622033
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdcb6c1ca58>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.835, val_acc:0.075]
Epoch [2/120    avg_loss:2.718, val_acc:0.232]
Epoch [3/120    avg_loss:2.607, val_acc:0.402]
Epoch [4/120    avg_loss:2.491, val_acc:0.465]
Epoch [5/120    avg_loss:2.399, val_acc:0.487]
Epoch [6/120    avg_loss:2.333, val_acc:0.515]
Epoch [7/120    avg_loss:2.217, val_acc:0.537]
Epoch [8/120    avg_loss:2.168, val_acc:0.519]
Epoch [9/120    avg_loss:2.077, val_acc:0.560]
Epoch [10/120    avg_loss:1.995, val_acc:0.565]
Epoch [11/120    avg_loss:1.938, val_acc:0.587]
Epoch [12/120    avg_loss:1.886, val_acc:0.594]
Epoch [13/120    avg_loss:1.790, val_acc:0.629]
Epoch [14/120    avg_loss:1.721, val_acc:0.629]
Epoch [15/120    avg_loss:1.652, val_acc:0.623]
Epoch [16/120    avg_loss:1.572, val_acc:0.664]
Epoch [17/120    avg_loss:1.489, val_acc:0.690]
Epoch [18/120    avg_loss:1.370, val_acc:0.709]
Epoch [19/120    avg_loss:1.350, val_acc:0.699]
Epoch [20/120    avg_loss:1.234, val_acc:0.718]
Epoch [21/120    avg_loss:1.187, val_acc:0.730]
Epoch [22/120    avg_loss:1.046, val_acc:0.733]
Epoch [23/120    avg_loss:0.988, val_acc:0.732]
Epoch [24/120    avg_loss:1.063, val_acc:0.698]
Epoch [25/120    avg_loss:0.855, val_acc:0.756]
Epoch [26/120    avg_loss:0.789, val_acc:0.765]
Epoch [27/120    avg_loss:0.763, val_acc:0.782]
Epoch [28/120    avg_loss:0.688, val_acc:0.794]
Epoch [29/120    avg_loss:0.645, val_acc:0.782]
Epoch [30/120    avg_loss:0.590, val_acc:0.829]
Epoch [31/120    avg_loss:0.605, val_acc:0.769]
Epoch [32/120    avg_loss:0.522, val_acc:0.836]
Epoch [33/120    avg_loss:0.520, val_acc:0.806]
Epoch [34/120    avg_loss:0.694, val_acc:0.801]
Epoch [35/120    avg_loss:0.484, val_acc:0.789]
Epoch [36/120    avg_loss:0.675, val_acc:0.789]
Epoch [37/120    avg_loss:0.502, val_acc:0.787]
Epoch [38/120    avg_loss:0.403, val_acc:0.843]
Epoch [39/120    avg_loss:0.402, val_acc:0.847]
Epoch [40/120    avg_loss:0.353, val_acc:0.857]
Epoch [41/120    avg_loss:0.351, val_acc:0.853]
Epoch [42/120    avg_loss:0.331, val_acc:0.862]
Epoch [43/120    avg_loss:0.255, val_acc:0.882]
Epoch [44/120    avg_loss:0.288, val_acc:0.882]
Epoch [45/120    avg_loss:0.262, val_acc:0.876]
Epoch [46/120    avg_loss:0.292, val_acc:0.879]
Epoch [47/120    avg_loss:0.244, val_acc:0.884]
Epoch [48/120    avg_loss:0.222, val_acc:0.896]
Epoch [49/120    avg_loss:0.204, val_acc:0.911]
Epoch [50/120    avg_loss:0.170, val_acc:0.906]
Epoch [51/120    avg_loss:0.171, val_acc:0.911]
Epoch [52/120    avg_loss:0.182, val_acc:0.903]
Epoch [53/120    avg_loss:0.161, val_acc:0.919]
Epoch [54/120    avg_loss:0.135, val_acc:0.916]
Epoch [55/120    avg_loss:0.165, val_acc:0.877]
Epoch [56/120    avg_loss:0.167, val_acc:0.915]
Epoch [57/120    avg_loss:0.147, val_acc:0.910]
Epoch [58/120    avg_loss:0.114, val_acc:0.939]
Epoch [59/120    avg_loss:0.117, val_acc:0.930]
Epoch [60/120    avg_loss:0.124, val_acc:0.929]
Epoch [61/120    avg_loss:0.108, val_acc:0.941]
Epoch [62/120    avg_loss:0.082, val_acc:0.934]
Epoch [63/120    avg_loss:0.083, val_acc:0.939]
Epoch [64/120    avg_loss:0.082, val_acc:0.931]
Epoch [65/120    avg_loss:0.094, val_acc:0.916]
Epoch [66/120    avg_loss:0.080, val_acc:0.939]
Epoch [67/120    avg_loss:0.078, val_acc:0.950]
Epoch [68/120    avg_loss:0.089, val_acc:0.945]
Epoch [69/120    avg_loss:0.075, val_acc:0.943]
Epoch [70/120    avg_loss:0.068, val_acc:0.940]
Epoch [71/120    avg_loss:0.075, val_acc:0.942]
Epoch [72/120    avg_loss:0.064, val_acc:0.940]
Epoch [73/120    avg_loss:0.089, val_acc:0.944]
Epoch [74/120    avg_loss:0.085, val_acc:0.940]
Epoch [75/120    avg_loss:0.076, val_acc:0.929]
Epoch [76/120    avg_loss:0.067, val_acc:0.925]
Epoch [77/120    avg_loss:0.071, val_acc:0.948]
Epoch [78/120    avg_loss:0.069, val_acc:0.938]
Epoch [79/120    avg_loss:0.052, val_acc:0.950]
Epoch [80/120    avg_loss:0.056, val_acc:0.942]
Epoch [81/120    avg_loss:0.045, val_acc:0.943]
Epoch [82/120    avg_loss:0.046, val_acc:0.958]
Epoch [83/120    avg_loss:0.043, val_acc:0.953]
Epoch [84/120    avg_loss:0.039, val_acc:0.950]
Epoch [85/120    avg_loss:0.044, val_acc:0.948]
Epoch [86/120    avg_loss:0.039, val_acc:0.951]
Epoch [87/120    avg_loss:0.056, val_acc:0.945]
Epoch [88/120    avg_loss:0.044, val_acc:0.949]
Epoch [89/120    avg_loss:0.041, val_acc:0.950]
Epoch [90/120    avg_loss:0.035, val_acc:0.952]
Epoch [91/120    avg_loss:0.033, val_acc:0.965]
Epoch [92/120    avg_loss:0.031, val_acc:0.955]
Epoch [93/120    avg_loss:0.042, val_acc:0.962]
Epoch [94/120    avg_loss:0.031, val_acc:0.958]
Epoch [95/120    avg_loss:0.036, val_acc:0.960]
Epoch [96/120    avg_loss:0.037, val_acc:0.959]
Epoch [97/120    avg_loss:0.025, val_acc:0.964]
Epoch [98/120    avg_loss:0.029, val_acc:0.959]
Epoch [99/120    avg_loss:0.024, val_acc:0.956]
Epoch [100/120    avg_loss:0.024, val_acc:0.961]
Epoch [101/120    avg_loss:0.024, val_acc:0.963]
Epoch [102/120    avg_loss:0.028, val_acc:0.962]
Epoch [103/120    avg_loss:0.030, val_acc:0.956]
Epoch [104/120    avg_loss:0.036, val_acc:0.964]
Epoch [105/120    avg_loss:0.027, val_acc:0.962]
Epoch [106/120    avg_loss:0.026, val_acc:0.963]
Epoch [107/120    avg_loss:0.017, val_acc:0.963]
Epoch [108/120    avg_loss:0.019, val_acc:0.964]
Epoch [109/120    avg_loss:0.018, val_acc:0.965]
Epoch [110/120    avg_loss:0.015, val_acc:0.965]
Epoch [111/120    avg_loss:0.015, val_acc:0.962]
Epoch [112/120    avg_loss:0.018, val_acc:0.967]
Epoch [113/120    avg_loss:0.017, val_acc:0.967]
Epoch [114/120    avg_loss:0.015, val_acc:0.967]
Epoch [115/120    avg_loss:0.021, val_acc:0.967]
Epoch [116/120    avg_loss:0.016, val_acc:0.965]
Epoch [117/120    avg_loss:0.021, val_acc:0.969]
Epoch [118/120    avg_loss:0.017, val_acc:0.970]
Epoch [119/120    avg_loss:0.013, val_acc:0.969]
Epoch [120/120    avg_loss:0.015, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    1 1232    1    8    0    9    0    0    0    7   27    0    0
     0    0    0]
 [   0    0    1  677    2   23    1    0    0   17    1    0   24    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    1    0    3    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   11   26    0    4    0    0    0    0  819    8    2    0
     0    5    0]
 [   0    4    7    0    0    0    5    0    0    0   11 2175    4    4
     0    0    0]
 [   0    0    0    2    5   10    0    0    0    0    5    4  500    0
     0    2    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    0    0    0    0    0    0
  1129    0    0]
 [   0    0    0    0    0    1   69    0    0    1    0    0    0    0
    73  203    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.4579945799458

F1 scores:
[       nan 0.91764706 0.97160883 0.93186511 0.96598639 0.93846154
 0.93915533 0.98039216 0.9953271  0.60714286 0.95232558 0.98305085
 0.93457944 0.98666667 0.96289979 0.72890485 0.95953757]

Kappa:
0.9482062008189887
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f86bab90ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.793, val_acc:0.221]
Epoch [2/120    avg_loss:2.680, val_acc:0.350]
Epoch [3/120    avg_loss:2.580, val_acc:0.445]
Epoch [4/120    avg_loss:2.471, val_acc:0.486]
Epoch [5/120    avg_loss:2.390, val_acc:0.489]
Epoch [6/120    avg_loss:2.349, val_acc:0.513]
Epoch [7/120    avg_loss:2.267, val_acc:0.538]
Epoch [8/120    avg_loss:2.224, val_acc:0.511]
Epoch [9/120    avg_loss:2.099, val_acc:0.544]
Epoch [10/120    avg_loss:2.037, val_acc:0.556]
Epoch [11/120    avg_loss:1.946, val_acc:0.571]
Epoch [12/120    avg_loss:1.868, val_acc:0.578]
Epoch [13/120    avg_loss:1.811, val_acc:0.596]
Epoch [14/120    avg_loss:1.749, val_acc:0.591]
Epoch [15/120    avg_loss:1.660, val_acc:0.611]
Epoch [16/120    avg_loss:1.581, val_acc:0.608]
Epoch [17/120    avg_loss:1.462, val_acc:0.617]
Epoch [18/120    avg_loss:1.422, val_acc:0.636]
Epoch [19/120    avg_loss:1.329, val_acc:0.633]
Epoch [20/120    avg_loss:1.244, val_acc:0.676]
Epoch [21/120    avg_loss:1.166, val_acc:0.703]
Epoch [22/120    avg_loss:1.160, val_acc:0.693]
Epoch [23/120    avg_loss:1.072, val_acc:0.700]
Epoch [24/120    avg_loss:1.056, val_acc:0.722]
Epoch [25/120    avg_loss:0.901, val_acc:0.751]
Epoch [26/120    avg_loss:0.838, val_acc:0.764]
Epoch [27/120    avg_loss:0.808, val_acc:0.758]
Epoch [28/120    avg_loss:0.760, val_acc:0.775]
Epoch [29/120    avg_loss:0.705, val_acc:0.787]
Epoch [30/120    avg_loss:0.639, val_acc:0.805]
Epoch [31/120    avg_loss:0.605, val_acc:0.818]
Epoch [32/120    avg_loss:0.584, val_acc:0.811]
Epoch [33/120    avg_loss:0.530, val_acc:0.805]
Epoch [34/120    avg_loss:0.493, val_acc:0.851]
Epoch [35/120    avg_loss:0.433, val_acc:0.859]
Epoch [36/120    avg_loss:0.451, val_acc:0.859]
Epoch [37/120    avg_loss:0.411, val_acc:0.853]
Epoch [38/120    avg_loss:0.398, val_acc:0.866]
Epoch [39/120    avg_loss:0.361, val_acc:0.878]
Epoch [40/120    avg_loss:0.302, val_acc:0.881]
Epoch [41/120    avg_loss:0.288, val_acc:0.906]
Epoch [42/120    avg_loss:0.271, val_acc:0.902]
Epoch [43/120    avg_loss:0.314, val_acc:0.887]
Epoch [44/120    avg_loss:0.256, val_acc:0.903]
Epoch [45/120    avg_loss:0.281, val_acc:0.901]
Epoch [46/120    avg_loss:0.240, val_acc:0.912]
Epoch [47/120    avg_loss:0.247, val_acc:0.844]
Epoch [48/120    avg_loss:0.312, val_acc:0.863]
Epoch [49/120    avg_loss:0.291, val_acc:0.894]
Epoch [50/120    avg_loss:0.247, val_acc:0.908]
Epoch [51/120    avg_loss:0.188, val_acc:0.909]
Epoch [52/120    avg_loss:0.200, val_acc:0.897]
Epoch [53/120    avg_loss:0.200, val_acc:0.914]
Epoch [54/120    avg_loss:0.196, val_acc:0.897]
Epoch [55/120    avg_loss:0.175, val_acc:0.919]
Epoch [56/120    avg_loss:0.158, val_acc:0.922]
Epoch [57/120    avg_loss:0.161, val_acc:0.917]
Epoch [58/120    avg_loss:0.139, val_acc:0.931]
Epoch [59/120    avg_loss:0.120, val_acc:0.934]
Epoch [60/120    avg_loss:0.130, val_acc:0.928]
Epoch [61/120    avg_loss:0.120, val_acc:0.941]
Epoch [62/120    avg_loss:0.125, val_acc:0.931]
Epoch [63/120    avg_loss:0.102, val_acc:0.942]
Epoch [64/120    avg_loss:0.112, val_acc:0.950]
Epoch [65/120    avg_loss:0.118, val_acc:0.934]
Epoch [66/120    avg_loss:0.106, val_acc:0.939]
Epoch [67/120    avg_loss:0.099, val_acc:0.928]
Epoch [68/120    avg_loss:0.100, val_acc:0.940]
Epoch [69/120    avg_loss:0.095, val_acc:0.930]
Epoch [70/120    avg_loss:0.078, val_acc:0.948]
Epoch [71/120    avg_loss:0.077, val_acc:0.942]
Epoch [72/120    avg_loss:0.057, val_acc:0.949]
Epoch [73/120    avg_loss:0.061, val_acc:0.946]
Epoch [74/120    avg_loss:0.070, val_acc:0.954]
Epoch [75/120    avg_loss:0.075, val_acc:0.946]
Epoch [76/120    avg_loss:0.050, val_acc:0.940]
Epoch [77/120    avg_loss:0.063, val_acc:0.952]
Epoch [78/120    avg_loss:0.063, val_acc:0.953]
Epoch [79/120    avg_loss:0.056, val_acc:0.955]
Epoch [80/120    avg_loss:0.065, val_acc:0.936]
Epoch [81/120    avg_loss:0.055, val_acc:0.947]
Epoch [82/120    avg_loss:0.062, val_acc:0.952]
Epoch [83/120    avg_loss:0.062, val_acc:0.958]
Epoch [84/120    avg_loss:0.052, val_acc:0.957]
Epoch [85/120    avg_loss:0.050, val_acc:0.947]
Epoch [86/120    avg_loss:0.048, val_acc:0.954]
Epoch [87/120    avg_loss:0.069, val_acc:0.942]
Epoch [88/120    avg_loss:0.053, val_acc:0.949]
Epoch [89/120    avg_loss:0.051, val_acc:0.955]
Epoch [90/120    avg_loss:0.049, val_acc:0.953]
Epoch [91/120    avg_loss:0.078, val_acc:0.954]
Epoch [92/120    avg_loss:0.060, val_acc:0.960]
Epoch [93/120    avg_loss:0.095, val_acc:0.954]
Epoch [94/120    avg_loss:0.072, val_acc:0.949]
Epoch [95/120    avg_loss:0.073, val_acc:0.926]
Epoch [96/120    avg_loss:0.102, val_acc:0.938]
Epoch [97/120    avg_loss:0.081, val_acc:0.947]
Epoch [98/120    avg_loss:0.070, val_acc:0.949]
Epoch [99/120    avg_loss:0.048, val_acc:0.946]
Epoch [100/120    avg_loss:0.051, val_acc:0.952]
Epoch [101/120    avg_loss:0.045, val_acc:0.956]
Epoch [102/120    avg_loss:0.036, val_acc:0.944]
Epoch [103/120    avg_loss:0.046, val_acc:0.959]
Epoch [104/120    avg_loss:0.033, val_acc:0.957]
Epoch [105/120    avg_loss:0.050, val_acc:0.954]
Epoch [106/120    avg_loss:0.037, val_acc:0.957]
Epoch [107/120    avg_loss:0.036, val_acc:0.960]
Epoch [108/120    avg_loss:0.028, val_acc:0.958]
Epoch [109/120    avg_loss:0.031, val_acc:0.960]
Epoch [110/120    avg_loss:0.023, val_acc:0.958]
Epoch [111/120    avg_loss:0.028, val_acc:0.957]
Epoch [112/120    avg_loss:0.026, val_acc:0.956]
Epoch [113/120    avg_loss:0.029, val_acc:0.958]
Epoch [114/120    avg_loss:0.025, val_acc:0.956]
Epoch [115/120    avg_loss:0.024, val_acc:0.954]
Epoch [116/120    avg_loss:0.023, val_acc:0.955]
Epoch [117/120    avg_loss:0.021, val_acc:0.958]
Epoch [118/120    avg_loss:0.024, val_acc:0.960]
Epoch [119/120    avg_loss:0.026, val_acc:0.959]
Epoch [120/120    avg_loss:0.023, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1212   10   13    0    0    0    0    0   13   37    0    0
     0    0    0]
 [   0    0    5  705    1    9    0    0    0   10    1    1   13    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    5    0    0    2    0    0   10    0    0    1    0
     0    0    0]
 [   0    0   32   20    0    4    0    0    0    0  804    9    0    0
     0    6    0]
 [   0    0   18    0    0    0    8    0    0    0   10 2172    0    2
     0    0    0]
 [   0    0    0   38    6    7    0    0    0    0    2    2  472    0
     0    1    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    0    0    0
  1127    9    0]
 [   0    0    0    0    0    1    6    0    0    2    0    0    1    0
    32  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.10840108401084

F1 scores:
[       nan 0.96202532 0.94984326 0.92459016 0.95515695 0.97181511
 0.98720843 1.         0.99649942 0.47619048 0.93980129 0.9801444
 0.921875   0.98930481 0.98       0.91317365 0.96551724]

Kappa:
0.9556284279518897
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f912ffdb9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.787, val_acc:0.135]
Epoch [2/120    avg_loss:2.670, val_acc:0.240]
Epoch [3/120    avg_loss:2.557, val_acc:0.353]
Epoch [4/120    avg_loss:2.479, val_acc:0.433]
Epoch [5/120    avg_loss:2.374, val_acc:0.457]
Epoch [6/120    avg_loss:2.290, val_acc:0.486]
Epoch [7/120    avg_loss:2.226, val_acc:0.491]
Epoch [8/120    avg_loss:2.176, val_acc:0.527]
Epoch [9/120    avg_loss:2.075, val_acc:0.523]
Epoch [10/120    avg_loss:2.018, val_acc:0.562]
Epoch [11/120    avg_loss:1.950, val_acc:0.571]
Epoch [12/120    avg_loss:1.903, val_acc:0.609]
Epoch [13/120    avg_loss:1.827, val_acc:0.612]
Epoch [14/120    avg_loss:1.782, val_acc:0.629]
Epoch [15/120    avg_loss:1.622, val_acc:0.653]
Epoch [16/120    avg_loss:1.559, val_acc:0.671]
Epoch [17/120    avg_loss:1.496, val_acc:0.665]
Epoch [18/120    avg_loss:1.348, val_acc:0.688]
Epoch [19/120    avg_loss:1.331, val_acc:0.688]
Epoch [20/120    avg_loss:1.265, val_acc:0.699]
Epoch [21/120    avg_loss:1.123, val_acc:0.708]
Epoch [22/120    avg_loss:1.082, val_acc:0.740]
Epoch [23/120    avg_loss:0.999, val_acc:0.725]
Epoch [24/120    avg_loss:0.954, val_acc:0.754]
Epoch [25/120    avg_loss:0.902, val_acc:0.772]
Epoch [26/120    avg_loss:0.785, val_acc:0.789]
Epoch [27/120    avg_loss:0.825, val_acc:0.760]
Epoch [28/120    avg_loss:0.695, val_acc:0.807]
Epoch [29/120    avg_loss:0.626, val_acc:0.818]
Epoch [30/120    avg_loss:0.553, val_acc:0.833]
Epoch [31/120    avg_loss:0.512, val_acc:0.823]
Epoch [32/120    avg_loss:0.519, val_acc:0.833]
Epoch [33/120    avg_loss:0.423, val_acc:0.843]
Epoch [34/120    avg_loss:0.458, val_acc:0.859]
Epoch [35/120    avg_loss:0.445, val_acc:0.853]
Epoch [36/120    avg_loss:0.374, val_acc:0.849]
Epoch [37/120    avg_loss:0.367, val_acc:0.847]
Epoch [38/120    avg_loss:0.336, val_acc:0.893]
Epoch [39/120    avg_loss:0.301, val_acc:0.872]
Epoch [40/120    avg_loss:0.402, val_acc:0.841]
Epoch [41/120    avg_loss:0.358, val_acc:0.885]
Epoch [42/120    avg_loss:0.296, val_acc:0.888]
Epoch [43/120    avg_loss:0.258, val_acc:0.903]
Epoch [44/120    avg_loss:0.232, val_acc:0.873]
Epoch [45/120    avg_loss:0.283, val_acc:0.886]
Epoch [46/120    avg_loss:0.245, val_acc:0.885]
Epoch [47/120    avg_loss:0.227, val_acc:0.839]
Epoch [48/120    avg_loss:0.211, val_acc:0.912]
Epoch [49/120    avg_loss:0.211, val_acc:0.909]
Epoch [50/120    avg_loss:0.213, val_acc:0.893]
Epoch [51/120    avg_loss:0.183, val_acc:0.902]
Epoch [52/120    avg_loss:0.194, val_acc:0.914]
Epoch [53/120    avg_loss:0.144, val_acc:0.921]
Epoch [54/120    avg_loss:0.146, val_acc:0.920]
Epoch [55/120    avg_loss:0.141, val_acc:0.906]
Epoch [56/120    avg_loss:0.156, val_acc:0.906]
Epoch [57/120    avg_loss:0.132, val_acc:0.915]
Epoch [58/120    avg_loss:0.108, val_acc:0.921]
Epoch [59/120    avg_loss:0.097, val_acc:0.917]
Epoch [60/120    avg_loss:0.151, val_acc:0.931]
Epoch [61/120    avg_loss:0.102, val_acc:0.925]
Epoch [62/120    avg_loss:0.131, val_acc:0.925]
Epoch [63/120    avg_loss:0.106, val_acc:0.926]
Epoch [64/120    avg_loss:0.082, val_acc:0.929]
Epoch [65/120    avg_loss:0.088, val_acc:0.928]
Epoch [66/120    avg_loss:0.096, val_acc:0.938]
Epoch [67/120    avg_loss:0.107, val_acc:0.936]
Epoch [68/120    avg_loss:0.120, val_acc:0.921]
Epoch [69/120    avg_loss:0.099, val_acc:0.929]
Epoch [70/120    avg_loss:0.116, val_acc:0.930]
Epoch [71/120    avg_loss:0.128, val_acc:0.929]
Epoch [72/120    avg_loss:0.123, val_acc:0.938]
Epoch [73/120    avg_loss:0.094, val_acc:0.938]
Epoch [74/120    avg_loss:0.089, val_acc:0.932]
Epoch [75/120    avg_loss:0.088, val_acc:0.942]
Epoch [76/120    avg_loss:0.112, val_acc:0.929]
Epoch [77/120    avg_loss:0.087, val_acc:0.928]
Epoch [78/120    avg_loss:0.076, val_acc:0.933]
Epoch [79/120    avg_loss:0.081, val_acc:0.929]
Epoch [80/120    avg_loss:0.065, val_acc:0.948]
Epoch [81/120    avg_loss:0.055, val_acc:0.945]
Epoch [82/120    avg_loss:0.073, val_acc:0.941]
Epoch [83/120    avg_loss:0.065, val_acc:0.939]
Epoch [84/120    avg_loss:0.058, val_acc:0.928]
Epoch [85/120    avg_loss:0.052, val_acc:0.960]
Epoch [86/120    avg_loss:0.040, val_acc:0.945]
Epoch [87/120    avg_loss:0.052, val_acc:0.941]
Epoch [88/120    avg_loss:0.049, val_acc:0.952]
Epoch [89/120    avg_loss:0.046, val_acc:0.956]
Epoch [90/120    avg_loss:0.045, val_acc:0.946]
Epoch [91/120    avg_loss:0.056, val_acc:0.952]
Epoch [92/120    avg_loss:0.045, val_acc:0.950]
Epoch [93/120    avg_loss:0.040, val_acc:0.956]
Epoch [94/120    avg_loss:0.035, val_acc:0.950]
Epoch [95/120    avg_loss:0.039, val_acc:0.960]
Epoch [96/120    avg_loss:0.033, val_acc:0.960]
Epoch [97/120    avg_loss:0.032, val_acc:0.957]
Epoch [98/120    avg_loss:0.041, val_acc:0.960]
Epoch [99/120    avg_loss:0.031, val_acc:0.959]
Epoch [100/120    avg_loss:0.035, val_acc:0.958]
Epoch [101/120    avg_loss:0.042, val_acc:0.954]
Epoch [102/120    avg_loss:0.119, val_acc:0.931]
Epoch [103/120    avg_loss:0.108, val_acc:0.944]
Epoch [104/120    avg_loss:0.064, val_acc:0.949]
Epoch [105/120    avg_loss:0.048, val_acc:0.939]
Epoch [106/120    avg_loss:0.047, val_acc:0.956]
Epoch [107/120    avg_loss:0.033, val_acc:0.959]
Epoch [108/120    avg_loss:0.028, val_acc:0.963]
Epoch [109/120    avg_loss:0.029, val_acc:0.954]
Epoch [110/120    avg_loss:0.026, val_acc:0.953]
Epoch [111/120    avg_loss:0.028, val_acc:0.959]
Epoch [112/120    avg_loss:0.030, val_acc:0.954]
Epoch [113/120    avg_loss:0.027, val_acc:0.964]
Epoch [114/120    avg_loss:0.031, val_acc:0.957]
Epoch [115/120    avg_loss:0.028, val_acc:0.949]
Epoch [116/120    avg_loss:0.034, val_acc:0.962]
Epoch [117/120    avg_loss:0.026, val_acc:0.953]
Epoch [118/120    avg_loss:0.031, val_acc:0.968]
Epoch [119/120    avg_loss:0.035, val_acc:0.960]
Epoch [120/120    avg_loss:0.031, val_acc:0.955]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    0    0    0    0    0    0    0    0    6    0    0    0
     0    0    0]
 [   0    0 1232    0    0    0    0    0    0    0    6   47    0    0
     0    0    0]
 [   0    0   25  578    5    5    0    0    0    3    5    0  126    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    3    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   14    0    0    4    0
     0    0    0]
 [   0    0   45   52    0    3    0    0    0    0  767    0    8    0
     0    0    0]
 [   0    0   17    0    0    2    3    0    0    0   28 2153    7    0
     0    0    0]
 [   0    0    0    7   14    1    0    0    0    0   10    0  496    0
     0    1    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    0    0    2    1    0    0
  1126    0    0]
 [   0    0    0    0    0    0   24    0    0    0    0    0    0    0
    52  271    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
94.17886178861788

F1 scores:
[       nan 0.92105263 0.94623656 0.83526012 0.95730337 0.96949153
 0.97834205 1.         1.         0.71794872 0.90288405 0.97597461
 0.84067797 1.         0.97068966 0.87560582 0.94047619]

Kappa:
0.9336269446608837
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f48ef6f5ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.783, val_acc:0.141]
Epoch [2/120    avg_loss:2.676, val_acc:0.241]
Epoch [3/120    avg_loss:2.564, val_acc:0.388]
Epoch [4/120    avg_loss:2.433, val_acc:0.438]
Epoch [5/120    avg_loss:2.380, val_acc:0.473]
Epoch [6/120    avg_loss:2.336, val_acc:0.483]
Epoch [7/120    avg_loss:2.239, val_acc:0.496]
Epoch [8/120    avg_loss:2.159, val_acc:0.526]
Epoch [9/120    avg_loss:2.113, val_acc:0.539]
Epoch [10/120    avg_loss:2.042, val_acc:0.557]
Epoch [11/120    avg_loss:1.999, val_acc:0.573]
Epoch [12/120    avg_loss:1.891, val_acc:0.577]
Epoch [13/120    avg_loss:1.782, val_acc:0.592]
Epoch [14/120    avg_loss:1.748, val_acc:0.613]
Epoch [15/120    avg_loss:1.686, val_acc:0.646]
Epoch [16/120    avg_loss:1.584, val_acc:0.675]
Epoch [17/120    avg_loss:1.490, val_acc:0.666]
Epoch [18/120    avg_loss:1.429, val_acc:0.709]
Epoch [19/120    avg_loss:1.320, val_acc:0.720]
Epoch [20/120    avg_loss:1.268, val_acc:0.696]
Epoch [21/120    avg_loss:1.188, val_acc:0.744]
Epoch [22/120    avg_loss:1.141, val_acc:0.737]
Epoch [23/120    avg_loss:1.020, val_acc:0.753]
Epoch [24/120    avg_loss:0.991, val_acc:0.772]
Epoch [25/120    avg_loss:0.836, val_acc:0.750]
Epoch [26/120    avg_loss:0.783, val_acc:0.809]
Epoch [27/120    avg_loss:0.766, val_acc:0.806]
Epoch [28/120    avg_loss:0.740, val_acc:0.834]
Epoch [29/120    avg_loss:0.676, val_acc:0.797]
Epoch [30/120    avg_loss:0.640, val_acc:0.844]
Epoch [31/120    avg_loss:0.567, val_acc:0.850]
Epoch [32/120    avg_loss:0.558, val_acc:0.816]
Epoch [33/120    avg_loss:0.553, val_acc:0.852]
Epoch [34/120    avg_loss:0.481, val_acc:0.858]
Epoch [35/120    avg_loss:0.404, val_acc:0.849]
Epoch [36/120    avg_loss:0.392, val_acc:0.895]
Epoch [37/120    avg_loss:0.387, val_acc:0.890]
Epoch [38/120    avg_loss:0.321, val_acc:0.873]
Epoch [39/120    avg_loss:0.329, val_acc:0.901]
Epoch [40/120    avg_loss:0.289, val_acc:0.916]
Epoch [41/120    avg_loss:0.278, val_acc:0.911]
Epoch [42/120    avg_loss:0.219, val_acc:0.914]
Epoch [43/120    avg_loss:0.229, val_acc:0.935]
Epoch [44/120    avg_loss:0.325, val_acc:0.890]
Epoch [45/120    avg_loss:0.262, val_acc:0.914]
Epoch [46/120    avg_loss:0.205, val_acc:0.927]
Epoch [47/120    avg_loss:0.183, val_acc:0.912]
Epoch [48/120    avg_loss:0.202, val_acc:0.920]
Epoch [49/120    avg_loss:0.169, val_acc:0.946]
Epoch [50/120    avg_loss:0.170, val_acc:0.919]
Epoch [51/120    avg_loss:0.170, val_acc:0.939]
Epoch [52/120    avg_loss:0.143, val_acc:0.927]
Epoch [53/120    avg_loss:0.162, val_acc:0.935]
Epoch [54/120    avg_loss:0.149, val_acc:0.949]
Epoch [55/120    avg_loss:0.115, val_acc:0.951]
Epoch [56/120    avg_loss:0.107, val_acc:0.936]
Epoch [57/120    avg_loss:0.112, val_acc:0.942]
Epoch [58/120    avg_loss:0.097, val_acc:0.959]
Epoch [59/120    avg_loss:0.092, val_acc:0.952]
Epoch [60/120    avg_loss:0.090, val_acc:0.943]
Epoch [61/120    avg_loss:0.106, val_acc:0.945]
Epoch [62/120    avg_loss:0.104, val_acc:0.945]
Epoch [63/120    avg_loss:0.104, val_acc:0.960]
Epoch [64/120    avg_loss:0.102, val_acc:0.945]
Epoch [65/120    avg_loss:0.113, val_acc:0.950]
Epoch [66/120    avg_loss:0.107, val_acc:0.950]
Epoch [67/120    avg_loss:0.128, val_acc:0.952]
Epoch [68/120    avg_loss:0.086, val_acc:0.958]
Epoch [69/120    avg_loss:0.072, val_acc:0.958]
Epoch [70/120    avg_loss:0.073, val_acc:0.952]
Epoch [71/120    avg_loss:0.077, val_acc:0.967]
Epoch [72/120    avg_loss:0.067, val_acc:0.959]
Epoch [73/120    avg_loss:0.069, val_acc:0.960]
Epoch [74/120    avg_loss:0.101, val_acc:0.933]
Epoch [75/120    avg_loss:0.104, val_acc:0.955]
Epoch [76/120    avg_loss:0.067, val_acc:0.954]
Epoch [77/120    avg_loss:0.060, val_acc:0.961]
Epoch [78/120    avg_loss:0.057, val_acc:0.956]
Epoch [79/120    avg_loss:0.057, val_acc:0.965]
Epoch [80/120    avg_loss:0.049, val_acc:0.969]
Epoch [81/120    avg_loss:0.068, val_acc:0.943]
Epoch [82/120    avg_loss:0.087, val_acc:0.959]
Epoch [83/120    avg_loss:0.141, val_acc:0.956]
Epoch [84/120    avg_loss:0.070, val_acc:0.962]
Epoch [85/120    avg_loss:0.066, val_acc:0.955]
Epoch [86/120    avg_loss:0.062, val_acc:0.953]
Epoch [87/120    avg_loss:0.065, val_acc:0.965]
Epoch [88/120    avg_loss:0.060, val_acc:0.970]
Epoch [89/120    avg_loss:0.039, val_acc:0.975]
Epoch [90/120    avg_loss:0.037, val_acc:0.958]
Epoch [91/120    avg_loss:0.044, val_acc:0.970]
Epoch [92/120    avg_loss:0.039, val_acc:0.971]
Epoch [93/120    avg_loss:0.029, val_acc:0.974]
Epoch [94/120    avg_loss:0.031, val_acc:0.973]
Epoch [95/120    avg_loss:0.030, val_acc:0.971]
Epoch [96/120    avg_loss:0.032, val_acc:0.972]
Epoch [97/120    avg_loss:0.034, val_acc:0.956]
Epoch [98/120    avg_loss:0.027, val_acc:0.972]
Epoch [99/120    avg_loss:0.020, val_acc:0.975]
Epoch [100/120    avg_loss:0.034, val_acc:0.968]
Epoch [101/120    avg_loss:0.035, val_acc:0.975]
Epoch [102/120    avg_loss:0.028, val_acc:0.977]
Epoch [103/120    avg_loss:0.029, val_acc:0.982]
Epoch [104/120    avg_loss:0.022, val_acc:0.979]
Epoch [105/120    avg_loss:0.023, val_acc:0.981]
Epoch [106/120    avg_loss:0.030, val_acc:0.955]
Epoch [107/120    avg_loss:0.038, val_acc:0.975]
Epoch [108/120    avg_loss:0.026, val_acc:0.972]
Epoch [109/120    avg_loss:0.026, val_acc:0.965]
Epoch [110/120    avg_loss:0.039, val_acc:0.977]
Epoch [111/120    avg_loss:0.024, val_acc:0.971]
Epoch [112/120    avg_loss:0.037, val_acc:0.964]
Epoch [113/120    avg_loss:0.029, val_acc:0.975]
Epoch [114/120    avg_loss:0.025, val_acc:0.970]
Epoch [115/120    avg_loss:0.026, val_acc:0.977]
Epoch [116/120    avg_loss:0.030, val_acc:0.972]
Epoch [117/120    avg_loss:0.020, val_acc:0.975]
Epoch [118/120    avg_loss:0.016, val_acc:0.975]
Epoch [119/120    avg_loss:0.013, val_acc:0.975]
Epoch [120/120    avg_loss:0.015, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1240    0    0    0    3    0    0    1    4   35    2    0
     0    0    0]
 [   0    0    2  718    0   12    0    0    0   10    0    1    0    4
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    1    0    2    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0   10    0    3    0    0
     0    0    0]
 [   0    0   10   28    0    6    0    0    0    0  810    9    0    0
     0   12    0]
 [   0    0   11    0    0    0    8    0    0    0   12 2171    5    3
     0    0    0]
 [   0    0    1    9    9   13    0    0    0    0    2    2  495    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    0    0    0    0    2    2    0    0
  1126    0    0]
 [   0    0    0    0    0    2   32    0    0    1    0    1    0    0
    32  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.58536585365853

F1 scores:
[       nan 1.         0.97293056 0.95288653 0.97695853 0.94701987
 0.96755162 1.         1.         0.48780488 0.95014663 0.97858914
 0.95375723 0.98143236 0.97913043 0.87323944 0.98224852]

Kappa:
0.9610610450654711
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2baae61a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.787, val_acc:0.203]
Epoch [2/120    avg_loss:2.644, val_acc:0.298]
Epoch [3/120    avg_loss:2.517, val_acc:0.386]
Epoch [4/120    avg_loss:2.420, val_acc:0.454]
Epoch [5/120    avg_loss:2.349, val_acc:0.478]
Epoch [6/120    avg_loss:2.254, val_acc:0.496]
Epoch [7/120    avg_loss:2.224, val_acc:0.512]
Epoch [8/120    avg_loss:2.146, val_acc:0.560]
Epoch [9/120    avg_loss:2.066, val_acc:0.583]
Epoch [10/120    avg_loss:2.020, val_acc:0.603]
Epoch [11/120    avg_loss:1.937, val_acc:0.602]
Epoch [12/120    avg_loss:1.814, val_acc:0.580]
Epoch [13/120    avg_loss:1.722, val_acc:0.622]
Epoch [14/120    avg_loss:1.631, val_acc:0.625]
Epoch [15/120    avg_loss:1.528, val_acc:0.651]
Epoch [16/120    avg_loss:1.409, val_acc:0.644]
Epoch [17/120    avg_loss:1.355, val_acc:0.681]
Epoch [18/120    avg_loss:1.327, val_acc:0.665]
Epoch [19/120    avg_loss:1.176, val_acc:0.700]
Epoch [20/120    avg_loss:1.050, val_acc:0.718]
Epoch [21/120    avg_loss:1.001, val_acc:0.732]
Epoch [22/120    avg_loss:0.938, val_acc:0.756]
Epoch [23/120    avg_loss:0.874, val_acc:0.752]
Epoch [24/120    avg_loss:0.830, val_acc:0.773]
Epoch [25/120    avg_loss:0.762, val_acc:0.772]
Epoch [26/120    avg_loss:0.695, val_acc:0.778]
Epoch [27/120    avg_loss:0.652, val_acc:0.786]
Epoch [28/120    avg_loss:0.621, val_acc:0.811]
Epoch [29/120    avg_loss:0.559, val_acc:0.828]
Epoch [30/120    avg_loss:0.563, val_acc:0.826]
Epoch [31/120    avg_loss:0.480, val_acc:0.835]
Epoch [32/120    avg_loss:0.476, val_acc:0.830]
Epoch [33/120    avg_loss:0.455, val_acc:0.836]
Epoch [34/120    avg_loss:0.399, val_acc:0.852]
Epoch [35/120    avg_loss:0.352, val_acc:0.877]
Epoch [36/120    avg_loss:0.356, val_acc:0.849]
Epoch [37/120    avg_loss:0.344, val_acc:0.875]
Epoch [38/120    avg_loss:0.297, val_acc:0.874]
Epoch [39/120    avg_loss:0.352, val_acc:0.881]
Epoch [40/120    avg_loss:0.283, val_acc:0.883]
Epoch [41/120    avg_loss:0.266, val_acc:0.892]
Epoch [42/120    avg_loss:0.235, val_acc:0.890]
Epoch [43/120    avg_loss:0.263, val_acc:0.898]
Epoch [44/120    avg_loss:0.255, val_acc:0.892]
Epoch [45/120    avg_loss:0.239, val_acc:0.887]
Epoch [46/120    avg_loss:0.217, val_acc:0.902]
Epoch [47/120    avg_loss:0.172, val_acc:0.906]
Epoch [48/120    avg_loss:0.175, val_acc:0.895]
Epoch [49/120    avg_loss:0.202, val_acc:0.891]
Epoch [50/120    avg_loss:0.181, val_acc:0.914]
Epoch [51/120    avg_loss:0.430, val_acc:0.780]
Epoch [52/120    avg_loss:0.366, val_acc:0.864]
Epoch [53/120    avg_loss:0.339, val_acc:0.879]
Epoch [54/120    avg_loss:0.251, val_acc:0.888]
Epoch [55/120    avg_loss:0.236, val_acc:0.888]
Epoch [56/120    avg_loss:0.218, val_acc:0.897]
Epoch [57/120    avg_loss:0.208, val_acc:0.917]
Epoch [58/120    avg_loss:0.175, val_acc:0.911]
Epoch [59/120    avg_loss:0.160, val_acc:0.910]
Epoch [60/120    avg_loss:0.180, val_acc:0.913]
Epoch [61/120    avg_loss:0.141, val_acc:0.904]
Epoch [62/120    avg_loss:0.188, val_acc:0.906]
Epoch [63/120    avg_loss:0.151, val_acc:0.922]
Epoch [64/120    avg_loss:0.165, val_acc:0.888]
Epoch [65/120    avg_loss:0.136, val_acc:0.931]
Epoch [66/120    avg_loss:0.134, val_acc:0.911]
Epoch [67/120    avg_loss:0.140, val_acc:0.920]
Epoch [68/120    avg_loss:0.108, val_acc:0.923]
Epoch [69/120    avg_loss:0.103, val_acc:0.920]
Epoch [70/120    avg_loss:0.086, val_acc:0.934]
Epoch [71/120    avg_loss:0.075, val_acc:0.950]
Epoch [72/120    avg_loss:0.074, val_acc:0.938]
Epoch [73/120    avg_loss:0.074, val_acc:0.935]
Epoch [74/120    avg_loss:0.079, val_acc:0.930]
Epoch [75/120    avg_loss:0.094, val_acc:0.933]
Epoch [76/120    avg_loss:0.079, val_acc:0.929]
Epoch [77/120    avg_loss:0.077, val_acc:0.936]
Epoch [78/120    avg_loss:0.079, val_acc:0.931]
Epoch [79/120    avg_loss:0.084, val_acc:0.930]
Epoch [80/120    avg_loss:0.083, val_acc:0.936]
Epoch [81/120    avg_loss:0.071, val_acc:0.939]
Epoch [82/120    avg_loss:0.060, val_acc:0.946]
Epoch [83/120    avg_loss:0.057, val_acc:0.933]
Epoch [84/120    avg_loss:0.053, val_acc:0.941]
Epoch [85/120    avg_loss:0.045, val_acc:0.946]
Epoch [86/120    avg_loss:0.042, val_acc:0.945]
Epoch [87/120    avg_loss:0.035, val_acc:0.948]
Epoch [88/120    avg_loss:0.034, val_acc:0.948]
Epoch [89/120    avg_loss:0.038, val_acc:0.945]
Epoch [90/120    avg_loss:0.035, val_acc:0.945]
Epoch [91/120    avg_loss:0.038, val_acc:0.948]
Epoch [92/120    avg_loss:0.036, val_acc:0.946]
Epoch [93/120    avg_loss:0.043, val_acc:0.945]
Epoch [94/120    avg_loss:0.037, val_acc:0.946]
Epoch [95/120    avg_loss:0.039, val_acc:0.945]
Epoch [96/120    avg_loss:0.038, val_acc:0.946]
Epoch [97/120    avg_loss:0.031, val_acc:0.948]
Epoch [98/120    avg_loss:0.033, val_acc:0.948]
Epoch [99/120    avg_loss:0.033, val_acc:0.948]
Epoch [100/120    avg_loss:0.037, val_acc:0.949]
Epoch [101/120    avg_loss:0.036, val_acc:0.949]
Epoch [102/120    avg_loss:0.030, val_acc:0.949]
Epoch [103/120    avg_loss:0.029, val_acc:0.949]
Epoch [104/120    avg_loss:0.034, val_acc:0.949]
Epoch [105/120    avg_loss:0.031, val_acc:0.949]
Epoch [106/120    avg_loss:0.034, val_acc:0.949]
Epoch [107/120    avg_loss:0.033, val_acc:0.949]
Epoch [108/120    avg_loss:0.035, val_acc:0.949]
Epoch [109/120    avg_loss:0.034, val_acc:0.949]
Epoch [110/120    avg_loss:0.033, val_acc:0.949]
Epoch [111/120    avg_loss:0.036, val_acc:0.949]
Epoch [112/120    avg_loss:0.038, val_acc:0.949]
Epoch [113/120    avg_loss:0.039, val_acc:0.949]
Epoch [114/120    avg_loss:0.032, val_acc:0.949]
Epoch [115/120    avg_loss:0.030, val_acc:0.949]
Epoch [116/120    avg_loss:0.035, val_acc:0.949]
Epoch [117/120    avg_loss:0.030, val_acc:0.949]
Epoch [118/120    avg_loss:0.035, val_acc:0.949]
Epoch [119/120    avg_loss:0.037, val_acc:0.949]
Epoch [120/120    avg_loss:0.033, val_acc:0.949]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1239    0    0    0    7    0    0    1    0   36    0    0
     0    2    0]
 [   0    0    2  702    1   13    1    0    0   12    0    0   11    4
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  421    0    8    0    5    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   34   63    0    8    0    0    0    0  745   16    0    0
     2    7    0]
 [   0    0   19    0    0    0   15    0    0    0   14 2155    4    3
     0    0    0]
 [   0    0    0   20    9   12    0    0    0    1    8    5  470    0
     0    1    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    0    0    0    0    2    0    0    0
  1127    1    0]
 [   0    0    0    0    0    2   26    0    0    9    0    0    0    0
    66  244    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
94.8509485094851

F1 scores:
[       nan 1.         0.96083753 0.91644909 0.97706422 0.93555556
 0.96176471 0.86206897 1.         0.5625     0.90632603 0.97379123
 0.91976517 0.98143236 0.96531049 0.81063123 0.93641618]

Kappa:
0.9412829877718938
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdfd14ecac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.790, val_acc:0.163]
Epoch [2/120    avg_loss:2.663, val_acc:0.289]
Epoch [3/120    avg_loss:2.530, val_acc:0.311]
Epoch [4/120    avg_loss:2.430, val_acc:0.326]
Epoch [5/120    avg_loss:2.379, val_acc:0.377]
Epoch [6/120    avg_loss:2.271, val_acc:0.427]
Epoch [7/120    avg_loss:2.228, val_acc:0.520]
Epoch [8/120    avg_loss:2.171, val_acc:0.579]
Epoch [9/120    avg_loss:2.077, val_acc:0.577]
Epoch [10/120    avg_loss:1.989, val_acc:0.619]
Epoch [11/120    avg_loss:1.898, val_acc:0.627]
Epoch [12/120    avg_loss:1.764, val_acc:0.621]
Epoch [13/120    avg_loss:1.716, val_acc:0.623]
Epoch [14/120    avg_loss:1.603, val_acc:0.642]
Epoch [15/120    avg_loss:1.507, val_acc:0.653]
Epoch [16/120    avg_loss:1.405, val_acc:0.654]
Epoch [17/120    avg_loss:1.345, val_acc:0.663]
Epoch [18/120    avg_loss:1.232, val_acc:0.708]
Epoch [19/120    avg_loss:1.176, val_acc:0.700]
Epoch [20/120    avg_loss:1.078, val_acc:0.706]
Epoch [21/120    avg_loss:1.015, val_acc:0.741]
Epoch [22/120    avg_loss:1.007, val_acc:0.696]
Epoch [23/120    avg_loss:0.936, val_acc:0.733]
Epoch [24/120    avg_loss:0.904, val_acc:0.731]
Epoch [25/120    avg_loss:0.775, val_acc:0.741]
Epoch [26/120    avg_loss:0.817, val_acc:0.773]
Epoch [27/120    avg_loss:0.801, val_acc:0.758]
Epoch [28/120    avg_loss:0.731, val_acc:0.739]
Epoch [29/120    avg_loss:0.730, val_acc:0.791]
Epoch [30/120    avg_loss:0.667, val_acc:0.821]
Epoch [31/120    avg_loss:0.621, val_acc:0.858]
Epoch [32/120    avg_loss:0.597, val_acc:0.776]
Epoch [33/120    avg_loss:0.602, val_acc:0.807]
Epoch [34/120    avg_loss:0.508, val_acc:0.842]
Epoch [35/120    avg_loss:0.474, val_acc:0.862]
Epoch [36/120    avg_loss:0.448, val_acc:0.860]
Epoch [37/120    avg_loss:0.365, val_acc:0.883]
Epoch [38/120    avg_loss:0.375, val_acc:0.845]
Epoch [39/120    avg_loss:0.385, val_acc:0.882]
Epoch [40/120    avg_loss:0.365, val_acc:0.888]
Epoch [41/120    avg_loss:0.379, val_acc:0.881]
Epoch [42/120    avg_loss:0.317, val_acc:0.877]
Epoch [43/120    avg_loss:0.303, val_acc:0.873]
Epoch [44/120    avg_loss:0.301, val_acc:0.893]
Epoch [45/120    avg_loss:0.312, val_acc:0.895]
Epoch [46/120    avg_loss:0.293, val_acc:0.873]
Epoch [47/120    avg_loss:0.299, val_acc:0.906]
Epoch [48/120    avg_loss:0.246, val_acc:0.911]
Epoch [49/120    avg_loss:0.280, val_acc:0.916]
Epoch [50/120    avg_loss:0.247, val_acc:0.893]
Epoch [51/120    avg_loss:0.304, val_acc:0.888]
Epoch [52/120    avg_loss:0.281, val_acc:0.892]
Epoch [53/120    avg_loss:0.197, val_acc:0.911]
Epoch [54/120    avg_loss:0.184, val_acc:0.924]
Epoch [55/120    avg_loss:0.167, val_acc:0.917]
Epoch [56/120    avg_loss:0.161, val_acc:0.916]
Epoch [57/120    avg_loss:0.141, val_acc:0.930]
Epoch [58/120    avg_loss:0.145, val_acc:0.933]
Epoch [59/120    avg_loss:0.173, val_acc:0.919]
Epoch [60/120    avg_loss:0.192, val_acc:0.925]
Epoch [61/120    avg_loss:0.186, val_acc:0.919]
Epoch [62/120    avg_loss:0.146, val_acc:0.924]
Epoch [63/120    avg_loss:0.141, val_acc:0.930]
Epoch [64/120    avg_loss:0.121, val_acc:0.931]
Epoch [65/120    avg_loss:0.117, val_acc:0.927]
Epoch [66/120    avg_loss:0.116, val_acc:0.942]
Epoch [67/120    avg_loss:0.119, val_acc:0.939]
Epoch [68/120    avg_loss:0.141, val_acc:0.929]
Epoch [69/120    avg_loss:0.168, val_acc:0.913]
Epoch [70/120    avg_loss:0.199, val_acc:0.892]
Epoch [71/120    avg_loss:0.134, val_acc:0.936]
Epoch [72/120    avg_loss:0.110, val_acc:0.945]
Epoch [73/120    avg_loss:0.109, val_acc:0.952]
Epoch [74/120    avg_loss:0.083, val_acc:0.946]
Epoch [75/120    avg_loss:0.082, val_acc:0.959]
Epoch [76/120    avg_loss:0.104, val_acc:0.925]
Epoch [77/120    avg_loss:0.098, val_acc:0.955]
Epoch [78/120    avg_loss:0.068, val_acc:0.954]
Epoch [79/120    avg_loss:0.077, val_acc:0.943]
Epoch [80/120    avg_loss:0.094, val_acc:0.949]
Epoch [81/120    avg_loss:0.057, val_acc:0.964]
Epoch [82/120    avg_loss:0.064, val_acc:0.965]
Epoch [83/120    avg_loss:0.065, val_acc:0.952]
Epoch [84/120    avg_loss:0.071, val_acc:0.960]
Epoch [85/120    avg_loss:0.061, val_acc:0.961]
Epoch [86/120    avg_loss:0.056, val_acc:0.951]
Epoch [87/120    avg_loss:0.059, val_acc:0.962]
Epoch [88/120    avg_loss:0.072, val_acc:0.943]
Epoch [89/120    avg_loss:0.066, val_acc:0.963]
Epoch [90/120    avg_loss:0.079, val_acc:0.946]
Epoch [91/120    avg_loss:0.055, val_acc:0.963]
Epoch [92/120    avg_loss:0.051, val_acc:0.965]
Epoch [93/120    avg_loss:0.062, val_acc:0.955]
Epoch [94/120    avg_loss:0.054, val_acc:0.964]
Epoch [95/120    avg_loss:0.043, val_acc:0.963]
Epoch [96/120    avg_loss:0.044, val_acc:0.958]
Epoch [97/120    avg_loss:0.056, val_acc:0.969]
Epoch [98/120    avg_loss:0.045, val_acc:0.967]
Epoch [99/120    avg_loss:0.045, val_acc:0.964]
Epoch [100/120    avg_loss:0.047, val_acc:0.968]
Epoch [101/120    avg_loss:0.050, val_acc:0.953]
Epoch [102/120    avg_loss:0.051, val_acc:0.955]
Epoch [103/120    avg_loss:0.053, val_acc:0.961]
Epoch [104/120    avg_loss:0.059, val_acc:0.967]
Epoch [105/120    avg_loss:0.049, val_acc:0.967]
Epoch [106/120    avg_loss:0.069, val_acc:0.951]
Epoch [107/120    avg_loss:0.057, val_acc:0.964]
Epoch [108/120    avg_loss:0.041, val_acc:0.969]
Epoch [109/120    avg_loss:0.034, val_acc:0.972]
Epoch [110/120    avg_loss:0.038, val_acc:0.962]
Epoch [111/120    avg_loss:0.044, val_acc:0.972]
Epoch [112/120    avg_loss:0.106, val_acc:0.952]
Epoch [113/120    avg_loss:0.075, val_acc:0.969]
Epoch [114/120    avg_loss:0.049, val_acc:0.971]
Epoch [115/120    avg_loss:0.040, val_acc:0.962]
Epoch [116/120    avg_loss:0.031, val_acc:0.969]
Epoch [117/120    avg_loss:0.044, val_acc:0.979]
Epoch [118/120    avg_loss:0.037, val_acc:0.968]
Epoch [119/120    avg_loss:0.032, val_acc:0.979]
Epoch [120/120    avg_loss:0.034, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1242    2    0    0    2    0    0    1    5   12    2    0
     0   19    0]
 [   0    0    8  705    2    6    0    0    0   22    0    0    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    7    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  648    0    0    0    0    4    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   20   90    0    2    0    0    0    0  756    4    0    0
     0    3    0]
 [   0    0   34    0    0    1    2    0    0    0   10 2147   12    4
     0    0    0]
 [   0    0    0    8    3    4    0    0    0    0    7    7  500    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0    2    0    0    2    0    0    2    0
   130  211    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.93766937669376

F1 scores:
[       nan 0.96202532 0.9594438  0.90850515 0.98839907 0.97011494
 0.98855835 0.87719298 1.         0.51515152 0.91139241 0.9794708
 0.9469697  0.98930481 0.94273859 0.72758621 0.96511628]

Kappa:
0.942274239593592
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9f7d54dac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.810, val_acc:0.145]
Epoch [2/120    avg_loss:2.701, val_acc:0.393]
Epoch [3/120    avg_loss:2.603, val_acc:0.450]
Epoch [4/120    avg_loss:2.516, val_acc:0.463]
Epoch [5/120    avg_loss:2.425, val_acc:0.456]
Epoch [6/120    avg_loss:2.355, val_acc:0.445]
Epoch [7/120    avg_loss:2.244, val_acc:0.462]
Epoch [8/120    avg_loss:2.205, val_acc:0.482]
Epoch [9/120    avg_loss:2.088, val_acc:0.500]
Epoch [10/120    avg_loss:2.034, val_acc:0.509]
Epoch [11/120    avg_loss:1.989, val_acc:0.512]
Epoch [12/120    avg_loss:1.907, val_acc:0.547]
Epoch [13/120    avg_loss:1.864, val_acc:0.567]
Epoch [14/120    avg_loss:1.771, val_acc:0.579]
Epoch [15/120    avg_loss:1.717, val_acc:0.566]
Epoch [16/120    avg_loss:1.688, val_acc:0.593]
Epoch [17/120    avg_loss:1.579, val_acc:0.637]
Epoch [18/120    avg_loss:1.427, val_acc:0.661]
Epoch [19/120    avg_loss:1.362, val_acc:0.636]
Epoch [20/120    avg_loss:1.280, val_acc:0.691]
Epoch [21/120    avg_loss:1.192, val_acc:0.700]
Epoch [22/120    avg_loss:1.123, val_acc:0.710]
Epoch [23/120    avg_loss:1.076, val_acc:0.688]
Epoch [24/120    avg_loss:0.994, val_acc:0.734]
Epoch [25/120    avg_loss:0.966, val_acc:0.714]
Epoch [26/120    avg_loss:0.995, val_acc:0.725]
Epoch [27/120    avg_loss:0.828, val_acc:0.768]
Epoch [28/120    avg_loss:0.770, val_acc:0.746]
Epoch [29/120    avg_loss:0.783, val_acc:0.800]
Epoch [30/120    avg_loss:0.728, val_acc:0.810]
Epoch [31/120    avg_loss:0.662, val_acc:0.801]
Epoch [32/120    avg_loss:0.629, val_acc:0.768]
Epoch [33/120    avg_loss:0.603, val_acc:0.821]
Epoch [34/120    avg_loss:0.544, val_acc:0.843]
Epoch [35/120    avg_loss:0.474, val_acc:0.857]
Epoch [36/120    avg_loss:0.424, val_acc:0.844]
Epoch [37/120    avg_loss:0.436, val_acc:0.853]
Epoch [38/120    avg_loss:0.390, val_acc:0.890]
Epoch [39/120    avg_loss:0.407, val_acc:0.830]
Epoch [40/120    avg_loss:0.387, val_acc:0.858]
Epoch [41/120    avg_loss:0.388, val_acc:0.875]
Epoch [42/120    avg_loss:0.355, val_acc:0.882]
Epoch [43/120    avg_loss:0.339, val_acc:0.888]
Epoch [44/120    avg_loss:0.295, val_acc:0.891]
Epoch [45/120    avg_loss:0.293, val_acc:0.887]
Epoch [46/120    avg_loss:0.305, val_acc:0.879]
Epoch [47/120    avg_loss:0.252, val_acc:0.896]
Epoch [48/120    avg_loss:0.238, val_acc:0.904]
Epoch [49/120    avg_loss:0.216, val_acc:0.904]
Epoch [50/120    avg_loss:0.222, val_acc:0.910]
Epoch [51/120    avg_loss:0.234, val_acc:0.898]
Epoch [52/120    avg_loss:0.266, val_acc:0.888]
Epoch [53/120    avg_loss:0.192, val_acc:0.921]
Epoch [54/120    avg_loss:0.167, val_acc:0.914]
Epoch [55/120    avg_loss:0.189, val_acc:0.897]
Epoch [56/120    avg_loss:0.216, val_acc:0.921]
Epoch [57/120    avg_loss:0.163, val_acc:0.922]
Epoch [58/120    avg_loss:0.165, val_acc:0.934]
Epoch [59/120    avg_loss:0.142, val_acc:0.923]
Epoch [60/120    avg_loss:0.171, val_acc:0.924]
Epoch [61/120    avg_loss:0.174, val_acc:0.910]
Epoch [62/120    avg_loss:0.183, val_acc:0.922]
Epoch [63/120    avg_loss:0.162, val_acc:0.941]
Epoch [64/120    avg_loss:0.149, val_acc:0.895]
Epoch [65/120    avg_loss:0.174, val_acc:0.906]
Epoch [66/120    avg_loss:0.151, val_acc:0.936]
Epoch [67/120    avg_loss:0.121, val_acc:0.945]
Epoch [68/120    avg_loss:0.117, val_acc:0.929]
Epoch [69/120    avg_loss:0.141, val_acc:0.934]
Epoch [70/120    avg_loss:0.122, val_acc:0.939]
Epoch [71/120    avg_loss:0.126, val_acc:0.921]
Epoch [72/120    avg_loss:0.108, val_acc:0.935]
Epoch [73/120    avg_loss:0.101, val_acc:0.944]
Epoch [74/120    avg_loss:0.114, val_acc:0.938]
Epoch [75/120    avg_loss:0.118, val_acc:0.933]
Epoch [76/120    avg_loss:0.102, val_acc:0.938]
Epoch [77/120    avg_loss:0.112, val_acc:0.938]
Epoch [78/120    avg_loss:0.100, val_acc:0.940]
Epoch [79/120    avg_loss:0.150, val_acc:0.943]
Epoch [80/120    avg_loss:0.092, val_acc:0.939]
Epoch [81/120    avg_loss:0.083, val_acc:0.948]
Epoch [82/120    avg_loss:0.070, val_acc:0.950]
Epoch [83/120    avg_loss:0.068, val_acc:0.952]
Epoch [84/120    avg_loss:0.064, val_acc:0.953]
Epoch [85/120    avg_loss:0.064, val_acc:0.951]
Epoch [86/120    avg_loss:0.064, val_acc:0.952]
Epoch [87/120    avg_loss:0.059, val_acc:0.950]
Epoch [88/120    avg_loss:0.061, val_acc:0.953]
Epoch [89/120    avg_loss:0.052, val_acc:0.951]
Epoch [90/120    avg_loss:0.047, val_acc:0.954]
Epoch [91/120    avg_loss:0.054, val_acc:0.949]
Epoch [92/120    avg_loss:0.057, val_acc:0.950]
Epoch [93/120    avg_loss:0.057, val_acc:0.958]
Epoch [94/120    avg_loss:0.062, val_acc:0.960]
Epoch [95/120    avg_loss:0.046, val_acc:0.961]
Epoch [96/120    avg_loss:0.054, val_acc:0.960]
Epoch [97/120    avg_loss:0.043, val_acc:0.959]
Epoch [98/120    avg_loss:0.049, val_acc:0.962]
Epoch [99/120    avg_loss:0.050, val_acc:0.963]
Epoch [100/120    avg_loss:0.046, val_acc:0.960]
Epoch [101/120    avg_loss:0.048, val_acc:0.960]
Epoch [102/120    avg_loss:0.045, val_acc:0.961]
Epoch [103/120    avg_loss:0.047, val_acc:0.962]
Epoch [104/120    avg_loss:0.049, val_acc:0.962]
Epoch [105/120    avg_loss:0.046, val_acc:0.961]
Epoch [106/120    avg_loss:0.043, val_acc:0.963]
Epoch [107/120    avg_loss:0.051, val_acc:0.962]
Epoch [108/120    avg_loss:0.043, val_acc:0.962]
Epoch [109/120    avg_loss:0.042, val_acc:0.960]
Epoch [110/120    avg_loss:0.043, val_acc:0.963]
Epoch [111/120    avg_loss:0.044, val_acc:0.961]
Epoch [112/120    avg_loss:0.042, val_acc:0.959]
Epoch [113/120    avg_loss:0.042, val_acc:0.959]
Epoch [114/120    avg_loss:0.047, val_acc:0.962]
Epoch [115/120    avg_loss:0.042, val_acc:0.959]
Epoch [116/120    avg_loss:0.045, val_acc:0.961]
Epoch [117/120    avg_loss:0.046, val_acc:0.960]
Epoch [118/120    avg_loss:0.050, val_acc:0.960]
Epoch [119/120    avg_loss:0.044, val_acc:0.960]
Epoch [120/120    avg_loss:0.047, val_acc:0.959]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    1 1184   18    0    0    5    0    0    0    7   49   10    0
     0   11    0]
 [   0    0    3  715    2    1    0    0    0    8    0    0   17    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  415    0    3    0   10    0    0    0    0
     7    0    0]
 [   0    0    1    0    0    0  653    0    0    1    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    2    0    0   10    0    0    4    0
     0    0    0]
 [   0    0   50   88    0    4    2    0    0    0  717    0    0    0
     2   12    0]
 [   0    0   22    0    0    6   11    0    0    0    6 2142   10    5
     8    0    0]
 [   0    0    0   20   14    8    0    0    0    2    5    0  468    0
     0    2   15]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    4    3    1    2    0
  1123    5    0]
 [   0    0    0    0    0    0    9    0    0    5    0    0    1    0
   111  221    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
93.45257452574526

F1 scores:
[       nan 0.92307692 0.93045187 0.89937107 0.9638009  0.95512083
 0.97535474 0.94339623 0.99883856 0.34482759 0.88627936 0.97275204
 0.89483748 0.98404255 0.93974895 0.73913043 0.91803279]

Kappa:
0.9253596645014776
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6558d50b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.821, val_acc:0.354]
Epoch [2/120    avg_loss:2.694, val_acc:0.362]
Epoch [3/120    avg_loss:2.572, val_acc:0.403]
Epoch [4/120    avg_loss:2.457, val_acc:0.423]
Epoch [5/120    avg_loss:2.365, val_acc:0.452]
Epoch [6/120    avg_loss:2.266, val_acc:0.481]
Epoch [7/120    avg_loss:2.216, val_acc:0.493]
Epoch [8/120    avg_loss:2.196, val_acc:0.472]
Epoch [9/120    avg_loss:2.116, val_acc:0.558]
Epoch [10/120    avg_loss:2.039, val_acc:0.558]
Epoch [11/120    avg_loss:1.935, val_acc:0.579]
Epoch [12/120    avg_loss:1.875, val_acc:0.605]
Epoch [13/120    avg_loss:1.792, val_acc:0.609]
Epoch [14/120    avg_loss:1.704, val_acc:0.621]
Epoch [15/120    avg_loss:1.634, val_acc:0.596]
Epoch [16/120    avg_loss:1.540, val_acc:0.631]
Epoch [17/120    avg_loss:1.437, val_acc:0.632]
Epoch [18/120    avg_loss:1.377, val_acc:0.656]
Epoch [19/120    avg_loss:1.250, val_acc:0.661]
Epoch [20/120    avg_loss:1.226, val_acc:0.638]
Epoch [21/120    avg_loss:1.207, val_acc:0.652]
Epoch [22/120    avg_loss:1.099, val_acc:0.681]
Epoch [23/120    avg_loss:1.047, val_acc:0.705]
Epoch [24/120    avg_loss:1.004, val_acc:0.681]
Epoch [25/120    avg_loss:0.975, val_acc:0.717]
Epoch [26/120    avg_loss:0.933, val_acc:0.712]
Epoch [27/120    avg_loss:0.802, val_acc:0.727]
Epoch [28/120    avg_loss:0.759, val_acc:0.753]
Epoch [29/120    avg_loss:0.741, val_acc:0.743]
Epoch [30/120    avg_loss:0.702, val_acc:0.777]
Epoch [31/120    avg_loss:0.602, val_acc:0.780]
Epoch [32/120    avg_loss:0.607, val_acc:0.788]
Epoch [33/120    avg_loss:0.560, val_acc:0.768]
Epoch [34/120    avg_loss:0.568, val_acc:0.788]
Epoch [35/120    avg_loss:0.526, val_acc:0.810]
Epoch [36/120    avg_loss:0.514, val_acc:0.802]
Epoch [37/120    avg_loss:0.440, val_acc:0.806]
Epoch [38/120    avg_loss:0.593, val_acc:0.771]
Epoch [39/120    avg_loss:0.521, val_acc:0.795]
Epoch [40/120    avg_loss:0.447, val_acc:0.828]
Epoch [41/120    avg_loss:0.440, val_acc:0.812]
Epoch [42/120    avg_loss:0.427, val_acc:0.830]
Epoch [43/120    avg_loss:0.430, val_acc:0.821]
Epoch [44/120    avg_loss:0.345, val_acc:0.855]
Epoch [45/120    avg_loss:0.348, val_acc:0.835]
Epoch [46/120    avg_loss:0.344, val_acc:0.839]
Epoch [47/120    avg_loss:0.332, val_acc:0.815]
Epoch [48/120    avg_loss:0.337, val_acc:0.852]
Epoch [49/120    avg_loss:0.282, val_acc:0.872]
Epoch [50/120    avg_loss:0.306, val_acc:0.888]
Epoch [51/120    avg_loss:0.285, val_acc:0.852]
Epoch [52/120    avg_loss:0.334, val_acc:0.815]
Epoch [53/120    avg_loss:0.348, val_acc:0.844]
Epoch [54/120    avg_loss:0.262, val_acc:0.883]
Epoch [55/120    avg_loss:0.351, val_acc:0.859]
Epoch [56/120    avg_loss:0.238, val_acc:0.867]
Epoch [57/120    avg_loss:0.229, val_acc:0.890]
Epoch [58/120    avg_loss:0.208, val_acc:0.893]
Epoch [59/120    avg_loss:0.187, val_acc:0.915]
Epoch [60/120    avg_loss:0.155, val_acc:0.916]
Epoch [61/120    avg_loss:0.171, val_acc:0.906]
Epoch [62/120    avg_loss:0.149, val_acc:0.912]
Epoch [63/120    avg_loss:0.176, val_acc:0.895]
Epoch [64/120    avg_loss:0.197, val_acc:0.893]
Epoch [65/120    avg_loss:0.186, val_acc:0.906]
Epoch [66/120    avg_loss:0.176, val_acc:0.897]
Epoch [67/120    avg_loss:0.203, val_acc:0.901]
Epoch [68/120    avg_loss:0.153, val_acc:0.914]
Epoch [69/120    avg_loss:0.150, val_acc:0.903]
Epoch [70/120    avg_loss:0.125, val_acc:0.920]
Epoch [71/120    avg_loss:0.117, val_acc:0.926]
Epoch [72/120    avg_loss:0.106, val_acc:0.921]
Epoch [73/120    avg_loss:0.113, val_acc:0.912]
Epoch [74/120    avg_loss:0.106, val_acc:0.917]
Epoch [75/120    avg_loss:0.121, val_acc:0.921]
Epoch [76/120    avg_loss:0.121, val_acc:0.927]
Epoch [77/120    avg_loss:0.113, val_acc:0.916]
Epoch [78/120    avg_loss:0.101, val_acc:0.923]
Epoch [79/120    avg_loss:0.081, val_acc:0.938]
Epoch [80/120    avg_loss:0.085, val_acc:0.930]
Epoch [81/120    avg_loss:0.085, val_acc:0.938]
Epoch [82/120    avg_loss:0.121, val_acc:0.916]
Epoch [83/120    avg_loss:0.121, val_acc:0.921]
Epoch [84/120    avg_loss:0.087, val_acc:0.936]
Epoch [85/120    avg_loss:0.064, val_acc:0.938]
Epoch [86/120    avg_loss:0.082, val_acc:0.930]
Epoch [87/120    avg_loss:0.087, val_acc:0.922]
Epoch [88/120    avg_loss:0.086, val_acc:0.925]
Epoch [89/120    avg_loss:0.085, val_acc:0.931]
Epoch [90/120    avg_loss:0.087, val_acc:0.926]
Epoch [91/120    avg_loss:0.095, val_acc:0.930]
Epoch [92/120    avg_loss:0.073, val_acc:0.938]
Epoch [93/120    avg_loss:0.068, val_acc:0.948]
Epoch [94/120    avg_loss:0.060, val_acc:0.942]
Epoch [95/120    avg_loss:0.063, val_acc:0.941]
Epoch [96/120    avg_loss:0.067, val_acc:0.936]
Epoch [97/120    avg_loss:0.078, val_acc:0.942]
Epoch [98/120    avg_loss:0.063, val_acc:0.945]
Epoch [99/120    avg_loss:0.066, val_acc:0.941]
Epoch [100/120    avg_loss:0.056, val_acc:0.944]
Epoch [101/120    avg_loss:0.049, val_acc:0.945]
Epoch [102/120    avg_loss:0.040, val_acc:0.955]
Epoch [103/120    avg_loss:0.047, val_acc:0.952]
Epoch [104/120    avg_loss:0.070, val_acc:0.951]
Epoch [105/120    avg_loss:0.143, val_acc:0.912]
Epoch [106/120    avg_loss:0.130, val_acc:0.916]
Epoch [107/120    avg_loss:0.110, val_acc:0.924]
Epoch [108/120    avg_loss:0.101, val_acc:0.932]
Epoch [109/120    avg_loss:0.061, val_acc:0.932]
Epoch [110/120    avg_loss:0.052, val_acc:0.955]
Epoch [111/120    avg_loss:0.049, val_acc:0.943]
Epoch [112/120    avg_loss:0.065, val_acc:0.941]
Epoch [113/120    avg_loss:0.058, val_acc:0.952]
Epoch [114/120    avg_loss:0.038, val_acc:0.958]
Epoch [115/120    avg_loss:0.045, val_acc:0.954]
Epoch [116/120    avg_loss:0.053, val_acc:0.954]
Epoch [117/120    avg_loss:0.047, val_acc:0.953]
Epoch [118/120    avg_loss:0.049, val_acc:0.955]
Epoch [119/120    avg_loss:0.040, val_acc:0.960]
Epoch [120/120    avg_loss:0.032, val_acc:0.962]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1218    7    0    0    3    0    0    0   12   45    0    0
     0    0    0]
 [   0    0    4  720    0    1    0    0    0   11    0    0   11    0
     0    0    0]
 [   0    0    0    6  207    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    3    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    2    0    0   10    0    0    2    0
     0    0    0]
 [   0    0   50   81    0    3    9    0    0    0  722    7    0    0
     0    3    0]
 [   0    0   14    0    0    0    5    0    0    0    2 2185    4    0
     0    0    0]
 [   0    0    0    7    0    8    0    0    0    0    9    0  505    0
     0    0    5]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0  183
     0    0    0]
 [   0    0    0    0    0   20    0    0    0    0    1    2    0    0
  1116    0    0]
 [   0    0    1    0    0    0   14    0    0    0    0    0    0    0
    94  238    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.04607046070461

F1 scores:
[       nan 1.         0.94712286 0.91603053 0.98571429 0.95429208
 0.97473997 1.         1.         0.47619048 0.89080814 0.98202247
 0.95643939 0.99456522 0.94857629 0.80952381 0.97109827]

Kappa:
0.9434539313696718
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f05f8771b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.819, val_acc:0.215]
Epoch [2/120    avg_loss:2.723, val_acc:0.257]
Epoch [3/120    avg_loss:2.610, val_acc:0.310]
Epoch [4/120    avg_loss:2.496, val_acc:0.388]
Epoch [5/120    avg_loss:2.389, val_acc:0.413]
Epoch [6/120    avg_loss:2.307, val_acc:0.427]
Epoch [7/120    avg_loss:2.229, val_acc:0.425]
Epoch [8/120    avg_loss:2.184, val_acc:0.461]
Epoch [9/120    avg_loss:2.086, val_acc:0.496]
Epoch [10/120    avg_loss:2.018, val_acc:0.555]
Epoch [11/120    avg_loss:1.947, val_acc:0.594]
Epoch [12/120    avg_loss:1.827, val_acc:0.604]
Epoch [13/120    avg_loss:1.796, val_acc:0.568]
Epoch [14/120    avg_loss:1.715, val_acc:0.595]
Epoch [15/120    avg_loss:1.611, val_acc:0.624]
Epoch [16/120    avg_loss:1.525, val_acc:0.655]
Epoch [17/120    avg_loss:1.428, val_acc:0.662]
Epoch [18/120    avg_loss:1.435, val_acc:0.642]
Epoch [19/120    avg_loss:1.284, val_acc:0.672]
Epoch [20/120    avg_loss:1.239, val_acc:0.706]
Epoch [21/120    avg_loss:1.056, val_acc:0.717]
Epoch [22/120    avg_loss:1.023, val_acc:0.731]
Epoch [23/120    avg_loss:0.987, val_acc:0.718]
Epoch [24/120    avg_loss:0.927, val_acc:0.729]
Epoch [25/120    avg_loss:0.926, val_acc:0.699]
Epoch [26/120    avg_loss:0.874, val_acc:0.711]
Epoch [27/120    avg_loss:0.801, val_acc:0.756]
Epoch [28/120    avg_loss:0.805, val_acc:0.753]
Epoch [29/120    avg_loss:0.741, val_acc:0.782]
Epoch [30/120    avg_loss:0.736, val_acc:0.782]
Epoch [31/120    avg_loss:0.665, val_acc:0.810]
Epoch [32/120    avg_loss:0.673, val_acc:0.772]
Epoch [33/120    avg_loss:0.590, val_acc:0.805]
Epoch [34/120    avg_loss:0.548, val_acc:0.829]
Epoch [35/120    avg_loss:0.508, val_acc:0.837]
Epoch [36/120    avg_loss:0.510, val_acc:0.854]
Epoch [37/120    avg_loss:0.500, val_acc:0.849]
Epoch [38/120    avg_loss:0.515, val_acc:0.833]
Epoch [39/120    avg_loss:0.445, val_acc:0.864]
Epoch [40/120    avg_loss:0.403, val_acc:0.890]
Epoch [41/120    avg_loss:0.366, val_acc:0.884]
Epoch [42/120    avg_loss:0.341, val_acc:0.865]
Epoch [43/120    avg_loss:0.323, val_acc:0.878]
Epoch [44/120    avg_loss:0.289, val_acc:0.902]
Epoch [45/120    avg_loss:0.312, val_acc:0.911]
Epoch [46/120    avg_loss:0.296, val_acc:0.894]
Epoch [47/120    avg_loss:0.306, val_acc:0.897]
Epoch [48/120    avg_loss:0.286, val_acc:0.891]
Epoch [49/120    avg_loss:0.335, val_acc:0.917]
Epoch [50/120    avg_loss:0.277, val_acc:0.902]
Epoch [51/120    avg_loss:0.247, val_acc:0.924]
Epoch [52/120    avg_loss:0.249, val_acc:0.895]
Epoch [53/120    avg_loss:0.236, val_acc:0.896]
Epoch [54/120    avg_loss:0.279, val_acc:0.911]
Epoch [55/120    avg_loss:0.221, val_acc:0.942]
Epoch [56/120    avg_loss:0.229, val_acc:0.916]
Epoch [57/120    avg_loss:0.180, val_acc:0.925]
Epoch [58/120    avg_loss:0.194, val_acc:0.930]
Epoch [59/120    avg_loss:0.214, val_acc:0.932]
Epoch [60/120    avg_loss:0.198, val_acc:0.931]
Epoch [61/120    avg_loss:0.163, val_acc:0.943]
Epoch [62/120    avg_loss:0.159, val_acc:0.940]
Epoch [63/120    avg_loss:0.159, val_acc:0.911]
Epoch [64/120    avg_loss:0.159, val_acc:0.929]
Epoch [65/120    avg_loss:0.172, val_acc:0.946]
Epoch [66/120    avg_loss:0.145, val_acc:0.949]
Epoch [67/120    avg_loss:0.116, val_acc:0.952]
Epoch [68/120    avg_loss:0.138, val_acc:0.950]
Epoch [69/120    avg_loss:0.110, val_acc:0.945]
Epoch [70/120    avg_loss:0.111, val_acc:0.925]
Epoch [71/120    avg_loss:0.126, val_acc:0.939]
Epoch [72/120    avg_loss:0.113, val_acc:0.955]
Epoch [73/120    avg_loss:0.217, val_acc:0.925]
Epoch [74/120    avg_loss:0.216, val_acc:0.929]
Epoch [75/120    avg_loss:0.148, val_acc:0.936]
Epoch [76/120    avg_loss:0.112, val_acc:0.944]
Epoch [77/120    avg_loss:0.135, val_acc:0.934]
Epoch [78/120    avg_loss:0.114, val_acc:0.941]
Epoch [79/120    avg_loss:0.132, val_acc:0.940]
Epoch [80/120    avg_loss:0.106, val_acc:0.953]
Epoch [81/120    avg_loss:0.085, val_acc:0.953]
Epoch [82/120    avg_loss:0.075, val_acc:0.962]
Epoch [83/120    avg_loss:0.089, val_acc:0.951]
Epoch [84/120    avg_loss:0.070, val_acc:0.955]
Epoch [85/120    avg_loss:0.103, val_acc:0.943]
Epoch [86/120    avg_loss:0.084, val_acc:0.949]
Epoch [87/120    avg_loss:0.064, val_acc:0.958]
Epoch [88/120    avg_loss:0.071, val_acc:0.951]
Epoch [89/120    avg_loss:0.066, val_acc:0.959]
Epoch [90/120    avg_loss:0.086, val_acc:0.954]
Epoch [91/120    avg_loss:0.106, val_acc:0.958]
Epoch [92/120    avg_loss:0.111, val_acc:0.932]
Epoch [93/120    avg_loss:0.117, val_acc:0.952]
Epoch [94/120    avg_loss:0.073, val_acc:0.955]
Epoch [95/120    avg_loss:0.066, val_acc:0.962]
Epoch [96/120    avg_loss:0.055, val_acc:0.961]
Epoch [97/120    avg_loss:0.053, val_acc:0.963]
Epoch [98/120    avg_loss:0.057, val_acc:0.962]
Epoch [99/120    avg_loss:0.061, val_acc:0.950]
Epoch [100/120    avg_loss:0.065, val_acc:0.956]
Epoch [101/120    avg_loss:0.055, val_acc:0.961]
Epoch [102/120    avg_loss:0.065, val_acc:0.930]
Epoch [103/120    avg_loss:0.073, val_acc:0.967]
Epoch [104/120    avg_loss:0.042, val_acc:0.967]
Epoch [105/120    avg_loss:0.045, val_acc:0.970]
Epoch [106/120    avg_loss:0.058, val_acc:0.969]
Epoch [107/120    avg_loss:0.061, val_acc:0.965]
Epoch [108/120    avg_loss:0.053, val_acc:0.959]
Epoch [109/120    avg_loss:0.079, val_acc:0.958]
Epoch [110/120    avg_loss:0.046, val_acc:0.975]
Epoch [111/120    avg_loss:0.112, val_acc:0.949]
Epoch [112/120    avg_loss:0.075, val_acc:0.948]
Epoch [113/120    avg_loss:0.059, val_acc:0.965]
Epoch [114/120    avg_loss:0.046, val_acc:0.968]
Epoch [115/120    avg_loss:0.044, val_acc:0.965]
Epoch [116/120    avg_loss:0.046, val_acc:0.962]
Epoch [117/120    avg_loss:0.040, val_acc:0.974]
Epoch [118/120    avg_loss:0.044, val_acc:0.964]
Epoch [119/120    avg_loss:0.042, val_acc:0.967]
Epoch [120/120    avg_loss:0.043, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    1    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    3 1162    4    0    1    7    0    0    4    5   99    0    0
     0    0    0]
 [   0    0    4  715    1   15    0    0    0    8    0    0    0    1
     3    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  419    0    2    0    3    0    0    0    0
    11    0    0]
 [   0    0    0    0    0    0  643    0    0    0    2    5    0    0
     7    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   15    0    0    1    0
     0    0    0]
 [   0    0   18   90    0    0    0    0    0    0  759    0    0    0
     2    6    0]
 [   0    0    2    0    0    4   13    0    0    0    3 2171    7    1
     9    0    0]
 [   0    0    6   43   18    0    0    0    0    0   26    0  427    0
     0    0   14]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    3    2    0    0
  1132    0    0]
 [   0    0    0    0    0    0   40    0    0    5    1    0    1    0
    64  236    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
93.79945799457994

F1 scores:
[       nan 0.91358025 0.93785311 0.89319176 0.95730337 0.956621
 0.94558824 0.96153846 1.         0.56603774 0.90518784 0.96768442
 0.88041237 0.99462366 0.956485   0.80135823 0.92307692]

Kappa:
0.9292159718129138
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2979d4fac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.803, val_acc:0.250]
Epoch [2/120    avg_loss:2.697, val_acc:0.402]
Epoch [3/120    avg_loss:2.601, val_acc:0.421]
Epoch [4/120    avg_loss:2.513, val_acc:0.429]
Epoch [5/120    avg_loss:2.437, val_acc:0.477]
Epoch [6/120    avg_loss:2.343, val_acc:0.515]
Epoch [7/120    avg_loss:2.256, val_acc:0.523]
Epoch [8/120    avg_loss:2.223, val_acc:0.539]
Epoch [9/120    avg_loss:2.135, val_acc:0.489]
Epoch [10/120    avg_loss:2.082, val_acc:0.565]
Epoch [11/120    avg_loss:2.043, val_acc:0.585]
Epoch [12/120    avg_loss:1.977, val_acc:0.596]
Epoch [13/120    avg_loss:1.920, val_acc:0.600]
Epoch [14/120    avg_loss:1.876, val_acc:0.621]
Epoch [15/120    avg_loss:1.805, val_acc:0.624]
Epoch [16/120    avg_loss:1.772, val_acc:0.622]
Epoch [17/120    avg_loss:1.729, val_acc:0.646]
Epoch [18/120    avg_loss:1.608, val_acc:0.636]
Epoch [19/120    avg_loss:1.557, val_acc:0.642]
Epoch [20/120    avg_loss:1.505, val_acc:0.646]
Epoch [21/120    avg_loss:1.414, val_acc:0.674]
Epoch [22/120    avg_loss:1.301, val_acc:0.683]
Epoch [23/120    avg_loss:1.250, val_acc:0.712]
Epoch [24/120    avg_loss:1.160, val_acc:0.693]
Epoch [25/120    avg_loss:1.121, val_acc:0.720]
Epoch [26/120    avg_loss:1.112, val_acc:0.698]
Epoch [27/120    avg_loss:1.036, val_acc:0.723]
Epoch [28/120    avg_loss:0.935, val_acc:0.746]
Epoch [29/120    avg_loss:0.912, val_acc:0.741]
Epoch [30/120    avg_loss:0.861, val_acc:0.740]
Epoch [31/120    avg_loss:0.821, val_acc:0.740]
Epoch [32/120    avg_loss:0.794, val_acc:0.788]
Epoch [33/120    avg_loss:0.683, val_acc:0.801]
Epoch [34/120    avg_loss:0.662, val_acc:0.812]
Epoch [35/120    avg_loss:0.637, val_acc:0.815]
Epoch [36/120    avg_loss:0.599, val_acc:0.816]
Epoch [37/120    avg_loss:0.493, val_acc:0.853]
Epoch [38/120    avg_loss:0.490, val_acc:0.862]
Epoch [39/120    avg_loss:0.478, val_acc:0.843]
Epoch [40/120    avg_loss:0.428, val_acc:0.858]
Epoch [41/120    avg_loss:0.450, val_acc:0.854]
Epoch [42/120    avg_loss:0.378, val_acc:0.886]
Epoch [43/120    avg_loss:0.427, val_acc:0.852]
Epoch [44/120    avg_loss:0.404, val_acc:0.809]
Epoch [45/120    avg_loss:0.388, val_acc:0.864]
Epoch [46/120    avg_loss:0.344, val_acc:0.876]
Epoch [47/120    avg_loss:0.367, val_acc:0.840]
Epoch [48/120    avg_loss:0.463, val_acc:0.883]
Epoch [49/120    avg_loss:0.351, val_acc:0.864]
Epoch [50/120    avg_loss:0.298, val_acc:0.907]
Epoch [51/120    avg_loss:0.277, val_acc:0.886]
Epoch [52/120    avg_loss:0.276, val_acc:0.902]
Epoch [53/120    avg_loss:0.246, val_acc:0.914]
Epoch [54/120    avg_loss:0.231, val_acc:0.894]
Epoch [55/120    avg_loss:0.241, val_acc:0.915]
Epoch [56/120    avg_loss:0.235, val_acc:0.895]
Epoch [57/120    avg_loss:0.186, val_acc:0.914]
Epoch [58/120    avg_loss:0.256, val_acc:0.871]
Epoch [59/120    avg_loss:0.256, val_acc:0.904]
Epoch [60/120    avg_loss:0.227, val_acc:0.903]
Epoch [61/120    avg_loss:0.236, val_acc:0.915]
Epoch [62/120    avg_loss:0.193, val_acc:0.908]
Epoch [63/120    avg_loss:0.175, val_acc:0.916]
Epoch [64/120    avg_loss:0.150, val_acc:0.914]
Epoch [65/120    avg_loss:0.184, val_acc:0.931]
Epoch [66/120    avg_loss:0.167, val_acc:0.935]
Epoch [67/120    avg_loss:0.103, val_acc:0.930]
Epoch [68/120    avg_loss:0.133, val_acc:0.925]
Epoch [69/120    avg_loss:0.138, val_acc:0.915]
Epoch [70/120    avg_loss:0.187, val_acc:0.923]
Epoch [71/120    avg_loss:0.156, val_acc:0.920]
Epoch [72/120    avg_loss:0.162, val_acc:0.927]
Epoch [73/120    avg_loss:0.117, val_acc:0.935]
Epoch [74/120    avg_loss:0.139, val_acc:0.925]
Epoch [75/120    avg_loss:0.107, val_acc:0.946]
Epoch [76/120    avg_loss:0.153, val_acc:0.930]
Epoch [77/120    avg_loss:0.157, val_acc:0.927]
Epoch [78/120    avg_loss:0.142, val_acc:0.945]
Epoch [79/120    avg_loss:0.086, val_acc:0.946]
Epoch [80/120    avg_loss:0.086, val_acc:0.952]
Epoch [81/120    avg_loss:0.079, val_acc:0.934]
Epoch [82/120    avg_loss:0.078, val_acc:0.958]
Epoch [83/120    avg_loss:0.099, val_acc:0.949]
Epoch [84/120    avg_loss:0.094, val_acc:0.953]
Epoch [85/120    avg_loss:0.066, val_acc:0.951]
Epoch [86/120    avg_loss:0.059, val_acc:0.960]
Epoch [87/120    avg_loss:0.059, val_acc:0.944]
Epoch [88/120    avg_loss:0.086, val_acc:0.946]
Epoch [89/120    avg_loss:0.060, val_acc:0.949]
Epoch [90/120    avg_loss:0.059, val_acc:0.963]
Epoch [91/120    avg_loss:0.062, val_acc:0.958]
Epoch [92/120    avg_loss:0.064, val_acc:0.958]
Epoch [93/120    avg_loss:0.060, val_acc:0.963]
Epoch [94/120    avg_loss:0.051, val_acc:0.962]
Epoch [95/120    avg_loss:0.060, val_acc:0.962]
Epoch [96/120    avg_loss:0.053, val_acc:0.958]
Epoch [97/120    avg_loss:0.046, val_acc:0.956]
Epoch [98/120    avg_loss:0.057, val_acc:0.959]
Epoch [99/120    avg_loss:0.045, val_acc:0.952]
Epoch [100/120    avg_loss:0.058, val_acc:0.958]
Epoch [101/120    avg_loss:0.059, val_acc:0.942]
Epoch [102/120    avg_loss:0.052, val_acc:0.959]
Epoch [103/120    avg_loss:0.047, val_acc:0.953]
Epoch [104/120    avg_loss:0.044, val_acc:0.956]
Epoch [105/120    avg_loss:0.044, val_acc:0.959]
Epoch [106/120    avg_loss:0.035, val_acc:0.960]
Epoch [107/120    avg_loss:0.031, val_acc:0.962]
Epoch [108/120    avg_loss:0.028, val_acc:0.963]
Epoch [109/120    avg_loss:0.026, val_acc:0.965]
Epoch [110/120    avg_loss:0.027, val_acc:0.963]
Epoch [111/120    avg_loss:0.023, val_acc:0.964]
Epoch [112/120    avg_loss:0.029, val_acc:0.968]
Epoch [113/120    avg_loss:0.026, val_acc:0.965]
Epoch [114/120    avg_loss:0.022, val_acc:0.964]
Epoch [115/120    avg_loss:0.027, val_acc:0.963]
Epoch [116/120    avg_loss:0.023, val_acc:0.963]
Epoch [117/120    avg_loss:0.026, val_acc:0.964]
Epoch [118/120    avg_loss:0.021, val_acc:0.964]
Epoch [119/120    avg_loss:0.022, val_acc:0.962]
Epoch [120/120    avg_loss:0.023, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    1    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1219    3    4    0    5    0    0    5    6   27    8    0
     0    8    0]
 [   0    0    0  672    4   11    0    0    0   24    0    0   35    1
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    6    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    5    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    0    2    0    0   10    0    0    1    0
     0    0    0]
 [   0    0   24   90    0    5    0    0    0    0  747    7    2    0
     0    0    0]
 [   0    0   18    0    0    0    2    0    0    0    1 2180    8    1
     0    0    0]
 [   0    0    0   23   13    7    0    0    0    0    7   11  467    0
     0    2    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    0    1    0    0
  1136    0    0]
 [   0    0    0    0    0    1   56    0    0    0    0    4    2    0
    67  217    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
94.28726287262873

F1 scores:
[       nan 0.94871795 0.95720455 0.87272727 0.95067265 0.96396396
 0.94905386 1.         0.99883856 0.31746032 0.91153142 0.98087739
 0.8803016  0.99462366 0.96969697 0.75609756 0.95857988]

Kappa:
0.9348309285135069
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc786114b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.812, val_acc:0.098]
Epoch [2/120    avg_loss:2.731, val_acc:0.209]
Epoch [3/120    avg_loss:2.621, val_acc:0.275]
Epoch [4/120    avg_loss:2.481, val_acc:0.354]
Epoch [5/120    avg_loss:2.396, val_acc:0.376]
Epoch [6/120    avg_loss:2.327, val_acc:0.389]
Epoch [7/120    avg_loss:2.250, val_acc:0.397]
Epoch [8/120    avg_loss:2.136, val_acc:0.411]
Epoch [9/120    avg_loss:2.048, val_acc:0.411]
Epoch [10/120    avg_loss:1.964, val_acc:0.421]
Epoch [11/120    avg_loss:1.869, val_acc:0.464]
Epoch [12/120    avg_loss:1.735, val_acc:0.534]
Epoch [13/120    avg_loss:1.666, val_acc:0.498]
Epoch [14/120    avg_loss:1.581, val_acc:0.587]
Epoch [15/120    avg_loss:1.499, val_acc:0.633]
Epoch [16/120    avg_loss:1.458, val_acc:0.631]
Epoch [17/120    avg_loss:1.337, val_acc:0.646]
Epoch [18/120    avg_loss:1.285, val_acc:0.654]
Epoch [19/120    avg_loss:1.255, val_acc:0.670]
Epoch [20/120    avg_loss:1.144, val_acc:0.692]
Epoch [21/120    avg_loss:1.090, val_acc:0.731]
Epoch [22/120    avg_loss:1.032, val_acc:0.704]
Epoch [23/120    avg_loss:0.952, val_acc:0.685]
Epoch [24/120    avg_loss:0.872, val_acc:0.712]
Epoch [25/120    avg_loss:0.828, val_acc:0.750]
Epoch [26/120    avg_loss:0.801, val_acc:0.763]
Epoch [27/120    avg_loss:0.701, val_acc:0.749]
Epoch [28/120    avg_loss:0.664, val_acc:0.758]
Epoch [29/120    avg_loss:0.653, val_acc:0.787]
Epoch [30/120    avg_loss:0.597, val_acc:0.799]
Epoch [31/120    avg_loss:0.544, val_acc:0.804]
Epoch [32/120    avg_loss:0.508, val_acc:0.802]
Epoch [33/120    avg_loss:0.473, val_acc:0.831]
Epoch [34/120    avg_loss:0.462, val_acc:0.838]
Epoch [35/120    avg_loss:0.424, val_acc:0.828]
Epoch [36/120    avg_loss:0.447, val_acc:0.816]
Epoch [37/120    avg_loss:0.449, val_acc:0.841]
Epoch [38/120    avg_loss:0.395, val_acc:0.828]
Epoch [39/120    avg_loss:0.394, val_acc:0.825]
Epoch [40/120    avg_loss:0.405, val_acc:0.852]
Epoch [41/120    avg_loss:0.416, val_acc:0.838]
Epoch [42/120    avg_loss:0.442, val_acc:0.837]
Epoch [43/120    avg_loss:0.414, val_acc:0.834]
Epoch [44/120    avg_loss:0.396, val_acc:0.818]
Epoch [45/120    avg_loss:0.336, val_acc:0.878]
Epoch [46/120    avg_loss:0.285, val_acc:0.869]
Epoch [47/120    avg_loss:0.307, val_acc:0.884]
Epoch [48/120    avg_loss:0.252, val_acc:0.877]
Epoch [49/120    avg_loss:0.271, val_acc:0.878]
Epoch [50/120    avg_loss:0.216, val_acc:0.883]
Epoch [51/120    avg_loss:0.204, val_acc:0.900]
Epoch [52/120    avg_loss:0.174, val_acc:0.904]
Epoch [53/120    avg_loss:0.190, val_acc:0.895]
Epoch [54/120    avg_loss:0.197, val_acc:0.897]
Epoch [55/120    avg_loss:0.233, val_acc:0.911]
Epoch [56/120    avg_loss:0.184, val_acc:0.903]
Epoch [57/120    avg_loss:0.197, val_acc:0.889]
Epoch [58/120    avg_loss:0.215, val_acc:0.894]
Epoch [59/120    avg_loss:0.153, val_acc:0.898]
Epoch [60/120    avg_loss:0.166, val_acc:0.910]
Epoch [61/120    avg_loss:0.157, val_acc:0.917]
Epoch [62/120    avg_loss:0.137, val_acc:0.922]
Epoch [63/120    avg_loss:0.130, val_acc:0.918]
Epoch [64/120    avg_loss:0.121, val_acc:0.928]
Epoch [65/120    avg_loss:0.096, val_acc:0.924]
Epoch [66/120    avg_loss:0.086, val_acc:0.933]
Epoch [67/120    avg_loss:0.090, val_acc:0.939]
Epoch [68/120    avg_loss:0.088, val_acc:0.929]
Epoch [69/120    avg_loss:0.095, val_acc:0.932]
Epoch [70/120    avg_loss:0.097, val_acc:0.920]
Epoch [71/120    avg_loss:0.107, val_acc:0.924]
Epoch [72/120    avg_loss:0.087, val_acc:0.941]
Epoch [73/120    avg_loss:0.085, val_acc:0.912]
Epoch [74/120    avg_loss:0.134, val_acc:0.922]
Epoch [75/120    avg_loss:0.114, val_acc:0.933]
Epoch [76/120    avg_loss:0.094, val_acc:0.931]
Epoch [77/120    avg_loss:0.082, val_acc:0.932]
Epoch [78/120    avg_loss:0.074, val_acc:0.939]
Epoch [79/120    avg_loss:0.084, val_acc:0.933]
Epoch [80/120    avg_loss:0.073, val_acc:0.933]
Epoch [81/120    avg_loss:0.091, val_acc:0.934]
Epoch [82/120    avg_loss:0.095, val_acc:0.948]
Epoch [83/120    avg_loss:0.087, val_acc:0.933]
Epoch [84/120    avg_loss:0.068, val_acc:0.944]
Epoch [85/120    avg_loss:0.060, val_acc:0.946]
Epoch [86/120    avg_loss:0.054, val_acc:0.958]
Epoch [87/120    avg_loss:0.052, val_acc:0.955]
Epoch [88/120    avg_loss:0.058, val_acc:0.955]
Epoch [89/120    avg_loss:0.065, val_acc:0.947]
Epoch [90/120    avg_loss:0.070, val_acc:0.957]
Epoch [91/120    avg_loss:0.071, val_acc:0.943]
Epoch [92/120    avg_loss:0.072, val_acc:0.940]
Epoch [93/120    avg_loss:0.048, val_acc:0.956]
Epoch [94/120    avg_loss:0.057, val_acc:0.948]
Epoch [95/120    avg_loss:0.075, val_acc:0.942]
Epoch [96/120    avg_loss:0.088, val_acc:0.934]
Epoch [97/120    avg_loss:0.070, val_acc:0.948]
Epoch [98/120    avg_loss:0.057, val_acc:0.948]
Epoch [99/120    avg_loss:0.057, val_acc:0.955]
Epoch [100/120    avg_loss:0.042, val_acc:0.961]
Epoch [101/120    avg_loss:0.040, val_acc:0.963]
Epoch [102/120    avg_loss:0.033, val_acc:0.963]
Epoch [103/120    avg_loss:0.032, val_acc:0.964]
Epoch [104/120    avg_loss:0.036, val_acc:0.962]
Epoch [105/120    avg_loss:0.033, val_acc:0.965]
Epoch [106/120    avg_loss:0.033, val_acc:0.965]
Epoch [107/120    avg_loss:0.030, val_acc:0.965]
Epoch [108/120    avg_loss:0.034, val_acc:0.966]
Epoch [109/120    avg_loss:0.035, val_acc:0.963]
Epoch [110/120    avg_loss:0.035, val_acc:0.963]
Epoch [111/120    avg_loss:0.033, val_acc:0.962]
Epoch [112/120    avg_loss:0.034, val_acc:0.963]
Epoch [113/120    avg_loss:0.029, val_acc:0.959]
Epoch [114/120    avg_loss:0.037, val_acc:0.963]
Epoch [115/120    avg_loss:0.029, val_acc:0.959]
Epoch [116/120    avg_loss:0.033, val_acc:0.962]
Epoch [117/120    avg_loss:0.033, val_acc:0.961]
Epoch [118/120    avg_loss:0.034, val_acc:0.965]
Epoch [119/120    avg_loss:0.031, val_acc:0.968]
Epoch [120/120    avg_loss:0.033, val_acc:0.962]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1227    2    0    1    3    0    0    0   15   22    1    0
     0   14    0]
 [   0    0    1  721    2   17    0    0    0    4    0    0    0    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    1    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   24   88    0    6    1    0    0    0  739    7    0    0
     0   10    0]
 [   0    0   21    0    0    4    6    0    0    0   13 2158    5    3
     0    0    0]
 [   0    0    0   15    7    6    0    0    0    0   11   21  466    0
     0    0    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    1    0    2    1    0    0
  1131    0    0]
 [   0    0    0    0    0    0   12    0    0   13    0    0    0    0
   100  222    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.69918699186992

F1 scores:
[       nan 0.93670886 0.95934324 0.91671964 0.97931034 0.95005549
 0.98124531 0.98039216 0.99767442 0.66666667 0.89089813 0.97602895
 0.92552135 0.98666667 0.95242105 0.74873524 0.94857143]

Kappa:
0.9395328076733581
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8758375ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.854, val_acc:0.034]
Epoch [2/120    avg_loss:2.754, val_acc:0.324]
Epoch [3/120    avg_loss:2.661, val_acc:0.394]
Epoch [4/120    avg_loss:2.557, val_acc:0.426]
Epoch [5/120    avg_loss:2.459, val_acc:0.461]
Epoch [6/120    avg_loss:2.376, val_acc:0.472]
Epoch [7/120    avg_loss:2.320, val_acc:0.506]
Epoch [8/120    avg_loss:2.240, val_acc:0.530]
Epoch [9/120    avg_loss:2.163, val_acc:0.533]
Epoch [10/120    avg_loss:2.084, val_acc:0.559]
Epoch [11/120    avg_loss:1.997, val_acc:0.592]
Epoch [12/120    avg_loss:1.881, val_acc:0.589]
Epoch [13/120    avg_loss:1.815, val_acc:0.610]
Epoch [14/120    avg_loss:1.701, val_acc:0.634]
Epoch [15/120    avg_loss:1.619, val_acc:0.629]
Epoch [16/120    avg_loss:1.475, val_acc:0.638]
Epoch [17/120    avg_loss:1.385, val_acc:0.622]
Epoch [18/120    avg_loss:1.345, val_acc:0.637]
Epoch [19/120    avg_loss:1.248, val_acc:0.654]
Epoch [20/120    avg_loss:1.110, val_acc:0.683]
Epoch [21/120    avg_loss:1.061, val_acc:0.698]
Epoch [22/120    avg_loss:0.994, val_acc:0.710]
Epoch [23/120    avg_loss:0.940, val_acc:0.747]
Epoch [24/120    avg_loss:0.918, val_acc:0.763]
Epoch [25/120    avg_loss:0.825, val_acc:0.754]
Epoch [26/120    avg_loss:0.819, val_acc:0.717]
Epoch [27/120    avg_loss:0.780, val_acc:0.762]
Epoch [28/120    avg_loss:0.728, val_acc:0.796]
Epoch [29/120    avg_loss:0.683, val_acc:0.778]
Epoch [30/120    avg_loss:0.657, val_acc:0.810]
Epoch [31/120    avg_loss:0.592, val_acc:0.790]
Epoch [32/120    avg_loss:0.600, val_acc:0.835]
Epoch [33/120    avg_loss:0.588, val_acc:0.824]
Epoch [34/120    avg_loss:0.571, val_acc:0.800]
Epoch [35/120    avg_loss:0.547, val_acc:0.831]
Epoch [36/120    avg_loss:0.468, val_acc:0.839]
Epoch [37/120    avg_loss:0.449, val_acc:0.835]
Epoch [38/120    avg_loss:0.467, val_acc:0.821]
Epoch [39/120    avg_loss:0.401, val_acc:0.847]
Epoch [40/120    avg_loss:0.347, val_acc:0.856]
Epoch [41/120    avg_loss:0.386, val_acc:0.875]
Epoch [42/120    avg_loss:0.354, val_acc:0.866]
Epoch [43/120    avg_loss:0.307, val_acc:0.900]
Epoch [44/120    avg_loss:0.295, val_acc:0.859]
Epoch [45/120    avg_loss:0.340, val_acc:0.838]
Epoch [46/120    avg_loss:0.548, val_acc:0.830]
Epoch [47/120    avg_loss:0.395, val_acc:0.848]
Epoch [48/120    avg_loss:0.324, val_acc:0.882]
Epoch [49/120    avg_loss:0.307, val_acc:0.853]
Epoch [50/120    avg_loss:0.330, val_acc:0.856]
Epoch [51/120    avg_loss:0.303, val_acc:0.883]
Epoch [52/120    avg_loss:0.245, val_acc:0.904]
Epoch [53/120    avg_loss:0.246, val_acc:0.901]
Epoch [54/120    avg_loss:0.250, val_acc:0.911]
Epoch [55/120    avg_loss:0.231, val_acc:0.893]
Epoch [56/120    avg_loss:0.258, val_acc:0.881]
Epoch [57/120    avg_loss:0.188, val_acc:0.921]
Epoch [58/120    avg_loss:0.222, val_acc:0.888]
Epoch [59/120    avg_loss:0.238, val_acc:0.891]
Epoch [60/120    avg_loss:0.224, val_acc:0.907]
Epoch [61/120    avg_loss:0.179, val_acc:0.926]
Epoch [62/120    avg_loss:0.204, val_acc:0.884]
Epoch [63/120    avg_loss:0.183, val_acc:0.913]
Epoch [64/120    avg_loss:0.139, val_acc:0.898]
Epoch [65/120    avg_loss:0.229, val_acc:0.884]
Epoch [66/120    avg_loss:0.201, val_acc:0.923]
Epoch [67/120    avg_loss:0.182, val_acc:0.919]
Epoch [68/120    avg_loss:0.184, val_acc:0.903]
Epoch [69/120    avg_loss:0.135, val_acc:0.925]
Epoch [70/120    avg_loss:0.140, val_acc:0.908]
Epoch [71/120    avg_loss:0.117, val_acc:0.929]
Epoch [72/120    avg_loss:0.117, val_acc:0.900]
Epoch [73/120    avg_loss:0.151, val_acc:0.886]
Epoch [74/120    avg_loss:0.143, val_acc:0.929]
Epoch [75/120    avg_loss:0.111, val_acc:0.926]
Epoch [76/120    avg_loss:0.116, val_acc:0.932]
Epoch [77/120    avg_loss:0.111, val_acc:0.930]
Epoch [78/120    avg_loss:0.096, val_acc:0.927]
Epoch [79/120    avg_loss:0.091, val_acc:0.934]
Epoch [80/120    avg_loss:0.112, val_acc:0.924]
Epoch [81/120    avg_loss:0.087, val_acc:0.932]
Epoch [82/120    avg_loss:0.077, val_acc:0.933]
Epoch [83/120    avg_loss:0.076, val_acc:0.939]
Epoch [84/120    avg_loss:0.067, val_acc:0.936]
Epoch [85/120    avg_loss:0.082, val_acc:0.938]
Epoch [86/120    avg_loss:0.099, val_acc:0.935]
Epoch [87/120    avg_loss:0.102, val_acc:0.939]
Epoch [88/120    avg_loss:0.091, val_acc:0.927]
Epoch [89/120    avg_loss:0.090, val_acc:0.948]
Epoch [90/120    avg_loss:0.067, val_acc:0.945]
Epoch [91/120    avg_loss:0.089, val_acc:0.936]
Epoch [92/120    avg_loss:0.116, val_acc:0.898]
Epoch [93/120    avg_loss:0.179, val_acc:0.908]
Epoch [94/120    avg_loss:0.126, val_acc:0.927]
Epoch [95/120    avg_loss:0.088, val_acc:0.941]
Epoch [96/120    avg_loss:0.087, val_acc:0.911]
Epoch [97/120    avg_loss:0.084, val_acc:0.938]
Epoch [98/120    avg_loss:0.088, val_acc:0.951]
Epoch [99/120    avg_loss:0.055, val_acc:0.940]
Epoch [100/120    avg_loss:0.060, val_acc:0.948]
Epoch [101/120    avg_loss:0.055, val_acc:0.943]
Epoch [102/120    avg_loss:0.071, val_acc:0.948]
Epoch [103/120    avg_loss:0.055, val_acc:0.942]
Epoch [104/120    avg_loss:0.059, val_acc:0.944]
Epoch [105/120    avg_loss:0.058, val_acc:0.952]
Epoch [106/120    avg_loss:0.052, val_acc:0.942]
Epoch [107/120    avg_loss:0.046, val_acc:0.946]
Epoch [108/120    avg_loss:0.059, val_acc:0.950]
Epoch [109/120    avg_loss:0.048, val_acc:0.941]
Epoch [110/120    avg_loss:0.064, val_acc:0.942]
Epoch [111/120    avg_loss:0.060, val_acc:0.951]
Epoch [112/120    avg_loss:0.064, val_acc:0.946]
Epoch [113/120    avg_loss:0.055, val_acc:0.936]
Epoch [114/120    avg_loss:0.071, val_acc:0.939]
Epoch [115/120    avg_loss:0.047, val_acc:0.943]
Epoch [116/120    avg_loss:0.034, val_acc:0.943]
Epoch [117/120    avg_loss:0.042, val_acc:0.945]
Epoch [118/120    avg_loss:0.045, val_acc:0.958]
Epoch [119/120    avg_loss:0.047, val_acc:0.956]
Epoch [120/120    avg_loss:0.035, val_acc:0.946]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    5 1206    3    0    0    3    0    0    0    6   53    2    0
     0    7    0]
 [   0    0    1  690    2    6    0    0    0    3    0    0   42    3
     0    0    0]
 [   0    0    0    0  211    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  424    0    5    0    1    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  645    0    0    0    0    4    0    0
     8    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0   69   86    0    3    3    0    0    0  707    2    1    0
     0    4    0]
 [   0    0   38    0    0    0   10    0    0    0    7 2148    3    1
     3    0    0]
 [   0    0    0    1    1    5    0    0    0    0   17   10  500    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    1    1    0
  1136    0    0]
 [   0    0    0    0    0    0   28    0    0    4    0    0    0    0
   143  172    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
93.3550135501355

F1 scores:
[       nan 0.89156627 0.92804925 0.90373281 0.9882904  0.97136312
 0.95839525 0.90909091 1.         0.73170732 0.87445887 0.96997065
 0.91659028 0.98930481 0.93382655 0.6490566  0.98181818]

Kappa:
0.9241184846200017
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6bc5f0dbe0>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.796, val_acc:0.174]
Epoch [2/120    avg_loss:2.699, val_acc:0.191]
Epoch [3/120    avg_loss:2.596, val_acc:0.291]
Epoch [4/120    avg_loss:2.549, val_acc:0.388]
Epoch [5/120    avg_loss:2.413, val_acc:0.405]
Epoch [6/120    avg_loss:2.343, val_acc:0.443]
Epoch [7/120    avg_loss:2.279, val_acc:0.468]
Epoch [8/120    avg_loss:2.205, val_acc:0.463]
Epoch [9/120    avg_loss:2.150, val_acc:0.488]
Epoch [10/120    avg_loss:2.081, val_acc:0.524]
Epoch [11/120    avg_loss:1.987, val_acc:0.530]
Epoch [12/120    avg_loss:1.924, val_acc:0.574]
Epoch [13/120    avg_loss:1.870, val_acc:0.590]
Epoch [14/120    avg_loss:1.819, val_acc:0.620]
Epoch [15/120    avg_loss:1.742, val_acc:0.618]
Epoch [16/120    avg_loss:1.633, val_acc:0.646]
Epoch [17/120    avg_loss:1.520, val_acc:0.660]
Epoch [18/120    avg_loss:1.475, val_acc:0.656]
Epoch [19/120    avg_loss:1.445, val_acc:0.663]
Epoch [20/120    avg_loss:1.307, val_acc:0.682]
Epoch [21/120    avg_loss:1.202, val_acc:0.685]
Epoch [22/120    avg_loss:1.199, val_acc:0.708]
Epoch [23/120    avg_loss:1.084, val_acc:0.730]
Epoch [24/120    avg_loss:1.056, val_acc:0.728]
Epoch [25/120    avg_loss:0.955, val_acc:0.741]
Epoch [26/120    avg_loss:0.917, val_acc:0.759]
Epoch [27/120    avg_loss:0.856, val_acc:0.731]
Epoch [28/120    avg_loss:0.822, val_acc:0.762]
Epoch [29/120    avg_loss:0.819, val_acc:0.723]
Epoch [30/120    avg_loss:0.814, val_acc:0.762]
Epoch [31/120    avg_loss:0.696, val_acc:0.782]
Epoch [32/120    avg_loss:0.703, val_acc:0.774]
Epoch [33/120    avg_loss:0.677, val_acc:0.765]
Epoch [34/120    avg_loss:0.713, val_acc:0.777]
Epoch [35/120    avg_loss:0.664, val_acc:0.771]
Epoch [36/120    avg_loss:0.554, val_acc:0.771]
Epoch [37/120    avg_loss:0.544, val_acc:0.799]
Epoch [38/120    avg_loss:0.483, val_acc:0.811]
Epoch [39/120    avg_loss:0.440, val_acc:0.817]
Epoch [40/120    avg_loss:0.444, val_acc:0.826]
Epoch [41/120    avg_loss:0.398, val_acc:0.844]
Epoch [42/120    avg_loss:0.363, val_acc:0.825]
Epoch [43/120    avg_loss:0.458, val_acc:0.793]
Epoch [44/120    avg_loss:0.426, val_acc:0.841]
Epoch [45/120    avg_loss:0.373, val_acc:0.853]
Epoch [46/120    avg_loss:0.345, val_acc:0.853]
Epoch [47/120    avg_loss:0.283, val_acc:0.862]
Epoch [48/120    avg_loss:0.288, val_acc:0.865]
Epoch [49/120    avg_loss:0.305, val_acc:0.838]
Epoch [50/120    avg_loss:0.389, val_acc:0.867]
Epoch [51/120    avg_loss:0.342, val_acc:0.862]
Epoch [52/120    avg_loss:0.302, val_acc:0.859]
Epoch [53/120    avg_loss:0.269, val_acc:0.898]
Epoch [54/120    avg_loss:0.218, val_acc:0.899]
Epoch [55/120    avg_loss:0.226, val_acc:0.897]
Epoch [56/120    avg_loss:0.234, val_acc:0.887]
Epoch [57/120    avg_loss:0.216, val_acc:0.909]
Epoch [58/120    avg_loss:0.188, val_acc:0.903]
Epoch [59/120    avg_loss:0.181, val_acc:0.900]
Epoch [60/120    avg_loss:0.211, val_acc:0.903]
Epoch [61/120    avg_loss:0.168, val_acc:0.925]
Epoch [62/120    avg_loss:0.164, val_acc:0.928]
Epoch [63/120    avg_loss:0.172, val_acc:0.883]
Epoch [64/120    avg_loss:0.235, val_acc:0.896]
Epoch [65/120    avg_loss:0.207, val_acc:0.912]
Epoch [66/120    avg_loss:0.159, val_acc:0.916]
Epoch [67/120    avg_loss:0.137, val_acc:0.902]
Epoch [68/120    avg_loss:0.147, val_acc:0.910]
Epoch [69/120    avg_loss:0.167, val_acc:0.905]
Epoch [70/120    avg_loss:0.147, val_acc:0.926]
Epoch [71/120    avg_loss:0.106, val_acc:0.919]
Epoch [72/120    avg_loss:0.116, val_acc:0.931]
Epoch [73/120    avg_loss:0.098, val_acc:0.938]
Epoch [74/120    avg_loss:0.102, val_acc:0.931]
Epoch [75/120    avg_loss:0.120, val_acc:0.917]
Epoch [76/120    avg_loss:0.126, val_acc:0.935]
Epoch [77/120    avg_loss:0.100, val_acc:0.935]
Epoch [78/120    avg_loss:0.136, val_acc:0.928]
Epoch [79/120    avg_loss:0.102, val_acc:0.929]
Epoch [80/120    avg_loss:0.107, val_acc:0.934]
Epoch [81/120    avg_loss:0.092, val_acc:0.943]
Epoch [82/120    avg_loss:0.092, val_acc:0.924]
Epoch [83/120    avg_loss:0.089, val_acc:0.928]
Epoch [84/120    avg_loss:0.097, val_acc:0.928]
Epoch [85/120    avg_loss:0.093, val_acc:0.920]
Epoch [86/120    avg_loss:0.147, val_acc:0.894]
Epoch [87/120    avg_loss:0.172, val_acc:0.927]
Epoch [88/120    avg_loss:0.138, val_acc:0.892]
Epoch [89/120    avg_loss:0.100, val_acc:0.944]
Epoch [90/120    avg_loss:0.072, val_acc:0.948]
Epoch [91/120    avg_loss:0.106, val_acc:0.933]
Epoch [92/120    avg_loss:0.094, val_acc:0.929]
Epoch [93/120    avg_loss:0.094, val_acc:0.933]
Epoch [94/120    avg_loss:0.077, val_acc:0.936]
Epoch [95/120    avg_loss:0.062, val_acc:0.944]
Epoch [96/120    avg_loss:0.059, val_acc:0.935]
Epoch [97/120    avg_loss:0.065, val_acc:0.949]
Epoch [98/120    avg_loss:0.082, val_acc:0.928]
Epoch [99/120    avg_loss:0.105, val_acc:0.928]
Epoch [100/120    avg_loss:0.127, val_acc:0.899]
Epoch [101/120    avg_loss:0.147, val_acc:0.927]
Epoch [102/120    avg_loss:0.088, val_acc:0.940]
Epoch [103/120    avg_loss:0.085, val_acc:0.941]
Epoch [104/120    avg_loss:0.066, val_acc:0.938]
Epoch [105/120    avg_loss:0.111, val_acc:0.929]
Epoch [106/120    avg_loss:0.096, val_acc:0.934]
Epoch [107/120    avg_loss:0.075, val_acc:0.935]
Epoch [108/120    avg_loss:0.047, val_acc:0.939]
Epoch [109/120    avg_loss:0.061, val_acc:0.939]
Epoch [110/120    avg_loss:0.056, val_acc:0.949]
Epoch [111/120    avg_loss:0.039, val_acc:0.955]
Epoch [112/120    avg_loss:0.038, val_acc:0.953]
Epoch [113/120    avg_loss:0.046, val_acc:0.943]
Epoch [114/120    avg_loss:0.051, val_acc:0.947]
Epoch [115/120    avg_loss:0.045, val_acc:0.956]
Epoch [116/120    avg_loss:0.099, val_acc:0.935]
Epoch [117/120    avg_loss:0.081, val_acc:0.926]
Epoch [118/120    avg_loss:0.076, val_acc:0.943]
Epoch [119/120    avg_loss:0.056, val_acc:0.943]
Epoch [120/120    avg_loss:0.051, val_acc:0.944]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1210    0    0    0    6    0    0    0    6   53    6    0
     0    4    0]
 [   0    0    3  638    2    5    0    0    0   12    0    0   86    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    8    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  647    0    0    1    0    3    0    0
     6    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    6    0    0    0    0    0    0  424    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   14    0    0    3    0
     0    0    0]
 [   0    0   19   90    0    2    0    0    0    0  760    0    0    0
     3    1    0]
 [   0    0   14    0    0    3   10    0    0    0   23 2154    4    1
     1    0    0]
 [   0    0    0   12    6    7    0    0    0    0   19    0  474    0
     0    0   16]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   12    0    0    1    0    3    2    0    0
  1121    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    62  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
94.25474254742548

F1 scores:
[       nan 0.90697674 0.95614382 0.85752688 0.98156682 0.95730337
 0.98030303 1.         0.99181287 0.52830189 0.90047393 0.97399955
 0.85328533 0.99462366 0.96140652 0.89481947 0.88888889]

Kappa:
0.9345194497022187
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff7c8b40b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.788, val_acc:0.273]
Epoch [2/120    avg_loss:2.693, val_acc:0.287]
Epoch [3/120    avg_loss:2.606, val_acc:0.296]
Epoch [4/120    avg_loss:2.511, val_acc:0.358]
Epoch [5/120    avg_loss:2.432, val_acc:0.387]
Epoch [6/120    avg_loss:2.329, val_acc:0.385]
Epoch [7/120    avg_loss:2.242, val_acc:0.390]
Epoch [8/120    avg_loss:2.196, val_acc:0.420]
Epoch [9/120    avg_loss:2.119, val_acc:0.435]
Epoch [10/120    avg_loss:2.007, val_acc:0.481]
Epoch [11/120    avg_loss:2.027, val_acc:0.566]
Epoch [12/120    avg_loss:1.921, val_acc:0.600]
Epoch [13/120    avg_loss:1.848, val_acc:0.598]
Epoch [14/120    avg_loss:1.777, val_acc:0.595]
Epoch [15/120    avg_loss:1.670, val_acc:0.609]
Epoch [16/120    avg_loss:1.603, val_acc:0.659]
Epoch [17/120    avg_loss:1.518, val_acc:0.646]
Epoch [18/120    avg_loss:1.465, val_acc:0.683]
Epoch [19/120    avg_loss:1.338, val_acc:0.703]
Epoch [20/120    avg_loss:1.269, val_acc:0.711]
Epoch [21/120    avg_loss:1.165, val_acc:0.721]
Epoch [22/120    avg_loss:1.067, val_acc:0.712]
Epoch [23/120    avg_loss:1.008, val_acc:0.731]
Epoch [24/120    avg_loss:0.926, val_acc:0.741]
Epoch [25/120    avg_loss:0.859, val_acc:0.749]
Epoch [26/120    avg_loss:0.828, val_acc:0.747]
Epoch [27/120    avg_loss:0.803, val_acc:0.755]
Epoch [28/120    avg_loss:0.736, val_acc:0.792]
Epoch [29/120    avg_loss:0.647, val_acc:0.775]
Epoch [30/120    avg_loss:0.628, val_acc:0.788]
Epoch [31/120    avg_loss:0.589, val_acc:0.803]
Epoch [32/120    avg_loss:0.545, val_acc:0.826]
Epoch [33/120    avg_loss:0.529, val_acc:0.819]
Epoch [34/120    avg_loss:0.538, val_acc:0.854]
Epoch [35/120    avg_loss:0.462, val_acc:0.826]
Epoch [36/120    avg_loss:0.457, val_acc:0.815]
Epoch [37/120    avg_loss:0.498, val_acc:0.847]
Epoch [38/120    avg_loss:0.396, val_acc:0.867]
Epoch [39/120    avg_loss:0.505, val_acc:0.848]
Epoch [40/120    avg_loss:0.449, val_acc:0.846]
Epoch [41/120    avg_loss:0.377, val_acc:0.884]
Epoch [42/120    avg_loss:0.373, val_acc:0.885]
Epoch [43/120    avg_loss:0.303, val_acc:0.874]
Epoch [44/120    avg_loss:0.258, val_acc:0.899]
Epoch [45/120    avg_loss:0.275, val_acc:0.898]
Epoch [46/120    avg_loss:0.262, val_acc:0.905]
Epoch [47/120    avg_loss:0.257, val_acc:0.926]
Epoch [48/120    avg_loss:0.220, val_acc:0.895]
Epoch [49/120    avg_loss:0.348, val_acc:0.844]
Epoch [50/120    avg_loss:0.339, val_acc:0.887]
Epoch [51/120    avg_loss:0.299, val_acc:0.897]
Epoch [52/120    avg_loss:0.291, val_acc:0.887]
Epoch [53/120    avg_loss:0.299, val_acc:0.887]
Epoch [54/120    avg_loss:0.302, val_acc:0.866]
Epoch [55/120    avg_loss:0.252, val_acc:0.902]
Epoch [56/120    avg_loss:0.268, val_acc:0.911]
Epoch [57/120    avg_loss:0.206, val_acc:0.898]
Epoch [58/120    avg_loss:0.221, val_acc:0.881]
Epoch [59/120    avg_loss:0.216, val_acc:0.903]
Epoch [60/120    avg_loss:0.175, val_acc:0.914]
Epoch [61/120    avg_loss:0.155, val_acc:0.939]
Epoch [62/120    avg_loss:0.134, val_acc:0.947]
Epoch [63/120    avg_loss:0.128, val_acc:0.954]
Epoch [64/120    avg_loss:0.127, val_acc:0.957]
Epoch [65/120    avg_loss:0.105, val_acc:0.955]
Epoch [66/120    avg_loss:0.115, val_acc:0.955]
Epoch [67/120    avg_loss:0.109, val_acc:0.955]
Epoch [68/120    avg_loss:0.108, val_acc:0.958]
Epoch [69/120    avg_loss:0.129, val_acc:0.954]
Epoch [70/120    avg_loss:0.112, val_acc:0.953]
Epoch [71/120    avg_loss:0.110, val_acc:0.957]
Epoch [72/120    avg_loss:0.095, val_acc:0.955]
Epoch [73/120    avg_loss:0.108, val_acc:0.954]
Epoch [74/120    avg_loss:0.102, val_acc:0.959]
Epoch [75/120    avg_loss:0.102, val_acc:0.958]
Epoch [76/120    avg_loss:0.108, val_acc:0.957]
Epoch [77/120    avg_loss:0.104, val_acc:0.955]
Epoch [78/120    avg_loss:0.095, val_acc:0.958]
Epoch [79/120    avg_loss:0.094, val_acc:0.957]
Epoch [80/120    avg_loss:0.097, val_acc:0.957]
Epoch [81/120    avg_loss:0.098, val_acc:0.955]
Epoch [82/120    avg_loss:0.096, val_acc:0.956]
Epoch [83/120    avg_loss:0.095, val_acc:0.957]
Epoch [84/120    avg_loss:0.095, val_acc:0.958]
Epoch [85/120    avg_loss:0.096, val_acc:0.963]
Epoch [86/120    avg_loss:0.100, val_acc:0.956]
Epoch [87/120    avg_loss:0.094, val_acc:0.955]
Epoch [88/120    avg_loss:0.088, val_acc:0.957]
Epoch [89/120    avg_loss:0.091, val_acc:0.956]
Epoch [90/120    avg_loss:0.086, val_acc:0.959]
Epoch [91/120    avg_loss:0.086, val_acc:0.961]
Epoch [92/120    avg_loss:0.081, val_acc:0.956]
Epoch [93/120    avg_loss:0.091, val_acc:0.959]
Epoch [94/120    avg_loss:0.102, val_acc:0.959]
Epoch [95/120    avg_loss:0.078, val_acc:0.957]
Epoch [96/120    avg_loss:0.078, val_acc:0.961]
Epoch [97/120    avg_loss:0.082, val_acc:0.958]
Epoch [98/120    avg_loss:0.090, val_acc:0.963]
Epoch [99/120    avg_loss:0.081, val_acc:0.962]
Epoch [100/120    avg_loss:0.085, val_acc:0.964]
Epoch [101/120    avg_loss:0.076, val_acc:0.959]
Epoch [102/120    avg_loss:0.085, val_acc:0.961]
Epoch [103/120    avg_loss:0.087, val_acc:0.957]
Epoch [104/120    avg_loss:0.087, val_acc:0.958]
Epoch [105/120    avg_loss:0.077, val_acc:0.959]
Epoch [106/120    avg_loss:0.080, val_acc:0.957]
Epoch [107/120    avg_loss:0.076, val_acc:0.958]
Epoch [108/120    avg_loss:0.085, val_acc:0.962]
Epoch [109/120    avg_loss:0.080, val_acc:0.962]
Epoch [110/120    avg_loss:0.082, val_acc:0.965]
Epoch [111/120    avg_loss:0.098, val_acc:0.963]
Epoch [112/120    avg_loss:0.078, val_acc:0.962]
Epoch [113/120    avg_loss:0.074, val_acc:0.959]
Epoch [114/120    avg_loss:0.085, val_acc:0.965]
Epoch [115/120    avg_loss:0.081, val_acc:0.961]
Epoch [116/120    avg_loss:0.078, val_acc:0.959]
Epoch [117/120    avg_loss:0.068, val_acc:0.958]
Epoch [118/120    avg_loss:0.068, val_acc:0.964]
Epoch [119/120    avg_loss:0.070, val_acc:0.965]
Epoch [120/120    avg_loss:0.067, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1157    3    0    0    9    0    0    0   13   94    6    0
     2    1    0]
 [   0    0    0  697    0   16    3    0    0   17    0    3    7    4
     0    0    0]
 [   0    0    0    4  209    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  419    0    4    0    6    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0   10    0    0    8    0    0    0    0
     0    0    0]
 [   0    0   50   83    0    5    0    0    0    0  715   16    0    0
     3    3    0]
 [   0    0   16    0    0    2    8    0    0    0    6 2167    3    5
     3    0    0]
 [   0    0    0    9    0    4    0    0    0    0   20    0  486    0
     0    0   15]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    0    0    3    4    0    0
  1122    0    0]
 [   0    0    0    0    0    0   43    0    0    0    0    0    0    0
   106  198    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
93.19241192411924

F1 scores:
[       nan 0.96202532 0.92264753 0.90343487 0.99052133 0.94051627
 0.94736842 0.92592593 1.         0.32653061 0.87461774 0.96439697
 0.93822394 0.9762533  0.94246115 0.72131148 0.91803279]

Kappa:
0.9222528035585625
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc6c865eb70>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.794, val_acc:0.255]
Epoch [2/120    avg_loss:2.684, val_acc:0.316]
Epoch [3/120    avg_loss:2.599, val_acc:0.362]
Epoch [4/120    avg_loss:2.517, val_acc:0.432]
Epoch [5/120    avg_loss:2.419, val_acc:0.501]
Epoch [6/120    avg_loss:2.360, val_acc:0.519]
Epoch [7/120    avg_loss:2.300, val_acc:0.531]
Epoch [8/120    avg_loss:2.255, val_acc:0.530]
Epoch [9/120    avg_loss:2.146, val_acc:0.532]
Epoch [10/120    avg_loss:2.143, val_acc:0.558]
Epoch [11/120    avg_loss:2.015, val_acc:0.564]
Epoch [12/120    avg_loss:2.021, val_acc:0.581]
Epoch [13/120    avg_loss:1.961, val_acc:0.601]
Epoch [14/120    avg_loss:1.859, val_acc:0.608]
Epoch [15/120    avg_loss:1.795, val_acc:0.630]
Epoch [16/120    avg_loss:1.727, val_acc:0.618]
Epoch [17/120    avg_loss:1.642, val_acc:0.645]
Epoch [18/120    avg_loss:1.516, val_acc:0.676]
Epoch [19/120    avg_loss:1.442, val_acc:0.670]
Epoch [20/120    avg_loss:1.360, val_acc:0.649]
Epoch [21/120    avg_loss:1.275, val_acc:0.730]
Epoch [22/120    avg_loss:1.273, val_acc:0.701]
Epoch [23/120    avg_loss:1.174, val_acc:0.735]
Epoch [24/120    avg_loss:1.070, val_acc:0.712]
Epoch [25/120    avg_loss:0.975, val_acc:0.737]
Epoch [26/120    avg_loss:0.947, val_acc:0.735]
Epoch [27/120    avg_loss:0.930, val_acc:0.735]
Epoch [28/120    avg_loss:0.865, val_acc:0.749]
Epoch [29/120    avg_loss:0.793, val_acc:0.764]
Epoch [30/120    avg_loss:0.726, val_acc:0.786]
Epoch [31/120    avg_loss:0.713, val_acc:0.760]
Epoch [32/120    avg_loss:0.681, val_acc:0.784]
Epoch [33/120    avg_loss:0.622, val_acc:0.763]
Epoch [34/120    avg_loss:0.590, val_acc:0.764]
Epoch [35/120    avg_loss:0.555, val_acc:0.758]
Epoch [36/120    avg_loss:0.585, val_acc:0.801]
Epoch [37/120    avg_loss:0.494, val_acc:0.837]
Epoch [38/120    avg_loss:0.499, val_acc:0.812]
Epoch [39/120    avg_loss:0.458, val_acc:0.829]
Epoch [40/120    avg_loss:0.402, val_acc:0.823]
Epoch [41/120    avg_loss:0.400, val_acc:0.791]
Epoch [42/120    avg_loss:0.461, val_acc:0.856]
Epoch [43/120    avg_loss:0.404, val_acc:0.852]
Epoch [44/120    avg_loss:0.341, val_acc:0.861]
Epoch [45/120    avg_loss:0.319, val_acc:0.847]
Epoch [46/120    avg_loss:0.289, val_acc:0.873]
Epoch [47/120    avg_loss:0.268, val_acc:0.881]
Epoch [48/120    avg_loss:0.251, val_acc:0.895]
Epoch [49/120    avg_loss:0.245, val_acc:0.888]
Epoch [50/120    avg_loss:0.255, val_acc:0.866]
Epoch [51/120    avg_loss:0.282, val_acc:0.892]
Epoch [52/120    avg_loss:0.264, val_acc:0.883]
Epoch [53/120    avg_loss:0.260, val_acc:0.887]
Epoch [54/120    avg_loss:0.254, val_acc:0.892]
Epoch [55/120    avg_loss:0.264, val_acc:0.891]
Epoch [56/120    avg_loss:0.213, val_acc:0.869]
Epoch [57/120    avg_loss:0.193, val_acc:0.899]
Epoch [58/120    avg_loss:0.196, val_acc:0.881]
Epoch [59/120    avg_loss:0.227, val_acc:0.881]
Epoch [60/120    avg_loss:0.391, val_acc:0.854]
Epoch [61/120    avg_loss:0.264, val_acc:0.897]
Epoch [62/120    avg_loss:0.201, val_acc:0.894]
Epoch [63/120    avg_loss:0.168, val_acc:0.902]
Epoch [64/120    avg_loss:0.166, val_acc:0.925]
Epoch [65/120    avg_loss:0.145, val_acc:0.926]
Epoch [66/120    avg_loss:0.127, val_acc:0.924]
Epoch [67/120    avg_loss:0.142, val_acc:0.922]
Epoch [68/120    avg_loss:0.114, val_acc:0.926]
Epoch [69/120    avg_loss:0.128, val_acc:0.894]
Epoch [70/120    avg_loss:0.132, val_acc:0.934]
Epoch [71/120    avg_loss:0.115, val_acc:0.922]
Epoch [72/120    avg_loss:0.119, val_acc:0.925]
Epoch [73/120    avg_loss:0.118, val_acc:0.929]
Epoch [74/120    avg_loss:0.101, val_acc:0.913]
Epoch [75/120    avg_loss:0.090, val_acc:0.910]
Epoch [76/120    avg_loss:0.099, val_acc:0.924]
Epoch [77/120    avg_loss:0.080, val_acc:0.926]
Epoch [78/120    avg_loss:0.095, val_acc:0.934]
Epoch [79/120    avg_loss:0.077, val_acc:0.927]
Epoch [80/120    avg_loss:0.084, val_acc:0.934]
Epoch [81/120    avg_loss:0.073, val_acc:0.947]
Epoch [82/120    avg_loss:0.069, val_acc:0.949]
Epoch [83/120    avg_loss:0.096, val_acc:0.904]
Epoch [84/120    avg_loss:0.108, val_acc:0.928]
Epoch [85/120    avg_loss:0.087, val_acc:0.933]
Epoch [86/120    avg_loss:0.088, val_acc:0.925]
Epoch [87/120    avg_loss:0.072, val_acc:0.948]
Epoch [88/120    avg_loss:0.075, val_acc:0.942]
Epoch [89/120    avg_loss:0.074, val_acc:0.944]
Epoch [90/120    avg_loss:0.064, val_acc:0.940]
Epoch [91/120    avg_loss:0.061, val_acc:0.941]
Epoch [92/120    avg_loss:0.072, val_acc:0.946]
Epoch [93/120    avg_loss:0.109, val_acc:0.921]
Epoch [94/120    avg_loss:0.078, val_acc:0.940]
Epoch [95/120    avg_loss:0.063, val_acc:0.936]
Epoch [96/120    avg_loss:0.043, val_acc:0.948]
Epoch [97/120    avg_loss:0.048, val_acc:0.950]
Epoch [98/120    avg_loss:0.045, val_acc:0.948]
Epoch [99/120    avg_loss:0.043, val_acc:0.947]
Epoch [100/120    avg_loss:0.046, val_acc:0.948]
Epoch [101/120    avg_loss:0.037, val_acc:0.949]
Epoch [102/120    avg_loss:0.044, val_acc:0.954]
Epoch [103/120    avg_loss:0.045, val_acc:0.951]
Epoch [104/120    avg_loss:0.039, val_acc:0.951]
Epoch [105/120    avg_loss:0.042, val_acc:0.953]
Epoch [106/120    avg_loss:0.036, val_acc:0.953]
Epoch [107/120    avg_loss:0.036, val_acc:0.953]
Epoch [108/120    avg_loss:0.035, val_acc:0.954]
Epoch [109/120    avg_loss:0.038, val_acc:0.951]
Epoch [110/120    avg_loss:0.036, val_acc:0.953]
Epoch [111/120    avg_loss:0.032, val_acc:0.953]
Epoch [112/120    avg_loss:0.048, val_acc:0.955]
Epoch [113/120    avg_loss:0.033, val_acc:0.953]
Epoch [114/120    avg_loss:0.037, val_acc:0.951]
Epoch [115/120    avg_loss:0.033, val_acc:0.953]
Epoch [116/120    avg_loss:0.045, val_acc:0.951]
Epoch [117/120    avg_loss:0.040, val_acc:0.956]
Epoch [118/120    avg_loss:0.036, val_acc:0.955]
Epoch [119/120    avg_loss:0.046, val_acc:0.958]
Epoch [120/120    avg_loss:0.032, val_acc:0.958]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1206    5    0    0    2    0    0    0    3   64    4    0
     0    1    0]
 [   0    0   10  686    2   30    1    0    0    7    0    0   10    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   13    0    0    2    0
     0    0    0]
 [   0    0   58   89    0    5    1    0    0    0  712    0    8    0
     2    0    0]
 [   0    0   30    0    0    2    5    0    0    0    1 2154    4    4
    10    0    0]
 [   0    0    1   36    5   13    0    0    0    0    9    0  465    0
     0    2    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    1    3    0    0
  1128    0    0]
 [   0    0    0    0    0    1   48    0    0    0    0    0    0    0
    96  202    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
93.46341463414635

F1 scores:
[       nan 0.95       0.93127413 0.8761175  0.98383372 0.93159609
 0.95690285 1.         0.99767442 0.57777778 0.88778055 0.97202166
 0.90379009 0.98930481 0.94949495 0.73188406 0.9704142 ]

Kappa:
0.9253965442888845
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3278b249e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.824, val_acc:0.148]
Epoch [2/120    avg_loss:2.696, val_acc:0.261]
Epoch [3/120    avg_loss:2.580, val_acc:0.342]
Epoch [4/120    avg_loss:2.475, val_acc:0.424]
Epoch [5/120    avg_loss:2.417, val_acc:0.409]
Epoch [6/120    avg_loss:2.369, val_acc:0.398]
Epoch [7/120    avg_loss:2.303, val_acc:0.402]
Epoch [8/120    avg_loss:2.251, val_acc:0.440]
Epoch [9/120    avg_loss:2.165, val_acc:0.460]
Epoch [10/120    avg_loss:2.106, val_acc:0.506]
Epoch [11/120    avg_loss:2.041, val_acc:0.493]
Epoch [12/120    avg_loss:1.970, val_acc:0.520]
Epoch [13/120    avg_loss:1.905, val_acc:0.583]
Epoch [14/120    avg_loss:1.826, val_acc:0.559]
Epoch [15/120    avg_loss:1.728, val_acc:0.514]
Epoch [16/120    avg_loss:1.640, val_acc:0.597]
Epoch [17/120    avg_loss:1.523, val_acc:0.530]
Epoch [18/120    avg_loss:1.506, val_acc:0.616]
Epoch [19/120    avg_loss:1.392, val_acc:0.646]
Epoch [20/120    avg_loss:1.317, val_acc:0.678]
Epoch [21/120    avg_loss:1.292, val_acc:0.639]
Epoch [22/120    avg_loss:1.137, val_acc:0.699]
Epoch [23/120    avg_loss:1.158, val_acc:0.698]
Epoch [24/120    avg_loss:1.171, val_acc:0.707]
Epoch [25/120    avg_loss:1.032, val_acc:0.758]
Epoch [26/120    avg_loss:0.928, val_acc:0.733]
Epoch [27/120    avg_loss:0.891, val_acc:0.720]
Epoch [28/120    avg_loss:0.861, val_acc:0.792]
Epoch [29/120    avg_loss:0.758, val_acc:0.850]
Epoch [30/120    avg_loss:0.672, val_acc:0.844]
Epoch [31/120    avg_loss:0.606, val_acc:0.854]
Epoch [32/120    avg_loss:0.576, val_acc:0.863]
Epoch [33/120    avg_loss:0.486, val_acc:0.881]
Epoch [34/120    avg_loss:0.478, val_acc:0.877]
Epoch [35/120    avg_loss:0.421, val_acc:0.851]
Epoch [36/120    avg_loss:0.350, val_acc:0.904]
Epoch [37/120    avg_loss:0.297, val_acc:0.910]
Epoch [38/120    avg_loss:0.267, val_acc:0.923]
Epoch [39/120    avg_loss:0.254, val_acc:0.895]
Epoch [40/120    avg_loss:0.266, val_acc:0.853]
Epoch [41/120    avg_loss:0.293, val_acc:0.894]
Epoch [42/120    avg_loss:0.253, val_acc:0.899]
Epoch [43/120    avg_loss:0.227, val_acc:0.928]
Epoch [44/120    avg_loss:0.180, val_acc:0.897]
Epoch [45/120    avg_loss:0.188, val_acc:0.927]
Epoch [46/120    avg_loss:0.169, val_acc:0.932]
Epoch [47/120    avg_loss:0.152, val_acc:0.944]
Epoch [48/120    avg_loss:0.121, val_acc:0.948]
Epoch [49/120    avg_loss:0.137, val_acc:0.911]
Epoch [50/120    avg_loss:0.132, val_acc:0.943]
Epoch [51/120    avg_loss:0.105, val_acc:0.946]
Epoch [52/120    avg_loss:0.110, val_acc:0.949]
Epoch [53/120    avg_loss:0.098, val_acc:0.930]
Epoch [54/120    avg_loss:0.089, val_acc:0.944]
Epoch [55/120    avg_loss:0.099, val_acc:0.946]
Epoch [56/120    avg_loss:0.097, val_acc:0.946]
Epoch [57/120    avg_loss:0.088, val_acc:0.936]
Epoch [58/120    avg_loss:0.070, val_acc:0.960]
Epoch [59/120    avg_loss:0.085, val_acc:0.950]
Epoch [60/120    avg_loss:0.084, val_acc:0.946]
Epoch [61/120    avg_loss:0.071, val_acc:0.948]
Epoch [62/120    avg_loss:0.062, val_acc:0.948]
Epoch [63/120    avg_loss:0.055, val_acc:0.965]
Epoch [64/120    avg_loss:0.048, val_acc:0.965]
Epoch [65/120    avg_loss:0.059, val_acc:0.943]
Epoch [66/120    avg_loss:0.054, val_acc:0.968]
Epoch [67/120    avg_loss:0.042, val_acc:0.967]
Epoch [68/120    avg_loss:0.039, val_acc:0.968]
Epoch [69/120    avg_loss:0.044, val_acc:0.964]
Epoch [70/120    avg_loss:0.038, val_acc:0.958]
Epoch [71/120    avg_loss:0.039, val_acc:0.971]
Epoch [72/120    avg_loss:0.044, val_acc:0.955]
Epoch [73/120    avg_loss:0.057, val_acc:0.936]
Epoch [74/120    avg_loss:0.046, val_acc:0.965]
Epoch [75/120    avg_loss:0.048, val_acc:0.965]
Epoch [76/120    avg_loss:0.037, val_acc:0.970]
Epoch [77/120    avg_loss:0.031, val_acc:0.972]
Epoch [78/120    avg_loss:0.030, val_acc:0.966]
Epoch [79/120    avg_loss:0.030, val_acc:0.959]
Epoch [80/120    avg_loss:0.026, val_acc:0.970]
Epoch [81/120    avg_loss:0.025, val_acc:0.969]
Epoch [82/120    avg_loss:0.026, val_acc:0.964]
Epoch [83/120    avg_loss:0.022, val_acc:0.975]
Epoch [84/120    avg_loss:0.020, val_acc:0.969]
Epoch [85/120    avg_loss:0.017, val_acc:0.975]
Epoch [86/120    avg_loss:0.017, val_acc:0.976]
Epoch [87/120    avg_loss:0.038, val_acc:0.961]
Epoch [88/120    avg_loss:0.037, val_acc:0.966]
Epoch [89/120    avg_loss:0.046, val_acc:0.974]
Epoch [90/120    avg_loss:0.029, val_acc:0.963]
Epoch [91/120    avg_loss:0.046, val_acc:0.946]
Epoch [92/120    avg_loss:0.045, val_acc:0.964]
Epoch [93/120    avg_loss:0.033, val_acc:0.966]
Epoch [94/120    avg_loss:0.028, val_acc:0.959]
Epoch [95/120    avg_loss:0.022, val_acc:0.958]
Epoch [96/120    avg_loss:0.034, val_acc:0.963]
Epoch [97/120    avg_loss:0.022, val_acc:0.974]
Epoch [98/120    avg_loss:0.020, val_acc:0.974]
Epoch [99/120    avg_loss:0.026, val_acc:0.959]
Epoch [100/120    avg_loss:0.023, val_acc:0.973]
Epoch [101/120    avg_loss:0.016, val_acc:0.977]
Epoch [102/120    avg_loss:0.014, val_acc:0.975]
Epoch [103/120    avg_loss:0.014, val_acc:0.977]
Epoch [104/120    avg_loss:0.012, val_acc:0.977]
Epoch [105/120    avg_loss:0.013, val_acc:0.978]
Epoch [106/120    avg_loss:0.011, val_acc:0.977]
Epoch [107/120    avg_loss:0.013, val_acc:0.977]
Epoch [108/120    avg_loss:0.012, val_acc:0.976]
Epoch [109/120    avg_loss:0.012, val_acc:0.975]
Epoch [110/120    avg_loss:0.012, val_acc:0.977]
Epoch [111/120    avg_loss:0.011, val_acc:0.977]
Epoch [112/120    avg_loss:0.012, val_acc:0.977]
Epoch [113/120    avg_loss:0.013, val_acc:0.973]
Epoch [114/120    avg_loss:0.013, val_acc:0.975]
Epoch [115/120    avg_loss:0.011, val_acc:0.977]
Epoch [116/120    avg_loss:0.011, val_acc:0.978]
Epoch [117/120    avg_loss:0.010, val_acc:0.978]
Epoch [118/120    avg_loss:0.010, val_acc:0.979]
Epoch [119/120    avg_loss:0.012, val_acc:0.977]
Epoch [120/120    avg_loss:0.012, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    0    2    0    0
     0    0    0]
 [   0    0 1253    3    4    0    1    0    0    0    2   22    0    0
     0    0    0]
 [   0    0    5  708   15    4    0    0    0    1    3    5    6    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    2    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   12    0    0    1    0    0    0    0  846   14    2    0
     0    0    0]
 [   0    0   19    4    0    0    0    0    0    0   14 2161   11    0
     0    1    0]
 [   0    0    0    2    0    0    0    0    0    0    0    2  528    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1136    3    0]
 [   0    0    0    0    0    1    4    0    0    0    0    0    0    0
    63  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.39837398373983

F1 scores:
[       nan 0.975      0.97358197 0.9665529  0.95495495 0.98504028
 0.99468489 0.96153846 1.         0.97297297 0.97241379 0.97827071
 0.97506925 1.         0.96969697 0.88571429 0.97619048]

Kappa:
0.970324988024779
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5cc1d68ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.797, val_acc:0.086]
Epoch [2/120    avg_loss:2.691, val_acc:0.123]
Epoch [3/120    avg_loss:2.566, val_acc:0.158]
Epoch [4/120    avg_loss:2.495, val_acc:0.325]
Epoch [5/120    avg_loss:2.393, val_acc:0.429]
Epoch [6/120    avg_loss:2.351, val_acc:0.506]
Epoch [7/120    avg_loss:2.267, val_acc:0.528]
Epoch [8/120    avg_loss:2.200, val_acc:0.524]
Epoch [9/120    avg_loss:2.119, val_acc:0.531]
Epoch [10/120    avg_loss:2.046, val_acc:0.553]
Epoch [11/120    avg_loss:1.932, val_acc:0.556]
Epoch [12/120    avg_loss:1.867, val_acc:0.575]
Epoch [13/120    avg_loss:1.775, val_acc:0.545]
Epoch [14/120    avg_loss:1.694, val_acc:0.575]
Epoch [15/120    avg_loss:1.612, val_acc:0.567]
Epoch [16/120    avg_loss:1.522, val_acc:0.558]
Epoch [17/120    avg_loss:1.443, val_acc:0.608]
Epoch [18/120    avg_loss:1.388, val_acc:0.593]
Epoch [19/120    avg_loss:1.309, val_acc:0.605]
Epoch [20/120    avg_loss:1.235, val_acc:0.599]
Epoch [21/120    avg_loss:1.158, val_acc:0.635]
Epoch [22/120    avg_loss:1.050, val_acc:0.641]
Epoch [23/120    avg_loss:0.996, val_acc:0.694]
Epoch [24/120    avg_loss:0.917, val_acc:0.722]
Epoch [25/120    avg_loss:0.820, val_acc:0.740]
Epoch [26/120    avg_loss:0.791, val_acc:0.728]
Epoch [27/120    avg_loss:0.809, val_acc:0.748]
Epoch [28/120    avg_loss:0.716, val_acc:0.770]
Epoch [29/120    avg_loss:0.632, val_acc:0.801]
Epoch [30/120    avg_loss:0.524, val_acc:0.798]
Epoch [31/120    avg_loss:0.470, val_acc:0.806]
Epoch [32/120    avg_loss:0.481, val_acc:0.828]
Epoch [33/120    avg_loss:0.436, val_acc:0.805]
Epoch [34/120    avg_loss:0.442, val_acc:0.832]
Epoch [35/120    avg_loss:0.395, val_acc:0.842]
Epoch [36/120    avg_loss:0.337, val_acc:0.835]
Epoch [37/120    avg_loss:0.300, val_acc:0.872]
Epoch [38/120    avg_loss:0.297, val_acc:0.826]
Epoch [39/120    avg_loss:0.302, val_acc:0.852]
Epoch [40/120    avg_loss:0.277, val_acc:0.884]
Epoch [41/120    avg_loss:0.264, val_acc:0.878]
Epoch [42/120    avg_loss:0.209, val_acc:0.902]
Epoch [43/120    avg_loss:0.185, val_acc:0.903]
Epoch [44/120    avg_loss:0.168, val_acc:0.904]
Epoch [45/120    avg_loss:0.173, val_acc:0.916]
Epoch [46/120    avg_loss:0.163, val_acc:0.910]
Epoch [47/120    avg_loss:0.147, val_acc:0.924]
Epoch [48/120    avg_loss:0.135, val_acc:0.907]
Epoch [49/120    avg_loss:0.174, val_acc:0.904]
Epoch [50/120    avg_loss:0.195, val_acc:0.902]
Epoch [51/120    avg_loss:0.185, val_acc:0.900]
Epoch [52/120    avg_loss:0.155, val_acc:0.924]
Epoch [53/120    avg_loss:0.119, val_acc:0.930]
Epoch [54/120    avg_loss:0.101, val_acc:0.950]
Epoch [55/120    avg_loss:0.094, val_acc:0.946]
Epoch [56/120    avg_loss:0.089, val_acc:0.932]
Epoch [57/120    avg_loss:0.107, val_acc:0.940]
Epoch [58/120    avg_loss:0.084, val_acc:0.946]
Epoch [59/120    avg_loss:0.124, val_acc:0.935]
Epoch [60/120    avg_loss:0.091, val_acc:0.939]
Epoch [61/120    avg_loss:0.089, val_acc:0.936]
Epoch [62/120    avg_loss:0.115, val_acc:0.945]
Epoch [63/120    avg_loss:0.111, val_acc:0.947]
Epoch [64/120    avg_loss:0.101, val_acc:0.946]
Epoch [65/120    avg_loss:0.109, val_acc:0.927]
Epoch [66/120    avg_loss:0.103, val_acc:0.941]
Epoch [67/120    avg_loss:0.079, val_acc:0.931]
Epoch [68/120    avg_loss:0.076, val_acc:0.953]
Epoch [69/120    avg_loss:0.081, val_acc:0.956]
Epoch [70/120    avg_loss:0.050, val_acc:0.963]
Epoch [71/120    avg_loss:0.044, val_acc:0.960]
Epoch [72/120    avg_loss:0.048, val_acc:0.959]
Epoch [73/120    avg_loss:0.042, val_acc:0.960]
Epoch [74/120    avg_loss:0.042, val_acc:0.956]
Epoch [75/120    avg_loss:0.041, val_acc:0.957]
Epoch [76/120    avg_loss:0.039, val_acc:0.961]
Epoch [77/120    avg_loss:0.039, val_acc:0.960]
Epoch [78/120    avg_loss:0.041, val_acc:0.963]
Epoch [79/120    avg_loss:0.037, val_acc:0.963]
Epoch [80/120    avg_loss:0.037, val_acc:0.958]
Epoch [81/120    avg_loss:0.039, val_acc:0.963]
Epoch [82/120    avg_loss:0.038, val_acc:0.960]
Epoch [83/120    avg_loss:0.039, val_acc:0.964]
Epoch [84/120    avg_loss:0.037, val_acc:0.959]
Epoch [85/120    avg_loss:0.043, val_acc:0.966]
Epoch [86/120    avg_loss:0.031, val_acc:0.967]
Epoch [87/120    avg_loss:0.037, val_acc:0.966]
Epoch [88/120    avg_loss:0.034, val_acc:0.963]
Epoch [89/120    avg_loss:0.031, val_acc:0.964]
Epoch [90/120    avg_loss:0.033, val_acc:0.958]
Epoch [91/120    avg_loss:0.031, val_acc:0.965]
Epoch [92/120    avg_loss:0.039, val_acc:0.958]
Epoch [93/120    avg_loss:0.033, val_acc:0.961]
Epoch [94/120    avg_loss:0.034, val_acc:0.961]
Epoch [95/120    avg_loss:0.034, val_acc:0.965]
Epoch [96/120    avg_loss:0.031, val_acc:0.964]
Epoch [97/120    avg_loss:0.034, val_acc:0.968]
Epoch [98/120    avg_loss:0.033, val_acc:0.966]
Epoch [99/120    avg_loss:0.036, val_acc:0.960]
Epoch [100/120    avg_loss:0.030, val_acc:0.964]
Epoch [101/120    avg_loss:0.030, val_acc:0.963]
Epoch [102/120    avg_loss:0.027, val_acc:0.960]
Epoch [103/120    avg_loss:0.028, val_acc:0.961]
Epoch [104/120    avg_loss:0.034, val_acc:0.963]
Epoch [105/120    avg_loss:0.035, val_acc:0.964]
Epoch [106/120    avg_loss:0.035, val_acc:0.960]
Epoch [107/120    avg_loss:0.030, val_acc:0.961]
Epoch [108/120    avg_loss:0.033, val_acc:0.963]
Epoch [109/120    avg_loss:0.033, val_acc:0.964]
Epoch [110/120    avg_loss:0.028, val_acc:0.964]
Epoch [111/120    avg_loss:0.030, val_acc:0.964]
Epoch [112/120    avg_loss:0.029, val_acc:0.964]
Epoch [113/120    avg_loss:0.029, val_acc:0.963]
Epoch [114/120    avg_loss:0.031, val_acc:0.963]
Epoch [115/120    avg_loss:0.029, val_acc:0.964]
Epoch [116/120    avg_loss:0.034, val_acc:0.965]
Epoch [117/120    avg_loss:0.031, val_acc:0.965]
Epoch [118/120    avg_loss:0.032, val_acc:0.965]
Epoch [119/120    avg_loss:0.028, val_acc:0.965]
Epoch [120/120    avg_loss:0.032, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1217    9    7    0    0    0    0    0   10   42    0    0
     0    0    0]
 [   0    0    2  721    1    0    0    0    0    7    2   10    4    0
     0    0    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    1    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    1    1    0    0    0  846   20    1    0
     0    0    0]
 [   0    0   15    5    0    0    1    1    0    0   36 2137   14    0
     0    0    1]
 [   0    0    0    3    0    1    0    0    0    0    0    2  522    0
     0    3    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    47  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.96476964769647

F1 scores:
[       nan 0.96202532 0.96319747 0.97039031 0.97685185 0.99192618
 0.99695586 0.98039216 0.997669   0.8372093  0.95431472 0.96653098
 0.96846011 1.         0.97371822 0.91047041 0.96470588]

Kappa:
0.9653933950367001
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1b4f2c2b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.814, val_acc:0.072]
Epoch [2/120    avg_loss:2.692, val_acc:0.119]
Epoch [3/120    avg_loss:2.574, val_acc:0.146]
Epoch [4/120    avg_loss:2.478, val_acc:0.277]
Epoch [5/120    avg_loss:2.354, val_acc:0.332]
Epoch [6/120    avg_loss:2.311, val_acc:0.409]
Epoch [7/120    avg_loss:2.233, val_acc:0.399]
Epoch [8/120    avg_loss:2.187, val_acc:0.350]
Epoch [9/120    avg_loss:2.110, val_acc:0.466]
Epoch [10/120    avg_loss:2.024, val_acc:0.515]
Epoch [11/120    avg_loss:1.954, val_acc:0.500]
Epoch [12/120    avg_loss:1.882, val_acc:0.562]
Epoch [13/120    avg_loss:1.802, val_acc:0.565]
Epoch [14/120    avg_loss:1.743, val_acc:0.568]
Epoch [15/120    avg_loss:1.653, val_acc:0.648]
Epoch [16/120    avg_loss:1.555, val_acc:0.641]
Epoch [17/120    avg_loss:1.432, val_acc:0.654]
Epoch [18/120    avg_loss:1.328, val_acc:0.678]
Epoch [19/120    avg_loss:1.248, val_acc:0.698]
Epoch [20/120    avg_loss:1.136, val_acc:0.710]
Epoch [21/120    avg_loss:1.004, val_acc:0.736]
Epoch [22/120    avg_loss:0.964, val_acc:0.732]
Epoch [23/120    avg_loss:0.896, val_acc:0.726]
Epoch [24/120    avg_loss:0.809, val_acc:0.787]
Epoch [25/120    avg_loss:0.685, val_acc:0.811]
Epoch [26/120    avg_loss:0.662, val_acc:0.818]
Epoch [27/120    avg_loss:0.631, val_acc:0.786]
Epoch [28/120    avg_loss:0.536, val_acc:0.835]
Epoch [29/120    avg_loss:0.530, val_acc:0.802]
Epoch [30/120    avg_loss:0.476, val_acc:0.857]
Epoch [31/120    avg_loss:0.424, val_acc:0.816]
Epoch [32/120    avg_loss:0.435, val_acc:0.833]
Epoch [33/120    avg_loss:0.343, val_acc:0.864]
Epoch [34/120    avg_loss:0.324, val_acc:0.883]
Epoch [35/120    avg_loss:0.315, val_acc:0.878]
Epoch [36/120    avg_loss:0.320, val_acc:0.870]
Epoch [37/120    avg_loss:0.284, val_acc:0.865]
Epoch [38/120    avg_loss:0.240, val_acc:0.890]
Epoch [39/120    avg_loss:0.220, val_acc:0.921]
Epoch [40/120    avg_loss:0.209, val_acc:0.910]
Epoch [41/120    avg_loss:0.168, val_acc:0.916]
Epoch [42/120    avg_loss:0.199, val_acc:0.903]
Epoch [43/120    avg_loss:0.336, val_acc:0.875]
Epoch [44/120    avg_loss:0.292, val_acc:0.878]
Epoch [45/120    avg_loss:0.243, val_acc:0.915]
Epoch [46/120    avg_loss:0.193, val_acc:0.887]
Epoch [47/120    avg_loss:0.194, val_acc:0.910]
Epoch [48/120    avg_loss:0.137, val_acc:0.925]
Epoch [49/120    avg_loss:0.122, val_acc:0.936]
Epoch [50/120    avg_loss:0.106, val_acc:0.940]
Epoch [51/120    avg_loss:0.114, val_acc:0.948]
Epoch [52/120    avg_loss:0.107, val_acc:0.942]
Epoch [53/120    avg_loss:0.098, val_acc:0.938]
Epoch [54/120    avg_loss:0.087, val_acc:0.923]
Epoch [55/120    avg_loss:0.092, val_acc:0.936]
Epoch [56/120    avg_loss:0.084, val_acc:0.957]
Epoch [57/120    avg_loss:0.077, val_acc:0.956]
Epoch [58/120    avg_loss:0.086, val_acc:0.946]
Epoch [59/120    avg_loss:0.060, val_acc:0.956]
Epoch [60/120    avg_loss:0.076, val_acc:0.956]
Epoch [61/120    avg_loss:0.075, val_acc:0.952]
Epoch [62/120    avg_loss:0.069, val_acc:0.958]
Epoch [63/120    avg_loss:0.059, val_acc:0.955]
Epoch [64/120    avg_loss:0.068, val_acc:0.964]
Epoch [65/120    avg_loss:0.061, val_acc:0.946]
Epoch [66/120    avg_loss:0.054, val_acc:0.964]
Epoch [67/120    avg_loss:0.054, val_acc:0.957]
Epoch [68/120    avg_loss:0.048, val_acc:0.963]
Epoch [69/120    avg_loss:0.051, val_acc:0.957]
Epoch [70/120    avg_loss:0.045, val_acc:0.960]
Epoch [71/120    avg_loss:0.057, val_acc:0.946]
Epoch [72/120    avg_loss:0.047, val_acc:0.974]
Epoch [73/120    avg_loss:0.043, val_acc:0.969]
Epoch [74/120    avg_loss:0.061, val_acc:0.966]
Epoch [75/120    avg_loss:0.054, val_acc:0.952]
Epoch [76/120    avg_loss:0.054, val_acc:0.966]
Epoch [77/120    avg_loss:0.043, val_acc:0.958]
Epoch [78/120    avg_loss:0.039, val_acc:0.971]
Epoch [79/120    avg_loss:0.032, val_acc:0.969]
Epoch [80/120    avg_loss:0.038, val_acc:0.961]
Epoch [81/120    avg_loss:0.035, val_acc:0.950]
Epoch [82/120    avg_loss:0.028, val_acc:0.967]
Epoch [83/120    avg_loss:0.033, val_acc:0.964]
Epoch [84/120    avg_loss:0.031, val_acc:0.970]
Epoch [85/120    avg_loss:0.035, val_acc:0.966]
Epoch [86/120    avg_loss:0.027, val_acc:0.971]
Epoch [87/120    avg_loss:0.024, val_acc:0.972]
Epoch [88/120    avg_loss:0.020, val_acc:0.971]
Epoch [89/120    avg_loss:0.021, val_acc:0.972]
Epoch [90/120    avg_loss:0.019, val_acc:0.973]
Epoch [91/120    avg_loss:0.019, val_acc:0.971]
Epoch [92/120    avg_loss:0.025, val_acc:0.972]
Epoch [93/120    avg_loss:0.019, val_acc:0.971]
Epoch [94/120    avg_loss:0.019, val_acc:0.969]
Epoch [95/120    avg_loss:0.021, val_acc:0.973]
Epoch [96/120    avg_loss:0.017, val_acc:0.973]
Epoch [97/120    avg_loss:0.017, val_acc:0.973]
Epoch [98/120    avg_loss:0.017, val_acc:0.974]
Epoch [99/120    avg_loss:0.017, val_acc:0.973]
Epoch [100/120    avg_loss:0.016, val_acc:0.973]
Epoch [101/120    avg_loss:0.014, val_acc:0.972]
Epoch [102/120    avg_loss:0.018, val_acc:0.972]
Epoch [103/120    avg_loss:0.020, val_acc:0.973]
Epoch [104/120    avg_loss:0.019, val_acc:0.973]
Epoch [105/120    avg_loss:0.018, val_acc:0.974]
Epoch [106/120    avg_loss:0.015, val_acc:0.973]
Epoch [107/120    avg_loss:0.019, val_acc:0.974]
Epoch [108/120    avg_loss:0.016, val_acc:0.975]
Epoch [109/120    avg_loss:0.020, val_acc:0.975]
Epoch [110/120    avg_loss:0.016, val_acc:0.974]
Epoch [111/120    avg_loss:0.015, val_acc:0.974]
Epoch [112/120    avg_loss:0.016, val_acc:0.975]
Epoch [113/120    avg_loss:0.018, val_acc:0.974]
Epoch [114/120    avg_loss:0.014, val_acc:0.972]
Epoch [115/120    avg_loss:0.016, val_acc:0.973]
Epoch [116/120    avg_loss:0.017, val_acc:0.973]
Epoch [117/120    avg_loss:0.017, val_acc:0.974]
Epoch [118/120    avg_loss:0.018, val_acc:0.974]
Epoch [119/120    avg_loss:0.017, val_acc:0.974]
Epoch [120/120    avg_loss:0.015, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0 1252    2    6    0    1    0    0    0    0   24    0    0
     0    0    0]
 [   0    0    0  718    4    0    0    0    0   12    2    4    6    0
     0    1    0]
 [   0    0    0    2  209    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  432    0    3    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    3    0    0    0    1  843   16    2    0
     0    0    0]
 [   0    0   14    2    0    0    0    0    0    1   11 2167   14    1
     0    0    0]
 [   0    0    0    7    1    0    0    0    0    2    1    2  515    0
     1    2    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1127   11    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    33  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.64769647696477

F1 scores:
[       nan 0.98765432 0.97774307 0.97158322 0.96535797 0.99310345
 0.99393939 0.94339623 1.         0.67924528 0.97344111 0.97943503
 0.95903166 0.99730458 0.98       0.92077728 0.97647059]

Kappa:
0.973182503657586
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff16e2ffb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.845, val_acc:0.097]
Epoch [2/120    avg_loss:2.713, val_acc:0.208]
Epoch [3/120    avg_loss:2.587, val_acc:0.358]
Epoch [4/120    avg_loss:2.482, val_acc:0.442]
Epoch [5/120    avg_loss:2.380, val_acc:0.478]
Epoch [6/120    avg_loss:2.315, val_acc:0.562]
Epoch [7/120    avg_loss:2.231, val_acc:0.590]
Epoch [8/120    avg_loss:2.189, val_acc:0.567]
Epoch [9/120    avg_loss:2.118, val_acc:0.610]
Epoch [10/120    avg_loss:2.073, val_acc:0.605]
Epoch [11/120    avg_loss:2.005, val_acc:0.605]
Epoch [12/120    avg_loss:1.931, val_acc:0.613]
Epoch [13/120    avg_loss:1.837, val_acc:0.588]
Epoch [14/120    avg_loss:1.686, val_acc:0.636]
Epoch [15/120    avg_loss:1.619, val_acc:0.617]
Epoch [16/120    avg_loss:1.491, val_acc:0.631]
Epoch [17/120    avg_loss:1.429, val_acc:0.706]
Epoch [18/120    avg_loss:1.322, val_acc:0.695]
Epoch [19/120    avg_loss:1.202, val_acc:0.724]
Epoch [20/120    avg_loss:1.089, val_acc:0.743]
Epoch [21/120    avg_loss:1.041, val_acc:0.738]
Epoch [22/120    avg_loss:0.954, val_acc:0.769]
Epoch [23/120    avg_loss:0.951, val_acc:0.738]
Epoch [24/120    avg_loss:0.945, val_acc:0.733]
Epoch [25/120    avg_loss:0.798, val_acc:0.777]
Epoch [26/120    avg_loss:0.744, val_acc:0.800]
Epoch [27/120    avg_loss:0.706, val_acc:0.824]
Epoch [28/120    avg_loss:0.610, val_acc:0.826]
Epoch [29/120    avg_loss:0.574, val_acc:0.820]
Epoch [30/120    avg_loss:0.519, val_acc:0.853]
Epoch [31/120    avg_loss:0.546, val_acc:0.773]
Epoch [32/120    avg_loss:0.567, val_acc:0.828]
Epoch [33/120    avg_loss:0.532, val_acc:0.838]
Epoch [34/120    avg_loss:0.425, val_acc:0.820]
Epoch [35/120    avg_loss:0.370, val_acc:0.858]
Epoch [36/120    avg_loss:0.341, val_acc:0.840]
Epoch [37/120    avg_loss:0.286, val_acc:0.870]
Epoch [38/120    avg_loss:0.255, val_acc:0.874]
Epoch [39/120    avg_loss:0.224, val_acc:0.872]
Epoch [40/120    avg_loss:0.228, val_acc:0.902]
Epoch [41/120    avg_loss:0.213, val_acc:0.912]
Epoch [42/120    avg_loss:0.186, val_acc:0.911]
Epoch [43/120    avg_loss:0.176, val_acc:0.901]
Epoch [44/120    avg_loss:0.167, val_acc:0.922]
Epoch [45/120    avg_loss:0.154, val_acc:0.929]
Epoch [46/120    avg_loss:0.135, val_acc:0.926]
Epoch [47/120    avg_loss:0.142, val_acc:0.942]
Epoch [48/120    avg_loss:0.133, val_acc:0.927]
Epoch [49/120    avg_loss:0.118, val_acc:0.922]
Epoch [50/120    avg_loss:0.141, val_acc:0.921]
Epoch [51/120    avg_loss:0.130, val_acc:0.945]
Epoch [52/120    avg_loss:0.125, val_acc:0.941]
Epoch [53/120    avg_loss:0.135, val_acc:0.893]
Epoch [54/120    avg_loss:0.131, val_acc:0.943]
Epoch [55/120    avg_loss:0.104, val_acc:0.942]
Epoch [56/120    avg_loss:0.091, val_acc:0.933]
Epoch [57/120    avg_loss:0.098, val_acc:0.934]
Epoch [58/120    avg_loss:0.093, val_acc:0.947]
Epoch [59/120    avg_loss:0.069, val_acc:0.949]
Epoch [60/120    avg_loss:0.073, val_acc:0.956]
Epoch [61/120    avg_loss:0.075, val_acc:0.943]
Epoch [62/120    avg_loss:0.097, val_acc:0.943]
Epoch [63/120    avg_loss:0.086, val_acc:0.942]
Epoch [64/120    avg_loss:0.090, val_acc:0.950]
Epoch [65/120    avg_loss:0.075, val_acc:0.952]
Epoch [66/120    avg_loss:0.085, val_acc:0.958]
Epoch [67/120    avg_loss:0.063, val_acc:0.958]
Epoch [68/120    avg_loss:0.063, val_acc:0.957]
Epoch [69/120    avg_loss:0.055, val_acc:0.952]
Epoch [70/120    avg_loss:0.054, val_acc:0.956]
Epoch [71/120    avg_loss:0.066, val_acc:0.946]
Epoch [72/120    avg_loss:0.049, val_acc:0.964]
Epoch [73/120    avg_loss:0.042, val_acc:0.953]
Epoch [74/120    avg_loss:0.041, val_acc:0.967]
Epoch [75/120    avg_loss:0.040, val_acc:0.960]
Epoch [76/120    avg_loss:0.055, val_acc:0.958]
Epoch [77/120    avg_loss:0.044, val_acc:0.965]
Epoch [78/120    avg_loss:0.046, val_acc:0.961]
Epoch [79/120    avg_loss:0.037, val_acc:0.961]
Epoch [80/120    avg_loss:0.030, val_acc:0.969]
Epoch [81/120    avg_loss:0.037, val_acc:0.961]
Epoch [82/120    avg_loss:0.030, val_acc:0.974]
Epoch [83/120    avg_loss:0.027, val_acc:0.972]
Epoch [84/120    avg_loss:0.035, val_acc:0.963]
Epoch [85/120    avg_loss:0.035, val_acc:0.959]
Epoch [86/120    avg_loss:0.032, val_acc:0.964]
Epoch [87/120    avg_loss:0.025, val_acc:0.974]
Epoch [88/120    avg_loss:0.022, val_acc:0.972]
Epoch [89/120    avg_loss:0.019, val_acc:0.971]
Epoch [90/120    avg_loss:0.025, val_acc:0.974]
Epoch [91/120    avg_loss:0.026, val_acc:0.966]
Epoch [92/120    avg_loss:0.020, val_acc:0.969]
Epoch [93/120    avg_loss:0.017, val_acc:0.972]
Epoch [94/120    avg_loss:0.019, val_acc:0.969]
Epoch [95/120    avg_loss:0.019, val_acc:0.974]
Epoch [96/120    avg_loss:0.018, val_acc:0.974]
Epoch [97/120    avg_loss:0.017, val_acc:0.978]
Epoch [98/120    avg_loss:0.016, val_acc:0.969]
Epoch [99/120    avg_loss:0.016, val_acc:0.974]
Epoch [100/120    avg_loss:0.017, val_acc:0.974]
Epoch [101/120    avg_loss:0.015, val_acc:0.974]
Epoch [102/120    avg_loss:0.015, val_acc:0.976]
Epoch [103/120    avg_loss:0.019, val_acc:0.973]
Epoch [104/120    avg_loss:0.014, val_acc:0.975]
Epoch [105/120    avg_loss:0.015, val_acc:0.975]
Epoch [106/120    avg_loss:0.013, val_acc:0.976]
Epoch [107/120    avg_loss:0.013, val_acc:0.970]
Epoch [108/120    avg_loss:0.014, val_acc:0.975]
Epoch [109/120    avg_loss:0.013, val_acc:0.974]
Epoch [110/120    avg_loss:0.014, val_acc:0.968]
Epoch [111/120    avg_loss:0.013, val_acc:0.973]
Epoch [112/120    avg_loss:0.011, val_acc:0.974]
Epoch [113/120    avg_loss:0.009, val_acc:0.975]
Epoch [114/120    avg_loss:0.012, val_acc:0.980]
Epoch [115/120    avg_loss:0.013, val_acc:0.981]
Epoch [116/120    avg_loss:0.011, val_acc:0.979]
Epoch [117/120    avg_loss:0.010, val_acc:0.977]
Epoch [118/120    avg_loss:0.010, val_acc:0.978]
Epoch [119/120    avg_loss:0.010, val_acc:0.981]
Epoch [120/120    avg_loss:0.010, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    2    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1248    4    8    2    0    0    0    0    2   21    0    0
     0    0    0]
 [   0    0    2  728    7    1    1    0    0    1    1    5    0    1
     0    0    0]
 [   0    0    0    1  209    0    0    0    0    0    0    2    1    0
     0    0    0]
 [   0    0    0    0    2  433    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    2    0    0    0    0  842   24    0    0
     0    0    0]
 [   0    0    2    0    0    1    1    0    0    0    0 2180   26    0
     0    0    0]
 [   0    0    0    7    0    1    0    0    0    0    3    2  517    0
     3    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1136    3    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    51  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.78861788617886

F1 scores:
[       nan 0.96202532 0.98036135 0.97915266 0.95216401 0.98971429
 0.99619772 1.         1.         0.94444444 0.97679814 0.98087739
 0.95740741 0.99730458 0.9751073  0.91640867 0.98203593]

Kappa:
0.9747710018305268
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f24f5f239b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.844, val_acc:0.078]
Epoch [2/120    avg_loss:2.728, val_acc:0.366]
Epoch [3/120    avg_loss:2.621, val_acc:0.417]
Epoch [4/120    avg_loss:2.534, val_acc:0.420]
Epoch [5/120    avg_loss:2.441, val_acc:0.445]
Epoch [6/120    avg_loss:2.374, val_acc:0.484]
Epoch [7/120    avg_loss:2.289, val_acc:0.491]
Epoch [8/120    avg_loss:2.191, val_acc:0.503]
Epoch [9/120    avg_loss:2.153, val_acc:0.505]
Epoch [10/120    avg_loss:2.068, val_acc:0.565]
Epoch [11/120    avg_loss:2.000, val_acc:0.530]
Epoch [12/120    avg_loss:1.899, val_acc:0.522]
Epoch [13/120    avg_loss:1.824, val_acc:0.592]
Epoch [14/120    avg_loss:1.701, val_acc:0.570]
Epoch [15/120    avg_loss:1.616, val_acc:0.641]
Epoch [16/120    avg_loss:1.440, val_acc:0.634]
Epoch [17/120    avg_loss:1.343, val_acc:0.649]
Epoch [18/120    avg_loss:1.238, val_acc:0.672]
Epoch [19/120    avg_loss:1.161, val_acc:0.682]
Epoch [20/120    avg_loss:1.016, val_acc:0.717]
Epoch [21/120    avg_loss:1.025, val_acc:0.718]
Epoch [22/120    avg_loss:0.939, val_acc:0.692]
Epoch [23/120    avg_loss:0.827, val_acc:0.736]
Epoch [24/120    avg_loss:0.816, val_acc:0.757]
Epoch [25/120    avg_loss:0.704, val_acc:0.778]
Epoch [26/120    avg_loss:0.635, val_acc:0.796]
Epoch [27/120    avg_loss:0.579, val_acc:0.839]
Epoch [28/120    avg_loss:0.516, val_acc:0.827]
Epoch [29/120    avg_loss:0.492, val_acc:0.845]
Epoch [30/120    avg_loss:0.460, val_acc:0.858]
Epoch [31/120    avg_loss:0.396, val_acc:0.861]
Epoch [32/120    avg_loss:0.352, val_acc:0.880]
Epoch [33/120    avg_loss:0.337, val_acc:0.877]
Epoch [34/120    avg_loss:0.333, val_acc:0.867]
Epoch [35/120    avg_loss:0.286, val_acc:0.885]
Epoch [36/120    avg_loss:0.258, val_acc:0.897]
Epoch [37/120    avg_loss:0.246, val_acc:0.914]
Epoch [38/120    avg_loss:0.237, val_acc:0.897]
Epoch [39/120    avg_loss:0.315, val_acc:0.810]
Epoch [40/120    avg_loss:0.416, val_acc:0.850]
Epoch [41/120    avg_loss:0.389, val_acc:0.820]
Epoch [42/120    avg_loss:0.330, val_acc:0.807]
Epoch [43/120    avg_loss:0.260, val_acc:0.859]
Epoch [44/120    avg_loss:0.240, val_acc:0.885]
Epoch [45/120    avg_loss:0.219, val_acc:0.907]
Epoch [46/120    avg_loss:0.201, val_acc:0.920]
Epoch [47/120    avg_loss:0.176, val_acc:0.927]
Epoch [48/120    avg_loss:0.145, val_acc:0.934]
Epoch [49/120    avg_loss:0.119, val_acc:0.943]
Epoch [50/120    avg_loss:0.121, val_acc:0.940]
Epoch [51/120    avg_loss:0.139, val_acc:0.879]
Epoch [52/120    avg_loss:0.128, val_acc:0.930]
Epoch [53/120    avg_loss:0.125, val_acc:0.948]
Epoch [54/120    avg_loss:0.095, val_acc:0.948]
Epoch [55/120    avg_loss:0.085, val_acc:0.942]
Epoch [56/120    avg_loss:0.082, val_acc:0.949]
Epoch [57/120    avg_loss:0.080, val_acc:0.957]
Epoch [58/120    avg_loss:0.070, val_acc:0.955]
Epoch [59/120    avg_loss:0.059, val_acc:0.958]
Epoch [60/120    avg_loss:0.071, val_acc:0.945]
Epoch [61/120    avg_loss:0.085, val_acc:0.958]
Epoch [62/120    avg_loss:0.079, val_acc:0.953]
Epoch [63/120    avg_loss:0.063, val_acc:0.946]
Epoch [64/120    avg_loss:0.076, val_acc:0.921]
Epoch [65/120    avg_loss:0.081, val_acc:0.941]
Epoch [66/120    avg_loss:0.067, val_acc:0.955]
Epoch [67/120    avg_loss:0.055, val_acc:0.963]
Epoch [68/120    avg_loss:0.049, val_acc:0.961]
Epoch [69/120    avg_loss:0.045, val_acc:0.954]
Epoch [70/120    avg_loss:0.041, val_acc:0.959]
Epoch [71/120    avg_loss:0.043, val_acc:0.957]
Epoch [72/120    avg_loss:0.039, val_acc:0.961]
Epoch [73/120    avg_loss:0.040, val_acc:0.948]
Epoch [74/120    avg_loss:0.040, val_acc:0.963]
Epoch [75/120    avg_loss:0.039, val_acc:0.953]
Epoch [76/120    avg_loss:0.078, val_acc:0.960]
Epoch [77/120    avg_loss:0.054, val_acc:0.949]
Epoch [78/120    avg_loss:0.055, val_acc:0.949]
Epoch [79/120    avg_loss:0.046, val_acc:0.954]
Epoch [80/120    avg_loss:0.030, val_acc:0.963]
Epoch [81/120    avg_loss:0.028, val_acc:0.957]
Epoch [82/120    avg_loss:0.030, val_acc:0.965]
Epoch [83/120    avg_loss:0.029, val_acc:0.966]
Epoch [84/120    avg_loss:0.033, val_acc:0.963]
Epoch [85/120    avg_loss:0.035, val_acc:0.961]
Epoch [86/120    avg_loss:0.033, val_acc:0.966]
Epoch [87/120    avg_loss:0.025, val_acc:0.968]
Epoch [88/120    avg_loss:0.021, val_acc:0.968]
Epoch [89/120    avg_loss:0.019, val_acc:0.966]
Epoch [90/120    avg_loss:0.021, val_acc:0.966]
Epoch [91/120    avg_loss:0.018, val_acc:0.965]
Epoch [92/120    avg_loss:0.017, val_acc:0.968]
Epoch [93/120    avg_loss:0.024, val_acc:0.966]
Epoch [94/120    avg_loss:0.027, val_acc:0.960]
Epoch [95/120    avg_loss:0.021, val_acc:0.967]
Epoch [96/120    avg_loss:0.023, val_acc:0.971]
Epoch [97/120    avg_loss:0.017, val_acc:0.971]
Epoch [98/120    avg_loss:0.015, val_acc:0.975]
Epoch [99/120    avg_loss:0.024, val_acc:0.965]
Epoch [100/120    avg_loss:0.024, val_acc:0.969]
Epoch [101/120    avg_loss:0.025, val_acc:0.967]
Epoch [102/120    avg_loss:0.020, val_acc:0.966]
Epoch [103/120    avg_loss:0.015, val_acc:0.968]
Epoch [104/120    avg_loss:0.017, val_acc:0.970]
Epoch [105/120    avg_loss:0.015, val_acc:0.972]
Epoch [106/120    avg_loss:0.012, val_acc:0.969]
Epoch [107/120    avg_loss:0.011, val_acc:0.974]
Epoch [108/120    avg_loss:0.013, val_acc:0.974]
Epoch [109/120    avg_loss:0.013, val_acc:0.969]
Epoch [110/120    avg_loss:0.013, val_acc:0.971]
Epoch [111/120    avg_loss:0.013, val_acc:0.972]
Epoch [112/120    avg_loss:0.013, val_acc:0.972]
Epoch [113/120    avg_loss:0.010, val_acc:0.971]
Epoch [114/120    avg_loss:0.009, val_acc:0.971]
Epoch [115/120    avg_loss:0.011, val_acc:0.971]
Epoch [116/120    avg_loss:0.011, val_acc:0.970]
Epoch [117/120    avg_loss:0.009, val_acc:0.969]
Epoch [118/120    avg_loss:0.010, val_acc:0.970]
Epoch [119/120    avg_loss:0.010, val_acc:0.970]
Epoch [120/120    avg_loss:0.009, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    2    1    0    0
     0    0    0]
 [   0    0 1246    5    4    0    0    0    0    0    5   24    1    0
     0    0    0]
 [   0    0    0  721    2    0    0    0    0    1    1    8   14    0
     0    0    0]
 [   0    0    0    2  207    0    0    0    0    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    2    0    0    0  653    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0  839   31    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    6 2180   22    0
     0    1    0]
 [   0    0    0    7    0    0    0    0    0    0    0    1  522    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1113   26    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    41  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.51761517615176

F1 scores:
[       nan 0.9382716  0.98264984 0.97300945 0.97183099 0.99769585
 0.99618612 1.         0.997669   0.91891892 0.97106481 0.97801705
 0.9499545  1.         0.96993464 0.89705882 0.98245614]

Kappa:
0.97168351273308
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f36a2e5da90>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.832, val_acc:0.034]
Epoch [2/120    avg_loss:2.718, val_acc:0.223]
Epoch [3/120    avg_loss:2.564, val_acc:0.400]
Epoch [4/120    avg_loss:2.491, val_acc:0.426]
Epoch [5/120    avg_loss:2.382, val_acc:0.434]
Epoch [6/120    avg_loss:2.317, val_acc:0.457]
Epoch [7/120    avg_loss:2.252, val_acc:0.486]
Epoch [8/120    avg_loss:2.189, val_acc:0.517]
Epoch [9/120    avg_loss:2.097, val_acc:0.527]
Epoch [10/120    avg_loss:2.008, val_acc:0.552]
Epoch [11/120    avg_loss:1.942, val_acc:0.582]
Epoch [12/120    avg_loss:1.848, val_acc:0.617]
Epoch [13/120    avg_loss:1.721, val_acc:0.605]
Epoch [14/120    avg_loss:1.607, val_acc:0.664]
Epoch [15/120    avg_loss:1.471, val_acc:0.670]
Epoch [16/120    avg_loss:1.395, val_acc:0.690]
Epoch [17/120    avg_loss:1.314, val_acc:0.692]
Epoch [18/120    avg_loss:1.202, val_acc:0.691]
Epoch [19/120    avg_loss:1.054, val_acc:0.726]
Epoch [20/120    avg_loss:1.089, val_acc:0.677]
Epoch [21/120    avg_loss:0.953, val_acc:0.748]
Epoch [22/120    avg_loss:0.862, val_acc:0.767]
Epoch [23/120    avg_loss:0.806, val_acc:0.791]
Epoch [24/120    avg_loss:0.689, val_acc:0.792]
Epoch [25/120    avg_loss:0.642, val_acc:0.795]
Epoch [26/120    avg_loss:0.642, val_acc:0.793]
Epoch [27/120    avg_loss:0.529, val_acc:0.836]
Epoch [28/120    avg_loss:0.512, val_acc:0.797]
Epoch [29/120    avg_loss:0.473, val_acc:0.818]
Epoch [30/120    avg_loss:0.493, val_acc:0.777]
Epoch [31/120    avg_loss:0.431, val_acc:0.838]
Epoch [32/120    avg_loss:0.375, val_acc:0.876]
Epoch [33/120    avg_loss:0.366, val_acc:0.859]
Epoch [34/120    avg_loss:0.329, val_acc:0.882]
Epoch [35/120    avg_loss:0.328, val_acc:0.908]
Epoch [36/120    avg_loss:0.285, val_acc:0.870]
Epoch [37/120    avg_loss:0.248, val_acc:0.920]
Epoch [38/120    avg_loss:0.275, val_acc:0.902]
Epoch [39/120    avg_loss:0.234, val_acc:0.897]
Epoch [40/120    avg_loss:0.232, val_acc:0.928]
Epoch [41/120    avg_loss:0.195, val_acc:0.929]
Epoch [42/120    avg_loss:0.173, val_acc:0.936]
Epoch [43/120    avg_loss:0.171, val_acc:0.936]
Epoch [44/120    avg_loss:0.173, val_acc:0.947]
Epoch [45/120    avg_loss:0.164, val_acc:0.949]
Epoch [46/120    avg_loss:0.124, val_acc:0.953]
Epoch [47/120    avg_loss:0.102, val_acc:0.957]
Epoch [48/120    avg_loss:0.116, val_acc:0.960]
Epoch [49/120    avg_loss:0.111, val_acc:0.946]
Epoch [50/120    avg_loss:0.118, val_acc:0.938]
Epoch [51/120    avg_loss:0.092, val_acc:0.959]
Epoch [52/120    avg_loss:0.190, val_acc:0.944]
Epoch [53/120    avg_loss:0.177, val_acc:0.920]
Epoch [54/120    avg_loss:0.126, val_acc:0.953]
Epoch [55/120    avg_loss:0.112, val_acc:0.954]
Epoch [56/120    avg_loss:0.099, val_acc:0.940]
Epoch [57/120    avg_loss:0.097, val_acc:0.957]
Epoch [58/120    avg_loss:0.082, val_acc:0.942]
Epoch [59/120    avg_loss:0.091, val_acc:0.955]
Epoch [60/120    avg_loss:0.078, val_acc:0.969]
Epoch [61/120    avg_loss:0.061, val_acc:0.958]
Epoch [62/120    avg_loss:0.071, val_acc:0.957]
Epoch [63/120    avg_loss:0.070, val_acc:0.952]
Epoch [64/120    avg_loss:0.070, val_acc:0.951]
Epoch [65/120    avg_loss:0.062, val_acc:0.969]
Epoch [66/120    avg_loss:0.092, val_acc:0.942]
Epoch [67/120    avg_loss:0.075, val_acc:0.949]
Epoch [68/120    avg_loss:0.064, val_acc:0.969]
Epoch [69/120    avg_loss:0.059, val_acc:0.958]
Epoch [70/120    avg_loss:0.055, val_acc:0.965]
Epoch [71/120    avg_loss:0.056, val_acc:0.964]
Epoch [72/120    avg_loss:0.046, val_acc:0.969]
Epoch [73/120    avg_loss:0.035, val_acc:0.960]
Epoch [74/120    avg_loss:0.038, val_acc:0.973]
Epoch [75/120    avg_loss:0.034, val_acc:0.973]
Epoch [76/120    avg_loss:0.030, val_acc:0.976]
Epoch [77/120    avg_loss:0.047, val_acc:0.969]
Epoch [78/120    avg_loss:0.045, val_acc:0.975]
Epoch [79/120    avg_loss:0.057, val_acc:0.956]
Epoch [80/120    avg_loss:0.057, val_acc:0.972]
Epoch [81/120    avg_loss:0.057, val_acc:0.971]
Epoch [82/120    avg_loss:0.041, val_acc:0.965]
Epoch [83/120    avg_loss:0.039, val_acc:0.971]
Epoch [84/120    avg_loss:0.036, val_acc:0.970]
Epoch [85/120    avg_loss:0.028, val_acc:0.968]
Epoch [86/120    avg_loss:0.026, val_acc:0.968]
Epoch [87/120    avg_loss:0.028, val_acc:0.967]
Epoch [88/120    avg_loss:0.022, val_acc:0.975]
Epoch [89/120    avg_loss:0.023, val_acc:0.970]
Epoch [90/120    avg_loss:0.025, val_acc:0.970]
Epoch [91/120    avg_loss:0.019, val_acc:0.971]
Epoch [92/120    avg_loss:0.017, val_acc:0.973]
Epoch [93/120    avg_loss:0.016, val_acc:0.975]
Epoch [94/120    avg_loss:0.019, val_acc:0.975]
Epoch [95/120    avg_loss:0.016, val_acc:0.975]
Epoch [96/120    avg_loss:0.014, val_acc:0.974]
Epoch [97/120    avg_loss:0.015, val_acc:0.973]
Epoch [98/120    avg_loss:0.013, val_acc:0.974]
Epoch [99/120    avg_loss:0.015, val_acc:0.973]
Epoch [100/120    avg_loss:0.016, val_acc:0.973]
Epoch [101/120    avg_loss:0.017, val_acc:0.973]
Epoch [102/120    avg_loss:0.015, val_acc:0.973]
Epoch [103/120    avg_loss:0.014, val_acc:0.973]
Epoch [104/120    avg_loss:0.017, val_acc:0.973]
Epoch [105/120    avg_loss:0.014, val_acc:0.973]
Epoch [106/120    avg_loss:0.017, val_acc:0.973]
Epoch [107/120    avg_loss:0.013, val_acc:0.973]
Epoch [108/120    avg_loss:0.014, val_acc:0.973]
Epoch [109/120    avg_loss:0.014, val_acc:0.973]
Epoch [110/120    avg_loss:0.015, val_acc:0.973]
Epoch [111/120    avg_loss:0.015, val_acc:0.973]
Epoch [112/120    avg_loss:0.015, val_acc:0.974]
Epoch [113/120    avg_loss:0.017, val_acc:0.974]
Epoch [114/120    avg_loss:0.015, val_acc:0.974]
Epoch [115/120    avg_loss:0.013, val_acc:0.974]
Epoch [116/120    avg_loss:0.013, val_acc:0.974]
Epoch [117/120    avg_loss:0.014, val_acc:0.974]
Epoch [118/120    avg_loss:0.015, val_acc:0.974]
Epoch [119/120    avg_loss:0.014, val_acc:0.974]
Epoch [120/120    avg_loss:0.018, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    2    3    0    0    0    0    0    3   16    0    0
     0    0    0]
 [   0    0    0  728    0    0    0    0    0    7    3    6    2    0
     0    1    0]
 [   0    0    0    2  210    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    1    0    0  430    0    1    0    0    0    0    0    0
     2    1    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0  847   22    3    0
     0    0    0]
 [   0    0   15    0    0    0    0    0    0    1   20 2166    6    0
     0    2    0]
 [   0    0    0    8    0    0    0    0    0    0    0    8  514    0
     0    3    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1129    9    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    19  325    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    7    0
     0    0   77]]

Accuracy:
97.99457994579946

F1 scores:
[       nan 0.98765432 0.98285269 0.97915266 0.98591549 0.99421965
 0.99696049 0.98039216 1.         0.77272727 0.96910755 0.97809889
 0.96344892 1.         0.9860262  0.94476744 0.95061728]

Kappa:
0.9771310026218695
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9ceb52cb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.830, val_acc:0.052]
Epoch [2/120    avg_loss:2.743, val_acc:0.165]
Epoch [3/120    avg_loss:2.657, val_acc:0.211]
Epoch [4/120    avg_loss:2.561, val_acc:0.312]
Epoch [5/120    avg_loss:2.458, val_acc:0.402]
Epoch [6/120    avg_loss:2.351, val_acc:0.453]
Epoch [7/120    avg_loss:2.298, val_acc:0.480]
Epoch [8/120    avg_loss:2.180, val_acc:0.500]
Epoch [9/120    avg_loss:2.120, val_acc:0.541]
Epoch [10/120    avg_loss:2.007, val_acc:0.535]
Epoch [11/120    avg_loss:1.917, val_acc:0.550]
Epoch [12/120    avg_loss:1.883, val_acc:0.578]
Epoch [13/120    avg_loss:1.757, val_acc:0.600]
Epoch [14/120    avg_loss:1.678, val_acc:0.644]
Epoch [15/120    avg_loss:1.525, val_acc:0.634]
Epoch [16/120    avg_loss:1.437, val_acc:0.680]
Epoch [17/120    avg_loss:1.311, val_acc:0.708]
Epoch [18/120    avg_loss:1.199, val_acc:0.660]
Epoch [19/120    avg_loss:1.215, val_acc:0.696]
Epoch [20/120    avg_loss:1.134, val_acc:0.736]
Epoch [21/120    avg_loss:0.968, val_acc:0.766]
Epoch [22/120    avg_loss:0.878, val_acc:0.791]
Epoch [23/120    avg_loss:0.794, val_acc:0.806]
Epoch [24/120    avg_loss:0.722, val_acc:0.762]
Epoch [25/120    avg_loss:0.699, val_acc:0.790]
Epoch [26/120    avg_loss:0.655, val_acc:0.739]
Epoch [27/120    avg_loss:0.587, val_acc:0.827]
Epoch [28/120    avg_loss:0.576, val_acc:0.851]
Epoch [29/120    avg_loss:0.444, val_acc:0.809]
Epoch [30/120    avg_loss:0.471, val_acc:0.871]
Epoch [31/120    avg_loss:0.392, val_acc:0.877]
Epoch [32/120    avg_loss:0.364, val_acc:0.895]
Epoch [33/120    avg_loss:0.325, val_acc:0.878]
Epoch [34/120    avg_loss:0.302, val_acc:0.910]
Epoch [35/120    avg_loss:0.289, val_acc:0.887]
Epoch [36/120    avg_loss:0.273, val_acc:0.916]
Epoch [37/120    avg_loss:0.244, val_acc:0.906]
Epoch [38/120    avg_loss:0.188, val_acc:0.925]
Epoch [39/120    avg_loss:0.183, val_acc:0.935]
Epoch [40/120    avg_loss:0.165, val_acc:0.934]
Epoch [41/120    avg_loss:0.176, val_acc:0.916]
Epoch [42/120    avg_loss:0.206, val_acc:0.897]
Epoch [43/120    avg_loss:0.190, val_acc:0.895]
Epoch [44/120    avg_loss:0.163, val_acc:0.934]
Epoch [45/120    avg_loss:0.126, val_acc:0.938]
Epoch [46/120    avg_loss:0.138, val_acc:0.946]
Epoch [47/120    avg_loss:0.113, val_acc:0.946]
Epoch [48/120    avg_loss:0.107, val_acc:0.942]
Epoch [49/120    avg_loss:0.105, val_acc:0.951]
Epoch [50/120    avg_loss:0.101, val_acc:0.944]
Epoch [51/120    avg_loss:0.105, val_acc:0.943]
Epoch [52/120    avg_loss:0.086, val_acc:0.948]
Epoch [53/120    avg_loss:0.075, val_acc:0.952]
Epoch [54/120    avg_loss:0.079, val_acc:0.960]
Epoch [55/120    avg_loss:0.076, val_acc:0.958]
Epoch [56/120    avg_loss:0.101, val_acc:0.952]
Epoch [57/120    avg_loss:0.073, val_acc:0.958]
Epoch [58/120    avg_loss:0.060, val_acc:0.958]
Epoch [59/120    avg_loss:0.065, val_acc:0.964]
Epoch [60/120    avg_loss:0.056, val_acc:0.960]
Epoch [61/120    avg_loss:0.099, val_acc:0.941]
Epoch [62/120    avg_loss:0.070, val_acc:0.947]
Epoch [63/120    avg_loss:0.068, val_acc:0.952]
Epoch [64/120    avg_loss:0.061, val_acc:0.956]
Epoch [65/120    avg_loss:0.061, val_acc:0.952]
Epoch [66/120    avg_loss:0.064, val_acc:0.956]
Epoch [67/120    avg_loss:0.055, val_acc:0.954]
Epoch [68/120    avg_loss:0.062, val_acc:0.970]
Epoch [69/120    avg_loss:0.049, val_acc:0.965]
Epoch [70/120    avg_loss:0.041, val_acc:0.968]
Epoch [71/120    avg_loss:0.037, val_acc:0.976]
Epoch [72/120    avg_loss:0.040, val_acc:0.969]
Epoch [73/120    avg_loss:0.032, val_acc:0.968]
Epoch [74/120    avg_loss:0.031, val_acc:0.969]
Epoch [75/120    avg_loss:0.041, val_acc:0.957]
Epoch [76/120    avg_loss:0.042, val_acc:0.965]
Epoch [77/120    avg_loss:0.042, val_acc:0.970]
Epoch [78/120    avg_loss:0.044, val_acc:0.971]
Epoch [79/120    avg_loss:0.037, val_acc:0.969]
Epoch [80/120    avg_loss:0.029, val_acc:0.976]
Epoch [81/120    avg_loss:0.034, val_acc:0.974]
Epoch [82/120    avg_loss:0.035, val_acc:0.965]
Epoch [83/120    avg_loss:0.035, val_acc:0.968]
Epoch [84/120    avg_loss:0.051, val_acc:0.954]
Epoch [85/120    avg_loss:0.028, val_acc:0.965]
Epoch [86/120    avg_loss:0.025, val_acc:0.970]
Epoch [87/120    avg_loss:0.025, val_acc:0.964]
Epoch [88/120    avg_loss:0.025, val_acc:0.971]
Epoch [89/120    avg_loss:0.026, val_acc:0.972]
Epoch [90/120    avg_loss:0.022, val_acc:0.973]
Epoch [91/120    avg_loss:0.024, val_acc:0.977]
Epoch [92/120    avg_loss:0.016, val_acc:0.976]
Epoch [93/120    avg_loss:0.025, val_acc:0.975]
Epoch [94/120    avg_loss:0.035, val_acc:0.960]
Epoch [95/120    avg_loss:0.041, val_acc:0.973]
Epoch [96/120    avg_loss:0.030, val_acc:0.971]
Epoch [97/120    avg_loss:0.034, val_acc:0.972]
Epoch [98/120    avg_loss:0.027, val_acc:0.976]
Epoch [99/120    avg_loss:0.021, val_acc:0.977]
Epoch [100/120    avg_loss:0.019, val_acc:0.971]
Epoch [101/120    avg_loss:0.017, val_acc:0.974]
Epoch [102/120    avg_loss:0.017, val_acc:0.969]
Epoch [103/120    avg_loss:0.017, val_acc:0.972]
Epoch [104/120    avg_loss:0.024, val_acc:0.971]
Epoch [105/120    avg_loss:0.020, val_acc:0.968]
Epoch [106/120    avg_loss:0.017, val_acc:0.976]
Epoch [107/120    avg_loss:0.016, val_acc:0.976]
Epoch [108/120    avg_loss:0.011, val_acc:0.980]
Epoch [109/120    avg_loss:0.016, val_acc:0.975]
Epoch [110/120    avg_loss:0.012, val_acc:0.978]
Epoch [111/120    avg_loss:0.017, val_acc:0.974]
Epoch [112/120    avg_loss:0.015, val_acc:0.970]
Epoch [113/120    avg_loss:0.016, val_acc:0.975]
Epoch [114/120    avg_loss:0.014, val_acc:0.973]
Epoch [115/120    avg_loss:0.017, val_acc:0.974]
Epoch [116/120    avg_loss:0.014, val_acc:0.969]
Epoch [117/120    avg_loss:0.012, val_acc:0.977]
Epoch [118/120    avg_loss:0.015, val_acc:0.965]
Epoch [119/120    avg_loss:0.015, val_acc:0.972]
Epoch [120/120    avg_loss:0.017, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1258    0    3    0    0    0    0    0    1   23    0    0
     0    0    0]
 [   0    0    0  722   12    0    1    0    0    4    2    3    3    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   15    0    0    0    0    0    0    0  827   32    1    0
     0    0    0]
 [   0    0    5    0    0    1    0    0    0    0    1 2182   21    0
     0    0    0]
 [   0    0    1    5    0    0    0    0    0    0    0    7  517    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    2    0  183
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0    0    0    0    0
  1130    7    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    81  263    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.24661246612466

F1 scores:
[       nan 0.98765432 0.98013245 0.97831978 0.96127563 0.99190751
 0.99620349 1.         1.         0.87179487 0.96951934 0.97847534
 0.95740741 0.99456522 0.95925297 0.84975767 0.97005988]

Kappa:
0.9685651062711778
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f021ee14ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.788, val_acc:0.250]
Epoch [2/120    avg_loss:2.656, val_acc:0.267]
Epoch [3/120    avg_loss:2.576, val_acc:0.278]
Epoch [4/120    avg_loss:2.478, val_acc:0.302]
Epoch [5/120    avg_loss:2.409, val_acc:0.348]
Epoch [6/120    avg_loss:2.345, val_acc:0.418]
Epoch [7/120    avg_loss:2.247, val_acc:0.443]
Epoch [8/120    avg_loss:2.228, val_acc:0.468]
Epoch [9/120    avg_loss:2.136, val_acc:0.498]
Epoch [10/120    avg_loss:2.074, val_acc:0.501]
Epoch [11/120    avg_loss:2.028, val_acc:0.514]
Epoch [12/120    avg_loss:1.956, val_acc:0.481]
Epoch [13/120    avg_loss:1.926, val_acc:0.548]
Epoch [14/120    avg_loss:1.864, val_acc:0.526]
Epoch [15/120    avg_loss:1.793, val_acc:0.528]
Epoch [16/120    avg_loss:1.719, val_acc:0.607]
Epoch [17/120    avg_loss:1.641, val_acc:0.649]
Epoch [18/120    avg_loss:1.523, val_acc:0.672]
Epoch [19/120    avg_loss:1.434, val_acc:0.691]
Epoch [20/120    avg_loss:1.397, val_acc:0.694]
Epoch [21/120    avg_loss:1.374, val_acc:0.708]
Epoch [22/120    avg_loss:1.261, val_acc:0.718]
Epoch [23/120    avg_loss:1.169, val_acc:0.709]
Epoch [24/120    avg_loss:1.151, val_acc:0.723]
Epoch [25/120    avg_loss:1.001, val_acc:0.762]
Epoch [26/120    avg_loss:0.922, val_acc:0.775]
Epoch [27/120    avg_loss:0.934, val_acc:0.691]
Epoch [28/120    avg_loss:0.877, val_acc:0.769]
Epoch [29/120    avg_loss:0.741, val_acc:0.814]
Epoch [30/120    avg_loss:0.658, val_acc:0.811]
Epoch [31/120    avg_loss:0.564, val_acc:0.830]
Epoch [32/120    avg_loss:0.563, val_acc:0.844]
Epoch [33/120    avg_loss:0.559, val_acc:0.839]
Epoch [34/120    avg_loss:0.479, val_acc:0.839]
Epoch [35/120    avg_loss:0.450, val_acc:0.842]
Epoch [36/120    avg_loss:0.387, val_acc:0.884]
Epoch [37/120    avg_loss:0.317, val_acc:0.875]
Epoch [38/120    avg_loss:0.268, val_acc:0.912]
Epoch [39/120    avg_loss:0.246, val_acc:0.907]
Epoch [40/120    avg_loss:0.272, val_acc:0.894]
Epoch [41/120    avg_loss:0.231, val_acc:0.903]
Epoch [42/120    avg_loss:0.211, val_acc:0.905]
Epoch [43/120    avg_loss:0.220, val_acc:0.928]
Epoch [44/120    avg_loss:0.181, val_acc:0.903]
Epoch [45/120    avg_loss:0.178, val_acc:0.924]
Epoch [46/120    avg_loss:0.192, val_acc:0.917]
Epoch [47/120    avg_loss:0.184, val_acc:0.926]
Epoch [48/120    avg_loss:0.148, val_acc:0.942]
Epoch [49/120    avg_loss:0.126, val_acc:0.939]
Epoch [50/120    avg_loss:0.115, val_acc:0.945]
Epoch [51/120    avg_loss:0.114, val_acc:0.946]
Epoch [52/120    avg_loss:0.105, val_acc:0.940]
Epoch [53/120    avg_loss:0.087, val_acc:0.949]
Epoch [54/120    avg_loss:0.114, val_acc:0.947]
Epoch [55/120    avg_loss:0.115, val_acc:0.941]
Epoch [56/120    avg_loss:0.110, val_acc:0.923]
Epoch [57/120    avg_loss:0.102, val_acc:0.952]
Epoch [58/120    avg_loss:0.084, val_acc:0.961]
Epoch [59/120    avg_loss:0.078, val_acc:0.961]
Epoch [60/120    avg_loss:0.083, val_acc:0.934]
Epoch [61/120    avg_loss:0.090, val_acc:0.954]
Epoch [62/120    avg_loss:0.110, val_acc:0.929]
Epoch [63/120    avg_loss:0.274, val_acc:0.917]
Epoch [64/120    avg_loss:0.213, val_acc:0.928]
Epoch [65/120    avg_loss:0.136, val_acc:0.949]
Epoch [66/120    avg_loss:0.098, val_acc:0.939]
Epoch [67/120    avg_loss:0.094, val_acc:0.941]
Epoch [68/120    avg_loss:0.066, val_acc:0.961]
Epoch [69/120    avg_loss:0.066, val_acc:0.965]
Epoch [70/120    avg_loss:0.062, val_acc:0.964]
Epoch [71/120    avg_loss:0.050, val_acc:0.967]
Epoch [72/120    avg_loss:0.052, val_acc:0.967]
Epoch [73/120    avg_loss:0.047, val_acc:0.956]
Epoch [74/120    avg_loss:0.058, val_acc:0.969]
Epoch [75/120    avg_loss:0.055, val_acc:0.963]
Epoch [76/120    avg_loss:0.044, val_acc:0.970]
Epoch [77/120    avg_loss:0.045, val_acc:0.969]
Epoch [78/120    avg_loss:0.044, val_acc:0.967]
Epoch [79/120    avg_loss:0.072, val_acc:0.945]
Epoch [80/120    avg_loss:0.054, val_acc:0.954]
Epoch [81/120    avg_loss:0.054, val_acc:0.954]
Epoch [82/120    avg_loss:0.071, val_acc:0.951]
Epoch [83/120    avg_loss:0.061, val_acc:0.957]
Epoch [84/120    avg_loss:0.118, val_acc:0.943]
Epoch [85/120    avg_loss:0.078, val_acc:0.965]
Epoch [86/120    avg_loss:0.049, val_acc:0.961]
Epoch [87/120    avg_loss:0.043, val_acc:0.963]
Epoch [88/120    avg_loss:0.047, val_acc:0.970]
Epoch [89/120    avg_loss:0.030, val_acc:0.975]
Epoch [90/120    avg_loss:0.029, val_acc:0.975]
Epoch [91/120    avg_loss:0.026, val_acc:0.971]
Epoch [92/120    avg_loss:0.027, val_acc:0.972]
Epoch [93/120    avg_loss:0.022, val_acc:0.966]
Epoch [94/120    avg_loss:0.019, val_acc:0.972]
Epoch [95/120    avg_loss:0.025, val_acc:0.971]
Epoch [96/120    avg_loss:0.023, val_acc:0.978]
Epoch [97/120    avg_loss:0.029, val_acc:0.969]
Epoch [98/120    avg_loss:0.021, val_acc:0.980]
Epoch [99/120    avg_loss:0.024, val_acc:0.960]
Epoch [100/120    avg_loss:0.023, val_acc:0.973]
Epoch [101/120    avg_loss:0.030, val_acc:0.964]
Epoch [102/120    avg_loss:0.036, val_acc:0.968]
Epoch [103/120    avg_loss:0.024, val_acc:0.971]
Epoch [104/120    avg_loss:0.025, val_acc:0.967]
Epoch [105/120    avg_loss:0.032, val_acc:0.972]
Epoch [106/120    avg_loss:0.025, val_acc:0.975]
Epoch [107/120    avg_loss:0.039, val_acc:0.959]
Epoch [108/120    avg_loss:0.022, val_acc:0.972]
Epoch [109/120    avg_loss:0.020, val_acc:0.977]
Epoch [110/120    avg_loss:0.017, val_acc:0.976]
Epoch [111/120    avg_loss:0.018, val_acc:0.974]
Epoch [112/120    avg_loss:0.014, val_acc:0.975]
Epoch [113/120    avg_loss:0.014, val_acc:0.977]
Epoch [114/120    avg_loss:0.011, val_acc:0.976]
Epoch [115/120    avg_loss:0.012, val_acc:0.978]
Epoch [116/120    avg_loss:0.015, val_acc:0.977]
Epoch [117/120    avg_loss:0.010, val_acc:0.978]
Epoch [118/120    avg_loss:0.011, val_acc:0.978]
Epoch [119/120    avg_loss:0.011, val_acc:0.980]
Epoch [120/120    avg_loss:0.011, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    2    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1247    2    5    5    2    0    0    1    1   22    0    0
     0    0    0]
 [   0    0    1  708    6    0    0    0    0    2    1   16   13    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0  848   25    0    0
     0    1    0]
 [   0    0   15    0    0    1    0    0    0    0   19 2161   14    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0    1  529    0
     0    2    1]
 [   0    0    0    0    1    0    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1129    9    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    36  303    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.55013550135502

F1 scores:
[       nan 0.975      0.97842291 0.97052776 0.97025172 0.98514286
 0.99092284 0.98039216 1.         0.89473684 0.97247706 0.9738621
 0.96797804 0.99728997 0.97876029 0.91540785 0.97590361]

Kappa:
0.9720595592605712
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcb0a48ba58>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.821, val_acc:0.070]
Epoch [2/120    avg_loss:2.711, val_acc:0.156]
Epoch [3/120    avg_loss:2.610, val_acc:0.346]
Epoch [4/120    avg_loss:2.524, val_acc:0.431]
Epoch [5/120    avg_loss:2.439, val_acc:0.464]
Epoch [6/120    avg_loss:2.369, val_acc:0.495]
Epoch [7/120    avg_loss:2.289, val_acc:0.516]
Epoch [8/120    avg_loss:2.241, val_acc:0.532]
Epoch [9/120    avg_loss:2.184, val_acc:0.514]
Epoch [10/120    avg_loss:2.092, val_acc:0.524]
Epoch [11/120    avg_loss:2.016, val_acc:0.529]
Epoch [12/120    avg_loss:1.987, val_acc:0.550]
Epoch [13/120    avg_loss:1.911, val_acc:0.582]
Epoch [14/120    avg_loss:1.807, val_acc:0.571]
Epoch [15/120    avg_loss:1.757, val_acc:0.633]
Epoch [16/120    avg_loss:1.637, val_acc:0.649]
Epoch [17/120    avg_loss:1.564, val_acc:0.628]
Epoch [18/120    avg_loss:1.447, val_acc:0.665]
Epoch [19/120    avg_loss:1.340, val_acc:0.620]
Epoch [20/120    avg_loss:1.252, val_acc:0.613]
Epoch [21/120    avg_loss:1.221, val_acc:0.665]
Epoch [22/120    avg_loss:1.065, val_acc:0.699]
Epoch [23/120    avg_loss:0.941, val_acc:0.696]
Epoch [24/120    avg_loss:0.908, val_acc:0.756]
Epoch [25/120    avg_loss:0.847, val_acc:0.703]
Epoch [26/120    avg_loss:0.800, val_acc:0.800]
Epoch [27/120    avg_loss:0.728, val_acc:0.806]
Epoch [28/120    avg_loss:0.686, val_acc:0.814]
Epoch [29/120    avg_loss:0.618, val_acc:0.811]
Epoch [30/120    avg_loss:0.540, val_acc:0.833]
Epoch [31/120    avg_loss:0.533, val_acc:0.845]
Epoch [32/120    avg_loss:0.439, val_acc:0.855]
Epoch [33/120    avg_loss:0.389, val_acc:0.874]
Epoch [34/120    avg_loss:0.378, val_acc:0.861]
Epoch [35/120    avg_loss:0.351, val_acc:0.892]
Epoch [36/120    avg_loss:0.348, val_acc:0.882]
Epoch [37/120    avg_loss:0.333, val_acc:0.894]
Epoch [38/120    avg_loss:0.279, val_acc:0.923]
Epoch [39/120    avg_loss:0.247, val_acc:0.898]
Epoch [40/120    avg_loss:0.248, val_acc:0.921]
Epoch [41/120    avg_loss:0.207, val_acc:0.922]
Epoch [42/120    avg_loss:0.194, val_acc:0.923]
Epoch [43/120    avg_loss:0.180, val_acc:0.931]
Epoch [44/120    avg_loss:0.162, val_acc:0.931]
Epoch [45/120    avg_loss:0.165, val_acc:0.925]
Epoch [46/120    avg_loss:0.194, val_acc:0.895]
Epoch [47/120    avg_loss:0.169, val_acc:0.917]
Epoch [48/120    avg_loss:0.167, val_acc:0.923]
Epoch [49/120    avg_loss:0.208, val_acc:0.919]
Epoch [50/120    avg_loss:0.184, val_acc:0.915]
Epoch [51/120    avg_loss:0.242, val_acc:0.918]
Epoch [52/120    avg_loss:0.154, val_acc:0.934]
Epoch [53/120    avg_loss:0.145, val_acc:0.938]
Epoch [54/120    avg_loss:0.122, val_acc:0.945]
Epoch [55/120    avg_loss:0.106, val_acc:0.948]
Epoch [56/120    avg_loss:0.100, val_acc:0.950]
Epoch [57/120    avg_loss:0.099, val_acc:0.952]
Epoch [58/120    avg_loss:0.091, val_acc:0.940]
Epoch [59/120    avg_loss:0.105, val_acc:0.938]
Epoch [60/120    avg_loss:0.082, val_acc:0.951]
Epoch [61/120    avg_loss:0.079, val_acc:0.938]
Epoch [62/120    avg_loss:0.127, val_acc:0.935]
Epoch [63/120    avg_loss:0.097, val_acc:0.951]
Epoch [64/120    avg_loss:0.082, val_acc:0.960]
Epoch [65/120    avg_loss:0.070, val_acc:0.959]
Epoch [66/120    avg_loss:0.078, val_acc:0.947]
Epoch [67/120    avg_loss:0.078, val_acc:0.947]
Epoch [68/120    avg_loss:0.062, val_acc:0.963]
Epoch [69/120    avg_loss:0.059, val_acc:0.958]
Epoch [70/120    avg_loss:0.060, val_acc:0.957]
Epoch [71/120    avg_loss:0.058, val_acc:0.944]
Epoch [72/120    avg_loss:0.063, val_acc:0.945]
Epoch [73/120    avg_loss:0.069, val_acc:0.957]
Epoch [74/120    avg_loss:0.063, val_acc:0.959]
Epoch [75/120    avg_loss:0.054, val_acc:0.961]
Epoch [76/120    avg_loss:0.069, val_acc:0.949]
Epoch [77/120    avg_loss:0.066, val_acc:0.948]
Epoch [78/120    avg_loss:0.071, val_acc:0.952]
Epoch [79/120    avg_loss:0.056, val_acc:0.960]
Epoch [80/120    avg_loss:0.040, val_acc:0.966]
Epoch [81/120    avg_loss:0.037, val_acc:0.966]
Epoch [82/120    avg_loss:0.050, val_acc:0.957]
Epoch [83/120    avg_loss:0.051, val_acc:0.961]
Epoch [84/120    avg_loss:0.038, val_acc:0.963]
Epoch [85/120    avg_loss:0.041, val_acc:0.957]
Epoch [86/120    avg_loss:0.037, val_acc:0.953]
Epoch [87/120    avg_loss:0.032, val_acc:0.961]
Epoch [88/120    avg_loss:0.026, val_acc:0.961]
Epoch [89/120    avg_loss:0.030, val_acc:0.964]
Epoch [90/120    avg_loss:0.041, val_acc:0.944]
Epoch [91/120    avg_loss:0.056, val_acc:0.957]
Epoch [92/120    avg_loss:0.052, val_acc:0.966]
Epoch [93/120    avg_loss:0.042, val_acc:0.958]
Epoch [94/120    avg_loss:0.044, val_acc:0.963]
Epoch [95/120    avg_loss:0.031, val_acc:0.963]
Epoch [96/120    avg_loss:0.025, val_acc:0.973]
Epoch [97/120    avg_loss:0.021, val_acc:0.974]
Epoch [98/120    avg_loss:0.017, val_acc:0.966]
Epoch [99/120    avg_loss:0.019, val_acc:0.972]
Epoch [100/120    avg_loss:0.020, val_acc:0.968]
Epoch [101/120    avg_loss:0.025, val_acc:0.968]
Epoch [102/120    avg_loss:0.024, val_acc:0.968]
Epoch [103/120    avg_loss:0.020, val_acc:0.965]
Epoch [104/120    avg_loss:0.021, val_acc:0.972]
Epoch [105/120    avg_loss:0.021, val_acc:0.969]
Epoch [106/120    avg_loss:0.031, val_acc:0.968]
Epoch [107/120    avg_loss:0.026, val_acc:0.949]
Epoch [108/120    avg_loss:0.032, val_acc:0.964]
Epoch [109/120    avg_loss:0.026, val_acc:0.968]
Epoch [110/120    avg_loss:0.026, val_acc:0.965]
Epoch [111/120    avg_loss:0.018, val_acc:0.966]
Epoch [112/120    avg_loss:0.018, val_acc:0.969]
Epoch [113/120    avg_loss:0.014, val_acc:0.969]
Epoch [114/120    avg_loss:0.014, val_acc:0.971]
Epoch [115/120    avg_loss:0.015, val_acc:0.969]
Epoch [116/120    avg_loss:0.014, val_acc:0.968]
Epoch [117/120    avg_loss:0.016, val_acc:0.970]
Epoch [118/120    avg_loss:0.015, val_acc:0.969]
Epoch [119/120    avg_loss:0.013, val_acc:0.968]
Epoch [120/120    avg_loss:0.014, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1255    1    3    0    1    0    0    0    6   19    0    0
     0    0    0]
 [   0    0    0  721    7    0    3    0    0    8    3    2    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   16    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  852   20    3    0
     0    0    0]
 [   0    0   13    0    0    0    1    0    0    0   21 2167    8    0
     0    0    0]
 [   0    0    1    1    1    0    0    0    0    0    0    2  524    0
     1    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
  1127   10    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    53  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.5609756097561

F1 scores:
[       nan 0.975      0.98277212 0.98095238 0.97482838 0.99305556
 0.98350825 1.         1.         0.76190476 0.96873223 0.97987791
 0.9739777  1.         0.96946237 0.878125   0.96385542]

Kappa:
0.9721825416029565
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcc0d4ae9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 39048==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.791, val_acc:0.077]
Epoch [2/120    avg_loss:2.648, val_acc:0.167]
Epoch [3/120    avg_loss:2.545, val_acc:0.287]
Epoch [4/120    avg_loss:2.450, val_acc:0.370]
Epoch [5/120    avg_loss:2.362, val_acc:0.423]
Epoch [6/120    avg_loss:2.291, val_acc:0.457]
Epoch [7/120    avg_loss:2.218, val_acc:0.554]
Epoch [8/120    avg_loss:2.154, val_acc:0.550]
Epoch [9/120    avg_loss:2.084, val_acc:0.564]
Epoch [10/120    avg_loss:2.010, val_acc:0.564]
Epoch [11/120    avg_loss:1.954, val_acc:0.564]
Epoch [12/120    avg_loss:1.865, val_acc:0.571]
Epoch [13/120    avg_loss:1.823, val_acc:0.595]
Epoch [14/120    avg_loss:1.710, val_acc:0.600]
Epoch [15/120    avg_loss:1.643, val_acc:0.634]
Epoch [16/120    avg_loss:1.587, val_acc:0.617]
Epoch [17/120    avg_loss:1.507, val_acc:0.670]
Epoch [18/120    avg_loss:1.334, val_acc:0.676]
Epoch [19/120    avg_loss:1.278, val_acc:0.697]
Epoch [20/120    avg_loss:1.171, val_acc:0.716]
Epoch [21/120    avg_loss:1.112, val_acc:0.750]
Epoch [22/120    avg_loss:0.999, val_acc:0.787]
Epoch [23/120    avg_loss:0.828, val_acc:0.781]
Epoch [24/120    avg_loss:0.790, val_acc:0.809]
Epoch [25/120    avg_loss:0.717, val_acc:0.819]
Epoch [26/120    avg_loss:0.644, val_acc:0.828]
Epoch [27/120    avg_loss:0.536, val_acc:0.854]
Epoch [28/120    avg_loss:0.483, val_acc:0.848]
Epoch [29/120    avg_loss:0.480, val_acc:0.847]
Epoch [30/120    avg_loss:0.504, val_acc:0.821]
Epoch [31/120    avg_loss:0.473, val_acc:0.824]
Epoch [32/120    avg_loss:0.462, val_acc:0.858]
Epoch [33/120    avg_loss:0.365, val_acc:0.881]
Epoch [34/120    avg_loss:0.425, val_acc:0.812]
Epoch [35/120    avg_loss:0.375, val_acc:0.880]
Epoch [36/120    avg_loss:0.278, val_acc:0.912]
Epoch [37/120    avg_loss:0.250, val_acc:0.904]
Epoch [38/120    avg_loss:0.239, val_acc:0.902]
Epoch [39/120    avg_loss:0.219, val_acc:0.900]
Epoch [40/120    avg_loss:0.237, val_acc:0.910]
Epoch [41/120    avg_loss:0.210, val_acc:0.884]
Epoch [42/120    avg_loss:0.242, val_acc:0.882]
Epoch [43/120    avg_loss:0.255, val_acc:0.910]
Epoch [44/120    avg_loss:0.200, val_acc:0.875]
Epoch [45/120    avg_loss:0.330, val_acc:0.930]
Epoch [46/120    avg_loss:0.254, val_acc:0.872]
Epoch [47/120    avg_loss:0.215, val_acc:0.930]
Epoch [48/120    avg_loss:0.177, val_acc:0.936]
Epoch [49/120    avg_loss:0.144, val_acc:0.931]
Epoch [50/120    avg_loss:0.140, val_acc:0.929]
Epoch [51/120    avg_loss:0.139, val_acc:0.932]
Epoch [52/120    avg_loss:0.126, val_acc:0.952]
Epoch [53/120    avg_loss:0.126, val_acc:0.948]
Epoch [54/120    avg_loss:0.103, val_acc:0.944]
Epoch [55/120    avg_loss:0.088, val_acc:0.955]
Epoch [56/120    avg_loss:0.081, val_acc:0.949]
Epoch [57/120    avg_loss:0.093, val_acc:0.950]
Epoch [58/120    avg_loss:0.092, val_acc:0.951]
Epoch [59/120    avg_loss:0.087, val_acc:0.955]
Epoch [60/120    avg_loss:0.082, val_acc:0.949]
Epoch [61/120    avg_loss:0.071, val_acc:0.963]
Epoch [62/120    avg_loss:0.086, val_acc:0.953]
Epoch [63/120    avg_loss:0.080, val_acc:0.949]
Epoch [64/120    avg_loss:0.105, val_acc:0.950]
Epoch [65/120    avg_loss:0.063, val_acc:0.955]
Epoch [66/120    avg_loss:0.080, val_acc:0.950]
Epoch [67/120    avg_loss:0.081, val_acc:0.935]
Epoch [68/120    avg_loss:0.061, val_acc:0.966]
Epoch [69/120    avg_loss:0.061, val_acc:0.959]
Epoch [70/120    avg_loss:0.061, val_acc:0.967]
Epoch [71/120    avg_loss:0.046, val_acc:0.977]
Epoch [72/120    avg_loss:0.052, val_acc:0.969]
Epoch [73/120    avg_loss:0.049, val_acc:0.970]
Epoch [74/120    avg_loss:0.042, val_acc:0.970]
Epoch [75/120    avg_loss:0.037, val_acc:0.967]
Epoch [76/120    avg_loss:0.040, val_acc:0.979]
Epoch [77/120    avg_loss:0.037, val_acc:0.973]
Epoch [78/120    avg_loss:0.055, val_acc:0.972]
Epoch [79/120    avg_loss:0.049, val_acc:0.961]
Epoch [80/120    avg_loss:0.075, val_acc:0.964]
Epoch [81/120    avg_loss:0.055, val_acc:0.961]
Epoch [82/120    avg_loss:0.058, val_acc:0.960]
Epoch [83/120    avg_loss:0.052, val_acc:0.961]
Epoch [84/120    avg_loss:0.049, val_acc:0.912]
Epoch [85/120    avg_loss:0.055, val_acc:0.961]
Epoch [86/120    avg_loss:0.037, val_acc:0.968]
Epoch [87/120    avg_loss:0.030, val_acc:0.973]
Epoch [88/120    avg_loss:0.034, val_acc:0.952]
Epoch [89/120    avg_loss:0.036, val_acc:0.968]
Epoch [90/120    avg_loss:0.033, val_acc:0.973]
Epoch [91/120    avg_loss:0.027, val_acc:0.973]
Epoch [92/120    avg_loss:0.018, val_acc:0.974]
Epoch [93/120    avg_loss:0.024, val_acc:0.974]
Epoch [94/120    avg_loss:0.019, val_acc:0.973]
Epoch [95/120    avg_loss:0.020, val_acc:0.977]
Epoch [96/120    avg_loss:0.022, val_acc:0.974]
Epoch [97/120    avg_loss:0.021, val_acc:0.975]
Epoch [98/120    avg_loss:0.018, val_acc:0.974]
Epoch [99/120    avg_loss:0.021, val_acc:0.976]
Epoch [100/120    avg_loss:0.020, val_acc:0.976]
Epoch [101/120    avg_loss:0.019, val_acc:0.977]
Epoch [102/120    avg_loss:0.018, val_acc:0.976]
Epoch [103/120    avg_loss:0.018, val_acc:0.977]
Epoch [104/120    avg_loss:0.020, val_acc:0.976]
Epoch [105/120    avg_loss:0.016, val_acc:0.976]
Epoch [106/120    avg_loss:0.018, val_acc:0.976]
Epoch [107/120    avg_loss:0.021, val_acc:0.976]
Epoch [108/120    avg_loss:0.016, val_acc:0.976]
Epoch [109/120    avg_loss:0.017, val_acc:0.977]
Epoch [110/120    avg_loss:0.018, val_acc:0.976]
Epoch [111/120    avg_loss:0.020, val_acc:0.976]
Epoch [112/120    avg_loss:0.017, val_acc:0.976]
Epoch [113/120    avg_loss:0.018, val_acc:0.976]
Epoch [114/120    avg_loss:0.019, val_acc:0.976]
Epoch [115/120    avg_loss:0.019, val_acc:0.976]
Epoch [116/120    avg_loss:0.023, val_acc:0.976]
Epoch [117/120    avg_loss:0.019, val_acc:0.976]
Epoch [118/120    avg_loss:0.022, val_acc:0.976]
Epoch [119/120    avg_loss:0.016, val_acc:0.976]
Epoch [120/120    avg_loss:0.020, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1256    1    1    0    2    0    0    0    8   17    0    0
     0    0    0]
 [   0    0    0  709    5    0    0    0    0    8    1    8   16    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  851   18    0    0
     0    0    0]
 [   0    0    4    0    0    1    0    0    0    0   21 2162   22    0
     0    0    0]
 [   0    0    0    4    0    1    0    0    0    0    1    5  520    0
     2    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1122   16    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    35  304    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.61517615176152

F1 scores:
[       nan 1.         0.98471188 0.96990424 0.9837587  0.99655568
 0.99017385 1.         1.         0.81818182 0.96869664 0.97805926
 0.94977169 1.         0.97565217 0.91154423 0.97590361]

Kappa:
0.9728117490325765
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6a1c1eaa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.806, val_acc:0.272]
Epoch [2/120    avg_loss:2.682, val_acc:0.277]
Epoch [3/120    avg_loss:2.553, val_acc:0.259]
Epoch [4/120    avg_loss:2.445, val_acc:0.256]
Epoch [5/120    avg_loss:2.364, val_acc:0.263]
Epoch [6/120    avg_loss:2.307, val_acc:0.323]
Epoch [7/120    avg_loss:2.228, val_acc:0.328]
Epoch [8/120    avg_loss:2.149, val_acc:0.340]
Epoch [9/120    avg_loss:2.082, val_acc:0.364]
Epoch [10/120    avg_loss:2.003, val_acc:0.394]
Epoch [11/120    avg_loss:1.930, val_acc:0.465]
Epoch [12/120    avg_loss:1.856, val_acc:0.490]
Epoch [13/120    avg_loss:1.797, val_acc:0.517]
Epoch [14/120    avg_loss:1.654, val_acc:0.536]
Epoch [15/120    avg_loss:1.556, val_acc:0.581]
Epoch [16/120    avg_loss:1.478, val_acc:0.623]
Epoch [17/120    avg_loss:1.379, val_acc:0.671]
Epoch [18/120    avg_loss:1.275, val_acc:0.671]
Epoch [19/120    avg_loss:1.199, val_acc:0.677]
Epoch [20/120    avg_loss:1.111, val_acc:0.708]
Epoch [21/120    avg_loss:1.024, val_acc:0.729]
Epoch [22/120    avg_loss:0.961, val_acc:0.704]
Epoch [23/120    avg_loss:0.881, val_acc:0.764]
Epoch [24/120    avg_loss:0.761, val_acc:0.777]
Epoch [25/120    avg_loss:0.731, val_acc:0.781]
Epoch [26/120    avg_loss:0.726, val_acc:0.797]
Epoch [27/120    avg_loss:0.717, val_acc:0.771]
Epoch [28/120    avg_loss:0.611, val_acc:0.782]
Epoch [29/120    avg_loss:0.542, val_acc:0.851]
Epoch [30/120    avg_loss:0.491, val_acc:0.832]
Epoch [31/120    avg_loss:0.405, val_acc:0.852]
Epoch [32/120    avg_loss:0.391, val_acc:0.880]
Epoch [33/120    avg_loss:0.371, val_acc:0.876]
Epoch [34/120    avg_loss:0.339, val_acc:0.889]
Epoch [35/120    avg_loss:0.290, val_acc:0.873]
Epoch [36/120    avg_loss:0.313, val_acc:0.908]
Epoch [37/120    avg_loss:0.274, val_acc:0.902]
Epoch [38/120    avg_loss:0.260, val_acc:0.904]
Epoch [39/120    avg_loss:0.224, val_acc:0.901]
Epoch [40/120    avg_loss:0.213, val_acc:0.917]
Epoch [41/120    avg_loss:0.190, val_acc:0.901]
Epoch [42/120    avg_loss:0.165, val_acc:0.928]
Epoch [43/120    avg_loss:0.152, val_acc:0.918]
Epoch [44/120    avg_loss:0.144, val_acc:0.896]
Epoch [45/120    avg_loss:0.149, val_acc:0.907]
Epoch [46/120    avg_loss:0.122, val_acc:0.929]
Epoch [47/120    avg_loss:0.135, val_acc:0.934]
Epoch [48/120    avg_loss:0.124, val_acc:0.925]
Epoch [49/120    avg_loss:0.110, val_acc:0.940]
Epoch [50/120    avg_loss:0.098, val_acc:0.945]
Epoch [51/120    avg_loss:0.112, val_acc:0.944]
Epoch [52/120    avg_loss:0.101, val_acc:0.931]
Epoch [53/120    avg_loss:0.096, val_acc:0.919]
Epoch [54/120    avg_loss:0.088, val_acc:0.948]
Epoch [55/120    avg_loss:0.075, val_acc:0.941]
Epoch [56/120    avg_loss:0.075, val_acc:0.950]
Epoch [57/120    avg_loss:0.075, val_acc:0.955]
Epoch [58/120    avg_loss:0.089, val_acc:0.944]
Epoch [59/120    avg_loss:0.081, val_acc:0.947]
Epoch [60/120    avg_loss:0.083, val_acc:0.928]
Epoch [61/120    avg_loss:0.084, val_acc:0.956]
Epoch [62/120    avg_loss:0.058, val_acc:0.953]
Epoch [63/120    avg_loss:0.060, val_acc:0.964]
Epoch [64/120    avg_loss:0.067, val_acc:0.944]
Epoch [65/120    avg_loss:0.076, val_acc:0.959]
Epoch [66/120    avg_loss:0.057, val_acc:0.961]
Epoch [67/120    avg_loss:0.050, val_acc:0.949]
Epoch [68/120    avg_loss:0.042, val_acc:0.965]
Epoch [69/120    avg_loss:0.051, val_acc:0.966]
Epoch [70/120    avg_loss:0.046, val_acc:0.970]
Epoch [71/120    avg_loss:0.042, val_acc:0.968]
Epoch [72/120    avg_loss:0.048, val_acc:0.964]
Epoch [73/120    avg_loss:0.045, val_acc:0.955]
Epoch [74/120    avg_loss:0.043, val_acc:0.964]
Epoch [75/120    avg_loss:0.042, val_acc:0.957]
Epoch [76/120    avg_loss:0.045, val_acc:0.971]
Epoch [77/120    avg_loss:0.033, val_acc:0.967]
Epoch [78/120    avg_loss:0.048, val_acc:0.930]
Epoch [79/120    avg_loss:0.048, val_acc:0.968]
Epoch [80/120    avg_loss:0.047, val_acc:0.952]
Epoch [81/120    avg_loss:0.038, val_acc:0.961]
Epoch [82/120    avg_loss:0.053, val_acc:0.946]
Epoch [83/120    avg_loss:0.056, val_acc:0.964]
Epoch [84/120    avg_loss:0.046, val_acc:0.959]
Epoch [85/120    avg_loss:0.033, val_acc:0.969]
Epoch [86/120    avg_loss:0.031, val_acc:0.966]
Epoch [87/120    avg_loss:0.026, val_acc:0.973]
Epoch [88/120    avg_loss:0.029, val_acc:0.971]
Epoch [89/120    avg_loss:0.026, val_acc:0.969]
Epoch [90/120    avg_loss:0.025, val_acc:0.972]
Epoch [91/120    avg_loss:0.029, val_acc:0.967]
Epoch [92/120    avg_loss:0.050, val_acc:0.973]
Epoch [93/120    avg_loss:0.028, val_acc:0.973]
Epoch [94/120    avg_loss:0.021, val_acc:0.974]
Epoch [95/120    avg_loss:0.024, val_acc:0.972]
Epoch [96/120    avg_loss:0.017, val_acc:0.975]
Epoch [97/120    avg_loss:0.020, val_acc:0.976]
Epoch [98/120    avg_loss:0.019, val_acc:0.975]
Epoch [99/120    avg_loss:0.020, val_acc:0.975]
Epoch [100/120    avg_loss:0.028, val_acc:0.966]
Epoch [101/120    avg_loss:0.044, val_acc:0.970]
Epoch [102/120    avg_loss:0.036, val_acc:0.967]
Epoch [103/120    avg_loss:0.039, val_acc:0.956]
Epoch [104/120    avg_loss:0.036, val_acc:0.965]
Epoch [105/120    avg_loss:0.028, val_acc:0.973]
Epoch [106/120    avg_loss:0.030, val_acc:0.977]
Epoch [107/120    avg_loss:0.026, val_acc:0.976]
Epoch [108/120    avg_loss:0.027, val_acc:0.974]
Epoch [109/120    avg_loss:0.023, val_acc:0.980]
Epoch [110/120    avg_loss:0.021, val_acc:0.978]
Epoch [111/120    avg_loss:0.014, val_acc:0.973]
Epoch [112/120    avg_loss:0.016, val_acc:0.977]
Epoch [113/120    avg_loss:0.015, val_acc:0.978]
Epoch [114/120    avg_loss:0.018, val_acc:0.976]
Epoch [115/120    avg_loss:0.013, val_acc:0.976]
Epoch [116/120    avg_loss:0.013, val_acc:0.977]
Epoch [117/120    avg_loss:0.012, val_acc:0.979]
Epoch [118/120    avg_loss:0.012, val_acc:0.976]
Epoch [119/120    avg_loss:0.011, val_acc:0.979]
Epoch [120/120    avg_loss:0.012, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1272    2    2    0    0    0    0    0    0    9    0    0
     0    0    0]
 [   0    0    0  718    3    0    0    0    0    9    1    8    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   14    0    0    0    1    0    0    0  840   18    0    0
     0    2    0]
 [   0    0   10    0    0    1    1    0    0    1   13 2167   17    0
     0    0    0]
 [   0    0    0    4    0    1    0    0    0    2    9    8  507    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0    0
  1134    4    0]
 [   0    0    0    0    0    1    4    0    0    0    0    0    0    0
    11  331    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.05962059620596

F1 scores:
[       nan 0.975      0.98490128 0.97620666 0.98839907 0.99310345
 0.99469295 1.         0.99883586 0.69387755 0.96662831 0.98032119
 0.94677871 1.         0.99212598 0.96783626 0.96428571]

Kappa:
0.9778771572409309
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb8e8d4ba90>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.794, val_acc:0.060]
Epoch [2/120    avg_loss:2.683, val_acc:0.242]
Epoch [3/120    avg_loss:2.560, val_acc:0.296]
Epoch [4/120    avg_loss:2.473, val_acc:0.380]
Epoch [5/120    avg_loss:2.413, val_acc:0.415]
Epoch [6/120    avg_loss:2.356, val_acc:0.443]
Epoch [7/120    avg_loss:2.280, val_acc:0.472]
Epoch [8/120    avg_loss:2.232, val_acc:0.510]
Epoch [9/120    avg_loss:2.175, val_acc:0.507]
Epoch [10/120    avg_loss:2.107, val_acc:0.537]
Epoch [11/120    avg_loss:2.054, val_acc:0.562]
Epoch [12/120    avg_loss:1.972, val_acc:0.608]
Epoch [13/120    avg_loss:1.869, val_acc:0.623]
Epoch [14/120    avg_loss:1.814, val_acc:0.645]
Epoch [15/120    avg_loss:1.760, val_acc:0.652]
Epoch [16/120    avg_loss:1.707, val_acc:0.638]
Epoch [17/120    avg_loss:1.570, val_acc:0.662]
Epoch [18/120    avg_loss:1.517, val_acc:0.713]
Epoch [19/120    avg_loss:1.397, val_acc:0.704]
Epoch [20/120    avg_loss:1.351, val_acc:0.707]
Epoch [21/120    avg_loss:1.231, val_acc:0.742]
Epoch [22/120    avg_loss:1.164, val_acc:0.757]
Epoch [23/120    avg_loss:1.047, val_acc:0.759]
Epoch [24/120    avg_loss:0.974, val_acc:0.787]
Epoch [25/120    avg_loss:0.882, val_acc:0.805]
Epoch [26/120    avg_loss:0.828, val_acc:0.777]
Epoch [27/120    avg_loss:0.753, val_acc:0.804]
Epoch [28/120    avg_loss:0.781, val_acc:0.795]
Epoch [29/120    avg_loss:0.721, val_acc:0.820]
Epoch [30/120    avg_loss:0.634, val_acc:0.823]
Epoch [31/120    avg_loss:0.611, val_acc:0.797]
Epoch [32/120    avg_loss:0.597, val_acc:0.816]
Epoch [33/120    avg_loss:0.537, val_acc:0.829]
Epoch [34/120    avg_loss:0.479, val_acc:0.880]
Epoch [35/120    avg_loss:0.434, val_acc:0.877]
Epoch [36/120    avg_loss:0.396, val_acc:0.852]
Epoch [37/120    avg_loss:0.397, val_acc:0.895]
Epoch [38/120    avg_loss:0.336, val_acc:0.867]
Epoch [39/120    avg_loss:0.324, val_acc:0.867]
Epoch [40/120    avg_loss:0.335, val_acc:0.858]
Epoch [41/120    avg_loss:0.353, val_acc:0.892]
Epoch [42/120    avg_loss:0.283, val_acc:0.886]
Epoch [43/120    avg_loss:0.251, val_acc:0.905]
Epoch [44/120    avg_loss:0.232, val_acc:0.911]
Epoch [45/120    avg_loss:0.210, val_acc:0.915]
Epoch [46/120    avg_loss:0.189, val_acc:0.927]
Epoch [47/120    avg_loss:0.170, val_acc:0.893]
Epoch [48/120    avg_loss:0.175, val_acc:0.911]
Epoch [49/120    avg_loss:0.144, val_acc:0.922]
Epoch [50/120    avg_loss:0.140, val_acc:0.934]
Epoch [51/120    avg_loss:0.187, val_acc:0.913]
Epoch [52/120    avg_loss:0.152, val_acc:0.930]
Epoch [53/120    avg_loss:0.134, val_acc:0.937]
Epoch [54/120    avg_loss:0.139, val_acc:0.917]
Epoch [55/120    avg_loss:0.133, val_acc:0.938]
Epoch [56/120    avg_loss:0.104, val_acc:0.939]
Epoch [57/120    avg_loss:0.099, val_acc:0.938]
Epoch [58/120    avg_loss:0.098, val_acc:0.941]
Epoch [59/120    avg_loss:0.105, val_acc:0.927]
Epoch [60/120    avg_loss:0.084, val_acc:0.948]
Epoch [61/120    avg_loss:0.064, val_acc:0.951]
Epoch [62/120    avg_loss:0.076, val_acc:0.951]
Epoch [63/120    avg_loss:0.071, val_acc:0.935]
Epoch [64/120    avg_loss:0.069, val_acc:0.934]
Epoch [65/120    avg_loss:0.071, val_acc:0.948]
Epoch [66/120    avg_loss:0.085, val_acc:0.954]
Epoch [67/120    avg_loss:0.054, val_acc:0.955]
Epoch [68/120    avg_loss:0.052, val_acc:0.941]
Epoch [69/120    avg_loss:0.059, val_acc:0.955]
Epoch [70/120    avg_loss:0.046, val_acc:0.945]
Epoch [71/120    avg_loss:0.050, val_acc:0.953]
Epoch [72/120    avg_loss:0.079, val_acc:0.952]
Epoch [73/120    avg_loss:0.066, val_acc:0.916]
Epoch [74/120    avg_loss:0.072, val_acc:0.946]
Epoch [75/120    avg_loss:0.055, val_acc:0.948]
Epoch [76/120    avg_loss:0.066, val_acc:0.935]
Epoch [77/120    avg_loss:0.075, val_acc:0.948]
Epoch [78/120    avg_loss:0.069, val_acc:0.953]
Epoch [79/120    avg_loss:0.066, val_acc:0.935]
Epoch [80/120    avg_loss:0.049, val_acc:0.955]
Epoch [81/120    avg_loss:0.050, val_acc:0.938]
Epoch [82/120    avg_loss:0.037, val_acc:0.964]
Epoch [83/120    avg_loss:0.036, val_acc:0.954]
Epoch [84/120    avg_loss:0.047, val_acc:0.957]
Epoch [85/120    avg_loss:0.031, val_acc:0.962]
Epoch [86/120    avg_loss:0.032, val_acc:0.962]
Epoch [87/120    avg_loss:0.026, val_acc:0.967]
Epoch [88/120    avg_loss:0.033, val_acc:0.948]
Epoch [89/120    avg_loss:0.033, val_acc:0.959]
Epoch [90/120    avg_loss:0.026, val_acc:0.966]
Epoch [91/120    avg_loss:0.030, val_acc:0.962]
Epoch [92/120    avg_loss:0.028, val_acc:0.966]
Epoch [93/120    avg_loss:0.032, val_acc:0.968]
Epoch [94/120    avg_loss:0.029, val_acc:0.966]
Epoch [95/120    avg_loss:0.025, val_acc:0.970]
Epoch [96/120    avg_loss:0.020, val_acc:0.965]
Epoch [97/120    avg_loss:0.019, val_acc:0.964]
Epoch [98/120    avg_loss:0.027, val_acc:0.965]
Epoch [99/120    avg_loss:0.025, val_acc:0.958]
Epoch [100/120    avg_loss:0.027, val_acc:0.964]
Epoch [101/120    avg_loss:0.019, val_acc:0.973]
Epoch [102/120    avg_loss:0.025, val_acc:0.959]
Epoch [103/120    avg_loss:0.022, val_acc:0.961]
Epoch [104/120    avg_loss:0.033, val_acc:0.963]
Epoch [105/120    avg_loss:0.026, val_acc:0.959]
Epoch [106/120    avg_loss:0.015, val_acc:0.972]
Epoch [107/120    avg_loss:0.029, val_acc:0.962]
Epoch [108/120    avg_loss:0.029, val_acc:0.963]
Epoch [109/120    avg_loss:0.023, val_acc:0.963]
Epoch [110/120    avg_loss:0.026, val_acc:0.970]
Epoch [111/120    avg_loss:0.020, val_acc:0.970]
Epoch [112/120    avg_loss:0.022, val_acc:0.970]
Epoch [113/120    avg_loss:0.018, val_acc:0.961]
Epoch [114/120    avg_loss:0.019, val_acc:0.971]
Epoch [115/120    avg_loss:0.013, val_acc:0.972]
Epoch [116/120    avg_loss:0.011, val_acc:0.972]
Epoch [117/120    avg_loss:0.013, val_acc:0.970]
Epoch [118/120    avg_loss:0.009, val_acc:0.968]
Epoch [119/120    avg_loss:0.011, val_acc:0.968]
Epoch [120/120    avg_loss:0.011, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1254    1    6    0    2    0    0    0   10   12    0    0
     0    0    0]
 [   0    0    2  715    1    1    0    0    0   13    1    1   11    1
     0    1    0]
 [   0    0    0    5  208    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    2    0    0    1    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    3    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0   12    0    0    3    7    0    0    0  842    9    0    0
     0    2    0]
 [   0    0    7    0    0    0    4    0    0    0   19 2168   12    0
     0    0    0]
 [   0    0    0    4    0    1    0    0    0    0    5    0  516    0
     2    1    5]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0    0
  1137    1    0]
 [   0    0    0    0    0    0   26    0    0    0    0    0    0    0
    11  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.6260162601626

F1 scores:
[       nan 0.96202532 0.9796875  0.97146739 0.97196262 0.9862069
 0.96592593 1.         0.99883586 0.69387755 0.95954416 0.98388927
 0.95910781 0.99459459 0.9921466  0.93655589 0.95906433]

Kappa:
0.9729418661463641
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f624cf5ea90>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.800, val_acc:0.274]
Epoch [2/120    avg_loss:2.719, val_acc:0.371]
Epoch [3/120    avg_loss:2.621, val_acc:0.404]
Epoch [4/120    avg_loss:2.512, val_acc:0.411]
Epoch [5/120    avg_loss:2.417, val_acc:0.396]
Epoch [6/120    avg_loss:2.336, val_acc:0.402]
Epoch [7/120    avg_loss:2.225, val_acc:0.401]
Epoch [8/120    avg_loss:2.173, val_acc:0.465]
Epoch [9/120    avg_loss:2.140, val_acc:0.513]
Epoch [10/120    avg_loss:2.067, val_acc:0.550]
Epoch [11/120    avg_loss:1.997, val_acc:0.568]
Epoch [12/120    avg_loss:1.945, val_acc:0.588]
Epoch [13/120    avg_loss:1.861, val_acc:0.584]
Epoch [14/120    avg_loss:1.782, val_acc:0.626]
Epoch [15/120    avg_loss:1.717, val_acc:0.620]
Epoch [16/120    avg_loss:1.642, val_acc:0.654]
Epoch [17/120    avg_loss:1.546, val_acc:0.645]
Epoch [18/120    avg_loss:1.435, val_acc:0.654]
Epoch [19/120    avg_loss:1.321, val_acc:0.648]
Epoch [20/120    avg_loss:1.220, val_acc:0.689]
Epoch [21/120    avg_loss:1.151, val_acc:0.763]
Epoch [22/120    avg_loss:1.027, val_acc:0.758]
Epoch [23/120    avg_loss:0.949, val_acc:0.747]
Epoch [24/120    avg_loss:0.893, val_acc:0.785]
Epoch [25/120    avg_loss:0.829, val_acc:0.765]
Epoch [26/120    avg_loss:0.775, val_acc:0.791]
Epoch [27/120    avg_loss:0.650, val_acc:0.791]
Epoch [28/120    avg_loss:0.626, val_acc:0.800]
Epoch [29/120    avg_loss:0.608, val_acc:0.801]
Epoch [30/120    avg_loss:0.537, val_acc:0.828]
Epoch [31/120    avg_loss:0.534, val_acc:0.833]
Epoch [32/120    avg_loss:0.418, val_acc:0.849]
Epoch [33/120    avg_loss:0.387, val_acc:0.877]
Epoch [34/120    avg_loss:0.351, val_acc:0.872]
Epoch [35/120    avg_loss:0.364, val_acc:0.857]
Epoch [36/120    avg_loss:0.421, val_acc:0.864]
Epoch [37/120    avg_loss:0.410, val_acc:0.855]
Epoch [38/120    avg_loss:0.332, val_acc:0.895]
Epoch [39/120    avg_loss:0.319, val_acc:0.878]
Epoch [40/120    avg_loss:0.306, val_acc:0.892]
Epoch [41/120    avg_loss:0.270, val_acc:0.882]
Epoch [42/120    avg_loss:0.267, val_acc:0.901]
Epoch [43/120    avg_loss:0.224, val_acc:0.909]
Epoch [44/120    avg_loss:0.225, val_acc:0.893]
Epoch [45/120    avg_loss:0.309, val_acc:0.909]
Epoch [46/120    avg_loss:0.253, val_acc:0.904]
Epoch [47/120    avg_loss:0.212, val_acc:0.913]
Epoch [48/120    avg_loss:0.170, val_acc:0.920]
Epoch [49/120    avg_loss:0.176, val_acc:0.940]
Epoch [50/120    avg_loss:0.165, val_acc:0.923]
Epoch [51/120    avg_loss:0.150, val_acc:0.901]
Epoch [52/120    avg_loss:0.187, val_acc:0.903]
Epoch [53/120    avg_loss:0.162, val_acc:0.930]
Epoch [54/120    avg_loss:0.179, val_acc:0.924]
Epoch [55/120    avg_loss:0.145, val_acc:0.923]
Epoch [56/120    avg_loss:0.132, val_acc:0.943]
Epoch [57/120    avg_loss:0.110, val_acc:0.939]
Epoch [58/120    avg_loss:0.091, val_acc:0.938]
Epoch [59/120    avg_loss:0.100, val_acc:0.942]
Epoch [60/120    avg_loss:0.101, val_acc:0.959]
Epoch [61/120    avg_loss:0.103, val_acc:0.951]
Epoch [62/120    avg_loss:0.082, val_acc:0.942]
Epoch [63/120    avg_loss:0.084, val_acc:0.936]
Epoch [64/120    avg_loss:0.072, val_acc:0.958]
Epoch [65/120    avg_loss:0.069, val_acc:0.962]
Epoch [66/120    avg_loss:0.062, val_acc:0.955]
Epoch [67/120    avg_loss:0.074, val_acc:0.952]
Epoch [68/120    avg_loss:0.076, val_acc:0.955]
Epoch [69/120    avg_loss:0.066, val_acc:0.959]
Epoch [70/120    avg_loss:0.065, val_acc:0.965]
Epoch [71/120    avg_loss:0.053, val_acc:0.964]
Epoch [72/120    avg_loss:0.052, val_acc:0.953]
Epoch [73/120    avg_loss:0.050, val_acc:0.964]
Epoch [74/120    avg_loss:0.043, val_acc:0.970]
Epoch [75/120    avg_loss:0.054, val_acc:0.967]
Epoch [76/120    avg_loss:0.051, val_acc:0.958]
Epoch [77/120    avg_loss:0.051, val_acc:0.965]
Epoch [78/120    avg_loss:0.054, val_acc:0.955]
Epoch [79/120    avg_loss:0.060, val_acc:0.962]
Epoch [80/120    avg_loss:0.038, val_acc:0.963]
Epoch [81/120    avg_loss:0.032, val_acc:0.968]
Epoch [82/120    avg_loss:0.034, val_acc:0.974]
Epoch [83/120    avg_loss:0.035, val_acc:0.968]
Epoch [84/120    avg_loss:0.041, val_acc:0.958]
Epoch [85/120    avg_loss:0.060, val_acc:0.964]
Epoch [86/120    avg_loss:0.041, val_acc:0.966]
Epoch [87/120    avg_loss:0.038, val_acc:0.970]
Epoch [88/120    avg_loss:0.029, val_acc:0.966]
Epoch [89/120    avg_loss:0.040, val_acc:0.953]
Epoch [90/120    avg_loss:0.046, val_acc:0.962]
Epoch [91/120    avg_loss:0.055, val_acc:0.953]
Epoch [92/120    avg_loss:0.036, val_acc:0.966]
Epoch [93/120    avg_loss:0.028, val_acc:0.967]
Epoch [94/120    avg_loss:0.029, val_acc:0.966]
Epoch [95/120    avg_loss:0.025, val_acc:0.966]
Epoch [96/120    avg_loss:0.023, val_acc:0.968]
Epoch [97/120    avg_loss:0.016, val_acc:0.968]
Epoch [98/120    avg_loss:0.019, val_acc:0.968]
Epoch [99/120    avg_loss:0.016, val_acc:0.971]
Epoch [100/120    avg_loss:0.021, val_acc:0.973]
Epoch [101/120    avg_loss:0.018, val_acc:0.976]
Epoch [102/120    avg_loss:0.016, val_acc:0.973]
Epoch [103/120    avg_loss:0.019, val_acc:0.974]
Epoch [104/120    avg_loss:0.015, val_acc:0.974]
Epoch [105/120    avg_loss:0.014, val_acc:0.974]
Epoch [106/120    avg_loss:0.016, val_acc:0.974]
Epoch [107/120    avg_loss:0.014, val_acc:0.974]
Epoch [108/120    avg_loss:0.016, val_acc:0.974]
Epoch [109/120    avg_loss:0.017, val_acc:0.972]
Epoch [110/120    avg_loss:0.015, val_acc:0.972]
Epoch [111/120    avg_loss:0.018, val_acc:0.973]
Epoch [112/120    avg_loss:0.017, val_acc:0.973]
Epoch [113/120    avg_loss:0.015, val_acc:0.973]
Epoch [114/120    avg_loss:0.015, val_acc:0.975]
Epoch [115/120    avg_loss:0.014, val_acc:0.975]
Epoch [116/120    avg_loss:0.017, val_acc:0.975]
Epoch [117/120    avg_loss:0.017, val_acc:0.975]
Epoch [118/120    avg_loss:0.015, val_acc:0.975]
Epoch [119/120    avg_loss:0.018, val_acc:0.975]
Epoch [120/120    avg_loss:0.016, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1247    1    1    2    3    0    0    1    7   23    0    0
     0    0    0]
 [   0    0    0  707    7    2    2    0    0    8   14    0    7    0
     0    0    0]
 [   0    0    0    0  211    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  428    0    6    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    7    0    0    2    4    0    0    0  842   14    4    0
     1    1    0]
 [   0    0    7    0    0    3    0    0    0    0   26 2158   15    1
     0    0    0]
 [   0    0    0    1    0    1    0    0    0    1    9    0  517    0
     2    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1133    6    0]
 [   0    0    0    0    0    0   18    0    0    0    0    0    0    0
     8  321    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.4959349593496

F1 scores:
[       nan 0.975      0.97957581 0.97115385 0.97685185 0.98052692
 0.97757848 0.89285714 1.         0.75555556 0.94873239 0.97935103
 0.95387454 0.99730458 0.9916849  0.94970414 0.96385542]

Kappa:
0.9714656101400333
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcfecf44a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.812, val_acc:0.244]
Epoch [2/120    avg_loss:2.694, val_acc:0.338]
Epoch [3/120    avg_loss:2.593, val_acc:0.427]
Epoch [4/120    avg_loss:2.489, val_acc:0.445]
Epoch [5/120    avg_loss:2.411, val_acc:0.451]
Epoch [6/120    avg_loss:2.327, val_acc:0.454]
Epoch [7/120    avg_loss:2.276, val_acc:0.449]
Epoch [8/120    avg_loss:2.167, val_acc:0.461]
Epoch [9/120    avg_loss:2.106, val_acc:0.453]
Epoch [10/120    avg_loss:2.024, val_acc:0.498]
Epoch [11/120    avg_loss:1.928, val_acc:0.532]
Epoch [12/120    avg_loss:1.825, val_acc:0.583]
Epoch [13/120    avg_loss:1.731, val_acc:0.552]
Epoch [14/120    avg_loss:1.649, val_acc:0.618]
Epoch [15/120    avg_loss:1.538, val_acc:0.617]
Epoch [16/120    avg_loss:1.432, val_acc:0.652]
Epoch [17/120    avg_loss:1.409, val_acc:0.593]
Epoch [18/120    avg_loss:1.339, val_acc:0.609]
Epoch [19/120    avg_loss:1.226, val_acc:0.634]
Epoch [20/120    avg_loss:1.134, val_acc:0.643]
Epoch [21/120    avg_loss:1.063, val_acc:0.664]
Epoch [22/120    avg_loss:1.033, val_acc:0.655]
Epoch [23/120    avg_loss:0.966, val_acc:0.683]
Epoch [24/120    avg_loss:0.833, val_acc:0.728]
Epoch [25/120    avg_loss:0.763, val_acc:0.738]
Epoch [26/120    avg_loss:0.690, val_acc:0.745]
Epoch [27/120    avg_loss:0.735, val_acc:0.750]
Epoch [28/120    avg_loss:0.725, val_acc:0.747]
Epoch [29/120    avg_loss:0.672, val_acc:0.762]
Epoch [30/120    avg_loss:0.564, val_acc:0.797]
Epoch [31/120    avg_loss:0.497, val_acc:0.799]
Epoch [32/120    avg_loss:0.471, val_acc:0.796]
Epoch [33/120    avg_loss:0.447, val_acc:0.857]
Epoch [34/120    avg_loss:0.407, val_acc:0.842]
Epoch [35/120    avg_loss:0.377, val_acc:0.845]
Epoch [36/120    avg_loss:0.353, val_acc:0.856]
Epoch [37/120    avg_loss:0.342, val_acc:0.852]
Epoch [38/120    avg_loss:0.315, val_acc:0.872]
Epoch [39/120    avg_loss:0.257, val_acc:0.894]
Epoch [40/120    avg_loss:0.268, val_acc:0.890]
Epoch [41/120    avg_loss:0.397, val_acc:0.808]
Epoch [42/120    avg_loss:0.314, val_acc:0.891]
Epoch [43/120    avg_loss:0.259, val_acc:0.902]
Epoch [44/120    avg_loss:0.215, val_acc:0.887]
Epoch [45/120    avg_loss:0.189, val_acc:0.910]
Epoch [46/120    avg_loss:0.171, val_acc:0.925]
Epoch [47/120    avg_loss:0.186, val_acc:0.920]
Epoch [48/120    avg_loss:0.163, val_acc:0.934]
Epoch [49/120    avg_loss:0.155, val_acc:0.928]
Epoch [50/120    avg_loss:0.153, val_acc:0.941]
Epoch [51/120    avg_loss:0.131, val_acc:0.938]
Epoch [52/120    avg_loss:0.127, val_acc:0.943]
Epoch [53/120    avg_loss:0.112, val_acc:0.943]
Epoch [54/120    avg_loss:0.114, val_acc:0.952]
Epoch [55/120    avg_loss:0.109, val_acc:0.958]
Epoch [56/120    avg_loss:0.102, val_acc:0.958]
Epoch [57/120    avg_loss:0.121, val_acc:0.917]
Epoch [58/120    avg_loss:0.141, val_acc:0.917]
Epoch [59/120    avg_loss:0.138, val_acc:0.920]
Epoch [60/120    avg_loss:0.118, val_acc:0.922]
Epoch [61/120    avg_loss:0.113, val_acc:0.943]
Epoch [62/120    avg_loss:0.104, val_acc:0.939]
Epoch [63/120    avg_loss:0.106, val_acc:0.936]
Epoch [64/120    avg_loss:0.102, val_acc:0.945]
Epoch [65/120    avg_loss:0.099, val_acc:0.957]
Epoch [66/120    avg_loss:0.089, val_acc:0.960]
Epoch [67/120    avg_loss:0.076, val_acc:0.952]
Epoch [68/120    avg_loss:0.076, val_acc:0.957]
Epoch [69/120    avg_loss:0.066, val_acc:0.961]
Epoch [70/120    avg_loss:0.067, val_acc:0.967]
Epoch [71/120    avg_loss:0.079, val_acc:0.961]
Epoch [72/120    avg_loss:0.071, val_acc:0.932]
Epoch [73/120    avg_loss:0.063, val_acc:0.959]
Epoch [74/120    avg_loss:0.057, val_acc:0.971]
Epoch [75/120    avg_loss:0.062, val_acc:0.958]
Epoch [76/120    avg_loss:0.074, val_acc:0.941]
Epoch [77/120    avg_loss:0.059, val_acc:0.944]
Epoch [78/120    avg_loss:0.061, val_acc:0.960]
Epoch [79/120    avg_loss:0.045, val_acc:0.961]
Epoch [80/120    avg_loss:0.051, val_acc:0.967]
Epoch [81/120    avg_loss:0.063, val_acc:0.956]
Epoch [82/120    avg_loss:0.049, val_acc:0.970]
Epoch [83/120    avg_loss:0.039, val_acc:0.969]
Epoch [84/120    avg_loss:0.045, val_acc:0.969]
Epoch [85/120    avg_loss:0.048, val_acc:0.960]
Epoch [86/120    avg_loss:0.083, val_acc:0.961]
Epoch [87/120    avg_loss:0.047, val_acc:0.963]
Epoch [88/120    avg_loss:0.038, val_acc:0.967]
Epoch [89/120    avg_loss:0.030, val_acc:0.970]
Epoch [90/120    avg_loss:0.034, val_acc:0.973]
Epoch [91/120    avg_loss:0.032, val_acc:0.973]
Epoch [92/120    avg_loss:0.026, val_acc:0.973]
Epoch [93/120    avg_loss:0.030, val_acc:0.973]
Epoch [94/120    avg_loss:0.031, val_acc:0.975]
Epoch [95/120    avg_loss:0.026, val_acc:0.974]
Epoch [96/120    avg_loss:0.025, val_acc:0.975]
Epoch [97/120    avg_loss:0.031, val_acc:0.975]
Epoch [98/120    avg_loss:0.030, val_acc:0.975]
Epoch [99/120    avg_loss:0.026, val_acc:0.974]
Epoch [100/120    avg_loss:0.033, val_acc:0.975]
Epoch [101/120    avg_loss:0.026, val_acc:0.976]
Epoch [102/120    avg_loss:0.027, val_acc:0.976]
Epoch [103/120    avg_loss:0.025, val_acc:0.974]
Epoch [104/120    avg_loss:0.031, val_acc:0.974]
Epoch [105/120    avg_loss:0.027, val_acc:0.974]
Epoch [106/120    avg_loss:0.022, val_acc:0.976]
Epoch [107/120    avg_loss:0.032, val_acc:0.976]
Epoch [108/120    avg_loss:0.022, val_acc:0.975]
Epoch [109/120    avg_loss:0.024, val_acc:0.975]
Epoch [110/120    avg_loss:0.023, val_acc:0.977]
Epoch [111/120    avg_loss:0.026, val_acc:0.977]
Epoch [112/120    avg_loss:0.025, val_acc:0.976]
Epoch [113/120    avg_loss:0.027, val_acc:0.976]
Epoch [114/120    avg_loss:0.028, val_acc:0.975]
Epoch [115/120    avg_loss:0.026, val_acc:0.978]
Epoch [116/120    avg_loss:0.021, val_acc:0.976]
Epoch [117/120    avg_loss:0.027, val_acc:0.977]
Epoch [118/120    avg_loss:0.025, val_acc:0.977]
Epoch [119/120    avg_loss:0.027, val_acc:0.979]
Epoch [120/120    avg_loss:0.023, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1248    7    6    0    2    0    0    0    6   16    0    0
     0    0    0]
 [   0    0    1  720    2    2    0    0    0    9    1    0   12    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    3    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   14    0    0    0    2    0    0    0  839   19    0    0
     0    1    0]
 [   0    0    9    0    0    0    0    0    0    0   23 2160   17    0
     1    0    0]
 [   0    0    0    1    0    0    0    0    0    0    5    3  521    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1128   10    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    79  245    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
96.84552845528455

F1 scores:
[       nan 0.92682927 0.97614392 0.97627119 0.98156682 0.99310345
 0.97615499 1.         0.99649942 0.77272727 0.95776256 0.9800363
 0.95772059 1.         0.96       0.81260365 0.96428571]

Kappa:
0.9640244017739694
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc4030a6a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.823, val_acc:0.135]
Epoch [2/120    avg_loss:2.735, val_acc:0.270]
Epoch [3/120    avg_loss:2.643, val_acc:0.337]
Epoch [4/120    avg_loss:2.535, val_acc:0.358]
Epoch [5/120    avg_loss:2.438, val_acc:0.387]
Epoch [6/120    avg_loss:2.332, val_acc:0.395]
Epoch [7/120    avg_loss:2.256, val_acc:0.416]
Epoch [8/120    avg_loss:2.160, val_acc:0.454]
Epoch [9/120    avg_loss:2.070, val_acc:0.473]
Epoch [10/120    avg_loss:1.992, val_acc:0.485]
Epoch [11/120    avg_loss:1.913, val_acc:0.489]
Epoch [12/120    avg_loss:1.845, val_acc:0.505]
Epoch [13/120    avg_loss:1.771, val_acc:0.571]
Epoch [14/120    avg_loss:1.683, val_acc:0.553]
Epoch [15/120    avg_loss:1.570, val_acc:0.572]
Epoch [16/120    avg_loss:1.506, val_acc:0.584]
Epoch [17/120    avg_loss:1.417, val_acc:0.604]
Epoch [18/120    avg_loss:1.296, val_acc:0.620]
Epoch [19/120    avg_loss:1.219, val_acc:0.637]
Epoch [20/120    avg_loss:1.167, val_acc:0.620]
Epoch [21/120    avg_loss:1.176, val_acc:0.614]
Epoch [22/120    avg_loss:0.988, val_acc:0.682]
Epoch [23/120    avg_loss:0.871, val_acc:0.716]
Epoch [24/120    avg_loss:0.836, val_acc:0.730]
Epoch [25/120    avg_loss:0.845, val_acc:0.745]
Epoch [26/120    avg_loss:0.692, val_acc:0.768]
Epoch [27/120    avg_loss:0.616, val_acc:0.779]
Epoch [28/120    avg_loss:0.561, val_acc:0.786]
Epoch [29/120    avg_loss:0.540, val_acc:0.809]
Epoch [30/120    avg_loss:0.467, val_acc:0.824]
Epoch [31/120    avg_loss:0.431, val_acc:0.837]
Epoch [32/120    avg_loss:0.396, val_acc:0.832]
Epoch [33/120    avg_loss:0.377, val_acc:0.836]
Epoch [34/120    avg_loss:0.347, val_acc:0.840]
Epoch [35/120    avg_loss:0.334, val_acc:0.840]
Epoch [36/120    avg_loss:0.336, val_acc:0.849]
Epoch [37/120    avg_loss:0.318, val_acc:0.874]
Epoch [38/120    avg_loss:0.289, val_acc:0.860]
Epoch [39/120    avg_loss:0.270, val_acc:0.876]
Epoch [40/120    avg_loss:0.286, val_acc:0.872]
Epoch [41/120    avg_loss:0.225, val_acc:0.910]
Epoch [42/120    avg_loss:0.213, val_acc:0.858]
Epoch [43/120    avg_loss:0.236, val_acc:0.846]
Epoch [44/120    avg_loss:0.245, val_acc:0.888]
Epoch [45/120    avg_loss:0.214, val_acc:0.903]
Epoch [46/120    avg_loss:0.170, val_acc:0.925]
Epoch [47/120    avg_loss:0.183, val_acc:0.899]
Epoch [48/120    avg_loss:0.200, val_acc:0.901]
Epoch [49/120    avg_loss:0.325, val_acc:0.872]
Epoch [50/120    avg_loss:0.280, val_acc:0.902]
Epoch [51/120    avg_loss:0.192, val_acc:0.884]
Epoch [52/120    avg_loss:0.177, val_acc:0.915]
Epoch [53/120    avg_loss:0.159, val_acc:0.920]
Epoch [54/120    avg_loss:0.128, val_acc:0.925]
Epoch [55/120    avg_loss:0.141, val_acc:0.918]
Epoch [56/120    avg_loss:0.132, val_acc:0.913]
Epoch [57/120    avg_loss:0.121, val_acc:0.932]
Epoch [58/120    avg_loss:0.095, val_acc:0.928]
Epoch [59/120    avg_loss:0.085, val_acc:0.945]
Epoch [60/120    avg_loss:0.082, val_acc:0.933]
Epoch [61/120    avg_loss:0.119, val_acc:0.932]
Epoch [62/120    avg_loss:0.104, val_acc:0.939]
Epoch [63/120    avg_loss:0.092, val_acc:0.932]
Epoch [64/120    avg_loss:0.093, val_acc:0.941]
Epoch [65/120    avg_loss:0.070, val_acc:0.950]
Epoch [66/120    avg_loss:0.066, val_acc:0.949]
Epoch [67/120    avg_loss:0.056, val_acc:0.953]
Epoch [68/120    avg_loss:0.058, val_acc:0.946]
Epoch [69/120    avg_loss:0.053, val_acc:0.940]
Epoch [70/120    avg_loss:0.069, val_acc:0.954]
Epoch [71/120    avg_loss:0.063, val_acc:0.946]
Epoch [72/120    avg_loss:0.067, val_acc:0.943]
Epoch [73/120    avg_loss:0.053, val_acc:0.961]
Epoch [74/120    avg_loss:0.065, val_acc:0.932]
Epoch [75/120    avg_loss:0.059, val_acc:0.959]
Epoch [76/120    avg_loss:0.060, val_acc:0.964]
Epoch [77/120    avg_loss:0.044, val_acc:0.953]
Epoch [78/120    avg_loss:0.049, val_acc:0.955]
Epoch [79/120    avg_loss:0.038, val_acc:0.972]
Epoch [80/120    avg_loss:0.050, val_acc:0.957]
Epoch [81/120    avg_loss:0.064, val_acc:0.959]
Epoch [82/120    avg_loss:0.054, val_acc:0.960]
Epoch [83/120    avg_loss:0.040, val_acc:0.966]
Epoch [84/120    avg_loss:0.034, val_acc:0.966]
Epoch [85/120    avg_loss:0.038, val_acc:0.968]
Epoch [86/120    avg_loss:0.027, val_acc:0.972]
Epoch [87/120    avg_loss:0.031, val_acc:0.966]
Epoch [88/120    avg_loss:0.028, val_acc:0.964]
Epoch [89/120    avg_loss:0.028, val_acc:0.976]
Epoch [90/120    avg_loss:0.026, val_acc:0.968]
Epoch [91/120    avg_loss:0.020, val_acc:0.971]
Epoch [92/120    avg_loss:0.023, val_acc:0.968]
Epoch [93/120    avg_loss:0.020, val_acc:0.973]
Epoch [94/120    avg_loss:0.021, val_acc:0.976]
Epoch [95/120    avg_loss:0.017, val_acc:0.975]
Epoch [96/120    avg_loss:0.021, val_acc:0.971]
Epoch [97/120    avg_loss:0.026, val_acc:0.966]
Epoch [98/120    avg_loss:0.023, val_acc:0.970]
Epoch [99/120    avg_loss:0.024, val_acc:0.974]
Epoch [100/120    avg_loss:0.018, val_acc:0.977]
Epoch [101/120    avg_loss:0.019, val_acc:0.965]
Epoch [102/120    avg_loss:0.016, val_acc:0.971]
Epoch [103/120    avg_loss:0.020, val_acc:0.968]
Epoch [104/120    avg_loss:0.024, val_acc:0.974]
Epoch [105/120    avg_loss:0.018, val_acc:0.978]
Epoch [106/120    avg_loss:0.019, val_acc:0.974]
Epoch [107/120    avg_loss:0.015, val_acc:0.978]
Epoch [108/120    avg_loss:0.014, val_acc:0.972]
Epoch [109/120    avg_loss:0.029, val_acc:0.960]
Epoch [110/120    avg_loss:0.027, val_acc:0.955]
Epoch [111/120    avg_loss:0.034, val_acc:0.962]
Epoch [112/120    avg_loss:0.042, val_acc:0.962]
Epoch [113/120    avg_loss:0.033, val_acc:0.961]
Epoch [114/120    avg_loss:0.032, val_acc:0.968]
Epoch [115/120    avg_loss:0.031, val_acc:0.965]
Epoch [116/120    avg_loss:0.022, val_acc:0.968]
Epoch [117/120    avg_loss:0.019, val_acc:0.971]
Epoch [118/120    avg_loss:0.020, val_acc:0.974]
Epoch [119/120    avg_loss:0.016, val_acc:0.973]
Epoch [120/120    avg_loss:0.013, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1243    2    6    1    1    0    0    1    6   25    0    0
     0    0    0]
 [   0    0    0  704    2    4    1    0    0    9    2    6   12    7
     0    0    0]
 [   0    0    0    0  208    0    0    0    0    0    0    0    5    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   13    0    2    0    0
     0    0    0]
 [   0    0   15    0    0    2    2    0    0    0  843   13    0    0
     0    0    0]
 [   0    0   23    0    0    0    2    0    0    4   11 2158   12    0
     0    0    0]
 [   0    0    1    0    0    4    0    0    0    0    3    7  515    0
     2    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0    6    0    0    3    0    0    0    0
    42  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.19241192411924

F1 scores:
[       nan 0.975      0.96844566 0.96902959 0.96969697 0.98524404
 0.98718915 0.98039216 0.9953271  0.54166667 0.96785304 0.97624972
 0.95194085 0.98143236 0.97976754 0.91925466 0.99408284]

Kappa:
0.9679841304251172
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4373e90a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.804, val_acc:0.025]
Epoch [2/120    avg_loss:2.705, val_acc:0.073]
Epoch [3/120    avg_loss:2.627, val_acc:0.265]
Epoch [4/120    avg_loss:2.558, val_acc:0.330]
Epoch [5/120    avg_loss:2.467, val_acc:0.370]
Epoch [6/120    avg_loss:2.401, val_acc:0.450]
Epoch [7/120    avg_loss:2.333, val_acc:0.501]
Epoch [8/120    avg_loss:2.279, val_acc:0.520]
Epoch [9/120    avg_loss:2.211, val_acc:0.514]
Epoch [10/120    avg_loss:2.148, val_acc:0.523]
Epoch [11/120    avg_loss:2.074, val_acc:0.533]
Epoch [12/120    avg_loss:2.056, val_acc:0.523]
Epoch [13/120    avg_loss:1.962, val_acc:0.535]
Epoch [14/120    avg_loss:1.891, val_acc:0.551]
Epoch [15/120    avg_loss:1.840, val_acc:0.563]
Epoch [16/120    avg_loss:1.784, val_acc:0.573]
Epoch [17/120    avg_loss:1.709, val_acc:0.609]
Epoch [18/120    avg_loss:1.618, val_acc:0.587]
Epoch [19/120    avg_loss:1.510, val_acc:0.637]
Epoch [20/120    avg_loss:1.452, val_acc:0.634]
Epoch [21/120    avg_loss:1.379, val_acc:0.647]
Epoch [22/120    avg_loss:1.260, val_acc:0.662]
Epoch [23/120    avg_loss:1.153, val_acc:0.680]
Epoch [24/120    avg_loss:1.041, val_acc:0.709]
Epoch [25/120    avg_loss:0.948, val_acc:0.722]
Epoch [26/120    avg_loss:0.915, val_acc:0.737]
Epoch [27/120    avg_loss:0.802, val_acc:0.764]
Epoch [28/120    avg_loss:0.681, val_acc:0.786]
Epoch [29/120    avg_loss:0.646, val_acc:0.811]
Epoch [30/120    avg_loss:0.624, val_acc:0.787]
Epoch [31/120    avg_loss:0.606, val_acc:0.836]
Epoch [32/120    avg_loss:0.514, val_acc:0.822]
Epoch [33/120    avg_loss:0.483, val_acc:0.833]
Epoch [34/120    avg_loss:0.447, val_acc:0.866]
Epoch [35/120    avg_loss:0.393, val_acc:0.853]
Epoch [36/120    avg_loss:0.348, val_acc:0.870]
Epoch [37/120    avg_loss:0.358, val_acc:0.882]
Epoch [38/120    avg_loss:0.340, val_acc:0.876]
Epoch [39/120    avg_loss:0.314, val_acc:0.877]
Epoch [40/120    avg_loss:0.252, val_acc:0.899]
Epoch [41/120    avg_loss:0.234, val_acc:0.910]
Epoch [42/120    avg_loss:0.232, val_acc:0.924]
Epoch [43/120    avg_loss:0.225, val_acc:0.854]
Epoch [44/120    avg_loss:0.220, val_acc:0.899]
Epoch [45/120    avg_loss:0.177, val_acc:0.924]
Epoch [46/120    avg_loss:0.182, val_acc:0.936]
Epoch [47/120    avg_loss:0.148, val_acc:0.934]
Epoch [48/120    avg_loss:0.147, val_acc:0.936]
Epoch [49/120    avg_loss:0.131, val_acc:0.928]
Epoch [50/120    avg_loss:0.139, val_acc:0.932]
Epoch [51/120    avg_loss:0.270, val_acc:0.808]
Epoch [52/120    avg_loss:0.312, val_acc:0.879]
Epoch [53/120    avg_loss:0.252, val_acc:0.907]
Epoch [54/120    avg_loss:0.193, val_acc:0.917]
Epoch [55/120    avg_loss:0.154, val_acc:0.900]
Epoch [56/120    avg_loss:0.149, val_acc:0.939]
Epoch [57/120    avg_loss:0.116, val_acc:0.950]
Epoch [58/120    avg_loss:0.090, val_acc:0.942]
Epoch [59/120    avg_loss:0.078, val_acc:0.943]
Epoch [60/120    avg_loss:0.080, val_acc:0.954]
Epoch [61/120    avg_loss:0.066, val_acc:0.939]
Epoch [62/120    avg_loss:0.080, val_acc:0.954]
Epoch [63/120    avg_loss:0.062, val_acc:0.961]
Epoch [64/120    avg_loss:0.065, val_acc:0.959]
Epoch [65/120    avg_loss:0.057, val_acc:0.951]
Epoch [66/120    avg_loss:0.058, val_acc:0.950]
Epoch [67/120    avg_loss:0.054, val_acc:0.960]
Epoch [68/120    avg_loss:0.053, val_acc:0.947]
Epoch [69/120    avg_loss:0.048, val_acc:0.968]
Epoch [70/120    avg_loss:0.055, val_acc:0.968]
Epoch [71/120    avg_loss:0.059, val_acc:0.958]
Epoch [72/120    avg_loss:0.048, val_acc:0.960]
Epoch [73/120    avg_loss:0.047, val_acc:0.964]
Epoch [74/120    avg_loss:0.045, val_acc:0.950]
Epoch [75/120    avg_loss:0.054, val_acc:0.948]
Epoch [76/120    avg_loss:0.060, val_acc:0.945]
Epoch [77/120    avg_loss:0.049, val_acc:0.953]
Epoch [78/120    avg_loss:0.046, val_acc:0.950]
Epoch [79/120    avg_loss:0.053, val_acc:0.959]
Epoch [80/120    avg_loss:0.085, val_acc:0.932]
Epoch [81/120    avg_loss:0.082, val_acc:0.949]
Epoch [82/120    avg_loss:0.108, val_acc:0.954]
Epoch [83/120    avg_loss:0.081, val_acc:0.929]
Epoch [84/120    avg_loss:0.066, val_acc:0.957]
Epoch [85/120    avg_loss:0.043, val_acc:0.962]
Epoch [86/120    avg_loss:0.041, val_acc:0.966]
Epoch [87/120    avg_loss:0.029, val_acc:0.970]
Epoch [88/120    avg_loss:0.033, val_acc:0.968]
Epoch [89/120    avg_loss:0.034, val_acc:0.967]
Epoch [90/120    avg_loss:0.031, val_acc:0.971]
Epoch [91/120    avg_loss:0.029, val_acc:0.971]
Epoch [92/120    avg_loss:0.026, val_acc:0.968]
Epoch [93/120    avg_loss:0.026, val_acc:0.970]
Epoch [94/120    avg_loss:0.027, val_acc:0.967]
Epoch [95/120    avg_loss:0.027, val_acc:0.968]
Epoch [96/120    avg_loss:0.034, val_acc:0.967]
Epoch [97/120    avg_loss:0.036, val_acc:0.967]
Epoch [98/120    avg_loss:0.027, val_acc:0.968]
Epoch [99/120    avg_loss:0.023, val_acc:0.968]
Epoch [100/120    avg_loss:0.028, val_acc:0.967]
Epoch [101/120    avg_loss:0.025, val_acc:0.970]
Epoch [102/120    avg_loss:0.026, val_acc:0.967]
Epoch [103/120    avg_loss:0.028, val_acc:0.968]
Epoch [104/120    avg_loss:0.028, val_acc:0.970]
Epoch [105/120    avg_loss:0.022, val_acc:0.970]
Epoch [106/120    avg_loss:0.024, val_acc:0.970]
Epoch [107/120    avg_loss:0.025, val_acc:0.970]
Epoch [108/120    avg_loss:0.023, val_acc:0.970]
Epoch [109/120    avg_loss:0.024, val_acc:0.970]
Epoch [110/120    avg_loss:0.023, val_acc:0.970]
Epoch [111/120    avg_loss:0.022, val_acc:0.970]
Epoch [112/120    avg_loss:0.023, val_acc:0.970]
Epoch [113/120    avg_loss:0.023, val_acc:0.970]
Epoch [114/120    avg_loss:0.023, val_acc:0.970]
Epoch [115/120    avg_loss:0.022, val_acc:0.970]
Epoch [116/120    avg_loss:0.023, val_acc:0.970]
Epoch [117/120    avg_loss:0.023, val_acc:0.970]
Epoch [118/120    avg_loss:0.023, val_acc:0.970]
Epoch [119/120    avg_loss:0.026, val_acc:0.970]
Epoch [120/120    avg_loss:0.020, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1248    3    5    0    0    0    0    1    3   25    0    0
     0    0    0]
 [   0    0    0  719    1    4    0    0    0    4    8    5    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   10    0    0    0    0    0    0  419    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   13    0    0    1    2    0    0    0  851    6    1    0
     0    1    0]
 [   0    0   17    0    0    0    0    0    0    0   18 2157   18    0
     0    0    0]
 [   0    0    0    2    0    1    0    0    0    0    0    2  526    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    5    0    0    0    0    0    0    0
  1126    6    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    38  297    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.47425474254743

F1 scores:
[       nan 0.87912088 0.97385876 0.97756628 0.98611111 0.98858447
 0.98269375 0.98039216 0.98704358 0.87804878 0.96924829 0.97889721
 0.96691176 1.         0.97658283 0.9124424  0.9704142 ]

Kappa:
0.9712090028751817
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8fe3743ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.814, val_acc:0.062]
Epoch [2/120    avg_loss:2.701, val_acc:0.231]
Epoch [3/120    avg_loss:2.593, val_acc:0.300]
Epoch [4/120    avg_loss:2.501, val_acc:0.319]
Epoch [5/120    avg_loss:2.408, val_acc:0.373]
Epoch [6/120    avg_loss:2.324, val_acc:0.412]
Epoch [7/120    avg_loss:2.270, val_acc:0.452]
Epoch [8/120    avg_loss:2.207, val_acc:0.477]
Epoch [9/120    avg_loss:2.164, val_acc:0.478]
Epoch [10/120    avg_loss:2.109, val_acc:0.507]
Epoch [11/120    avg_loss:2.068, val_acc:0.522]
Epoch [12/120    avg_loss:2.003, val_acc:0.501]
Epoch [13/120    avg_loss:1.943, val_acc:0.509]
Epoch [14/120    avg_loss:1.841, val_acc:0.581]
Epoch [15/120    avg_loss:1.805, val_acc:0.570]
Epoch [16/120    avg_loss:1.707, val_acc:0.578]
Epoch [17/120    avg_loss:1.639, val_acc:0.601]
Epoch [18/120    avg_loss:1.569, val_acc:0.651]
Epoch [19/120    avg_loss:1.487, val_acc:0.658]
Epoch [20/120    avg_loss:1.364, val_acc:0.683]
Epoch [21/120    avg_loss:1.294, val_acc:0.706]
Epoch [22/120    avg_loss:1.188, val_acc:0.738]
Epoch [23/120    avg_loss:1.105, val_acc:0.731]
Epoch [24/120    avg_loss:1.054, val_acc:0.769]
Epoch [25/120    avg_loss:0.945, val_acc:0.786]
Epoch [26/120    avg_loss:0.845, val_acc:0.826]
Epoch [27/120    avg_loss:0.744, val_acc:0.808]
Epoch [28/120    avg_loss:0.657, val_acc:0.853]
Epoch [29/120    avg_loss:0.600, val_acc:0.848]
Epoch [30/120    avg_loss:0.558, val_acc:0.802]
Epoch [31/120    avg_loss:0.503, val_acc:0.843]
Epoch [32/120    avg_loss:0.431, val_acc:0.863]
Epoch [33/120    avg_loss:0.376, val_acc:0.878]
Epoch [34/120    avg_loss:0.319, val_acc:0.874]
Epoch [35/120    avg_loss:0.302, val_acc:0.912]
Epoch [36/120    avg_loss:0.285, val_acc:0.897]
Epoch [37/120    avg_loss:0.261, val_acc:0.885]
Epoch [38/120    avg_loss:0.249, val_acc:0.891]
Epoch [39/120    avg_loss:0.238, val_acc:0.906]
Epoch [40/120    avg_loss:0.235, val_acc:0.911]
Epoch [41/120    avg_loss:0.195, val_acc:0.908]
Epoch [42/120    avg_loss:0.171, val_acc:0.914]
Epoch [43/120    avg_loss:0.188, val_acc:0.925]
Epoch [44/120    avg_loss:0.212, val_acc:0.910]
Epoch [45/120    avg_loss:0.196, val_acc:0.914]
Epoch [46/120    avg_loss:0.194, val_acc:0.903]
Epoch [47/120    avg_loss:0.146, val_acc:0.931]
Epoch [48/120    avg_loss:0.138, val_acc:0.933]
Epoch [49/120    avg_loss:0.146, val_acc:0.919]
Epoch [50/120    avg_loss:0.213, val_acc:0.919]
Epoch [51/120    avg_loss:0.174, val_acc:0.918]
Epoch [52/120    avg_loss:0.139, val_acc:0.904]
Epoch [53/120    avg_loss:0.145, val_acc:0.933]
Epoch [54/120    avg_loss:0.109, val_acc:0.931]
Epoch [55/120    avg_loss:0.083, val_acc:0.938]
Epoch [56/120    avg_loss:0.092, val_acc:0.941]
Epoch [57/120    avg_loss:0.088, val_acc:0.944]
Epoch [58/120    avg_loss:0.075, val_acc:0.942]
Epoch [59/120    avg_loss:0.086, val_acc:0.940]
Epoch [60/120    avg_loss:0.080, val_acc:0.932]
Epoch [61/120    avg_loss:0.068, val_acc:0.957]
Epoch [62/120    avg_loss:0.070, val_acc:0.947]
Epoch [63/120    avg_loss:0.070, val_acc:0.953]
Epoch [64/120    avg_loss:0.084, val_acc:0.939]
Epoch [65/120    avg_loss:0.084, val_acc:0.949]
Epoch [66/120    avg_loss:0.071, val_acc:0.958]
Epoch [67/120    avg_loss:0.056, val_acc:0.949]
Epoch [68/120    avg_loss:0.060, val_acc:0.942]
Epoch [69/120    avg_loss:0.059, val_acc:0.956]
Epoch [70/120    avg_loss:0.043, val_acc:0.972]
Epoch [71/120    avg_loss:0.050, val_acc:0.963]
Epoch [72/120    avg_loss:0.059, val_acc:0.938]
Epoch [73/120    avg_loss:0.121, val_acc:0.930]
Epoch [74/120    avg_loss:0.124, val_acc:0.921]
Epoch [75/120    avg_loss:0.075, val_acc:0.942]
Epoch [76/120    avg_loss:0.078, val_acc:0.947]
Epoch [77/120    avg_loss:0.112, val_acc:0.934]
Epoch [78/120    avg_loss:0.073, val_acc:0.950]
Epoch [79/120    avg_loss:0.047, val_acc:0.952]
Epoch [80/120    avg_loss:0.056, val_acc:0.951]
Epoch [81/120    avg_loss:0.072, val_acc:0.926]
Epoch [82/120    avg_loss:0.061, val_acc:0.942]
Epoch [83/120    avg_loss:0.054, val_acc:0.946]
Epoch [84/120    avg_loss:0.045, val_acc:0.953]
Epoch [85/120    avg_loss:0.038, val_acc:0.960]
Epoch [86/120    avg_loss:0.035, val_acc:0.963]
Epoch [87/120    avg_loss:0.023, val_acc:0.963]
Epoch [88/120    avg_loss:0.027, val_acc:0.964]
Epoch [89/120    avg_loss:0.023, val_acc:0.963]
Epoch [90/120    avg_loss:0.025, val_acc:0.961]
Epoch [91/120    avg_loss:0.028, val_acc:0.963]
Epoch [92/120    avg_loss:0.029, val_acc:0.963]
Epoch [93/120    avg_loss:0.027, val_acc:0.961]
Epoch [94/120    avg_loss:0.023, val_acc:0.961]
Epoch [95/120    avg_loss:0.021, val_acc:0.963]
Epoch [96/120    avg_loss:0.027, val_acc:0.964]
Epoch [97/120    avg_loss:0.026, val_acc:0.965]
Epoch [98/120    avg_loss:0.026, val_acc:0.964]
Epoch [99/120    avg_loss:0.022, val_acc:0.964]
Epoch [100/120    avg_loss:0.024, val_acc:0.964]
Epoch [101/120    avg_loss:0.023, val_acc:0.964]
Epoch [102/120    avg_loss:0.019, val_acc:0.964]
Epoch [103/120    avg_loss:0.024, val_acc:0.966]
Epoch [104/120    avg_loss:0.021, val_acc:0.966]
Epoch [105/120    avg_loss:0.021, val_acc:0.966]
Epoch [106/120    avg_loss:0.028, val_acc:0.966]
Epoch [107/120    avg_loss:0.020, val_acc:0.966]
Epoch [108/120    avg_loss:0.019, val_acc:0.966]
Epoch [109/120    avg_loss:0.027, val_acc:0.966]
Epoch [110/120    avg_loss:0.023, val_acc:0.966]
Epoch [111/120    avg_loss:0.021, val_acc:0.966]
Epoch [112/120    avg_loss:0.024, val_acc:0.966]
Epoch [113/120    avg_loss:0.022, val_acc:0.966]
Epoch [114/120    avg_loss:0.026, val_acc:0.966]
Epoch [115/120    avg_loss:0.024, val_acc:0.966]
Epoch [116/120    avg_loss:0.025, val_acc:0.966]
Epoch [117/120    avg_loss:0.022, val_acc:0.966]
Epoch [118/120    avg_loss:0.022, val_acc:0.966]
Epoch [119/120    avg_loss:0.024, val_acc:0.966]
Epoch [120/120    avg_loss:0.023, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1246    2    5    0    1    0    0    0    6   25    0    0
     0    0    0]
 [   0    0    0  700   12    6    0    0    0    7    1    2   19    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    2    0    1    0    1    0    0
     9    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  421    0    0    0    9    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    5    0    0    4    2    0    0    1  851   11    1    0
     0    0    0]
 [   0    0   20    0    0    1    1    0    0    1   14 2156   14    0
     1    2    0]
 [   0    0    0    1    0    3    0    0    0    0    8    0  517    0
     3    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    1    0    0
  1116   21    0]
 [   0    0    1    0    0    0    1    0    0    0    0    0    0    0
    16  329    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.22493224932249

F1 scores:
[       nan 0.96202532 0.97419859 0.96551724 0.96162528 0.96788991
 0.99315589 0.96153846 0.98942421 0.75555556 0.96869664 0.97777778
 0.9417122  1.         0.97680525 0.94134478 0.96385542]

Kappa:
0.968377525215161
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0a3a6eba90>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.812, val_acc:0.009]
Epoch [2/120    avg_loss:2.682, val_acc:0.123]
Epoch [3/120    avg_loss:2.596, val_acc:0.224]
Epoch [4/120    avg_loss:2.561, val_acc:0.317]
Epoch [5/120    avg_loss:2.468, val_acc:0.372]
Epoch [6/120    avg_loss:2.383, val_acc:0.396]
Epoch [7/120    avg_loss:2.289, val_acc:0.429]
Epoch [8/120    avg_loss:2.270, val_acc:0.475]
Epoch [9/120    avg_loss:2.180, val_acc:0.515]
Epoch [10/120    avg_loss:2.110, val_acc:0.550]
Epoch [11/120    avg_loss:2.050, val_acc:0.570]
Epoch [12/120    avg_loss:1.977, val_acc:0.558]
Epoch [13/120    avg_loss:1.908, val_acc:0.620]
Epoch [14/120    avg_loss:1.826, val_acc:0.629]
Epoch [15/120    avg_loss:1.710, val_acc:0.626]
Epoch [16/120    avg_loss:1.663, val_acc:0.648]
Epoch [17/120    avg_loss:1.540, val_acc:0.683]
Epoch [18/120    avg_loss:1.500, val_acc:0.649]
Epoch [19/120    avg_loss:1.360, val_acc:0.647]
Epoch [20/120    avg_loss:1.323, val_acc:0.729]
Epoch [21/120    avg_loss:1.181, val_acc:0.746]
Epoch [22/120    avg_loss:1.112, val_acc:0.749]
Epoch [23/120    avg_loss:1.012, val_acc:0.729]
Epoch [24/120    avg_loss:0.972, val_acc:0.720]
Epoch [25/120    avg_loss:0.904, val_acc:0.780]
Epoch [26/120    avg_loss:0.870, val_acc:0.759]
Epoch [27/120    avg_loss:0.852, val_acc:0.793]
Epoch [28/120    avg_loss:0.705, val_acc:0.795]
Epoch [29/120    avg_loss:0.618, val_acc:0.832]
Epoch [30/120    avg_loss:0.578, val_acc:0.774]
Epoch [31/120    avg_loss:0.568, val_acc:0.804]
Epoch [32/120    avg_loss:0.529, val_acc:0.854]
Epoch [33/120    avg_loss:0.430, val_acc:0.887]
Epoch [34/120    avg_loss:0.432, val_acc:0.889]
Epoch [35/120    avg_loss:0.357, val_acc:0.885]
Epoch [36/120    avg_loss:0.314, val_acc:0.911]
Epoch [37/120    avg_loss:0.338, val_acc:0.891]
Epoch [38/120    avg_loss:0.345, val_acc:0.896]
Epoch [39/120    avg_loss:0.273, val_acc:0.909]
Epoch [40/120    avg_loss:0.249, val_acc:0.915]
Epoch [41/120    avg_loss:0.240, val_acc:0.922]
Epoch [42/120    avg_loss:0.229, val_acc:0.917]
Epoch [43/120    avg_loss:0.180, val_acc:0.934]
Epoch [44/120    avg_loss:0.154, val_acc:0.937]
Epoch [45/120    avg_loss:0.150, val_acc:0.936]
Epoch [46/120    avg_loss:0.149, val_acc:0.916]
Epoch [47/120    avg_loss:0.184, val_acc:0.921]
Epoch [48/120    avg_loss:0.166, val_acc:0.928]
Epoch [49/120    avg_loss:0.175, val_acc:0.942]
Epoch [50/120    avg_loss:0.161, val_acc:0.939]
Epoch [51/120    avg_loss:0.176, val_acc:0.935]
Epoch [52/120    avg_loss:0.130, val_acc:0.949]
Epoch [53/120    avg_loss:0.102, val_acc:0.946]
Epoch [54/120    avg_loss:0.113, val_acc:0.953]
Epoch [55/120    avg_loss:0.096, val_acc:0.932]
Epoch [56/120    avg_loss:0.092, val_acc:0.958]
Epoch [57/120    avg_loss:0.080, val_acc:0.958]
Epoch [58/120    avg_loss:0.072, val_acc:0.961]
Epoch [59/120    avg_loss:0.063, val_acc:0.960]
Epoch [60/120    avg_loss:0.057, val_acc:0.962]
Epoch [61/120    avg_loss:0.067, val_acc:0.963]
Epoch [62/120    avg_loss:0.087, val_acc:0.947]
Epoch [63/120    avg_loss:0.071, val_acc:0.950]
Epoch [64/120    avg_loss:0.057, val_acc:0.967]
Epoch [65/120    avg_loss:0.053, val_acc:0.965]
Epoch [66/120    avg_loss:0.059, val_acc:0.955]
Epoch [67/120    avg_loss:0.073, val_acc:0.961]
Epoch [68/120    avg_loss:0.059, val_acc:0.972]
Epoch [69/120    avg_loss:0.050, val_acc:0.968]
Epoch [70/120    avg_loss:0.046, val_acc:0.955]
Epoch [71/120    avg_loss:0.055, val_acc:0.972]
Epoch [72/120    avg_loss:0.036, val_acc:0.976]
Epoch [73/120    avg_loss:0.038, val_acc:0.968]
Epoch [74/120    avg_loss:0.030, val_acc:0.974]
Epoch [75/120    avg_loss:0.034, val_acc:0.974]
Epoch [76/120    avg_loss:0.050, val_acc:0.934]
Epoch [77/120    avg_loss:0.059, val_acc:0.961]
Epoch [78/120    avg_loss:0.055, val_acc:0.964]
Epoch [79/120    avg_loss:0.046, val_acc:0.963]
Epoch [80/120    avg_loss:0.035, val_acc:0.965]
Epoch [81/120    avg_loss:0.035, val_acc:0.962]
Epoch [82/120    avg_loss:0.048, val_acc:0.963]
Epoch [83/120    avg_loss:0.036, val_acc:0.973]
Epoch [84/120    avg_loss:0.027, val_acc:0.975]
Epoch [85/120    avg_loss:0.025, val_acc:0.972]
Epoch [86/120    avg_loss:0.025, val_acc:0.974]
Epoch [87/120    avg_loss:0.022, val_acc:0.975]
Epoch [88/120    avg_loss:0.020, val_acc:0.975]
Epoch [89/120    avg_loss:0.021, val_acc:0.975]
Epoch [90/120    avg_loss:0.016, val_acc:0.976]
Epoch [91/120    avg_loss:0.019, val_acc:0.975]
Epoch [92/120    avg_loss:0.019, val_acc:0.977]
Epoch [93/120    avg_loss:0.018, val_acc:0.977]
Epoch [94/120    avg_loss:0.019, val_acc:0.976]
Epoch [95/120    avg_loss:0.018, val_acc:0.976]
Epoch [96/120    avg_loss:0.020, val_acc:0.976]
Epoch [97/120    avg_loss:0.015, val_acc:0.976]
Epoch [98/120    avg_loss:0.017, val_acc:0.974]
Epoch [99/120    avg_loss:0.017, val_acc:0.975]
Epoch [100/120    avg_loss:0.021, val_acc:0.977]
Epoch [101/120    avg_loss:0.015, val_acc:0.977]
Epoch [102/120    avg_loss:0.016, val_acc:0.977]
Epoch [103/120    avg_loss:0.017, val_acc:0.976]
Epoch [104/120    avg_loss:0.017, val_acc:0.974]
Epoch [105/120    avg_loss:0.018, val_acc:0.977]
Epoch [106/120    avg_loss:0.018, val_acc:0.978]
Epoch [107/120    avg_loss:0.018, val_acc:0.977]
Epoch [108/120    avg_loss:0.017, val_acc:0.978]
Epoch [109/120    avg_loss:0.014, val_acc:0.978]
Epoch [110/120    avg_loss:0.014, val_acc:0.977]
Epoch [111/120    avg_loss:0.015, val_acc:0.976]
Epoch [112/120    avg_loss:0.021, val_acc:0.977]
Epoch [113/120    avg_loss:0.017, val_acc:0.977]
Epoch [114/120    avg_loss:0.016, val_acc:0.976]
Epoch [115/120    avg_loss:0.017, val_acc:0.976]
Epoch [116/120    avg_loss:0.017, val_acc:0.976]
Epoch [117/120    avg_loss:0.014, val_acc:0.977]
Epoch [118/120    avg_loss:0.015, val_acc:0.975]
Epoch [119/120    avg_loss:0.015, val_acc:0.978]
Epoch [120/120    avg_loss:0.015, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1246    7    7    0    2    0    0    1    1   21    0    0
     0    0    0]
 [   0    0    1  715    4    7    0    0    0    5    3    7    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    1    0    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    1    0    0    0  859    9    1    0
     0    0    0]
 [   0    0    5    0    0    0    5    0    0    0   20 2160   20    0
     0    0    0]
 [   0    0    0    3    0    1    0    0    0    1    1    0  523    0
     2    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
  1128    9    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    14  321    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.86449864498645

F1 scores:
[       nan 0.975      0.98033045 0.97146739 0.97482838 0.98169336
 0.98121713 1.         1.         0.81818182 0.97558206 0.97981402
 0.96583564 1.         0.98558322 0.94830133 0.98245614]

Kappa:
0.9756653731003488
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4cc614fa58>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.786, val_acc:0.024]
Epoch [2/120    avg_loss:2.684, val_acc:0.249]
Epoch [3/120    avg_loss:2.578, val_acc:0.403]
Epoch [4/120    avg_loss:2.483, val_acc:0.479]
Epoch [5/120    avg_loss:2.411, val_acc:0.502]
Epoch [6/120    avg_loss:2.323, val_acc:0.513]
Epoch [7/120    avg_loss:2.241, val_acc:0.536]
Epoch [8/120    avg_loss:2.190, val_acc:0.547]
Epoch [9/120    avg_loss:2.162, val_acc:0.570]
Epoch [10/120    avg_loss:2.082, val_acc:0.567]
Epoch [11/120    avg_loss:2.021, val_acc:0.578]
Epoch [12/120    avg_loss:1.970, val_acc:0.612]
Epoch [13/120    avg_loss:1.883, val_acc:0.621]
Epoch [14/120    avg_loss:1.800, val_acc:0.671]
Epoch [15/120    avg_loss:1.729, val_acc:0.649]
Epoch [16/120    avg_loss:1.615, val_acc:0.671]
Epoch [17/120    avg_loss:1.513, val_acc:0.734]
Epoch [18/120    avg_loss:1.391, val_acc:0.743]
Epoch [19/120    avg_loss:1.314, val_acc:0.728]
Epoch [20/120    avg_loss:1.194, val_acc:0.732]
Epoch [21/120    avg_loss:1.087, val_acc:0.772]
Epoch [22/120    avg_loss:0.984, val_acc:0.773]
Epoch [23/120    avg_loss:0.915, val_acc:0.799]
Epoch [24/120    avg_loss:0.858, val_acc:0.785]
Epoch [25/120    avg_loss:0.856, val_acc:0.813]
Epoch [26/120    avg_loss:0.768, val_acc:0.813]
Epoch [27/120    avg_loss:0.712, val_acc:0.832]
Epoch [28/120    avg_loss:0.607, val_acc:0.813]
Epoch [29/120    avg_loss:0.598, val_acc:0.857]
Epoch [30/120    avg_loss:0.521, val_acc:0.868]
Epoch [31/120    avg_loss:0.423, val_acc:0.882]
Epoch [32/120    avg_loss:0.416, val_acc:0.882]
Epoch [33/120    avg_loss:0.368, val_acc:0.882]
Epoch [34/120    avg_loss:0.357, val_acc:0.909]
Epoch [35/120    avg_loss:0.319, val_acc:0.875]
Epoch [36/120    avg_loss:0.326, val_acc:0.874]
Epoch [37/120    avg_loss:0.306, val_acc:0.909]
Epoch [38/120    avg_loss:0.253, val_acc:0.904]
Epoch [39/120    avg_loss:0.252, val_acc:0.916]
Epoch [40/120    avg_loss:0.235, val_acc:0.918]
Epoch [41/120    avg_loss:0.189, val_acc:0.933]
Epoch [42/120    avg_loss:0.179, val_acc:0.930]
Epoch [43/120    avg_loss:0.176, val_acc:0.927]
Epoch [44/120    avg_loss:0.162, val_acc:0.949]
Epoch [45/120    avg_loss:0.139, val_acc:0.938]
Epoch [46/120    avg_loss:0.163, val_acc:0.939]
Epoch [47/120    avg_loss:0.133, val_acc:0.911]
Epoch [48/120    avg_loss:0.124, val_acc:0.947]
Epoch [49/120    avg_loss:0.116, val_acc:0.946]
Epoch [50/120    avg_loss:0.102, val_acc:0.943]
Epoch [51/120    avg_loss:0.098, val_acc:0.945]
Epoch [52/120    avg_loss:0.100, val_acc:0.950]
Epoch [53/120    avg_loss:0.104, val_acc:0.953]
Epoch [54/120    avg_loss:0.110, val_acc:0.948]
Epoch [55/120    avg_loss:0.091, val_acc:0.963]
Epoch [56/120    avg_loss:0.110, val_acc:0.950]
Epoch [57/120    avg_loss:0.086, val_acc:0.957]
Epoch [58/120    avg_loss:0.070, val_acc:0.960]
Epoch [59/120    avg_loss:0.070, val_acc:0.962]
Epoch [60/120    avg_loss:0.057, val_acc:0.966]
Epoch [61/120    avg_loss:0.079, val_acc:0.961]
Epoch [62/120    avg_loss:0.103, val_acc:0.950]
Epoch [63/120    avg_loss:0.065, val_acc:0.970]
Epoch [64/120    avg_loss:0.055, val_acc:0.964]
Epoch [65/120    avg_loss:0.070, val_acc:0.977]
Epoch [66/120    avg_loss:0.063, val_acc:0.960]
Epoch [67/120    avg_loss:0.049, val_acc:0.965]
Epoch [68/120    avg_loss:0.046, val_acc:0.973]
Epoch [69/120    avg_loss:0.042, val_acc:0.963]
Epoch [70/120    avg_loss:0.045, val_acc:0.972]
Epoch [71/120    avg_loss:0.047, val_acc:0.978]
Epoch [72/120    avg_loss:0.055, val_acc:0.975]
Epoch [73/120    avg_loss:0.051, val_acc:0.977]
Epoch [74/120    avg_loss:0.044, val_acc:0.967]
Epoch [75/120    avg_loss:0.043, val_acc:0.974]
Epoch [76/120    avg_loss:0.043, val_acc:0.971]
Epoch [77/120    avg_loss:0.033, val_acc:0.977]
Epoch [78/120    avg_loss:0.036, val_acc:0.975]
Epoch [79/120    avg_loss:0.034, val_acc:0.970]
Epoch [80/120    avg_loss:0.043, val_acc:0.976]
Epoch [81/120    avg_loss:0.043, val_acc:0.973]
Epoch [82/120    avg_loss:0.038, val_acc:0.964]
Epoch [83/120    avg_loss:0.045, val_acc:0.973]
Epoch [84/120    avg_loss:0.031, val_acc:0.977]
Epoch [85/120    avg_loss:0.030, val_acc:0.979]
Epoch [86/120    avg_loss:0.028, val_acc:0.980]
Epoch [87/120    avg_loss:0.027, val_acc:0.979]
Epoch [88/120    avg_loss:0.026, val_acc:0.979]
Epoch [89/120    avg_loss:0.021, val_acc:0.977]
Epoch [90/120    avg_loss:0.021, val_acc:0.983]
Epoch [91/120    avg_loss:0.025, val_acc:0.984]
Epoch [92/120    avg_loss:0.028, val_acc:0.979]
Epoch [93/120    avg_loss:0.021, val_acc:0.982]
Epoch [94/120    avg_loss:0.022, val_acc:0.983]
Epoch [95/120    avg_loss:0.024, val_acc:0.980]
Epoch [96/120    avg_loss:0.026, val_acc:0.979]
Epoch [97/120    avg_loss:0.022, val_acc:0.982]
Epoch [98/120    avg_loss:0.023, val_acc:0.980]
Epoch [99/120    avg_loss:0.022, val_acc:0.982]
Epoch [100/120    avg_loss:0.020, val_acc:0.983]
Epoch [101/120    avg_loss:0.021, val_acc:0.985]
Epoch [102/120    avg_loss:0.018, val_acc:0.985]
Epoch [103/120    avg_loss:0.020, val_acc:0.984]
Epoch [104/120    avg_loss:0.021, val_acc:0.984]
Epoch [105/120    avg_loss:0.027, val_acc:0.983]
Epoch [106/120    avg_loss:0.020, val_acc:0.983]
Epoch [107/120    avg_loss:0.019, val_acc:0.983]
Epoch [108/120    avg_loss:0.020, val_acc:0.984]
Epoch [109/120    avg_loss:0.020, val_acc:0.982]
Epoch [110/120    avg_loss:0.022, val_acc:0.982]
Epoch [111/120    avg_loss:0.017, val_acc:0.984]
Epoch [112/120    avg_loss:0.021, val_acc:0.982]
Epoch [113/120    avg_loss:0.022, val_acc:0.982]
Epoch [114/120    avg_loss:0.022, val_acc:0.982]
Epoch [115/120    avg_loss:0.021, val_acc:0.985]
Epoch [116/120    avg_loss:0.020, val_acc:0.984]
Epoch [117/120    avg_loss:0.020, val_acc:0.985]
Epoch [118/120    avg_loss:0.022, val_acc:0.980]
Epoch [119/120    avg_loss:0.021, val_acc:0.980]
Epoch [120/120    avg_loss:0.019, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1224    2    0    0    2    0    0    0   15   42    0    0
     0    0    0]
 [   0    0    0  707    1    0    1    0    0   17    3    7   11    0
     0    0    0]
 [   0    0    0    4  209    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    2    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    8    0    0    0    2    0    0    0  851   11    1    0
     0    2    0]
 [   0    2    1    0    0    0    4    1    0    0   12 2174   15    1
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0    0  530    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    1    0    0
  1128    8    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    19  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.51761517615176

F1 scores:
[       nan 0.89411765 0.97220016 0.96716826 0.98817967 0.99769585
 0.97897898 0.98039216 0.99416569 0.64150943 0.96704545 0.9777378
 0.96980787 0.99730458 0.98515284 0.93591654 0.98224852]

Kappa:
0.9716961008326634
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3f8b046a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 40648==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.823, val_acc:0.082]
Epoch [2/120    avg_loss:2.715, val_acc:0.344]
Epoch [3/120    avg_loss:2.596, val_acc:0.431]
Epoch [4/120    avg_loss:2.494, val_acc:0.452]
Epoch [5/120    avg_loss:2.399, val_acc:0.471]
Epoch [6/120    avg_loss:2.354, val_acc:0.480]
Epoch [7/120    avg_loss:2.265, val_acc:0.487]
Epoch [8/120    avg_loss:2.225, val_acc:0.486]
Epoch [9/120    avg_loss:2.135, val_acc:0.482]
Epoch [10/120    avg_loss:2.070, val_acc:0.484]
Epoch [11/120    avg_loss:1.994, val_acc:0.540]
Epoch [12/120    avg_loss:1.942, val_acc:0.485]
Epoch [13/120    avg_loss:1.848, val_acc:0.530]
Epoch [14/120    avg_loss:1.789, val_acc:0.517]
Epoch [15/120    avg_loss:1.697, val_acc:0.556]
Epoch [16/120    avg_loss:1.634, val_acc:0.591]
Epoch [17/120    avg_loss:1.547, val_acc:0.621]
Epoch [18/120    avg_loss:1.446, val_acc:0.643]
Epoch [19/120    avg_loss:1.469, val_acc:0.635]
Epoch [20/120    avg_loss:1.360, val_acc:0.657]
Epoch [21/120    avg_loss:1.310, val_acc:0.661]
Epoch [22/120    avg_loss:1.186, val_acc:0.692]
Epoch [23/120    avg_loss:1.099, val_acc:0.728]
Epoch [24/120    avg_loss:1.012, val_acc:0.746]
Epoch [25/120    avg_loss:0.932, val_acc:0.759]
Epoch [26/120    avg_loss:0.881, val_acc:0.770]
Epoch [27/120    avg_loss:0.792, val_acc:0.797]
Epoch [28/120    avg_loss:0.765, val_acc:0.769]
Epoch [29/120    avg_loss:0.704, val_acc:0.804]
Epoch [30/120    avg_loss:0.641, val_acc:0.767]
Epoch [31/120    avg_loss:0.571, val_acc:0.841]
Epoch [32/120    avg_loss:0.510, val_acc:0.824]
Epoch [33/120    avg_loss:0.489, val_acc:0.832]
Epoch [34/120    avg_loss:0.472, val_acc:0.855]
Epoch [35/120    avg_loss:0.428, val_acc:0.849]
Epoch [36/120    avg_loss:0.402, val_acc:0.865]
Epoch [37/120    avg_loss:0.353, val_acc:0.868]
Epoch [38/120    avg_loss:0.327, val_acc:0.898]
Epoch [39/120    avg_loss:0.293, val_acc:0.871]
Epoch [40/120    avg_loss:0.274, val_acc:0.890]
Epoch [41/120    avg_loss:0.255, val_acc:0.914]
Epoch [42/120    avg_loss:0.244, val_acc:0.927]
Epoch [43/120    avg_loss:0.182, val_acc:0.922]
Epoch [44/120    avg_loss:0.191, val_acc:0.927]
Epoch [45/120    avg_loss:0.183, val_acc:0.926]
Epoch [46/120    avg_loss:0.157, val_acc:0.922]
Epoch [47/120    avg_loss:0.162, val_acc:0.923]
Epoch [48/120    avg_loss:0.157, val_acc:0.918]
Epoch [49/120    avg_loss:0.166, val_acc:0.928]
Epoch [50/120    avg_loss:0.157, val_acc:0.935]
Epoch [51/120    avg_loss:0.122, val_acc:0.947]
Epoch [52/120    avg_loss:0.116, val_acc:0.927]
Epoch [53/120    avg_loss:0.110, val_acc:0.938]
Epoch [54/120    avg_loss:0.103, val_acc:0.919]
Epoch [55/120    avg_loss:0.113, val_acc:0.947]
Epoch [56/120    avg_loss:0.118, val_acc:0.919]
Epoch [57/120    avg_loss:0.111, val_acc:0.950]
Epoch [58/120    avg_loss:0.110, val_acc:0.958]
Epoch [59/120    avg_loss:0.076, val_acc:0.947]
Epoch [60/120    avg_loss:0.068, val_acc:0.942]
Epoch [61/120    avg_loss:0.066, val_acc:0.959]
Epoch [62/120    avg_loss:0.075, val_acc:0.942]
Epoch [63/120    avg_loss:0.068, val_acc:0.963]
Epoch [64/120    avg_loss:0.111, val_acc:0.934]
Epoch [65/120    avg_loss:0.115, val_acc:0.952]
Epoch [66/120    avg_loss:0.087, val_acc:0.934]
Epoch [67/120    avg_loss:0.079, val_acc:0.953]
Epoch [68/120    avg_loss:0.079, val_acc:0.960]
Epoch [69/120    avg_loss:0.066, val_acc:0.947]
Epoch [70/120    avg_loss:0.065, val_acc:0.953]
Epoch [71/120    avg_loss:0.068, val_acc:0.963]
Epoch [72/120    avg_loss:0.057, val_acc:0.949]
Epoch [73/120    avg_loss:0.053, val_acc:0.966]
Epoch [74/120    avg_loss:0.061, val_acc:0.946]
Epoch [75/120    avg_loss:0.054, val_acc:0.969]
Epoch [76/120    avg_loss:0.062, val_acc:0.960]
Epoch [77/120    avg_loss:0.056, val_acc:0.968]
Epoch [78/120    avg_loss:0.049, val_acc:0.949]
Epoch [79/120    avg_loss:0.050, val_acc:0.969]
Epoch [80/120    avg_loss:0.037, val_acc:0.971]
Epoch [81/120    avg_loss:0.034, val_acc:0.960]
Epoch [82/120    avg_loss:0.038, val_acc:0.951]
Epoch [83/120    avg_loss:0.038, val_acc:0.972]
Epoch [84/120    avg_loss:0.043, val_acc:0.972]
Epoch [85/120    avg_loss:0.034, val_acc:0.965]
Epoch [86/120    avg_loss:0.041, val_acc:0.949]
Epoch [87/120    avg_loss:0.064, val_acc:0.935]
Epoch [88/120    avg_loss:0.081, val_acc:0.961]
Epoch [89/120    avg_loss:0.063, val_acc:0.952]
Epoch [90/120    avg_loss:0.084, val_acc:0.930]
Epoch [91/120    avg_loss:0.070, val_acc:0.942]
Epoch [92/120    avg_loss:0.054, val_acc:0.947]
Epoch [93/120    avg_loss:0.034, val_acc:0.961]
Epoch [94/120    avg_loss:0.039, val_acc:0.965]
Epoch [95/120    avg_loss:0.036, val_acc:0.958]
Epoch [96/120    avg_loss:0.028, val_acc:0.975]
Epoch [97/120    avg_loss:0.025, val_acc:0.977]
Epoch [98/120    avg_loss:0.027, val_acc:0.960]
Epoch [99/120    avg_loss:0.037, val_acc:0.961]
Epoch [100/120    avg_loss:0.032, val_acc:0.970]
Epoch [101/120    avg_loss:0.025, val_acc:0.977]
Epoch [102/120    avg_loss:0.025, val_acc:0.966]
Epoch [103/120    avg_loss:0.032, val_acc:0.970]
Epoch [104/120    avg_loss:0.024, val_acc:0.965]
Epoch [105/120    avg_loss:0.022, val_acc:0.977]
Epoch [106/120    avg_loss:0.023, val_acc:0.969]
Epoch [107/120    avg_loss:0.029, val_acc:0.971]
Epoch [108/120    avg_loss:0.020, val_acc:0.979]
Epoch [109/120    avg_loss:0.018, val_acc:0.975]
Epoch [110/120    avg_loss:0.018, val_acc:0.973]
Epoch [111/120    avg_loss:0.017, val_acc:0.974]
Epoch [112/120    avg_loss:0.022, val_acc:0.960]
Epoch [113/120    avg_loss:0.033, val_acc:0.961]
Epoch [114/120    avg_loss:0.032, val_acc:0.968]
Epoch [115/120    avg_loss:0.022, val_acc:0.971]
Epoch [116/120    avg_loss:0.014, val_acc:0.973]
Epoch [117/120    avg_loss:0.015, val_acc:0.976]
Epoch [118/120    avg_loss:0.017, val_acc:0.977]
Epoch [119/120    avg_loss:0.017, val_acc:0.969]
Epoch [120/120    avg_loss:0.025, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1206    9    8    0    0    0    0    1   25   36    0    0
     0    0    0]
 [   0    0    0  716    0    0    0    0    0    8    4    2   17    0
     0    0    0]
 [   0    0    0    0  211    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  429    0    5    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  859    7    0    0
     3    2    0]
 [   0    0    1    2    0    0    3    1    0    0  112 2069   22    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0   15  514    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    1    0    1    0
  1127    9    0]
 [   0    0    0    0    0    0   30    0    0    0    0    0    0    0
    18  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
95.98915989159892

F1 scores:
[       nan 0.96202532 0.96634615 0.9701897  0.97685185 0.99305556
 0.97477745 0.89285714 0.9953271  0.77272727 0.91431613 0.95345622
 0.93369664 0.99728997 0.98470948 0.91019787 0.95121951]

Kappa:
0.954356074026752
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3007fbea90>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.834, val_acc:0.147]
Epoch [2/120    avg_loss:2.736, val_acc:0.300]
Epoch [3/120    avg_loss:2.639, val_acc:0.372]
Epoch [4/120    avg_loss:2.533, val_acc:0.376]
Epoch [5/120    avg_loss:2.420, val_acc:0.397]
Epoch [6/120    avg_loss:2.382, val_acc:0.393]
Epoch [7/120    avg_loss:2.313, val_acc:0.380]
Epoch [8/120    avg_loss:2.243, val_acc:0.383]
Epoch [9/120    avg_loss:2.191, val_acc:0.389]
Epoch [10/120    avg_loss:2.161, val_acc:0.401]
Epoch [11/120    avg_loss:2.077, val_acc:0.425]
Epoch [12/120    avg_loss:2.012, val_acc:0.462]
Epoch [13/120    avg_loss:1.985, val_acc:0.505]
Epoch [14/120    avg_loss:1.934, val_acc:0.497]
Epoch [15/120    avg_loss:1.862, val_acc:0.486]
Epoch [16/120    avg_loss:1.808, val_acc:0.526]
Epoch [17/120    avg_loss:1.687, val_acc:0.612]
Epoch [18/120    avg_loss:1.596, val_acc:0.628]
Epoch [19/120    avg_loss:1.484, val_acc:0.665]
Epoch [20/120    avg_loss:1.406, val_acc:0.677]
Epoch [21/120    avg_loss:1.359, val_acc:0.683]
Epoch [22/120    avg_loss:1.248, val_acc:0.705]
Epoch [23/120    avg_loss:1.191, val_acc:0.658]
Epoch [24/120    avg_loss:1.123, val_acc:0.693]
Epoch [25/120    avg_loss:1.062, val_acc:0.709]
Epoch [26/120    avg_loss:0.988, val_acc:0.704]
Epoch [27/120    avg_loss:0.989, val_acc:0.736]
Epoch [28/120    avg_loss:0.912, val_acc:0.749]
Epoch [29/120    avg_loss:0.808, val_acc:0.732]
Epoch [30/120    avg_loss:0.776, val_acc:0.750]
Epoch [31/120    avg_loss:0.733, val_acc:0.778]
Epoch [32/120    avg_loss:0.634, val_acc:0.801]
Epoch [33/120    avg_loss:0.628, val_acc:0.785]
Epoch [34/120    avg_loss:0.611, val_acc:0.792]
Epoch [35/120    avg_loss:0.547, val_acc:0.811]
Epoch [36/120    avg_loss:0.521, val_acc:0.814]
Epoch [37/120    avg_loss:0.496, val_acc:0.825]
Epoch [38/120    avg_loss:0.450, val_acc:0.841]
Epoch [39/120    avg_loss:0.397, val_acc:0.868]
Epoch [40/120    avg_loss:0.362, val_acc:0.858]
Epoch [41/120    avg_loss:0.334, val_acc:0.885]
Epoch [42/120    avg_loss:0.319, val_acc:0.877]
Epoch [43/120    avg_loss:0.331, val_acc:0.866]
Epoch [44/120    avg_loss:0.301, val_acc:0.888]
Epoch [45/120    avg_loss:0.276, val_acc:0.890]
Epoch [46/120    avg_loss:0.298, val_acc:0.877]
Epoch [47/120    avg_loss:0.276, val_acc:0.917]
Epoch [48/120    avg_loss:0.293, val_acc:0.904]
Epoch [49/120    avg_loss:0.251, val_acc:0.908]
Epoch [50/120    avg_loss:0.211, val_acc:0.917]
Epoch [51/120    avg_loss:0.188, val_acc:0.929]
Epoch [52/120    avg_loss:0.186, val_acc:0.912]
Epoch [53/120    avg_loss:0.183, val_acc:0.920]
Epoch [54/120    avg_loss:0.161, val_acc:0.927]
Epoch [55/120    avg_loss:0.145, val_acc:0.930]
Epoch [56/120    avg_loss:0.132, val_acc:0.937]
Epoch [57/120    avg_loss:0.136, val_acc:0.936]
Epoch [58/120    avg_loss:0.133, val_acc:0.913]
Epoch [59/120    avg_loss:0.130, val_acc:0.920]
Epoch [60/120    avg_loss:0.105, val_acc:0.934]
Epoch [61/120    avg_loss:0.103, val_acc:0.940]
Epoch [62/120    avg_loss:0.117, val_acc:0.943]
Epoch [63/120    avg_loss:0.104, val_acc:0.943]
Epoch [64/120    avg_loss:0.091, val_acc:0.948]
Epoch [65/120    avg_loss:0.078, val_acc:0.949]
Epoch [66/120    avg_loss:0.086, val_acc:0.936]
Epoch [67/120    avg_loss:0.080, val_acc:0.952]
Epoch [68/120    avg_loss:0.070, val_acc:0.950]
Epoch [69/120    avg_loss:0.070, val_acc:0.951]
Epoch [70/120    avg_loss:0.075, val_acc:0.951]
Epoch [71/120    avg_loss:0.091, val_acc:0.951]
Epoch [72/120    avg_loss:0.083, val_acc:0.961]
Epoch [73/120    avg_loss:0.071, val_acc:0.960]
Epoch [74/120    avg_loss:0.069, val_acc:0.953]
Epoch [75/120    avg_loss:0.064, val_acc:0.957]
Epoch [76/120    avg_loss:0.052, val_acc:0.954]
Epoch [77/120    avg_loss:0.054, val_acc:0.960]
Epoch [78/120    avg_loss:0.058, val_acc:0.957]
Epoch [79/120    avg_loss:0.076, val_acc:0.951]
Epoch [80/120    avg_loss:0.070, val_acc:0.962]
Epoch [81/120    avg_loss:0.055, val_acc:0.970]
Epoch [82/120    avg_loss:0.053, val_acc:0.967]
Epoch [83/120    avg_loss:0.042, val_acc:0.957]
Epoch [84/120    avg_loss:0.045, val_acc:0.963]
Epoch [85/120    avg_loss:0.047, val_acc:0.966]
Epoch [86/120    avg_loss:0.038, val_acc:0.960]
Epoch [87/120    avg_loss:0.046, val_acc:0.927]
Epoch [88/120    avg_loss:0.054, val_acc:0.961]
Epoch [89/120    avg_loss:0.048, val_acc:0.950]
Epoch [90/120    avg_loss:0.042, val_acc:0.964]
Epoch [91/120    avg_loss:0.030, val_acc:0.963]
Epoch [92/120    avg_loss:0.047, val_acc:0.959]
Epoch [93/120    avg_loss:0.050, val_acc:0.952]
Epoch [94/120    avg_loss:0.048, val_acc:0.957]
Epoch [95/120    avg_loss:0.034, val_acc:0.968]
Epoch [96/120    avg_loss:0.024, val_acc:0.971]
Epoch [97/120    avg_loss:0.029, val_acc:0.967]
Epoch [98/120    avg_loss:0.027, val_acc:0.967]
Epoch [99/120    avg_loss:0.031, val_acc:0.968]
Epoch [100/120    avg_loss:0.025, val_acc:0.971]
Epoch [101/120    avg_loss:0.030, val_acc:0.970]
Epoch [102/120    avg_loss:0.027, val_acc:0.970]
Epoch [103/120    avg_loss:0.024, val_acc:0.968]
Epoch [104/120    avg_loss:0.022, val_acc:0.970]
Epoch [105/120    avg_loss:0.022, val_acc:0.968]
Epoch [106/120    avg_loss:0.021, val_acc:0.972]
Epoch [107/120    avg_loss:0.024, val_acc:0.967]
Epoch [108/120    avg_loss:0.022, val_acc:0.970]
Epoch [109/120    avg_loss:0.024, val_acc:0.968]
Epoch [110/120    avg_loss:0.023, val_acc:0.970]
Epoch [111/120    avg_loss:0.020, val_acc:0.968]
Epoch [112/120    avg_loss:0.023, val_acc:0.970]
Epoch [113/120    avg_loss:0.024, val_acc:0.970]
Epoch [114/120    avg_loss:0.018, val_acc:0.971]
Epoch [115/120    avg_loss:0.019, val_acc:0.968]
Epoch [116/120    avg_loss:0.021, val_acc:0.971]
Epoch [117/120    avg_loss:0.022, val_acc:0.968]
Epoch [118/120    avg_loss:0.021, val_acc:0.968]
Epoch [119/120    avg_loss:0.022, val_acc:0.970]
Epoch [120/120    avg_loss:0.022, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    1    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1232    2    2    0    5    0    0    1    7   34    2    0
     0    0    0]
 [   0    0    6  693    0   15    0    0    0   18    3    0   12    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    1    2    0    2    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    4    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   23    0    0    9    0    0    0    0  834    5    0    0
     0    4    0]
 [   0    0   12    0    0    0    2    0    0    0   13 2171   12    0
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    3    5  517    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0  184
     0    0    0]
 [   0    0    0    0    0   14    0    0    0    0    3    0    0    0
  1118    4    0]
 [   0    0    0    0    0    0   32    0    0    0    0    0    0    0
     3  312    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
96.8780487804878

F1 scores:
[       nan 0.94871795 0.96287612 0.9598338  0.99297424 0.94143646
 0.96740741 0.96153846 0.9953271  0.57142857 0.95807007 0.97991424
 0.95652174 0.99728997 0.98676081 0.93553223 0.96428571]

Kappa:
0.9644134596632298
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efbccd58a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.824, val_acc:0.163]
Epoch [2/120    avg_loss:2.716, val_acc:0.225]
Epoch [3/120    avg_loss:2.627, val_acc:0.255]
Epoch [4/120    avg_loss:2.535, val_acc:0.401]
Epoch [5/120    avg_loss:2.417, val_acc:0.407]
Epoch [6/120    avg_loss:2.333, val_acc:0.427]
Epoch [7/120    avg_loss:2.271, val_acc:0.455]
Epoch [8/120    avg_loss:2.208, val_acc:0.480]
Epoch [9/120    avg_loss:2.144, val_acc:0.530]
Epoch [10/120    avg_loss:2.031, val_acc:0.562]
Epoch [11/120    avg_loss:1.952, val_acc:0.573]
Epoch [12/120    avg_loss:1.895, val_acc:0.565]
Epoch [13/120    avg_loss:1.792, val_acc:0.566]
Epoch [14/120    avg_loss:1.740, val_acc:0.598]
Epoch [15/120    avg_loss:1.633, val_acc:0.617]
Epoch [16/120    avg_loss:1.590, val_acc:0.610]
Epoch [17/120    avg_loss:1.421, val_acc:0.620]
Epoch [18/120    avg_loss:1.365, val_acc:0.602]
Epoch [19/120    avg_loss:1.306, val_acc:0.649]
Epoch [20/120    avg_loss:1.201, val_acc:0.646]
Epoch [21/120    avg_loss:1.167, val_acc:0.708]
Epoch [22/120    avg_loss:1.084, val_acc:0.759]
Epoch [23/120    avg_loss:1.038, val_acc:0.734]
Epoch [24/120    avg_loss:0.986, val_acc:0.747]
Epoch [25/120    avg_loss:0.909, val_acc:0.768]
Epoch [26/120    avg_loss:0.818, val_acc:0.789]
Epoch [27/120    avg_loss:0.766, val_acc:0.749]
Epoch [28/120    avg_loss:0.733, val_acc:0.826]
Epoch [29/120    avg_loss:0.691, val_acc:0.798]
Epoch [30/120    avg_loss:0.624, val_acc:0.810]
Epoch [31/120    avg_loss:0.528, val_acc:0.826]
Epoch [32/120    avg_loss:0.524, val_acc:0.828]
Epoch [33/120    avg_loss:0.473, val_acc:0.855]
Epoch [34/120    avg_loss:0.406, val_acc:0.879]
Epoch [35/120    avg_loss:0.372, val_acc:0.887]
Epoch [36/120    avg_loss:0.352, val_acc:0.884]
Epoch [37/120    avg_loss:0.316, val_acc:0.879]
Epoch [38/120    avg_loss:0.293, val_acc:0.901]
Epoch [39/120    avg_loss:0.313, val_acc:0.893]
Epoch [40/120    avg_loss:0.271, val_acc:0.917]
Epoch [41/120    avg_loss:0.237, val_acc:0.918]
Epoch [42/120    avg_loss:0.208, val_acc:0.930]
Epoch [43/120    avg_loss:0.243, val_acc:0.927]
Epoch [44/120    avg_loss:0.228, val_acc:0.926]
Epoch [45/120    avg_loss:0.163, val_acc:0.946]
Epoch [46/120    avg_loss:0.159, val_acc:0.926]
Epoch [47/120    avg_loss:0.183, val_acc:0.927]
Epoch [48/120    avg_loss:0.148, val_acc:0.940]
Epoch [49/120    avg_loss:0.146, val_acc:0.933]
Epoch [50/120    avg_loss:0.184, val_acc:0.920]
Epoch [51/120    avg_loss:0.191, val_acc:0.924]
Epoch [52/120    avg_loss:0.164, val_acc:0.938]
Epoch [53/120    avg_loss:0.132, val_acc:0.932]
Epoch [54/120    avg_loss:0.107, val_acc:0.946]
Epoch [55/120    avg_loss:0.111, val_acc:0.927]
Epoch [56/120    avg_loss:0.099, val_acc:0.948]
Epoch [57/120    avg_loss:0.104, val_acc:0.962]
Epoch [58/120    avg_loss:0.093, val_acc:0.959]
Epoch [59/120    avg_loss:0.071, val_acc:0.961]
Epoch [60/120    avg_loss:0.077, val_acc:0.962]
Epoch [61/120    avg_loss:0.072, val_acc:0.951]
Epoch [62/120    avg_loss:0.075, val_acc:0.959]
Epoch [63/120    avg_loss:0.077, val_acc:0.963]
Epoch [64/120    avg_loss:0.073, val_acc:0.959]
Epoch [65/120    avg_loss:0.087, val_acc:0.966]
Epoch [66/120    avg_loss:0.058, val_acc:0.964]
Epoch [67/120    avg_loss:0.063, val_acc:0.976]
Epoch [68/120    avg_loss:0.065, val_acc:0.958]
Epoch [69/120    avg_loss:0.107, val_acc:0.914]
Epoch [70/120    avg_loss:0.178, val_acc:0.920]
Epoch [71/120    avg_loss:0.133, val_acc:0.942]
Epoch [72/120    avg_loss:0.090, val_acc:0.948]
Epoch [73/120    avg_loss:0.083, val_acc:0.962]
Epoch [74/120    avg_loss:0.074, val_acc:0.961]
Epoch [75/120    avg_loss:0.059, val_acc:0.971]
Epoch [76/120    avg_loss:0.059, val_acc:0.964]
Epoch [77/120    avg_loss:0.049, val_acc:0.968]
Epoch [78/120    avg_loss:0.037, val_acc:0.974]
Epoch [79/120    avg_loss:0.034, val_acc:0.971]
Epoch [80/120    avg_loss:0.048, val_acc:0.968]
Epoch [81/120    avg_loss:0.046, val_acc:0.977]
Epoch [82/120    avg_loss:0.037, val_acc:0.976]
Epoch [83/120    avg_loss:0.032, val_acc:0.976]
Epoch [84/120    avg_loss:0.031, val_acc:0.978]
Epoch [85/120    avg_loss:0.033, val_acc:0.982]
Epoch [86/120    avg_loss:0.029, val_acc:0.982]
Epoch [87/120    avg_loss:0.031, val_acc:0.982]
Epoch [88/120    avg_loss:0.031, val_acc:0.978]
Epoch [89/120    avg_loss:0.025, val_acc:0.979]
Epoch [90/120    avg_loss:0.027, val_acc:0.979]
Epoch [91/120    avg_loss:0.031, val_acc:0.983]
Epoch [92/120    avg_loss:0.030, val_acc:0.982]
Epoch [93/120    avg_loss:0.032, val_acc:0.980]
Epoch [94/120    avg_loss:0.030, val_acc:0.979]
Epoch [95/120    avg_loss:0.030, val_acc:0.977]
Epoch [96/120    avg_loss:0.029, val_acc:0.980]
Epoch [97/120    avg_loss:0.029, val_acc:0.980]
Epoch [98/120    avg_loss:0.025, val_acc:0.980]
Epoch [99/120    avg_loss:0.033, val_acc:0.980]
Epoch [100/120    avg_loss:0.028, val_acc:0.982]
Epoch [101/120    avg_loss:0.025, val_acc:0.984]
Epoch [102/120    avg_loss:0.026, val_acc:0.982]
Epoch [103/120    avg_loss:0.029, val_acc:0.982]
Epoch [104/120    avg_loss:0.028, val_acc:0.982]
Epoch [105/120    avg_loss:0.025, val_acc:0.983]
Epoch [106/120    avg_loss:0.028, val_acc:0.982]
Epoch [107/120    avg_loss:0.026, val_acc:0.979]
Epoch [108/120    avg_loss:0.023, val_acc:0.983]
Epoch [109/120    avg_loss:0.022, val_acc:0.984]
Epoch [110/120    avg_loss:0.027, val_acc:0.984]
Epoch [111/120    avg_loss:0.025, val_acc:0.983]
Epoch [112/120    avg_loss:0.024, val_acc:0.982]
Epoch [113/120    avg_loss:0.023, val_acc:0.978]
Epoch [114/120    avg_loss:0.025, val_acc:0.984]
Epoch [115/120    avg_loss:0.022, val_acc:0.983]
Epoch [116/120    avg_loss:0.026, val_acc:0.983]
Epoch [117/120    avg_loss:0.026, val_acc:0.983]
Epoch [118/120    avg_loss:0.021, val_acc:0.983]
Epoch [119/120    avg_loss:0.022, val_acc:0.982]
Epoch [120/120    avg_loss:0.026, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1246    4    6    0    4    0    0    1    5   17    0    0
     0    2    0]
 [   0    0    2  692    2   12    0    0    0   10    3    3   23    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    2    0    1    0    0    0    0
    10    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    1    1    0
     0    0    0]
 [   0    0   11    0    0   14    0    0    0    0  834    9    0    0
     0    7    0]
 [   0    0   18    0    0    0    5    0    1    0   15 2160   10    1
     0    0    0]
 [   0    0    0    0    0   13    0    0    0    0    6    5  502    0
     5    1    2]
 [   0    0    0    0    0    1    0    0    0    0    0    1    0  183
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    19  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.8780487804878

F1 scores:
[       nan 0.975      0.97229809 0.95911296 0.98156682 0.94091416
 0.97393894 0.96153846 0.99416569 0.69565217 0.95862069 0.97981402
 0.9330855  0.99186992 0.98485504 0.92145015 0.97619048]

Kappa:
0.9644156083363911
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f732f5a2b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.802, val_acc:0.157]
Epoch [2/120    avg_loss:2.703, val_acc:0.295]
Epoch [3/120    avg_loss:2.610, val_acc:0.396]
Epoch [4/120    avg_loss:2.531, val_acc:0.451]
Epoch [5/120    avg_loss:2.440, val_acc:0.492]
Epoch [6/120    avg_loss:2.374, val_acc:0.510]
Epoch [7/120    avg_loss:2.300, val_acc:0.517]
Epoch [8/120    avg_loss:2.202, val_acc:0.527]
Epoch [9/120    avg_loss:2.139, val_acc:0.551]
Epoch [10/120    avg_loss:2.066, val_acc:0.542]
Epoch [11/120    avg_loss:2.002, val_acc:0.515]
Epoch [12/120    avg_loss:1.915, val_acc:0.505]
Epoch [13/120    avg_loss:1.857, val_acc:0.564]
Epoch [14/120    avg_loss:1.750, val_acc:0.567]
Epoch [15/120    avg_loss:1.726, val_acc:0.565]
Epoch [16/120    avg_loss:1.676, val_acc:0.590]
Epoch [17/120    avg_loss:1.500, val_acc:0.618]
Epoch [18/120    avg_loss:1.460, val_acc:0.605]
Epoch [19/120    avg_loss:1.419, val_acc:0.626]
Epoch [20/120    avg_loss:1.298, val_acc:0.634]
Epoch [21/120    avg_loss:1.258, val_acc:0.625]
Epoch [22/120    avg_loss:1.239, val_acc:0.651]
Epoch [23/120    avg_loss:1.082, val_acc:0.682]
Epoch [24/120    avg_loss:1.031, val_acc:0.686]
Epoch [25/120    avg_loss:0.947, val_acc:0.726]
Epoch [26/120    avg_loss:0.862, val_acc:0.754]
Epoch [27/120    avg_loss:0.819, val_acc:0.777]
Epoch [28/120    avg_loss:0.753, val_acc:0.748]
Epoch [29/120    avg_loss:0.759, val_acc:0.775]
Epoch [30/120    avg_loss:0.691, val_acc:0.793]
Epoch [31/120    avg_loss:0.625, val_acc:0.766]
Epoch [32/120    avg_loss:0.597, val_acc:0.805]
Epoch [33/120    avg_loss:0.565, val_acc:0.791]
Epoch [34/120    avg_loss:0.496, val_acc:0.801]
Epoch [35/120    avg_loss:0.474, val_acc:0.848]
Epoch [36/120    avg_loss:0.434, val_acc:0.828]
Epoch [37/120    avg_loss:0.411, val_acc:0.827]
Epoch [38/120    avg_loss:0.371, val_acc:0.839]
Epoch [39/120    avg_loss:0.422, val_acc:0.850]
Epoch [40/120    avg_loss:0.409, val_acc:0.834]
Epoch [41/120    avg_loss:0.332, val_acc:0.866]
Epoch [42/120    avg_loss:0.297, val_acc:0.859]
Epoch [43/120    avg_loss:0.279, val_acc:0.889]
Epoch [44/120    avg_loss:0.270, val_acc:0.892]
Epoch [45/120    avg_loss:0.232, val_acc:0.872]
Epoch [46/120    avg_loss:0.230, val_acc:0.899]
Epoch [47/120    avg_loss:0.222, val_acc:0.900]
Epoch [48/120    avg_loss:0.205, val_acc:0.890]
Epoch [49/120    avg_loss:0.198, val_acc:0.913]
Epoch [50/120    avg_loss:0.205, val_acc:0.918]
Epoch [51/120    avg_loss:0.173, val_acc:0.927]
Epoch [52/120    avg_loss:0.170, val_acc:0.922]
Epoch [53/120    avg_loss:0.157, val_acc:0.909]
Epoch [54/120    avg_loss:0.179, val_acc:0.911]
Epoch [55/120    avg_loss:0.206, val_acc:0.912]
Epoch [56/120    avg_loss:0.198, val_acc:0.901]
Epoch [57/120    avg_loss:0.192, val_acc:0.910]
Epoch [58/120    avg_loss:0.163, val_acc:0.905]
Epoch [59/120    avg_loss:0.180, val_acc:0.902]
Epoch [60/120    avg_loss:0.133, val_acc:0.936]
Epoch [61/120    avg_loss:0.158, val_acc:0.899]
Epoch [62/120    avg_loss:0.162, val_acc:0.925]
Epoch [63/120    avg_loss:0.140, val_acc:0.933]
Epoch [64/120    avg_loss:0.154, val_acc:0.938]
Epoch [65/120    avg_loss:0.135, val_acc:0.932]
Epoch [66/120    avg_loss:0.109, val_acc:0.954]
Epoch [67/120    avg_loss:0.100, val_acc:0.945]
Epoch [68/120    avg_loss:0.101, val_acc:0.961]
Epoch [69/120    avg_loss:0.080, val_acc:0.964]
Epoch [70/120    avg_loss:0.086, val_acc:0.952]
Epoch [71/120    avg_loss:0.092, val_acc:0.948]
Epoch [72/120    avg_loss:0.077, val_acc:0.955]
Epoch [73/120    avg_loss:0.080, val_acc:0.957]
Epoch [74/120    avg_loss:0.056, val_acc:0.968]
Epoch [75/120    avg_loss:0.065, val_acc:0.966]
Epoch [76/120    avg_loss:0.061, val_acc:0.961]
Epoch [77/120    avg_loss:0.067, val_acc:0.960]
Epoch [78/120    avg_loss:0.092, val_acc:0.949]
Epoch [79/120    avg_loss:0.084, val_acc:0.953]
Epoch [80/120    avg_loss:0.074, val_acc:0.950]
Epoch [81/120    avg_loss:0.084, val_acc:0.953]
Epoch [82/120    avg_loss:0.076, val_acc:0.953]
Epoch [83/120    avg_loss:0.093, val_acc:0.950]
Epoch [84/120    avg_loss:0.073, val_acc:0.959]
Epoch [85/120    avg_loss:0.077, val_acc:0.965]
Epoch [86/120    avg_loss:0.066, val_acc:0.971]
Epoch [87/120    avg_loss:0.048, val_acc:0.959]
Epoch [88/120    avg_loss:0.061, val_acc:0.965]
Epoch [89/120    avg_loss:0.045, val_acc:0.970]
Epoch [90/120    avg_loss:0.049, val_acc:0.973]
Epoch [91/120    avg_loss:0.044, val_acc:0.961]
Epoch [92/120    avg_loss:0.047, val_acc:0.965]
Epoch [93/120    avg_loss:0.045, val_acc:0.966]
Epoch [94/120    avg_loss:0.042, val_acc:0.965]
Epoch [95/120    avg_loss:0.045, val_acc:0.966]
Epoch [96/120    avg_loss:0.039, val_acc:0.966]
Epoch [97/120    avg_loss:0.035, val_acc:0.968]
Epoch [98/120    avg_loss:0.032, val_acc:0.971]
Epoch [99/120    avg_loss:0.033, val_acc:0.970]
Epoch [100/120    avg_loss:0.033, val_acc:0.976]
Epoch [101/120    avg_loss:0.032, val_acc:0.977]
Epoch [102/120    avg_loss:0.030, val_acc:0.980]
Epoch [103/120    avg_loss:0.027, val_acc:0.976]
Epoch [104/120    avg_loss:0.024, val_acc:0.977]
Epoch [105/120    avg_loss:0.031, val_acc:0.968]
Epoch [106/120    avg_loss:0.036, val_acc:0.970]
Epoch [107/120    avg_loss:0.041, val_acc:0.971]
Epoch [108/120    avg_loss:0.035, val_acc:0.977]
Epoch [109/120    avg_loss:0.031, val_acc:0.966]
Epoch [110/120    avg_loss:0.064, val_acc:0.965]
Epoch [111/120    avg_loss:0.044, val_acc:0.963]
Epoch [112/120    avg_loss:0.049, val_acc:0.961]
Epoch [113/120    avg_loss:0.042, val_acc:0.966]
Epoch [114/120    avg_loss:0.034, val_acc:0.971]
Epoch [115/120    avg_loss:0.024, val_acc:0.973]
Epoch [116/120    avg_loss:0.023, val_acc:0.976]
Epoch [117/120    avg_loss:0.025, val_acc:0.971]
Epoch [118/120    avg_loss:0.023, val_acc:0.972]
Epoch [119/120    avg_loss:0.021, val_acc:0.973]
Epoch [120/120    avg_loss:0.022, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1247    8    6    0    1    0    0    0    4   16    2    0
     0    1    0]
 [   0    0    9  708    0    6    0    0    0    6    1    1   16    0
     0    0    0]
 [   0    0    0    0  210    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    1    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   14    0    0    2    0
     0    0    0]
 [   0    0   16    0    0    3    0    0    0    0  849    5    0    0
     0    2    0]
 [   0    1    9    0    0    0    2    0    0    0   21 2175    0    1
     0    1    0]
 [   0    0    0    2    0    1    0    0    0    2   17    0  507    0
     0    2    3]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1135    3    0]
 [   0    0    0    0    0    1   27    0    0    0    0    0    0    0
    17  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.44173441734418

F1 scores:
[       nan 0.92307692 0.97194076 0.96523517 0.97902098 0.98057143
 0.97691735 1.         1.         0.66666667 0.95769882 0.98684211
 0.95121951 0.99459459 0.98867596 0.91793313 0.9704142 ]

Kappa:
0.9708345697706525
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc339870ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.819, val_acc:0.120]
Epoch [2/120    avg_loss:2.693, val_acc:0.250]
Epoch [3/120    avg_loss:2.580, val_acc:0.360]
Epoch [4/120    avg_loss:2.495, val_acc:0.465]
Epoch [5/120    avg_loss:2.418, val_acc:0.471]
Epoch [6/120    avg_loss:2.371, val_acc:0.496]
Epoch [7/120    avg_loss:2.325, val_acc:0.524]
Epoch [8/120    avg_loss:2.270, val_acc:0.528]
Epoch [9/120    avg_loss:2.236, val_acc:0.521]
Epoch [10/120    avg_loss:2.186, val_acc:0.538]
Epoch [11/120    avg_loss:2.091, val_acc:0.542]
Epoch [12/120    avg_loss:2.053, val_acc:0.537]
Epoch [13/120    avg_loss:1.994, val_acc:0.530]
Epoch [14/120    avg_loss:1.937, val_acc:0.547]
Epoch [15/120    avg_loss:1.881, val_acc:0.553]
Epoch [16/120    avg_loss:1.809, val_acc:0.559]
Epoch [17/120    avg_loss:1.721, val_acc:0.603]
Epoch [18/120    avg_loss:1.663, val_acc:0.600]
Epoch [19/120    avg_loss:1.640, val_acc:0.604]
Epoch [20/120    avg_loss:1.529, val_acc:0.624]
Epoch [21/120    avg_loss:1.506, val_acc:0.640]
Epoch [22/120    avg_loss:1.422, val_acc:0.654]
Epoch [23/120    avg_loss:1.351, val_acc:0.684]
Epoch [24/120    avg_loss:1.274, val_acc:0.699]
Epoch [25/120    avg_loss:1.189, val_acc:0.737]
Epoch [26/120    avg_loss:1.118, val_acc:0.734]
Epoch [27/120    avg_loss:1.052, val_acc:0.738]
Epoch [28/120    avg_loss:0.919, val_acc:0.741]
Epoch [29/120    avg_loss:0.872, val_acc:0.760]
Epoch [30/120    avg_loss:0.815, val_acc:0.755]
Epoch [31/120    avg_loss:0.717, val_acc:0.787]
Epoch [32/120    avg_loss:0.621, val_acc:0.812]
Epoch [33/120    avg_loss:0.624, val_acc:0.828]
Epoch [34/120    avg_loss:0.596, val_acc:0.845]
Epoch [35/120    avg_loss:0.549, val_acc:0.843]
Epoch [36/120    avg_loss:0.508, val_acc:0.864]
Epoch [37/120    avg_loss:0.457, val_acc:0.848]
Epoch [38/120    avg_loss:0.472, val_acc:0.848]
Epoch [39/120    avg_loss:0.408, val_acc:0.883]
Epoch [40/120    avg_loss:0.326, val_acc:0.887]
Epoch [41/120    avg_loss:0.358, val_acc:0.874]
Epoch [42/120    avg_loss:0.365, val_acc:0.885]
Epoch [43/120    avg_loss:0.363, val_acc:0.874]
Epoch [44/120    avg_loss:0.411, val_acc:0.841]
Epoch [45/120    avg_loss:0.364, val_acc:0.882]
Epoch [46/120    avg_loss:0.300, val_acc:0.889]
Epoch [47/120    avg_loss:0.261, val_acc:0.915]
Epoch [48/120    avg_loss:0.213, val_acc:0.933]
Epoch [49/120    avg_loss:0.239, val_acc:0.921]
Epoch [50/120    avg_loss:0.216, val_acc:0.913]
Epoch [51/120    avg_loss:0.180, val_acc:0.936]
Epoch [52/120    avg_loss:0.173, val_acc:0.926]
Epoch [53/120    avg_loss:0.138, val_acc:0.951]
Epoch [54/120    avg_loss:0.137, val_acc:0.935]
Epoch [55/120    avg_loss:0.140, val_acc:0.941]
Epoch [56/120    avg_loss:0.148, val_acc:0.930]
Epoch [57/120    avg_loss:0.151, val_acc:0.927]
Epoch [58/120    avg_loss:0.119, val_acc:0.954]
Epoch [59/120    avg_loss:0.107, val_acc:0.951]
Epoch [60/120    avg_loss:0.113, val_acc:0.912]
Epoch [61/120    avg_loss:0.150, val_acc:0.916]
Epoch [62/120    avg_loss:0.166, val_acc:0.926]
Epoch [63/120    avg_loss:0.119, val_acc:0.951]
Epoch [64/120    avg_loss:0.132, val_acc:0.920]
Epoch [65/120    avg_loss:0.120, val_acc:0.922]
Epoch [66/120    avg_loss:0.129, val_acc:0.937]
Epoch [67/120    avg_loss:0.113, val_acc:0.946]
Epoch [68/120    avg_loss:0.096, val_acc:0.953]
Epoch [69/120    avg_loss:0.077, val_acc:0.955]
Epoch [70/120    avg_loss:0.061, val_acc:0.957]
Epoch [71/120    avg_loss:0.074, val_acc:0.964]
Epoch [72/120    avg_loss:0.060, val_acc:0.952]
Epoch [73/120    avg_loss:0.059, val_acc:0.958]
Epoch [74/120    avg_loss:0.056, val_acc:0.959]
Epoch [75/120    avg_loss:0.066, val_acc:0.959]
Epoch [76/120    avg_loss:0.056, val_acc:0.955]
Epoch [77/120    avg_loss:0.064, val_acc:0.948]
Epoch [78/120    avg_loss:0.069, val_acc:0.964]
Epoch [79/120    avg_loss:0.046, val_acc:0.961]
Epoch [80/120    avg_loss:0.055, val_acc:0.959]
Epoch [81/120    avg_loss:0.064, val_acc:0.940]
Epoch [82/120    avg_loss:0.079, val_acc:0.939]
Epoch [83/120    avg_loss:0.075, val_acc:0.967]
Epoch [84/120    avg_loss:0.064, val_acc:0.968]
Epoch [85/120    avg_loss:0.057, val_acc:0.948]
Epoch [86/120    avg_loss:0.048, val_acc:0.958]
Epoch [87/120    avg_loss:0.047, val_acc:0.973]
Epoch [88/120    avg_loss:0.043, val_acc:0.970]
Epoch [89/120    avg_loss:0.041, val_acc:0.971]
Epoch [90/120    avg_loss:0.058, val_acc:0.962]
Epoch [91/120    avg_loss:0.044, val_acc:0.958]
Epoch [92/120    avg_loss:0.038, val_acc:0.965]
Epoch [93/120    avg_loss:0.055, val_acc:0.949]
Epoch [94/120    avg_loss:0.061, val_acc:0.950]
Epoch [95/120    avg_loss:0.065, val_acc:0.953]
Epoch [96/120    avg_loss:0.073, val_acc:0.955]
Epoch [97/120    avg_loss:0.067, val_acc:0.957]
Epoch [98/120    avg_loss:0.055, val_acc:0.959]
Epoch [99/120    avg_loss:0.041, val_acc:0.961]
Epoch [100/120    avg_loss:0.046, val_acc:0.961]
Epoch [101/120    avg_loss:0.031, val_acc:0.965]
Epoch [102/120    avg_loss:0.033, val_acc:0.963]
Epoch [103/120    avg_loss:0.030, val_acc:0.967]
Epoch [104/120    avg_loss:0.024, val_acc:0.967]
Epoch [105/120    avg_loss:0.023, val_acc:0.968]
Epoch [106/120    avg_loss:0.024, val_acc:0.968]
Epoch [107/120    avg_loss:0.027, val_acc:0.968]
Epoch [108/120    avg_loss:0.026, val_acc:0.967]
Epoch [109/120    avg_loss:0.019, val_acc:0.967]
Epoch [110/120    avg_loss:0.019, val_acc:0.970]
Epoch [111/120    avg_loss:0.021, val_acc:0.971]
Epoch [112/120    avg_loss:0.025, val_acc:0.971]
Epoch [113/120    avg_loss:0.018, val_acc:0.972]
Epoch [114/120    avg_loss:0.018, val_acc:0.972]
Epoch [115/120    avg_loss:0.024, val_acc:0.972]
Epoch [116/120    avg_loss:0.019, val_acc:0.972]
Epoch [117/120    avg_loss:0.022, val_acc:0.972]
Epoch [118/120    avg_loss:0.019, val_acc:0.972]
Epoch [119/120    avg_loss:0.020, val_acc:0.973]
Epoch [120/120    avg_loss:0.020, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1245    1    0    0    1    0    0    0   12   24    2    0
     0    0    0]
 [   0    0    1  704    9    6    0    0    0    8    2    2   15    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   24    0    0    7    1    0    0    0  830    4    9    0
     0    0    0]
 [   0    3    8    0    0    3    1    0    0    0   16 2172    6    1
     0    0    0]
 [   0    0    1    0    1    1    0    0    0    0    1   17  509    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    1    0    1    0    0    1    0    0    0
  1130    6    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    17  316    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.32249322493224

F1 scores:
[       nan 0.91358025 0.97113885 0.96969697 0.97706422 0.97395243
 0.98417483 0.98039216 1.         0.72727273 0.95347501 0.9797023
 0.9452182  0.99459459 0.98689956 0.94469357 0.98245614]

Kappa:
0.9694697027758677
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f66d514fa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.825, val_acc:0.208]
Epoch [2/120    avg_loss:2.716, val_acc:0.260]
Epoch [3/120    avg_loss:2.614, val_acc:0.264]
Epoch [4/120    avg_loss:2.543, val_acc:0.301]
Epoch [5/120    avg_loss:2.475, val_acc:0.317]
Epoch [6/120    avg_loss:2.361, val_acc:0.329]
Epoch [7/120    avg_loss:2.322, val_acc:0.363]
Epoch [8/120    avg_loss:2.255, val_acc:0.397]
Epoch [9/120    avg_loss:2.149, val_acc:0.397]
Epoch [10/120    avg_loss:2.120, val_acc:0.414]
Epoch [11/120    avg_loss:2.072, val_acc:0.537]
Epoch [12/120    avg_loss:1.946, val_acc:0.563]
Epoch [13/120    avg_loss:1.967, val_acc:0.523]
Epoch [14/120    avg_loss:1.880, val_acc:0.573]
Epoch [15/120    avg_loss:1.790, val_acc:0.564]
Epoch [16/120    avg_loss:1.712, val_acc:0.521]
Epoch [17/120    avg_loss:1.676, val_acc:0.550]
Epoch [18/120    avg_loss:1.638, val_acc:0.583]
Epoch [19/120    avg_loss:1.557, val_acc:0.621]
Epoch [20/120    avg_loss:1.479, val_acc:0.611]
Epoch [21/120    avg_loss:1.427, val_acc:0.659]
Epoch [22/120    avg_loss:1.362, val_acc:0.672]
Epoch [23/120    avg_loss:1.236, val_acc:0.682]
Epoch [24/120    avg_loss:1.147, val_acc:0.703]
Epoch [25/120    avg_loss:1.067, val_acc:0.721]
Epoch [26/120    avg_loss:0.954, val_acc:0.734]
Epoch [27/120    avg_loss:0.895, val_acc:0.750]
Epoch [28/120    avg_loss:0.852, val_acc:0.728]
Epoch [29/120    avg_loss:0.814, val_acc:0.715]
Epoch [30/120    avg_loss:0.801, val_acc:0.732]
Epoch [31/120    avg_loss:0.731, val_acc:0.779]
Epoch [32/120    avg_loss:0.719, val_acc:0.777]
Epoch [33/120    avg_loss:0.603, val_acc:0.787]
Epoch [34/120    avg_loss:0.537, val_acc:0.805]
Epoch [35/120    avg_loss:0.545, val_acc:0.836]
Epoch [36/120    avg_loss:0.467, val_acc:0.836]
Epoch [37/120    avg_loss:0.435, val_acc:0.845]
Epoch [38/120    avg_loss:0.370, val_acc:0.860]
Epoch [39/120    avg_loss:0.369, val_acc:0.870]
Epoch [40/120    avg_loss:0.337, val_acc:0.832]
Epoch [41/120    avg_loss:0.364, val_acc:0.855]
Epoch [42/120    avg_loss:0.341, val_acc:0.842]
Epoch [43/120    avg_loss:0.304, val_acc:0.868]
Epoch [44/120    avg_loss:0.288, val_acc:0.868]
Epoch [45/120    avg_loss:0.240, val_acc:0.887]
Epoch [46/120    avg_loss:0.220, val_acc:0.907]
Epoch [47/120    avg_loss:0.226, val_acc:0.903]
Epoch [48/120    avg_loss:0.222, val_acc:0.884]
Epoch [49/120    avg_loss:0.213, val_acc:0.918]
Epoch [50/120    avg_loss:0.194, val_acc:0.896]
Epoch [51/120    avg_loss:0.195, val_acc:0.923]
Epoch [52/120    avg_loss:0.180, val_acc:0.917]
Epoch [53/120    avg_loss:0.166, val_acc:0.903]
Epoch [54/120    avg_loss:0.199, val_acc:0.927]
Epoch [55/120    avg_loss:0.177, val_acc:0.903]
Epoch [56/120    avg_loss:0.165, val_acc:0.911]
Epoch [57/120    avg_loss:0.150, val_acc:0.924]
Epoch [58/120    avg_loss:0.114, val_acc:0.934]
Epoch [59/120    avg_loss:0.107, val_acc:0.933]
Epoch [60/120    avg_loss:0.120, val_acc:0.937]
Epoch [61/120    avg_loss:0.108, val_acc:0.935]
Epoch [62/120    avg_loss:0.108, val_acc:0.928]
Epoch [63/120    avg_loss:0.112, val_acc:0.933]
Epoch [64/120    avg_loss:0.094, val_acc:0.939]
Epoch [65/120    avg_loss:0.105, val_acc:0.927]
Epoch [66/120    avg_loss:0.116, val_acc:0.921]
Epoch [67/120    avg_loss:0.108, val_acc:0.938]
Epoch [68/120    avg_loss:0.084, val_acc:0.938]
Epoch [69/120    avg_loss:0.084, val_acc:0.945]
Epoch [70/120    avg_loss:0.091, val_acc:0.941]
Epoch [71/120    avg_loss:0.068, val_acc:0.953]
Epoch [72/120    avg_loss:0.076, val_acc:0.950]
Epoch [73/120    avg_loss:0.068, val_acc:0.946]
Epoch [74/120    avg_loss:0.054, val_acc:0.938]
Epoch [75/120    avg_loss:0.070, val_acc:0.955]
Epoch [76/120    avg_loss:0.057, val_acc:0.939]
Epoch [77/120    avg_loss:0.061, val_acc:0.952]
Epoch [78/120    avg_loss:0.060, val_acc:0.950]
Epoch [79/120    avg_loss:0.063, val_acc:0.949]
Epoch [80/120    avg_loss:0.087, val_acc:0.915]
Epoch [81/120    avg_loss:0.124, val_acc:0.954]
Epoch [82/120    avg_loss:0.101, val_acc:0.945]
Epoch [83/120    avg_loss:0.096, val_acc:0.928]
Epoch [84/120    avg_loss:0.106, val_acc:0.923]
Epoch [85/120    avg_loss:0.106, val_acc:0.933]
Epoch [86/120    avg_loss:0.077, val_acc:0.951]
Epoch [87/120    avg_loss:0.084, val_acc:0.942]
Epoch [88/120    avg_loss:0.080, val_acc:0.948]
Epoch [89/120    avg_loss:0.083, val_acc:0.955]
Epoch [90/120    avg_loss:0.069, val_acc:0.957]
Epoch [91/120    avg_loss:0.047, val_acc:0.957]
Epoch [92/120    avg_loss:0.043, val_acc:0.959]
Epoch [93/120    avg_loss:0.051, val_acc:0.958]
Epoch [94/120    avg_loss:0.037, val_acc:0.960]
Epoch [95/120    avg_loss:0.044, val_acc:0.961]
Epoch [96/120    avg_loss:0.045, val_acc:0.962]
Epoch [97/120    avg_loss:0.035, val_acc:0.962]
Epoch [98/120    avg_loss:0.035, val_acc:0.961]
Epoch [99/120    avg_loss:0.036, val_acc:0.963]
Epoch [100/120    avg_loss:0.032, val_acc:0.962]
Epoch [101/120    avg_loss:0.032, val_acc:0.963]
Epoch [102/120    avg_loss:0.036, val_acc:0.962]
Epoch [103/120    avg_loss:0.035, val_acc:0.961]
Epoch [104/120    avg_loss:0.035, val_acc:0.962]
Epoch [105/120    avg_loss:0.032, val_acc:0.963]
Epoch [106/120    avg_loss:0.032, val_acc:0.962]
Epoch [107/120    avg_loss:0.030, val_acc:0.963]
Epoch [108/120    avg_loss:0.030, val_acc:0.964]
Epoch [109/120    avg_loss:0.029, val_acc:0.964]
Epoch [110/120    avg_loss:0.030, val_acc:0.963]
Epoch [111/120    avg_loss:0.032, val_acc:0.963]
Epoch [112/120    avg_loss:0.031, val_acc:0.964]
Epoch [113/120    avg_loss:0.032, val_acc:0.962]
Epoch [114/120    avg_loss:0.033, val_acc:0.963]
Epoch [115/120    avg_loss:0.031, val_acc:0.961]
Epoch [116/120    avg_loss:0.031, val_acc:0.962]
Epoch [117/120    avg_loss:0.029, val_acc:0.965]
Epoch [118/120    avg_loss:0.028, val_acc:0.963]
Epoch [119/120    avg_loss:0.031, val_acc:0.964]
Epoch [120/120    avg_loss:0.033, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1197    5    7    0    2    0    0    0    4   70    0    0
     0    0    0]
 [   0    0    0  707    4    0    0    0    0   13    1    0   22    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   27    3    0    5    0    0    0    0  821   16    3    0
     0    0    0]
 [   0    0    5    0    0    0    1    0    0    0   16 2179    7    2
     0    0    0]
 [   0    0    0    8    0    5    0    0    0    0    2    1  510    0
     3    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    0    0    0
  1133    3    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    13  320    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
96.93224932249322

F1 scores:
[       nan 0.94871795 0.9522673  0.96059783 0.97482838 0.98633257
 0.98646617 1.         1.         0.66666667 0.95133256 0.97320232
 0.94444444 0.99462366 0.99038462 0.95522388 0.94674556]

Kappa:
0.9650037134060466
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2423bdaac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.799, val_acc:0.186]
Epoch [2/120    avg_loss:2.695, val_acc:0.245]
Epoch [3/120    avg_loss:2.613, val_acc:0.259]
Epoch [4/120    avg_loss:2.526, val_acc:0.291]
Epoch [5/120    avg_loss:2.480, val_acc:0.323]
Epoch [6/120    avg_loss:2.388, val_acc:0.350]
Epoch [7/120    avg_loss:2.321, val_acc:0.377]
Epoch [8/120    avg_loss:2.283, val_acc:0.416]
Epoch [9/120    avg_loss:2.251, val_acc:0.407]
Epoch [10/120    avg_loss:2.197, val_acc:0.463]
Epoch [11/120    avg_loss:2.123, val_acc:0.485]
Epoch [12/120    avg_loss:2.036, val_acc:0.497]
Epoch [13/120    avg_loss:1.973, val_acc:0.543]
Epoch [14/120    avg_loss:1.903, val_acc:0.614]
Epoch [15/120    avg_loss:1.878, val_acc:0.578]
Epoch [16/120    avg_loss:1.837, val_acc:0.624]
Epoch [17/120    avg_loss:1.755, val_acc:0.643]
Epoch [18/120    avg_loss:1.640, val_acc:0.665]
Epoch [19/120    avg_loss:1.596, val_acc:0.662]
Epoch [20/120    avg_loss:1.521, val_acc:0.692]
Epoch [21/120    avg_loss:1.443, val_acc:0.704]
Epoch [22/120    avg_loss:1.345, val_acc:0.690]
Epoch [23/120    avg_loss:1.277, val_acc:0.700]
Epoch [24/120    avg_loss:1.184, val_acc:0.739]
Epoch [25/120    avg_loss:1.163, val_acc:0.735]
Epoch [26/120    avg_loss:1.054, val_acc:0.743]
Epoch [27/120    avg_loss:1.006, val_acc:0.758]
Epoch [28/120    avg_loss:0.944, val_acc:0.698]
Epoch [29/120    avg_loss:0.837, val_acc:0.751]
Epoch [30/120    avg_loss:0.817, val_acc:0.801]
Epoch [31/120    avg_loss:0.723, val_acc:0.810]
Epoch [32/120    avg_loss:0.688, val_acc:0.821]
Epoch [33/120    avg_loss:0.630, val_acc:0.821]
Epoch [34/120    avg_loss:0.588, val_acc:0.851]
Epoch [35/120    avg_loss:0.586, val_acc:0.864]
Epoch [36/120    avg_loss:0.507, val_acc:0.852]
Epoch [37/120    avg_loss:0.442, val_acc:0.860]
Epoch [38/120    avg_loss:0.473, val_acc:0.822]
Epoch [39/120    avg_loss:0.434, val_acc:0.860]
Epoch [40/120    avg_loss:0.387, val_acc:0.888]
Epoch [41/120    avg_loss:0.360, val_acc:0.876]
Epoch [42/120    avg_loss:0.334, val_acc:0.901]
Epoch [43/120    avg_loss:0.317, val_acc:0.887]
Epoch [44/120    avg_loss:0.271, val_acc:0.909]
Epoch [45/120    avg_loss:0.301, val_acc:0.883]
Epoch [46/120    avg_loss:0.239, val_acc:0.915]
Epoch [47/120    avg_loss:0.249, val_acc:0.908]
Epoch [48/120    avg_loss:0.263, val_acc:0.879]
Epoch [49/120    avg_loss:0.345, val_acc:0.882]
Epoch [50/120    avg_loss:0.241, val_acc:0.891]
Epoch [51/120    avg_loss:0.261, val_acc:0.887]
Epoch [52/120    avg_loss:0.342, val_acc:0.899]
Epoch [53/120    avg_loss:0.312, val_acc:0.893]
Epoch [54/120    avg_loss:0.253, val_acc:0.874]
Epoch [55/120    avg_loss:0.228, val_acc:0.911]
Epoch [56/120    avg_loss:0.191, val_acc:0.939]
Epoch [57/120    avg_loss:0.164, val_acc:0.928]
Epoch [58/120    avg_loss:0.147, val_acc:0.942]
Epoch [59/120    avg_loss:0.144, val_acc:0.921]
Epoch [60/120    avg_loss:0.137, val_acc:0.925]
Epoch [61/120    avg_loss:0.126, val_acc:0.946]
Epoch [62/120    avg_loss:0.136, val_acc:0.932]
Epoch [63/120    avg_loss:0.133, val_acc:0.920]
Epoch [64/120    avg_loss:0.164, val_acc:0.909]
Epoch [65/120    avg_loss:0.162, val_acc:0.917]
Epoch [66/120    avg_loss:0.122, val_acc:0.932]
Epoch [67/120    avg_loss:0.104, val_acc:0.930]
Epoch [68/120    avg_loss:0.109, val_acc:0.949]
Epoch [69/120    avg_loss:0.096, val_acc:0.949]
Epoch [70/120    avg_loss:0.087, val_acc:0.947]
Epoch [71/120    avg_loss:0.091, val_acc:0.924]
Epoch [72/120    avg_loss:0.100, val_acc:0.951]
Epoch [73/120    avg_loss:0.103, val_acc:0.941]
Epoch [74/120    avg_loss:0.093, val_acc:0.960]
Epoch [75/120    avg_loss:0.086, val_acc:0.949]
Epoch [76/120    avg_loss:0.074, val_acc:0.953]
Epoch [77/120    avg_loss:0.064, val_acc:0.955]
Epoch [78/120    avg_loss:0.081, val_acc:0.954]
Epoch [79/120    avg_loss:0.073, val_acc:0.940]
Epoch [80/120    avg_loss:0.065, val_acc:0.964]
Epoch [81/120    avg_loss:0.067, val_acc:0.958]
Epoch [82/120    avg_loss:0.071, val_acc:0.940]
Epoch [83/120    avg_loss:0.066, val_acc:0.947]
Epoch [84/120    avg_loss:0.057, val_acc:0.953]
Epoch [85/120    avg_loss:0.048, val_acc:0.961]
Epoch [86/120    avg_loss:0.038, val_acc:0.955]
Epoch [87/120    avg_loss:0.044, val_acc:0.964]
Epoch [88/120    avg_loss:0.036, val_acc:0.973]
Epoch [89/120    avg_loss:0.038, val_acc:0.976]
Epoch [90/120    avg_loss:0.036, val_acc:0.965]
Epoch [91/120    avg_loss:0.037, val_acc:0.958]
Epoch [92/120    avg_loss:0.043, val_acc:0.963]
Epoch [93/120    avg_loss:0.038, val_acc:0.955]
Epoch [94/120    avg_loss:0.040, val_acc:0.967]
Epoch [95/120    avg_loss:0.044, val_acc:0.947]
Epoch [96/120    avg_loss:0.068, val_acc:0.953]
Epoch [97/120    avg_loss:0.065, val_acc:0.965]
Epoch [98/120    avg_loss:0.050, val_acc:0.966]
Epoch [99/120    avg_loss:0.038, val_acc:0.962]
Epoch [100/120    avg_loss:0.036, val_acc:0.966]
Epoch [101/120    avg_loss:0.038, val_acc:0.962]
Epoch [102/120    avg_loss:0.038, val_acc:0.974]
Epoch [103/120    avg_loss:0.031, val_acc:0.976]
Epoch [104/120    avg_loss:0.024, val_acc:0.976]
Epoch [105/120    avg_loss:0.029, val_acc:0.977]
Epoch [106/120    avg_loss:0.025, val_acc:0.977]
Epoch [107/120    avg_loss:0.023, val_acc:0.975]
Epoch [108/120    avg_loss:0.026, val_acc:0.973]
Epoch [109/120    avg_loss:0.023, val_acc:0.975]
Epoch [110/120    avg_loss:0.021, val_acc:0.973]
Epoch [111/120    avg_loss:0.020, val_acc:0.975]
Epoch [112/120    avg_loss:0.020, val_acc:0.972]
Epoch [113/120    avg_loss:0.022, val_acc:0.975]
Epoch [114/120    avg_loss:0.020, val_acc:0.975]
Epoch [115/120    avg_loss:0.021, val_acc:0.974]
Epoch [116/120    avg_loss:0.020, val_acc:0.973]
Epoch [117/120    avg_loss:0.021, val_acc:0.976]
Epoch [118/120    avg_loss:0.020, val_acc:0.975]
Epoch [119/120    avg_loss:0.021, val_acc:0.976]
Epoch [120/120    avg_loss:0.024, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1254    7    0    0    2    0    0    0    4   18    0    0
     0    0    0]
 [   0    0    1  697    0   15    0    0    0   13    0    1   20    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  416    0    7    0    1    0    0    0    0
    11    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   14    2    0    6    6    0    0    0  832   11    3    0
     0    1    0]
 [   0    0   10    0    0    0    3    0    0    0    6 2186    3    2
     0    0    0]
 [   0    0    0    2    2   11    0    0    0    0    1    0  510    0
     3    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   41    0    0    0    0    0    0    0
    12  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.20325203252033

F1 scores:
[       nan 0.94871795 0.97815913 0.9580756  0.9953271  0.94224236
 0.96041056 0.87719298 0.99416569 0.69387755 0.9657574  0.98735321
 0.94795539 0.99462366 0.98783666 0.91588785 0.96511628]

Kappa:
0.9681105316133355
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1914cdda90>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.819, val_acc:0.102]
Epoch [2/120    avg_loss:2.741, val_acc:0.175]
Epoch [3/120    avg_loss:2.660, val_acc:0.334]
Epoch [4/120    avg_loss:2.581, val_acc:0.361]
Epoch [5/120    avg_loss:2.490, val_acc:0.425]
Epoch [6/120    avg_loss:2.429, val_acc:0.451]
Epoch [7/120    avg_loss:2.362, val_acc:0.473]
Epoch [8/120    avg_loss:2.275, val_acc:0.461]
Epoch [9/120    avg_loss:2.224, val_acc:0.510]
Epoch [10/120    avg_loss:2.129, val_acc:0.499]
Epoch [11/120    avg_loss:2.071, val_acc:0.559]
Epoch [12/120    avg_loss:2.005, val_acc:0.549]
Epoch [13/120    avg_loss:1.904, val_acc:0.615]
Epoch [14/120    avg_loss:1.794, val_acc:0.607]
Epoch [15/120    avg_loss:1.721, val_acc:0.654]
Epoch [16/120    avg_loss:1.652, val_acc:0.662]
Epoch [17/120    avg_loss:1.547, val_acc:0.679]
Epoch [18/120    avg_loss:1.411, val_acc:0.663]
Epoch [19/120    avg_loss:1.296, val_acc:0.700]
Epoch [20/120    avg_loss:1.248, val_acc:0.699]
Epoch [21/120    avg_loss:1.183, val_acc:0.697]
Epoch [22/120    avg_loss:1.083, val_acc:0.692]
Epoch [23/120    avg_loss:0.992, val_acc:0.724]
Epoch [24/120    avg_loss:0.926, val_acc:0.751]
Epoch [25/120    avg_loss:0.891, val_acc:0.724]
Epoch [26/120    avg_loss:0.849, val_acc:0.718]
Epoch [27/120    avg_loss:1.012, val_acc:0.726]
Epoch [28/120    avg_loss:0.807, val_acc:0.740]
Epoch [29/120    avg_loss:0.702, val_acc:0.800]
Epoch [30/120    avg_loss:0.612, val_acc:0.821]
Epoch [31/120    avg_loss:0.606, val_acc:0.808]
Epoch [32/120    avg_loss:0.575, val_acc:0.821]
Epoch [33/120    avg_loss:0.476, val_acc:0.821]
Epoch [34/120    avg_loss:0.427, val_acc:0.863]
Epoch [35/120    avg_loss:0.390, val_acc:0.862]
Epoch [36/120    avg_loss:0.358, val_acc:0.861]
Epoch [37/120    avg_loss:0.379, val_acc:0.863]
Epoch [38/120    avg_loss:0.295, val_acc:0.888]
Epoch [39/120    avg_loss:0.283, val_acc:0.903]
Epoch [40/120    avg_loss:0.301, val_acc:0.891]
Epoch [41/120    avg_loss:0.264, val_acc:0.916]
Epoch [42/120    avg_loss:0.250, val_acc:0.887]
Epoch [43/120    avg_loss:0.263, val_acc:0.872]
Epoch [44/120    avg_loss:0.226, val_acc:0.914]
Epoch [45/120    avg_loss:0.193, val_acc:0.913]
Epoch [46/120    avg_loss:0.181, val_acc:0.916]
Epoch [47/120    avg_loss:0.172, val_acc:0.907]
Epoch [48/120    avg_loss:0.171, val_acc:0.927]
Epoch [49/120    avg_loss:0.148, val_acc:0.929]
Epoch [50/120    avg_loss:0.156, val_acc:0.917]
Epoch [51/120    avg_loss:0.157, val_acc:0.927]
Epoch [52/120    avg_loss:0.219, val_acc:0.901]
Epoch [53/120    avg_loss:0.263, val_acc:0.900]
Epoch [54/120    avg_loss:0.232, val_acc:0.902]
Epoch [55/120    avg_loss:0.210, val_acc:0.897]
Epoch [56/120    avg_loss:0.225, val_acc:0.912]
Epoch [57/120    avg_loss:0.167, val_acc:0.905]
Epoch [58/120    avg_loss:0.131, val_acc:0.920]
Epoch [59/120    avg_loss:0.130, val_acc:0.948]
Epoch [60/120    avg_loss:0.123, val_acc:0.934]
Epoch [61/120    avg_loss:0.125, val_acc:0.924]
Epoch [62/120    avg_loss:0.140, val_acc:0.934]
Epoch [63/120    avg_loss:0.141, val_acc:0.920]
Epoch [64/120    avg_loss:0.130, val_acc:0.930]
Epoch [65/120    avg_loss:0.105, val_acc:0.951]
Epoch [66/120    avg_loss:0.083, val_acc:0.951]
Epoch [67/120    avg_loss:0.093, val_acc:0.949]
Epoch [68/120    avg_loss:0.123, val_acc:0.930]
Epoch [69/120    avg_loss:0.101, val_acc:0.952]
Epoch [70/120    avg_loss:0.096, val_acc:0.945]
Epoch [71/120    avg_loss:0.086, val_acc:0.942]
Epoch [72/120    avg_loss:0.090, val_acc:0.952]
Epoch [73/120    avg_loss:0.062, val_acc:0.955]
Epoch [74/120    avg_loss:0.060, val_acc:0.957]
Epoch [75/120    avg_loss:0.066, val_acc:0.958]
Epoch [76/120    avg_loss:0.066, val_acc:0.968]
Epoch [77/120    avg_loss:0.058, val_acc:0.963]
Epoch [78/120    avg_loss:0.054, val_acc:0.967]
Epoch [79/120    avg_loss:0.048, val_acc:0.963]
Epoch [80/120    avg_loss:0.053, val_acc:0.974]
Epoch [81/120    avg_loss:0.114, val_acc:0.930]
Epoch [82/120    avg_loss:0.147, val_acc:0.942]
Epoch [83/120    avg_loss:0.103, val_acc:0.959]
Epoch [84/120    avg_loss:0.075, val_acc:0.955]
Epoch [85/120    avg_loss:0.061, val_acc:0.957]
Epoch [86/120    avg_loss:0.056, val_acc:0.962]
Epoch [87/120    avg_loss:0.054, val_acc:0.979]
Epoch [88/120    avg_loss:0.050, val_acc:0.961]
Epoch [89/120    avg_loss:0.041, val_acc:0.968]
Epoch [90/120    avg_loss:0.049, val_acc:0.942]
Epoch [91/120    avg_loss:0.068, val_acc:0.940]
Epoch [92/120    avg_loss:0.053, val_acc:0.955]
Epoch [93/120    avg_loss:0.050, val_acc:0.961]
Epoch [94/120    avg_loss:0.045, val_acc:0.946]
Epoch [95/120    avg_loss:0.154, val_acc:0.928]
Epoch [96/120    avg_loss:0.284, val_acc:0.887]
Epoch [97/120    avg_loss:0.214, val_acc:0.942]
Epoch [98/120    avg_loss:0.171, val_acc:0.898]
Epoch [99/120    avg_loss:0.149, val_acc:0.908]
Epoch [100/120    avg_loss:0.084, val_acc:0.943]
Epoch [101/120    avg_loss:0.075, val_acc:0.953]
Epoch [102/120    avg_loss:0.056, val_acc:0.955]
Epoch [103/120    avg_loss:0.048, val_acc:0.959]
Epoch [104/120    avg_loss:0.055, val_acc:0.965]
Epoch [105/120    avg_loss:0.047, val_acc:0.964]
Epoch [106/120    avg_loss:0.048, val_acc:0.966]
Epoch [107/120    avg_loss:0.044, val_acc:0.965]
Epoch [108/120    avg_loss:0.046, val_acc:0.966]
Epoch [109/120    avg_loss:0.044, val_acc:0.964]
Epoch [110/120    avg_loss:0.043, val_acc:0.964]
Epoch [111/120    avg_loss:0.039, val_acc:0.964]
Epoch [112/120    avg_loss:0.038, val_acc:0.964]
Epoch [113/120    avg_loss:0.043, val_acc:0.965]
Epoch [114/120    avg_loss:0.038, val_acc:0.964]
Epoch [115/120    avg_loss:0.035, val_acc:0.965]
Epoch [116/120    avg_loss:0.038, val_acc:0.965]
Epoch [117/120    avg_loss:0.038, val_acc:0.965]
Epoch [118/120    avg_loss:0.058, val_acc:0.965]
Epoch [119/120    avg_loss:0.042, val_acc:0.965]
Epoch [120/120    avg_loss:0.040, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1213    6    2    0    1    0    0    0   15   48    0    0
     0    0    0]
 [   0    0    0  680   22   15    1    0    0    7    3    1   16    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  415    0    6    0    3    0    2    0    0
     9    0    0]
 [   0    0    0    0    0    0  647    0    0    2    0    3    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    7    1    0    6    1    0    0    0  835   17    2    0
     2    4    0]
 [   0    0   17    0    0    1    0    0    0    0   29 2160    1    2
     0    0    0]
 [   0    0    0    2    0    4    0    0    0    1    6   15  500    0
     4    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   11    0    0    0    0    1    2    0    0
  1122    3    0]
 [   0    0    0    0    0    0   37    0    0    0    0    0    0    0
    11  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.0650406504065

F1 scores:
[       nan 0.975      0.96193497 0.94510076 0.94666667 0.93573844
 0.96279762 0.89285714 1.         0.65217391 0.94563986 0.96904441
 0.94966762 0.98930481 0.97905759 0.91577335 0.98823529]

Kappa:
0.9551244857559616
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f697bdc2b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.821, val_acc:0.270]
Epoch [2/120    avg_loss:2.722, val_acc:0.300]
Epoch [3/120    avg_loss:2.641, val_acc:0.298]
Epoch [4/120    avg_loss:2.518, val_acc:0.373]
Epoch [5/120    avg_loss:2.465, val_acc:0.430]
Epoch [6/120    avg_loss:2.374, val_acc:0.489]
Epoch [7/120    avg_loss:2.334, val_acc:0.532]
Epoch [8/120    avg_loss:2.233, val_acc:0.528]
Epoch [9/120    avg_loss:2.168, val_acc:0.548]
Epoch [10/120    avg_loss:2.104, val_acc:0.585]
Epoch [11/120    avg_loss:2.056, val_acc:0.590]
Epoch [12/120    avg_loss:1.953, val_acc:0.607]
Epoch [13/120    avg_loss:1.878, val_acc:0.639]
Epoch [14/120    avg_loss:1.804, val_acc:0.653]
Epoch [15/120    avg_loss:1.704, val_acc:0.651]
Epoch [16/120    avg_loss:1.629, val_acc:0.613]
Epoch [17/120    avg_loss:1.530, val_acc:0.679]
Epoch [18/120    avg_loss:1.416, val_acc:0.714]
Epoch [19/120    avg_loss:1.304, val_acc:0.736]
Epoch [20/120    avg_loss:1.227, val_acc:0.735]
Epoch [21/120    avg_loss:1.243, val_acc:0.735]
Epoch [22/120    avg_loss:1.120, val_acc:0.749]
Epoch [23/120    avg_loss:0.989, val_acc:0.785]
Epoch [24/120    avg_loss:0.982, val_acc:0.741]
Epoch [25/120    avg_loss:0.913, val_acc:0.768]
Epoch [26/120    avg_loss:0.852, val_acc:0.793]
Epoch [27/120    avg_loss:0.825, val_acc:0.782]
Epoch [28/120    avg_loss:0.739, val_acc:0.758]
Epoch [29/120    avg_loss:0.726, val_acc:0.835]
Epoch [30/120    avg_loss:0.635, val_acc:0.820]
Epoch [31/120    avg_loss:0.593, val_acc:0.820]
Epoch [32/120    avg_loss:0.559, val_acc:0.864]
Epoch [33/120    avg_loss:0.571, val_acc:0.829]
Epoch [34/120    avg_loss:0.495, val_acc:0.862]
Epoch [35/120    avg_loss:0.454, val_acc:0.849]
Epoch [36/120    avg_loss:0.438, val_acc:0.872]
Epoch [37/120    avg_loss:0.389, val_acc:0.886]
Epoch [38/120    avg_loss:0.339, val_acc:0.871]
Epoch [39/120    avg_loss:0.376, val_acc:0.880]
Epoch [40/120    avg_loss:0.356, val_acc:0.880]
Epoch [41/120    avg_loss:0.306, val_acc:0.884]
Epoch [42/120    avg_loss:0.248, val_acc:0.900]
Epoch [43/120    avg_loss:0.231, val_acc:0.908]
Epoch [44/120    avg_loss:0.223, val_acc:0.918]
Epoch [45/120    avg_loss:0.236, val_acc:0.895]
Epoch [46/120    avg_loss:0.230, val_acc:0.915]
Epoch [47/120    avg_loss:0.188, val_acc:0.910]
Epoch [48/120    avg_loss:0.178, val_acc:0.932]
Epoch [49/120    avg_loss:0.185, val_acc:0.893]
Epoch [50/120    avg_loss:0.225, val_acc:0.911]
Epoch [51/120    avg_loss:0.176, val_acc:0.929]
Epoch [52/120    avg_loss:0.187, val_acc:0.918]
Epoch [53/120    avg_loss:0.148, val_acc:0.904]
Epoch [54/120    avg_loss:0.152, val_acc:0.916]
Epoch [55/120    avg_loss:0.155, val_acc:0.929]
Epoch [56/120    avg_loss:0.124, val_acc:0.930]
Epoch [57/120    avg_loss:0.116, val_acc:0.933]
Epoch [58/120    avg_loss:0.115, val_acc:0.936]
Epoch [59/120    avg_loss:0.122, val_acc:0.916]
Epoch [60/120    avg_loss:0.113, val_acc:0.946]
Epoch [61/120    avg_loss:0.122, val_acc:0.940]
Epoch [62/120    avg_loss:0.155, val_acc:0.938]
Epoch [63/120    avg_loss:0.157, val_acc:0.932]
Epoch [64/120    avg_loss:0.107, val_acc:0.935]
Epoch [65/120    avg_loss:0.139, val_acc:0.937]
Epoch [66/120    avg_loss:0.162, val_acc:0.936]
Epoch [67/120    avg_loss:0.124, val_acc:0.928]
Epoch [68/120    avg_loss:0.097, val_acc:0.939]
Epoch [69/120    avg_loss:0.079, val_acc:0.951]
Epoch [70/120    avg_loss:0.072, val_acc:0.947]
Epoch [71/120    avg_loss:0.082, val_acc:0.940]
Epoch [72/120    avg_loss:0.064, val_acc:0.954]
Epoch [73/120    avg_loss:0.055, val_acc:0.965]
Epoch [74/120    avg_loss:0.074, val_acc:0.947]
Epoch [75/120    avg_loss:0.058, val_acc:0.957]
Epoch [76/120    avg_loss:0.048, val_acc:0.955]
Epoch [77/120    avg_loss:0.073, val_acc:0.954]
Epoch [78/120    avg_loss:0.058, val_acc:0.959]
Epoch [79/120    avg_loss:0.065, val_acc:0.946]
Epoch [80/120    avg_loss:0.069, val_acc:0.942]
Epoch [81/120    avg_loss:0.063, val_acc:0.949]
Epoch [82/120    avg_loss:0.061, val_acc:0.954]
Epoch [83/120    avg_loss:0.059, val_acc:0.951]
Epoch [84/120    avg_loss:0.070, val_acc:0.954]
Epoch [85/120    avg_loss:0.053, val_acc:0.961]
Epoch [86/120    avg_loss:0.042, val_acc:0.947]
Epoch [87/120    avg_loss:0.050, val_acc:0.954]
Epoch [88/120    avg_loss:0.026, val_acc:0.961]
Epoch [89/120    avg_loss:0.026, val_acc:0.963]
Epoch [90/120    avg_loss:0.027, val_acc:0.964]
Epoch [91/120    avg_loss:0.025, val_acc:0.967]
Epoch [92/120    avg_loss:0.024, val_acc:0.967]
Epoch [93/120    avg_loss:0.020, val_acc:0.968]
Epoch [94/120    avg_loss:0.025, val_acc:0.971]
Epoch [95/120    avg_loss:0.027, val_acc:0.971]
Epoch [96/120    avg_loss:0.027, val_acc:0.970]
Epoch [97/120    avg_loss:0.024, val_acc:0.971]
Epoch [98/120    avg_loss:0.020, val_acc:0.971]
Epoch [99/120    avg_loss:0.020, val_acc:0.968]
Epoch [100/120    avg_loss:0.025, val_acc:0.968]
Epoch [101/120    avg_loss:0.023, val_acc:0.970]
Epoch [102/120    avg_loss:0.021, val_acc:0.970]
Epoch [103/120    avg_loss:0.024, val_acc:0.972]
Epoch [104/120    avg_loss:0.023, val_acc:0.971]
Epoch [105/120    avg_loss:0.028, val_acc:0.968]
Epoch [106/120    avg_loss:0.021, val_acc:0.972]
Epoch [107/120    avg_loss:0.023, val_acc:0.972]
Epoch [108/120    avg_loss:0.020, val_acc:0.970]
Epoch [109/120    avg_loss:0.024, val_acc:0.970]
Epoch [110/120    avg_loss:0.023, val_acc:0.970]
Epoch [111/120    avg_loss:0.019, val_acc:0.970]
Epoch [112/120    avg_loss:0.018, val_acc:0.974]
Epoch [113/120    avg_loss:0.019, val_acc:0.972]
Epoch [114/120    avg_loss:0.018, val_acc:0.973]
Epoch [115/120    avg_loss:0.020, val_acc:0.968]
Epoch [116/120    avg_loss:0.023, val_acc:0.971]
Epoch [117/120    avg_loss:0.020, val_acc:0.972]
Epoch [118/120    avg_loss:0.020, val_acc:0.973]
Epoch [119/120    avg_loss:0.021, val_acc:0.972]
Epoch [120/120    avg_loss:0.017, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1253    2    5    0    5    0    0    0    6   12    2    0
     0    0    0]
 [   0    0    2  698    3    4    0    0    0   10    6    1   23    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   24    0    0    8    2    0    0    0  826    8    3    0
     1    3    0]
 [   0    0   11    0    0    0    3    0    0    0    7 2171   18    0
     0    0    0]
 [   0    0    0    0    0    8    0    0    0    1    5    1  513    0
     2    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   17    0    0    0    0    1    1    0    0
  1103   17    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    18  313    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.04065040650407

F1 scores:
[       nan 1.         0.97320388 0.96475466 0.98156682 0.95343681
 0.97601199 0.98039216 1.         0.70833333 0.9571263  0.98569807
 0.9369863  1.         0.97180617 0.92058824 0.97076023]

Kappa:
0.9662847251065658
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe1c0ff9a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.803, val_acc:0.148]
Epoch [2/120    avg_loss:2.730, val_acc:0.259]
Epoch [3/120    avg_loss:2.625, val_acc:0.365]
Epoch [4/120    avg_loss:2.514, val_acc:0.422]
Epoch [5/120    avg_loss:2.454, val_acc:0.445]
Epoch [6/120    avg_loss:2.389, val_acc:0.480]
Epoch [7/120    avg_loss:2.312, val_acc:0.518]
Epoch [8/120    avg_loss:2.255, val_acc:0.532]
Epoch [9/120    avg_loss:2.182, val_acc:0.560]
Epoch [10/120    avg_loss:2.112, val_acc:0.563]
Epoch [11/120    avg_loss:2.057, val_acc:0.561]
Epoch [12/120    avg_loss:1.987, val_acc:0.575]
Epoch [13/120    avg_loss:1.916, val_acc:0.586]
Epoch [14/120    avg_loss:1.864, val_acc:0.602]
Epoch [15/120    avg_loss:1.762, val_acc:0.639]
Epoch [16/120    avg_loss:1.658, val_acc:0.646]
Epoch [17/120    avg_loss:1.614, val_acc:0.665]
Epoch [18/120    avg_loss:1.452, val_acc:0.683]
Epoch [19/120    avg_loss:1.362, val_acc:0.665]
Epoch [20/120    avg_loss:1.299, val_acc:0.712]
Epoch [21/120    avg_loss:1.205, val_acc:0.700]
Epoch [22/120    avg_loss:1.126, val_acc:0.712]
Epoch [23/120    avg_loss:1.080, val_acc:0.737]
Epoch [24/120    avg_loss:0.980, val_acc:0.714]
Epoch [25/120    avg_loss:0.926, val_acc:0.728]
Epoch [26/120    avg_loss:0.850, val_acc:0.754]
Epoch [27/120    avg_loss:0.868, val_acc:0.772]
Epoch [28/120    avg_loss:0.764, val_acc:0.755]
Epoch [29/120    avg_loss:0.698, val_acc:0.771]
Epoch [30/120    avg_loss:0.594, val_acc:0.814]
Epoch [31/120    avg_loss:0.576, val_acc:0.810]
Epoch [32/120    avg_loss:0.553, val_acc:0.839]
Epoch [33/120    avg_loss:0.533, val_acc:0.836]
Epoch [34/120    avg_loss:0.467, val_acc:0.830]
Epoch [35/120    avg_loss:0.447, val_acc:0.834]
Epoch [36/120    avg_loss:0.485, val_acc:0.833]
Epoch [37/120    avg_loss:0.420, val_acc:0.850]
Epoch [38/120    avg_loss:0.418, val_acc:0.835]
Epoch [39/120    avg_loss:0.459, val_acc:0.823]
Epoch [40/120    avg_loss:0.586, val_acc:0.836]
Epoch [41/120    avg_loss:0.420, val_acc:0.843]
Epoch [42/120    avg_loss:0.406, val_acc:0.849]
Epoch [43/120    avg_loss:0.406, val_acc:0.863]
Epoch [44/120    avg_loss:0.318, val_acc:0.888]
Epoch [45/120    avg_loss:0.296, val_acc:0.887]
Epoch [46/120    avg_loss:0.249, val_acc:0.882]
Epoch [47/120    avg_loss:0.243, val_acc:0.879]
Epoch [48/120    avg_loss:0.218, val_acc:0.914]
Epoch [49/120    avg_loss:0.198, val_acc:0.914]
Epoch [50/120    avg_loss:0.181, val_acc:0.921]
Epoch [51/120    avg_loss:0.194, val_acc:0.907]
Epoch [52/120    avg_loss:0.189, val_acc:0.910]
Epoch [53/120    avg_loss:0.190, val_acc:0.924]
Epoch [54/120    avg_loss:0.185, val_acc:0.929]
Epoch [55/120    avg_loss:0.187, val_acc:0.918]
Epoch [56/120    avg_loss:0.165, val_acc:0.936]
Epoch [57/120    avg_loss:0.183, val_acc:0.922]
Epoch [58/120    avg_loss:0.155, val_acc:0.929]
Epoch [59/120    avg_loss:0.319, val_acc:0.902]
Epoch [60/120    avg_loss:0.232, val_acc:0.909]
Epoch [61/120    avg_loss:0.201, val_acc:0.913]
Epoch [62/120    avg_loss:0.149, val_acc:0.933]
Epoch [63/120    avg_loss:0.117, val_acc:0.943]
Epoch [64/120    avg_loss:0.135, val_acc:0.930]
Epoch [65/120    avg_loss:0.131, val_acc:0.947]
Epoch [66/120    avg_loss:0.133, val_acc:0.948]
Epoch [67/120    avg_loss:0.130, val_acc:0.937]
Epoch [68/120    avg_loss:0.119, val_acc:0.947]
Epoch [69/120    avg_loss:0.115, val_acc:0.948]
Epoch [70/120    avg_loss:0.091, val_acc:0.955]
Epoch [71/120    avg_loss:0.089, val_acc:0.961]
Epoch [72/120    avg_loss:0.076, val_acc:0.959]
Epoch [73/120    avg_loss:0.087, val_acc:0.957]
Epoch [74/120    avg_loss:0.082, val_acc:0.963]
Epoch [75/120    avg_loss:0.099, val_acc:0.954]
Epoch [76/120    avg_loss:0.074, val_acc:0.967]
Epoch [77/120    avg_loss:0.061, val_acc:0.962]
Epoch [78/120    avg_loss:0.066, val_acc:0.965]
Epoch [79/120    avg_loss:0.080, val_acc:0.941]
Epoch [80/120    avg_loss:0.096, val_acc:0.958]
Epoch [81/120    avg_loss:0.073, val_acc:0.947]
Epoch [82/120    avg_loss:0.079, val_acc:0.953]
Epoch [83/120    avg_loss:0.071, val_acc:0.959]
Epoch [84/120    avg_loss:0.075, val_acc:0.966]
Epoch [85/120    avg_loss:0.070, val_acc:0.949]
Epoch [86/120    avg_loss:0.088, val_acc:0.960]
Epoch [87/120    avg_loss:0.059, val_acc:0.967]
Epoch [88/120    avg_loss:0.070, val_acc:0.958]
Epoch [89/120    avg_loss:0.055, val_acc:0.953]
Epoch [90/120    avg_loss:0.109, val_acc:0.939]
Epoch [91/120    avg_loss:0.096, val_acc:0.938]
Epoch [92/120    avg_loss:0.089, val_acc:0.960]
Epoch [93/120    avg_loss:0.073, val_acc:0.960]
Epoch [94/120    avg_loss:0.074, val_acc:0.963]
Epoch [95/120    avg_loss:0.057, val_acc:0.964]
Epoch [96/120    avg_loss:0.076, val_acc:0.961]
Epoch [97/120    avg_loss:0.051, val_acc:0.975]
Epoch [98/120    avg_loss:0.050, val_acc:0.965]
Epoch [99/120    avg_loss:0.046, val_acc:0.955]
Epoch [100/120    avg_loss:0.049, val_acc:0.968]
Epoch [101/120    avg_loss:0.044, val_acc:0.964]
Epoch [102/120    avg_loss:0.048, val_acc:0.970]
Epoch [103/120    avg_loss:0.042, val_acc:0.976]
Epoch [104/120    avg_loss:0.038, val_acc:0.974]
Epoch [105/120    avg_loss:0.043, val_acc:0.966]
Epoch [106/120    avg_loss:0.041, val_acc:0.973]
Epoch [107/120    avg_loss:0.036, val_acc:0.973]
Epoch [108/120    avg_loss:0.028, val_acc:0.972]
Epoch [109/120    avg_loss:0.043, val_acc:0.970]
Epoch [110/120    avg_loss:0.027, val_acc:0.973]
Epoch [111/120    avg_loss:0.026, val_acc:0.974]
Epoch [112/120    avg_loss:0.021, val_acc:0.973]
Epoch [113/120    avg_loss:0.023, val_acc:0.979]
Epoch [114/120    avg_loss:0.030, val_acc:0.973]
Epoch [115/120    avg_loss:0.021, val_acc:0.980]
Epoch [116/120    avg_loss:0.024, val_acc:0.979]
Epoch [117/120    avg_loss:0.028, val_acc:0.975]
Epoch [118/120    avg_loss:0.029, val_acc:0.978]
Epoch [119/120    avg_loss:0.023, val_acc:0.980]
Epoch [120/120    avg_loss:0.033, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1255    0    1    0    3    0    0    0    1   25    0    0
     0    0    0]
 [   0    0    3  687    8   13    3    0    0    2   10    4   17    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  421    0    7    0    3    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    9    0    0    9    0    0    0    0
     0    0    0]
 [   0    0   18    0    0    3    2    0    0    0  846    5    1    0
     0    0    0]
 [   0    0   12    0    0    0    4    0    0    0   15 2170    9    0
     0    0    0]
 [   0    0    9    0    2   13    0    0    0    0    0    1  507    0
     0    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    2    0    1    0    2    0    0    0
  1132    2    0]
 [   0    0    0    0    0    0   48    0    0    0    0    0    2    0
    11  286    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.88888888888889

F1 scores:
[       nan 0.93506494 0.97211464 0.958159   0.97482838 0.9503386
 0.9472162  0.87719298 0.99650757 0.5625     0.96465222 0.98234495
 0.94589552 0.99728997 0.99080963 0.9007874  0.98823529]

Kappa:
0.9645241385328053
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f86e28f1a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 42568==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.802, val_acc:0.133]
Epoch [2/120    avg_loss:2.714, val_acc:0.241]
Epoch [3/120    avg_loss:2.615, val_acc:0.261]
Epoch [4/120    avg_loss:2.525, val_acc:0.339]
Epoch [5/120    avg_loss:2.444, val_acc:0.416]
Epoch [6/120    avg_loss:2.369, val_acc:0.438]
Epoch [7/120    avg_loss:2.276, val_acc:0.452]
Epoch [8/120    avg_loss:2.177, val_acc:0.493]
Epoch [9/120    avg_loss:2.144, val_acc:0.568]
Epoch [10/120    avg_loss:2.069, val_acc:0.566]
Epoch [11/120    avg_loss:2.007, val_acc:0.605]
Epoch [12/120    avg_loss:1.936, val_acc:0.599]
Epoch [13/120    avg_loss:1.811, val_acc:0.610]
Epoch [14/120    avg_loss:1.755, val_acc:0.643]
Epoch [15/120    avg_loss:1.701, val_acc:0.632]
Epoch [16/120    avg_loss:1.644, val_acc:0.625]
Epoch [17/120    avg_loss:1.521, val_acc:0.657]
Epoch [18/120    avg_loss:1.454, val_acc:0.672]
Epoch [19/120    avg_loss:1.347, val_acc:0.700]
Epoch [20/120    avg_loss:1.313, val_acc:0.709]
Epoch [21/120    avg_loss:1.224, val_acc:0.734]
Epoch [22/120    avg_loss:1.163, val_acc:0.752]
Epoch [23/120    avg_loss:1.171, val_acc:0.724]
Epoch [24/120    avg_loss:1.033, val_acc:0.745]
Epoch [25/120    avg_loss:0.959, val_acc:0.772]
Epoch [26/120    avg_loss:0.863, val_acc:0.783]
Epoch [27/120    avg_loss:0.780, val_acc:0.811]
Epoch [28/120    avg_loss:0.743, val_acc:0.851]
Epoch [29/120    avg_loss:0.728, val_acc:0.798]
Epoch [30/120    avg_loss:0.710, val_acc:0.805]
Epoch [31/120    avg_loss:0.648, val_acc:0.827]
Epoch [32/120    avg_loss:0.556, val_acc:0.863]
Epoch [33/120    avg_loss:0.486, val_acc:0.871]
Epoch [34/120    avg_loss:0.455, val_acc:0.890]
Epoch [35/120    avg_loss:0.392, val_acc:0.866]
Epoch [36/120    avg_loss:0.398, val_acc:0.872]
Epoch [37/120    avg_loss:0.355, val_acc:0.892]
Epoch [38/120    avg_loss:0.298, val_acc:0.910]
Epoch [39/120    avg_loss:0.287, val_acc:0.872]
Epoch [40/120    avg_loss:0.334, val_acc:0.893]
Epoch [41/120    avg_loss:0.308, val_acc:0.882]
Epoch [42/120    avg_loss:0.296, val_acc:0.899]
Epoch [43/120    avg_loss:0.304, val_acc:0.872]
Epoch [44/120    avg_loss:0.291, val_acc:0.884]
Epoch [45/120    avg_loss:0.212, val_acc:0.934]
Epoch [46/120    avg_loss:0.179, val_acc:0.940]
Epoch [47/120    avg_loss:0.147, val_acc:0.934]
Epoch [48/120    avg_loss:0.157, val_acc:0.905]
Epoch [49/120    avg_loss:0.169, val_acc:0.905]
Epoch [50/120    avg_loss:0.179, val_acc:0.923]
Epoch [51/120    avg_loss:0.154, val_acc:0.928]
Epoch [52/120    avg_loss:0.125, val_acc:0.940]
Epoch [53/120    avg_loss:0.118, val_acc:0.940]
Epoch [54/120    avg_loss:0.162, val_acc:0.932]
Epoch [55/120    avg_loss:0.136, val_acc:0.948]
Epoch [56/120    avg_loss:0.129, val_acc:0.935]
Epoch [57/120    avg_loss:0.110, val_acc:0.937]
Epoch [58/120    avg_loss:0.106, val_acc:0.948]
Epoch [59/120    avg_loss:0.102, val_acc:0.929]
Epoch [60/120    avg_loss:0.084, val_acc:0.949]
Epoch [61/120    avg_loss:0.083, val_acc:0.955]
Epoch [62/120    avg_loss:0.088, val_acc:0.951]
Epoch [63/120    avg_loss:0.115, val_acc:0.924]
Epoch [64/120    avg_loss:0.125, val_acc:0.937]
Epoch [65/120    avg_loss:0.088, val_acc:0.954]
Epoch [66/120    avg_loss:0.069, val_acc:0.953]
Epoch [67/120    avg_loss:0.078, val_acc:0.959]
Epoch [68/120    avg_loss:0.075, val_acc:0.953]
Epoch [69/120    avg_loss:0.067, val_acc:0.963]
Epoch [70/120    avg_loss:0.064, val_acc:0.964]
Epoch [71/120    avg_loss:0.059, val_acc:0.964]
Epoch [72/120    avg_loss:0.077, val_acc:0.946]
Epoch [73/120    avg_loss:0.095, val_acc:0.950]
Epoch [74/120    avg_loss:0.075, val_acc:0.942]
Epoch [75/120    avg_loss:0.060, val_acc:0.955]
Epoch [76/120    avg_loss:0.054, val_acc:0.970]
Epoch [77/120    avg_loss:0.055, val_acc:0.949]
Epoch [78/120    avg_loss:0.055, val_acc:0.957]
Epoch [79/120    avg_loss:0.042, val_acc:0.963]
Epoch [80/120    avg_loss:0.053, val_acc:0.967]
Epoch [81/120    avg_loss:0.058, val_acc:0.958]
Epoch [82/120    avg_loss:0.055, val_acc:0.957]
Epoch [83/120    avg_loss:0.046, val_acc:0.967]
Epoch [84/120    avg_loss:0.038, val_acc:0.962]
Epoch [85/120    avg_loss:0.031, val_acc:0.968]
Epoch [86/120    avg_loss:0.032, val_acc:0.965]
Epoch [87/120    avg_loss:0.030, val_acc:0.968]
Epoch [88/120    avg_loss:0.041, val_acc:0.966]
Epoch [89/120    avg_loss:0.039, val_acc:0.964]
Epoch [90/120    avg_loss:0.031, val_acc:0.974]
Epoch [91/120    avg_loss:0.030, val_acc:0.973]
Epoch [92/120    avg_loss:0.031, val_acc:0.975]
Epoch [93/120    avg_loss:0.033, val_acc:0.975]
Epoch [94/120    avg_loss:0.026, val_acc:0.977]
Epoch [95/120    avg_loss:0.021, val_acc:0.978]
Epoch [96/120    avg_loss:0.020, val_acc:0.979]
Epoch [97/120    avg_loss:0.021, val_acc:0.977]
Epoch [98/120    avg_loss:0.024, val_acc:0.977]
Epoch [99/120    avg_loss:0.022, val_acc:0.976]
Epoch [100/120    avg_loss:0.023, val_acc:0.979]
Epoch [101/120    avg_loss:0.020, val_acc:0.979]
Epoch [102/120    avg_loss:0.024, val_acc:0.978]
Epoch [103/120    avg_loss:0.026, val_acc:0.979]
Epoch [104/120    avg_loss:0.022, val_acc:0.975]
Epoch [105/120    avg_loss:0.024, val_acc:0.977]
Epoch [106/120    avg_loss:0.021, val_acc:0.977]
Epoch [107/120    avg_loss:0.019, val_acc:0.977]
Epoch [108/120    avg_loss:0.025, val_acc:0.978]
Epoch [109/120    avg_loss:0.019, val_acc:0.976]
Epoch [110/120    avg_loss:0.024, val_acc:0.975]
Epoch [111/120    avg_loss:0.026, val_acc:0.977]
Epoch [112/120    avg_loss:0.024, val_acc:0.973]
Epoch [113/120    avg_loss:0.020, val_acc:0.973]
Epoch [114/120    avg_loss:0.019, val_acc:0.973]
Epoch [115/120    avg_loss:0.019, val_acc:0.976]
Epoch [116/120    avg_loss:0.021, val_acc:0.975]
Epoch [117/120    avg_loss:0.019, val_acc:0.975]
Epoch [118/120    avg_loss:0.021, val_acc:0.974]
Epoch [119/120    avg_loss:0.023, val_acc:0.974]
Epoch [120/120    avg_loss:0.018, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1249    0    2    0    2    0    0    0    3   29    0    0
     0    0    0]
 [   0    0    1  717    1    3    1    0    0   12    4    0    8    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    3    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   13    0    0    7    2    0    0    0  842    5    4    0
     0    2    0]
 [   0    0    4    0    0    2    1    0    0    0   12 2185    5    1
     0    0    0]
 [   0    0    0    0    0   12    0    0    0    3    4    0  512    0
     1    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0   13    0    0    0    0    1    0    0    0
  1123    2    0]
 [   0    0    0    0    0    0   35    0    0    0    0    0    0    0
    12  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.47425474254743

F1 scores:
[       nan 0.93506494 0.97884013 0.97883959 0.99065421 0.94900222
 0.96821877 0.97959184 0.9953271  0.66666667 0.96449026 0.98667871
 0.95700935 0.99459459 0.98465585 0.92165899 0.97005988]

Kappa:
0.9712044345351674
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feeaeb7ba58>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.780, val_acc:0.085]
Epoch [2/120    avg_loss:2.702, val_acc:0.263]
Epoch [3/120    avg_loss:2.604, val_acc:0.412]
Epoch [4/120    avg_loss:2.512, val_acc:0.438]
Epoch [5/120    avg_loss:2.468, val_acc:0.451]
Epoch [6/120    avg_loss:2.411, val_acc:0.478]
Epoch [7/120    avg_loss:2.337, val_acc:0.508]
Epoch [8/120    avg_loss:2.291, val_acc:0.492]
Epoch [9/120    avg_loss:2.232, val_acc:0.508]
Epoch [10/120    avg_loss:2.154, val_acc:0.588]
Epoch [11/120    avg_loss:2.115, val_acc:0.515]
Epoch [12/120    avg_loss:2.040, val_acc:0.580]
Epoch [13/120    avg_loss:1.985, val_acc:0.580]
Epoch [14/120    avg_loss:1.952, val_acc:0.561]
Epoch [15/120    avg_loss:1.944, val_acc:0.589]
Epoch [16/120    avg_loss:1.858, val_acc:0.580]
Epoch [17/120    avg_loss:1.758, val_acc:0.590]
Epoch [18/120    avg_loss:1.725, val_acc:0.602]
Epoch [19/120    avg_loss:1.654, val_acc:0.606]
Epoch [20/120    avg_loss:1.577, val_acc:0.618]
Epoch [21/120    avg_loss:1.482, val_acc:0.635]
Epoch [22/120    avg_loss:1.438, val_acc:0.647]
Epoch [23/120    avg_loss:1.353, val_acc:0.666]
Epoch [24/120    avg_loss:1.302, val_acc:0.693]
Epoch [25/120    avg_loss:1.224, val_acc:0.685]
Epoch [26/120    avg_loss:1.120, val_acc:0.698]
Epoch [27/120    avg_loss:1.060, val_acc:0.670]
Epoch [28/120    avg_loss:1.033, val_acc:0.719]
Epoch [29/120    avg_loss:0.909, val_acc:0.686]
Epoch [30/120    avg_loss:0.870, val_acc:0.727]
Epoch [31/120    avg_loss:0.801, val_acc:0.750]
Epoch [32/120    avg_loss:0.759, val_acc:0.751]
Epoch [33/120    avg_loss:0.667, val_acc:0.766]
Epoch [34/120    avg_loss:0.627, val_acc:0.740]
Epoch [35/120    avg_loss:0.578, val_acc:0.775]
Epoch [36/120    avg_loss:0.550, val_acc:0.810]
Epoch [37/120    avg_loss:0.526, val_acc:0.811]
Epoch [38/120    avg_loss:0.475, val_acc:0.833]
Epoch [39/120    avg_loss:0.461, val_acc:0.830]
Epoch [40/120    avg_loss:0.399, val_acc:0.850]
Epoch [41/120    avg_loss:0.377, val_acc:0.839]
Epoch [42/120    avg_loss:0.354, val_acc:0.853]
Epoch [43/120    avg_loss:0.341, val_acc:0.873]
Epoch [44/120    avg_loss:0.352, val_acc:0.830]
Epoch [45/120    avg_loss:0.354, val_acc:0.833]
Epoch [46/120    avg_loss:0.321, val_acc:0.878]
Epoch [47/120    avg_loss:0.302, val_acc:0.858]
Epoch [48/120    avg_loss:0.269, val_acc:0.869]
Epoch [49/120    avg_loss:0.260, val_acc:0.887]
Epoch [50/120    avg_loss:0.217, val_acc:0.899]
Epoch [51/120    avg_loss:0.214, val_acc:0.887]
Epoch [52/120    avg_loss:0.201, val_acc:0.909]
Epoch [53/120    avg_loss:0.194, val_acc:0.898]
Epoch [54/120    avg_loss:0.177, val_acc:0.926]
Epoch [55/120    avg_loss:0.159, val_acc:0.923]
Epoch [56/120    avg_loss:0.163, val_acc:0.907]
Epoch [57/120    avg_loss:0.135, val_acc:0.923]
Epoch [58/120    avg_loss:0.160, val_acc:0.926]
Epoch [59/120    avg_loss:0.149, val_acc:0.908]
Epoch [60/120    avg_loss:0.166, val_acc:0.935]
Epoch [61/120    avg_loss:0.132, val_acc:0.924]
Epoch [62/120    avg_loss:0.118, val_acc:0.919]
Epoch [63/120    avg_loss:0.151, val_acc:0.911]
Epoch [64/120    avg_loss:0.155, val_acc:0.895]
Epoch [65/120    avg_loss:0.151, val_acc:0.915]
Epoch [66/120    avg_loss:0.146, val_acc:0.932]
Epoch [67/120    avg_loss:0.128, val_acc:0.917]
Epoch [68/120    avg_loss:0.101, val_acc:0.932]
Epoch [69/120    avg_loss:0.095, val_acc:0.936]
Epoch [70/120    avg_loss:0.092, val_acc:0.938]
Epoch [71/120    avg_loss:0.086, val_acc:0.942]
Epoch [72/120    avg_loss:0.081, val_acc:0.934]
Epoch [73/120    avg_loss:0.124, val_acc:0.914]
Epoch [74/120    avg_loss:0.116, val_acc:0.918]
Epoch [75/120    avg_loss:0.111, val_acc:0.935]
Epoch [76/120    avg_loss:0.095, val_acc:0.928]
Epoch [77/120    avg_loss:0.079, val_acc:0.925]
Epoch [78/120    avg_loss:0.075, val_acc:0.942]
Epoch [79/120    avg_loss:0.072, val_acc:0.944]
Epoch [80/120    avg_loss:0.074, val_acc:0.952]
Epoch [81/120    avg_loss:0.067, val_acc:0.949]
Epoch [82/120    avg_loss:0.067, val_acc:0.944]
Epoch [83/120    avg_loss:0.068, val_acc:0.941]
Epoch [84/120    avg_loss:0.057, val_acc:0.942]
Epoch [85/120    avg_loss:0.074, val_acc:0.939]
Epoch [86/120    avg_loss:0.063, val_acc:0.934]
Epoch [87/120    avg_loss:0.077, val_acc:0.936]
Epoch [88/120    avg_loss:0.064, val_acc:0.938]
Epoch [89/120    avg_loss:0.059, val_acc:0.956]
Epoch [90/120    avg_loss:0.064, val_acc:0.955]
Epoch [91/120    avg_loss:0.046, val_acc:0.951]
Epoch [92/120    avg_loss:0.049, val_acc:0.956]
Epoch [93/120    avg_loss:0.039, val_acc:0.952]
Epoch [94/120    avg_loss:0.043, val_acc:0.955]
Epoch [95/120    avg_loss:0.048, val_acc:0.958]
Epoch [96/120    avg_loss:0.045, val_acc:0.952]
Epoch [97/120    avg_loss:0.038, val_acc:0.963]
Epoch [98/120    avg_loss:0.043, val_acc:0.950]
Epoch [99/120    avg_loss:0.039, val_acc:0.960]
Epoch [100/120    avg_loss:0.036, val_acc:0.960]
Epoch [101/120    avg_loss:0.044, val_acc:0.963]
Epoch [102/120    avg_loss:0.033, val_acc:0.947]
Epoch [103/120    avg_loss:0.063, val_acc:0.952]
Epoch [104/120    avg_loss:0.174, val_acc:0.906]
Epoch [105/120    avg_loss:0.228, val_acc:0.906]
Epoch [106/120    avg_loss:0.259, val_acc:0.899]
Epoch [107/120    avg_loss:0.121, val_acc:0.940]
Epoch [108/120    avg_loss:0.087, val_acc:0.939]
Epoch [109/120    avg_loss:0.102, val_acc:0.909]
Epoch [110/120    avg_loss:0.113, val_acc:0.932]
Epoch [111/120    avg_loss:0.078, val_acc:0.924]
Epoch [112/120    avg_loss:0.077, val_acc:0.938]
Epoch [113/120    avg_loss:0.064, val_acc:0.939]
Epoch [114/120    avg_loss:0.057, val_acc:0.939]
Epoch [115/120    avg_loss:0.055, val_acc:0.949]
Epoch [116/120    avg_loss:0.039, val_acc:0.951]
Epoch [117/120    avg_loss:0.032, val_acc:0.950]
Epoch [118/120    avg_loss:0.035, val_acc:0.950]
Epoch [119/120    avg_loss:0.038, val_acc:0.950]
Epoch [120/120    avg_loss:0.032, val_acc:0.951]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1217    4    3    0    2    0    0    2   11   42    0    0
     0    4    0]
 [   0    0    1  672    1   19    0    0    0    7    2    0   39    6
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  424    0    1    0    6    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    4    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    4    0    0   11    0    0    2    0
     0    0    0]
 [   0    0   54   60    0    5    1    0    0    0  745    2    0    0
     0    8    0]
 [   0    0   26    0    0    0    2    0    0    0   11 2160    4    2
     5    0    0]
 [   0    0    0    5    7   10    0    0    0    0   11   15  480    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    3    0    0    0
  1129    0    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    29  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.1219512195122

F1 scores:
[       nan 1.         0.94231514 0.90261921 0.97247706 0.94222222
 0.98340875 0.98039216 1.         0.5        0.8986731  0.97450936
 0.9039548  0.97883598 0.97876029 0.92675635 0.95348837]

Kappa:
0.9443803000750639
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f808def9b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.794, val_acc:0.098]
Epoch [2/120    avg_loss:2.731, val_acc:0.137]
Epoch [3/120    avg_loss:2.646, val_acc:0.139]
Epoch [4/120    avg_loss:2.586, val_acc:0.157]
Epoch [5/120    avg_loss:2.509, val_acc:0.220]
Epoch [6/120    avg_loss:2.429, val_acc:0.265]
Epoch [7/120    avg_loss:2.383, val_acc:0.298]
Epoch [8/120    avg_loss:2.328, val_acc:0.404]
Epoch [9/120    avg_loss:2.270, val_acc:0.430]
Epoch [10/120    avg_loss:2.221, val_acc:0.422]
Epoch [11/120    avg_loss:2.154, val_acc:0.400]
Epoch [12/120    avg_loss:2.100, val_acc:0.495]
Epoch [13/120    avg_loss:2.040, val_acc:0.509]
Epoch [14/120    avg_loss:2.000, val_acc:0.549]
Epoch [15/120    avg_loss:1.917, val_acc:0.578]
Epoch [16/120    avg_loss:1.889, val_acc:0.560]
Epoch [17/120    avg_loss:1.808, val_acc:0.585]
Epoch [18/120    avg_loss:1.767, val_acc:0.609]
Epoch [19/120    avg_loss:1.660, val_acc:0.614]
Epoch [20/120    avg_loss:1.608, val_acc:0.628]
Epoch [21/120    avg_loss:1.555, val_acc:0.628]
Epoch [22/120    avg_loss:1.486, val_acc:0.658]
Epoch [23/120    avg_loss:1.435, val_acc:0.668]
Epoch [24/120    avg_loss:1.304, val_acc:0.700]
Epoch [25/120    avg_loss:1.220, val_acc:0.708]
Epoch [26/120    avg_loss:1.194, val_acc:0.726]
Epoch [27/120    avg_loss:1.035, val_acc:0.736]
Epoch [28/120    avg_loss:1.001, val_acc:0.752]
Epoch [29/120    avg_loss:1.047, val_acc:0.738]
Epoch [30/120    avg_loss:0.896, val_acc:0.782]
Epoch [31/120    avg_loss:0.877, val_acc:0.740]
Epoch [32/120    avg_loss:0.819, val_acc:0.784]
Epoch [33/120    avg_loss:0.774, val_acc:0.802]
Epoch [34/120    avg_loss:0.694, val_acc:0.729]
Epoch [35/120    avg_loss:0.729, val_acc:0.789]
Epoch [36/120    avg_loss:0.711, val_acc:0.754]
Epoch [37/120    avg_loss:0.660, val_acc:0.809]
Epoch [38/120    avg_loss:0.577, val_acc:0.811]
Epoch [39/120    avg_loss:0.570, val_acc:0.862]
Epoch [40/120    avg_loss:0.501, val_acc:0.878]
Epoch [41/120    avg_loss:0.427, val_acc:0.855]
Epoch [42/120    avg_loss:0.449, val_acc:0.884]
Epoch [43/120    avg_loss:0.408, val_acc:0.897]
Epoch [44/120    avg_loss:0.388, val_acc:0.900]
Epoch [45/120    avg_loss:0.354, val_acc:0.914]
Epoch [46/120    avg_loss:0.304, val_acc:0.917]
Epoch [47/120    avg_loss:0.274, val_acc:0.898]
Epoch [48/120    avg_loss:0.265, val_acc:0.896]
Epoch [49/120    avg_loss:0.253, val_acc:0.909]
Epoch [50/120    avg_loss:0.322, val_acc:0.851]
Epoch [51/120    avg_loss:0.304, val_acc:0.908]
Epoch [52/120    avg_loss:0.250, val_acc:0.917]
Epoch [53/120    avg_loss:0.247, val_acc:0.911]
Epoch [54/120    avg_loss:0.233, val_acc:0.910]
Epoch [55/120    avg_loss:0.193, val_acc:0.937]
Epoch [56/120    avg_loss:0.169, val_acc:0.929]
Epoch [57/120    avg_loss:0.147, val_acc:0.932]
Epoch [58/120    avg_loss:0.136, val_acc:0.938]
Epoch [59/120    avg_loss:0.145, val_acc:0.945]
Epoch [60/120    avg_loss:0.139, val_acc:0.933]
Epoch [61/120    avg_loss:0.157, val_acc:0.930]
Epoch [62/120    avg_loss:0.176, val_acc:0.936]
Epoch [63/120    avg_loss:0.142, val_acc:0.949]
Epoch [64/120    avg_loss:0.117, val_acc:0.948]
Epoch [65/120    avg_loss:0.141, val_acc:0.940]
Epoch [66/120    avg_loss:0.120, val_acc:0.950]
Epoch [67/120    avg_loss:0.102, val_acc:0.949]
Epoch [68/120    avg_loss:0.098, val_acc:0.939]
Epoch [69/120    avg_loss:0.097, val_acc:0.948]
Epoch [70/120    avg_loss:0.118, val_acc:0.949]
Epoch [71/120    avg_loss:0.086, val_acc:0.955]
Epoch [72/120    avg_loss:0.088, val_acc:0.957]
Epoch [73/120    avg_loss:0.082, val_acc:0.952]
Epoch [74/120    avg_loss:0.089, val_acc:0.952]
Epoch [75/120    avg_loss:0.205, val_acc:0.842]
Epoch [76/120    avg_loss:0.279, val_acc:0.902]
Epoch [77/120    avg_loss:0.180, val_acc:0.940]
Epoch [78/120    avg_loss:0.145, val_acc:0.927]
Epoch [79/120    avg_loss:0.097, val_acc:0.941]
Epoch [80/120    avg_loss:0.104, val_acc:0.948]
Epoch [81/120    avg_loss:0.104, val_acc:0.896]
Epoch [82/120    avg_loss:0.146, val_acc:0.941]
Epoch [83/120    avg_loss:0.099, val_acc:0.948]
Epoch [84/120    avg_loss:0.088, val_acc:0.959]
Epoch [85/120    avg_loss:0.095, val_acc:0.947]
Epoch [86/120    avg_loss:0.070, val_acc:0.951]
Epoch [87/120    avg_loss:0.075, val_acc:0.950]
Epoch [88/120    avg_loss:0.080, val_acc:0.958]
Epoch [89/120    avg_loss:0.064, val_acc:0.958]
Epoch [90/120    avg_loss:0.065, val_acc:0.966]
Epoch [91/120    avg_loss:0.066, val_acc:0.958]
Epoch [92/120    avg_loss:0.062, val_acc:0.955]
Epoch [93/120    avg_loss:0.065, val_acc:0.963]
Epoch [94/120    avg_loss:0.058, val_acc:0.954]
Epoch [95/120    avg_loss:0.051, val_acc:0.965]
Epoch [96/120    avg_loss:0.046, val_acc:0.961]
Epoch [97/120    avg_loss:0.057, val_acc:0.957]
Epoch [98/120    avg_loss:0.052, val_acc:0.962]
Epoch [99/120    avg_loss:0.047, val_acc:0.955]
Epoch [100/120    avg_loss:0.051, val_acc:0.964]
Epoch [101/120    avg_loss:0.036, val_acc:0.959]
Epoch [102/120    avg_loss:0.043, val_acc:0.960]
Epoch [103/120    avg_loss:0.041, val_acc:0.961]
Epoch [104/120    avg_loss:0.037, val_acc:0.961]
Epoch [105/120    avg_loss:0.031, val_acc:0.964]
Epoch [106/120    avg_loss:0.036, val_acc:0.965]
Epoch [107/120    avg_loss:0.032, val_acc:0.966]
Epoch [108/120    avg_loss:0.030, val_acc:0.962]
Epoch [109/120    avg_loss:0.034, val_acc:0.965]
Epoch [110/120    avg_loss:0.026, val_acc:0.964]
Epoch [111/120    avg_loss:0.034, val_acc:0.964]
Epoch [112/120    avg_loss:0.027, val_acc:0.964]
Epoch [113/120    avg_loss:0.026, val_acc:0.965]
Epoch [114/120    avg_loss:0.028, val_acc:0.964]
Epoch [115/120    avg_loss:0.030, val_acc:0.963]
Epoch [116/120    avg_loss:0.030, val_acc:0.965]
Epoch [117/120    avg_loss:0.027, val_acc:0.967]
Epoch [118/120    avg_loss:0.028, val_acc:0.965]
Epoch [119/120    avg_loss:0.029, val_acc:0.965]
Epoch [120/120    avg_loss:0.027, val_acc:0.963]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1230    3    0    0    5    0    0    0   11   36    0    0
     0    0    0]
 [   0    0    6  714    1   11    1    0    0   12    0    0    2    0
     0    0    0]
 [   0    0    0    2  208    0    3    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    5    4    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    0    4    0    0    9    0    0    0    0
     0    0    0]
 [   0    0   44   46    0    2    1    0    0    0  760   13    6    0
     0    3    0]
 [   0    0   18    0    0    0   16    0    3    0    5 2157    9    2
     0    0    0]
 [   0    0    3   18    4    5    0    0    0    0   13    6  481    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    1    0    0
  1134    1    0]
 [   0    0    0    0    0    0   37    0    0    0    0    5    0    0
    28  277    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
95.44715447154472

F1 scores:
[       nan 0.94871795 0.9512761  0.93029316 0.97652582 0.96693273
 0.94524496 0.92592593 0.99652375 0.43902439 0.91127098 0.97403477
 0.92678227 0.99462366 0.98565841 0.88216561 0.93975904]

Kappa:
0.9480657476777067
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f30b3898a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.803, val_acc:0.334]
Epoch [2/120    avg_loss:2.735, val_acc:0.377]
Epoch [3/120    avg_loss:2.657, val_acc:0.364]
Epoch [4/120    avg_loss:2.578, val_acc:0.366]
Epoch [5/120    avg_loss:2.495, val_acc:0.382]
Epoch [6/120    avg_loss:2.431, val_acc:0.395]
Epoch [7/120    avg_loss:2.374, val_acc:0.417]
Epoch [8/120    avg_loss:2.313, val_acc:0.443]
Epoch [9/120    avg_loss:2.243, val_acc:0.460]
Epoch [10/120    avg_loss:2.189, val_acc:0.485]
Epoch [11/120    avg_loss:2.128, val_acc:0.498]
Epoch [12/120    avg_loss:2.071, val_acc:0.510]
Epoch [13/120    avg_loss:2.030, val_acc:0.520]
Epoch [14/120    avg_loss:1.949, val_acc:0.523]
Epoch [15/120    avg_loss:1.931, val_acc:0.567]
Epoch [16/120    avg_loss:1.844, val_acc:0.593]
Epoch [17/120    avg_loss:1.799, val_acc:0.603]
Epoch [18/120    avg_loss:1.770, val_acc:0.593]
Epoch [19/120    avg_loss:1.664, val_acc:0.610]
Epoch [20/120    avg_loss:1.577, val_acc:0.608]
Epoch [21/120    avg_loss:1.534, val_acc:0.631]
Epoch [22/120    avg_loss:1.427, val_acc:0.665]
Epoch [23/120    avg_loss:1.358, val_acc:0.650]
Epoch [24/120    avg_loss:1.315, val_acc:0.644]
Epoch [25/120    avg_loss:1.233, val_acc:0.684]
Epoch [26/120    avg_loss:1.151, val_acc:0.691]
Epoch [27/120    avg_loss:1.078, val_acc:0.719]
Epoch [28/120    avg_loss:1.081, val_acc:0.681]
Epoch [29/120    avg_loss:0.996, val_acc:0.697]
Epoch [30/120    avg_loss:0.964, val_acc:0.694]
Epoch [31/120    avg_loss:0.925, val_acc:0.762]
Epoch [32/120    avg_loss:0.857, val_acc:0.765]
Epoch [33/120    avg_loss:0.780, val_acc:0.758]
Epoch [34/120    avg_loss:0.758, val_acc:0.759]
Epoch [35/120    avg_loss:0.743, val_acc:0.780]
Epoch [36/120    avg_loss:0.714, val_acc:0.787]
Epoch [37/120    avg_loss:0.609, val_acc:0.828]
Epoch [38/120    avg_loss:0.564, val_acc:0.847]
Epoch [39/120    avg_loss:0.526, val_acc:0.868]
Epoch [40/120    avg_loss:0.487, val_acc:0.873]
Epoch [41/120    avg_loss:0.498, val_acc:0.857]
Epoch [42/120    avg_loss:0.441, val_acc:0.855]
Epoch [43/120    avg_loss:0.409, val_acc:0.886]
Epoch [44/120    avg_loss:0.401, val_acc:0.893]
Epoch [45/120    avg_loss:0.390, val_acc:0.889]
Epoch [46/120    avg_loss:0.346, val_acc:0.908]
Epoch [47/120    avg_loss:0.304, val_acc:0.898]
Epoch [48/120    avg_loss:0.319, val_acc:0.900]
Epoch [49/120    avg_loss:0.295, val_acc:0.912]
Epoch [50/120    avg_loss:0.261, val_acc:0.923]
Epoch [51/120    avg_loss:0.229, val_acc:0.925]
Epoch [52/120    avg_loss:0.204, val_acc:0.930]
Epoch [53/120    avg_loss:0.189, val_acc:0.935]
Epoch [54/120    avg_loss:0.198, val_acc:0.932]
Epoch [55/120    avg_loss:0.205, val_acc:0.928]
Epoch [56/120    avg_loss:0.170, val_acc:0.943]
Epoch [57/120    avg_loss:0.157, val_acc:0.947]
Epoch [58/120    avg_loss:0.173, val_acc:0.941]
Epoch [59/120    avg_loss:0.220, val_acc:0.919]
Epoch [60/120    avg_loss:0.206, val_acc:0.915]
Epoch [61/120    avg_loss:0.186, val_acc:0.924]
Epoch [62/120    avg_loss:0.156, val_acc:0.933]
Epoch [63/120    avg_loss:0.161, val_acc:0.938]
Epoch [64/120    avg_loss:0.238, val_acc:0.926]
Epoch [65/120    avg_loss:0.173, val_acc:0.942]
Epoch [66/120    avg_loss:0.151, val_acc:0.949]
Epoch [67/120    avg_loss:0.161, val_acc:0.925]
Epoch [68/120    avg_loss:0.174, val_acc:0.932]
Epoch [69/120    avg_loss:0.137, val_acc:0.949]
Epoch [70/120    avg_loss:0.099, val_acc:0.951]
Epoch [71/120    avg_loss:0.106, val_acc:0.948]
Epoch [72/120    avg_loss:0.094, val_acc:0.963]
Epoch [73/120    avg_loss:0.102, val_acc:0.951]
Epoch [74/120    avg_loss:0.096, val_acc:0.951]
Epoch [75/120    avg_loss:0.073, val_acc:0.964]
Epoch [76/120    avg_loss:0.081, val_acc:0.960]
Epoch [77/120    avg_loss:0.072, val_acc:0.950]
Epoch [78/120    avg_loss:0.078, val_acc:0.949]
Epoch [79/120    avg_loss:0.077, val_acc:0.963]
Epoch [80/120    avg_loss:0.059, val_acc:0.959]
Epoch [81/120    avg_loss:0.078, val_acc:0.960]
Epoch [82/120    avg_loss:0.064, val_acc:0.952]
Epoch [83/120    avg_loss:0.078, val_acc:0.963]
Epoch [84/120    avg_loss:0.072, val_acc:0.965]
Epoch [85/120    avg_loss:0.059, val_acc:0.951]
Epoch [86/120    avg_loss:0.062, val_acc:0.963]
Epoch [87/120    avg_loss:0.077, val_acc:0.963]
Epoch [88/120    avg_loss:0.081, val_acc:0.966]
Epoch [89/120    avg_loss:0.064, val_acc:0.959]
Epoch [90/120    avg_loss:0.054, val_acc:0.966]
Epoch [91/120    avg_loss:0.052, val_acc:0.955]
Epoch [92/120    avg_loss:0.064, val_acc:0.957]
Epoch [93/120    avg_loss:0.085, val_acc:0.955]
Epoch [94/120    avg_loss:0.109, val_acc:0.953]
Epoch [95/120    avg_loss:0.085, val_acc:0.945]
Epoch [96/120    avg_loss:0.066, val_acc:0.961]
Epoch [97/120    avg_loss:0.042, val_acc:0.972]
Epoch [98/120    avg_loss:0.040, val_acc:0.966]
Epoch [99/120    avg_loss:0.046, val_acc:0.972]
Epoch [100/120    avg_loss:0.034, val_acc:0.959]
Epoch [101/120    avg_loss:0.043, val_acc:0.965]
Epoch [102/120    avg_loss:0.037, val_acc:0.969]
Epoch [103/120    avg_loss:0.037, val_acc:0.966]
Epoch [104/120    avg_loss:0.041, val_acc:0.973]
Epoch [105/120    avg_loss:0.035, val_acc:0.969]
Epoch [106/120    avg_loss:0.034, val_acc:0.967]
Epoch [107/120    avg_loss:0.029, val_acc:0.973]
Epoch [108/120    avg_loss:0.029, val_acc:0.969]
Epoch [109/120    avg_loss:0.033, val_acc:0.963]
Epoch [110/120    avg_loss:0.027, val_acc:0.972]
Epoch [111/120    avg_loss:0.029, val_acc:0.964]
Epoch [112/120    avg_loss:0.039, val_acc:0.965]
Epoch [113/120    avg_loss:0.090, val_acc:0.952]
Epoch [114/120    avg_loss:0.058, val_acc:0.964]
Epoch [115/120    avg_loss:0.053, val_acc:0.963]
Epoch [116/120    avg_loss:0.040, val_acc:0.967]
Epoch [117/120    avg_loss:0.030, val_acc:0.969]
Epoch [118/120    avg_loss:0.031, val_acc:0.957]
Epoch [119/120    avg_loss:0.034, val_acc:0.966]
Epoch [120/120    avg_loss:0.030, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1248    7    3    0    1    0    0    0    3   16    7    0
     0    0    0]
 [   0    0    0  737    3    1    0    0    0    2    0    1    0    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  398    5   12    0    6    0    1    0    0
    13    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0   56   69    0    7    0    0    0    0  742    1    0    0
     0    0    0]
 [   0    0   29    0    0    0    1    0    0    0   14 2158    8    0
     0    0    0]
 [   0    0    0    5   13    4    0    0    0    0    4    7  497    0
     4    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0    6    0    0    3    0    0    0    0
   133  205    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1   24    0
     0    0   59]]

Accuracy:
94.69918699186992

F1 scores:
[       nan 0.94871795 0.95339954 0.93945188 0.95730337 0.94201183
 0.98867925 0.80645161 0.99650757 0.65116279 0.90212766 0.98157835
 0.92723881 0.9919571  0.93646865 0.74275362 0.82517483]

Kappa:
0.9394906571463325
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8fbbb3eac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.807, val_acc:0.180]
Epoch [2/120    avg_loss:2.730, val_acc:0.215]
Epoch [3/120    avg_loss:2.616, val_acc:0.295]
Epoch [4/120    avg_loss:2.561, val_acc:0.388]
Epoch [5/120    avg_loss:2.489, val_acc:0.450]
Epoch [6/120    avg_loss:2.419, val_acc:0.522]
Epoch [7/120    avg_loss:2.349, val_acc:0.542]
Epoch [8/120    avg_loss:2.280, val_acc:0.527]
Epoch [9/120    avg_loss:2.270, val_acc:0.503]
Epoch [10/120    avg_loss:2.172, val_acc:0.553]
Epoch [11/120    avg_loss:2.145, val_acc:0.509]
Epoch [12/120    avg_loss:2.033, val_acc:0.555]
Epoch [13/120    avg_loss:2.016, val_acc:0.542]
Epoch [14/120    avg_loss:1.971, val_acc:0.545]
Epoch [15/120    avg_loss:1.904, val_acc:0.591]
Epoch [16/120    avg_loss:1.814, val_acc:0.600]
Epoch [17/120    avg_loss:1.748, val_acc:0.597]
Epoch [18/120    avg_loss:1.698, val_acc:0.601]
Epoch [19/120    avg_loss:1.673, val_acc:0.623]
Epoch [20/120    avg_loss:1.602, val_acc:0.622]
Epoch [21/120    avg_loss:1.540, val_acc:0.662]
Epoch [22/120    avg_loss:1.464, val_acc:0.666]
Epoch [23/120    avg_loss:1.369, val_acc:0.700]
Epoch [24/120    avg_loss:1.328, val_acc:0.675]
Epoch [25/120    avg_loss:1.205, val_acc:0.684]
Epoch [26/120    avg_loss:1.179, val_acc:0.687]
Epoch [27/120    avg_loss:1.127, val_acc:0.720]
Epoch [28/120    avg_loss:1.157, val_acc:0.696]
Epoch [29/120    avg_loss:1.033, val_acc:0.651]
Epoch [30/120    avg_loss:0.965, val_acc:0.717]
Epoch [31/120    avg_loss:0.887, val_acc:0.759]
Epoch [32/120    avg_loss:0.852, val_acc:0.746]
Epoch [33/120    avg_loss:0.808, val_acc:0.792]
Epoch [34/120    avg_loss:0.717, val_acc:0.787]
Epoch [35/120    avg_loss:0.675, val_acc:0.812]
Epoch [36/120    avg_loss:0.658, val_acc:0.812]
Epoch [37/120    avg_loss:0.600, val_acc:0.784]
Epoch [38/120    avg_loss:0.605, val_acc:0.827]
Epoch [39/120    avg_loss:0.554, val_acc:0.830]
Epoch [40/120    avg_loss:0.487, val_acc:0.832]
Epoch [41/120    avg_loss:0.491, val_acc:0.854]
Epoch [42/120    avg_loss:0.435, val_acc:0.864]
Epoch [43/120    avg_loss:0.427, val_acc:0.863]
Epoch [44/120    avg_loss:0.398, val_acc:0.880]
Epoch [45/120    avg_loss:0.393, val_acc:0.866]
Epoch [46/120    avg_loss:0.359, val_acc:0.912]
Epoch [47/120    avg_loss:0.319, val_acc:0.893]
Epoch [48/120    avg_loss:0.339, val_acc:0.885]
Epoch [49/120    avg_loss:0.307, val_acc:0.865]
Epoch [50/120    avg_loss:0.321, val_acc:0.888]
Epoch [51/120    avg_loss:0.340, val_acc:0.899]
Epoch [52/120    avg_loss:0.286, val_acc:0.857]
Epoch [53/120    avg_loss:0.256, val_acc:0.911]
Epoch [54/120    avg_loss:0.221, val_acc:0.916]
Epoch [55/120    avg_loss:0.216, val_acc:0.915]
Epoch [56/120    avg_loss:0.208, val_acc:0.914]
Epoch [57/120    avg_loss:0.206, val_acc:0.905]
Epoch [58/120    avg_loss:0.235, val_acc:0.910]
Epoch [59/120    avg_loss:0.182, val_acc:0.922]
Epoch [60/120    avg_loss:0.169, val_acc:0.922]
Epoch [61/120    avg_loss:0.181, val_acc:0.912]
Epoch [62/120    avg_loss:0.166, val_acc:0.933]
Epoch [63/120    avg_loss:0.152, val_acc:0.925]
Epoch [64/120    avg_loss:0.152, val_acc:0.928]
Epoch [65/120    avg_loss:0.151, val_acc:0.916]
Epoch [66/120    avg_loss:0.161, val_acc:0.924]
Epoch [67/120    avg_loss:0.207, val_acc:0.916]
Epoch [68/120    avg_loss:0.144, val_acc:0.929]
Epoch [69/120    avg_loss:0.129, val_acc:0.941]
Epoch [70/120    avg_loss:0.108, val_acc:0.927]
Epoch [71/120    avg_loss:0.100, val_acc:0.946]
Epoch [72/120    avg_loss:0.107, val_acc:0.945]
Epoch [73/120    avg_loss:0.097, val_acc:0.939]
Epoch [74/120    avg_loss:0.117, val_acc:0.938]
Epoch [75/120    avg_loss:0.089, val_acc:0.951]
Epoch [76/120    avg_loss:0.083, val_acc:0.932]
Epoch [77/120    avg_loss:0.088, val_acc:0.949]
Epoch [78/120    avg_loss:0.085, val_acc:0.941]
Epoch [79/120    avg_loss:0.079, val_acc:0.941]
Epoch [80/120    avg_loss:0.082, val_acc:0.929]
Epoch [81/120    avg_loss:0.085, val_acc:0.928]
Epoch [82/120    avg_loss:0.073, val_acc:0.939]
Epoch [83/120    avg_loss:0.073, val_acc:0.947]
Epoch [84/120    avg_loss:0.063, val_acc:0.952]
Epoch [85/120    avg_loss:0.086, val_acc:0.937]
Epoch [86/120    avg_loss:0.069, val_acc:0.943]
Epoch [87/120    avg_loss:0.049, val_acc:0.952]
Epoch [88/120    avg_loss:0.063, val_acc:0.941]
Epoch [89/120    avg_loss:0.060, val_acc:0.952]
Epoch [90/120    avg_loss:0.057, val_acc:0.953]
Epoch [91/120    avg_loss:0.065, val_acc:0.942]
Epoch [92/120    avg_loss:0.081, val_acc:0.936]
Epoch [93/120    avg_loss:0.084, val_acc:0.946]
Epoch [94/120    avg_loss:0.059, val_acc:0.939]
Epoch [95/120    avg_loss:0.071, val_acc:0.929]
Epoch [96/120    avg_loss:0.078, val_acc:0.947]
Epoch [97/120    avg_loss:0.062, val_acc:0.945]
Epoch [98/120    avg_loss:0.062, val_acc:0.948]
Epoch [99/120    avg_loss:0.065, val_acc:0.948]
Epoch [100/120    avg_loss:0.061, val_acc:0.951]
Epoch [101/120    avg_loss:0.051, val_acc:0.951]
Epoch [102/120    avg_loss:0.049, val_acc:0.948]
Epoch [103/120    avg_loss:0.067, val_acc:0.957]
Epoch [104/120    avg_loss:0.045, val_acc:0.952]
Epoch [105/120    avg_loss:0.046, val_acc:0.955]
Epoch [106/120    avg_loss:0.041, val_acc:0.958]
Epoch [107/120    avg_loss:0.041, val_acc:0.958]
Epoch [108/120    avg_loss:0.039, val_acc:0.955]
Epoch [109/120    avg_loss:0.044, val_acc:0.949]
Epoch [110/120    avg_loss:0.037, val_acc:0.953]
Epoch [111/120    avg_loss:0.045, val_acc:0.961]
Epoch [112/120    avg_loss:0.045, val_acc:0.957]
Epoch [113/120    avg_loss:0.062, val_acc:0.945]
Epoch [114/120    avg_loss:0.060, val_acc:0.950]
Epoch [115/120    avg_loss:0.048, val_acc:0.962]
Epoch [116/120    avg_loss:0.054, val_acc:0.958]
Epoch [117/120    avg_loss:0.042, val_acc:0.955]
Epoch [118/120    avg_loss:0.034, val_acc:0.959]
Epoch [119/120    avg_loss:0.043, val_acc:0.948]
Epoch [120/120    avg_loss:0.063, val_acc:0.940]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    0    0    0    0    0    0    0    0    6    0    0    0
     0    0    0]
 [   0    0 1217    3    0    0    7    0    0    0   20   33    4    1
     0    0    0]
 [   0    0    2  692    0   16    0    0    0    9    0    0   19    9
     0    0    0]
 [   0    0    0    0  211    0    0    0    0    0    0    1    1    0
     0    0    0]
 [   0    0    0    0    0  393    0   34    0    3    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    1    2    0
     0    0    0]
 [   0    0   37   54    0   11    0    0    0    0  727   33    2    0
     3    8    0]
 [   0    0   36    0    0    0    3    0    0    0    0 2162    7    2
     0    0    0]
 [   0    0    0    0    0    8    0    0    0    0    5   11  505    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1134    4    0]
 [   0    0    0    0    0    0   31    0    0    0    0    0    0   25
    23  268    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0   13    0
     0    0   71]]

Accuracy:
94.5691056910569

F1 scores:
[       nan 0.92105263 0.94450912 0.92513369 0.99528302 0.91077636
 0.96745562 0.5952381  0.99883856 0.66666667 0.89038579 0.97081275
 0.92916283 0.90909091 0.984375   0.85486443 0.8875    ]

Kappa:
0.9380591169103485
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb3f3fefa58>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.833, val_acc:0.043]
Epoch [2/120    avg_loss:2.759, val_acc:0.203]
Epoch [3/120    avg_loss:2.682, val_acc:0.239]
Epoch [4/120    avg_loss:2.604, val_acc:0.290]
Epoch [5/120    avg_loss:2.527, val_acc:0.315]
Epoch [6/120    avg_loss:2.411, val_acc:0.320]
Epoch [7/120    avg_loss:2.343, val_acc:0.324]
Epoch [8/120    avg_loss:2.278, val_acc:0.386]
Epoch [9/120    avg_loss:2.252, val_acc:0.417]
Epoch [10/120    avg_loss:2.152, val_acc:0.433]
Epoch [11/120    avg_loss:2.138, val_acc:0.480]
Epoch [12/120    avg_loss:2.074, val_acc:0.450]
Epoch [13/120    avg_loss:2.017, val_acc:0.525]
Epoch [14/120    avg_loss:1.987, val_acc:0.537]
Epoch [15/120    avg_loss:1.879, val_acc:0.536]
Epoch [16/120    avg_loss:1.834, val_acc:0.558]
Epoch [17/120    avg_loss:1.734, val_acc:0.590]
Epoch [18/120    avg_loss:1.675, val_acc:0.599]
Epoch [19/120    avg_loss:1.601, val_acc:0.602]
Epoch [20/120    avg_loss:1.525, val_acc:0.640]
Epoch [21/120    avg_loss:1.436, val_acc:0.658]
Epoch [22/120    avg_loss:1.431, val_acc:0.657]
Epoch [23/120    avg_loss:1.321, val_acc:0.684]
Epoch [24/120    avg_loss:1.229, val_acc:0.711]
Epoch [25/120    avg_loss:1.153, val_acc:0.722]
Epoch [26/120    avg_loss:1.089, val_acc:0.721]
Epoch [27/120    avg_loss:1.005, val_acc:0.733]
Epoch [28/120    avg_loss:0.955, val_acc:0.755]
Epoch [29/120    avg_loss:0.868, val_acc:0.758]
Epoch [30/120    avg_loss:0.797, val_acc:0.764]
Epoch [31/120    avg_loss:0.759, val_acc:0.766]
Epoch [32/120    avg_loss:0.726, val_acc:0.797]
Epoch [33/120    avg_loss:0.672, val_acc:0.809]
Epoch [34/120    avg_loss:0.662, val_acc:0.791]
Epoch [35/120    avg_loss:0.596, val_acc:0.809]
Epoch [36/120    avg_loss:0.656, val_acc:0.780]
Epoch [37/120    avg_loss:0.653, val_acc:0.836]
Epoch [38/120    avg_loss:0.571, val_acc:0.848]
Epoch [39/120    avg_loss:0.526, val_acc:0.830]
Epoch [40/120    avg_loss:0.516, val_acc:0.845]
Epoch [41/120    avg_loss:0.473, val_acc:0.854]
Epoch [42/120    avg_loss:0.425, val_acc:0.863]
Epoch [43/120    avg_loss:0.365, val_acc:0.895]
Epoch [44/120    avg_loss:0.355, val_acc:0.889]
Epoch [45/120    avg_loss:0.331, val_acc:0.867]
Epoch [46/120    avg_loss:0.320, val_acc:0.886]
Epoch [47/120    avg_loss:0.279, val_acc:0.889]
Epoch [48/120    avg_loss:0.294, val_acc:0.886]
Epoch [49/120    avg_loss:0.279, val_acc:0.917]
Epoch [50/120    avg_loss:0.255, val_acc:0.916]
Epoch [51/120    avg_loss:0.233, val_acc:0.909]
Epoch [52/120    avg_loss:0.218, val_acc:0.924]
Epoch [53/120    avg_loss:0.220, val_acc:0.914]
Epoch [54/120    avg_loss:0.194, val_acc:0.917]
Epoch [55/120    avg_loss:0.176, val_acc:0.938]
Epoch [56/120    avg_loss:0.170, val_acc:0.926]
Epoch [57/120    avg_loss:0.171, val_acc:0.908]
Epoch [58/120    avg_loss:0.216, val_acc:0.922]
Epoch [59/120    avg_loss:0.180, val_acc:0.907]
Epoch [60/120    avg_loss:0.181, val_acc:0.938]
Epoch [61/120    avg_loss:0.181, val_acc:0.925]
Epoch [62/120    avg_loss:0.189, val_acc:0.903]
Epoch [63/120    avg_loss:0.151, val_acc:0.932]
Epoch [64/120    avg_loss:0.146, val_acc:0.932]
Epoch [65/120    avg_loss:0.138, val_acc:0.921]
Epoch [66/120    avg_loss:0.151, val_acc:0.927]
Epoch [67/120    avg_loss:0.114, val_acc:0.939]
Epoch [68/120    avg_loss:0.116, val_acc:0.934]
Epoch [69/120    avg_loss:0.117, val_acc:0.916]
Epoch [70/120    avg_loss:0.136, val_acc:0.929]
Epoch [71/120    avg_loss:0.141, val_acc:0.938]
Epoch [72/120    avg_loss:0.101, val_acc:0.937]
Epoch [73/120    avg_loss:0.118, val_acc:0.937]
Epoch [74/120    avg_loss:0.091, val_acc:0.946]
Epoch [75/120    avg_loss:0.085, val_acc:0.934]
Epoch [76/120    avg_loss:0.092, val_acc:0.929]
Epoch [77/120    avg_loss:0.089, val_acc:0.950]
Epoch [78/120    avg_loss:0.082, val_acc:0.946]
Epoch [79/120    avg_loss:0.088, val_acc:0.950]
Epoch [80/120    avg_loss:0.080, val_acc:0.935]
Epoch [81/120    avg_loss:0.087, val_acc:0.945]
Epoch [82/120    avg_loss:0.068, val_acc:0.951]
Epoch [83/120    avg_loss:0.081, val_acc:0.951]
Epoch [84/120    avg_loss:0.086, val_acc:0.945]
Epoch [85/120    avg_loss:0.096, val_acc:0.953]
Epoch [86/120    avg_loss:0.076, val_acc:0.949]
Epoch [87/120    avg_loss:0.055, val_acc:0.949]
Epoch [88/120    avg_loss:0.056, val_acc:0.937]
Epoch [89/120    avg_loss:0.073, val_acc:0.949]
Epoch [90/120    avg_loss:0.068, val_acc:0.957]
Epoch [91/120    avg_loss:0.060, val_acc:0.947]
Epoch [92/120    avg_loss:0.073, val_acc:0.947]
Epoch [93/120    avg_loss:0.065, val_acc:0.952]
Epoch [94/120    avg_loss:0.053, val_acc:0.959]
Epoch [95/120    avg_loss:0.053, val_acc:0.952]
Epoch [96/120    avg_loss:0.047, val_acc:0.960]
Epoch [97/120    avg_loss:0.042, val_acc:0.947]
Epoch [98/120    avg_loss:0.049, val_acc:0.954]
Epoch [99/120    avg_loss:0.045, val_acc:0.959]
Epoch [100/120    avg_loss:0.036, val_acc:0.959]
Epoch [101/120    avg_loss:0.041, val_acc:0.959]
Epoch [102/120    avg_loss:0.059, val_acc:0.954]
Epoch [103/120    avg_loss:0.046, val_acc:0.957]
Epoch [104/120    avg_loss:0.044, val_acc:0.952]
Epoch [105/120    avg_loss:0.038, val_acc:0.965]
Epoch [106/120    avg_loss:0.031, val_acc:0.957]
Epoch [107/120    avg_loss:0.031, val_acc:0.963]
Epoch [108/120    avg_loss:0.029, val_acc:0.963]
Epoch [109/120    avg_loss:0.032, val_acc:0.966]
Epoch [110/120    avg_loss:0.030, val_acc:0.962]
Epoch [111/120    avg_loss:0.040, val_acc:0.960]
Epoch [112/120    avg_loss:0.034, val_acc:0.963]
Epoch [113/120    avg_loss:0.035, val_acc:0.961]
Epoch [114/120    avg_loss:0.031, val_acc:0.961]
Epoch [115/120    avg_loss:0.038, val_acc:0.960]
Epoch [116/120    avg_loss:0.048, val_acc:0.952]
Epoch [117/120    avg_loss:0.050, val_acc:0.963]
Epoch [118/120    avg_loss:0.038, val_acc:0.966]
Epoch [119/120    avg_loss:0.032, val_acc:0.964]
Epoch [120/120    avg_loss:0.028, val_acc:0.959]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0 1238    1    3    0    1    0    0    1    0   36    2    0
     0    3    0]
 [   0    0    4  679    2   21    1    0    0   20    1    0   14    5
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    5    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   14   52    0    4    2    0    0    0  780   10    0    0
     0   13    0]
 [   0    0   16    0    0    0    2    0    0    0    2 2181    9    0
     0    0    0]
 [   0    0    2   11    1    4    0    0    0    1   15    0  469    0
     0    4   27]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0    0    0    0    0
  1128   10    0]
 [   0    0    0    0    0    0   13    0    0    3    0    0    0    0
    40  291    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.84823848238483

F1 scores:
[       nan 0.98765432 0.9671875  0.9114094  0.98611111 0.95730337
 0.98498498 1.         0.99883586 0.54545455 0.93245666 0.98265375
 0.91156463 0.98666667 0.97620078 0.87125749 0.86153846]

Kappa:
0.9526714865684234
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f726261bb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.794, val_acc:0.217]
Epoch [2/120    avg_loss:2.712, val_acc:0.319]
Epoch [3/120    avg_loss:2.612, val_acc:0.348]
Epoch [4/120    avg_loss:2.542, val_acc:0.380]
Epoch [5/120    avg_loss:2.464, val_acc:0.402]
Epoch [6/120    avg_loss:2.432, val_acc:0.427]
Epoch [7/120    avg_loss:2.335, val_acc:0.407]
Epoch [8/120    avg_loss:2.283, val_acc:0.415]
Epoch [9/120    avg_loss:2.258, val_acc:0.425]
Epoch [10/120    avg_loss:2.180, val_acc:0.407]
Epoch [11/120    avg_loss:2.174, val_acc:0.451]
Epoch [12/120    avg_loss:2.075, val_acc:0.443]
Epoch [13/120    avg_loss:2.050, val_acc:0.530]
Epoch [14/120    avg_loss:1.985, val_acc:0.570]
Epoch [15/120    avg_loss:1.905, val_acc:0.594]
Epoch [16/120    avg_loss:1.808, val_acc:0.630]
Epoch [17/120    avg_loss:1.767, val_acc:0.651]
Epoch [18/120    avg_loss:1.687, val_acc:0.667]
Epoch [19/120    avg_loss:1.614, val_acc:0.643]
Epoch [20/120    avg_loss:1.524, val_acc:0.681]
Epoch [21/120    avg_loss:1.452, val_acc:0.689]
Epoch [22/120    avg_loss:1.362, val_acc:0.692]
Epoch [23/120    avg_loss:1.188, val_acc:0.731]
Epoch [24/120    avg_loss:1.222, val_acc:0.728]
Epoch [25/120    avg_loss:1.152, val_acc:0.723]
Epoch [26/120    avg_loss:1.079, val_acc:0.745]
Epoch [27/120    avg_loss:1.004, val_acc:0.776]
Epoch [28/120    avg_loss:0.970, val_acc:0.767]
Epoch [29/120    avg_loss:0.939, val_acc:0.782]
Epoch [30/120    avg_loss:0.855, val_acc:0.791]
Epoch [31/120    avg_loss:0.745, val_acc:0.777]
Epoch [32/120    avg_loss:0.760, val_acc:0.797]
Epoch [33/120    avg_loss:0.721, val_acc:0.801]
Epoch [34/120    avg_loss:0.624, val_acc:0.801]
Epoch [35/120    avg_loss:0.608, val_acc:0.824]
Epoch [36/120    avg_loss:0.625, val_acc:0.811]
Epoch [37/120    avg_loss:0.581, val_acc:0.819]
Epoch [38/120    avg_loss:0.570, val_acc:0.823]
Epoch [39/120    avg_loss:0.505, val_acc:0.834]
Epoch [40/120    avg_loss:0.462, val_acc:0.874]
Epoch [41/120    avg_loss:0.392, val_acc:0.882]
Epoch [42/120    avg_loss:0.385, val_acc:0.886]
Epoch [43/120    avg_loss:0.387, val_acc:0.887]
Epoch [44/120    avg_loss:0.385, val_acc:0.891]
Epoch [45/120    avg_loss:0.367, val_acc:0.853]
Epoch [46/120    avg_loss:0.351, val_acc:0.891]
Epoch [47/120    avg_loss:0.283, val_acc:0.912]
Epoch [48/120    avg_loss:0.289, val_acc:0.905]
Epoch [49/120    avg_loss:0.300, val_acc:0.868]
Epoch [50/120    avg_loss:0.276, val_acc:0.903]
Epoch [51/120    avg_loss:0.235, val_acc:0.901]
Epoch [52/120    avg_loss:0.244, val_acc:0.917]
Epoch [53/120    avg_loss:0.243, val_acc:0.885]
Epoch [54/120    avg_loss:0.221, val_acc:0.910]
Epoch [55/120    avg_loss:0.251, val_acc:0.920]
Epoch [56/120    avg_loss:0.224, val_acc:0.901]
Epoch [57/120    avg_loss:0.170, val_acc:0.923]
Epoch [58/120    avg_loss:0.157, val_acc:0.932]
Epoch [59/120    avg_loss:0.140, val_acc:0.935]
Epoch [60/120    avg_loss:0.132, val_acc:0.941]
Epoch [61/120    avg_loss:0.130, val_acc:0.933]
Epoch [62/120    avg_loss:0.121, val_acc:0.938]
Epoch [63/120    avg_loss:0.124, val_acc:0.948]
Epoch [64/120    avg_loss:0.109, val_acc:0.928]
Epoch [65/120    avg_loss:0.129, val_acc:0.944]
Epoch [66/120    avg_loss:0.107, val_acc:0.947]
Epoch [67/120    avg_loss:0.096, val_acc:0.947]
Epoch [68/120    avg_loss:0.101, val_acc:0.939]
Epoch [69/120    avg_loss:0.097, val_acc:0.932]
Epoch [70/120    avg_loss:0.112, val_acc:0.932]
Epoch [71/120    avg_loss:0.113, val_acc:0.943]
Epoch [72/120    avg_loss:0.096, val_acc:0.943]
Epoch [73/120    avg_loss:0.091, val_acc:0.956]
Epoch [74/120    avg_loss:0.065, val_acc:0.955]
Epoch [75/120    avg_loss:0.073, val_acc:0.948]
Epoch [76/120    avg_loss:0.083, val_acc:0.957]
Epoch [77/120    avg_loss:0.075, val_acc:0.952]
Epoch [78/120    avg_loss:0.068, val_acc:0.941]
Epoch [79/120    avg_loss:0.085, val_acc:0.957]
Epoch [80/120    avg_loss:0.080, val_acc:0.936]
Epoch [81/120    avg_loss:0.099, val_acc:0.932]
Epoch [82/120    avg_loss:0.090, val_acc:0.951]
Epoch [83/120    avg_loss:0.087, val_acc:0.955]
Epoch [84/120    avg_loss:0.062, val_acc:0.955]
Epoch [85/120    avg_loss:0.063, val_acc:0.948]
Epoch [86/120    avg_loss:0.064, val_acc:0.951]
Epoch [87/120    avg_loss:0.048, val_acc:0.947]
Epoch [88/120    avg_loss:0.040, val_acc:0.959]
Epoch [89/120    avg_loss:0.037, val_acc:0.961]
Epoch [90/120    avg_loss:0.045, val_acc:0.953]
Epoch [91/120    avg_loss:0.035, val_acc:0.963]
Epoch [92/120    avg_loss:0.036, val_acc:0.966]
Epoch [93/120    avg_loss:0.035, val_acc:0.965]
Epoch [94/120    avg_loss:0.033, val_acc:0.963]
Epoch [95/120    avg_loss:0.034, val_acc:0.967]
Epoch [96/120    avg_loss:0.030, val_acc:0.961]
Epoch [97/120    avg_loss:0.028, val_acc:0.961]
Epoch [98/120    avg_loss:0.032, val_acc:0.969]
Epoch [99/120    avg_loss:0.032, val_acc:0.969]
Epoch [100/120    avg_loss:0.035, val_acc:0.964]
Epoch [101/120    avg_loss:0.037, val_acc:0.966]
Epoch [102/120    avg_loss:0.028, val_acc:0.972]
Epoch [103/120    avg_loss:0.036, val_acc:0.968]
Epoch [104/120    avg_loss:0.027, val_acc:0.965]
Epoch [105/120    avg_loss:0.027, val_acc:0.958]
Epoch [106/120    avg_loss:0.034, val_acc:0.966]
Epoch [107/120    avg_loss:0.023, val_acc:0.965]
Epoch [108/120    avg_loss:0.029, val_acc:0.953]
Epoch [109/120    avg_loss:0.029, val_acc:0.966]
Epoch [110/120    avg_loss:0.026, val_acc:0.965]
Epoch [111/120    avg_loss:0.028, val_acc:0.952]
Epoch [112/120    avg_loss:0.035, val_acc:0.966]
Epoch [113/120    avg_loss:0.039, val_acc:0.951]
Epoch [114/120    avg_loss:0.049, val_acc:0.948]
Epoch [115/120    avg_loss:0.042, val_acc:0.956]
Epoch [116/120    avg_loss:0.036, val_acc:0.965]
Epoch [117/120    avg_loss:0.028, val_acc:0.968]
Epoch [118/120    avg_loss:0.027, val_acc:0.972]
Epoch [119/120    avg_loss:0.022, val_acc:0.970]
Epoch [120/120    avg_loss:0.019, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1236    2    0    0    4    0    0    1    7   35    0    0
     0    0    0]
 [   0    0    1  702    6   15    1    0    0    7    2    0   11    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   15    0    0    2    0
     0    0    0]
 [   0    0   35   49    0    6    0    0    0    0  770    1    6    0
     0    8    0]
 [   0    0   15    0    0    0    3    0    3    0   11 2171    4    3
     0    0    0]
 [   0    0    0   23   12   10    0    0    0    0    9    0  477    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    0    0    0    0    1    1    0    0
  1128    0    0]
 [   0    0    0    0    0    0    2    0    0    4    0    0    0    0
    42  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.0

F1 scores:
[       nan 0.96202532 0.96111975 0.92186474 0.95945946 0.94911504
 0.99017385 0.98039216 0.99652375 0.63829787 0.91775924 0.98235294
 0.92173913 0.98666667 0.97577855 0.91437309 0.97647059]

Kappa:
0.9543991313178691
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f486dfe1a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.808, val_acc:0.105]
Epoch [2/120    avg_loss:2.724, val_acc:0.243]
Epoch [3/120    avg_loss:2.643, val_acc:0.267]
Epoch [4/120    avg_loss:2.574, val_acc:0.382]
Epoch [5/120    avg_loss:2.494, val_acc:0.453]
Epoch [6/120    avg_loss:2.437, val_acc:0.480]
Epoch [7/120    avg_loss:2.344, val_acc:0.524]
Epoch [8/120    avg_loss:2.282, val_acc:0.545]
Epoch [9/120    avg_loss:2.201, val_acc:0.567]
Epoch [10/120    avg_loss:2.172, val_acc:0.561]
Epoch [11/120    avg_loss:2.083, val_acc:0.577]
Epoch [12/120    avg_loss:2.065, val_acc:0.575]
Epoch [13/120    avg_loss:1.990, val_acc:0.600]
Epoch [14/120    avg_loss:1.907, val_acc:0.621]
Epoch [15/120    avg_loss:1.882, val_acc:0.595]
Epoch [16/120    avg_loss:1.811, val_acc:0.634]
Epoch [17/120    avg_loss:1.681, val_acc:0.666]
Epoch [18/120    avg_loss:1.640, val_acc:0.664]
Epoch [19/120    avg_loss:1.541, val_acc:0.689]
Epoch [20/120    avg_loss:1.413, val_acc:0.711]
Epoch [21/120    avg_loss:1.357, val_acc:0.721]
Epoch [22/120    avg_loss:1.300, val_acc:0.693]
Epoch [23/120    avg_loss:1.256, val_acc:0.705]
Epoch [24/120    avg_loss:1.111, val_acc:0.723]
Epoch [25/120    avg_loss:0.987, val_acc:0.764]
Epoch [26/120    avg_loss:0.971, val_acc:0.764]
Epoch [27/120    avg_loss:0.898, val_acc:0.746]
Epoch [28/120    avg_loss:0.905, val_acc:0.765]
Epoch [29/120    avg_loss:0.861, val_acc:0.720]
Epoch [30/120    avg_loss:0.838, val_acc:0.740]
Epoch [31/120    avg_loss:0.823, val_acc:0.763]
Epoch [32/120    avg_loss:0.712, val_acc:0.789]
Epoch [33/120    avg_loss:0.676, val_acc:0.789]
Epoch [34/120    avg_loss:0.637, val_acc:0.809]
Epoch [35/120    avg_loss:0.554, val_acc:0.811]
Epoch [36/120    avg_loss:0.548, val_acc:0.836]
Epoch [37/120    avg_loss:0.568, val_acc:0.825]
Epoch [38/120    avg_loss:0.483, val_acc:0.848]
Epoch [39/120    avg_loss:0.461, val_acc:0.864]
Epoch [40/120    avg_loss:0.466, val_acc:0.852]
Epoch [41/120    avg_loss:0.424, val_acc:0.864]
Epoch [42/120    avg_loss:0.453, val_acc:0.858]
Epoch [43/120    avg_loss:0.407, val_acc:0.867]
Epoch [44/120    avg_loss:0.371, val_acc:0.883]
Epoch [45/120    avg_loss:0.354, val_acc:0.866]
Epoch [46/120    avg_loss:0.333, val_acc:0.885]
Epoch [47/120    avg_loss:0.305, val_acc:0.893]
Epoch [48/120    avg_loss:0.291, val_acc:0.904]
Epoch [49/120    avg_loss:0.269, val_acc:0.907]
Epoch [50/120    avg_loss:0.250, val_acc:0.893]
Epoch [51/120    avg_loss:0.262, val_acc:0.895]
Epoch [52/120    avg_loss:0.238, val_acc:0.927]
Epoch [53/120    avg_loss:0.186, val_acc:0.927]
Epoch [54/120    avg_loss:0.216, val_acc:0.922]
Epoch [55/120    avg_loss:0.216, val_acc:0.896]
Epoch [56/120    avg_loss:0.232, val_acc:0.916]
Epoch [57/120    avg_loss:0.188, val_acc:0.908]
Epoch [58/120    avg_loss:0.193, val_acc:0.929]
Epoch [59/120    avg_loss:0.166, val_acc:0.949]
Epoch [60/120    avg_loss:0.136, val_acc:0.945]
Epoch [61/120    avg_loss:0.135, val_acc:0.942]
Epoch [62/120    avg_loss:0.138, val_acc:0.938]
Epoch [63/120    avg_loss:0.149, val_acc:0.947]
Epoch [64/120    avg_loss:0.211, val_acc:0.929]
Epoch [65/120    avg_loss:0.167, val_acc:0.923]
Epoch [66/120    avg_loss:0.139, val_acc:0.948]
Epoch [67/120    avg_loss:0.130, val_acc:0.939]
Epoch [68/120    avg_loss:0.126, val_acc:0.947]
Epoch [69/120    avg_loss:0.118, val_acc:0.951]
Epoch [70/120    avg_loss:0.097, val_acc:0.940]
Epoch [71/120    avg_loss:0.117, val_acc:0.936]
Epoch [72/120    avg_loss:0.097, val_acc:0.943]
Epoch [73/120    avg_loss:0.099, val_acc:0.960]
Epoch [74/120    avg_loss:0.082, val_acc:0.947]
Epoch [75/120    avg_loss:0.090, val_acc:0.949]
Epoch [76/120    avg_loss:0.077, val_acc:0.942]
Epoch [77/120    avg_loss:0.065, val_acc:0.945]
Epoch [78/120    avg_loss:0.072, val_acc:0.959]
Epoch [79/120    avg_loss:0.061, val_acc:0.964]
Epoch [80/120    avg_loss:0.052, val_acc:0.957]
Epoch [81/120    avg_loss:0.057, val_acc:0.963]
Epoch [82/120    avg_loss:0.047, val_acc:0.959]
Epoch [83/120    avg_loss:0.053, val_acc:0.968]
Epoch [84/120    avg_loss:0.054, val_acc:0.955]
Epoch [85/120    avg_loss:0.055, val_acc:0.970]
Epoch [86/120    avg_loss:0.051, val_acc:0.954]
Epoch [87/120    avg_loss:0.056, val_acc:0.964]
Epoch [88/120    avg_loss:0.052, val_acc:0.957]
Epoch [89/120    avg_loss:0.050, val_acc:0.967]
Epoch [90/120    avg_loss:0.170, val_acc:0.908]
Epoch [91/120    avg_loss:0.142, val_acc:0.941]
Epoch [92/120    avg_loss:0.081, val_acc:0.965]
Epoch [93/120    avg_loss:0.055, val_acc:0.947]
Epoch [94/120    avg_loss:0.054, val_acc:0.954]
Epoch [95/120    avg_loss:0.049, val_acc:0.968]
Epoch [96/120    avg_loss:0.056, val_acc:0.957]
Epoch [97/120    avg_loss:0.036, val_acc:0.968]
Epoch [98/120    avg_loss:0.042, val_acc:0.972]
Epoch [99/120    avg_loss:0.050, val_acc:0.973]
Epoch [100/120    avg_loss:0.036, val_acc:0.974]
Epoch [101/120    avg_loss:0.042, val_acc:0.959]
Epoch [102/120    avg_loss:0.055, val_acc:0.965]
Epoch [103/120    avg_loss:0.056, val_acc:0.941]
Epoch [104/120    avg_loss:0.047, val_acc:0.963]
Epoch [105/120    avg_loss:0.038, val_acc:0.971]
Epoch [106/120    avg_loss:0.039, val_acc:0.971]
Epoch [107/120    avg_loss:0.030, val_acc:0.971]
Epoch [108/120    avg_loss:0.031, val_acc:0.980]
Epoch [109/120    avg_loss:0.030, val_acc:0.961]
Epoch [110/120    avg_loss:0.038, val_acc:0.966]
Epoch [111/120    avg_loss:0.048, val_acc:0.958]
Epoch [112/120    avg_loss:0.044, val_acc:0.974]
Epoch [113/120    avg_loss:0.030, val_acc:0.970]
Epoch [114/120    avg_loss:0.028, val_acc:0.973]
Epoch [115/120    avg_loss:0.030, val_acc:0.975]
Epoch [116/120    avg_loss:0.024, val_acc:0.973]
Epoch [117/120    avg_loss:0.026, val_acc:0.970]
Epoch [118/120    avg_loss:0.037, val_acc:0.965]
Epoch [119/120    avg_loss:0.062, val_acc:0.967]
Epoch [120/120    avg_loss:0.050, val_acc:0.924]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    1    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1245    9    1    1    0    0    0    0    3   26    0    0
     0    0    0]
 [   0    0    1  726    1    9    0    0    0    2    1    2    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   39   25    0    5    0    0    0    0  799    2    1    0
     1    3    0]
 [   0    0   32    0    0    0    0    1    0    0   33 2141    2    1
     0    0    0]
 [   0    0    0   26    0    3    0    0    0    0   16   13  473    0
     2    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0  486    2    0    2    0    1    0    0    0
   647    1    0]
 [   0    0    0    0    0    0   41    0    0    1    0    0    0    0
    48  257    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
90.6449864498645

F1 scores:
[       nan 0.93506494 0.95658855 0.94716243 0.9953271  0.63119534
 0.96617647 0.96153846 0.99767981 0.83333333 0.92263279 0.97428896
 0.92836114 0.99730458 0.70440936 0.84539474 0.96969697]

Kappa:
0.893782792974566
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f064dff1ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.830, val_acc:0.186]
Epoch [2/120    avg_loss:2.736, val_acc:0.259]
Epoch [3/120    avg_loss:2.667, val_acc:0.314]
Epoch [4/120    avg_loss:2.565, val_acc:0.417]
Epoch [5/120    avg_loss:2.473, val_acc:0.491]
Epoch [6/120    avg_loss:2.368, val_acc:0.533]
Epoch [7/120    avg_loss:2.352, val_acc:0.543]
Epoch [8/120    avg_loss:2.310, val_acc:0.551]
Epoch [9/120    avg_loss:2.246, val_acc:0.575]
Epoch [10/120    avg_loss:2.239, val_acc:0.572]
Epoch [11/120    avg_loss:2.118, val_acc:0.593]
Epoch [12/120    avg_loss:2.049, val_acc:0.600]
Epoch [13/120    avg_loss:1.971, val_acc:0.597]
Epoch [14/120    avg_loss:1.916, val_acc:0.611]
Epoch [15/120    avg_loss:1.841, val_acc:0.616]
Epoch [16/120    avg_loss:1.798, val_acc:0.621]
Epoch [17/120    avg_loss:1.718, val_acc:0.627]
Epoch [18/120    avg_loss:1.650, val_acc:0.628]
Epoch [19/120    avg_loss:1.628, val_acc:0.630]
Epoch [20/120    avg_loss:1.525, val_acc:0.661]
Epoch [21/120    avg_loss:1.469, val_acc:0.672]
Epoch [22/120    avg_loss:1.363, val_acc:0.674]
Epoch [23/120    avg_loss:1.334, val_acc:0.714]
Epoch [24/120    avg_loss:1.180, val_acc:0.736]
Epoch [25/120    avg_loss:1.135, val_acc:0.757]
Epoch [26/120    avg_loss:1.080, val_acc:0.730]
Epoch [27/120    avg_loss:0.976, val_acc:0.745]
Epoch [28/120    avg_loss:0.889, val_acc:0.763]
Epoch [29/120    avg_loss:0.889, val_acc:0.749]
Epoch [30/120    avg_loss:0.905, val_acc:0.742]
Epoch [31/120    avg_loss:0.794, val_acc:0.777]
Epoch [32/120    avg_loss:0.725, val_acc:0.778]
Epoch [33/120    avg_loss:0.618, val_acc:0.823]
Epoch [34/120    avg_loss:0.623, val_acc:0.808]
Epoch [35/120    avg_loss:0.566, val_acc:0.840]
Epoch [36/120    avg_loss:0.531, val_acc:0.838]
Epoch [37/120    avg_loss:0.531, val_acc:0.854]
Epoch [38/120    avg_loss:0.439, val_acc:0.849]
Epoch [39/120    avg_loss:0.451, val_acc:0.815]
Epoch [40/120    avg_loss:0.391, val_acc:0.862]
Epoch [41/120    avg_loss:0.352, val_acc:0.889]
Epoch [42/120    avg_loss:0.306, val_acc:0.897]
Epoch [43/120    avg_loss:0.280, val_acc:0.912]
Epoch [44/120    avg_loss:0.257, val_acc:0.900]
Epoch [45/120    avg_loss:0.229, val_acc:0.912]
Epoch [46/120    avg_loss:0.283, val_acc:0.912]
Epoch [47/120    avg_loss:0.228, val_acc:0.904]
Epoch [48/120    avg_loss:0.237, val_acc:0.893]
Epoch [49/120    avg_loss:0.211, val_acc:0.911]
Epoch [50/120    avg_loss:0.201, val_acc:0.920]
Epoch [51/120    avg_loss:0.241, val_acc:0.905]
Epoch [52/120    avg_loss:0.212, val_acc:0.900]
Epoch [53/120    avg_loss:0.185, val_acc:0.913]
Epoch [54/120    avg_loss:0.186, val_acc:0.934]
Epoch [55/120    avg_loss:0.166, val_acc:0.914]
Epoch [56/120    avg_loss:0.155, val_acc:0.923]
Epoch [57/120    avg_loss:0.130, val_acc:0.939]
Epoch [58/120    avg_loss:0.140, val_acc:0.928]
Epoch [59/120    avg_loss:0.142, val_acc:0.933]
Epoch [60/120    avg_loss:0.126, val_acc:0.924]
Epoch [61/120    avg_loss:0.189, val_acc:0.913]
Epoch [62/120    avg_loss:0.210, val_acc:0.910]
Epoch [63/120    avg_loss:0.165, val_acc:0.926]
Epoch [64/120    avg_loss:0.130, val_acc:0.928]
Epoch [65/120    avg_loss:0.122, val_acc:0.939]
Epoch [66/120    avg_loss:0.113, val_acc:0.934]
Epoch [67/120    avg_loss:0.094, val_acc:0.940]
Epoch [68/120    avg_loss:0.086, val_acc:0.950]
Epoch [69/120    avg_loss:0.080, val_acc:0.954]
Epoch [70/120    avg_loss:0.074, val_acc:0.947]
Epoch [71/120    avg_loss:0.075, val_acc:0.946]
Epoch [72/120    avg_loss:0.089, val_acc:0.943]
Epoch [73/120    avg_loss:0.067, val_acc:0.953]
Epoch [74/120    avg_loss:0.075, val_acc:0.946]
Epoch [75/120    avg_loss:0.076, val_acc:0.955]
Epoch [76/120    avg_loss:0.070, val_acc:0.955]
Epoch [77/120    avg_loss:0.082, val_acc:0.937]
Epoch [78/120    avg_loss:0.072, val_acc:0.947]
Epoch [79/120    avg_loss:0.076, val_acc:0.938]
Epoch [80/120    avg_loss:0.063, val_acc:0.948]
Epoch [81/120    avg_loss:0.060, val_acc:0.951]
Epoch [82/120    avg_loss:0.060, val_acc:0.963]
Epoch [83/120    avg_loss:0.051, val_acc:0.961]
Epoch [84/120    avg_loss:0.054, val_acc:0.952]
Epoch [85/120    avg_loss:0.051, val_acc:0.959]
Epoch [86/120    avg_loss:0.055, val_acc:0.954]
Epoch [87/120    avg_loss:0.056, val_acc:0.958]
Epoch [88/120    avg_loss:0.044, val_acc:0.957]
Epoch [89/120    avg_loss:0.044, val_acc:0.966]
Epoch [90/120    avg_loss:0.044, val_acc:0.964]
Epoch [91/120    avg_loss:0.040, val_acc:0.971]
Epoch [92/120    avg_loss:0.035, val_acc:0.972]
Epoch [93/120    avg_loss:0.034, val_acc:0.962]
Epoch [94/120    avg_loss:0.031, val_acc:0.968]
Epoch [95/120    avg_loss:0.032, val_acc:0.954]
Epoch [96/120    avg_loss:0.053, val_acc:0.968]
Epoch [97/120    avg_loss:0.035, val_acc:0.968]
Epoch [98/120    avg_loss:0.046, val_acc:0.961]
Epoch [99/120    avg_loss:0.046, val_acc:0.963]
Epoch [100/120    avg_loss:0.034, val_acc:0.973]
Epoch [101/120    avg_loss:0.035, val_acc:0.968]
Epoch [102/120    avg_loss:0.026, val_acc:0.977]
Epoch [103/120    avg_loss:0.026, val_acc:0.974]
Epoch [104/120    avg_loss:0.028, val_acc:0.975]
Epoch [105/120    avg_loss:0.026, val_acc:0.975]
Epoch [106/120    avg_loss:0.029, val_acc:0.977]
Epoch [107/120    avg_loss:0.030, val_acc:0.971]
Epoch [108/120    avg_loss:0.026, val_acc:0.965]
Epoch [109/120    avg_loss:0.029, val_acc:0.972]
Epoch [110/120    avg_loss:0.026, val_acc:0.963]
Epoch [111/120    avg_loss:0.020, val_acc:0.979]
Epoch [112/120    avg_loss:0.022, val_acc:0.977]
Epoch [113/120    avg_loss:0.017, val_acc:0.982]
Epoch [114/120    avg_loss:0.019, val_acc:0.978]
Epoch [115/120    avg_loss:0.018, val_acc:0.977]
Epoch [116/120    avg_loss:0.015, val_acc:0.979]
Epoch [117/120    avg_loss:0.017, val_acc:0.974]
Epoch [118/120    avg_loss:0.017, val_acc:0.977]
Epoch [119/120    avg_loss:0.017, val_acc:0.974]
Epoch [120/120    avg_loss:0.021, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1198   16   12    0    0    0    0    0   11   47    1    0
     0    0    0]
 [   0    0    0  696    3   24    0    0    0   14    0    3    6    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   15    0    1    0    0
     0    0    0]
 [   0    0   16   14    0    4    0    0    0    0  836    4    1    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0   24 2181    2    1
     0    0    0]
 [   0    0    0    8    0    0    0    0    0    0   17   23  484    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   15    0    0    1    0    3    0    0    0
  1120    0    0]
 [   0    0    0    1    0    1   23    0    0    2    0    0    0    0
    67  253    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.869918699187

F1 scores:
[       nan 0.96202532 0.95801679 0.93800539 0.96598639 0.95185996
 0.97974494 1.         0.99883856 0.6122449  0.94516676 0.97583893
 0.93980583 0.99462366 0.96137339 0.84333333 0.98203593]

Kappa:
0.9528672580610101
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f63a91bbac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.827, val_acc:0.086]
Epoch [2/120    avg_loss:2.747, val_acc:0.282]
Epoch [3/120    avg_loss:2.697, val_acc:0.443]
Epoch [4/120    avg_loss:2.588, val_acc:0.477]
Epoch [5/120    avg_loss:2.557, val_acc:0.468]
Epoch [6/120    avg_loss:2.453, val_acc:0.512]
Epoch [7/120    avg_loss:2.353, val_acc:0.480]
Epoch [8/120    avg_loss:2.285, val_acc:0.461]
Epoch [9/120    avg_loss:2.277, val_acc:0.499]
Epoch [10/120    avg_loss:2.224, val_acc:0.543]
Epoch [11/120    avg_loss:2.114, val_acc:0.487]
Epoch [12/120    avg_loss:2.043, val_acc:0.571]
Epoch [13/120    avg_loss:1.994, val_acc:0.589]
Epoch [14/120    avg_loss:1.957, val_acc:0.589]
Epoch [15/120    avg_loss:1.885, val_acc:0.590]
Epoch [16/120    avg_loss:1.808, val_acc:0.568]
Epoch [17/120    avg_loss:1.738, val_acc:0.597]
Epoch [18/120    avg_loss:1.637, val_acc:0.653]
Epoch [19/120    avg_loss:1.558, val_acc:0.682]
Epoch [20/120    avg_loss:1.419, val_acc:0.687]
Epoch [21/120    avg_loss:1.382, val_acc:0.678]
Epoch [22/120    avg_loss:1.252, val_acc:0.718]
Epoch [23/120    avg_loss:1.137, val_acc:0.742]
Epoch [24/120    avg_loss:1.064, val_acc:0.739]
Epoch [25/120    avg_loss:1.010, val_acc:0.755]
Epoch [26/120    avg_loss:0.922, val_acc:0.762]
Epoch [27/120    avg_loss:0.910, val_acc:0.730]
Epoch [28/120    avg_loss:0.902, val_acc:0.737]
Epoch [29/120    avg_loss:0.807, val_acc:0.771]
Epoch [30/120    avg_loss:0.771, val_acc:0.789]
Epoch [31/120    avg_loss:0.710, val_acc:0.783]
Epoch [32/120    avg_loss:0.689, val_acc:0.783]
Epoch [33/120    avg_loss:0.646, val_acc:0.775]
Epoch [34/120    avg_loss:0.575, val_acc:0.821]
Epoch [35/120    avg_loss:0.505, val_acc:0.835]
Epoch [36/120    avg_loss:0.474, val_acc:0.832]
Epoch [37/120    avg_loss:0.492, val_acc:0.846]
Epoch [38/120    avg_loss:0.416, val_acc:0.850]
Epoch [39/120    avg_loss:0.384, val_acc:0.891]
Epoch [40/120    avg_loss:0.363, val_acc:0.859]
Epoch [41/120    avg_loss:0.342, val_acc:0.872]
Epoch [42/120    avg_loss:0.325, val_acc:0.866]
Epoch [43/120    avg_loss:0.316, val_acc:0.883]
Epoch [44/120    avg_loss:0.317, val_acc:0.893]
Epoch [45/120    avg_loss:0.302, val_acc:0.871]
Epoch [46/120    avg_loss:0.266, val_acc:0.892]
Epoch [47/120    avg_loss:0.222, val_acc:0.910]
Epoch [48/120    avg_loss:0.220, val_acc:0.902]
Epoch [49/120    avg_loss:0.204, val_acc:0.904]
Epoch [50/120    avg_loss:0.214, val_acc:0.903]
Epoch [51/120    avg_loss:0.257, val_acc:0.895]
Epoch [52/120    avg_loss:0.236, val_acc:0.903]
Epoch [53/120    avg_loss:0.203, val_acc:0.922]
Epoch [54/120    avg_loss:0.200, val_acc:0.897]
Epoch [55/120    avg_loss:0.190, val_acc:0.899]
Epoch [56/120    avg_loss:0.203, val_acc:0.899]
Epoch [57/120    avg_loss:0.237, val_acc:0.900]
Epoch [58/120    avg_loss:0.194, val_acc:0.913]
Epoch [59/120    avg_loss:0.174, val_acc:0.915]
Epoch [60/120    avg_loss:0.177, val_acc:0.904]
Epoch [61/120    avg_loss:0.156, val_acc:0.932]
Epoch [62/120    avg_loss:0.241, val_acc:0.890]
Epoch [63/120    avg_loss:0.233, val_acc:0.914]
Epoch [64/120    avg_loss:0.185, val_acc:0.923]
Epoch [65/120    avg_loss:0.144, val_acc:0.935]
Epoch [66/120    avg_loss:0.133, val_acc:0.916]
Epoch [67/120    avg_loss:0.120, val_acc:0.942]
Epoch [68/120    avg_loss:0.098, val_acc:0.939]
Epoch [69/120    avg_loss:0.099, val_acc:0.939]
Epoch [70/120    avg_loss:0.121, val_acc:0.938]
Epoch [71/120    avg_loss:0.118, val_acc:0.909]
Epoch [72/120    avg_loss:0.141, val_acc:0.921]
Epoch [73/120    avg_loss:0.172, val_acc:0.929]
Epoch [74/120    avg_loss:0.143, val_acc:0.926]
Epoch [75/120    avg_loss:0.109, val_acc:0.928]
Epoch [76/120    avg_loss:0.118, val_acc:0.941]
Epoch [77/120    avg_loss:0.088, val_acc:0.941]
Epoch [78/120    avg_loss:0.081, val_acc:0.952]
Epoch [79/120    avg_loss:0.096, val_acc:0.937]
Epoch [80/120    avg_loss:0.085, val_acc:0.938]
Epoch [81/120    avg_loss:0.094, val_acc:0.925]
Epoch [82/120    avg_loss:0.092, val_acc:0.951]
Epoch [83/120    avg_loss:0.073, val_acc:0.955]
Epoch [84/120    avg_loss:0.076, val_acc:0.945]
Epoch [85/120    avg_loss:0.061, val_acc:0.952]
Epoch [86/120    avg_loss:0.071, val_acc:0.948]
Epoch [87/120    avg_loss:0.068, val_acc:0.951]
Epoch [88/120    avg_loss:0.062, val_acc:0.949]
Epoch [89/120    avg_loss:0.058, val_acc:0.947]
Epoch [90/120    avg_loss:0.067, val_acc:0.946]
Epoch [91/120    avg_loss:0.060, val_acc:0.952]
Epoch [92/120    avg_loss:0.055, val_acc:0.953]
Epoch [93/120    avg_loss:0.053, val_acc:0.949]
Epoch [94/120    avg_loss:0.072, val_acc:0.943]
Epoch [95/120    avg_loss:0.071, val_acc:0.951]
Epoch [96/120    avg_loss:0.052, val_acc:0.953]
Epoch [97/120    avg_loss:0.055, val_acc:0.955]
Epoch [98/120    avg_loss:0.042, val_acc:0.955]
Epoch [99/120    avg_loss:0.041, val_acc:0.954]
Epoch [100/120    avg_loss:0.042, val_acc:0.955]
Epoch [101/120    avg_loss:0.039, val_acc:0.958]
Epoch [102/120    avg_loss:0.041, val_acc:0.954]
Epoch [103/120    avg_loss:0.038, val_acc:0.957]
Epoch [104/120    avg_loss:0.032, val_acc:0.958]
Epoch [105/120    avg_loss:0.037, val_acc:0.955]
Epoch [106/120    avg_loss:0.036, val_acc:0.959]
Epoch [107/120    avg_loss:0.036, val_acc:0.960]
Epoch [108/120    avg_loss:0.035, val_acc:0.957]
Epoch [109/120    avg_loss:0.037, val_acc:0.960]
Epoch [110/120    avg_loss:0.032, val_acc:0.958]
Epoch [111/120    avg_loss:0.030, val_acc:0.959]
Epoch [112/120    avg_loss:0.039, val_acc:0.958]
Epoch [113/120    avg_loss:0.037, val_acc:0.955]
Epoch [114/120    avg_loss:0.031, val_acc:0.960]
Epoch [115/120    avg_loss:0.031, val_acc:0.960]
Epoch [116/120    avg_loss:0.026, val_acc:0.958]
Epoch [117/120    avg_loss:0.034, val_acc:0.960]
Epoch [118/120    avg_loss:0.033, val_acc:0.959]
Epoch [119/120    avg_loss:0.032, val_acc:0.957]
Epoch [120/120    avg_loss:0.030, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1214    0    0    0    5    0    0    5   12   31    5    0
     0   13    0]
 [   0    0    5  700    0   12    0    0    0   19    0    2    7    2
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  427    1    5    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    4    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    1]
 [   0    0    0    1    0    0    4    0    0   11    0    0    2    0
     0    0    0]
 [   0    0   33   68    0    4    0    0    0    0  756    1    0    0
     0   13    0]
 [   0    0   16    0    0    2    6    0    0    0   21 2153    8    3
     1    0    0]
 [   0    0    1    0    8    5    0    0    0    0   13   11  490    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    0    0    3    1    0    0
  1125    0    0]
 [   0    0    0    0    0    0   22    0    0    1    0    0    0    0
    46  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.15447154471545

F1 scores:
[       nan 0.96202532 0.95066562 0.92348285 0.97921478 0.95418994
 0.96807721 0.90909091 0.99883586 0.4        0.89839572 0.9755324
 0.9351145  0.98666667 0.97318339 0.85407066 0.95402299]

Kappa:
0.9447769182081954
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f46c55a2b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 44808==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.828, val_acc:0.164]
Epoch [2/120    avg_loss:2.751, val_acc:0.445]
Epoch [3/120    avg_loss:2.673, val_acc:0.522]
Epoch [4/120    avg_loss:2.590, val_acc:0.531]
Epoch [5/120    avg_loss:2.500, val_acc:0.532]
Epoch [6/120    avg_loss:2.435, val_acc:0.518]
Epoch [7/120    avg_loss:2.343, val_acc:0.517]
Epoch [8/120    avg_loss:2.301, val_acc:0.522]
Epoch [9/120    avg_loss:2.210, val_acc:0.526]
Epoch [10/120    avg_loss:2.126, val_acc:0.527]
Epoch [11/120    avg_loss:2.100, val_acc:0.531]
Epoch [12/120    avg_loss:2.047, val_acc:0.559]
Epoch [13/120    avg_loss:1.977, val_acc:0.516]
Epoch [14/120    avg_loss:1.946, val_acc:0.558]
Epoch [15/120    avg_loss:1.901, val_acc:0.624]
Epoch [16/120    avg_loss:1.797, val_acc:0.635]
Epoch [17/120    avg_loss:1.759, val_acc:0.649]
Epoch [18/120    avg_loss:1.676, val_acc:0.669]
Epoch [19/120    avg_loss:1.579, val_acc:0.688]
Epoch [20/120    avg_loss:1.555, val_acc:0.689]
Epoch [21/120    avg_loss:1.483, val_acc:0.735]
Epoch [22/120    avg_loss:1.402, val_acc:0.677]
Epoch [23/120    avg_loss:1.357, val_acc:0.717]
Epoch [24/120    avg_loss:1.199, val_acc:0.744]
Epoch [25/120    avg_loss:1.143, val_acc:0.757]
Epoch [26/120    avg_loss:1.072, val_acc:0.759]
Epoch [27/120    avg_loss:0.939, val_acc:0.778]
Epoch [28/120    avg_loss:0.886, val_acc:0.793]
Epoch [29/120    avg_loss:0.786, val_acc:0.819]
Epoch [30/120    avg_loss:0.745, val_acc:0.818]
Epoch [31/120    avg_loss:0.693, val_acc:0.816]
Epoch [32/120    avg_loss:0.690, val_acc:0.794]
Epoch [33/120    avg_loss:0.653, val_acc:0.839]
Epoch [34/120    avg_loss:0.584, val_acc:0.849]
Epoch [35/120    avg_loss:0.578, val_acc:0.810]
Epoch [36/120    avg_loss:0.541, val_acc:0.864]
Epoch [37/120    avg_loss:0.434, val_acc:0.893]
Epoch [38/120    avg_loss:0.409, val_acc:0.886]
Epoch [39/120    avg_loss:0.365, val_acc:0.868]
Epoch [40/120    avg_loss:0.354, val_acc:0.883]
Epoch [41/120    avg_loss:0.340, val_acc:0.915]
Epoch [42/120    avg_loss:0.303, val_acc:0.899]
Epoch [43/120    avg_loss:0.300, val_acc:0.898]
Epoch [44/120    avg_loss:0.300, val_acc:0.916]
Epoch [45/120    avg_loss:0.281, val_acc:0.920]
Epoch [46/120    avg_loss:0.250, val_acc:0.911]
Epoch [47/120    avg_loss:0.258, val_acc:0.907]
Epoch [48/120    avg_loss:0.241, val_acc:0.932]
Epoch [49/120    avg_loss:0.242, val_acc:0.919]
Epoch [50/120    avg_loss:0.250, val_acc:0.915]
Epoch [51/120    avg_loss:0.228, val_acc:0.900]
Epoch [52/120    avg_loss:0.241, val_acc:0.918]
Epoch [53/120    avg_loss:0.219, val_acc:0.912]
Epoch [54/120    avg_loss:0.194, val_acc:0.939]
Epoch [55/120    avg_loss:0.173, val_acc:0.928]
Epoch [56/120    avg_loss:0.167, val_acc:0.926]
Epoch [57/120    avg_loss:0.162, val_acc:0.936]
Epoch [58/120    avg_loss:0.151, val_acc:0.932]
Epoch [59/120    avg_loss:0.143, val_acc:0.945]
Epoch [60/120    avg_loss:0.166, val_acc:0.927]
Epoch [61/120    avg_loss:0.146, val_acc:0.933]
Epoch [62/120    avg_loss:0.123, val_acc:0.939]
Epoch [63/120    avg_loss:0.113, val_acc:0.940]
Epoch [64/120    avg_loss:0.141, val_acc:0.923]
Epoch [65/120    avg_loss:0.189, val_acc:0.918]
Epoch [66/120    avg_loss:0.217, val_acc:0.901]
Epoch [67/120    avg_loss:0.194, val_acc:0.925]
Epoch [68/120    avg_loss:0.139, val_acc:0.943]
Epoch [69/120    avg_loss:0.113, val_acc:0.944]
Epoch [70/120    avg_loss:0.097, val_acc:0.959]
Epoch [71/120    avg_loss:0.100, val_acc:0.934]
Epoch [72/120    avg_loss:0.132, val_acc:0.955]
Epoch [73/120    avg_loss:0.109, val_acc:0.928]
Epoch [74/120    avg_loss:0.102, val_acc:0.950]
Epoch [75/120    avg_loss:0.082, val_acc:0.964]
Epoch [76/120    avg_loss:0.065, val_acc:0.956]
Epoch [77/120    avg_loss:0.062, val_acc:0.961]
Epoch [78/120    avg_loss:0.061, val_acc:0.964]
Epoch [79/120    avg_loss:0.081, val_acc:0.952]
Epoch [80/120    avg_loss:0.109, val_acc:0.935]
Epoch [81/120    avg_loss:0.097, val_acc:0.959]
Epoch [82/120    avg_loss:0.098, val_acc:0.945]
Epoch [83/120    avg_loss:0.096, val_acc:0.957]
Epoch [84/120    avg_loss:0.057, val_acc:0.966]
Epoch [85/120    avg_loss:0.069, val_acc:0.960]
Epoch [86/120    avg_loss:0.074, val_acc:0.968]
Epoch [87/120    avg_loss:0.063, val_acc:0.959]
Epoch [88/120    avg_loss:0.048, val_acc:0.965]
Epoch [89/120    avg_loss:0.057, val_acc:0.951]
Epoch [90/120    avg_loss:0.065, val_acc:0.970]
Epoch [91/120    avg_loss:0.078, val_acc:0.947]
Epoch [92/120    avg_loss:0.096, val_acc:0.959]
Epoch [93/120    avg_loss:0.066, val_acc:0.960]
Epoch [94/120    avg_loss:0.053, val_acc:0.953]
Epoch [95/120    avg_loss:0.043, val_acc:0.970]
Epoch [96/120    avg_loss:0.100, val_acc:0.953]
Epoch [97/120    avg_loss:0.134, val_acc:0.919]
Epoch [98/120    avg_loss:0.109, val_acc:0.945]
Epoch [99/120    avg_loss:0.082, val_acc:0.952]
Epoch [100/120    avg_loss:0.071, val_acc:0.957]
Epoch [101/120    avg_loss:0.057, val_acc:0.960]
Epoch [102/120    avg_loss:0.044, val_acc:0.961]
Epoch [103/120    avg_loss:0.045, val_acc:0.961]
Epoch [104/120    avg_loss:0.042, val_acc:0.964]
Epoch [105/120    avg_loss:0.031, val_acc:0.970]
Epoch [106/120    avg_loss:0.035, val_acc:0.968]
Epoch [107/120    avg_loss:0.037, val_acc:0.961]
Epoch [108/120    avg_loss:0.035, val_acc:0.975]
Epoch [109/120    avg_loss:0.037, val_acc:0.957]
Epoch [110/120    avg_loss:0.064, val_acc:0.951]
Epoch [111/120    avg_loss:0.058, val_acc:0.967]
Epoch [112/120    avg_loss:0.060, val_acc:0.968]
Epoch [113/120    avg_loss:0.050, val_acc:0.966]
Epoch [114/120    avg_loss:0.032, val_acc:0.967]
Epoch [115/120    avg_loss:0.039, val_acc:0.974]
Epoch [116/120    avg_loss:0.032, val_acc:0.968]
Epoch [117/120    avg_loss:0.028, val_acc:0.966]
Epoch [118/120    avg_loss:0.032, val_acc:0.969]
Epoch [119/120    avg_loss:0.033, val_acc:0.967]
Epoch [120/120    avg_loss:0.030, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1214    8    0    0    4    0    0    3    3   47    5    0
     0    1    0]
 [   0    0    2  699    2   15   10    0    0    9    0    0   10    0
     0    0    0]
 [   0    0    0    0  212    0    1    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    3    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   22   51    0    0    0    0    0    0  767   29    0    0
     1    5    0]
 [   0    0    8    0    0    0    6    0    0    0   28 2167    0    1
     0    0    0]
 [   0    0    0   18    2    0    0    0    0    0    5    4  498    0
     0    1    6]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0  182
     0    0    0]
 [   0    1    0    0    0    0    0    0    1    0    3    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0   46    0    0    0    0    0    0    0
    80  221    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
95.03523035230353

F1 scores:
[       nan 0.95       0.95930462 0.91792515 0.98834499 0.97278912
 0.95072464 0.94339623 0.995338   0.68       0.91092637 0.97218484
 0.94497154 0.98913043 0.96346644 0.76869565 0.94736842]

Kappa:
0.9433333022620206
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f52f608fb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.818, val_acc:0.239]
Epoch [2/120    avg_loss:2.748, val_acc:0.316]
Epoch [3/120    avg_loss:2.681, val_acc:0.389]
Epoch [4/120    avg_loss:2.607, val_acc:0.442]
Epoch [5/120    avg_loss:2.533, val_acc:0.473]
Epoch [6/120    avg_loss:2.475, val_acc:0.483]
Epoch [7/120    avg_loss:2.363, val_acc:0.480]
Epoch [8/120    avg_loss:2.309, val_acc:0.477]
Epoch [9/120    avg_loss:2.245, val_acc:0.470]
Epoch [10/120    avg_loss:2.211, val_acc:0.457]
Epoch [11/120    avg_loss:2.153, val_acc:0.515]
Epoch [12/120    avg_loss:2.095, val_acc:0.532]
Epoch [13/120    avg_loss:2.063, val_acc:0.588]
Epoch [14/120    avg_loss:2.015, val_acc:0.559]
Epoch [15/120    avg_loss:1.983, val_acc:0.601]
Epoch [16/120    avg_loss:1.920, val_acc:0.613]
Epoch [17/120    avg_loss:1.883, val_acc:0.640]
Epoch [18/120    avg_loss:1.831, val_acc:0.614]
Epoch [19/120    avg_loss:1.761, val_acc:0.631]
Epoch [20/120    avg_loss:1.753, val_acc:0.651]
Epoch [21/120    avg_loss:1.685, val_acc:0.664]
Epoch [22/120    avg_loss:1.624, val_acc:0.666]
Epoch [23/120    avg_loss:1.558, val_acc:0.649]
Epoch [24/120    avg_loss:1.483, val_acc:0.688]
Epoch [25/120    avg_loss:1.427, val_acc:0.726]
Epoch [26/120    avg_loss:1.334, val_acc:0.702]
Epoch [27/120    avg_loss:1.264, val_acc:0.713]
Epoch [28/120    avg_loss:1.218, val_acc:0.747]
Epoch [29/120    avg_loss:1.181, val_acc:0.743]
Epoch [30/120    avg_loss:1.119, val_acc:0.757]
Epoch [31/120    avg_loss:1.016, val_acc:0.759]
Epoch [32/120    avg_loss:1.016, val_acc:0.765]
Epoch [33/120    avg_loss:0.909, val_acc:0.784]
Epoch [34/120    avg_loss:0.849, val_acc:0.795]
Epoch [35/120    avg_loss:0.830, val_acc:0.802]
Epoch [36/120    avg_loss:0.768, val_acc:0.787]
Epoch [37/120    avg_loss:0.699, val_acc:0.805]
Epoch [38/120    avg_loss:0.612, val_acc:0.826]
Epoch [39/120    avg_loss:0.600, val_acc:0.831]
Epoch [40/120    avg_loss:0.542, val_acc:0.827]
Epoch [41/120    avg_loss:0.481, val_acc:0.827]
Epoch [42/120    avg_loss:0.502, val_acc:0.851]
Epoch [43/120    avg_loss:0.509, val_acc:0.839]
Epoch [44/120    avg_loss:0.481, val_acc:0.856]
Epoch [45/120    avg_loss:0.398, val_acc:0.866]
Epoch [46/120    avg_loss:0.490, val_acc:0.808]
Epoch [47/120    avg_loss:0.502, val_acc:0.842]
Epoch [48/120    avg_loss:0.468, val_acc:0.868]
Epoch [49/120    avg_loss:0.399, val_acc:0.872]
Epoch [50/120    avg_loss:0.348, val_acc:0.873]
Epoch [51/120    avg_loss:0.379, val_acc:0.882]
Epoch [52/120    avg_loss:0.325, val_acc:0.877]
Epoch [53/120    avg_loss:0.270, val_acc:0.901]
Epoch [54/120    avg_loss:0.288, val_acc:0.890]
Epoch [55/120    avg_loss:0.325, val_acc:0.883]
Epoch [56/120    avg_loss:0.279, val_acc:0.905]
Epoch [57/120    avg_loss:0.258, val_acc:0.910]
Epoch [58/120    avg_loss:0.226, val_acc:0.909]
Epoch [59/120    avg_loss:0.222, val_acc:0.903]
Epoch [60/120    avg_loss:0.212, val_acc:0.919]
Epoch [61/120    avg_loss:0.210, val_acc:0.915]
Epoch [62/120    avg_loss:0.208, val_acc:0.878]
Epoch [63/120    avg_loss:0.188, val_acc:0.927]
Epoch [64/120    avg_loss:0.183, val_acc:0.918]
Epoch [65/120    avg_loss:0.172, val_acc:0.918]
Epoch [66/120    avg_loss:0.163, val_acc:0.928]
Epoch [67/120    avg_loss:0.157, val_acc:0.923]
Epoch [68/120    avg_loss:0.148, val_acc:0.919]
Epoch [69/120    avg_loss:0.179, val_acc:0.924]
Epoch [70/120    avg_loss:0.160, val_acc:0.925]
Epoch [71/120    avg_loss:0.139, val_acc:0.927]
Epoch [72/120    avg_loss:0.117, val_acc:0.935]
Epoch [73/120    avg_loss:0.148, val_acc:0.925]
Epoch [74/120    avg_loss:0.130, val_acc:0.926]
Epoch [75/120    avg_loss:0.102, val_acc:0.944]
Epoch [76/120    avg_loss:0.118, val_acc:0.916]
Epoch [77/120    avg_loss:0.127, val_acc:0.932]
Epoch [78/120    avg_loss:0.125, val_acc:0.935]
Epoch [79/120    avg_loss:0.124, val_acc:0.930]
Epoch [80/120    avg_loss:0.138, val_acc:0.926]
Epoch [81/120    avg_loss:0.108, val_acc:0.944]
Epoch [82/120    avg_loss:0.092, val_acc:0.945]
Epoch [83/120    avg_loss:0.085, val_acc:0.935]
Epoch [84/120    avg_loss:0.102, val_acc:0.950]
Epoch [85/120    avg_loss:0.089, val_acc:0.939]
Epoch [86/120    avg_loss:0.081, val_acc:0.949]
Epoch [87/120    avg_loss:0.086, val_acc:0.944]
Epoch [88/120    avg_loss:0.081, val_acc:0.944]
Epoch [89/120    avg_loss:0.062, val_acc:0.948]
Epoch [90/120    avg_loss:0.064, val_acc:0.956]
Epoch [91/120    avg_loss:0.060, val_acc:0.948]
Epoch [92/120    avg_loss:0.068, val_acc:0.947]
Epoch [93/120    avg_loss:0.085, val_acc:0.953]
Epoch [94/120    avg_loss:0.075, val_acc:0.944]
Epoch [95/120    avg_loss:0.090, val_acc:0.944]
Epoch [96/120    avg_loss:0.093, val_acc:0.932]
Epoch [97/120    avg_loss:0.087, val_acc:0.939]
Epoch [98/120    avg_loss:0.072, val_acc:0.947]
Epoch [99/120    avg_loss:0.061, val_acc:0.947]
Epoch [100/120    avg_loss:0.063, val_acc:0.953]
Epoch [101/120    avg_loss:0.058, val_acc:0.938]
Epoch [102/120    avg_loss:0.060, val_acc:0.950]
Epoch [103/120    avg_loss:0.046, val_acc:0.956]
Epoch [104/120    avg_loss:0.037, val_acc:0.943]
Epoch [105/120    avg_loss:0.049, val_acc:0.955]
Epoch [106/120    avg_loss:0.041, val_acc:0.941]
Epoch [107/120    avg_loss:0.044, val_acc:0.945]
Epoch [108/120    avg_loss:0.044, val_acc:0.952]
Epoch [109/120    avg_loss:0.037, val_acc:0.951]
Epoch [110/120    avg_loss:0.046, val_acc:0.960]
Epoch [111/120    avg_loss:0.051, val_acc:0.957]
Epoch [112/120    avg_loss:0.058, val_acc:0.942]
Epoch [113/120    avg_loss:0.065, val_acc:0.951]
Epoch [114/120    avg_loss:0.058, val_acc:0.953]
Epoch [115/120    avg_loss:0.042, val_acc:0.958]
Epoch [116/120    avg_loss:0.044, val_acc:0.960]
Epoch [117/120    avg_loss:0.049, val_acc:0.941]
Epoch [118/120    avg_loss:0.082, val_acc:0.952]
Epoch [119/120    avg_loss:0.058, val_acc:0.955]
Epoch [120/120    avg_loss:0.037, val_acc:0.959]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1227    1    4    0    0    0    0    0    7   25    3    0
     0   18    0]
 [   0    0    2  699    1   18    0    0    0    9    0    0   16    0
     2    0    0]
 [   0    0    0    2  209    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  402    0    0    0    7    0    0    0    0
    26    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    4    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    8    0    0    0    0    0    9    0    0    1    0
     0    0    0]
 [   0    0   66   89    0    7    0    0    0    0  706    2    0    0
     1    4    0]
 [   0    0   13    0    0    5   10    0    1    0    3 2161    6    4
     7    0    0]
 [   0    0    1    3    3   10    0    0    0    0   14   13  482    0
     0    1    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    1    0    0
  1137    0    0]
 [   0    0    0    0    0    0   48    0    0    1    0    0    0    0
   101  197    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
93.66937669376694

F1 scores:
[       nan 0.96202532 0.9460293  0.90251775 0.97209302 0.91676169
 0.95314788 1.         0.99883856 0.40909091 0.8775637  0.97871377
 0.92248804 0.98930481 0.94161491 0.69488536 0.95402299]

Kappa:
0.9277485252781187
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7eff4964cac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.846, val_acc:0.016]
Epoch [2/120    avg_loss:2.754, val_acc:0.186]
Epoch [3/120    avg_loss:2.664, val_acc:0.433]
Epoch [4/120    avg_loss:2.578, val_acc:0.464]
Epoch [5/120    avg_loss:2.497, val_acc:0.524]
Epoch [6/120    avg_loss:2.428, val_acc:0.565]
Epoch [7/120    avg_loss:2.369, val_acc:0.562]
Epoch [8/120    avg_loss:2.331, val_acc:0.570]
Epoch [9/120    avg_loss:2.273, val_acc:0.590]
Epoch [10/120    avg_loss:2.203, val_acc:0.592]
Epoch [11/120    avg_loss:2.177, val_acc:0.607]
Epoch [12/120    avg_loss:2.096, val_acc:0.609]
Epoch [13/120    avg_loss:2.055, val_acc:0.609]
Epoch [14/120    avg_loss:1.962, val_acc:0.635]
Epoch [15/120    avg_loss:1.949, val_acc:0.644]
Epoch [16/120    avg_loss:1.885, val_acc:0.639]
Epoch [17/120    avg_loss:1.846, val_acc:0.648]
Epoch [18/120    avg_loss:1.798, val_acc:0.643]
Epoch [19/120    avg_loss:1.737, val_acc:0.664]
Epoch [20/120    avg_loss:1.664, val_acc:0.683]
Epoch [21/120    avg_loss:1.640, val_acc:0.664]
Epoch [22/120    avg_loss:1.527, val_acc:0.681]
Epoch [23/120    avg_loss:1.483, val_acc:0.689]
Epoch [24/120    avg_loss:1.461, val_acc:0.700]
Epoch [25/120    avg_loss:1.394, val_acc:0.710]
Epoch [26/120    avg_loss:1.355, val_acc:0.726]
Epoch [27/120    avg_loss:1.310, val_acc:0.724]
Epoch [28/120    avg_loss:1.260, val_acc:0.743]
Epoch [29/120    avg_loss:1.159, val_acc:0.769]
Epoch [30/120    avg_loss:1.091, val_acc:0.766]
Epoch [31/120    avg_loss:1.059, val_acc:0.753]
Epoch [32/120    avg_loss:0.949, val_acc:0.768]
Epoch [33/120    avg_loss:0.905, val_acc:0.803]
Epoch [34/120    avg_loss:0.814, val_acc:0.828]
Epoch [35/120    avg_loss:0.834, val_acc:0.793]
Epoch [36/120    avg_loss:0.805, val_acc:0.822]
Epoch [37/120    avg_loss:0.733, val_acc:0.787]
Epoch [38/120    avg_loss:0.699, val_acc:0.802]
Epoch [39/120    avg_loss:0.642, val_acc:0.827]
Epoch [40/120    avg_loss:0.613, val_acc:0.822]
Epoch [41/120    avg_loss:0.541, val_acc:0.860]
Epoch [42/120    avg_loss:0.530, val_acc:0.851]
Epoch [43/120    avg_loss:0.465, val_acc:0.857]
Epoch [44/120    avg_loss:0.467, val_acc:0.876]
Epoch [45/120    avg_loss:0.438, val_acc:0.865]
Epoch [46/120    avg_loss:0.398, val_acc:0.864]
Epoch [47/120    avg_loss:0.395, val_acc:0.897]
Epoch [48/120    avg_loss:0.351, val_acc:0.886]
Epoch [49/120    avg_loss:0.336, val_acc:0.884]
Epoch [50/120    avg_loss:0.319, val_acc:0.897]
Epoch [51/120    avg_loss:0.340, val_acc:0.881]
Epoch [52/120    avg_loss:0.291, val_acc:0.908]
Epoch [53/120    avg_loss:0.306, val_acc:0.895]
Epoch [54/120    avg_loss:0.247, val_acc:0.919]
Epoch [55/120    avg_loss:0.251, val_acc:0.928]
Epoch [56/120    avg_loss:0.239, val_acc:0.914]
Epoch [57/120    avg_loss:0.237, val_acc:0.919]
Epoch [58/120    avg_loss:0.215, val_acc:0.930]
Epoch [59/120    avg_loss:0.245, val_acc:0.924]
Epoch [60/120    avg_loss:0.241, val_acc:0.912]
Epoch [61/120    avg_loss:0.213, val_acc:0.889]
Epoch [62/120    avg_loss:0.222, val_acc:0.919]
Epoch [63/120    avg_loss:0.218, val_acc:0.906]
Epoch [64/120    avg_loss:0.217, val_acc:0.910]
Epoch [65/120    avg_loss:0.197, val_acc:0.936]
Epoch [66/120    avg_loss:0.199, val_acc:0.920]
Epoch [67/120    avg_loss:0.149, val_acc:0.939]
Epoch [68/120    avg_loss:0.150, val_acc:0.947]
Epoch [69/120    avg_loss:0.147, val_acc:0.947]
Epoch [70/120    avg_loss:0.144, val_acc:0.931]
Epoch [71/120    avg_loss:0.151, val_acc:0.941]
Epoch [72/120    avg_loss:0.182, val_acc:0.917]
Epoch [73/120    avg_loss:0.151, val_acc:0.948]
Epoch [74/120    avg_loss:0.123, val_acc:0.938]
Epoch [75/120    avg_loss:0.149, val_acc:0.952]
Epoch [76/120    avg_loss:0.129, val_acc:0.924]
Epoch [77/120    avg_loss:0.126, val_acc:0.945]
Epoch [78/120    avg_loss:0.154, val_acc:0.948]
Epoch [79/120    avg_loss:0.134, val_acc:0.951]
Epoch [80/120    avg_loss:0.121, val_acc:0.927]
Epoch [81/120    avg_loss:0.138, val_acc:0.944]
Epoch [82/120    avg_loss:0.117, val_acc:0.956]
Epoch [83/120    avg_loss:0.083, val_acc:0.959]
Epoch [84/120    avg_loss:0.090, val_acc:0.951]
Epoch [85/120    avg_loss:0.088, val_acc:0.953]
Epoch [86/120    avg_loss:0.080, val_acc:0.942]
Epoch [87/120    avg_loss:0.098, val_acc:0.951]
Epoch [88/120    avg_loss:0.085, val_acc:0.951]
Epoch [89/120    avg_loss:0.089, val_acc:0.949]
Epoch [90/120    avg_loss:0.117, val_acc:0.952]
Epoch [91/120    avg_loss:0.093, val_acc:0.945]
Epoch [92/120    avg_loss:0.093, val_acc:0.945]
Epoch [93/120    avg_loss:0.071, val_acc:0.944]
Epoch [94/120    avg_loss:0.086, val_acc:0.943]
Epoch [95/120    avg_loss:0.108, val_acc:0.956]
Epoch [96/120    avg_loss:0.098, val_acc:0.947]
Epoch [97/120    avg_loss:0.101, val_acc:0.958]
Epoch [98/120    avg_loss:0.064, val_acc:0.966]
Epoch [99/120    avg_loss:0.058, val_acc:0.966]
Epoch [100/120    avg_loss:0.056, val_acc:0.963]
Epoch [101/120    avg_loss:0.056, val_acc:0.964]
Epoch [102/120    avg_loss:0.052, val_acc:0.961]
Epoch [103/120    avg_loss:0.046, val_acc:0.959]
Epoch [104/120    avg_loss:0.043, val_acc:0.960]
Epoch [105/120    avg_loss:0.049, val_acc:0.964]
Epoch [106/120    avg_loss:0.039, val_acc:0.964]
Epoch [107/120    avg_loss:0.049, val_acc:0.964]
Epoch [108/120    avg_loss:0.049, val_acc:0.966]
Epoch [109/120    avg_loss:0.041, val_acc:0.965]
Epoch [110/120    avg_loss:0.039, val_acc:0.965]
Epoch [111/120    avg_loss:0.046, val_acc:0.967]
Epoch [112/120    avg_loss:0.048, val_acc:0.965]
Epoch [113/120    avg_loss:0.047, val_acc:0.965]
Epoch [114/120    avg_loss:0.038, val_acc:0.965]
Epoch [115/120    avg_loss:0.037, val_acc:0.966]
Epoch [116/120    avg_loss:0.054, val_acc:0.964]
Epoch [117/120    avg_loss:0.044, val_acc:0.964]
Epoch [118/120    avg_loss:0.042, val_acc:0.961]
Epoch [119/120    avg_loss:0.036, val_acc:0.965]
Epoch [120/120    avg_loss:0.038, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    3    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1200    1    0    0    7    0    0    2    6   52    1    0
     0   16    0]
 [   0    0    6  715    1   12    0    0    0   12    0    0    0    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    2    0    3    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   14    0    2    0    0
     0    0    0]
 [   0    0   39   87    0    6    1    0    0    0  722   14    0    2
     2    2    0]
 [   0    0   18    0    0    4   12    0    0    0    5 2159    4    6
     2    0    0]
 [   0    0    0    2   10   13    0    0    0    0    4    3  493    0
     0    1    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    3    2    0    0
  1132    0    0]
 [   0    0    0    0    0    0   58    0    0    0    0    0    0    0
    93  196    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.12466124661246

F1 scores:
[       nan 0.96202532 0.94080753 0.92020592 0.97482838 0.94866071
 0.94168467 0.96153846 0.99883856 0.57142857 0.89411765 0.97142857
 0.95357834 0.9762533  0.95406658 0.6975089  0.94252874]

Kappa:
0.9329552085494013
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe3a0baba90>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.821, val_acc:0.220]
Epoch [2/120    avg_loss:2.733, val_acc:0.306]
Epoch [3/120    avg_loss:2.642, val_acc:0.361]
Epoch [4/120    avg_loss:2.544, val_acc:0.388]
Epoch [5/120    avg_loss:2.486, val_acc:0.406]
Epoch [6/120    avg_loss:2.415, val_acc:0.401]
Epoch [7/120    avg_loss:2.361, val_acc:0.390]
Epoch [8/120    avg_loss:2.313, val_acc:0.407]
Epoch [9/120    avg_loss:2.268, val_acc:0.448]
Epoch [10/120    avg_loss:2.201, val_acc:0.492]
Epoch [11/120    avg_loss:2.147, val_acc:0.524]
Epoch [12/120    avg_loss:2.108, val_acc:0.569]
Epoch [13/120    avg_loss:2.036, val_acc:0.578]
Epoch [14/120    avg_loss:2.042, val_acc:0.581]
Epoch [15/120    avg_loss:1.951, val_acc:0.608]
Epoch [16/120    avg_loss:1.886, val_acc:0.626]
Epoch [17/120    avg_loss:1.814, val_acc:0.633]
Epoch [18/120    avg_loss:1.738, val_acc:0.647]
Epoch [19/120    avg_loss:1.666, val_acc:0.639]
Epoch [20/120    avg_loss:1.596, val_acc:0.676]
Epoch [21/120    avg_loss:1.524, val_acc:0.701]
Epoch [22/120    avg_loss:1.431, val_acc:0.685]
Epoch [23/120    avg_loss:1.364, val_acc:0.685]
Epoch [24/120    avg_loss:1.254, val_acc:0.715]
Epoch [25/120    avg_loss:1.174, val_acc:0.742]
Epoch [26/120    avg_loss:1.096, val_acc:0.770]
Epoch [27/120    avg_loss:1.009, val_acc:0.752]
Epoch [28/120    avg_loss:0.922, val_acc:0.769]
Epoch [29/120    avg_loss:0.934, val_acc:0.758]
Epoch [30/120    avg_loss:0.864, val_acc:0.761]
Epoch [31/120    avg_loss:0.805, val_acc:0.768]
Epoch [32/120    avg_loss:0.753, val_acc:0.790]
Epoch [33/120    avg_loss:0.716, val_acc:0.770]
Epoch [34/120    avg_loss:0.718, val_acc:0.819]
Epoch [35/120    avg_loss:0.619, val_acc:0.833]
Epoch [36/120    avg_loss:0.645, val_acc:0.823]
Epoch [37/120    avg_loss:0.557, val_acc:0.852]
Epoch [38/120    avg_loss:0.511, val_acc:0.876]
Epoch [39/120    avg_loss:0.473, val_acc:0.867]
Epoch [40/120    avg_loss:0.474, val_acc:0.883]
Epoch [41/120    avg_loss:0.404, val_acc:0.876]
Epoch [42/120    avg_loss:0.417, val_acc:0.845]
Epoch [43/120    avg_loss:0.449, val_acc:0.867]
Epoch [44/120    avg_loss:0.424, val_acc:0.891]
Epoch [45/120    avg_loss:0.356, val_acc:0.894]
Epoch [46/120    avg_loss:0.305, val_acc:0.916]
Epoch [47/120    avg_loss:0.289, val_acc:0.897]
Epoch [48/120    avg_loss:0.305, val_acc:0.894]
Epoch [49/120    avg_loss:0.289, val_acc:0.880]
Epoch [50/120    avg_loss:0.300, val_acc:0.914]
Epoch [51/120    avg_loss:0.311, val_acc:0.922]
Epoch [52/120    avg_loss:0.265, val_acc:0.914]
Epoch [53/120    avg_loss:0.268, val_acc:0.918]
Epoch [54/120    avg_loss:0.214, val_acc:0.920]
Epoch [55/120    avg_loss:0.200, val_acc:0.930]
Epoch [56/120    avg_loss:0.180, val_acc:0.924]
Epoch [57/120    avg_loss:0.172, val_acc:0.914]
Epoch [58/120    avg_loss:0.199, val_acc:0.926]
Epoch [59/120    avg_loss:0.151, val_acc:0.936]
Epoch [60/120    avg_loss:0.164, val_acc:0.908]
Epoch [61/120    avg_loss:0.191, val_acc:0.928]
Epoch [62/120    avg_loss:0.173, val_acc:0.944]
Epoch [63/120    avg_loss:0.140, val_acc:0.938]
Epoch [64/120    avg_loss:0.155, val_acc:0.953]
Epoch [65/120    avg_loss:0.127, val_acc:0.924]
Epoch [66/120    avg_loss:0.109, val_acc:0.933]
Epoch [67/120    avg_loss:0.126, val_acc:0.949]
Epoch [68/120    avg_loss:0.115, val_acc:0.934]
Epoch [69/120    avg_loss:0.148, val_acc:0.950]
Epoch [70/120    avg_loss:0.130, val_acc:0.952]
Epoch [71/120    avg_loss:0.105, val_acc:0.930]
Epoch [72/120    avg_loss:0.110, val_acc:0.948]
Epoch [73/120    avg_loss:0.118, val_acc:0.942]
Epoch [74/120    avg_loss:0.122, val_acc:0.939]
Epoch [75/120    avg_loss:0.140, val_acc:0.923]
Epoch [76/120    avg_loss:0.117, val_acc:0.947]
Epoch [77/120    avg_loss:0.127, val_acc:0.939]
Epoch [78/120    avg_loss:0.092, val_acc:0.953]
Epoch [79/120    avg_loss:0.084, val_acc:0.957]
Epoch [80/120    avg_loss:0.078, val_acc:0.958]
Epoch [81/120    avg_loss:0.073, val_acc:0.958]
Epoch [82/120    avg_loss:0.067, val_acc:0.959]
Epoch [83/120    avg_loss:0.065, val_acc:0.957]
Epoch [84/120    avg_loss:0.063, val_acc:0.956]
Epoch [85/120    avg_loss:0.074, val_acc:0.961]
Epoch [86/120    avg_loss:0.060, val_acc:0.960]
Epoch [87/120    avg_loss:0.065, val_acc:0.959]
Epoch [88/120    avg_loss:0.055, val_acc:0.965]
Epoch [89/120    avg_loss:0.058, val_acc:0.964]
Epoch [90/120    avg_loss:0.054, val_acc:0.961]
Epoch [91/120    avg_loss:0.056, val_acc:0.965]
Epoch [92/120    avg_loss:0.062, val_acc:0.963]
Epoch [93/120    avg_loss:0.065, val_acc:0.963]
Epoch [94/120    avg_loss:0.066, val_acc:0.964]
Epoch [95/120    avg_loss:0.060, val_acc:0.964]
Epoch [96/120    avg_loss:0.051, val_acc:0.964]
Epoch [97/120    avg_loss:0.054, val_acc:0.967]
Epoch [98/120    avg_loss:0.051, val_acc:0.965]
Epoch [99/120    avg_loss:0.059, val_acc:0.965]
Epoch [100/120    avg_loss:0.051, val_acc:0.965]
Epoch [101/120    avg_loss:0.054, val_acc:0.968]
Epoch [102/120    avg_loss:0.050, val_acc:0.967]
Epoch [103/120    avg_loss:0.060, val_acc:0.965]
Epoch [104/120    avg_loss:0.057, val_acc:0.967]
Epoch [105/120    avg_loss:0.058, val_acc:0.967]
Epoch [106/120    avg_loss:0.050, val_acc:0.963]
Epoch [107/120    avg_loss:0.057, val_acc:0.963]
Epoch [108/120    avg_loss:0.054, val_acc:0.964]
Epoch [109/120    avg_loss:0.045, val_acc:0.966]
Epoch [110/120    avg_loss:0.049, val_acc:0.965]
Epoch [111/120    avg_loss:0.050, val_acc:0.966]
Epoch [112/120    avg_loss:0.049, val_acc:0.967]
Epoch [113/120    avg_loss:0.047, val_acc:0.967]
Epoch [114/120    avg_loss:0.049, val_acc:0.964]
Epoch [115/120    avg_loss:0.047, val_acc:0.964]
Epoch [116/120    avg_loss:0.048, val_acc:0.964]
Epoch [117/120    avg_loss:0.047, val_acc:0.964]
Epoch [118/120    avg_loss:0.045, val_acc:0.966]
Epoch [119/120    avg_loss:0.051, val_acc:0.965]
Epoch [120/120    avg_loss:0.048, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1165    5    0    0    9    0    0    5    9   75    3    0
     0   14    0]
 [   0    0    3  711    2   13    0    0    0   13    0    0    4    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  414    0    4    0    1    0    1    0    0
    15    0    0]
 [   0    0    0    0    0    0  650    0    0    2    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    4    0    0   10    0    0    1    0
     0    0    0]
 [   0    0   50   90    0    5    0    0    0    0  717    4    0    0
     0    9    0]
 [   0    0   29    0    0    3   14    0    0    0    4 2152    3    4
     1    0    0]
 [   0    0    0   32    4    9    0    0    0    0    9    2  463    0
     0    0   15]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    3    0    1    0
  1127    0    0]
 [   0    0    0    0    0   17    0    0    0   15    0    0    2    0
   110  203    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
93.07317073170732

F1 scores:
[       nan 0.94871795 0.92022117 0.89546599 0.98611111 0.91694352
 0.97451274 0.92592593 0.99883856 0.3125     0.88463911 0.96827897
 0.91592483 0.98930481 0.94034209 0.70855148 0.91803279]

Kappa:
0.9209658929135597
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4a3d6a1ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.824, val_acc:0.136]
Epoch [2/120    avg_loss:2.731, val_acc:0.180]
Epoch [3/120    avg_loss:2.622, val_acc:0.195]
Epoch [4/120    avg_loss:2.520, val_acc:0.224]
Epoch [5/120    avg_loss:2.470, val_acc:0.258]
Epoch [6/120    avg_loss:2.385, val_acc:0.294]
Epoch [7/120    avg_loss:2.321, val_acc:0.347]
Epoch [8/120    avg_loss:2.263, val_acc:0.374]
Epoch [9/120    avg_loss:2.213, val_acc:0.449]
Epoch [10/120    avg_loss:2.146, val_acc:0.482]
Epoch [11/120    avg_loss:2.109, val_acc:0.526]
Epoch [12/120    avg_loss:2.062, val_acc:0.545]
Epoch [13/120    avg_loss:2.007, val_acc:0.572]
Epoch [14/120    avg_loss:1.934, val_acc:0.555]
Epoch [15/120    avg_loss:1.864, val_acc:0.593]
Epoch [16/120    avg_loss:1.759, val_acc:0.620]
Epoch [17/120    avg_loss:1.683, val_acc:0.637]
Epoch [18/120    avg_loss:1.605, val_acc:0.659]
Epoch [19/120    avg_loss:1.502, val_acc:0.674]
Epoch [20/120    avg_loss:1.445, val_acc:0.669]
Epoch [21/120    avg_loss:1.320, val_acc:0.686]
Epoch [22/120    avg_loss:1.284, val_acc:0.691]
Epoch [23/120    avg_loss:1.215, val_acc:0.722]
Epoch [24/120    avg_loss:1.112, val_acc:0.733]
Epoch [25/120    avg_loss:1.059, val_acc:0.711]
Epoch [26/120    avg_loss:0.997, val_acc:0.741]
Epoch [27/120    avg_loss:0.958, val_acc:0.735]
Epoch [28/120    avg_loss:0.893, val_acc:0.735]
Epoch [29/120    avg_loss:0.885, val_acc:0.744]
Epoch [30/120    avg_loss:0.893, val_acc:0.688]
Epoch [31/120    avg_loss:0.909, val_acc:0.784]
Epoch [32/120    avg_loss:0.792, val_acc:0.757]
Epoch [33/120    avg_loss:0.746, val_acc:0.769]
Epoch [34/120    avg_loss:0.698, val_acc:0.818]
Epoch [35/120    avg_loss:0.638, val_acc:0.799]
Epoch [36/120    avg_loss:0.624, val_acc:0.782]
Epoch [37/120    avg_loss:0.617, val_acc:0.817]
Epoch [38/120    avg_loss:0.607, val_acc:0.774]
Epoch [39/120    avg_loss:0.588, val_acc:0.807]
Epoch [40/120    avg_loss:0.513, val_acc:0.795]
Epoch [41/120    avg_loss:0.536, val_acc:0.807]
Epoch [42/120    avg_loss:0.506, val_acc:0.830]
Epoch [43/120    avg_loss:0.496, val_acc:0.842]
Epoch [44/120    avg_loss:0.395, val_acc:0.863]
Epoch [45/120    avg_loss:0.405, val_acc:0.843]
Epoch [46/120    avg_loss:0.364, val_acc:0.852]
Epoch [47/120    avg_loss:0.354, val_acc:0.882]
Epoch [48/120    avg_loss:0.334, val_acc:0.892]
Epoch [49/120    avg_loss:0.357, val_acc:0.840]
Epoch [50/120    avg_loss:0.346, val_acc:0.843]
Epoch [51/120    avg_loss:0.338, val_acc:0.877]
Epoch [52/120    avg_loss:0.316, val_acc:0.898]
Epoch [53/120    avg_loss:0.277, val_acc:0.893]
Epoch [54/120    avg_loss:0.286, val_acc:0.892]
Epoch [55/120    avg_loss:0.248, val_acc:0.893]
Epoch [56/120    avg_loss:0.256, val_acc:0.881]
Epoch [57/120    avg_loss:0.246, val_acc:0.894]
Epoch [58/120    avg_loss:0.254, val_acc:0.886]
Epoch [59/120    avg_loss:0.288, val_acc:0.892]
Epoch [60/120    avg_loss:0.238, val_acc:0.891]
Epoch [61/120    avg_loss:0.190, val_acc:0.920]
Epoch [62/120    avg_loss:0.165, val_acc:0.920]
Epoch [63/120    avg_loss:0.160, val_acc:0.926]
Epoch [64/120    avg_loss:0.169, val_acc:0.922]
Epoch [65/120    avg_loss:0.195, val_acc:0.920]
Epoch [66/120    avg_loss:0.168, val_acc:0.934]
Epoch [67/120    avg_loss:0.169, val_acc:0.932]
Epoch [68/120    avg_loss:0.130, val_acc:0.933]
Epoch [69/120    avg_loss:0.123, val_acc:0.942]
Epoch [70/120    avg_loss:0.129, val_acc:0.919]
Epoch [71/120    avg_loss:0.155, val_acc:0.941]
Epoch [72/120    avg_loss:0.143, val_acc:0.943]
Epoch [73/120    avg_loss:0.134, val_acc:0.951]
Epoch [74/120    avg_loss:0.156, val_acc:0.919]
Epoch [75/120    avg_loss:0.163, val_acc:0.930]
Epoch [76/120    avg_loss:0.139, val_acc:0.931]
Epoch [77/120    avg_loss:0.114, val_acc:0.945]
Epoch [78/120    avg_loss:0.098, val_acc:0.948]
Epoch [79/120    avg_loss:0.153, val_acc:0.919]
Epoch [80/120    avg_loss:0.419, val_acc:0.851]
Epoch [81/120    avg_loss:0.360, val_acc:0.878]
Epoch [82/120    avg_loss:0.250, val_acc:0.902]
Epoch [83/120    avg_loss:0.195, val_acc:0.910]
Epoch [84/120    avg_loss:0.179, val_acc:0.925]
Epoch [85/120    avg_loss:0.155, val_acc:0.933]
Epoch [86/120    avg_loss:0.133, val_acc:0.932]
Epoch [87/120    avg_loss:0.115, val_acc:0.934]
Epoch [88/120    avg_loss:0.114, val_acc:0.940]
Epoch [89/120    avg_loss:0.100, val_acc:0.945]
Epoch [90/120    avg_loss:0.096, val_acc:0.947]
Epoch [91/120    avg_loss:0.089, val_acc:0.952]
Epoch [92/120    avg_loss:0.086, val_acc:0.950]
Epoch [93/120    avg_loss:0.089, val_acc:0.949]
Epoch [94/120    avg_loss:0.089, val_acc:0.951]
Epoch [95/120    avg_loss:0.092, val_acc:0.953]
Epoch [96/120    avg_loss:0.094, val_acc:0.958]
Epoch [97/120    avg_loss:0.092, val_acc:0.956]
Epoch [98/120    avg_loss:0.093, val_acc:0.955]
Epoch [99/120    avg_loss:0.081, val_acc:0.953]
Epoch [100/120    avg_loss:0.076, val_acc:0.956]
Epoch [101/120    avg_loss:0.089, val_acc:0.953]
Epoch [102/120    avg_loss:0.093, val_acc:0.953]
Epoch [103/120    avg_loss:0.089, val_acc:0.953]
Epoch [104/120    avg_loss:0.081, val_acc:0.955]
Epoch [105/120    avg_loss:0.087, val_acc:0.956]
Epoch [106/120    avg_loss:0.102, val_acc:0.956]
Epoch [107/120    avg_loss:0.089, val_acc:0.953]
Epoch [108/120    avg_loss:0.082, val_acc:0.956]
Epoch [109/120    avg_loss:0.093, val_acc:0.958]
Epoch [110/120    avg_loss:0.079, val_acc:0.958]
Epoch [111/120    avg_loss:0.073, val_acc:0.960]
Epoch [112/120    avg_loss:0.078, val_acc:0.961]
Epoch [113/120    avg_loss:0.078, val_acc:0.963]
Epoch [114/120    avg_loss:0.083, val_acc:0.961]
Epoch [115/120    avg_loss:0.082, val_acc:0.960]
Epoch [116/120    avg_loss:0.073, val_acc:0.957]
Epoch [117/120    avg_loss:0.075, val_acc:0.959]
Epoch [118/120    avg_loss:0.083, val_acc:0.957]
Epoch [119/120    avg_loss:0.073, val_acc:0.956]
Epoch [120/120    avg_loss:0.069, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    7 1160    3    0    0   12    0    0    0   20   79    1    0
     1    2    0]
 [   0    0    2  713    0    4    0    0    0   16    0    1    9    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0  420    0    6    0    4    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    5    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    4    0    0   12    0    0    1    0
     0    0    0]
 [   0    0   55   90    0   10    0    0    0    0  702    6    4    0
     3    5    0]
 [   0    0   28    0    0    0   21    0    3    0   12 2129    6    8
     3    0    0]
 [   0    0    0   24    7    8    0    0    0    0    8    8  456    0
     0    0   23]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    3    1    0    0
  1128    0    0]
 [   0    0    0    0    0    0   46    0    0    0    0    0    3    0
    86  212    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
92.75880758807588

F1 scores:
[       nan 0.85714286 0.91699605 0.90310323 0.98383372 0.95022624
 0.93678161 0.89285714 0.99652375 0.48       0.864      0.95922505
 0.89940828 0.97368421 0.95431472 0.74911661 0.87958115]

Kappa:
0.9174064374888645
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f07e375fa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.841, val_acc:0.188]
Epoch [2/120    avg_loss:2.765, val_acc:0.263]
Epoch [3/120    avg_loss:2.679, val_acc:0.267]
Epoch [4/120    avg_loss:2.602, val_acc:0.267]
Epoch [5/120    avg_loss:2.513, val_acc:0.318]
Epoch [6/120    avg_loss:2.407, val_acc:0.333]
Epoch [7/120    avg_loss:2.337, val_acc:0.333]
Epoch [8/120    avg_loss:2.272, val_acc:0.385]
Epoch [9/120    avg_loss:2.203, val_acc:0.427]
Epoch [10/120    avg_loss:2.146, val_acc:0.440]
Epoch [11/120    avg_loss:2.063, val_acc:0.498]
Epoch [12/120    avg_loss:2.013, val_acc:0.611]
Epoch [13/120    avg_loss:1.946, val_acc:0.597]
Epoch [14/120    avg_loss:1.885, val_acc:0.624]
Epoch [15/120    avg_loss:1.834, val_acc:0.632]
Epoch [16/120    avg_loss:1.748, val_acc:0.662]
Epoch [17/120    avg_loss:1.679, val_acc:0.666]
Epoch [18/120    avg_loss:1.600, val_acc:0.662]
Epoch [19/120    avg_loss:1.515, val_acc:0.666]
Epoch [20/120    avg_loss:1.452, val_acc:0.669]
Epoch [21/120    avg_loss:1.330, val_acc:0.669]
Epoch [22/120    avg_loss:1.323, val_acc:0.681]
Epoch [23/120    avg_loss:1.217, val_acc:0.693]
Epoch [24/120    avg_loss:1.179, val_acc:0.708]
Epoch [25/120    avg_loss:1.085, val_acc:0.730]
Epoch [26/120    avg_loss:1.029, val_acc:0.733]
Epoch [27/120    avg_loss:0.978, val_acc:0.759]
Epoch [28/120    avg_loss:0.900, val_acc:0.761]
Epoch [29/120    avg_loss:0.871, val_acc:0.769]
Epoch [30/120    avg_loss:0.824, val_acc:0.784]
Epoch [31/120    avg_loss:0.750, val_acc:0.799]
Epoch [32/120    avg_loss:0.749, val_acc:0.801]
Epoch [33/120    avg_loss:0.696, val_acc:0.805]
Epoch [34/120    avg_loss:0.637, val_acc:0.814]
Epoch [35/120    avg_loss:0.623, val_acc:0.809]
Epoch [36/120    avg_loss:0.557, val_acc:0.824]
Epoch [37/120    avg_loss:0.546, val_acc:0.823]
Epoch [38/120    avg_loss:0.532, val_acc:0.811]
Epoch [39/120    avg_loss:0.494, val_acc:0.857]
Epoch [40/120    avg_loss:0.484, val_acc:0.864]
Epoch [41/120    avg_loss:0.433, val_acc:0.858]
Epoch [42/120    avg_loss:0.433, val_acc:0.878]
Epoch [43/120    avg_loss:0.420, val_acc:0.887]
Epoch [44/120    avg_loss:0.438, val_acc:0.849]
Epoch [45/120    avg_loss:0.400, val_acc:0.883]
Epoch [46/120    avg_loss:0.358, val_acc:0.874]
Epoch [47/120    avg_loss:0.342, val_acc:0.887]
Epoch [48/120    avg_loss:0.317, val_acc:0.851]
Epoch [49/120    avg_loss:0.310, val_acc:0.905]
Epoch [50/120    avg_loss:0.265, val_acc:0.874]
Epoch [51/120    avg_loss:0.276, val_acc:0.903]
Epoch [52/120    avg_loss:0.249, val_acc:0.891]
Epoch [53/120    avg_loss:0.260, val_acc:0.877]
Epoch [54/120    avg_loss:0.276, val_acc:0.906]
Epoch [55/120    avg_loss:0.254, val_acc:0.899]
Epoch [56/120    avg_loss:0.221, val_acc:0.907]
Epoch [57/120    avg_loss:0.193, val_acc:0.915]
Epoch [58/120    avg_loss:0.189, val_acc:0.910]
Epoch [59/120    avg_loss:0.200, val_acc:0.899]
Epoch [60/120    avg_loss:0.209, val_acc:0.843]
Epoch [61/120    avg_loss:0.172, val_acc:0.924]
Epoch [62/120    avg_loss:0.187, val_acc:0.918]
Epoch [63/120    avg_loss:0.212, val_acc:0.915]
Epoch [64/120    avg_loss:0.187, val_acc:0.915]
Epoch [65/120    avg_loss:0.159, val_acc:0.920]
Epoch [66/120    avg_loss:0.127, val_acc:0.925]
Epoch [67/120    avg_loss:0.135, val_acc:0.927]
Epoch [68/120    avg_loss:0.120, val_acc:0.933]
Epoch [69/120    avg_loss:0.146, val_acc:0.950]
Epoch [70/120    avg_loss:0.183, val_acc:0.911]
Epoch [71/120    avg_loss:0.145, val_acc:0.912]
Epoch [72/120    avg_loss:0.165, val_acc:0.947]
Epoch [73/120    avg_loss:0.141, val_acc:0.934]
Epoch [74/120    avg_loss:0.127, val_acc:0.941]
Epoch [75/120    avg_loss:0.101, val_acc:0.938]
Epoch [76/120    avg_loss:0.104, val_acc:0.941]
Epoch [77/120    avg_loss:0.099, val_acc:0.939]
Epoch [78/120    avg_loss:0.104, val_acc:0.923]
Epoch [79/120    avg_loss:0.083, val_acc:0.931]
Epoch [80/120    avg_loss:0.079, val_acc:0.938]
Epoch [81/120    avg_loss:0.077, val_acc:0.943]
Epoch [82/120    avg_loss:0.075, val_acc:0.949]
Epoch [83/120    avg_loss:0.058, val_acc:0.953]
Epoch [84/120    avg_loss:0.056, val_acc:0.956]
Epoch [85/120    avg_loss:0.054, val_acc:0.955]
Epoch [86/120    avg_loss:0.058, val_acc:0.953]
Epoch [87/120    avg_loss:0.059, val_acc:0.951]
Epoch [88/120    avg_loss:0.054, val_acc:0.956]
Epoch [89/120    avg_loss:0.055, val_acc:0.953]
Epoch [90/120    avg_loss:0.051, val_acc:0.955]
Epoch [91/120    avg_loss:0.057, val_acc:0.957]
Epoch [92/120    avg_loss:0.055, val_acc:0.956]
Epoch [93/120    avg_loss:0.049, val_acc:0.956]
Epoch [94/120    avg_loss:0.052, val_acc:0.956]
Epoch [95/120    avg_loss:0.045, val_acc:0.955]
Epoch [96/120    avg_loss:0.053, val_acc:0.952]
Epoch [97/120    avg_loss:0.047, val_acc:0.952]
Epoch [98/120    avg_loss:0.051, val_acc:0.953]
Epoch [99/120    avg_loss:0.053, val_acc:0.958]
Epoch [100/120    avg_loss:0.053, val_acc:0.959]
Epoch [101/120    avg_loss:0.047, val_acc:0.958]
Epoch [102/120    avg_loss:0.046, val_acc:0.955]
Epoch [103/120    avg_loss:0.047, val_acc:0.956]
Epoch [104/120    avg_loss:0.047, val_acc:0.956]
Epoch [105/120    avg_loss:0.045, val_acc:0.956]
Epoch [106/120    avg_loss:0.043, val_acc:0.953]
Epoch [107/120    avg_loss:0.045, val_acc:0.956]
Epoch [108/120    avg_loss:0.046, val_acc:0.959]
Epoch [109/120    avg_loss:0.051, val_acc:0.956]
Epoch [110/120    avg_loss:0.044, val_acc:0.960]
Epoch [111/120    avg_loss:0.044, val_acc:0.960]
Epoch [112/120    avg_loss:0.053, val_acc:0.961]
Epoch [113/120    avg_loss:0.047, val_acc:0.963]
Epoch [114/120    avg_loss:0.044, val_acc:0.958]
Epoch [115/120    avg_loss:0.046, val_acc:0.961]
Epoch [116/120    avg_loss:0.039, val_acc:0.964]
Epoch [117/120    avg_loss:0.049, val_acc:0.965]
Epoch [118/120    avg_loss:0.043, val_acc:0.960]
Epoch [119/120    avg_loss:0.045, val_acc:0.964]
Epoch [120/120    avg_loss:0.043, val_acc:0.963]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1170    5    0    0    1    0    0    0    7   93    2    0
     0    7    0]
 [   0    0    2  698    1   24    0    0    0   11    0    0    8    1
     2    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    1    0    4    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    1    0    0   14    0    0    0    0
     0    0    0]
 [   0    0   37   90    0    4    0    0    0    0  732    4    1    0
     2    5    0]
 [   0    0   11    0    0    0   10    0    0    0   14 2161    6    5
     3    0    0]
 [   0    0    2   18    4   10    0    0    0    0    9   13  473    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    3    2    0    0
  1130    2    0]
 [   0    0    0    0    0    0   22    0    0    0    0    0    0    0
   103  222    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
93.78861788617886

F1 scores:
[       nan 0.94871795 0.93338652 0.89429853 0.98839907 0.94537347
 0.97249071 0.98039216 0.99767981 0.59574468 0.89051095 0.96365663
 0.92382812 0.98404255 0.94719195 0.76157804 0.97109827]

Kappa:
0.9290737932441145
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5d341ceb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.854, val_acc:0.186]
Epoch [2/120    avg_loss:2.755, val_acc:0.283]
Epoch [3/120    avg_loss:2.669, val_acc:0.321]
Epoch [4/120    avg_loss:2.568, val_acc:0.340]
Epoch [5/120    avg_loss:2.483, val_acc:0.414]
Epoch [6/120    avg_loss:2.415, val_acc:0.449]
Epoch [7/120    avg_loss:2.362, val_acc:0.462]
Epoch [8/120    avg_loss:2.308, val_acc:0.526]
Epoch [9/120    avg_loss:2.220, val_acc:0.536]
Epoch [10/120    avg_loss:2.151, val_acc:0.536]
Epoch [11/120    avg_loss:2.095, val_acc:0.568]
Epoch [12/120    avg_loss:2.050, val_acc:0.586]
Epoch [13/120    avg_loss:2.019, val_acc:0.573]
Epoch [14/120    avg_loss:1.903, val_acc:0.546]
Epoch [15/120    avg_loss:1.820, val_acc:0.587]
Epoch [16/120    avg_loss:1.772, val_acc:0.596]
Epoch [17/120    avg_loss:1.684, val_acc:0.605]
Epoch [18/120    avg_loss:1.629, val_acc:0.619]
Epoch [19/120    avg_loss:1.492, val_acc:0.642]
Epoch [20/120    avg_loss:1.438, val_acc:0.642]
Epoch [21/120    avg_loss:1.364, val_acc:0.661]
Epoch [22/120    avg_loss:1.352, val_acc:0.661]
Epoch [23/120    avg_loss:1.270, val_acc:0.676]
Epoch [24/120    avg_loss:1.187, val_acc:0.688]
Epoch [25/120    avg_loss:1.119, val_acc:0.707]
Epoch [26/120    avg_loss:1.136, val_acc:0.717]
Epoch [27/120    avg_loss:0.960, val_acc:0.720]
Epoch [28/120    avg_loss:0.945, val_acc:0.718]
Epoch [29/120    avg_loss:0.916, val_acc:0.718]
Epoch [30/120    avg_loss:0.899, val_acc:0.760]
Epoch [31/120    avg_loss:0.755, val_acc:0.760]
Epoch [32/120    avg_loss:0.779, val_acc:0.783]
Epoch [33/120    avg_loss:0.704, val_acc:0.774]
Epoch [34/120    avg_loss:0.633, val_acc:0.808]
Epoch [35/120    avg_loss:0.638, val_acc:0.805]
Epoch [36/120    avg_loss:0.583, val_acc:0.819]
Epoch [37/120    avg_loss:0.568, val_acc:0.830]
Epoch [38/120    avg_loss:0.534, val_acc:0.843]
Epoch [39/120    avg_loss:0.495, val_acc:0.860]
Epoch [40/120    avg_loss:0.476, val_acc:0.850]
Epoch [41/120    avg_loss:0.462, val_acc:0.868]
Epoch [42/120    avg_loss:0.363, val_acc:0.877]
Epoch [43/120    avg_loss:0.369, val_acc:0.870]
Epoch [44/120    avg_loss:0.348, val_acc:0.902]
Epoch [45/120    avg_loss:0.317, val_acc:0.910]
Epoch [46/120    avg_loss:0.309, val_acc:0.893]
Epoch [47/120    avg_loss:0.348, val_acc:0.893]
Epoch [48/120    avg_loss:0.305, val_acc:0.896]
Epoch [49/120    avg_loss:0.295, val_acc:0.882]
Epoch [50/120    avg_loss:0.364, val_acc:0.894]
Epoch [51/120    avg_loss:0.296, val_acc:0.900]
Epoch [52/120    avg_loss:0.302, val_acc:0.901]
Epoch [53/120    avg_loss:0.297, val_acc:0.895]
Epoch [54/120    avg_loss:0.287, val_acc:0.889]
Epoch [55/120    avg_loss:0.267, val_acc:0.911]
Epoch [56/120    avg_loss:0.224, val_acc:0.923]
Epoch [57/120    avg_loss:0.216, val_acc:0.912]
Epoch [58/120    avg_loss:0.266, val_acc:0.901]
Epoch [59/120    avg_loss:0.242, val_acc:0.926]
Epoch [60/120    avg_loss:0.192, val_acc:0.926]
Epoch [61/120    avg_loss:0.185, val_acc:0.917]
Epoch [62/120    avg_loss:0.178, val_acc:0.927]
Epoch [63/120    avg_loss:0.158, val_acc:0.924]
Epoch [64/120    avg_loss:0.164, val_acc:0.932]
Epoch [65/120    avg_loss:0.143, val_acc:0.938]
Epoch [66/120    avg_loss:0.160, val_acc:0.938]
Epoch [67/120    avg_loss:0.149, val_acc:0.921]
Epoch [68/120    avg_loss:0.175, val_acc:0.945]
Epoch [69/120    avg_loss:0.133, val_acc:0.939]
Epoch [70/120    avg_loss:0.147, val_acc:0.942]
Epoch [71/120    avg_loss:0.118, val_acc:0.954]
Epoch [72/120    avg_loss:0.132, val_acc:0.926]
Epoch [73/120    avg_loss:0.156, val_acc:0.954]
Epoch [74/120    avg_loss:0.120, val_acc:0.949]
Epoch [75/120    avg_loss:0.103, val_acc:0.951]
Epoch [76/120    avg_loss:0.106, val_acc:0.946]
Epoch [77/120    avg_loss:0.102, val_acc:0.958]
Epoch [78/120    avg_loss:0.122, val_acc:0.943]
Epoch [79/120    avg_loss:0.103, val_acc:0.951]
Epoch [80/120    avg_loss:0.097, val_acc:0.961]
Epoch [81/120    avg_loss:0.085, val_acc:0.950]
Epoch [82/120    avg_loss:0.091, val_acc:0.951]
Epoch [83/120    avg_loss:0.085, val_acc:0.945]
Epoch [84/120    avg_loss:0.101, val_acc:0.954]
Epoch [85/120    avg_loss:0.086, val_acc:0.951]
Epoch [86/120    avg_loss:0.108, val_acc:0.946]
Epoch [87/120    avg_loss:0.090, val_acc:0.952]
Epoch [88/120    avg_loss:0.082, val_acc:0.952]
Epoch [89/120    avg_loss:0.079, val_acc:0.957]
Epoch [90/120    avg_loss:0.072, val_acc:0.956]
Epoch [91/120    avg_loss:0.074, val_acc:0.961]
Epoch [92/120    avg_loss:0.077, val_acc:0.956]
Epoch [93/120    avg_loss:0.086, val_acc:0.965]
Epoch [94/120    avg_loss:0.160, val_acc:0.942]
Epoch [95/120    avg_loss:0.130, val_acc:0.942]
Epoch [96/120    avg_loss:0.096, val_acc:0.954]
Epoch [97/120    avg_loss:0.083, val_acc:0.955]
Epoch [98/120    avg_loss:0.086, val_acc:0.960]
Epoch [99/120    avg_loss:0.073, val_acc:0.963]
Epoch [100/120    avg_loss:0.079, val_acc:0.944]
Epoch [101/120    avg_loss:0.057, val_acc:0.961]
Epoch [102/120    avg_loss:0.062, val_acc:0.964]
Epoch [103/120    avg_loss:0.069, val_acc:0.971]
Epoch [104/120    avg_loss:0.059, val_acc:0.963]
Epoch [105/120    avg_loss:0.070, val_acc:0.950]
Epoch [106/120    avg_loss:0.063, val_acc:0.976]
Epoch [107/120    avg_loss:0.062, val_acc:0.943]
Epoch [108/120    avg_loss:0.071, val_acc:0.950]
Epoch [109/120    avg_loss:0.247, val_acc:0.820]
Epoch [110/120    avg_loss:0.541, val_acc:0.830]
Epoch [111/120    avg_loss:0.374, val_acc:0.881]
Epoch [112/120    avg_loss:0.254, val_acc:0.915]
Epoch [113/120    avg_loss:0.183, val_acc:0.910]
Epoch [114/120    avg_loss:0.214, val_acc:0.898]
Epoch [115/120    avg_loss:0.192, val_acc:0.924]
Epoch [116/120    avg_loss:0.115, val_acc:0.940]
Epoch [117/120    avg_loss:0.128, val_acc:0.935]
Epoch [118/120    avg_loss:0.126, val_acc:0.943]
Epoch [119/120    avg_loss:0.102, val_acc:0.936]
Epoch [120/120    avg_loss:0.098, val_acc:0.940]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    2 1131    0    1    0   10    0    0    0   22  103    0    0
     0   16    0]
 [   0    0    1  643    6   17    0    0    0   36    0    0   40    4
     0    0    0]
 [   0    0    5    0  208    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  405    0    8    0    8    0    2    0    3
     9    0    0]
 [   0    0    0    0    0    0  639    0    0    1    0    5    0    0
    12    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    7    0    0    9    0    0    1    0
     0    0    0]
 [   0    0   22   89    0    8    1    0    0    0  695   38    0    0
     4   18    0]
 [   0    0   24    0    0    0   23    0    0    0    6 2134    5    6
    12    0    0]
 [   0    0   10    2   10    9    0    0    0    0   14    8  459    0
     0    1   21]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    4    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0   42    0    0    0    0    0    1    0
   123  181    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
91.07859078590786

F1 scores:
[       nan 0.96385542 0.91283293 0.86774629 0.94977169 0.92571429
 0.92675852 0.86206897 1.         0.25       0.85961657 0.94844444
 0.88269231 0.96605744 0.93218249 0.64298401 0.88888889]

Kappa:
0.8981488319548795
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8b4aa63ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.830, val_acc:0.232]
Epoch [2/120    avg_loss:2.736, val_acc:0.241]
Epoch [3/120    avg_loss:2.647, val_acc:0.248]
Epoch [4/120    avg_loss:2.535, val_acc:0.276]
Epoch [5/120    avg_loss:2.451, val_acc:0.283]
Epoch [6/120    avg_loss:2.368, val_acc:0.297]
Epoch [7/120    avg_loss:2.350, val_acc:0.310]
Epoch [8/120    avg_loss:2.259, val_acc:0.308]
Epoch [9/120    avg_loss:2.236, val_acc:0.317]
Epoch [10/120    avg_loss:2.196, val_acc:0.326]
Epoch [11/120    avg_loss:2.116, val_acc:0.374]
Epoch [12/120    avg_loss:2.084, val_acc:0.435]
Epoch [13/120    avg_loss:2.006, val_acc:0.486]
Epoch [14/120    avg_loss:1.943, val_acc:0.501]
Epoch [15/120    avg_loss:1.919, val_acc:0.515]
Epoch [16/120    avg_loss:1.861, val_acc:0.531]
Epoch [17/120    avg_loss:1.774, val_acc:0.601]
Epoch [18/120    avg_loss:1.684, val_acc:0.592]
Epoch [19/120    avg_loss:1.630, val_acc:0.615]
Epoch [20/120    avg_loss:1.587, val_acc:0.664]
Epoch [21/120    avg_loss:1.500, val_acc:0.644]
Epoch [22/120    avg_loss:1.424, val_acc:0.635]
Epoch [23/120    avg_loss:1.341, val_acc:0.681]
Epoch [24/120    avg_loss:1.333, val_acc:0.677]
Epoch [25/120    avg_loss:1.307, val_acc:0.689]
Epoch [26/120    avg_loss:1.122, val_acc:0.725]
Epoch [27/120    avg_loss:1.081, val_acc:0.716]
Epoch [28/120    avg_loss:1.071, val_acc:0.724]
Epoch [29/120    avg_loss:0.969, val_acc:0.766]
Epoch [30/120    avg_loss:0.888, val_acc:0.760]
Epoch [31/120    avg_loss:0.866, val_acc:0.772]
Epoch [32/120    avg_loss:0.836, val_acc:0.797]
Epoch [33/120    avg_loss:0.767, val_acc:0.812]
Epoch [34/120    avg_loss:0.729, val_acc:0.817]
Epoch [35/120    avg_loss:0.714, val_acc:0.849]
Epoch [36/120    avg_loss:0.624, val_acc:0.848]
Epoch [37/120    avg_loss:0.638, val_acc:0.807]
Epoch [38/120    avg_loss:0.596, val_acc:0.850]
Epoch [39/120    avg_loss:0.521, val_acc:0.860]
Epoch [40/120    avg_loss:0.488, val_acc:0.852]
Epoch [41/120    avg_loss:0.485, val_acc:0.860]
Epoch [42/120    avg_loss:0.473, val_acc:0.873]
Epoch [43/120    avg_loss:0.454, val_acc:0.859]
Epoch [44/120    avg_loss:0.434, val_acc:0.857]
Epoch [45/120    avg_loss:0.422, val_acc:0.880]
Epoch [46/120    avg_loss:0.435, val_acc:0.889]
Epoch [47/120    avg_loss:0.372, val_acc:0.856]
Epoch [48/120    avg_loss:0.319, val_acc:0.881]
Epoch [49/120    avg_loss:0.270, val_acc:0.895]
Epoch [50/120    avg_loss:0.269, val_acc:0.908]
Epoch [51/120    avg_loss:0.280, val_acc:0.876]
Epoch [52/120    avg_loss:0.255, val_acc:0.908]
Epoch [53/120    avg_loss:0.264, val_acc:0.918]
Epoch [54/120    avg_loss:0.247, val_acc:0.894]
Epoch [55/120    avg_loss:0.237, val_acc:0.902]
Epoch [56/120    avg_loss:0.234, val_acc:0.917]
Epoch [57/120    avg_loss:0.197, val_acc:0.915]
Epoch [58/120    avg_loss:0.185, val_acc:0.934]
Epoch [59/120    avg_loss:0.173, val_acc:0.932]
Epoch [60/120    avg_loss:0.188, val_acc:0.908]
Epoch [61/120    avg_loss:0.160, val_acc:0.932]
Epoch [62/120    avg_loss:0.128, val_acc:0.933]
Epoch [63/120    avg_loss:0.160, val_acc:0.939]
Epoch [64/120    avg_loss:0.142, val_acc:0.939]
Epoch [65/120    avg_loss:0.119, val_acc:0.944]
Epoch [66/120    avg_loss:0.135, val_acc:0.948]
Epoch [67/120    avg_loss:0.113, val_acc:0.947]
Epoch [68/120    avg_loss:0.129, val_acc:0.940]
Epoch [69/120    avg_loss:0.135, val_acc:0.941]
Epoch [70/120    avg_loss:0.153, val_acc:0.938]
Epoch [71/120    avg_loss:0.135, val_acc:0.910]
Epoch [72/120    avg_loss:0.146, val_acc:0.926]
Epoch [73/120    avg_loss:0.121, val_acc:0.940]
Epoch [74/120    avg_loss:0.111, val_acc:0.948]
Epoch [75/120    avg_loss:0.096, val_acc:0.953]
Epoch [76/120    avg_loss:0.089, val_acc:0.938]
Epoch [77/120    avg_loss:0.153, val_acc:0.908]
Epoch [78/120    avg_loss:0.147, val_acc:0.950]
Epoch [79/120    avg_loss:0.139, val_acc:0.939]
Epoch [80/120    avg_loss:0.132, val_acc:0.941]
Epoch [81/120    avg_loss:0.105, val_acc:0.956]
Epoch [82/120    avg_loss:0.067, val_acc:0.943]
Epoch [83/120    avg_loss:0.080, val_acc:0.948]
Epoch [84/120    avg_loss:0.079, val_acc:0.949]
Epoch [85/120    avg_loss:0.087, val_acc:0.952]
Epoch [86/120    avg_loss:0.082, val_acc:0.956]
Epoch [87/120    avg_loss:0.067, val_acc:0.957]
Epoch [88/120    avg_loss:0.073, val_acc:0.966]
Epoch [89/120    avg_loss:0.056, val_acc:0.957]
Epoch [90/120    avg_loss:0.068, val_acc:0.947]
Epoch [91/120    avg_loss:0.056, val_acc:0.955]
Epoch [92/120    avg_loss:0.087, val_acc:0.947]
Epoch [93/120    avg_loss:0.089, val_acc:0.938]
Epoch [94/120    avg_loss:0.092, val_acc:0.943]
Epoch [95/120    avg_loss:0.073, val_acc:0.957]
Epoch [96/120    avg_loss:0.065, val_acc:0.959]
Epoch [97/120    avg_loss:0.059, val_acc:0.959]
Epoch [98/120    avg_loss:0.063, val_acc:0.952]
Epoch [99/120    avg_loss:0.061, val_acc:0.958]
Epoch [100/120    avg_loss:0.067, val_acc:0.963]
Epoch [101/120    avg_loss:0.059, val_acc:0.955]
Epoch [102/120    avg_loss:0.046, val_acc:0.964]
Epoch [103/120    avg_loss:0.040, val_acc:0.963]
Epoch [104/120    avg_loss:0.033, val_acc:0.965]
Epoch [105/120    avg_loss:0.030, val_acc:0.965]
Epoch [106/120    avg_loss:0.042, val_acc:0.968]
Epoch [107/120    avg_loss:0.034, val_acc:0.968]
Epoch [108/120    avg_loss:0.032, val_acc:0.968]
Epoch [109/120    avg_loss:0.034, val_acc:0.970]
Epoch [110/120    avg_loss:0.031, val_acc:0.970]
Epoch [111/120    avg_loss:0.029, val_acc:0.968]
Epoch [112/120    avg_loss:0.032, val_acc:0.968]
Epoch [113/120    avg_loss:0.031, val_acc:0.968]
Epoch [114/120    avg_loss:0.029, val_acc:0.967]
Epoch [115/120    avg_loss:0.033, val_acc:0.967]
Epoch [116/120    avg_loss:0.028, val_acc:0.967]
Epoch [117/120    avg_loss:0.030, val_acc:0.965]
Epoch [118/120    avg_loss:0.028, val_acc:0.966]
Epoch [119/120    avg_loss:0.029, val_acc:0.967]
Epoch [120/120    avg_loss:0.030, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1247    3    0    0    4    0    0    0    2   20    3    0
     0    6    0]
 [   0    0    4  644    0    0    0    0    0   11    0    0   84    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    2    0    3    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    3    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    6    0    0   10    0    0    2    0
     0    0    0]
 [   0    0   50   90    0    0    0    0    0    0  718    7    0    0
     0   10    0]
 [   0    0   13    0    0    0    9    0    0    0    0 2176    9    2
     1    0    0]
 [   0    0    0    7    3    0    0    0    0    0   13    1  503    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    3    2    0    0
  1132    0    0]
 [   0    0    0    0    0    0   29    0    0    5    0    0    0    0
   100  213    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.26558265582656

F1 scores:
[       nan 0.975      0.95959985 0.86384977 0.99300699 0.98722416
 0.96094326 0.96153846 0.99883856 0.42553191 0.89026658 0.9848382
 0.88634361 0.98404255 0.95166036 0.73958333 0.96      ]

Kappa:
0.9345785439341772
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9ca699cb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.813, val_acc:0.127]
Epoch [2/120    avg_loss:2.723, val_acc:0.211]
Epoch [3/120    avg_loss:2.609, val_acc:0.357]
Epoch [4/120    avg_loss:2.518, val_acc:0.364]
Epoch [5/120    avg_loss:2.441, val_acc:0.378]
Epoch [6/120    avg_loss:2.387, val_acc:0.439]
Epoch [7/120    avg_loss:2.300, val_acc:0.441]
Epoch [8/120    avg_loss:2.283, val_acc:0.483]
Epoch [9/120    avg_loss:2.223, val_acc:0.533]
Epoch [10/120    avg_loss:2.147, val_acc:0.468]
Epoch [11/120    avg_loss:2.083, val_acc:0.553]
Epoch [12/120    avg_loss:2.045, val_acc:0.564]
Epoch [13/120    avg_loss:1.991, val_acc:0.573]
Epoch [14/120    avg_loss:1.904, val_acc:0.598]
Epoch [15/120    avg_loss:1.812, val_acc:0.588]
Epoch [16/120    avg_loss:1.721, val_acc:0.620]
Epoch [17/120    avg_loss:1.668, val_acc:0.605]
Epoch [18/120    avg_loss:1.576, val_acc:0.622]
Epoch [19/120    avg_loss:1.513, val_acc:0.624]
Epoch [20/120    avg_loss:1.450, val_acc:0.637]
Epoch [21/120    avg_loss:1.335, val_acc:0.655]
Epoch [22/120    avg_loss:1.270, val_acc:0.666]
Epoch [23/120    avg_loss:1.181, val_acc:0.688]
Epoch [24/120    avg_loss:1.116, val_acc:0.713]
Epoch [25/120    avg_loss:1.084, val_acc:0.699]
Epoch [26/120    avg_loss:1.039, val_acc:0.748]
Epoch [27/120    avg_loss:0.927, val_acc:0.753]
Epoch [28/120    avg_loss:0.882, val_acc:0.747]
Epoch [29/120    avg_loss:0.824, val_acc:0.781]
Epoch [30/120    avg_loss:0.796, val_acc:0.759]
Epoch [31/120    avg_loss:0.732, val_acc:0.799]
Epoch [32/120    avg_loss:0.698, val_acc:0.794]
Epoch [33/120    avg_loss:0.630, val_acc:0.826]
Epoch [34/120    avg_loss:0.684, val_acc:0.795]
Epoch [35/120    avg_loss:0.600, val_acc:0.853]
Epoch [36/120    avg_loss:0.497, val_acc:0.860]
Epoch [37/120    avg_loss:0.457, val_acc:0.827]
Epoch [38/120    avg_loss:0.477, val_acc:0.855]
Epoch [39/120    avg_loss:0.430, val_acc:0.878]
Epoch [40/120    avg_loss:0.361, val_acc:0.875]
Epoch [41/120    avg_loss:0.403, val_acc:0.874]
Epoch [42/120    avg_loss:0.389, val_acc:0.880]
Epoch [43/120    avg_loss:0.353, val_acc:0.883]
Epoch [44/120    avg_loss:0.391, val_acc:0.873]
Epoch [45/120    avg_loss:0.363, val_acc:0.872]
Epoch [46/120    avg_loss:0.307, val_acc:0.907]
Epoch [47/120    avg_loss:0.270, val_acc:0.902]
Epoch [48/120    avg_loss:0.238, val_acc:0.917]
Epoch [49/120    avg_loss:0.242, val_acc:0.905]
Epoch [50/120    avg_loss:0.236, val_acc:0.901]
Epoch [51/120    avg_loss:0.254, val_acc:0.927]
Epoch [52/120    avg_loss:0.367, val_acc:0.868]
Epoch [53/120    avg_loss:0.326, val_acc:0.882]
Epoch [54/120    avg_loss:0.270, val_acc:0.892]
Epoch [55/120    avg_loss:0.233, val_acc:0.915]
Epoch [56/120    avg_loss:0.218, val_acc:0.916]
Epoch [57/120    avg_loss:0.190, val_acc:0.922]
Epoch [58/120    avg_loss:0.171, val_acc:0.925]
Epoch [59/120    avg_loss:0.162, val_acc:0.931]
Epoch [60/120    avg_loss:0.145, val_acc:0.932]
Epoch [61/120    avg_loss:0.133, val_acc:0.944]
Epoch [62/120    avg_loss:0.117, val_acc:0.936]
Epoch [63/120    avg_loss:0.108, val_acc:0.932]
Epoch [64/120    avg_loss:0.132, val_acc:0.950]
Epoch [65/120    avg_loss:0.124, val_acc:0.945]
Epoch [66/120    avg_loss:0.134, val_acc:0.936]
Epoch [67/120    avg_loss:0.116, val_acc:0.950]
Epoch [68/120    avg_loss:0.092, val_acc:0.950]
Epoch [69/120    avg_loss:0.083, val_acc:0.945]
Epoch [70/120    avg_loss:0.100, val_acc:0.944]
Epoch [71/120    avg_loss:0.129, val_acc:0.947]
Epoch [72/120    avg_loss:0.132, val_acc:0.940]
Epoch [73/120    avg_loss:0.226, val_acc:0.916]
Epoch [74/120    avg_loss:0.177, val_acc:0.941]
Epoch [75/120    avg_loss:0.147, val_acc:0.935]
Epoch [76/120    avg_loss:0.126, val_acc:0.947]
Epoch [77/120    avg_loss:0.088, val_acc:0.947]
Epoch [78/120    avg_loss:0.079, val_acc:0.947]
Epoch [79/120    avg_loss:0.088, val_acc:0.949]
Epoch [80/120    avg_loss:0.074, val_acc:0.953]
Epoch [81/120    avg_loss:0.073, val_acc:0.953]
Epoch [82/120    avg_loss:0.067, val_acc:0.960]
Epoch [83/120    avg_loss:0.060, val_acc:0.951]
Epoch [84/120    avg_loss:0.065, val_acc:0.953]
Epoch [85/120    avg_loss:0.058, val_acc:0.943]
Epoch [86/120    avg_loss:0.062, val_acc:0.952]
Epoch [87/120    avg_loss:0.047, val_acc:0.956]
Epoch [88/120    avg_loss:0.059, val_acc:0.949]
Epoch [89/120    avg_loss:0.066, val_acc:0.960]
Epoch [90/120    avg_loss:0.053, val_acc:0.955]
Epoch [91/120    avg_loss:0.046, val_acc:0.961]
Epoch [92/120    avg_loss:0.061, val_acc:0.957]
Epoch [93/120    avg_loss:0.053, val_acc:0.955]
Epoch [94/120    avg_loss:0.044, val_acc:0.966]
Epoch [95/120    avg_loss:0.044, val_acc:0.961]
Epoch [96/120    avg_loss:0.038, val_acc:0.961]
Epoch [97/120    avg_loss:0.037, val_acc:0.952]
Epoch [98/120    avg_loss:0.048, val_acc:0.961]
Epoch [99/120    avg_loss:0.039, val_acc:0.960]
Epoch [100/120    avg_loss:0.036, val_acc:0.968]
Epoch [101/120    avg_loss:0.035, val_acc:0.964]
Epoch [102/120    avg_loss:0.062, val_acc:0.965]
Epoch [103/120    avg_loss:0.048, val_acc:0.970]
Epoch [104/120    avg_loss:0.040, val_acc:0.966]
Epoch [105/120    avg_loss:0.037, val_acc:0.959]
Epoch [106/120    avg_loss:0.035, val_acc:0.965]
Epoch [107/120    avg_loss:0.030, val_acc:0.964]
Epoch [108/120    avg_loss:0.032, val_acc:0.967]
Epoch [109/120    avg_loss:0.029, val_acc:0.967]
Epoch [110/120    avg_loss:0.031, val_acc:0.964]
Epoch [111/120    avg_loss:0.029, val_acc:0.970]
Epoch [112/120    avg_loss:0.024, val_acc:0.973]
Epoch [113/120    avg_loss:0.032, val_acc:0.969]
Epoch [114/120    avg_loss:0.028, val_acc:0.964]
Epoch [115/120    avg_loss:0.024, val_acc:0.972]
Epoch [116/120    avg_loss:0.029, val_acc:0.967]
Epoch [117/120    avg_loss:0.025, val_acc:0.968]
Epoch [118/120    avg_loss:0.024, val_acc:0.968]
Epoch [119/120    avg_loss:0.026, val_acc:0.967]
Epoch [120/120    avg_loss:0.025, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    1 1202    0    5    0    3    0    0    0    3   54   15    1
     0    1    0]
 [   0    0    2  728    3    5    0    0    0    2    0    1    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   39   89    0    0    1    0    0    0  713   21   10    0
     0    2    0]
 [   0    0    8    0    0    1    4    0    5    0    5 2182    1    4
     0    0    0]
 [   0    0    2    9    1    3    0    0    0    0   14   16  484    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    2    0    3    5    0    0
  1128    0    0]
 [   0    0    0    0    0    0   42    0    0    0    0    0    0   13
    64  228    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.75338753387534

F1 scores:
[       nan 0.95       0.94720252 0.92561983 0.97931034 0.98401826
 0.96182085 1.         0.99192618 0.82926829 0.88242574 0.97172122
 0.91927825 0.95360825 0.96782497 0.78892734 0.95906433]

Kappa:
0.9400937094906503
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa24a2f9b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.813, val_acc:0.291]
Epoch [2/120    avg_loss:2.709, val_acc:0.298]
Epoch [3/120    avg_loss:2.616, val_acc:0.373]
Epoch [4/120    avg_loss:2.528, val_acc:0.408]
Epoch [5/120    avg_loss:2.456, val_acc:0.475]
Epoch [6/120    avg_loss:2.351, val_acc:0.492]
Epoch [7/120    avg_loss:2.309, val_acc:0.528]
Epoch [8/120    avg_loss:2.271, val_acc:0.548]
Epoch [9/120    avg_loss:2.180, val_acc:0.535]
Epoch [10/120    avg_loss:2.144, val_acc:0.550]
Epoch [11/120    avg_loss:2.047, val_acc:0.543]
Epoch [12/120    avg_loss:2.034, val_acc:0.589]
Epoch [13/120    avg_loss:1.974, val_acc:0.606]
Epoch [14/120    avg_loss:1.917, val_acc:0.595]
Epoch [15/120    avg_loss:1.842, val_acc:0.608]
Epoch [16/120    avg_loss:1.712, val_acc:0.610]
Epoch [17/120    avg_loss:1.666, val_acc:0.632]
Epoch [18/120    avg_loss:1.590, val_acc:0.628]
Epoch [19/120    avg_loss:1.485, val_acc:0.660]
Epoch [20/120    avg_loss:1.428, val_acc:0.625]
Epoch [21/120    avg_loss:1.347, val_acc:0.676]
Epoch [22/120    avg_loss:1.210, val_acc:0.683]
Epoch [23/120    avg_loss:1.205, val_acc:0.680]
Epoch [24/120    avg_loss:1.114, val_acc:0.690]
Epoch [25/120    avg_loss:1.066, val_acc:0.720]
Epoch [26/120    avg_loss:0.973, val_acc:0.720]
Epoch [27/120    avg_loss:0.925, val_acc:0.734]
Epoch [28/120    avg_loss:0.907, val_acc:0.756]
Epoch [29/120    avg_loss:0.789, val_acc:0.760]
Epoch [30/120    avg_loss:0.745, val_acc:0.742]
Epoch [31/120    avg_loss:0.770, val_acc:0.768]
Epoch [32/120    avg_loss:0.688, val_acc:0.794]
Epoch [33/120    avg_loss:0.636, val_acc:0.818]
Epoch [34/120    avg_loss:0.617, val_acc:0.815]
Epoch [35/120    avg_loss:0.531, val_acc:0.819]
Epoch [36/120    avg_loss:0.562, val_acc:0.808]
Epoch [37/120    avg_loss:0.547, val_acc:0.817]
Epoch [38/120    avg_loss:0.498, val_acc:0.812]
Epoch [39/120    avg_loss:0.484, val_acc:0.831]
Epoch [40/120    avg_loss:0.504, val_acc:0.867]
Epoch [41/120    avg_loss:0.496, val_acc:0.825]
Epoch [42/120    avg_loss:0.453, val_acc:0.860]
Epoch [43/120    avg_loss:0.410, val_acc:0.850]
Epoch [44/120    avg_loss:0.474, val_acc:0.864]
Epoch [45/120    avg_loss:0.437, val_acc:0.856]
Epoch [46/120    avg_loss:0.371, val_acc:0.872]
Epoch [47/120    avg_loss:0.352, val_acc:0.897]
Epoch [48/120    avg_loss:0.340, val_acc:0.875]
Epoch [49/120    avg_loss:0.324, val_acc:0.856]
Epoch [50/120    avg_loss:0.305, val_acc:0.889]
Epoch [51/120    avg_loss:0.307, val_acc:0.874]
Epoch [52/120    avg_loss:0.320, val_acc:0.907]
Epoch [53/120    avg_loss:0.290, val_acc:0.881]
Epoch [54/120    avg_loss:0.361, val_acc:0.849]
Epoch [55/120    avg_loss:0.324, val_acc:0.908]
Epoch [56/120    avg_loss:0.238, val_acc:0.915]
Epoch [57/120    avg_loss:0.226, val_acc:0.908]
Epoch [58/120    avg_loss:0.236, val_acc:0.911]
Epoch [59/120    avg_loss:0.225, val_acc:0.927]
Epoch [60/120    avg_loss:0.212, val_acc:0.914]
Epoch [61/120    avg_loss:0.195, val_acc:0.930]
Epoch [62/120    avg_loss:0.194, val_acc:0.924]
Epoch [63/120    avg_loss:0.202, val_acc:0.923]
Epoch [64/120    avg_loss:0.194, val_acc:0.923]
Epoch [65/120    avg_loss:0.159, val_acc:0.943]
Epoch [66/120    avg_loss:0.149, val_acc:0.926]
Epoch [67/120    avg_loss:0.152, val_acc:0.938]
Epoch [68/120    avg_loss:0.161, val_acc:0.932]
Epoch [69/120    avg_loss:0.183, val_acc:0.917]
Epoch [70/120    avg_loss:0.173, val_acc:0.928]
Epoch [71/120    avg_loss:0.206, val_acc:0.924]
Epoch [72/120    avg_loss:0.183, val_acc:0.936]
Epoch [73/120    avg_loss:0.185, val_acc:0.881]
Epoch [74/120    avg_loss:0.171, val_acc:0.936]
Epoch [75/120    avg_loss:0.150, val_acc:0.928]
Epoch [76/120    avg_loss:0.133, val_acc:0.940]
Epoch [77/120    avg_loss:0.127, val_acc:0.922]
Epoch [78/120    avg_loss:0.119, val_acc:0.944]
Epoch [79/120    avg_loss:0.106, val_acc:0.947]
Epoch [80/120    avg_loss:0.104, val_acc:0.942]
Epoch [81/120    avg_loss:0.116, val_acc:0.959]
Epoch [82/120    avg_loss:0.123, val_acc:0.938]
Epoch [83/120    avg_loss:0.117, val_acc:0.932]
Epoch [84/120    avg_loss:0.109, val_acc:0.941]
Epoch [85/120    avg_loss:0.113, val_acc:0.947]
Epoch [86/120    avg_loss:0.110, val_acc:0.944]
Epoch [87/120    avg_loss:0.098, val_acc:0.940]
Epoch [88/120    avg_loss:0.099, val_acc:0.945]
Epoch [89/120    avg_loss:0.099, val_acc:0.952]
Epoch [90/120    avg_loss:0.082, val_acc:0.956]
Epoch [91/120    avg_loss:0.083, val_acc:0.951]
Epoch [92/120    avg_loss:0.073, val_acc:0.959]
Epoch [93/120    avg_loss:0.078, val_acc:0.958]
Epoch [94/120    avg_loss:0.071, val_acc:0.952]
Epoch [95/120    avg_loss:0.082, val_acc:0.956]
Epoch [96/120    avg_loss:0.068, val_acc:0.956]
Epoch [97/120    avg_loss:0.061, val_acc:0.963]
Epoch [98/120    avg_loss:0.070, val_acc:0.964]
Epoch [99/120    avg_loss:0.082, val_acc:0.934]
Epoch [100/120    avg_loss:0.073, val_acc:0.959]
Epoch [101/120    avg_loss:0.073, val_acc:0.940]
Epoch [102/120    avg_loss:0.068, val_acc:0.963]
Epoch [103/120    avg_loss:0.062, val_acc:0.956]
Epoch [104/120    avg_loss:0.062, val_acc:0.959]
Epoch [105/120    avg_loss:0.065, val_acc:0.945]
Epoch [106/120    avg_loss:0.068, val_acc:0.932]
Epoch [107/120    avg_loss:0.086, val_acc:0.953]
Epoch [108/120    avg_loss:0.102, val_acc:0.953]
Epoch [109/120    avg_loss:0.071, val_acc:0.961]
Epoch [110/120    avg_loss:0.071, val_acc:0.952]
Epoch [111/120    avg_loss:0.149, val_acc:0.934]
Epoch [112/120    avg_loss:0.108, val_acc:0.942]
Epoch [113/120    avg_loss:0.097, val_acc:0.957]
Epoch [114/120    avg_loss:0.063, val_acc:0.960]
Epoch [115/120    avg_loss:0.061, val_acc:0.966]
Epoch [116/120    avg_loss:0.059, val_acc:0.965]
Epoch [117/120    avg_loss:0.058, val_acc:0.964]
Epoch [118/120    avg_loss:0.056, val_acc:0.963]
Epoch [119/120    avg_loss:0.057, val_acc:0.965]
Epoch [120/120    avg_loss:0.049, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    2    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1192    4    0    0    2    0    0    0    1   68    0    0
     0   18    0]
 [   0    0    5  684    4   12    1    0    0   18    0    8   12    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  408    0    2    0    2    0    0    0    0
    23    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    2    0    0    4    0    0   10    0    0    2    0
     0    0    0]
 [   0    0   58   82    0    9    8    0    0    0  704    1    1    0
     2   10    0]
 [   0    0   14    0    0    0   12    0    0    0    9 2170    3    0
     2    0    0]
 [   0    0    0    7   21    2    0    0    0    0   17   15  464    0
     0    0    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    3    1    0    0
  1127    0    0]
 [   0    0    0    0    0    0   59    0    0    0    0    0    5    0
    96  187    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
92.92140921409214

F1 scores:
[       nan 0.94871795 0.93270736 0.89646134 0.94456763 0.9347079
 0.93553009 0.96153846 0.99767442 0.41666667 0.87399131 0.96939915
 0.90802348 0.9919571  0.943491   0.66548043 0.95454545]

Kappa:
0.9191588448229473
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2a6297cb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 47368==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.820, val_acc:0.072]
Epoch [2/120    avg_loss:2.757, val_acc:0.222]
Epoch [3/120    avg_loss:2.688, val_acc:0.286]
Epoch [4/120    avg_loss:2.616, val_acc:0.302]
Epoch [5/120    avg_loss:2.547, val_acc:0.326]
Epoch [6/120    avg_loss:2.463, val_acc:0.344]
Epoch [7/120    avg_loss:2.387, val_acc:0.362]
Epoch [8/120    avg_loss:2.328, val_acc:0.380]
Epoch [9/120    avg_loss:2.270, val_acc:0.392]
Epoch [10/120    avg_loss:2.224, val_acc:0.374]
Epoch [11/120    avg_loss:2.172, val_acc:0.362]
Epoch [12/120    avg_loss:2.069, val_acc:0.452]
Epoch [13/120    avg_loss:2.037, val_acc:0.492]
Epoch [14/120    avg_loss:1.951, val_acc:0.523]
Epoch [15/120    avg_loss:1.891, val_acc:0.562]
Epoch [16/120    avg_loss:1.905, val_acc:0.573]
Epoch [17/120    avg_loss:1.864, val_acc:0.509]
Epoch [18/120    avg_loss:1.751, val_acc:0.609]
Epoch [19/120    avg_loss:1.673, val_acc:0.605]
Epoch [20/120    avg_loss:1.592, val_acc:0.577]
Epoch [21/120    avg_loss:1.533, val_acc:0.627]
Epoch [22/120    avg_loss:1.461, val_acc:0.632]
Epoch [23/120    avg_loss:1.322, val_acc:0.674]
Epoch [24/120    avg_loss:1.262, val_acc:0.692]
Epoch [25/120    avg_loss:1.199, val_acc:0.708]
Epoch [26/120    avg_loss:1.110, val_acc:0.720]
Epoch [27/120    avg_loss:1.070, val_acc:0.730]
Epoch [28/120    avg_loss:0.991, val_acc:0.753]
Epoch [29/120    avg_loss:0.901, val_acc:0.776]
Epoch [30/120    avg_loss:0.854, val_acc:0.769]
Epoch [31/120    avg_loss:0.902, val_acc:0.751]
Epoch [32/120    avg_loss:0.784, val_acc:0.773]
Epoch [33/120    avg_loss:0.676, val_acc:0.776]
Epoch [34/120    avg_loss:0.631, val_acc:0.806]
Epoch [35/120    avg_loss:0.615, val_acc:0.794]
Epoch [36/120    avg_loss:0.588, val_acc:0.841]
Epoch [37/120    avg_loss:0.564, val_acc:0.799]
Epoch [38/120    avg_loss:0.480, val_acc:0.831]
Epoch [39/120    avg_loss:0.446, val_acc:0.848]
Epoch [40/120    avg_loss:0.447, val_acc:0.848]
Epoch [41/120    avg_loss:0.399, val_acc:0.827]
Epoch [42/120    avg_loss:0.396, val_acc:0.843]
Epoch [43/120    avg_loss:0.399, val_acc:0.860]
Epoch [44/120    avg_loss:0.429, val_acc:0.819]
Epoch [45/120    avg_loss:0.372, val_acc:0.870]
Epoch [46/120    avg_loss:0.335, val_acc:0.870]
Epoch [47/120    avg_loss:0.320, val_acc:0.866]
Epoch [48/120    avg_loss:0.289, val_acc:0.890]
Epoch [49/120    avg_loss:0.351, val_acc:0.842]
Epoch [50/120    avg_loss:0.346, val_acc:0.882]
Epoch [51/120    avg_loss:0.320, val_acc:0.863]
Epoch [52/120    avg_loss:0.291, val_acc:0.883]
Epoch [53/120    avg_loss:0.279, val_acc:0.864]
Epoch [54/120    avg_loss:0.279, val_acc:0.884]
Epoch [55/120    avg_loss:0.251, val_acc:0.885]
Epoch [56/120    avg_loss:0.249, val_acc:0.890]
Epoch [57/120    avg_loss:0.216, val_acc:0.915]
Epoch [58/120    avg_loss:0.180, val_acc:0.922]
Epoch [59/120    avg_loss:0.168, val_acc:0.934]
Epoch [60/120    avg_loss:0.189, val_acc:0.915]
Epoch [61/120    avg_loss:0.194, val_acc:0.907]
Epoch [62/120    avg_loss:0.182, val_acc:0.922]
Epoch [63/120    avg_loss:0.186, val_acc:0.912]
Epoch [64/120    avg_loss:0.213, val_acc:0.870]
Epoch [65/120    avg_loss:0.261, val_acc:0.880]
Epoch [66/120    avg_loss:0.180, val_acc:0.901]
Epoch [67/120    avg_loss:0.161, val_acc:0.938]
Epoch [68/120    avg_loss:0.166, val_acc:0.918]
Epoch [69/120    avg_loss:0.151, val_acc:0.938]
Epoch [70/120    avg_loss:0.163, val_acc:0.925]
Epoch [71/120    avg_loss:0.176, val_acc:0.945]
Epoch [72/120    avg_loss:0.139, val_acc:0.924]
Epoch [73/120    avg_loss:0.125, val_acc:0.944]
Epoch [74/120    avg_loss:0.119, val_acc:0.949]
Epoch [75/120    avg_loss:0.153, val_acc:0.906]
Epoch [76/120    avg_loss:0.147, val_acc:0.939]
Epoch [77/120    avg_loss:0.122, val_acc:0.944]
Epoch [78/120    avg_loss:0.122, val_acc:0.945]
Epoch [79/120    avg_loss:0.112, val_acc:0.953]
Epoch [80/120    avg_loss:0.109, val_acc:0.934]
Epoch [81/120    avg_loss:0.102, val_acc:0.960]
Epoch [82/120    avg_loss:0.091, val_acc:0.940]
Epoch [83/120    avg_loss:0.080, val_acc:0.963]
Epoch [84/120    avg_loss:0.071, val_acc:0.961]
Epoch [85/120    avg_loss:0.071, val_acc:0.963]
Epoch [86/120    avg_loss:0.075, val_acc:0.949]
Epoch [87/120    avg_loss:0.072, val_acc:0.959]
Epoch [88/120    avg_loss:0.062, val_acc:0.961]
Epoch [89/120    avg_loss:0.075, val_acc:0.958]
Epoch [90/120    avg_loss:0.159, val_acc:0.917]
Epoch [91/120    avg_loss:0.123, val_acc:0.942]
Epoch [92/120    avg_loss:0.082, val_acc:0.947]
Epoch [93/120    avg_loss:0.158, val_acc:0.925]
Epoch [94/120    avg_loss:0.128, val_acc:0.942]
Epoch [95/120    avg_loss:0.154, val_acc:0.940]
Epoch [96/120    avg_loss:0.095, val_acc:0.961]
Epoch [97/120    avg_loss:0.083, val_acc:0.949]
Epoch [98/120    avg_loss:0.061, val_acc:0.961]
Epoch [99/120    avg_loss:0.049, val_acc:0.966]
Epoch [100/120    avg_loss:0.048, val_acc:0.969]
Epoch [101/120    avg_loss:0.040, val_acc:0.967]
Epoch [102/120    avg_loss:0.045, val_acc:0.967]
Epoch [103/120    avg_loss:0.051, val_acc:0.966]
Epoch [104/120    avg_loss:0.043, val_acc:0.966]
Epoch [105/120    avg_loss:0.044, val_acc:0.964]
Epoch [106/120    avg_loss:0.046, val_acc:0.965]
Epoch [107/120    avg_loss:0.040, val_acc:0.968]
Epoch [108/120    avg_loss:0.043, val_acc:0.967]
Epoch [109/120    avg_loss:0.048, val_acc:0.966]
Epoch [110/120    avg_loss:0.043, val_acc:0.966]
Epoch [111/120    avg_loss:0.044, val_acc:0.968]
Epoch [112/120    avg_loss:0.043, val_acc:0.968]
Epoch [113/120    avg_loss:0.039, val_acc:0.966]
Epoch [114/120    avg_loss:0.044, val_acc:0.966]
Epoch [115/120    avg_loss:0.041, val_acc:0.966]
Epoch [116/120    avg_loss:0.037, val_acc:0.967]
Epoch [117/120    avg_loss:0.037, val_acc:0.967]
Epoch [118/120    avg_loss:0.042, val_acc:0.967]
Epoch [119/120    avg_loss:0.038, val_acc:0.967]
Epoch [120/120    avg_loss:0.046, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1187    0    0    0    1    0    0    0    7   82    6    0
     0    2    0]
 [   0    0    1  712    1    5    1    0    0   15    0    0   12    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  420    0    4    0    6    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    3    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   14    0    4    0    0
     0    0    0]
 [   0    0   26   89    0    4    1    0    0    0  744    1    4    0
     0    6    0]
 [   0    0   18    0    0    1    3    0    0    0   15 2163    7    3
     0    0    0]
 [   0    0    0    1    2    3    0    0    0    0   11   22  488    0
     0    1    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   13    0    0    0    0    3    1    0    0
  1122    0    0]
 [   0    0    0    0    0    0   50    0    0    0    0    0    0    0
    80  217    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.22222222222223

F1 scores:
[       nan 0.94871795 0.94318633 0.91930278 0.99300699 0.95346198
 0.95531136 0.92592593 1.         0.52830189 0.89692586 0.96433348
 0.92775665 0.9919571  0.95570698 0.7574171  0.95953757]

Kappa:
0.9340321435396586
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3a05cf7860>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.459, val_acc:0.507]
Epoch [2/120    avg_loss:1.899, val_acc:0.561]
Epoch [3/120    avg_loss:1.549, val_acc:0.620]
Epoch [4/120    avg_loss:1.361, val_acc:0.663]
Epoch [5/120    avg_loss:1.122, val_acc:0.701]
Epoch [6/120    avg_loss:0.926, val_acc:0.665]
Epoch [7/120    avg_loss:0.792, val_acc:0.721]
Epoch [8/120    avg_loss:0.781, val_acc:0.740]
Epoch [9/120    avg_loss:0.728, val_acc:0.711]
Epoch [10/120    avg_loss:0.631, val_acc:0.795]
Epoch [11/120    avg_loss:0.515, val_acc:0.817]
Epoch [12/120    avg_loss:0.516, val_acc:0.794]
Epoch [13/120    avg_loss:0.474, val_acc:0.792]
Epoch [14/120    avg_loss:0.445, val_acc:0.805]
Epoch [15/120    avg_loss:0.382, val_acc:0.839]
Epoch [16/120    avg_loss:0.350, val_acc:0.866]
Epoch [17/120    avg_loss:0.338, val_acc:0.759]
Epoch [18/120    avg_loss:0.375, val_acc:0.858]
Epoch [19/120    avg_loss:0.308, val_acc:0.871]
Epoch [20/120    avg_loss:0.305, val_acc:0.879]
Epoch [21/120    avg_loss:0.311, val_acc:0.837]
Epoch [22/120    avg_loss:0.242, val_acc:0.880]
Epoch [23/120    avg_loss:0.218, val_acc:0.897]
Epoch [24/120    avg_loss:0.244, val_acc:0.884]
Epoch [25/120    avg_loss:0.169, val_acc:0.910]
Epoch [26/120    avg_loss:0.201, val_acc:0.894]
Epoch [27/120    avg_loss:0.178, val_acc:0.918]
Epoch [28/120    avg_loss:0.268, val_acc:0.904]
Epoch [29/120    avg_loss:0.161, val_acc:0.859]
Epoch [30/120    avg_loss:0.141, val_acc:0.909]
Epoch [31/120    avg_loss:0.112, val_acc:0.933]
Epoch [32/120    avg_loss:0.101, val_acc:0.931]
Epoch [33/120    avg_loss:0.110, val_acc:0.929]
Epoch [34/120    avg_loss:0.113, val_acc:0.938]
Epoch [35/120    avg_loss:0.178, val_acc:0.918]
Epoch [36/120    avg_loss:0.137, val_acc:0.926]
Epoch [37/120    avg_loss:0.103, val_acc:0.927]
Epoch [38/120    avg_loss:0.129, val_acc:0.931]
Epoch [39/120    avg_loss:0.106, val_acc:0.934]
Epoch [40/120    avg_loss:0.095, val_acc:0.944]
Epoch [41/120    avg_loss:0.066, val_acc:0.953]
Epoch [42/120    avg_loss:0.062, val_acc:0.936]
Epoch [43/120    avg_loss:0.064, val_acc:0.938]
Epoch [44/120    avg_loss:0.052, val_acc:0.943]
Epoch [45/120    avg_loss:0.061, val_acc:0.947]
Epoch [46/120    avg_loss:0.112, val_acc:0.939]
Epoch [47/120    avg_loss:0.068, val_acc:0.941]
Epoch [48/120    avg_loss:0.068, val_acc:0.946]
Epoch [49/120    avg_loss:0.108, val_acc:0.946]
Epoch [50/120    avg_loss:0.057, val_acc:0.939]
Epoch [51/120    avg_loss:0.048, val_acc:0.953]
Epoch [52/120    avg_loss:0.035, val_acc:0.959]
Epoch [53/120    avg_loss:0.035, val_acc:0.962]
Epoch [54/120    avg_loss:0.045, val_acc:0.945]
Epoch [55/120    avg_loss:0.039, val_acc:0.903]
Epoch [56/120    avg_loss:0.080, val_acc:0.951]
Epoch [57/120    avg_loss:0.041, val_acc:0.956]
Epoch [58/120    avg_loss:0.041, val_acc:0.943]
Epoch [59/120    avg_loss:0.053, val_acc:0.947]
Epoch [60/120    avg_loss:0.056, val_acc:0.948]
Epoch [61/120    avg_loss:0.077, val_acc:0.934]
Epoch [62/120    avg_loss:0.062, val_acc:0.953]
Epoch [63/120    avg_loss:0.102, val_acc:0.928]
Epoch [64/120    avg_loss:0.110, val_acc:0.939]
Epoch [65/120    avg_loss:0.033, val_acc:0.956]
Epoch [66/120    avg_loss:0.043, val_acc:0.955]
Epoch [67/120    avg_loss:0.022, val_acc:0.957]
Epoch [68/120    avg_loss:0.022, val_acc:0.955]
Epoch [69/120    avg_loss:0.018, val_acc:0.955]
Epoch [70/120    avg_loss:0.026, val_acc:0.957]
Epoch [71/120    avg_loss:0.018, val_acc:0.956]
Epoch [72/120    avg_loss:0.017, val_acc:0.962]
Epoch [73/120    avg_loss:0.015, val_acc:0.963]
Epoch [74/120    avg_loss:0.012, val_acc:0.962]
Epoch [75/120    avg_loss:0.019, val_acc:0.965]
Epoch [76/120    avg_loss:0.013, val_acc:0.965]
Epoch [77/120    avg_loss:0.014, val_acc:0.964]
Epoch [78/120    avg_loss:0.015, val_acc:0.966]
Epoch [79/120    avg_loss:0.012, val_acc:0.966]
Epoch [80/120    avg_loss:0.013, val_acc:0.966]
Epoch [81/120    avg_loss:0.016, val_acc:0.966]
Epoch [82/120    avg_loss:0.016, val_acc:0.964]
Epoch [83/120    avg_loss:0.013, val_acc:0.963]
Epoch [84/120    avg_loss:0.011, val_acc:0.964]
Epoch [85/120    avg_loss:0.010, val_acc:0.962]
Epoch [86/120    avg_loss:0.015, val_acc:0.966]
Epoch [87/120    avg_loss:0.017, val_acc:0.966]
Epoch [88/120    avg_loss:0.011, val_acc:0.967]
Epoch [89/120    avg_loss:0.010, val_acc:0.965]
Epoch [90/120    avg_loss:0.013, val_acc:0.964]
Epoch [91/120    avg_loss:0.012, val_acc:0.968]
Epoch [92/120    avg_loss:0.012, val_acc:0.967]
Epoch [93/120    avg_loss:0.012, val_acc:0.966]
Epoch [94/120    avg_loss:0.013, val_acc:0.966]
Epoch [95/120    avg_loss:0.015, val_acc:0.965]
Epoch [96/120    avg_loss:0.012, val_acc:0.967]
Epoch [97/120    avg_loss:0.012, val_acc:0.966]
Epoch [98/120    avg_loss:0.013, val_acc:0.966]
Epoch [99/120    avg_loss:0.012, val_acc:0.967]
Epoch [100/120    avg_loss:0.010, val_acc:0.968]
Epoch [101/120    avg_loss:0.012, val_acc:0.967]
Epoch [102/120    avg_loss:0.009, val_acc:0.967]
Epoch [103/120    avg_loss:0.011, val_acc:0.968]
Epoch [104/120    avg_loss:0.010, val_acc:0.964]
Epoch [105/120    avg_loss:0.018, val_acc:0.966]
Epoch [106/120    avg_loss:0.013, val_acc:0.968]
Epoch [107/120    avg_loss:0.016, val_acc:0.966]
Epoch [108/120    avg_loss:0.010, val_acc:0.967]
Epoch [109/120    avg_loss:0.011, val_acc:0.965]
Epoch [110/120    avg_loss:0.010, val_acc:0.965]
Epoch [111/120    avg_loss:0.012, val_acc:0.967]
Epoch [112/120    avg_loss:0.011, val_acc:0.965]
Epoch [113/120    avg_loss:0.014, val_acc:0.965]
Epoch [114/120    avg_loss:0.009, val_acc:0.966]
Epoch [115/120    avg_loss:0.012, val_acc:0.968]
Epoch [116/120    avg_loss:0.008, val_acc:0.967]
Epoch [117/120    avg_loss:0.012, val_acc:0.966]
Epoch [118/120    avg_loss:0.011, val_acc:0.967]
Epoch [119/120    avg_loss:0.011, val_acc:0.968]
Epoch [120/120    avg_loss:0.011, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1256    4    3    1    0    0    0    1    3   17    0    0
     0    0    0]
 [   0    0    0  725    1    4    1    0    0    0    0   13    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  421    0    2    0    3    0    1    0    0
     8    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    4    0    0
     1    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  834   33    0    0
     0    2    0]
 [   0    0   10    0    0    2    0    0    0    1    8 2172   16    0
     0    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1  528    0
     0    4    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1120   19    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
    81  265    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.20325203252033

F1 scores:
[       nan 0.98765432 0.9820172  0.98238482 0.99069767 0.97453704
 0.99541985 0.94117647 1.         0.85714286 0.96976744 0.97596046
 0.97687327 1.         0.95359728 0.830721   0.99408284]

Kappa:
0.9680813668795054
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2f5a5af860>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.418, val_acc:0.483]
Epoch [2/120    avg_loss:1.870, val_acc:0.579]
Epoch [3/120    avg_loss:1.595, val_acc:0.611]
Epoch [4/120    avg_loss:1.403, val_acc:0.686]
Epoch [5/120    avg_loss:1.204, val_acc:0.684]
Epoch [6/120    avg_loss:0.998, val_acc:0.717]
Epoch [7/120    avg_loss:0.782, val_acc:0.771]
Epoch [8/120    avg_loss:0.709, val_acc:0.795]
Epoch [9/120    avg_loss:0.677, val_acc:0.758]
Epoch [10/120    avg_loss:0.619, val_acc:0.831]
Epoch [11/120    avg_loss:0.520, val_acc:0.755]
Epoch [12/120    avg_loss:0.441, val_acc:0.820]
Epoch [13/120    avg_loss:0.479, val_acc:0.799]
Epoch [14/120    avg_loss:0.447, val_acc:0.852]
Epoch [15/120    avg_loss:0.317, val_acc:0.819]
Epoch [16/120    avg_loss:0.306, val_acc:0.878]
Epoch [17/120    avg_loss:0.275, val_acc:0.876]
Epoch [18/120    avg_loss:0.219, val_acc:0.887]
Epoch [19/120    avg_loss:0.230, val_acc:0.893]
Epoch [20/120    avg_loss:0.207, val_acc:0.898]
Epoch [21/120    avg_loss:0.175, val_acc:0.882]
Epoch [22/120    avg_loss:0.214, val_acc:0.906]
Epoch [23/120    avg_loss:0.183, val_acc:0.910]
Epoch [24/120    avg_loss:0.189, val_acc:0.902]
Epoch [25/120    avg_loss:0.138, val_acc:0.924]
Epoch [26/120    avg_loss:0.209, val_acc:0.923]
Epoch [27/120    avg_loss:0.116, val_acc:0.914]
Epoch [28/120    avg_loss:0.199, val_acc:0.895]
Epoch [29/120    avg_loss:0.184, val_acc:0.907]
Epoch [30/120    avg_loss:0.110, val_acc:0.935]
Epoch [31/120    avg_loss:0.107, val_acc:0.911]
Epoch [32/120    avg_loss:0.111, val_acc:0.919]
Epoch [33/120    avg_loss:0.092, val_acc:0.892]
Epoch [34/120    avg_loss:0.100, val_acc:0.950]
Epoch [35/120    avg_loss:0.089, val_acc:0.902]
Epoch [36/120    avg_loss:0.136, val_acc:0.948]
Epoch [37/120    avg_loss:0.083, val_acc:0.952]
Epoch [38/120    avg_loss:0.116, val_acc:0.943]
Epoch [39/120    avg_loss:0.108, val_acc:0.951]
Epoch [40/120    avg_loss:0.072, val_acc:0.913]
Epoch [41/120    avg_loss:0.051, val_acc:0.934]
Epoch [42/120    avg_loss:0.059, val_acc:0.947]
Epoch [43/120    avg_loss:0.055, val_acc:0.933]
Epoch [44/120    avg_loss:0.071, val_acc:0.925]
Epoch [45/120    avg_loss:0.045, val_acc:0.958]
Epoch [46/120    avg_loss:0.064, val_acc:0.940]
Epoch [47/120    avg_loss:0.049, val_acc:0.951]
Epoch [48/120    avg_loss:0.031, val_acc:0.956]
Epoch [49/120    avg_loss:0.030, val_acc:0.964]
Epoch [50/120    avg_loss:0.053, val_acc:0.959]
Epoch [51/120    avg_loss:0.037, val_acc:0.955]
Epoch [52/120    avg_loss:0.035, val_acc:0.964]
Epoch [53/120    avg_loss:0.034, val_acc:0.957]
Epoch [54/120    avg_loss:0.028, val_acc:0.950]
Epoch [55/120    avg_loss:0.051, val_acc:0.955]
Epoch [56/120    avg_loss:0.046, val_acc:0.965]
Epoch [57/120    avg_loss:0.042, val_acc:0.946]
Epoch [58/120    avg_loss:0.029, val_acc:0.965]
Epoch [59/120    avg_loss:0.104, val_acc:0.935]
Epoch [60/120    avg_loss:0.100, val_acc:0.905]
Epoch [61/120    avg_loss:0.129, val_acc:0.923]
Epoch [62/120    avg_loss:0.309, val_acc:0.921]
Epoch [63/120    avg_loss:0.077, val_acc:0.933]
Epoch [64/120    avg_loss:0.040, val_acc:0.971]
Epoch [65/120    avg_loss:0.034, val_acc:0.966]
Epoch [66/120    avg_loss:0.041, val_acc:0.963]
Epoch [67/120    avg_loss:0.021, val_acc:0.961]
Epoch [68/120    avg_loss:0.020, val_acc:0.962]
Epoch [69/120    avg_loss:0.020, val_acc:0.962]
Epoch [70/120    avg_loss:0.012, val_acc:0.968]
Epoch [71/120    avg_loss:0.020, val_acc:0.954]
Epoch [72/120    avg_loss:0.018, val_acc:0.977]
Epoch [73/120    avg_loss:0.012, val_acc:0.974]
Epoch [74/120    avg_loss:0.017, val_acc:0.971]
Epoch [75/120    avg_loss:0.015, val_acc:0.963]
Epoch [76/120    avg_loss:0.015, val_acc:0.972]
Epoch [77/120    avg_loss:0.012, val_acc:0.975]
Epoch [78/120    avg_loss:0.013, val_acc:0.965]
Epoch [79/120    avg_loss:0.014, val_acc:0.972]
Epoch [80/120    avg_loss:0.021, val_acc:0.972]
Epoch [81/120    avg_loss:0.010, val_acc:0.970]
Epoch [82/120    avg_loss:0.021, val_acc:0.974]
Epoch [83/120    avg_loss:0.017, val_acc:0.966]
Epoch [84/120    avg_loss:0.017, val_acc:0.903]
Epoch [85/120    avg_loss:0.116, val_acc:0.926]
Epoch [86/120    avg_loss:0.070, val_acc:0.956]
Epoch [87/120    avg_loss:0.029, val_acc:0.961]
Epoch [88/120    avg_loss:0.029, val_acc:0.963]
Epoch [89/120    avg_loss:0.022, val_acc:0.964]
Epoch [90/120    avg_loss:0.020, val_acc:0.967]
Epoch [91/120    avg_loss:0.020, val_acc:0.968]
Epoch [92/120    avg_loss:0.014, val_acc:0.969]
Epoch [93/120    avg_loss:0.017, val_acc:0.968]
Epoch [94/120    avg_loss:0.018, val_acc:0.970]
Epoch [95/120    avg_loss:0.014, val_acc:0.972]
Epoch [96/120    avg_loss:0.014, val_acc:0.974]
Epoch [97/120    avg_loss:0.010, val_acc:0.971]
Epoch [98/120    avg_loss:0.012, val_acc:0.971]
Epoch [99/120    avg_loss:0.010, val_acc:0.971]
Epoch [100/120    avg_loss:0.010, val_acc:0.971]
Epoch [101/120    avg_loss:0.011, val_acc:0.972]
Epoch [102/120    avg_loss:0.014, val_acc:0.972]
Epoch [103/120    avg_loss:0.010, val_acc:0.971]
Epoch [104/120    avg_loss:0.013, val_acc:0.971]
Epoch [105/120    avg_loss:0.010, val_acc:0.971]
Epoch [106/120    avg_loss:0.014, val_acc:0.972]
Epoch [107/120    avg_loss:0.014, val_acc:0.973]
Epoch [108/120    avg_loss:0.013, val_acc:0.972]
Epoch [109/120    avg_loss:0.015, val_acc:0.972]
Epoch [110/120    avg_loss:0.010, val_acc:0.972]
Epoch [111/120    avg_loss:0.012, val_acc:0.973]
Epoch [112/120    avg_loss:0.010, val_acc:0.973]
Epoch [113/120    avg_loss:0.010, val_acc:0.973]
Epoch [114/120    avg_loss:0.013, val_acc:0.973]
Epoch [115/120    avg_loss:0.010, val_acc:0.973]
Epoch [116/120    avg_loss:0.011, val_acc:0.972]
Epoch [117/120    avg_loss:0.011, val_acc:0.973]
Epoch [118/120    avg_loss:0.017, val_acc:0.972]
Epoch [119/120    avg_loss:0.012, val_acc:0.972]
Epoch [120/120    avg_loss:0.012, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1241    3   10    0    0    0    0    0    7   22    2    0
     0    0    0]
 [   0    0    1  734    2    0    0    0    0    0    0    0   10    0
     0    0    0]
 [   0    0    0    1  209    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    0    0    0
     4    1    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    1    0    0    0    0  843   27    1    0
     0    0    0]
 [   0    0   11    0    0    0    0    0    0    0   16 2150   32    0
     0    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1  532    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1121   17    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    94  249    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.96476964769647

F1 scores:
[       nan 1.         0.97678079 0.98855219 0.96313364 0.99192618
 0.99467681 1.         1.         1.         0.96840896 0.97461469
 0.95426009 1.         0.95040271 0.8097561  0.98809524]

Kappa:
0.9653856679163696
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7f4e1ff7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.402, val_acc:0.512]
Epoch [2/120    avg_loss:1.856, val_acc:0.517]
Epoch [3/120    avg_loss:1.503, val_acc:0.592]
Epoch [4/120    avg_loss:1.243, val_acc:0.639]
Epoch [5/120    avg_loss:1.102, val_acc:0.681]
Epoch [6/120    avg_loss:0.955, val_acc:0.675]
Epoch [7/120    avg_loss:0.919, val_acc:0.675]
Epoch [8/120    avg_loss:0.699, val_acc:0.734]
Epoch [9/120    avg_loss:0.660, val_acc:0.767]
Epoch [10/120    avg_loss:0.633, val_acc:0.761]
Epoch [11/120    avg_loss:0.560, val_acc:0.787]
Epoch [12/120    avg_loss:0.561, val_acc:0.775]
Epoch [13/120    avg_loss:0.503, val_acc:0.818]
Epoch [14/120    avg_loss:0.424, val_acc:0.759]
Epoch [15/120    avg_loss:0.433, val_acc:0.849]
Epoch [16/120    avg_loss:0.358, val_acc:0.836]
Epoch [17/120    avg_loss:0.319, val_acc:0.842]
Epoch [18/120    avg_loss:0.273, val_acc:0.890]
Epoch [19/120    avg_loss:0.258, val_acc:0.898]
Epoch [20/120    avg_loss:0.228, val_acc:0.897]
Epoch [21/120    avg_loss:0.217, val_acc:0.864]
Epoch [22/120    avg_loss:0.187, val_acc:0.894]
Epoch [23/120    avg_loss:0.178, val_acc:0.903]
Epoch [24/120    avg_loss:0.222, val_acc:0.877]
Epoch [25/120    avg_loss:0.233, val_acc:0.907]
Epoch [26/120    avg_loss:0.212, val_acc:0.873]
Epoch [27/120    avg_loss:0.274, val_acc:0.910]
Epoch [28/120    avg_loss:0.130, val_acc:0.931]
Epoch [29/120    avg_loss:0.167, val_acc:0.877]
Epoch [30/120    avg_loss:0.112, val_acc:0.932]
Epoch [31/120    avg_loss:0.136, val_acc:0.914]
Epoch [32/120    avg_loss:0.129, val_acc:0.934]
Epoch [33/120    avg_loss:0.116, val_acc:0.934]
Epoch [34/120    avg_loss:0.094, val_acc:0.954]
Epoch [35/120    avg_loss:0.151, val_acc:0.931]
Epoch [36/120    avg_loss:0.120, val_acc:0.929]
Epoch [37/120    avg_loss:0.071, val_acc:0.941]
Epoch [38/120    avg_loss:0.074, val_acc:0.954]
Epoch [39/120    avg_loss:0.080, val_acc:0.947]
Epoch [40/120    avg_loss:0.087, val_acc:0.947]
Epoch [41/120    avg_loss:0.080, val_acc:0.951]
Epoch [42/120    avg_loss:0.077, val_acc:0.934]
Epoch [43/120    avg_loss:0.058, val_acc:0.965]
Epoch [44/120    avg_loss:0.062, val_acc:0.912]
Epoch [45/120    avg_loss:0.077, val_acc:0.940]
Epoch [46/120    avg_loss:0.073, val_acc:0.934]
Epoch [47/120    avg_loss:0.115, val_acc:0.926]
Epoch [48/120    avg_loss:0.129, val_acc:0.930]
Epoch [49/120    avg_loss:0.052, val_acc:0.952]
Epoch [50/120    avg_loss:0.053, val_acc:0.963]
Epoch [51/120    avg_loss:0.045, val_acc:0.949]
Epoch [52/120    avg_loss:0.069, val_acc:0.939]
Epoch [53/120    avg_loss:0.044, val_acc:0.948]
Epoch [54/120    avg_loss:0.044, val_acc:0.969]
Epoch [55/120    avg_loss:0.040, val_acc:0.968]
Epoch [56/120    avg_loss:0.044, val_acc:0.950]
Epoch [57/120    avg_loss:0.045, val_acc:0.963]
Epoch [58/120    avg_loss:0.168, val_acc:0.944]
Epoch [59/120    avg_loss:0.072, val_acc:0.951]
Epoch [60/120    avg_loss:0.052, val_acc:0.947]
Epoch [61/120    avg_loss:0.061, val_acc:0.955]
Epoch [62/120    avg_loss:0.041, val_acc:0.954]
Epoch [63/120    avg_loss:0.054, val_acc:0.963]
Epoch [64/120    avg_loss:0.030, val_acc:0.942]
Epoch [65/120    avg_loss:0.039, val_acc:0.958]
Epoch [66/120    avg_loss:0.051, val_acc:0.966]
Epoch [67/120    avg_loss:0.024, val_acc:0.973]
Epoch [68/120    avg_loss:0.025, val_acc:0.972]
Epoch [69/120    avg_loss:0.033, val_acc:0.959]
Epoch [70/120    avg_loss:0.022, val_acc:0.974]
Epoch [71/120    avg_loss:0.037, val_acc:0.971]
Epoch [72/120    avg_loss:0.021, val_acc:0.961]
Epoch [73/120    avg_loss:0.025, val_acc:0.953]
Epoch [74/120    avg_loss:0.037, val_acc:0.970]
Epoch [75/120    avg_loss:0.029, val_acc:0.969]
Epoch [76/120    avg_loss:0.025, val_acc:0.961]
Epoch [77/120    avg_loss:0.018, val_acc:0.976]
Epoch [78/120    avg_loss:0.011, val_acc:0.974]
Epoch [79/120    avg_loss:0.017, val_acc:0.971]
Epoch [80/120    avg_loss:0.021, val_acc:0.968]
Epoch [81/120    avg_loss:0.023, val_acc:0.970]
Epoch [82/120    avg_loss:0.020, val_acc:0.965]
Epoch [83/120    avg_loss:0.032, val_acc:0.964]
Epoch [84/120    avg_loss:0.544, val_acc:0.924]
Epoch [85/120    avg_loss:0.064, val_acc:0.967]
Epoch [86/120    avg_loss:0.033, val_acc:0.967]
Epoch [87/120    avg_loss:0.028, val_acc:0.973]
Epoch [88/120    avg_loss:0.030, val_acc:0.965]
Epoch [89/120    avg_loss:0.027, val_acc:0.971]
Epoch [90/120    avg_loss:0.032, val_acc:0.967]
Epoch [91/120    avg_loss:0.025, val_acc:0.970]
Epoch [92/120    avg_loss:0.028, val_acc:0.973]
Epoch [93/120    avg_loss:0.015, val_acc:0.973]
Epoch [94/120    avg_loss:0.014, val_acc:0.974]
Epoch [95/120    avg_loss:0.015, val_acc:0.975]
Epoch [96/120    avg_loss:0.014, val_acc:0.975]
Epoch [97/120    avg_loss:0.014, val_acc:0.977]
Epoch [98/120    avg_loss:0.014, val_acc:0.977]
Epoch [99/120    avg_loss:0.015, val_acc:0.979]
Epoch [100/120    avg_loss:0.012, val_acc:0.978]
Epoch [101/120    avg_loss:0.012, val_acc:0.978]
Epoch [102/120    avg_loss:0.016, val_acc:0.979]
Epoch [103/120    avg_loss:0.010, val_acc:0.979]
Epoch [104/120    avg_loss:0.015, val_acc:0.980]
Epoch [105/120    avg_loss:0.011, val_acc:0.980]
Epoch [106/120    avg_loss:0.015, val_acc:0.980]
Epoch [107/120    avg_loss:0.012, val_acc:0.980]
Epoch [108/120    avg_loss:0.017, val_acc:0.979]
Epoch [109/120    avg_loss:0.015, val_acc:0.978]
Epoch [110/120    avg_loss:0.008, val_acc:0.981]
Epoch [111/120    avg_loss:0.010, val_acc:0.980]
Epoch [112/120    avg_loss:0.008, val_acc:0.981]
Epoch [113/120    avg_loss:0.010, val_acc:0.981]
Epoch [114/120    avg_loss:0.009, val_acc:0.981]
Epoch [115/120    avg_loss:0.009, val_acc:0.982]
Epoch [116/120    avg_loss:0.013, val_acc:0.982]
Epoch [117/120    avg_loss:0.009, val_acc:0.982]
Epoch [118/120    avg_loss:0.009, val_acc:0.982]
Epoch [119/120    avg_loss:0.008, val_acc:0.982]
Epoch [120/120    avg_loss:0.010, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1262    3    8    0    0    0    0    1    1   10    0    0
     0    0    0]
 [   0    0    0  734    1    1    0    0    0    6    0    0    4    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    0    0    0    0    1    0    0
     9    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   11    0    0    1    1    0    0    0  844   17    0    0
     0    1    0]
 [   0    0   15    1    0    0    0    0    0    1   12 2180    0    0
     1    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0    0  529    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1125   13    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    44  303    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.12466124661246

F1 scores:
[       nan 0.98765432 0.98095608 0.9872226  0.97931034 0.98607889
 0.99771516 1.         1.         0.8        0.97403347 0.98687189
 0.99156514 1.         0.96940974 0.90990991 0.99408284]

Kappa:
0.9786148392145579
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd1bdf91860>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.424, val_acc:0.469]
Epoch [2/120    avg_loss:1.907, val_acc:0.581]
Epoch [3/120    avg_loss:1.602, val_acc:0.558]
Epoch [4/120    avg_loss:1.362, val_acc:0.627]
Epoch [5/120    avg_loss:1.270, val_acc:0.635]
Epoch [6/120    avg_loss:1.049, val_acc:0.683]
Epoch [7/120    avg_loss:0.920, val_acc:0.762]
Epoch [8/120    avg_loss:0.789, val_acc:0.775]
Epoch [9/120    avg_loss:0.758, val_acc:0.784]
Epoch [10/120    avg_loss:0.604, val_acc:0.810]
Epoch [11/120    avg_loss:0.574, val_acc:0.821]
Epoch [12/120    avg_loss:0.456, val_acc:0.860]
Epoch [13/120    avg_loss:0.455, val_acc:0.852]
Epoch [14/120    avg_loss:0.418, val_acc:0.813]
Epoch [15/120    avg_loss:0.339, val_acc:0.862]
Epoch [16/120    avg_loss:0.322, val_acc:0.896]
Epoch [17/120    avg_loss:0.371, val_acc:0.871]
Epoch [18/120    avg_loss:0.364, val_acc:0.864]
Epoch [19/120    avg_loss:0.260, val_acc:0.907]
Epoch [20/120    avg_loss:0.216, val_acc:0.878]
Epoch [21/120    avg_loss:0.294, val_acc:0.904]
Epoch [22/120    avg_loss:0.249, val_acc:0.921]
Epoch [23/120    avg_loss:0.167, val_acc:0.910]
Epoch [24/120    avg_loss:0.168, val_acc:0.899]
Epoch [25/120    avg_loss:0.221, val_acc:0.927]
Epoch [26/120    avg_loss:0.194, val_acc:0.908]
Epoch [27/120    avg_loss:0.196, val_acc:0.911]
Epoch [28/120    avg_loss:0.165, val_acc:0.926]
Epoch [29/120    avg_loss:0.142, val_acc:0.928]
Epoch [30/120    avg_loss:0.146, val_acc:0.940]
Epoch [31/120    avg_loss:0.129, val_acc:0.940]
Epoch [32/120    avg_loss:0.105, val_acc:0.946]
Epoch [33/120    avg_loss:0.096, val_acc:0.945]
Epoch [34/120    avg_loss:0.109, val_acc:0.943]
Epoch [35/120    avg_loss:0.100, val_acc:0.949]
Epoch [36/120    avg_loss:0.084, val_acc:0.947]
Epoch [37/120    avg_loss:0.098, val_acc:0.945]
Epoch [38/120    avg_loss:0.140, val_acc:0.949]
Epoch [39/120    avg_loss:0.103, val_acc:0.956]
Epoch [40/120    avg_loss:0.091, val_acc:0.940]
Epoch [41/120    avg_loss:0.082, val_acc:0.958]
Epoch [42/120    avg_loss:0.074, val_acc:0.954]
Epoch [43/120    avg_loss:0.041, val_acc:0.958]
Epoch [44/120    avg_loss:0.314, val_acc:0.916]
Epoch [45/120    avg_loss:0.111, val_acc:0.958]
Epoch [46/120    avg_loss:0.051, val_acc:0.965]
Epoch [47/120    avg_loss:0.066, val_acc:0.949]
Epoch [48/120    avg_loss:0.076, val_acc:0.965]
Epoch [49/120    avg_loss:0.038, val_acc:0.966]
Epoch [50/120    avg_loss:0.048, val_acc:0.963]
Epoch [51/120    avg_loss:0.053, val_acc:0.959]
Epoch [52/120    avg_loss:0.024, val_acc:0.969]
Epoch [53/120    avg_loss:0.041, val_acc:0.963]
Epoch [54/120    avg_loss:0.034, val_acc:0.974]
Epoch [55/120    avg_loss:0.033, val_acc:0.969]
Epoch [56/120    avg_loss:0.033, val_acc:0.964]
Epoch [57/120    avg_loss:0.030, val_acc:0.966]
Epoch [58/120    avg_loss:0.022, val_acc:0.968]
Epoch [59/120    avg_loss:0.040, val_acc:0.960]
Epoch [60/120    avg_loss:0.067, val_acc:0.964]
Epoch [61/120    avg_loss:0.063, val_acc:0.939]
Epoch [62/120    avg_loss:0.048, val_acc:0.956]
Epoch [63/120    avg_loss:0.143, val_acc:0.925]
Epoch [64/120    avg_loss:0.082, val_acc:0.949]
Epoch [65/120    avg_loss:0.067, val_acc:0.966]
Epoch [66/120    avg_loss:0.030, val_acc:0.959]
Epoch [67/120    avg_loss:0.031, val_acc:0.970]
Epoch [68/120    avg_loss:0.019, val_acc:0.969]
Epoch [69/120    avg_loss:0.021, val_acc:0.972]
Epoch [70/120    avg_loss:0.021, val_acc:0.976]
Epoch [71/120    avg_loss:0.016, val_acc:0.975]
Epoch [72/120    avg_loss:0.013, val_acc:0.974]
Epoch [73/120    avg_loss:0.012, val_acc:0.975]
Epoch [74/120    avg_loss:0.015, val_acc:0.978]
Epoch [75/120    avg_loss:0.015, val_acc:0.980]
Epoch [76/120    avg_loss:0.019, val_acc:0.977]
Epoch [77/120    avg_loss:0.019, val_acc:0.977]
Epoch [78/120    avg_loss:0.018, val_acc:0.977]
Epoch [79/120    avg_loss:0.012, val_acc:0.976]
Epoch [80/120    avg_loss:0.010, val_acc:0.976]
Epoch [81/120    avg_loss:0.013, val_acc:0.974]
Epoch [82/120    avg_loss:0.012, val_acc:0.978]
Epoch [83/120    avg_loss:0.011, val_acc:0.976]
Epoch [84/120    avg_loss:0.010, val_acc:0.979]
Epoch [85/120    avg_loss:0.013, val_acc:0.979]
Epoch [86/120    avg_loss:0.011, val_acc:0.978]
Epoch [87/120    avg_loss:0.011, val_acc:0.980]
Epoch [88/120    avg_loss:0.010, val_acc:0.977]
Epoch [89/120    avg_loss:0.012, val_acc:0.978]
Epoch [90/120    avg_loss:0.011, val_acc:0.978]
Epoch [91/120    avg_loss:0.014, val_acc:0.981]
Epoch [92/120    avg_loss:0.011, val_acc:0.975]
Epoch [93/120    avg_loss:0.012, val_acc:0.975]
Epoch [94/120    avg_loss:0.014, val_acc:0.977]
Epoch [95/120    avg_loss:0.011, val_acc:0.978]
Epoch [96/120    avg_loss:0.011, val_acc:0.977]
Epoch [97/120    avg_loss:0.012, val_acc:0.976]
Epoch [98/120    avg_loss:0.010, val_acc:0.981]
Epoch [99/120    avg_loss:0.019, val_acc:0.975]
Epoch [100/120    avg_loss:0.012, val_acc:0.978]
Epoch [101/120    avg_loss:0.014, val_acc:0.979]
Epoch [102/120    avg_loss:0.011, val_acc:0.977]
Epoch [103/120    avg_loss:0.012, val_acc:0.979]
Epoch [104/120    avg_loss:0.012, val_acc:0.979]
Epoch [105/120    avg_loss:0.014, val_acc:0.978]
Epoch [106/120    avg_loss:0.015, val_acc:0.978]
Epoch [107/120    avg_loss:0.009, val_acc:0.978]
Epoch [108/120    avg_loss:0.010, val_acc:0.976]
Epoch [109/120    avg_loss:0.010, val_acc:0.977]
Epoch [110/120    avg_loss:0.012, val_acc:0.978]
Epoch [111/120    avg_loss:0.010, val_acc:0.976]
Epoch [112/120    avg_loss:0.009, val_acc:0.975]
Epoch [113/120    avg_loss:0.010, val_acc:0.975]
Epoch [114/120    avg_loss:0.014, val_acc:0.975]
Epoch [115/120    avg_loss:0.012, val_acc:0.976]
Epoch [116/120    avg_loss:0.010, val_acc:0.975]
Epoch [117/120    avg_loss:0.010, val_acc:0.976]
Epoch [118/120    avg_loss:0.012, val_acc:0.976]
Epoch [119/120    avg_loss:0.009, val_acc:0.976]
Epoch [120/120    avg_loss:0.010, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    0    0    0    0
     0    1    0]
 [   0    0 1274    1    3    0    0    0    0    0    0    7    0    0
     0    0    0]
 [   0    0    0  733    1    3    0    0    0    0    0    8    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   15    0    1    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  836   32    0    0
     1    0    0]
 [   0    0    7    1    0    2    0    0    0    0   12 2171   16    0
     1    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0   10  519    0
     0    2    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    0    0    0    0
  1127   10    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    88  254    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.46341463414635

F1 scores:
[       nan 0.98765432 0.99066874 0.98853675 0.99069767 0.98969072
 0.99317665 1.         0.99767981 0.90909091 0.97040046 0.97792793
 0.96828358 0.99728997 0.95508475 0.82736156 0.98224852]

Kappa:
0.9710461756526914
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f950a83d7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.413, val_acc:0.508]
Epoch [2/120    avg_loss:1.824, val_acc:0.591]
Epoch [3/120    avg_loss:1.496, val_acc:0.627]
Epoch [4/120    avg_loss:1.228, val_acc:0.617]
Epoch [5/120    avg_loss:1.079, val_acc:0.703]
Epoch [6/120    avg_loss:0.920, val_acc:0.749]
Epoch [7/120    avg_loss:0.821, val_acc:0.770]
Epoch [8/120    avg_loss:0.667, val_acc:0.770]
Epoch [9/120    avg_loss:0.622, val_acc:0.798]
Epoch [10/120    avg_loss:0.519, val_acc:0.832]
Epoch [11/120    avg_loss:0.509, val_acc:0.816]
Epoch [12/120    avg_loss:0.413, val_acc:0.828]
Epoch [13/120    avg_loss:0.414, val_acc:0.855]
Epoch [14/120    avg_loss:0.322, val_acc:0.838]
Epoch [15/120    avg_loss:0.339, val_acc:0.860]
Epoch [16/120    avg_loss:0.325, val_acc:0.857]
Epoch [17/120    avg_loss:0.257, val_acc:0.844]
Epoch [18/120    avg_loss:0.248, val_acc:0.858]
Epoch [19/120    avg_loss:0.266, val_acc:0.917]
Epoch [20/120    avg_loss:0.223, val_acc:0.912]
Epoch [21/120    avg_loss:0.224, val_acc:0.873]
Epoch [22/120    avg_loss:0.197, val_acc:0.899]
Epoch [23/120    avg_loss:0.159, val_acc:0.897]
Epoch [24/120    avg_loss:0.162, val_acc:0.916]
Epoch [25/120    avg_loss:0.138, val_acc:0.925]
Epoch [26/120    avg_loss:0.137, val_acc:0.894]
Epoch [27/120    avg_loss:0.263, val_acc:0.921]
Epoch [28/120    avg_loss:0.182, val_acc:0.924]
Epoch [29/120    avg_loss:0.104, val_acc:0.933]
Epoch [30/120    avg_loss:0.098, val_acc:0.906]
Epoch [31/120    avg_loss:0.095, val_acc:0.936]
Epoch [32/120    avg_loss:0.136, val_acc:0.945]
Epoch [33/120    avg_loss:0.085, val_acc:0.944]
Epoch [34/120    avg_loss:0.138, val_acc:0.932]
Epoch [35/120    avg_loss:0.105, val_acc:0.941]
Epoch [36/120    avg_loss:0.083, val_acc:0.950]
Epoch [37/120    avg_loss:0.068, val_acc:0.961]
Epoch [38/120    avg_loss:0.061, val_acc:0.954]
Epoch [39/120    avg_loss:0.085, val_acc:0.952]
Epoch [40/120    avg_loss:0.054, val_acc:0.950]
Epoch [41/120    avg_loss:0.072, val_acc:0.944]
Epoch [42/120    avg_loss:0.056, val_acc:0.950]
Epoch [43/120    avg_loss:0.072, val_acc:0.952]
Epoch [44/120    avg_loss:0.062, val_acc:0.947]
Epoch [45/120    avg_loss:0.048, val_acc:0.959]
Epoch [46/120    avg_loss:0.036, val_acc:0.965]
Epoch [47/120    avg_loss:0.051, val_acc:0.950]
Epoch [48/120    avg_loss:0.039, val_acc:0.952]
Epoch [49/120    avg_loss:0.040, val_acc:0.931]
Epoch [50/120    avg_loss:0.048, val_acc:0.953]
Epoch [51/120    avg_loss:0.034, val_acc:0.966]
Epoch [52/120    avg_loss:0.040, val_acc:0.971]
Epoch [53/120    avg_loss:0.079, val_acc:0.925]
Epoch [54/120    avg_loss:0.053, val_acc:0.966]
Epoch [55/120    avg_loss:0.037, val_acc:0.972]
Epoch [56/120    avg_loss:0.028, val_acc:0.973]
Epoch [57/120    avg_loss:0.021, val_acc:0.972]
Epoch [58/120    avg_loss:0.324, val_acc:0.844]
Epoch [59/120    avg_loss:0.130, val_acc:0.936]
Epoch [60/120    avg_loss:0.103, val_acc:0.960]
Epoch [61/120    avg_loss:0.043, val_acc:0.945]
Epoch [62/120    avg_loss:0.071, val_acc:0.957]
Epoch [63/120    avg_loss:0.050, val_acc:0.955]
Epoch [64/120    avg_loss:0.039, val_acc:0.967]
Epoch [65/120    avg_loss:0.028, val_acc:0.960]
Epoch [66/120    avg_loss:0.025, val_acc:0.972]
Epoch [67/120    avg_loss:0.026, val_acc:0.977]
Epoch [68/120    avg_loss:0.056, val_acc:0.958]
Epoch [69/120    avg_loss:0.079, val_acc:0.958]
Epoch [70/120    avg_loss:0.047, val_acc:0.954]
Epoch [71/120    avg_loss:0.043, val_acc:0.970]
Epoch [72/120    avg_loss:0.040, val_acc:0.955]
Epoch [73/120    avg_loss:0.044, val_acc:0.972]
Epoch [74/120    avg_loss:0.026, val_acc:0.966]
Epoch [75/120    avg_loss:0.061, val_acc:0.952]
Epoch [76/120    avg_loss:0.052, val_acc:0.968]
Epoch [77/120    avg_loss:0.038, val_acc:0.965]
Epoch [78/120    avg_loss:0.022, val_acc:0.950]
Epoch [79/120    avg_loss:0.017, val_acc:0.981]
Epoch [80/120    avg_loss:0.014, val_acc:0.985]
Epoch [81/120    avg_loss:0.028, val_acc:0.974]
Epoch [82/120    avg_loss:0.029, val_acc:0.976]
Epoch [83/120    avg_loss:0.049, val_acc:0.959]
Epoch [84/120    avg_loss:0.038, val_acc:0.976]
Epoch [85/120    avg_loss:0.024, val_acc:0.977]
Epoch [86/120    avg_loss:0.018, val_acc:0.974]
Epoch [87/120    avg_loss:0.016, val_acc:0.983]
Epoch [88/120    avg_loss:0.011, val_acc:0.982]
Epoch [89/120    avg_loss:0.024, val_acc:0.978]
Epoch [90/120    avg_loss:0.015, val_acc:0.977]
Epoch [91/120    avg_loss:0.015, val_acc:0.979]
Epoch [92/120    avg_loss:0.010, val_acc:0.973]
Epoch [93/120    avg_loss:0.008, val_acc:0.981]
Epoch [94/120    avg_loss:0.010, val_acc:0.983]
Epoch [95/120    avg_loss:0.007, val_acc:0.983]
Epoch [96/120    avg_loss:0.006, val_acc:0.984]
Epoch [97/120    avg_loss:0.006, val_acc:0.985]
Epoch [98/120    avg_loss:0.007, val_acc:0.986]
Epoch [99/120    avg_loss:0.007, val_acc:0.983]
Epoch [100/120    avg_loss:0.009, val_acc:0.983]
Epoch [101/120    avg_loss:0.006, val_acc:0.983]
Epoch [102/120    avg_loss:0.007, val_acc:0.981]
Epoch [103/120    avg_loss:0.006, val_acc:0.982]
Epoch [104/120    avg_loss:0.007, val_acc:0.982]
Epoch [105/120    avg_loss:0.007, val_acc:0.984]
Epoch [106/120    avg_loss:0.005, val_acc:0.984]
Epoch [107/120    avg_loss:0.005, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.983]
Epoch [109/120    avg_loss:0.005, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.984]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.006, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.005, val_acc:0.984]
Epoch [115/120    avg_loss:0.005, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.008, val_acc:0.984]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.007, val_acc:0.985]
Epoch [120/120    avg_loss:0.008, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1262    3    0    5    0    0    0    0    3   12    0    0
     0    0    0]
 [   0    0    0  725    0    0    0    0    0    0    0   13    8    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    2    0    0    0    0    0    0  845   23    0    0
     1    0    0]
 [   0    0    0    0    0    0    0    0    0    3   10 2183   12    1
     0    1    0]
 [   0    0    0    4    2    0    0    0    0    0    0    3  522    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0   18    0    0    1    0    0    0    0
    62  266    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.65853658536585

F1 scores:
[       nan 0.975      0.98941592 0.9790682  0.9953271  0.99313501
 0.98419865 1.         1.         0.9        0.9740634  0.9820063
 0.97026022 0.99730458 0.96563574 0.84310618 0.98823529]

Kappa:
0.9732830232880433
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa4cd4ae898>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.446, val_acc:0.402]
Epoch [2/120    avg_loss:1.868, val_acc:0.563]
Epoch [3/120    avg_loss:1.570, val_acc:0.625]
Epoch [4/120    avg_loss:1.338, val_acc:0.574]
Epoch [5/120    avg_loss:1.163, val_acc:0.666]
Epoch [6/120    avg_loss:1.037, val_acc:0.718]
Epoch [7/120    avg_loss:0.842, val_acc:0.733]
Epoch [8/120    avg_loss:0.730, val_acc:0.762]
Epoch [9/120    avg_loss:0.633, val_acc:0.739]
Epoch [10/120    avg_loss:0.606, val_acc:0.769]
Epoch [11/120    avg_loss:0.590, val_acc:0.791]
Epoch [12/120    avg_loss:0.567, val_acc:0.798]
Epoch [13/120    avg_loss:0.505, val_acc:0.836]
Epoch [14/120    avg_loss:0.355, val_acc:0.857]
Epoch [15/120    avg_loss:0.392, val_acc:0.820]
Epoch [16/120    avg_loss:0.318, val_acc:0.883]
Epoch [17/120    avg_loss:0.275, val_acc:0.876]
Epoch [18/120    avg_loss:0.311, val_acc:0.853]
Epoch [19/120    avg_loss:0.268, val_acc:0.897]
Epoch [20/120    avg_loss:0.207, val_acc:0.905]
Epoch [21/120    avg_loss:0.233, val_acc:0.893]
Epoch [22/120    avg_loss:0.194, val_acc:0.920]
Epoch [23/120    avg_loss:0.171, val_acc:0.908]
Epoch [24/120    avg_loss:0.216, val_acc:0.911]
Epoch [25/120    avg_loss:0.194, val_acc:0.884]
Epoch [26/120    avg_loss:0.157, val_acc:0.915]
Epoch [27/120    avg_loss:0.141, val_acc:0.927]
Epoch [28/120    avg_loss:0.202, val_acc:0.935]
Epoch [29/120    avg_loss:0.137, val_acc:0.916]
Epoch [30/120    avg_loss:0.102, val_acc:0.938]
Epoch [31/120    avg_loss:0.131, val_acc:0.909]
Epoch [32/120    avg_loss:0.125, val_acc:0.927]
Epoch [33/120    avg_loss:0.084, val_acc:0.935]
Epoch [34/120    avg_loss:0.104, val_acc:0.924]
Epoch [35/120    avg_loss:0.111, val_acc:0.928]
Epoch [36/120    avg_loss:0.084, val_acc:0.934]
Epoch [37/120    avg_loss:0.194, val_acc:0.933]
Epoch [38/120    avg_loss:0.283, val_acc:0.927]
Epoch [39/120    avg_loss:0.101, val_acc:0.939]
Epoch [40/120    avg_loss:0.107, val_acc:0.939]
Epoch [41/120    avg_loss:0.087, val_acc:0.924]
Epoch [42/120    avg_loss:0.171, val_acc:0.935]
Epoch [43/120    avg_loss:0.119, val_acc:0.921]
Epoch [44/120    avg_loss:0.077, val_acc:0.939]
Epoch [45/120    avg_loss:0.113, val_acc:0.906]
Epoch [46/120    avg_loss:0.126, val_acc:0.924]
Epoch [47/120    avg_loss:0.089, val_acc:0.952]
Epoch [48/120    avg_loss:0.074, val_acc:0.944]
Epoch [49/120    avg_loss:0.073, val_acc:0.952]
Epoch [50/120    avg_loss:0.078, val_acc:0.934]
Epoch [51/120    avg_loss:0.087, val_acc:0.951]
Epoch [52/120    avg_loss:0.064, val_acc:0.964]
Epoch [53/120    avg_loss:0.055, val_acc:0.963]
Epoch [54/120    avg_loss:0.036, val_acc:0.963]
Epoch [55/120    avg_loss:0.045, val_acc:0.961]
Epoch [56/120    avg_loss:0.052, val_acc:0.966]
Epoch [57/120    avg_loss:0.047, val_acc:0.958]
Epoch [58/120    avg_loss:0.053, val_acc:0.958]
Epoch [59/120    avg_loss:0.047, val_acc:0.961]
Epoch [60/120    avg_loss:0.044, val_acc:0.963]
Epoch [61/120    avg_loss:0.041, val_acc:0.967]
Epoch [62/120    avg_loss:0.036, val_acc:0.964]
Epoch [63/120    avg_loss:0.022, val_acc:0.974]
Epoch [64/120    avg_loss:0.029, val_acc:0.959]
Epoch [65/120    avg_loss:0.046, val_acc:0.960]
Epoch [66/120    avg_loss:0.035, val_acc:0.963]
Epoch [67/120    avg_loss:0.035, val_acc:0.969]
Epoch [68/120    avg_loss:0.037, val_acc:0.956]
Epoch [69/120    avg_loss:0.034, val_acc:0.959]
Epoch [70/120    avg_loss:0.024, val_acc:0.966]
Epoch [71/120    avg_loss:0.041, val_acc:0.970]
Epoch [72/120    avg_loss:0.031, val_acc:0.963]
Epoch [73/120    avg_loss:0.048, val_acc:0.945]
Epoch [74/120    avg_loss:0.058, val_acc:0.929]
Epoch [75/120    avg_loss:0.066, val_acc:0.948]
Epoch [76/120    avg_loss:0.059, val_acc:0.962]
Epoch [77/120    avg_loss:0.036, val_acc:0.968]
Epoch [78/120    avg_loss:0.024, val_acc:0.971]
Epoch [79/120    avg_loss:0.018, val_acc:0.972]
Epoch [80/120    avg_loss:0.019, val_acc:0.973]
Epoch [81/120    avg_loss:0.021, val_acc:0.974]
Epoch [82/120    avg_loss:0.028, val_acc:0.975]
Epoch [83/120    avg_loss:0.016, val_acc:0.974]
Epoch [84/120    avg_loss:0.021, val_acc:0.976]
Epoch [85/120    avg_loss:0.018, val_acc:0.974]
Epoch [86/120    avg_loss:0.017, val_acc:0.974]
Epoch [87/120    avg_loss:0.015, val_acc:0.974]
Epoch [88/120    avg_loss:0.017, val_acc:0.974]
Epoch [89/120    avg_loss:0.016, val_acc:0.974]
Epoch [90/120    avg_loss:0.020, val_acc:0.974]
Epoch [91/120    avg_loss:0.015, val_acc:0.974]
Epoch [92/120    avg_loss:0.017, val_acc:0.976]
Epoch [93/120    avg_loss:0.013, val_acc:0.975]
Epoch [94/120    avg_loss:0.016, val_acc:0.977]
Epoch [95/120    avg_loss:0.015, val_acc:0.975]
Epoch [96/120    avg_loss:0.014, val_acc:0.977]
Epoch [97/120    avg_loss:0.013, val_acc:0.976]
Epoch [98/120    avg_loss:0.014, val_acc:0.976]
Epoch [99/120    avg_loss:0.013, val_acc:0.977]
Epoch [100/120    avg_loss:0.014, val_acc:0.976]
Epoch [101/120    avg_loss:0.017, val_acc:0.975]
Epoch [102/120    avg_loss:0.013, val_acc:0.975]
Epoch [103/120    avg_loss:0.013, val_acc:0.977]
Epoch [104/120    avg_loss:0.012, val_acc:0.974]
Epoch [105/120    avg_loss:0.014, val_acc:0.974]
Epoch [106/120    avg_loss:0.013, val_acc:0.973]
Epoch [107/120    avg_loss:0.014, val_acc:0.973]
Epoch [108/120    avg_loss:0.015, val_acc:0.974]
Epoch [109/120    avg_loss:0.011, val_acc:0.975]
Epoch [110/120    avg_loss:0.010, val_acc:0.975]
Epoch [111/120    avg_loss:0.010, val_acc:0.975]
Epoch [112/120    avg_loss:0.012, val_acc:0.974]
Epoch [113/120    avg_loss:0.009, val_acc:0.974]
Epoch [114/120    avg_loss:0.017, val_acc:0.974]
Epoch [115/120    avg_loss:0.011, val_acc:0.973]
Epoch [116/120    avg_loss:0.008, val_acc:0.972]
Epoch [117/120    avg_loss:0.010, val_acc:0.972]
Epoch [118/120    avg_loss:0.010, val_acc:0.972]
Epoch [119/120    avg_loss:0.012, val_acc:0.972]
Epoch [120/120    avg_loss:0.010, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1272    1    0    7    0    0    0    1    1    2    1    0
     0    0    0]
 [   0    0    1  731    7    0    0    0    0    2    1    2    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    1    0    0    0    0  842   27    0    0
     1    1    0]
 [   0    0   14    1    0    0    1    0    0    0   23 2150   19    0
     1    0    1]
 [   0    0    1    6    0    1    0    0    0    0    0    0  522    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1122   17    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   103  244    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.18157181571816

F1 scores:
[       nan 0.97619048 0.98757764 0.98384926 0.98383372 0.98976109
 0.99771516 1.         0.997669   0.92307692 0.96670494 0.97905282
 0.96577243 1.         0.94803549 0.79869067 0.9704142 ]

Kappa:
0.9678602659503028
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1713be2828>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.411, val_acc:0.379]
Epoch [2/120    avg_loss:1.920, val_acc:0.587]
Epoch [3/120    avg_loss:1.574, val_acc:0.615]
Epoch [4/120    avg_loss:1.272, val_acc:0.634]
Epoch [5/120    avg_loss:1.107, val_acc:0.708]
Epoch [6/120    avg_loss:0.950, val_acc:0.748]
Epoch [7/120    avg_loss:0.818, val_acc:0.764]
Epoch [8/120    avg_loss:0.703, val_acc:0.802]
Epoch [9/120    avg_loss:0.684, val_acc:0.784]
Epoch [10/120    avg_loss:0.536, val_acc:0.805]
Epoch [11/120    avg_loss:0.572, val_acc:0.802]
Epoch [12/120    avg_loss:0.445, val_acc:0.825]
Epoch [13/120    avg_loss:0.390, val_acc:0.869]
Epoch [14/120    avg_loss:0.384, val_acc:0.861]
Epoch [15/120    avg_loss:0.390, val_acc:0.856]
Epoch [16/120    avg_loss:0.310, val_acc:0.870]
Epoch [17/120    avg_loss:0.297, val_acc:0.849]
Epoch [18/120    avg_loss:0.238, val_acc:0.913]
Epoch [19/120    avg_loss:0.200, val_acc:0.914]
Epoch [20/120    avg_loss:0.251, val_acc:0.866]
Epoch [21/120    avg_loss:0.240, val_acc:0.909]
Epoch [22/120    avg_loss:0.138, val_acc:0.903]
Epoch [23/120    avg_loss:0.135, val_acc:0.934]
Epoch [24/120    avg_loss:0.127, val_acc:0.922]
Epoch [25/120    avg_loss:0.187, val_acc:0.919]
Epoch [26/120    avg_loss:0.166, val_acc:0.904]
Epoch [27/120    avg_loss:0.162, val_acc:0.906]
Epoch [28/120    avg_loss:0.118, val_acc:0.931]
Epoch [29/120    avg_loss:0.096, val_acc:0.942]
Epoch [30/120    avg_loss:0.090, val_acc:0.959]
Epoch [31/120    avg_loss:0.119, val_acc:0.944]
Epoch [32/120    avg_loss:0.112, val_acc:0.942]
Epoch [33/120    avg_loss:0.092, val_acc:0.946]
Epoch [34/120    avg_loss:0.072, val_acc:0.941]
Epoch [35/120    avg_loss:0.121, val_acc:0.927]
Epoch [36/120    avg_loss:0.097, val_acc:0.929]
Epoch [37/120    avg_loss:0.158, val_acc:0.938]
Epoch [38/120    avg_loss:0.081, val_acc:0.940]
Epoch [39/120    avg_loss:0.057, val_acc:0.958]
Epoch [40/120    avg_loss:0.046, val_acc:0.965]
Epoch [41/120    avg_loss:0.087, val_acc:0.941]
Epoch [42/120    avg_loss:0.073, val_acc:0.940]
Epoch [43/120    avg_loss:0.071, val_acc:0.950]
Epoch [44/120    avg_loss:0.057, val_acc:0.958]
Epoch [45/120    avg_loss:0.053, val_acc:0.947]
Epoch [46/120    avg_loss:0.063, val_acc:0.951]
Epoch [47/120    avg_loss:0.052, val_acc:0.930]
Epoch [48/120    avg_loss:0.068, val_acc:0.960]
Epoch [49/120    avg_loss:0.044, val_acc:0.964]
Epoch [50/120    avg_loss:0.030, val_acc:0.963]
Epoch [51/120    avg_loss:0.037, val_acc:0.961]
Epoch [52/120    avg_loss:0.044, val_acc:0.967]
Epoch [53/120    avg_loss:0.029, val_acc:0.967]
Epoch [54/120    avg_loss:0.029, val_acc:0.961]
Epoch [55/120    avg_loss:0.026, val_acc:0.959]
Epoch [56/120    avg_loss:0.022, val_acc:0.961]
Epoch [57/120    avg_loss:0.026, val_acc:0.973]
Epoch [58/120    avg_loss:0.022, val_acc:0.966]
Epoch [59/120    avg_loss:0.021, val_acc:0.973]
Epoch [60/120    avg_loss:0.019, val_acc:0.969]
Epoch [61/120    avg_loss:0.014, val_acc:0.975]
Epoch [62/120    avg_loss:0.013, val_acc:0.973]
Epoch [63/120    avg_loss:0.011, val_acc:0.975]
Epoch [64/120    avg_loss:0.017, val_acc:0.974]
Epoch [65/120    avg_loss:0.014, val_acc:0.972]
Epoch [66/120    avg_loss:0.041, val_acc:0.969]
Epoch [67/120    avg_loss:0.039, val_acc:0.946]
Epoch [68/120    avg_loss:0.043, val_acc:0.965]
Epoch [69/120    avg_loss:0.027, val_acc:0.972]
Epoch [70/120    avg_loss:0.036, val_acc:0.953]
Epoch [71/120    avg_loss:0.029, val_acc:0.975]
Epoch [72/120    avg_loss:0.027, val_acc:0.967]
Epoch [73/120    avg_loss:0.072, val_acc:0.939]
Epoch [74/120    avg_loss:0.085, val_acc:0.956]
Epoch [75/120    avg_loss:0.035, val_acc:0.973]
Epoch [76/120    avg_loss:0.019, val_acc:0.974]
Epoch [77/120    avg_loss:0.016, val_acc:0.963]
Epoch [78/120    avg_loss:0.011, val_acc:0.970]
Epoch [79/120    avg_loss:0.029, val_acc:0.972]
Epoch [80/120    avg_loss:0.014, val_acc:0.976]
Epoch [81/120    avg_loss:0.009, val_acc:0.977]
Epoch [82/120    avg_loss:0.012, val_acc:0.970]
Epoch [83/120    avg_loss:0.016, val_acc:0.965]
Epoch [84/120    avg_loss:0.013, val_acc:0.977]
Epoch [85/120    avg_loss:0.008, val_acc:0.984]
Epoch [86/120    avg_loss:0.015, val_acc:0.984]
Epoch [87/120    avg_loss:0.010, val_acc:0.978]
Epoch [88/120    avg_loss:0.006, val_acc:0.985]
Epoch [89/120    avg_loss:0.013, val_acc:0.976]
Epoch [90/120    avg_loss:0.014, val_acc:0.978]
Epoch [91/120    avg_loss:0.014, val_acc:0.987]
Epoch [92/120    avg_loss:0.010, val_acc:0.980]
Epoch [93/120    avg_loss:0.021, val_acc:0.961]
Epoch [94/120    avg_loss:0.015, val_acc:0.982]
Epoch [95/120    avg_loss:0.007, val_acc:0.978]
Epoch [96/120    avg_loss:0.007, val_acc:0.980]
Epoch [97/120    avg_loss:0.017, val_acc:0.972]
Epoch [98/120    avg_loss:0.009, val_acc:0.984]
Epoch [99/120    avg_loss:0.005, val_acc:0.985]
Epoch [100/120    avg_loss:0.006, val_acc:0.985]
Epoch [101/120    avg_loss:0.008, val_acc:0.985]
Epoch [102/120    avg_loss:0.004, val_acc:0.985]
Epoch [103/120    avg_loss:0.005, val_acc:0.985]
Epoch [104/120    avg_loss:0.004, val_acc:0.982]
Epoch [105/120    avg_loss:0.004, val_acc:0.980]
Epoch [106/120    avg_loss:0.008, val_acc:0.983]
Epoch [107/120    avg_loss:0.003, val_acc:0.983]
Epoch [108/120    avg_loss:0.004, val_acc:0.984]
Epoch [109/120    avg_loss:0.003, val_acc:0.985]
Epoch [110/120    avg_loss:0.004, val_acc:0.984]
Epoch [111/120    avg_loss:0.003, val_acc:0.984]
Epoch [112/120    avg_loss:0.004, val_acc:0.985]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.004, val_acc:0.984]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.004, val_acc:0.985]
Epoch [117/120    avg_loss:0.004, val_acc:0.984]
Epoch [118/120    avg_loss:0.003, val_acc:0.984]
Epoch [119/120    avg_loss:0.003, val_acc:0.984]
Epoch [120/120    avg_loss:0.004, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    0    0    0    0
     0    1    0]
 [   0    0 1247    4    9    3    0    0    0    0    7   15    0    0
     0    0    0]
 [   0    0    0  741    1    0    0    0    0    0    0    1    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    1  655    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    0    0    0    0    0  847   19    0    0
     1    0    0]
 [   0    0    4    0    0    1    0    0    0    0    2 2197    3    0
     3    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0    1  530    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    88  256    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
97.87533875338754

F1 scores:
[       nan 0.98765432 0.98034591 0.99196787 0.97706422 0.99428571
 0.99619772 1.         1.         1.         0.97862507 0.98897142
 0.98513011 1.         0.95725772 0.83660131 0.96341463]

Kappa:
0.9757515897420574
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5982b877f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.425, val_acc:0.513]
Epoch [2/120    avg_loss:1.858, val_acc:0.597]
Epoch [3/120    avg_loss:1.553, val_acc:0.589]
Epoch [4/120    avg_loss:1.246, val_acc:0.671]
Epoch [5/120    avg_loss:1.055, val_acc:0.724]
Epoch [6/120    avg_loss:0.884, val_acc:0.715]
Epoch [7/120    avg_loss:0.779, val_acc:0.758]
Epoch [8/120    avg_loss:0.775, val_acc:0.784]
Epoch [9/120    avg_loss:0.618, val_acc:0.788]
Epoch [10/120    avg_loss:0.518, val_acc:0.784]
Epoch [11/120    avg_loss:0.525, val_acc:0.821]
Epoch [12/120    avg_loss:0.453, val_acc:0.832]
Epoch [13/120    avg_loss:0.384, val_acc:0.858]
Epoch [14/120    avg_loss:0.356, val_acc:0.852]
Epoch [15/120    avg_loss:0.262, val_acc:0.886]
Epoch [16/120    avg_loss:0.331, val_acc:0.873]
Epoch [17/120    avg_loss:0.280, val_acc:0.890]
Epoch [18/120    avg_loss:0.223, val_acc:0.883]
Epoch [19/120    avg_loss:0.376, val_acc:0.885]
Epoch [20/120    avg_loss:0.233, val_acc:0.886]
Epoch [21/120    avg_loss:0.189, val_acc:0.906]
Epoch [22/120    avg_loss:0.165, val_acc:0.881]
Epoch [23/120    avg_loss:0.134, val_acc:0.913]
Epoch [24/120    avg_loss:0.140, val_acc:0.924]
Epoch [25/120    avg_loss:0.210, val_acc:0.920]
Epoch [26/120    avg_loss:0.124, val_acc:0.943]
Epoch [27/120    avg_loss:0.112, val_acc:0.923]
Epoch [28/120    avg_loss:0.135, val_acc:0.919]
Epoch [29/120    avg_loss:0.117, val_acc:0.915]
Epoch [30/120    avg_loss:0.110, val_acc:0.941]
Epoch [31/120    avg_loss:0.085, val_acc:0.943]
Epoch [32/120    avg_loss:0.109, val_acc:0.926]
Epoch [33/120    avg_loss:0.123, val_acc:0.943]
Epoch [34/120    avg_loss:0.107, val_acc:0.932]
Epoch [35/120    avg_loss:0.074, val_acc:0.943]
Epoch [36/120    avg_loss:0.063, val_acc:0.933]
Epoch [37/120    avg_loss:0.070, val_acc:0.925]
Epoch [38/120    avg_loss:0.053, val_acc:0.957]
Epoch [39/120    avg_loss:0.070, val_acc:0.927]
Epoch [40/120    avg_loss:0.064, val_acc:0.908]
Epoch [41/120    avg_loss:0.087, val_acc:0.946]
Epoch [42/120    avg_loss:0.070, val_acc:0.944]
Epoch [43/120    avg_loss:0.054, val_acc:0.943]
Epoch [44/120    avg_loss:0.035, val_acc:0.947]
Epoch [45/120    avg_loss:0.034, val_acc:0.950]
Epoch [46/120    avg_loss:0.046, val_acc:0.951]
Epoch [47/120    avg_loss:0.030, val_acc:0.957]
Epoch [48/120    avg_loss:0.025, val_acc:0.953]
Epoch [49/120    avg_loss:0.021, val_acc:0.958]
Epoch [50/120    avg_loss:0.030, val_acc:0.943]
Epoch [51/120    avg_loss:0.034, val_acc:0.962]
Epoch [52/120    avg_loss:0.028, val_acc:0.959]
Epoch [53/120    avg_loss:0.024, val_acc:0.955]
Epoch [54/120    avg_loss:0.022, val_acc:0.954]
Epoch [55/120    avg_loss:0.051, val_acc:0.953]
Epoch [56/120    avg_loss:0.053, val_acc:0.957]
Epoch [57/120    avg_loss:0.026, val_acc:0.959]
Epoch [58/120    avg_loss:0.024, val_acc:0.946]
Epoch [59/120    avg_loss:0.038, val_acc:0.940]
Epoch [60/120    avg_loss:0.029, val_acc:0.947]
Epoch [61/120    avg_loss:0.044, val_acc:0.919]
Epoch [62/120    avg_loss:0.038, val_acc:0.957]
Epoch [63/120    avg_loss:0.018, val_acc:0.960]
Epoch [64/120    avg_loss:0.015, val_acc:0.966]
Epoch [65/120    avg_loss:0.011, val_acc:0.967]
Epoch [66/120    avg_loss:0.015, val_acc:0.951]
Epoch [67/120    avg_loss:0.014, val_acc:0.968]
Epoch [68/120    avg_loss:0.010, val_acc:0.970]
Epoch [69/120    avg_loss:0.027, val_acc:0.948]
Epoch [70/120    avg_loss:0.032, val_acc:0.959]
Epoch [71/120    avg_loss:0.020, val_acc:0.957]
Epoch [72/120    avg_loss:0.022, val_acc:0.968]
Epoch [73/120    avg_loss:0.017, val_acc:0.957]
Epoch [74/120    avg_loss:0.018, val_acc:0.959]
Epoch [75/120    avg_loss:0.032, val_acc:0.964]
Epoch [76/120    avg_loss:0.020, val_acc:0.963]
Epoch [77/120    avg_loss:0.017, val_acc:0.965]
Epoch [78/120    avg_loss:0.019, val_acc:0.958]
Epoch [79/120    avg_loss:0.020, val_acc:0.960]
Epoch [80/120    avg_loss:0.046, val_acc:0.925]
Epoch [81/120    avg_loss:0.149, val_acc:0.949]
Epoch [82/120    avg_loss:0.057, val_acc:0.958]
Epoch [83/120    avg_loss:0.040, val_acc:0.961]
Epoch [84/120    avg_loss:0.029, val_acc:0.962]
Epoch [85/120    avg_loss:0.026, val_acc:0.964]
Epoch [86/120    avg_loss:0.026, val_acc:0.963]
Epoch [87/120    avg_loss:0.032, val_acc:0.965]
Epoch [88/120    avg_loss:0.025, val_acc:0.963]
Epoch [89/120    avg_loss:0.027, val_acc:0.964]
Epoch [90/120    avg_loss:0.021, val_acc:0.967]
Epoch [91/120    avg_loss:0.021, val_acc:0.966]
Epoch [92/120    avg_loss:0.014, val_acc:0.967]
Epoch [93/120    avg_loss:0.016, val_acc:0.969]
Epoch [94/120    avg_loss:0.016, val_acc:0.968]
Epoch [95/120    avg_loss:0.016, val_acc:0.967]
Epoch [96/120    avg_loss:0.019, val_acc:0.967]
Epoch [97/120    avg_loss:0.014, val_acc:0.968]
Epoch [98/120    avg_loss:0.014, val_acc:0.968]
Epoch [99/120    avg_loss:0.016, val_acc:0.967]
Epoch [100/120    avg_loss:0.018, val_acc:0.966]
Epoch [101/120    avg_loss:0.021, val_acc:0.966]
Epoch [102/120    avg_loss:0.013, val_acc:0.966]
Epoch [103/120    avg_loss:0.014, val_acc:0.965]
Epoch [104/120    avg_loss:0.012, val_acc:0.965]
Epoch [105/120    avg_loss:0.014, val_acc:0.965]
Epoch [106/120    avg_loss:0.015, val_acc:0.965]
Epoch [107/120    avg_loss:0.017, val_acc:0.965]
Epoch [108/120    avg_loss:0.017, val_acc:0.965]
Epoch [109/120    avg_loss:0.017, val_acc:0.965]
Epoch [110/120    avg_loss:0.015, val_acc:0.965]
Epoch [111/120    avg_loss:0.023, val_acc:0.965]
Epoch [112/120    avg_loss:0.016, val_acc:0.965]
Epoch [113/120    avg_loss:0.014, val_acc:0.965]
Epoch [114/120    avg_loss:0.016, val_acc:0.965]
Epoch [115/120    avg_loss:0.011, val_acc:0.965]
Epoch [116/120    avg_loss:0.016, val_acc:0.965]
Epoch [117/120    avg_loss:0.017, val_acc:0.965]
Epoch [118/120    avg_loss:0.015, val_acc:0.965]
Epoch [119/120    avg_loss:0.015, val_acc:0.965]
Epoch [120/120    avg_loss:0.014, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    6    1    1    1    0    0    0    5   11    0    0
     0    0    0]
 [   0    0    0  742    2    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    0    0    1    0    0
     6    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   18    0    0    0    1    0    0    0  821   34    0    0
     1    0    0]
 [   0    0    7    0    0    5    0    0    0    0    7 2180    8    1
     2    0    0]
 [   0    0    0    1    6    0    0    0    0    0    0    1  523    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1127   12    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
   110  232    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.11653116531166

F1 scores:
[       nan 1.         0.98054475 0.99131597 0.97459584 0.98504028
 0.99164768 1.         1.         1.         0.96135831 0.98198198
 0.97757009 0.99730458 0.94467728 0.78246206 0.98809524]

Kappa:
0.9670808817443208
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efff265d898>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.446, val_acc:0.513]
Epoch [2/120    avg_loss:1.882, val_acc:0.563]
Epoch [3/120    avg_loss:1.559, val_acc:0.638]
Epoch [4/120    avg_loss:1.279, val_acc:0.656]
Epoch [5/120    avg_loss:1.074, val_acc:0.727]
Epoch [6/120    avg_loss:0.968, val_acc:0.718]
Epoch [7/120    avg_loss:0.888, val_acc:0.759]
Epoch [8/120    avg_loss:0.729, val_acc:0.765]
Epoch [9/120    avg_loss:0.695, val_acc:0.793]
Epoch [10/120    avg_loss:0.613, val_acc:0.798]
Epoch [11/120    avg_loss:0.581, val_acc:0.793]
Epoch [12/120    avg_loss:0.518, val_acc:0.807]
Epoch [13/120    avg_loss:0.428, val_acc:0.849]
Epoch [14/120    avg_loss:0.380, val_acc:0.841]
Epoch [15/120    avg_loss:0.413, val_acc:0.838]
Epoch [16/120    avg_loss:0.342, val_acc:0.872]
Epoch [17/120    avg_loss:0.304, val_acc:0.875]
Epoch [18/120    avg_loss:0.300, val_acc:0.862]
Epoch [19/120    avg_loss:0.309, val_acc:0.886]
Epoch [20/120    avg_loss:0.271, val_acc:0.838]
Epoch [21/120    avg_loss:0.212, val_acc:0.881]
Epoch [22/120    avg_loss:0.206, val_acc:0.842]
Epoch [23/120    avg_loss:0.250, val_acc:0.902]
Epoch [24/120    avg_loss:0.215, val_acc:0.871]
Epoch [25/120    avg_loss:0.162, val_acc:0.912]
Epoch [26/120    avg_loss:0.178, val_acc:0.896]
Epoch [27/120    avg_loss:0.175, val_acc:0.912]
Epoch [28/120    avg_loss:0.140, val_acc:0.914]
Epoch [29/120    avg_loss:0.168, val_acc:0.860]
Epoch [30/120    avg_loss:0.150, val_acc:0.910]
Epoch [31/120    avg_loss:0.111, val_acc:0.915]
Epoch [32/120    avg_loss:0.110, val_acc:0.928]
Epoch [33/120    avg_loss:0.257, val_acc:0.883]
Epoch [34/120    avg_loss:0.153, val_acc:0.915]
Epoch [35/120    avg_loss:0.089, val_acc:0.931]
Epoch [36/120    avg_loss:0.108, val_acc:0.933]
Epoch [37/120    avg_loss:0.124, val_acc:0.931]
Epoch [38/120    avg_loss:0.087, val_acc:0.945]
Epoch [39/120    avg_loss:0.092, val_acc:0.935]
Epoch [40/120    avg_loss:0.076, val_acc:0.943]
Epoch [41/120    avg_loss:0.072, val_acc:0.943]
Epoch [42/120    avg_loss:0.151, val_acc:0.925]
Epoch [43/120    avg_loss:0.110, val_acc:0.944]
Epoch [44/120    avg_loss:0.055, val_acc:0.939]
Epoch [45/120    avg_loss:0.070, val_acc:0.943]
Epoch [46/120    avg_loss:0.114, val_acc:0.911]
Epoch [47/120    avg_loss:0.074, val_acc:0.941]
Epoch [48/120    avg_loss:0.066, val_acc:0.949]
Epoch [49/120    avg_loss:0.046, val_acc:0.948]
Epoch [50/120    avg_loss:0.092, val_acc:0.866]
Epoch [51/120    avg_loss:0.155, val_acc:0.911]
Epoch [52/120    avg_loss:0.150, val_acc:0.907]
Epoch [53/120    avg_loss:0.090, val_acc:0.941]
Epoch [54/120    avg_loss:0.049, val_acc:0.955]
Epoch [55/120    avg_loss:0.071, val_acc:0.948]
Epoch [56/120    avg_loss:0.069, val_acc:0.949]
Epoch [57/120    avg_loss:0.045, val_acc:0.957]
Epoch [58/120    avg_loss:0.040, val_acc:0.950]
Epoch [59/120    avg_loss:0.035, val_acc:0.962]
Epoch [60/120    avg_loss:0.057, val_acc:0.941]
Epoch [61/120    avg_loss:0.062, val_acc:0.954]
Epoch [62/120    avg_loss:0.117, val_acc:0.942]
Epoch [63/120    avg_loss:0.043, val_acc:0.959]
Epoch [64/120    avg_loss:0.058, val_acc:0.947]
Epoch [65/120    avg_loss:0.034, val_acc:0.952]
Epoch [66/120    avg_loss:0.035, val_acc:0.959]
Epoch [67/120    avg_loss:0.033, val_acc:0.966]
Epoch [68/120    avg_loss:0.032, val_acc:0.945]
Epoch [69/120    avg_loss:0.050, val_acc:0.954]
Epoch [70/120    avg_loss:0.025, val_acc:0.961]
Epoch [71/120    avg_loss:0.028, val_acc:0.969]
Epoch [72/120    avg_loss:0.021, val_acc:0.967]
Epoch [73/120    avg_loss:0.022, val_acc:0.962]
Epoch [74/120    avg_loss:0.021, val_acc:0.968]
Epoch [75/120    avg_loss:0.016, val_acc:0.971]
Epoch [76/120    avg_loss:0.025, val_acc:0.966]
Epoch [77/120    avg_loss:0.020, val_acc:0.955]
Epoch [78/120    avg_loss:0.015, val_acc:0.970]
Epoch [79/120    avg_loss:0.017, val_acc:0.968]
Epoch [80/120    avg_loss:0.029, val_acc:0.969]
Epoch [81/120    avg_loss:0.039, val_acc:0.927]
Epoch [82/120    avg_loss:0.122, val_acc:0.948]
Epoch [83/120    avg_loss:0.051, val_acc:0.932]
Epoch [84/120    avg_loss:0.044, val_acc:0.933]
Epoch [85/120    avg_loss:0.058, val_acc:0.949]
Epoch [86/120    avg_loss:0.034, val_acc:0.955]
Epoch [87/120    avg_loss:0.019, val_acc:0.966]
Epoch [88/120    avg_loss:0.017, val_acc:0.968]
Epoch [89/120    avg_loss:0.016, val_acc:0.970]
Epoch [90/120    avg_loss:0.014, val_acc:0.972]
Epoch [91/120    avg_loss:0.012, val_acc:0.972]
Epoch [92/120    avg_loss:0.011, val_acc:0.972]
Epoch [93/120    avg_loss:0.009, val_acc:0.973]
Epoch [94/120    avg_loss:0.012, val_acc:0.971]
Epoch [95/120    avg_loss:0.012, val_acc:0.973]
Epoch [96/120    avg_loss:0.011, val_acc:0.973]
Epoch [97/120    avg_loss:0.011, val_acc:0.971]
Epoch [98/120    avg_loss:0.012, val_acc:0.972]
Epoch [99/120    avg_loss:0.011, val_acc:0.973]
Epoch [100/120    avg_loss:0.012, val_acc:0.971]
Epoch [101/120    avg_loss:0.013, val_acc:0.972]
Epoch [102/120    avg_loss:0.009, val_acc:0.971]
Epoch [103/120    avg_loss:0.010, val_acc:0.971]
Epoch [104/120    avg_loss:0.014, val_acc:0.972]
Epoch [105/120    avg_loss:0.016, val_acc:0.972]
Epoch [106/120    avg_loss:0.010, val_acc:0.974]
Epoch [107/120    avg_loss:0.010, val_acc:0.973]
Epoch [108/120    avg_loss:0.012, val_acc:0.974]
Epoch [109/120    avg_loss:0.015, val_acc:0.973]
Epoch [110/120    avg_loss:0.011, val_acc:0.973]
Epoch [111/120    avg_loss:0.008, val_acc:0.971]
Epoch [112/120    avg_loss:0.009, val_acc:0.973]
Epoch [113/120    avg_loss:0.012, val_acc:0.974]
Epoch [114/120    avg_loss:0.009, val_acc:0.973]
Epoch [115/120    avg_loss:0.010, val_acc:0.973]
Epoch [116/120    avg_loss:0.011, val_acc:0.973]
Epoch [117/120    avg_loss:0.010, val_acc:0.972]
Epoch [118/120    avg_loss:0.008, val_acc:0.973]
Epoch [119/120    avg_loss:0.009, val_acc:0.973]
Epoch [120/120    avg_loss:0.009, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1242    4   14    2    0    0    0    0   10   13    0    0
     0    0    0]
 [   0    0    1  713    4    1    0    0    0    2    3    5   18    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  431    0    4    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    1    0    0    0    0  848   24    0    0
     1    0    0]
 [   0    0    5    0    0    0    0    0    0    0    5 2181   17    0
     0    2    0]
 [   0    0    0    0    0    0    0    0    0    0    0    2  530    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    93  249    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.17073170731707

F1 scores:
[       nan 1.         0.98026835 0.97404372 0.95711061 0.9908046
 0.99468489 0.92592593 1.         0.94736842 0.97415279 0.9833183
 0.96014493 1.         0.9533503  0.81239804 0.96385542]

Kappa:
0.9677236172970445
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8ba9d83828>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.462, val_acc:0.531]
Epoch [2/120    avg_loss:1.848, val_acc:0.592]
Epoch [3/120    avg_loss:1.495, val_acc:0.618]
Epoch [4/120    avg_loss:1.222, val_acc:0.676]
Epoch [5/120    avg_loss:1.081, val_acc:0.745]
Epoch [6/120    avg_loss:0.913, val_acc:0.751]
Epoch [7/120    avg_loss:0.760, val_acc:0.747]
Epoch [8/120    avg_loss:0.660, val_acc:0.776]
Epoch [9/120    avg_loss:0.646, val_acc:0.760]
Epoch [10/120    avg_loss:0.569, val_acc:0.826]
Epoch [11/120    avg_loss:0.509, val_acc:0.777]
Epoch [12/120    avg_loss:0.442, val_acc:0.845]
Epoch [13/120    avg_loss:0.415, val_acc:0.854]
Epoch [14/120    avg_loss:0.439, val_acc:0.802]
Epoch [15/120    avg_loss:0.359, val_acc:0.838]
Epoch [16/120    avg_loss:0.358, val_acc:0.856]
Epoch [17/120    avg_loss:0.276, val_acc:0.870]
Epoch [18/120    avg_loss:0.249, val_acc:0.854]
Epoch [19/120    avg_loss:0.253, val_acc:0.851]
Epoch [20/120    avg_loss:0.196, val_acc:0.909]
Epoch [21/120    avg_loss:0.222, val_acc:0.871]
Epoch [22/120    avg_loss:0.213, val_acc:0.878]
Epoch [23/120    avg_loss:0.203, val_acc:0.896]
Epoch [24/120    avg_loss:0.149, val_acc:0.913]
Epoch [25/120    avg_loss:0.154, val_acc:0.911]
Epoch [26/120    avg_loss:0.177, val_acc:0.907]
Epoch [27/120    avg_loss:0.146, val_acc:0.915]
Epoch [28/120    avg_loss:0.092, val_acc:0.926]
Epoch [29/120    avg_loss:0.111, val_acc:0.941]
Epoch [30/120    avg_loss:0.092, val_acc:0.917]
Epoch [31/120    avg_loss:0.094, val_acc:0.930]
Epoch [32/120    avg_loss:0.079, val_acc:0.916]
Epoch [33/120    avg_loss:0.086, val_acc:0.950]
Epoch [34/120    avg_loss:0.060, val_acc:0.947]
Epoch [35/120    avg_loss:0.141, val_acc:0.893]
Epoch [36/120    avg_loss:0.090, val_acc:0.950]
Epoch [37/120    avg_loss:0.204, val_acc:0.913]
Epoch [38/120    avg_loss:0.123, val_acc:0.938]
Epoch [39/120    avg_loss:0.063, val_acc:0.919]
Epoch [40/120    avg_loss:0.085, val_acc:0.936]
Epoch [41/120    avg_loss:0.058, val_acc:0.930]
Epoch [42/120    avg_loss:0.096, val_acc:0.892]
Epoch [43/120    avg_loss:0.092, val_acc:0.941]
Epoch [44/120    avg_loss:0.050, val_acc:0.948]
Epoch [45/120    avg_loss:0.047, val_acc:0.959]
Epoch [46/120    avg_loss:0.048, val_acc:0.918]
Epoch [47/120    avg_loss:0.068, val_acc:0.943]
Epoch [48/120    avg_loss:0.064, val_acc:0.954]
Epoch [49/120    avg_loss:0.046, val_acc:0.959]
Epoch [50/120    avg_loss:0.040, val_acc:0.947]
Epoch [51/120    avg_loss:0.084, val_acc:0.944]
Epoch [52/120    avg_loss:0.039, val_acc:0.948]
Epoch [53/120    avg_loss:0.043, val_acc:0.966]
Epoch [54/120    avg_loss:0.044, val_acc:0.947]
Epoch [55/120    avg_loss:0.039, val_acc:0.961]
Epoch [56/120    avg_loss:0.031, val_acc:0.958]
Epoch [57/120    avg_loss:0.039, val_acc:0.965]
Epoch [58/120    avg_loss:0.019, val_acc:0.960]
Epoch [59/120    avg_loss:0.040, val_acc:0.952]
Epoch [60/120    avg_loss:0.047, val_acc:0.958]
Epoch [61/120    avg_loss:0.027, val_acc:0.952]
Epoch [62/120    avg_loss:0.026, val_acc:0.967]
Epoch [63/120    avg_loss:0.018, val_acc:0.956]
Epoch [64/120    avg_loss:0.014, val_acc:0.967]
Epoch [65/120    avg_loss:0.025, val_acc:0.946]
Epoch [66/120    avg_loss:0.020, val_acc:0.958]
Epoch [67/120    avg_loss:0.039, val_acc:0.963]
Epoch [68/120    avg_loss:0.038, val_acc:0.948]
Epoch [69/120    avg_loss:0.054, val_acc:0.950]
Epoch [70/120    avg_loss:0.055, val_acc:0.943]
Epoch [71/120    avg_loss:0.036, val_acc:0.952]
Epoch [72/120    avg_loss:0.032, val_acc:0.959]
Epoch [73/120    avg_loss:0.017, val_acc:0.963]
Epoch [74/120    avg_loss:0.018, val_acc:0.969]
Epoch [75/120    avg_loss:0.018, val_acc:0.970]
Epoch [76/120    avg_loss:0.019, val_acc:0.974]
Epoch [77/120    avg_loss:0.012, val_acc:0.970]
Epoch [78/120    avg_loss:0.021, val_acc:0.960]
Epoch [79/120    avg_loss:0.014, val_acc:0.971]
Epoch [80/120    avg_loss:0.008, val_acc:0.965]
Epoch [81/120    avg_loss:0.008, val_acc:0.968]
Epoch [82/120    avg_loss:0.015, val_acc:0.954]
Epoch [83/120    avg_loss:0.019, val_acc:0.964]
Epoch [84/120    avg_loss:0.011, val_acc:0.971]
Epoch [85/120    avg_loss:0.014, val_acc:0.971]
Epoch [86/120    avg_loss:0.098, val_acc:0.950]
Epoch [87/120    avg_loss:0.038, val_acc:0.964]
Epoch [88/120    avg_loss:0.025, val_acc:0.963]
Epoch [89/120    avg_loss:0.038, val_acc:0.905]
Epoch [90/120    avg_loss:0.050, val_acc:0.969]
Epoch [91/120    avg_loss:0.023, val_acc:0.971]
Epoch [92/120    avg_loss:0.015, val_acc:0.972]
Epoch [93/120    avg_loss:0.013, val_acc:0.971]
Epoch [94/120    avg_loss:0.021, val_acc:0.972]
Epoch [95/120    avg_loss:0.014, val_acc:0.971]
Epoch [96/120    avg_loss:0.016, val_acc:0.970]
Epoch [97/120    avg_loss:0.010, val_acc:0.970]
Epoch [98/120    avg_loss:0.009, val_acc:0.971]
Epoch [99/120    avg_loss:0.010, val_acc:0.971]
Epoch [100/120    avg_loss:0.014, val_acc:0.974]
Epoch [101/120    avg_loss:0.009, val_acc:0.972]
Epoch [102/120    avg_loss:0.009, val_acc:0.971]
Epoch [103/120    avg_loss:0.006, val_acc:0.973]
Epoch [104/120    avg_loss:0.011, val_acc:0.974]
Epoch [105/120    avg_loss:0.008, val_acc:0.974]
Epoch [106/120    avg_loss:0.008, val_acc:0.975]
Epoch [107/120    avg_loss:0.007, val_acc:0.973]
Epoch [108/120    avg_loss:0.007, val_acc:0.975]
Epoch [109/120    avg_loss:0.007, val_acc:0.976]
Epoch [110/120    avg_loss:0.007, val_acc:0.976]
Epoch [111/120    avg_loss:0.010, val_acc:0.974]
Epoch [112/120    avg_loss:0.007, val_acc:0.972]
Epoch [113/120    avg_loss:0.009, val_acc:0.975]
Epoch [114/120    avg_loss:0.010, val_acc:0.975]
Epoch [115/120    avg_loss:0.009, val_acc:0.977]
Epoch [116/120    avg_loss:0.007, val_acc:0.977]
Epoch [117/120    avg_loss:0.007, val_acc:0.975]
Epoch [118/120    avg_loss:0.009, val_acc:0.975]
Epoch [119/120    avg_loss:0.010, val_acc:0.976]
Epoch [120/120    avg_loss:0.006, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1252   11    0    0    0    0    0    0    2   15    0    0
     0    5    0]
 [   0    0    0  737    0    0    6    0    0    0    0    0    1    0
     2    1    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    1  851   15    3    0
     0    1    0]
 [   0    0    4    0    0    0    0    0    0    1    4 2192    9    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    1    0    7  520    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   110  237    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.48509485094851

F1 scores:
[       nan 0.975      0.98388998 0.98463594 0.99764706 0.99884925
 0.99316629 1.         1.         0.92307692 0.98154556 0.98716505
 0.97378277 1.         0.94572991 0.77832512 0.98224852]

Kappa:
0.9712942745275717
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2f1f166898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.440, val_acc:0.492]
Epoch [2/120    avg_loss:1.912, val_acc:0.498]
Epoch [3/120    avg_loss:1.579, val_acc:0.567]
Epoch [4/120    avg_loss:1.459, val_acc:0.605]
Epoch [5/120    avg_loss:1.187, val_acc:0.652]
Epoch [6/120    avg_loss:1.039, val_acc:0.696]
Epoch [7/120    avg_loss:0.894, val_acc:0.699]
Epoch [8/120    avg_loss:0.762, val_acc:0.778]
Epoch [9/120    avg_loss:0.714, val_acc:0.770]
Epoch [10/120    avg_loss:0.571, val_acc:0.758]
Epoch [11/120    avg_loss:0.557, val_acc:0.809]
Epoch [12/120    avg_loss:0.455, val_acc:0.833]
Epoch [13/120    avg_loss:0.457, val_acc:0.834]
Epoch [14/120    avg_loss:0.394, val_acc:0.860]
Epoch [15/120    avg_loss:0.334, val_acc:0.887]
Epoch [16/120    avg_loss:0.341, val_acc:0.879]
Epoch [17/120    avg_loss:0.311, val_acc:0.877]
Epoch [18/120    avg_loss:0.385, val_acc:0.820]
Epoch [19/120    avg_loss:0.300, val_acc:0.892]
Epoch [20/120    avg_loss:0.273, val_acc:0.889]
Epoch [21/120    avg_loss:0.191, val_acc:0.921]
Epoch [22/120    avg_loss:0.224, val_acc:0.905]
Epoch [23/120    avg_loss:0.190, val_acc:0.890]
Epoch [24/120    avg_loss:0.249, val_acc:0.914]
Epoch [25/120    avg_loss:0.174, val_acc:0.922]
Epoch [26/120    avg_loss:0.141, val_acc:0.934]
Epoch [27/120    avg_loss:0.141, val_acc:0.928]
Epoch [28/120    avg_loss:0.200, val_acc:0.838]
Epoch [29/120    avg_loss:0.226, val_acc:0.927]
Epoch [30/120    avg_loss:0.150, val_acc:0.936]
Epoch [31/120    avg_loss:0.133, val_acc:0.904]
Epoch [32/120    avg_loss:0.141, val_acc:0.920]
Epoch [33/120    avg_loss:0.127, val_acc:0.945]
Epoch [34/120    avg_loss:0.123, val_acc:0.919]
Epoch [35/120    avg_loss:0.105, val_acc:0.940]
Epoch [36/120    avg_loss:0.113, val_acc:0.927]
Epoch [37/120    avg_loss:0.117, val_acc:0.936]
Epoch [38/120    avg_loss:0.075, val_acc:0.941]
Epoch [39/120    avg_loss:0.081, val_acc:0.940]
Epoch [40/120    avg_loss:0.080, val_acc:0.944]
Epoch [41/120    avg_loss:0.074, val_acc:0.957]
Epoch [42/120    avg_loss:0.078, val_acc:0.945]
Epoch [43/120    avg_loss:0.102, val_acc:0.948]
Epoch [44/120    avg_loss:0.068, val_acc:0.960]
Epoch [45/120    avg_loss:0.069, val_acc:0.950]
Epoch [46/120    avg_loss:0.062, val_acc:0.953]
Epoch [47/120    avg_loss:0.052, val_acc:0.961]
Epoch [48/120    avg_loss:0.041, val_acc:0.955]
Epoch [49/120    avg_loss:0.058, val_acc:0.957]
Epoch [50/120    avg_loss:0.053, val_acc:0.957]
Epoch [51/120    avg_loss:0.069, val_acc:0.959]
Epoch [52/120    avg_loss:0.070, val_acc:0.923]
Epoch [53/120    avg_loss:0.131, val_acc:0.959]
Epoch [54/120    avg_loss:0.063, val_acc:0.936]
Epoch [55/120    avg_loss:0.085, val_acc:0.964]
Epoch [56/120    avg_loss:0.045, val_acc:0.963]
Epoch [57/120    avg_loss:0.041, val_acc:0.971]
Epoch [58/120    avg_loss:0.040, val_acc:0.967]
Epoch [59/120    avg_loss:0.035, val_acc:0.966]
Epoch [60/120    avg_loss:0.025, val_acc:0.966]
Epoch [61/120    avg_loss:0.025, val_acc:0.968]
Epoch [62/120    avg_loss:0.073, val_acc:0.956]
Epoch [63/120    avg_loss:0.063, val_acc:0.961]
Epoch [64/120    avg_loss:0.070, val_acc:0.947]
Epoch [65/120    avg_loss:0.058, val_acc:0.965]
Epoch [66/120    avg_loss:0.063, val_acc:0.954]
Epoch [67/120    avg_loss:0.043, val_acc:0.954]
Epoch [68/120    avg_loss:0.036, val_acc:0.965]
Epoch [69/120    avg_loss:0.043, val_acc:0.970]
Epoch [70/120    avg_loss:0.021, val_acc:0.970]
Epoch [71/120    avg_loss:0.024, val_acc:0.974]
Epoch [72/120    avg_loss:0.019, val_acc:0.978]
Epoch [73/120    avg_loss:0.018, val_acc:0.978]
Epoch [74/120    avg_loss:0.017, val_acc:0.978]
Epoch [75/120    avg_loss:0.014, val_acc:0.981]
Epoch [76/120    avg_loss:0.015, val_acc:0.979]
Epoch [77/120    avg_loss:0.020, val_acc:0.980]
Epoch [78/120    avg_loss:0.016, val_acc:0.977]
Epoch [79/120    avg_loss:0.014, val_acc:0.977]
Epoch [80/120    avg_loss:0.016, val_acc:0.977]
Epoch [81/120    avg_loss:0.013, val_acc:0.977]
Epoch [82/120    avg_loss:0.014, val_acc:0.975]
Epoch [83/120    avg_loss:0.014, val_acc:0.975]
Epoch [84/120    avg_loss:0.011, val_acc:0.975]
Epoch [85/120    avg_loss:0.014, val_acc:0.974]
Epoch [86/120    avg_loss:0.013, val_acc:0.975]
Epoch [87/120    avg_loss:0.015, val_acc:0.975]
Epoch [88/120    avg_loss:0.012, val_acc:0.976]
Epoch [89/120    avg_loss:0.010, val_acc:0.976]
Epoch [90/120    avg_loss:0.016, val_acc:0.976]
Epoch [91/120    avg_loss:0.012, val_acc:0.976]
Epoch [92/120    avg_loss:0.012, val_acc:0.976]
Epoch [93/120    avg_loss:0.012, val_acc:0.976]
Epoch [94/120    avg_loss:0.015, val_acc:0.976]
Epoch [95/120    avg_loss:0.014, val_acc:0.976]
Epoch [96/120    avg_loss:0.011, val_acc:0.976]
Epoch [97/120    avg_loss:0.012, val_acc:0.976]
Epoch [98/120    avg_loss:0.015, val_acc:0.976]
Epoch [99/120    avg_loss:0.014, val_acc:0.976]
Epoch [100/120    avg_loss:0.010, val_acc:0.976]
Epoch [101/120    avg_loss:0.015, val_acc:0.976]
Epoch [102/120    avg_loss:0.010, val_acc:0.976]
Epoch [103/120    avg_loss:0.012, val_acc:0.976]
Epoch [104/120    avg_loss:0.013, val_acc:0.976]
Epoch [105/120    avg_loss:0.012, val_acc:0.976]
Epoch [106/120    avg_loss:0.010, val_acc:0.976]
Epoch [107/120    avg_loss:0.018, val_acc:0.976]
Epoch [108/120    avg_loss:0.020, val_acc:0.976]
Epoch [109/120    avg_loss:0.012, val_acc:0.976]
Epoch [110/120    avg_loss:0.011, val_acc:0.976]
Epoch [111/120    avg_loss:0.012, val_acc:0.976]
Epoch [112/120    avg_loss:0.009, val_acc:0.976]
Epoch [113/120    avg_loss:0.018, val_acc:0.976]
Epoch [114/120    avg_loss:0.013, val_acc:0.976]
Epoch [115/120    avg_loss:0.013, val_acc:0.976]
Epoch [116/120    avg_loss:0.012, val_acc:0.976]
Epoch [117/120    avg_loss:0.015, val_acc:0.976]
Epoch [118/120    avg_loss:0.014, val_acc:0.976]
Epoch [119/120    avg_loss:0.010, val_acc:0.976]
Epoch [120/120    avg_loss:0.012, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    0    0    0    0    0    0    0    0    6    0    0    0
     0    0    0]
 [   0    0 1243    9    9    0    2    0    0    0    7   13    0    0
     0    2    0]
 [   0    0    0  727    1    0    0    0    0    5    0    0   12    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    9    0    0    0    1    0    0    0  843   17    1    0
     0    4    0]
 [   0    0    7    0    0    1    3    0    0    1   13 2183    1    0
     0    1    0]
 [   0    0    0    3    0    0    0    0    0    0    0    0  526    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1122   16    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
    37  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.6260162601626

F1 scores:
[       nan 0.92105263 0.97720126 0.97846568 0.97706422 0.99196326
 0.98050975 1.         0.99649942 0.82926829 0.96618911 0.98688969
 0.97588126 0.99462366 0.97438124 0.87987988 0.97005988]

Kappa:
0.9729330798660338
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcb3fbf7860>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.412, val_acc:0.515]
Epoch [2/120    avg_loss:1.902, val_acc:0.545]
Epoch [3/120    avg_loss:1.615, val_acc:0.644]
Epoch [4/120    avg_loss:1.343, val_acc:0.681]
Epoch [5/120    avg_loss:1.100, val_acc:0.724]
Epoch [6/120    avg_loss:0.986, val_acc:0.701]
Epoch [7/120    avg_loss:0.885, val_acc:0.779]
Epoch [8/120    avg_loss:0.740, val_acc:0.800]
Epoch [9/120    avg_loss:0.622, val_acc:0.810]
Epoch [10/120    avg_loss:0.577, val_acc:0.801]
Epoch [11/120    avg_loss:0.525, val_acc:0.848]
Epoch [12/120    avg_loss:0.468, val_acc:0.817]
Epoch [13/120    avg_loss:0.438, val_acc:0.840]
Epoch [14/120    avg_loss:0.390, val_acc:0.866]
Epoch [15/120    avg_loss:0.300, val_acc:0.868]
Epoch [16/120    avg_loss:0.294, val_acc:0.859]
Epoch [17/120    avg_loss:0.259, val_acc:0.874]
Epoch [18/120    avg_loss:0.273, val_acc:0.872]
Epoch [19/120    avg_loss:0.251, val_acc:0.854]
Epoch [20/120    avg_loss:0.250, val_acc:0.873]
Epoch [21/120    avg_loss:0.276, val_acc:0.904]
Epoch [22/120    avg_loss:0.220, val_acc:0.896]
Epoch [23/120    avg_loss:0.194, val_acc:0.911]
Epoch [24/120    avg_loss:0.173, val_acc:0.907]
Epoch [25/120    avg_loss:0.145, val_acc:0.933]
Epoch [26/120    avg_loss:0.136, val_acc:0.924]
Epoch [27/120    avg_loss:0.157, val_acc:0.900]
Epoch [28/120    avg_loss:0.164, val_acc:0.904]
Epoch [29/120    avg_loss:0.177, val_acc:0.929]
Epoch [30/120    avg_loss:0.178, val_acc:0.903]
Epoch [31/120    avg_loss:0.130, val_acc:0.921]
Epoch [32/120    avg_loss:0.129, val_acc:0.930]
Epoch [33/120    avg_loss:0.104, val_acc:0.903]
Epoch [34/120    avg_loss:0.116, val_acc:0.931]
Epoch [35/120    avg_loss:0.191, val_acc:0.931]
Epoch [36/120    avg_loss:0.114, val_acc:0.938]
Epoch [37/120    avg_loss:0.084, val_acc:0.944]
Epoch [38/120    avg_loss:0.064, val_acc:0.954]
Epoch [39/120    avg_loss:0.064, val_acc:0.950]
Epoch [40/120    avg_loss:0.063, val_acc:0.955]
Epoch [41/120    avg_loss:0.062, val_acc:0.957]
Epoch [42/120    avg_loss:0.067, val_acc:0.950]
Epoch [43/120    avg_loss:0.084, val_acc:0.933]
Epoch [44/120    avg_loss:0.182, val_acc:0.888]
Epoch [45/120    avg_loss:0.260, val_acc:0.884]
Epoch [46/120    avg_loss:0.164, val_acc:0.918]
Epoch [47/120    avg_loss:0.124, val_acc:0.933]
Epoch [48/120    avg_loss:0.102, val_acc:0.958]
Epoch [49/120    avg_loss:0.091, val_acc:0.944]
Epoch [50/120    avg_loss:0.070, val_acc:0.958]
Epoch [51/120    avg_loss:0.091, val_acc:0.943]
Epoch [52/120    avg_loss:0.084, val_acc:0.952]
Epoch [53/120    avg_loss:0.057, val_acc:0.963]
Epoch [54/120    avg_loss:0.056, val_acc:0.923]
Epoch [55/120    avg_loss:0.061, val_acc:0.955]
Epoch [56/120    avg_loss:0.070, val_acc:0.954]
Epoch [57/120    avg_loss:0.042, val_acc:0.957]
Epoch [58/120    avg_loss:0.069, val_acc:0.952]
Epoch [59/120    avg_loss:0.056, val_acc:0.967]
Epoch [60/120    avg_loss:0.043, val_acc:0.955]
Epoch [61/120    avg_loss:0.032, val_acc:0.966]
Epoch [62/120    avg_loss:0.041, val_acc:0.967]
Epoch [63/120    avg_loss:0.062, val_acc:0.951]
Epoch [64/120    avg_loss:0.038, val_acc:0.970]
Epoch [65/120    avg_loss:0.030, val_acc:0.972]
Epoch [66/120    avg_loss:0.030, val_acc:0.945]
Epoch [67/120    avg_loss:0.032, val_acc:0.971]
Epoch [68/120    avg_loss:0.036, val_acc:0.974]
Epoch [69/120    avg_loss:0.038, val_acc:0.964]
Epoch [70/120    avg_loss:0.019, val_acc:0.967]
Epoch [71/120    avg_loss:0.028, val_acc:0.973]
Epoch [72/120    avg_loss:0.021, val_acc:0.970]
Epoch [73/120    avg_loss:0.016, val_acc:0.968]
Epoch [74/120    avg_loss:0.047, val_acc:0.956]
Epoch [75/120    avg_loss:0.037, val_acc:0.972]
Epoch [76/120    avg_loss:0.070, val_acc:0.934]
Epoch [77/120    avg_loss:0.081, val_acc:0.965]
Epoch [78/120    avg_loss:0.029, val_acc:0.965]
Epoch [79/120    avg_loss:0.022, val_acc:0.974]
Epoch [80/120    avg_loss:0.029, val_acc:0.967]
Epoch [81/120    avg_loss:0.022, val_acc:0.973]
Epoch [82/120    avg_loss:0.018, val_acc:0.973]
Epoch [83/120    avg_loss:0.016, val_acc:0.973]
Epoch [84/120    avg_loss:0.036, val_acc:0.961]
Epoch [85/120    avg_loss:0.024, val_acc:0.964]
Epoch [86/120    avg_loss:0.016, val_acc:0.962]
Epoch [87/120    avg_loss:0.023, val_acc:0.967]
Epoch [88/120    avg_loss:0.016, val_acc:0.968]
Epoch [89/120    avg_loss:0.026, val_acc:0.970]
Epoch [90/120    avg_loss:0.023, val_acc:0.973]
Epoch [91/120    avg_loss:0.031, val_acc:0.965]
Epoch [92/120    avg_loss:0.014, val_acc:0.975]
Epoch [93/120    avg_loss:0.032, val_acc:0.967]
Epoch [94/120    avg_loss:0.020, val_acc:0.959]
Epoch [95/120    avg_loss:0.018, val_acc:0.979]
Epoch [96/120    avg_loss:0.012, val_acc:0.974]
Epoch [97/120    avg_loss:0.012, val_acc:0.967]
Epoch [98/120    avg_loss:0.017, val_acc:0.976]
Epoch [99/120    avg_loss:0.020, val_acc:0.949]
Epoch [100/120    avg_loss:0.669, val_acc:0.826]
Epoch [101/120    avg_loss:0.180, val_acc:0.914]
Epoch [102/120    avg_loss:0.110, val_acc:0.949]
Epoch [103/120    avg_loss:0.045, val_acc:0.960]
Epoch [104/120    avg_loss:0.058, val_acc:0.962]
Epoch [105/120    avg_loss:0.067, val_acc:0.964]
Epoch [106/120    avg_loss:0.037, val_acc:0.960]
Epoch [107/120    avg_loss:0.030, val_acc:0.944]
Epoch [108/120    avg_loss:0.029, val_acc:0.967]
Epoch [109/120    avg_loss:0.024, val_acc:0.975]
Epoch [110/120    avg_loss:0.017, val_acc:0.971]
Epoch [111/120    avg_loss:0.016, val_acc:0.972]
Epoch [112/120    avg_loss:0.016, val_acc:0.973]
Epoch [113/120    avg_loss:0.015, val_acc:0.973]
Epoch [114/120    avg_loss:0.011, val_acc:0.974]
Epoch [115/120    avg_loss:0.016, val_acc:0.973]
Epoch [116/120    avg_loss:0.014, val_acc:0.974]
Epoch [117/120    avg_loss:0.012, val_acc:0.975]
Epoch [118/120    avg_loss:0.013, val_acc:0.976]
Epoch [119/120    avg_loss:0.016, val_acc:0.974]
Epoch [120/120    avg_loss:0.012, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1253    0    0    1    0    0    0    2   10   19    0    0
     0    0    0]
 [   0    0    0  708    2    0    0    0    0    8    1    8   13    0
     7    0    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    2    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0  850   19    1    0
     1    1    0]
 [   0    0    4    0    0    0    0    0    0    3    1 2190   12    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    2  530    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1127   11    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
    40  306    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
97.95121951219512

F1 scores:
[       nan 0.975      0.98467583 0.97252747 0.99061033 0.99770115
 0.99541284 0.98039216 1.         0.72       0.97701149 0.98426966
 0.96715328 1.         0.97238999 0.92030075 0.95757576]

Kappa:
0.9766284254566108
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4e915f77b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.385, val_acc:0.422]
Epoch [2/120    avg_loss:1.873, val_acc:0.518]
Epoch [3/120    avg_loss:1.631, val_acc:0.632]
Epoch [4/120    avg_loss:1.254, val_acc:0.691]
Epoch [5/120    avg_loss:1.101, val_acc:0.685]
Epoch [6/120    avg_loss:0.979, val_acc:0.683]
Epoch [7/120    avg_loss:0.842, val_acc:0.794]
Epoch [8/120    avg_loss:0.700, val_acc:0.781]
Epoch [9/120    avg_loss:0.601, val_acc:0.799]
Epoch [10/120    avg_loss:0.570, val_acc:0.831]
Epoch [11/120    avg_loss:0.506, val_acc:0.802]
Epoch [12/120    avg_loss:0.484, val_acc:0.816]
Epoch [13/120    avg_loss:0.413, val_acc:0.855]
Epoch [14/120    avg_loss:0.322, val_acc:0.850]
Epoch [15/120    avg_loss:0.344, val_acc:0.861]
Epoch [16/120    avg_loss:0.309, val_acc:0.885]
Epoch [17/120    avg_loss:0.335, val_acc:0.840]
Epoch [18/120    avg_loss:0.308, val_acc:0.885]
Epoch [19/120    avg_loss:0.540, val_acc:0.877]
Epoch [20/120    avg_loss:0.463, val_acc:0.852]
Epoch [21/120    avg_loss:0.255, val_acc:0.874]
Epoch [22/120    avg_loss:0.232, val_acc:0.898]
Epoch [23/120    avg_loss:0.251, val_acc:0.897]
Epoch [24/120    avg_loss:0.238, val_acc:0.895]
Epoch [25/120    avg_loss:0.217, val_acc:0.914]
Epoch [26/120    avg_loss:0.175, val_acc:0.927]
Epoch [27/120    avg_loss:0.146, val_acc:0.935]
Epoch [28/120    avg_loss:0.126, val_acc:0.923]
Epoch [29/120    avg_loss:0.114, val_acc:0.910]
Epoch [30/120    avg_loss:0.146, val_acc:0.898]
Epoch [31/120    avg_loss:0.138, val_acc:0.891]
Epoch [32/120    avg_loss:0.135, val_acc:0.934]
Epoch [33/120    avg_loss:0.112, val_acc:0.907]
Epoch [34/120    avg_loss:0.114, val_acc:0.949]
Epoch [35/120    avg_loss:0.140, val_acc:0.929]
Epoch [36/120    avg_loss:0.092, val_acc:0.949]
Epoch [37/120    avg_loss:0.193, val_acc:0.861]
Epoch [38/120    avg_loss:0.224, val_acc:0.936]
Epoch [39/120    avg_loss:0.139, val_acc:0.951]
Epoch [40/120    avg_loss:0.094, val_acc:0.940]
Epoch [41/120    avg_loss:0.142, val_acc:0.942]
Epoch [42/120    avg_loss:0.072, val_acc:0.954]
Epoch [43/120    avg_loss:0.087, val_acc:0.932]
Epoch [44/120    avg_loss:0.074, val_acc:0.933]
Epoch [45/120    avg_loss:0.073, val_acc:0.919]
Epoch [46/120    avg_loss:0.084, val_acc:0.954]
Epoch [47/120    avg_loss:0.051, val_acc:0.955]
Epoch [48/120    avg_loss:0.060, val_acc:0.956]
Epoch [49/120    avg_loss:0.081, val_acc:0.940]
Epoch [50/120    avg_loss:0.118, val_acc:0.950]
Epoch [51/120    avg_loss:0.080, val_acc:0.956]
Epoch [52/120    avg_loss:0.060, val_acc:0.957]
Epoch [53/120    avg_loss:0.046, val_acc:0.954]
Epoch [54/120    avg_loss:0.042, val_acc:0.961]
Epoch [55/120    avg_loss:0.053, val_acc:0.964]
Epoch [56/120    avg_loss:0.046, val_acc:0.950]
Epoch [57/120    avg_loss:0.052, val_acc:0.963]
Epoch [58/120    avg_loss:0.035, val_acc:0.959]
Epoch [59/120    avg_loss:0.043, val_acc:0.954]
Epoch [60/120    avg_loss:0.072, val_acc:0.955]
Epoch [61/120    avg_loss:0.048, val_acc:0.951]
Epoch [62/120    avg_loss:0.045, val_acc:0.958]
Epoch [63/120    avg_loss:0.061, val_acc:0.958]
Epoch [64/120    avg_loss:0.041, val_acc:0.965]
Epoch [65/120    avg_loss:0.111, val_acc:0.927]
Epoch [66/120    avg_loss:0.076, val_acc:0.928]
Epoch [67/120    avg_loss:0.063, val_acc:0.918]
Epoch [68/120    avg_loss:0.082, val_acc:0.944]
Epoch [69/120    avg_loss:0.066, val_acc:0.923]
Epoch [70/120    avg_loss:0.120, val_acc:0.949]
Epoch [71/120    avg_loss:0.065, val_acc:0.956]
Epoch [72/120    avg_loss:0.054, val_acc:0.968]
Epoch [73/120    avg_loss:0.055, val_acc:0.966]
Epoch [74/120    avg_loss:0.070, val_acc:0.945]
Epoch [75/120    avg_loss:0.046, val_acc:0.968]
Epoch [76/120    avg_loss:0.039, val_acc:0.964]
Epoch [77/120    avg_loss:0.034, val_acc:0.961]
Epoch [78/120    avg_loss:0.037, val_acc:0.970]
Epoch [79/120    avg_loss:0.026, val_acc:0.967]
Epoch [80/120    avg_loss:0.036, val_acc:0.960]
Epoch [81/120    avg_loss:0.026, val_acc:0.951]
Epoch [82/120    avg_loss:0.028, val_acc:0.975]
Epoch [83/120    avg_loss:0.018, val_acc:0.973]
Epoch [84/120    avg_loss:0.032, val_acc:0.964]
Epoch [85/120    avg_loss:0.034, val_acc:0.939]
Epoch [86/120    avg_loss:0.026, val_acc:0.973]
Epoch [87/120    avg_loss:0.023, val_acc:0.961]
Epoch [88/120    avg_loss:0.020, val_acc:0.972]
Epoch [89/120    avg_loss:0.055, val_acc:0.918]
Epoch [90/120    avg_loss:0.064, val_acc:0.959]
Epoch [91/120    avg_loss:0.025, val_acc:0.958]
Epoch [92/120    avg_loss:0.023, val_acc:0.963]
Epoch [93/120    avg_loss:0.023, val_acc:0.960]
Epoch [94/120    avg_loss:0.020, val_acc:0.970]
Epoch [95/120    avg_loss:0.033, val_acc:0.966]
Epoch [96/120    avg_loss:0.015, val_acc:0.969]
Epoch [97/120    avg_loss:0.011, val_acc:0.973]
Epoch [98/120    avg_loss:0.015, val_acc:0.975]
Epoch [99/120    avg_loss:0.012, val_acc:0.974]
Epoch [100/120    avg_loss:0.013, val_acc:0.973]
Epoch [101/120    avg_loss:0.010, val_acc:0.973]
Epoch [102/120    avg_loss:0.015, val_acc:0.974]
Epoch [103/120    avg_loss:0.009, val_acc:0.972]
Epoch [104/120    avg_loss:0.012, val_acc:0.971]
Epoch [105/120    avg_loss:0.009, val_acc:0.974]
Epoch [106/120    avg_loss:0.009, val_acc:0.975]
Epoch [107/120    avg_loss:0.012, val_acc:0.974]
Epoch [108/120    avg_loss:0.012, val_acc:0.974]
Epoch [109/120    avg_loss:0.009, val_acc:0.978]
Epoch [110/120    avg_loss:0.014, val_acc:0.980]
Epoch [111/120    avg_loss:0.013, val_acc:0.977]
Epoch [112/120    avg_loss:0.011, val_acc:0.976]
Epoch [113/120    avg_loss:0.010, val_acc:0.977]
Epoch [114/120    avg_loss:0.009, val_acc:0.975]
Epoch [115/120    avg_loss:0.010, val_acc:0.975]
Epoch [116/120    avg_loss:0.012, val_acc:0.978]
Epoch [117/120    avg_loss:0.021, val_acc:0.976]
Epoch [118/120    avg_loss:0.012, val_acc:0.975]
Epoch [119/120    avg_loss:0.010, val_acc:0.978]
Epoch [120/120    avg_loss:0.010, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1245    1    3    0    1    0    0    0   10   14    0    0
     0   11    0]
 [   0    0    2  724    1    0    0    0    0    3    0    0    6    9
     2    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    2    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    4    0    0    0  836   30    0    0
     0    1    0]
 [   0    0    6    0    0    0    1    0    0    0    8 2186    6    1
     2    0    0]
 [   0    0    0    0    1    0    0    0    0    0    0    0  529    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1125   13    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    56  275    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.55013550135502

F1 scores:
[       nan 0.98765432 0.97954367 0.98369565 0.98839907 0.99653979
 0.97901049 1.         0.99883856 0.92307692 0.96647399 0.98401981
 0.98327138 0.97368421 0.96691018 0.85007728 0.97647059]

Kappa:
0.972054325741996
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff41867f898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.404, val_acc:0.505]
Epoch [2/120    avg_loss:1.887, val_acc:0.561]
Epoch [3/120    avg_loss:1.581, val_acc:0.608]
Epoch [4/120    avg_loss:1.332, val_acc:0.614]
Epoch [5/120    avg_loss:1.181, val_acc:0.668]
Epoch [6/120    avg_loss:0.946, val_acc:0.734]
Epoch [7/120    avg_loss:0.891, val_acc:0.748]
Epoch [8/120    avg_loss:0.754, val_acc:0.727]
Epoch [9/120    avg_loss:0.690, val_acc:0.749]
Epoch [10/120    avg_loss:0.709, val_acc:0.780]
Epoch [11/120    avg_loss:0.643, val_acc:0.766]
Epoch [12/120    avg_loss:0.584, val_acc:0.827]
Epoch [13/120    avg_loss:0.480, val_acc:0.790]
Epoch [14/120    avg_loss:0.496, val_acc:0.819]
Epoch [15/120    avg_loss:0.472, val_acc:0.803]
Epoch [16/120    avg_loss:0.356, val_acc:0.851]
Epoch [17/120    avg_loss:0.389, val_acc:0.855]
Epoch [18/120    avg_loss:0.336, val_acc:0.852]
Epoch [19/120    avg_loss:0.340, val_acc:0.819]
Epoch [20/120    avg_loss:0.309, val_acc:0.860]
Epoch [21/120    avg_loss:0.309, val_acc:0.848]
Epoch [22/120    avg_loss:0.276, val_acc:0.894]
Epoch [23/120    avg_loss:0.271, val_acc:0.875]
Epoch [24/120    avg_loss:0.238, val_acc:0.894]
Epoch [25/120    avg_loss:0.171, val_acc:0.892]
Epoch [26/120    avg_loss:0.192, val_acc:0.867]
Epoch [27/120    avg_loss:0.206, val_acc:0.900]
Epoch [28/120    avg_loss:0.190, val_acc:0.915]
Epoch [29/120    avg_loss:0.176, val_acc:0.877]
Epoch [30/120    avg_loss:0.175, val_acc:0.910]
Epoch [31/120    avg_loss:0.119, val_acc:0.913]
Epoch [32/120    avg_loss:0.165, val_acc:0.917]
Epoch [33/120    avg_loss:0.127, val_acc:0.892]
Epoch [34/120    avg_loss:0.138, val_acc:0.926]
Epoch [35/120    avg_loss:0.097, val_acc:0.899]
Epoch [36/120    avg_loss:0.125, val_acc:0.930]
Epoch [37/120    avg_loss:0.085, val_acc:0.933]
Epoch [38/120    avg_loss:0.110, val_acc:0.937]
Epoch [39/120    avg_loss:0.125, val_acc:0.920]
Epoch [40/120    avg_loss:0.105, val_acc:0.936]
Epoch [41/120    avg_loss:0.056, val_acc:0.931]
Epoch [42/120    avg_loss:0.075, val_acc:0.928]
Epoch [43/120    avg_loss:0.086, val_acc:0.942]
Epoch [44/120    avg_loss:0.081, val_acc:0.936]
Epoch [45/120    avg_loss:0.048, val_acc:0.941]
Epoch [46/120    avg_loss:0.061, val_acc:0.947]
Epoch [47/120    avg_loss:0.065, val_acc:0.946]
Epoch [48/120    avg_loss:0.053, val_acc:0.950]
Epoch [49/120    avg_loss:0.064, val_acc:0.941]
Epoch [50/120    avg_loss:0.098, val_acc:0.951]
Epoch [51/120    avg_loss:0.054, val_acc:0.899]
Epoch [52/120    avg_loss:0.107, val_acc:0.936]
Epoch [53/120    avg_loss:0.077, val_acc:0.936]
Epoch [54/120    avg_loss:0.050, val_acc:0.934]
Epoch [55/120    avg_loss:0.066, val_acc:0.941]
Epoch [56/120    avg_loss:0.041, val_acc:0.945]
Epoch [57/120    avg_loss:0.043, val_acc:0.922]
Epoch [58/120    avg_loss:0.045, val_acc:0.954]
Epoch [59/120    avg_loss:0.056, val_acc:0.957]
Epoch [60/120    avg_loss:0.101, val_acc:0.952]
Epoch [61/120    avg_loss:0.032, val_acc:0.964]
Epoch [62/120    avg_loss:0.060, val_acc:0.953]
Epoch [63/120    avg_loss:0.031, val_acc:0.955]
Epoch [64/120    avg_loss:0.025, val_acc:0.945]
Epoch [65/120    avg_loss:0.044, val_acc:0.960]
Epoch [66/120    avg_loss:0.034, val_acc:0.961]
Epoch [67/120    avg_loss:0.054, val_acc:0.952]
Epoch [68/120    avg_loss:0.037, val_acc:0.963]
Epoch [69/120    avg_loss:0.048, val_acc:0.925]
Epoch [70/120    avg_loss:0.048, val_acc:0.943]
Epoch [71/120    avg_loss:0.155, val_acc:0.908]
Epoch [72/120    avg_loss:0.074, val_acc:0.964]
Epoch [73/120    avg_loss:0.038, val_acc:0.935]
Epoch [74/120    avg_loss:0.050, val_acc:0.966]
Epoch [75/120    avg_loss:0.158, val_acc:0.922]
Epoch [76/120    avg_loss:0.067, val_acc:0.957]
Epoch [77/120    avg_loss:0.026, val_acc:0.964]
Epoch [78/120    avg_loss:0.040, val_acc:0.953]
Epoch [79/120    avg_loss:0.045, val_acc:0.966]
Epoch [80/120    avg_loss:0.036, val_acc:0.925]
Epoch [81/120    avg_loss:0.036, val_acc:0.974]
Epoch [82/120    avg_loss:0.027, val_acc:0.952]
Epoch [83/120    avg_loss:0.020, val_acc:0.977]
Epoch [84/120    avg_loss:0.017, val_acc:0.962]
Epoch [85/120    avg_loss:0.024, val_acc:0.968]
Epoch [86/120    avg_loss:0.020, val_acc:0.964]
Epoch [87/120    avg_loss:0.045, val_acc:0.931]
Epoch [88/120    avg_loss:0.054, val_acc:0.959]
Epoch [89/120    avg_loss:0.046, val_acc:0.954]
Epoch [90/120    avg_loss:0.043, val_acc:0.965]
Epoch [91/120    avg_loss:0.025, val_acc:0.959]
Epoch [92/120    avg_loss:0.020, val_acc:0.965]
Epoch [93/120    avg_loss:0.025, val_acc:0.970]
Epoch [94/120    avg_loss:0.015, val_acc:0.974]
Epoch [95/120    avg_loss:0.016, val_acc:0.959]
Epoch [96/120    avg_loss:0.023, val_acc:0.967]
Epoch [97/120    avg_loss:0.015, val_acc:0.971]
Epoch [98/120    avg_loss:0.011, val_acc:0.972]
Epoch [99/120    avg_loss:0.009, val_acc:0.970]
Epoch [100/120    avg_loss:0.010, val_acc:0.968]
Epoch [101/120    avg_loss:0.010, val_acc:0.970]
Epoch [102/120    avg_loss:0.011, val_acc:0.968]
Epoch [103/120    avg_loss:0.011, val_acc:0.968]
Epoch [104/120    avg_loss:0.007, val_acc:0.970]
Epoch [105/120    avg_loss:0.012, val_acc:0.974]
Epoch [106/120    avg_loss:0.010, val_acc:0.975]
Epoch [107/120    avg_loss:0.007, val_acc:0.976]
Epoch [108/120    avg_loss:0.008, val_acc:0.976]
Epoch [109/120    avg_loss:0.009, val_acc:0.977]
Epoch [110/120    avg_loss:0.006, val_acc:0.976]
Epoch [111/120    avg_loss:0.011, val_acc:0.972]
Epoch [112/120    avg_loss:0.007, val_acc:0.971]
Epoch [113/120    avg_loss:0.005, val_acc:0.972]
Epoch [114/120    avg_loss:0.011, val_acc:0.971]
Epoch [115/120    avg_loss:0.010, val_acc:0.973]
Epoch [116/120    avg_loss:0.007, val_acc:0.972]
Epoch [117/120    avg_loss:0.006, val_acc:0.974]
Epoch [118/120    avg_loss:0.010, val_acc:0.974]
Epoch [119/120    avg_loss:0.010, val_acc:0.973]
Epoch [120/120    avg_loss:0.006, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1250    4    2    7    1    0    0    0    6   11    4    0
     0    0    0]
 [   0    0    0  726    8    0    1    0    0    5    0    3    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    2    4    0    0    0  858    8    0    0
     0    3    0]
 [   0    0    3    0    0    5    1    0    0    4   18 2179    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    2  529    0
     0    2    1]
 [   0    0    0    2    0    0    0    0    0    0    0    0    0  183
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1130    6    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    56  283    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
97.86449864498645

F1 scores:
[       nan 0.98765432 0.98502758 0.98174442 0.97706422 0.97742664
 0.98644578 0.97959184 1.         0.77272727 0.97610922 0.98731309
 0.9823584  0.99456522 0.97079038 0.88299532 0.95705521]

Kappa:
0.9756525771164168
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc6fbd56860>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.409, val_acc:0.517]
Epoch [2/120    avg_loss:1.883, val_acc:0.582]
Epoch [3/120    avg_loss:1.589, val_acc:0.623]
Epoch [4/120    avg_loss:1.376, val_acc:0.713]
Epoch [5/120    avg_loss:1.102, val_acc:0.703]
Epoch [6/120    avg_loss:0.988, val_acc:0.706]
Epoch [7/120    avg_loss:0.822, val_acc:0.777]
Epoch [8/120    avg_loss:0.732, val_acc:0.791]
Epoch [9/120    avg_loss:0.685, val_acc:0.790]
Epoch [10/120    avg_loss:0.584, val_acc:0.810]
Epoch [11/120    avg_loss:0.598, val_acc:0.804]
Epoch [12/120    avg_loss:0.602, val_acc:0.835]
Epoch [13/120    avg_loss:0.486, val_acc:0.831]
Epoch [14/120    avg_loss:0.435, val_acc:0.823]
Epoch [15/120    avg_loss:0.414, val_acc:0.865]
Epoch [16/120    avg_loss:0.377, val_acc:0.809]
Epoch [17/120    avg_loss:0.329, val_acc:0.885]
Epoch [18/120    avg_loss:0.359, val_acc:0.817]
Epoch [19/120    avg_loss:0.340, val_acc:0.886]
Epoch [20/120    avg_loss:0.328, val_acc:0.903]
Epoch [21/120    avg_loss:0.250, val_acc:0.906]
Epoch [22/120    avg_loss:0.279, val_acc:0.899]
Epoch [23/120    avg_loss:0.215, val_acc:0.884]
Epoch [24/120    avg_loss:0.241, val_acc:0.867]
Epoch [25/120    avg_loss:0.213, val_acc:0.889]
Epoch [26/120    avg_loss:0.193, val_acc:0.919]
Epoch [27/120    avg_loss:0.185, val_acc:0.931]
Epoch [28/120    avg_loss:0.235, val_acc:0.904]
Epoch [29/120    avg_loss:0.228, val_acc:0.902]
Epoch [30/120    avg_loss:0.147, val_acc:0.924]
Epoch [31/120    avg_loss:0.148, val_acc:0.914]
Epoch [32/120    avg_loss:0.126, val_acc:0.935]
Epoch [33/120    avg_loss:0.111, val_acc:0.895]
Epoch [34/120    avg_loss:0.117, val_acc:0.947]
Epoch [35/120    avg_loss:0.106, val_acc:0.935]
Epoch [36/120    avg_loss:0.105, val_acc:0.931]
Epoch [37/120    avg_loss:0.110, val_acc:0.910]
Epoch [38/120    avg_loss:0.128, val_acc:0.932]
Epoch [39/120    avg_loss:0.130, val_acc:0.957]
Epoch [40/120    avg_loss:0.125, val_acc:0.919]
Epoch [41/120    avg_loss:0.085, val_acc:0.938]
Epoch [42/120    avg_loss:0.079, val_acc:0.929]
Epoch [43/120    avg_loss:0.119, val_acc:0.944]
Epoch [44/120    avg_loss:0.106, val_acc:0.936]
Epoch [45/120    avg_loss:0.129, val_acc:0.897]
Epoch [46/120    avg_loss:0.148, val_acc:0.920]
Epoch [47/120    avg_loss:0.111, val_acc:0.940]
Epoch [48/120    avg_loss:0.069, val_acc:0.954]
Epoch [49/120    avg_loss:0.070, val_acc:0.960]
Epoch [50/120    avg_loss:0.067, val_acc:0.967]
Epoch [51/120    avg_loss:0.066, val_acc:0.964]
Epoch [52/120    avg_loss:0.044, val_acc:0.973]
Epoch [53/120    avg_loss:0.063, val_acc:0.956]
Epoch [54/120    avg_loss:0.087, val_acc:0.950]
Epoch [55/120    avg_loss:0.075, val_acc:0.958]
Epoch [56/120    avg_loss:0.039, val_acc:0.954]
Epoch [57/120    avg_loss:0.044, val_acc:0.970]
Epoch [58/120    avg_loss:0.057, val_acc:0.955]
Epoch [59/120    avg_loss:0.048, val_acc:0.965]
Epoch [60/120    avg_loss:0.075, val_acc:0.971]
Epoch [61/120    avg_loss:0.033, val_acc:0.972]
Epoch [62/120    avg_loss:0.026, val_acc:0.974]
Epoch [63/120    avg_loss:0.029, val_acc:0.963]
Epoch [64/120    avg_loss:0.034, val_acc:0.964]
Epoch [65/120    avg_loss:0.025, val_acc:0.972]
Epoch [66/120    avg_loss:0.035, val_acc:0.955]
Epoch [67/120    avg_loss:0.037, val_acc:0.974]
Epoch [68/120    avg_loss:0.032, val_acc:0.975]
Epoch [69/120    avg_loss:0.035, val_acc:0.964]
Epoch [70/120    avg_loss:0.029, val_acc:0.963]
Epoch [71/120    avg_loss:0.036, val_acc:0.946]
Epoch [72/120    avg_loss:0.040, val_acc:0.959]
Epoch [73/120    avg_loss:0.047, val_acc:0.943]
Epoch [74/120    avg_loss:0.087, val_acc:0.950]
Epoch [75/120    avg_loss:0.039, val_acc:0.965]
Epoch [76/120    avg_loss:0.028, val_acc:0.977]
Epoch [77/120    avg_loss:0.021, val_acc:0.974]
Epoch [78/120    avg_loss:0.022, val_acc:0.973]
Epoch [79/120    avg_loss:0.028, val_acc:0.981]
Epoch [80/120    avg_loss:0.023, val_acc:0.972]
Epoch [81/120    avg_loss:0.039, val_acc:0.938]
Epoch [82/120    avg_loss:0.070, val_acc:0.973]
Epoch [83/120    avg_loss:0.038, val_acc:0.980]
Epoch [84/120    avg_loss:0.039, val_acc:0.970]
Epoch [85/120    avg_loss:0.064, val_acc:0.891]
Epoch [86/120    avg_loss:0.186, val_acc:0.958]
Epoch [87/120    avg_loss:0.159, val_acc:0.960]
Epoch [88/120    avg_loss:0.059, val_acc:0.972]
Epoch [89/120    avg_loss:0.038, val_acc:0.979]
Epoch [90/120    avg_loss:0.045, val_acc:0.968]
Epoch [91/120    avg_loss:0.041, val_acc:0.975]
Epoch [92/120    avg_loss:0.042, val_acc:0.969]
Epoch [93/120    avg_loss:0.021, val_acc:0.979]
Epoch [94/120    avg_loss:0.025, val_acc:0.978]
Epoch [95/120    avg_loss:0.017, val_acc:0.980]
Epoch [96/120    avg_loss:0.014, val_acc:0.979]
Epoch [97/120    avg_loss:0.014, val_acc:0.981]
Epoch [98/120    avg_loss:0.016, val_acc:0.980]
Epoch [99/120    avg_loss:0.012, val_acc:0.979]
Epoch [100/120    avg_loss:0.013, val_acc:0.983]
Epoch [101/120    avg_loss:0.011, val_acc:0.983]
Epoch [102/120    avg_loss:0.014, val_acc:0.986]
Epoch [103/120    avg_loss:0.015, val_acc:0.985]
Epoch [104/120    avg_loss:0.011, val_acc:0.985]
Epoch [105/120    avg_loss:0.012, val_acc:0.986]
Epoch [106/120    avg_loss:0.014, val_acc:0.988]
Epoch [107/120    avg_loss:0.010, val_acc:0.985]
Epoch [108/120    avg_loss:0.012, val_acc:0.989]
Epoch [109/120    avg_loss:0.010, val_acc:0.988]
Epoch [110/120    avg_loss:0.013, val_acc:0.990]
Epoch [111/120    avg_loss:0.011, val_acc:0.986]
Epoch [112/120    avg_loss:0.013, val_acc:0.989]
Epoch [113/120    avg_loss:0.011, val_acc:0.986]
Epoch [114/120    avg_loss:0.009, val_acc:0.990]
Epoch [115/120    avg_loss:0.007, val_acc:0.989]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.011, val_acc:0.989]
Epoch [118/120    avg_loss:0.010, val_acc:0.990]
Epoch [119/120    avg_loss:0.008, val_acc:0.985]
Epoch [120/120    avg_loss:0.014, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1252    7    3    0    1    0    0    0    9    9    3    0
     0    1    0]
 [   0    0    0  734    0    2    0    0    0    1    0    0    9    0
     0    1    0]
 [   0    0    0    0  211    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    2  847   22    0    0
     0    1    0]
 [   0    0    6    0    0    0    5    0    0    0   13 2181    3    0
     2    0    0]
 [   0    0    0    2    0    1    0    0    0    0    5    0  524    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1124   14    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    43  304    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.03794037940379

F1 scores:
[       nan 1.         0.98350353 0.9852349  0.9882904  0.99542334
 0.99240122 0.97959184 0.997669   0.92307692 0.968      0.98620846
 0.97217069 1.         0.97273907 0.91017964 0.98224852]

Kappa:
0.9776279733585403
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc585f1f7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.401, val_acc:0.502]
Epoch [2/120    avg_loss:1.842, val_acc:0.583]
Epoch [3/120    avg_loss:1.503, val_acc:0.670]
Epoch [4/120    avg_loss:1.300, val_acc:0.678]
Epoch [5/120    avg_loss:1.051, val_acc:0.744]
Epoch [6/120    avg_loss:0.869, val_acc:0.788]
Epoch [7/120    avg_loss:0.799, val_acc:0.720]
Epoch [8/120    avg_loss:0.742, val_acc:0.819]
Epoch [9/120    avg_loss:0.713, val_acc:0.791]
Epoch [10/120    avg_loss:0.610, val_acc:0.804]
Epoch [11/120    avg_loss:0.508, val_acc:0.829]
Epoch [12/120    avg_loss:0.519, val_acc:0.803]
Epoch [13/120    avg_loss:0.481, val_acc:0.872]
Epoch [14/120    avg_loss:0.403, val_acc:0.880]
Epoch [15/120    avg_loss:0.369, val_acc:0.840]
Epoch [16/120    avg_loss:0.329, val_acc:0.897]
Epoch [17/120    avg_loss:0.316, val_acc:0.865]
Epoch [18/120    avg_loss:0.413, val_acc:0.880]
Epoch [19/120    avg_loss:0.291, val_acc:0.888]
Epoch [20/120    avg_loss:0.296, val_acc:0.875]
Epoch [21/120    avg_loss:0.295, val_acc:0.862]
Epoch [22/120    avg_loss:0.251, val_acc:0.911]
Epoch [23/120    avg_loss:0.178, val_acc:0.894]
Epoch [24/120    avg_loss:0.149, val_acc:0.907]
Epoch [25/120    avg_loss:0.195, val_acc:0.908]
Epoch [26/120    avg_loss:0.174, val_acc:0.918]
Epoch [27/120    avg_loss:0.175, val_acc:0.938]
Epoch [28/120    avg_loss:0.132, val_acc:0.927]
Epoch [29/120    avg_loss:0.152, val_acc:0.930]
Epoch [30/120    avg_loss:0.253, val_acc:0.916]
Epoch [31/120    avg_loss:0.127, val_acc:0.943]
Epoch [32/120    avg_loss:0.147, val_acc:0.919]
Epoch [33/120    avg_loss:0.108, val_acc:0.941]
Epoch [34/120    avg_loss:0.112, val_acc:0.917]
Epoch [35/120    avg_loss:0.101, val_acc:0.949]
Epoch [36/120    avg_loss:0.095, val_acc:0.951]
Epoch [37/120    avg_loss:0.111, val_acc:0.954]
Epoch [38/120    avg_loss:0.084, val_acc:0.942]
Epoch [39/120    avg_loss:0.081, val_acc:0.946]
Epoch [40/120    avg_loss:0.082, val_acc:0.956]
Epoch [41/120    avg_loss:0.087, val_acc:0.934]
Epoch [42/120    avg_loss:0.085, val_acc:0.955]
Epoch [43/120    avg_loss:0.059, val_acc:0.965]
Epoch [44/120    avg_loss:0.042, val_acc:0.956]
Epoch [45/120    avg_loss:0.047, val_acc:0.972]
Epoch [46/120    avg_loss:0.063, val_acc:0.962]
Epoch [47/120    avg_loss:0.069, val_acc:0.961]
Epoch [48/120    avg_loss:0.047, val_acc:0.965]
Epoch [49/120    avg_loss:0.044, val_acc:0.976]
Epoch [50/120    avg_loss:0.039, val_acc:0.957]
Epoch [51/120    avg_loss:0.038, val_acc:0.971]
Epoch [52/120    avg_loss:0.057, val_acc:0.975]
Epoch [53/120    avg_loss:0.049, val_acc:0.958]
Epoch [54/120    avg_loss:0.044, val_acc:0.972]
Epoch [55/120    avg_loss:0.033, val_acc:0.968]
Epoch [56/120    avg_loss:0.052, val_acc:0.956]
Epoch [57/120    avg_loss:0.033, val_acc:0.964]
Epoch [58/120    avg_loss:0.054, val_acc:0.965]
Epoch [59/120    avg_loss:0.054, val_acc:0.951]
Epoch [60/120    avg_loss:0.046, val_acc:0.964]
Epoch [61/120    avg_loss:0.060, val_acc:0.969]
Epoch [62/120    avg_loss:0.038, val_acc:0.957]
Epoch [63/120    avg_loss:0.027, val_acc:0.967]
Epoch [64/120    avg_loss:0.023, val_acc:0.974]
Epoch [65/120    avg_loss:0.024, val_acc:0.974]
Epoch [66/120    avg_loss:0.019, val_acc:0.977]
Epoch [67/120    avg_loss:0.019, val_acc:0.977]
Epoch [68/120    avg_loss:0.018, val_acc:0.979]
Epoch [69/120    avg_loss:0.015, val_acc:0.979]
Epoch [70/120    avg_loss:0.017, val_acc:0.975]
Epoch [71/120    avg_loss:0.017, val_acc:0.977]
Epoch [72/120    avg_loss:0.015, val_acc:0.977]
Epoch [73/120    avg_loss:0.013, val_acc:0.978]
Epoch [74/120    avg_loss:0.019, val_acc:0.979]
Epoch [75/120    avg_loss:0.017, val_acc:0.978]
Epoch [76/120    avg_loss:0.016, val_acc:0.979]
Epoch [77/120    avg_loss:0.013, val_acc:0.980]
Epoch [78/120    avg_loss:0.014, val_acc:0.979]
Epoch [79/120    avg_loss:0.014, val_acc:0.980]
Epoch [80/120    avg_loss:0.011, val_acc:0.980]
Epoch [81/120    avg_loss:0.014, val_acc:0.981]
Epoch [82/120    avg_loss:0.014, val_acc:0.981]
Epoch [83/120    avg_loss:0.015, val_acc:0.979]
Epoch [84/120    avg_loss:0.012, val_acc:0.979]
Epoch [85/120    avg_loss:0.012, val_acc:0.980]
Epoch [86/120    avg_loss:0.013, val_acc:0.979]
Epoch [87/120    avg_loss:0.012, val_acc:0.980]
Epoch [88/120    avg_loss:0.013, val_acc:0.979]
Epoch [89/120    avg_loss:0.014, val_acc:0.977]
Epoch [90/120    avg_loss:0.012, val_acc:0.978]
Epoch [91/120    avg_loss:0.011, val_acc:0.979]
Epoch [92/120    avg_loss:0.012, val_acc:0.980]
Epoch [93/120    avg_loss:0.016, val_acc:0.979]
Epoch [94/120    avg_loss:0.015, val_acc:0.978]
Epoch [95/120    avg_loss:0.013, val_acc:0.980]
Epoch [96/120    avg_loss:0.009, val_acc:0.980]
Epoch [97/120    avg_loss:0.012, val_acc:0.980]
Epoch [98/120    avg_loss:0.012, val_acc:0.980]
Epoch [99/120    avg_loss:0.013, val_acc:0.980]
Epoch [100/120    avg_loss:0.009, val_acc:0.980]
Epoch [101/120    avg_loss:0.015, val_acc:0.980]
Epoch [102/120    avg_loss:0.011, val_acc:0.980]
Epoch [103/120    avg_loss:0.013, val_acc:0.980]
Epoch [104/120    avg_loss:0.012, val_acc:0.980]
Epoch [105/120    avg_loss:0.011, val_acc:0.980]
Epoch [106/120    avg_loss:0.010, val_acc:0.980]
Epoch [107/120    avg_loss:0.010, val_acc:0.980]
Epoch [108/120    avg_loss:0.013, val_acc:0.981]
Epoch [109/120    avg_loss:0.012, val_acc:0.981]
Epoch [110/120    avg_loss:0.016, val_acc:0.980]
Epoch [111/120    avg_loss:0.012, val_acc:0.980]
Epoch [112/120    avg_loss:0.011, val_acc:0.980]
Epoch [113/120    avg_loss:0.011, val_acc:0.980]
Epoch [114/120    avg_loss:0.010, val_acc:0.980]
Epoch [115/120    avg_loss:0.012, val_acc:0.980]
Epoch [116/120    avg_loss:0.011, val_acc:0.980]
Epoch [117/120    avg_loss:0.012, val_acc:0.980]
Epoch [118/120    avg_loss:0.013, val_acc:0.980]
Epoch [119/120    avg_loss:0.012, val_acc:0.980]
Epoch [120/120    avg_loss:0.012, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1259    3    0    5    0    0    0    2   12    4    0    0
     0    0    0]
 [   0    0    0  734    0    0    0    0    0    7    0    3    3    0
     0    0    0]
 [   0    0    0    0  211    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     2    2    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    1    4    0    0    0  840   24    3    0
     0    1    0]
 [   0    0   15    0    0    1    1    0    0    2   12 2175    1    0
     2    1    0]
 [   0    0    0    0    0    2    0    0    0    0    2    1  527    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1133    6    0]
 [   0    0    0    0    0    0   18    0    0    0    0    0    0    0
    50  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.76693766937669

F1 scores:
[       nan 0.975      0.98320968 0.98921833 0.99528302 0.98514286
 0.98050975 1.         0.99649942 0.76595745 0.96385542 0.98460842
 0.98046512 1.         0.9733677  0.87735849 0.97619048]

Kappa:
0.9745326356364139
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4b72392898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.440, val_acc:0.506]
Epoch [2/120    avg_loss:1.913, val_acc:0.568]
Epoch [3/120    avg_loss:1.584, val_acc:0.591]
Epoch [4/120    avg_loss:1.376, val_acc:0.653]
Epoch [5/120    avg_loss:1.116, val_acc:0.724]
Epoch [6/120    avg_loss:0.957, val_acc:0.693]
Epoch [7/120    avg_loss:0.886, val_acc:0.747]
Epoch [8/120    avg_loss:0.708, val_acc:0.771]
Epoch [9/120    avg_loss:0.711, val_acc:0.790]
Epoch [10/120    avg_loss:0.565, val_acc:0.803]
Epoch [11/120    avg_loss:0.566, val_acc:0.805]
Epoch [12/120    avg_loss:0.481, val_acc:0.839]
Epoch [13/120    avg_loss:0.418, val_acc:0.841]
Epoch [14/120    avg_loss:0.392, val_acc:0.857]
Epoch [15/120    avg_loss:0.362, val_acc:0.851]
Epoch [16/120    avg_loss:0.326, val_acc:0.885]
Epoch [17/120    avg_loss:0.335, val_acc:0.852]
Epoch [18/120    avg_loss:0.312, val_acc:0.887]
Epoch [19/120    avg_loss:0.249, val_acc:0.899]
Epoch [20/120    avg_loss:0.269, val_acc:0.865]
Epoch [21/120    avg_loss:0.217, val_acc:0.900]
Epoch [22/120    avg_loss:0.295, val_acc:0.884]
Epoch [23/120    avg_loss:0.202, val_acc:0.891]
Epoch [24/120    avg_loss:0.192, val_acc:0.911]
Epoch [25/120    avg_loss:0.204, val_acc:0.915]
Epoch [26/120    avg_loss:0.208, val_acc:0.898]
Epoch [27/120    avg_loss:0.203, val_acc:0.905]
Epoch [28/120    avg_loss:0.204, val_acc:0.885]
Epoch [29/120    avg_loss:0.177, val_acc:0.927]
Epoch [30/120    avg_loss:0.132, val_acc:0.925]
Epoch [31/120    avg_loss:0.200, val_acc:0.908]
Epoch [32/120    avg_loss:0.139, val_acc:0.935]
Epoch [33/120    avg_loss:0.114, val_acc:0.917]
Epoch [34/120    avg_loss:0.141, val_acc:0.876]
Epoch [35/120    avg_loss:0.124, val_acc:0.948]
Epoch [36/120    avg_loss:0.127, val_acc:0.923]
Epoch [37/120    avg_loss:0.083, val_acc:0.951]
Epoch [38/120    avg_loss:0.095, val_acc:0.949]
Epoch [39/120    avg_loss:0.096, val_acc:0.946]
Epoch [40/120    avg_loss:0.166, val_acc:0.894]
Epoch [41/120    avg_loss:0.120, val_acc:0.947]
Epoch [42/120    avg_loss:0.102, val_acc:0.920]
Epoch [43/120    avg_loss:0.101, val_acc:0.961]
Epoch [44/120    avg_loss:0.103, val_acc:0.949]
Epoch [45/120    avg_loss:0.128, val_acc:0.955]
Epoch [46/120    avg_loss:0.084, val_acc:0.964]
Epoch [47/120    avg_loss:0.067, val_acc:0.948]
Epoch [48/120    avg_loss:0.056, val_acc:0.960]
Epoch [49/120    avg_loss:0.051, val_acc:0.974]
Epoch [50/120    avg_loss:0.054, val_acc:0.974]
Epoch [51/120    avg_loss:0.047, val_acc:0.968]
Epoch [52/120    avg_loss:0.047, val_acc:0.967]
Epoch [53/120    avg_loss:0.034, val_acc:0.976]
Epoch [54/120    avg_loss:0.037, val_acc:0.963]
Epoch [55/120    avg_loss:0.061, val_acc:0.965]
Epoch [56/120    avg_loss:0.046, val_acc:0.946]
Epoch [57/120    avg_loss:0.045, val_acc:0.953]
Epoch [58/120    avg_loss:0.046, val_acc:0.975]
Epoch [59/120    avg_loss:0.041, val_acc:0.955]
Epoch [60/120    avg_loss:0.040, val_acc:0.961]
Epoch [61/120    avg_loss:0.037, val_acc:0.966]
Epoch [62/120    avg_loss:0.032, val_acc:0.975]
Epoch [63/120    avg_loss:0.026, val_acc:0.970]
Epoch [64/120    avg_loss:0.026, val_acc:0.970]
Epoch [65/120    avg_loss:0.025, val_acc:0.976]
Epoch [66/120    avg_loss:0.040, val_acc:0.958]
Epoch [67/120    avg_loss:0.044, val_acc:0.961]
Epoch [68/120    avg_loss:0.048, val_acc:0.969]
Epoch [69/120    avg_loss:0.055, val_acc:0.968]
Epoch [70/120    avg_loss:0.027, val_acc:0.977]
Epoch [71/120    avg_loss:0.038, val_acc:0.980]
Epoch [72/120    avg_loss:0.048, val_acc:0.956]
Epoch [73/120    avg_loss:0.040, val_acc:0.975]
Epoch [74/120    avg_loss:0.023, val_acc:0.972]
Epoch [75/120    avg_loss:0.020, val_acc:0.980]
Epoch [76/120    avg_loss:0.039, val_acc:0.940]
Epoch [77/120    avg_loss:0.061, val_acc:0.936]
Epoch [78/120    avg_loss:0.056, val_acc:0.965]
Epoch [79/120    avg_loss:0.040, val_acc:0.969]
Epoch [80/120    avg_loss:0.078, val_acc:0.964]
Epoch [81/120    avg_loss:0.064, val_acc:0.970]
Epoch [82/120    avg_loss:0.024, val_acc:0.982]
Epoch [83/120    avg_loss:0.036, val_acc:0.977]
Epoch [84/120    avg_loss:0.028, val_acc:0.976]
Epoch [85/120    avg_loss:0.043, val_acc:0.972]
Epoch [86/120    avg_loss:0.036, val_acc:0.977]
Epoch [87/120    avg_loss:0.030, val_acc:0.989]
Epoch [88/120    avg_loss:0.018, val_acc:0.971]
Epoch [89/120    avg_loss:0.018, val_acc:0.977]
Epoch [90/120    avg_loss:0.016, val_acc:0.969]
Epoch [91/120    avg_loss:0.017, val_acc:0.985]
Epoch [92/120    avg_loss:0.016, val_acc:0.989]
Epoch [93/120    avg_loss:0.020, val_acc:0.986]
Epoch [94/120    avg_loss:0.029, val_acc:0.970]
Epoch [95/120    avg_loss:0.026, val_acc:0.973]
Epoch [96/120    avg_loss:0.022, val_acc:0.977]
Epoch [97/120    avg_loss:0.035, val_acc:0.972]
Epoch [98/120    avg_loss:0.020, val_acc:0.983]
Epoch [99/120    avg_loss:0.017, val_acc:0.980]
Epoch [100/120    avg_loss:0.014, val_acc:0.985]
Epoch [101/120    avg_loss:0.019, val_acc:0.979]
Epoch [102/120    avg_loss:0.018, val_acc:0.980]
Epoch [103/120    avg_loss:0.011, val_acc:0.983]
Epoch [104/120    avg_loss:0.009, val_acc:0.984]
Epoch [105/120    avg_loss:0.018, val_acc:0.959]
Epoch [106/120    avg_loss:0.024, val_acc:0.977]
Epoch [107/120    avg_loss:0.020, val_acc:0.980]
Epoch [108/120    avg_loss:0.008, val_acc:0.981]
Epoch [109/120    avg_loss:0.010, val_acc:0.982]
Epoch [110/120    avg_loss:0.013, val_acc:0.982]
Epoch [111/120    avg_loss:0.007, val_acc:0.982]
Epoch [112/120    avg_loss:0.006, val_acc:0.982]
Epoch [113/120    avg_loss:0.007, val_acc:0.982]
Epoch [114/120    avg_loss:0.007, val_acc:0.984]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.011, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.989]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.985]
Epoch [120/120    avg_loss:0.007, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1254    1    9    1    0    0    0    0    9    8    0    0
     0    3    0]
 [   0    0    0  707    9    4    1    0    0    6    0    0   15    3
     2    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    3    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  423    0    0    0    7    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    2    0    0    0  859   12    0    0
     0    0    0]
 [   0    0    4    2    0    1    0    0    0    0    0 2203    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0    0  529    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    2    0    0    0    1    0    0    0
  1127    9    0]
 [   0    0    0    0    0    0   11    0    0    1    0    0    0    0
    41  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.08130081300813

F1 scores:
[       nan 1.         0.98546169 0.96982167 0.95945946 0.98969072
 0.98568199 0.94339623 0.99179367 0.8372093  0.98509174 0.99368516
 0.97511521 0.9919571  0.97533535 0.89770992 0.98823529]

Kappa:
0.978121746090002
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdff5d80898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.440, val_acc:0.505]
Epoch [2/120    avg_loss:1.842, val_acc:0.574]
Epoch [3/120    avg_loss:1.561, val_acc:0.551]
Epoch [4/120    avg_loss:1.317, val_acc:0.666]
Epoch [5/120    avg_loss:1.087, val_acc:0.651]
Epoch [6/120    avg_loss:0.973, val_acc:0.700]
Epoch [7/120    avg_loss:0.892, val_acc:0.709]
Epoch [8/120    avg_loss:0.854, val_acc:0.745]
Epoch [9/120    avg_loss:0.668, val_acc:0.780]
Epoch [10/120    avg_loss:0.641, val_acc:0.782]
Epoch [11/120    avg_loss:0.574, val_acc:0.792]
Epoch [12/120    avg_loss:0.514, val_acc:0.782]
Epoch [13/120    avg_loss:0.472, val_acc:0.837]
Epoch [14/120    avg_loss:0.401, val_acc:0.839]
Epoch [15/120    avg_loss:0.459, val_acc:0.843]
Epoch [16/120    avg_loss:0.349, val_acc:0.870]
Epoch [17/120    avg_loss:0.306, val_acc:0.849]
Epoch [18/120    avg_loss:0.262, val_acc:0.876]
Epoch [19/120    avg_loss:0.294, val_acc:0.886]
Epoch [20/120    avg_loss:0.273, val_acc:0.865]
Epoch [21/120    avg_loss:0.204, val_acc:0.868]
Epoch [22/120    avg_loss:0.247, val_acc:0.873]
Epoch [23/120    avg_loss:0.335, val_acc:0.875]
Epoch [24/120    avg_loss:0.218, val_acc:0.878]
Epoch [25/120    avg_loss:0.195, val_acc:0.899]
Epoch [26/120    avg_loss:0.157, val_acc:0.924]
Epoch [27/120    avg_loss:0.141, val_acc:0.894]
Epoch [28/120    avg_loss:0.164, val_acc:0.929]
Epoch [29/120    avg_loss:0.173, val_acc:0.909]
Epoch [30/120    avg_loss:0.207, val_acc:0.922]
Epoch [31/120    avg_loss:0.109, val_acc:0.916]
Epoch [32/120    avg_loss:0.113, val_acc:0.933]
Epoch [33/120    avg_loss:0.090, val_acc:0.941]
Epoch [34/120    avg_loss:0.121, val_acc:0.908]
Epoch [35/120    avg_loss:0.085, val_acc:0.945]
Epoch [36/120    avg_loss:0.106, val_acc:0.914]
Epoch [37/120    avg_loss:0.082, val_acc:0.945]
Epoch [38/120    avg_loss:0.109, val_acc:0.945]
Epoch [39/120    avg_loss:0.131, val_acc:0.882]
Epoch [40/120    avg_loss:0.149, val_acc:0.897]
Epoch [41/120    avg_loss:0.145, val_acc:0.938]
Epoch [42/120    avg_loss:0.157, val_acc:0.947]
Epoch [43/120    avg_loss:0.088, val_acc:0.945]
Epoch [44/120    avg_loss:0.084, val_acc:0.956]
Epoch [45/120    avg_loss:0.055, val_acc:0.960]
Epoch [46/120    avg_loss:0.057, val_acc:0.968]
Epoch [47/120    avg_loss:0.040, val_acc:0.960]
Epoch [48/120    avg_loss:0.040, val_acc:0.957]
Epoch [49/120    avg_loss:0.037, val_acc:0.969]
Epoch [50/120    avg_loss:0.077, val_acc:0.900]
Epoch [51/120    avg_loss:0.184, val_acc:0.943]
Epoch [52/120    avg_loss:0.076, val_acc:0.951]
Epoch [53/120    avg_loss:0.062, val_acc:0.961]
Epoch [54/120    avg_loss:0.044, val_acc:0.957]
Epoch [55/120    avg_loss:0.040, val_acc:0.962]
Epoch [56/120    avg_loss:0.061, val_acc:0.933]
Epoch [57/120    avg_loss:0.041, val_acc:0.968]
Epoch [58/120    avg_loss:0.045, val_acc:0.936]
Epoch [59/120    avg_loss:0.036, val_acc:0.969]
Epoch [60/120    avg_loss:0.036, val_acc:0.957]
Epoch [61/120    avg_loss:0.034, val_acc:0.961]
Epoch [62/120    avg_loss:0.048, val_acc:0.968]
Epoch [63/120    avg_loss:0.038, val_acc:0.957]
Epoch [64/120    avg_loss:0.039, val_acc:0.968]
Epoch [65/120    avg_loss:0.034, val_acc:0.969]
Epoch [66/120    avg_loss:0.025, val_acc:0.963]
Epoch [67/120    avg_loss:0.018, val_acc:0.976]
Epoch [68/120    avg_loss:0.070, val_acc:0.915]
Epoch [69/120    avg_loss:0.085, val_acc:0.967]
Epoch [70/120    avg_loss:0.035, val_acc:0.944]
Epoch [71/120    avg_loss:0.063, val_acc:0.960]
Epoch [72/120    avg_loss:0.058, val_acc:0.964]
Epoch [73/120    avg_loss:0.029, val_acc:0.970]
Epoch [74/120    avg_loss:0.023, val_acc:0.969]
Epoch [75/120    avg_loss:0.041, val_acc:0.968]
Epoch [76/120    avg_loss:0.026, val_acc:0.976]
Epoch [77/120    avg_loss:0.023, val_acc:0.980]
Epoch [78/120    avg_loss:0.021, val_acc:0.978]
Epoch [79/120    avg_loss:0.015, val_acc:0.965]
Epoch [80/120    avg_loss:0.023, val_acc:0.975]
Epoch [81/120    avg_loss:0.015, val_acc:0.984]
Epoch [82/120    avg_loss:0.012, val_acc:0.983]
Epoch [83/120    avg_loss:0.010, val_acc:0.984]
Epoch [84/120    avg_loss:0.007, val_acc:0.984]
Epoch [85/120    avg_loss:0.013, val_acc:0.983]
Epoch [86/120    avg_loss:0.017, val_acc:0.969]
Epoch [87/120    avg_loss:0.039, val_acc:0.982]
Epoch [88/120    avg_loss:0.014, val_acc:0.985]
Epoch [89/120    avg_loss:0.013, val_acc:0.985]
Epoch [90/120    avg_loss:0.009, val_acc:0.978]
Epoch [91/120    avg_loss:0.008, val_acc:0.982]
Epoch [92/120    avg_loss:0.009, val_acc:0.946]
Epoch [93/120    avg_loss:0.019, val_acc:0.978]
Epoch [94/120    avg_loss:0.013, val_acc:0.985]
Epoch [95/120    avg_loss:0.013, val_acc:0.980]
Epoch [96/120    avg_loss:0.007, val_acc:0.986]
Epoch [97/120    avg_loss:0.006, val_acc:0.985]
Epoch [98/120    avg_loss:0.007, val_acc:0.985]
Epoch [99/120    avg_loss:0.006, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.986]
Epoch [101/120    avg_loss:0.008, val_acc:0.980]
Epoch [102/120    avg_loss:0.008, val_acc:0.985]
Epoch [103/120    avg_loss:0.006, val_acc:0.984]
Epoch [104/120    avg_loss:0.006, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.031, val_acc:0.979]
Epoch [108/120    avg_loss:0.013, val_acc:0.981]
Epoch [109/120    avg_loss:0.010, val_acc:0.984]
Epoch [110/120    avg_loss:0.006, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.983]
Epoch [112/120    avg_loss:0.013, val_acc:0.982]
Epoch [113/120    avg_loss:0.007, val_acc:0.983]
Epoch [114/120    avg_loss:0.008, val_acc:0.983]
Epoch [115/120    avg_loss:0.015, val_acc:0.983]
Epoch [116/120    avg_loss:0.009, val_acc:0.983]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.005, val_acc:0.982]
Epoch [119/120    avg_loss:0.005, val_acc:0.981]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    6    5    0    1    0    0    1    5    5    0    0
     0    1    0]
 [   0    0    0  740    0    1    0    0    0    1    0    4    1    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    1    4    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    1    0    0    0  858   12    1    0
     0    2    0]
 [   0    0   12    0    0    5    0    0    0    0    0 2193    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    4    2  523    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0    0    0
  1127   10    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    58  284    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.21138211382114

F1 scores:
[       nan 0.97619048 0.98554123 0.98996656 0.98604651 0.9862069
 0.9908953  0.92592593 0.997669   0.94736842 0.98394495 0.99006772
 0.98679245 1.         0.96946237 0.88198758 0.97076023]

Kappa:
0.9795976190044392
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4f586b5828>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.429, val_acc:0.459]
Epoch [2/120    avg_loss:1.878, val_acc:0.517]
Epoch [3/120    avg_loss:1.604, val_acc:0.562]
Epoch [4/120    avg_loss:1.324, val_acc:0.614]
Epoch [5/120    avg_loss:1.178, val_acc:0.661]
Epoch [6/120    avg_loss:0.966, val_acc:0.737]
Epoch [7/120    avg_loss:0.929, val_acc:0.714]
Epoch [8/120    avg_loss:0.748, val_acc:0.732]
Epoch [9/120    avg_loss:0.639, val_acc:0.772]
Epoch [10/120    avg_loss:0.605, val_acc:0.783]
Epoch [11/120    avg_loss:0.525, val_acc:0.781]
Epoch [12/120    avg_loss:0.479, val_acc:0.788]
Epoch [13/120    avg_loss:0.439, val_acc:0.812]
Epoch [14/120    avg_loss:0.378, val_acc:0.850]
Epoch [15/120    avg_loss:0.322, val_acc:0.869]
Epoch [16/120    avg_loss:0.305, val_acc:0.858]
Epoch [17/120    avg_loss:0.306, val_acc:0.763]
Epoch [18/120    avg_loss:0.252, val_acc:0.857]
Epoch [19/120    avg_loss:0.221, val_acc:0.872]
Epoch [20/120    avg_loss:0.242, val_acc:0.855]
Epoch [21/120    avg_loss:0.205, val_acc:0.875]
Epoch [22/120    avg_loss:0.267, val_acc:0.827]
Epoch [23/120    avg_loss:0.267, val_acc:0.863]
Epoch [24/120    avg_loss:0.233, val_acc:0.889]
Epoch [25/120    avg_loss:0.234, val_acc:0.891]
Epoch [26/120    avg_loss:0.161, val_acc:0.929]
Epoch [27/120    avg_loss:0.133, val_acc:0.893]
Epoch [28/120    avg_loss:0.131, val_acc:0.925]
Epoch [29/120    avg_loss:0.113, val_acc:0.939]
Epoch [30/120    avg_loss:0.112, val_acc:0.940]
Epoch [31/120    avg_loss:0.099, val_acc:0.947]
Epoch [32/120    avg_loss:0.087, val_acc:0.932]
Epoch [33/120    avg_loss:0.108, val_acc:0.946]
Epoch [34/120    avg_loss:0.086, val_acc:0.942]
Epoch [35/120    avg_loss:0.081, val_acc:0.934]
Epoch [36/120    avg_loss:0.086, val_acc:0.937]
Epoch [37/120    avg_loss:0.081, val_acc:0.947]
Epoch [38/120    avg_loss:0.079, val_acc:0.946]
Epoch [39/120    avg_loss:0.055, val_acc:0.959]
Epoch [40/120    avg_loss:0.056, val_acc:0.958]
Epoch [41/120    avg_loss:0.061, val_acc:0.956]
Epoch [42/120    avg_loss:0.046, val_acc:0.965]
Epoch [43/120    avg_loss:0.066, val_acc:0.960]
Epoch [44/120    avg_loss:0.072, val_acc:0.944]
Epoch [45/120    avg_loss:0.045, val_acc:0.960]
Epoch [46/120    avg_loss:0.042, val_acc:0.968]
Epoch [47/120    avg_loss:0.052, val_acc:0.959]
Epoch [48/120    avg_loss:0.048, val_acc:0.956]
Epoch [49/120    avg_loss:0.048, val_acc:0.951]
Epoch [50/120    avg_loss:0.055, val_acc:0.966]
Epoch [51/120    avg_loss:0.147, val_acc:0.947]
Epoch [52/120    avg_loss:0.068, val_acc:0.963]
Epoch [53/120    avg_loss:0.040, val_acc:0.968]
Epoch [54/120    avg_loss:0.036, val_acc:0.976]
Epoch [55/120    avg_loss:0.038, val_acc:0.951]
Epoch [56/120    avg_loss:0.035, val_acc:0.966]
Epoch [57/120    avg_loss:0.029, val_acc:0.966]
Epoch [58/120    avg_loss:0.022, val_acc:0.965]
Epoch [59/120    avg_loss:0.028, val_acc:0.970]
Epoch [60/120    avg_loss:0.044, val_acc:0.972]
Epoch [61/120    avg_loss:0.017, val_acc:0.978]
Epoch [62/120    avg_loss:0.012, val_acc:0.978]
Epoch [63/120    avg_loss:0.025, val_acc:0.982]
Epoch [64/120    avg_loss:0.015, val_acc:0.980]
Epoch [65/120    avg_loss:0.016, val_acc:0.982]
Epoch [66/120    avg_loss:0.015, val_acc:0.979]
Epoch [67/120    avg_loss:0.026, val_acc:0.967]
Epoch [68/120    avg_loss:0.028, val_acc:0.968]
Epoch [69/120    avg_loss:0.021, val_acc:0.966]
Epoch [70/120    avg_loss:0.016, val_acc:0.978]
Epoch [71/120    avg_loss:0.029, val_acc:0.967]
Epoch [72/120    avg_loss:0.017, val_acc:0.982]
Epoch [73/120    avg_loss:0.013, val_acc:0.975]
Epoch [74/120    avg_loss:0.010, val_acc:0.971]
Epoch [75/120    avg_loss:0.010, val_acc:0.978]
Epoch [76/120    avg_loss:0.011, val_acc:0.980]
Epoch [77/120    avg_loss:0.029, val_acc:0.982]
Epoch [78/120    avg_loss:0.026, val_acc:0.966]
Epoch [79/120    avg_loss:0.034, val_acc:0.968]
Epoch [80/120    avg_loss:0.031, val_acc:0.978]
Epoch [81/120    avg_loss:0.011, val_acc:0.982]
Epoch [82/120    avg_loss:0.022, val_acc:0.968]
Epoch [83/120    avg_loss:0.017, val_acc:0.974]
Epoch [84/120    avg_loss:0.012, val_acc:0.972]
Epoch [85/120    avg_loss:0.370, val_acc:0.901]
Epoch [86/120    avg_loss:0.106, val_acc:0.951]
Epoch [87/120    avg_loss:0.055, val_acc:0.965]
Epoch [88/120    avg_loss:0.022, val_acc:0.972]
Epoch [89/120    avg_loss:0.020, val_acc:0.969]
Epoch [90/120    avg_loss:0.022, val_acc:0.981]
Epoch [91/120    avg_loss:0.011, val_acc:0.986]
Epoch [92/120    avg_loss:0.011, val_acc:0.972]
Epoch [93/120    avg_loss:0.022, val_acc:0.978]
Epoch [94/120    avg_loss:0.015, val_acc:0.982]
Epoch [95/120    avg_loss:0.014, val_acc:0.978]
Epoch [96/120    avg_loss:0.017, val_acc:0.973]
Epoch [97/120    avg_loss:0.021, val_acc:0.976]
Epoch [98/120    avg_loss:0.012, val_acc:0.982]
Epoch [99/120    avg_loss:0.010, val_acc:0.984]
Epoch [100/120    avg_loss:0.008, val_acc:0.978]
Epoch [101/120    avg_loss:0.017, val_acc:0.984]
Epoch [102/120    avg_loss:0.011, val_acc:0.979]
Epoch [103/120    avg_loss:0.007, val_acc:0.986]
Epoch [104/120    avg_loss:0.005, val_acc:0.983]
Epoch [105/120    avg_loss:0.006, val_acc:0.976]
Epoch [106/120    avg_loss:0.005, val_acc:0.985]
Epoch [107/120    avg_loss:0.006, val_acc:0.978]
Epoch [108/120    avg_loss:0.007, val_acc:0.985]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.007, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.007, val_acc:0.987]
Epoch [114/120    avg_loss:0.004, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.967]
Epoch [116/120    avg_loss:0.030, val_acc:0.981]
Epoch [117/120    avg_loss:0.019, val_acc:0.969]
Epoch [118/120    avg_loss:0.027, val_acc:0.975]
Epoch [119/120    avg_loss:0.008, val_acc:0.982]
Epoch [120/120    avg_loss:0.010, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1240   15    3    0    1    0    0    0    8   18    0    0
     0    0    0]
 [   0    0    0  734    0    0    0    0    0    0    1    6    5    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    0    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0  842   21    2    0
     7    2    0]
 [   0    0    0    3    0    0    1    0    0    0    1 2197    6    0
     2    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0    2  525    0
     0    2    3]
 [   0    0    0    1    0    0    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    63  280    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.78861788617886

F1 scores:
[       nan 0.98765432 0.98140087 0.97736352 0.99300699 0.99071926
 0.99469295 1.         1.         1.         0.97510133 0.98630752
 0.97947761 0.99728997 0.9599318  0.86956522 0.98245614]

Kappa:
0.9747643492546478
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f38313c9860>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.432, val_acc:0.475]
Epoch [2/120    avg_loss:1.892, val_acc:0.575]
Epoch [3/120    avg_loss:1.602, val_acc:0.621]
Epoch [4/120    avg_loss:1.370, val_acc:0.652]
Epoch [5/120    avg_loss:1.180, val_acc:0.700]
Epoch [6/120    avg_loss:0.978, val_acc:0.748]
Epoch [7/120    avg_loss:0.970, val_acc:0.735]
Epoch [8/120    avg_loss:0.869, val_acc:0.808]
Epoch [9/120    avg_loss:0.668, val_acc:0.778]
Epoch [10/120    avg_loss:0.649, val_acc:0.828]
Epoch [11/120    avg_loss:0.581, val_acc:0.797]
Epoch [12/120    avg_loss:0.569, val_acc:0.839]
Epoch [13/120    avg_loss:0.570, val_acc:0.833]
Epoch [14/120    avg_loss:0.431, val_acc:0.833]
Epoch [15/120    avg_loss:0.495, val_acc:0.833]
Epoch [16/120    avg_loss:0.390, val_acc:0.848]
Epoch [17/120    avg_loss:0.314, val_acc:0.839]
Epoch [18/120    avg_loss:0.350, val_acc:0.895]
Epoch [19/120    avg_loss:0.260, val_acc:0.898]
Epoch [20/120    avg_loss:0.265, val_acc:0.909]
Epoch [21/120    avg_loss:0.244, val_acc:0.864]
Epoch [22/120    avg_loss:0.264, val_acc:0.884]
Epoch [23/120    avg_loss:0.206, val_acc:0.883]
Epoch [24/120    avg_loss:0.189, val_acc:0.904]
Epoch [25/120    avg_loss:0.155, val_acc:0.924]
Epoch [26/120    avg_loss:0.190, val_acc:0.899]
Epoch [27/120    avg_loss:0.262, val_acc:0.923]
Epoch [28/120    avg_loss:0.182, val_acc:0.913]
Epoch [29/120    avg_loss:0.180, val_acc:0.930]
Epoch [30/120    avg_loss:0.211, val_acc:0.940]
Epoch [31/120    avg_loss:0.140, val_acc:0.924]
Epoch [32/120    avg_loss:0.152, val_acc:0.942]
Epoch [33/120    avg_loss:0.110, val_acc:0.922]
Epoch [34/120    avg_loss:0.139, val_acc:0.924]
Epoch [35/120    avg_loss:0.152, val_acc:0.914]
Epoch [36/120    avg_loss:0.204, val_acc:0.937]
Epoch [37/120    avg_loss:0.105, val_acc:0.944]
Epoch [38/120    avg_loss:0.094, val_acc:0.938]
Epoch [39/120    avg_loss:0.089, val_acc:0.943]
Epoch [40/120    avg_loss:0.089, val_acc:0.956]
Epoch [41/120    avg_loss:0.085, val_acc:0.940]
Epoch [42/120    avg_loss:0.078, val_acc:0.956]
Epoch [43/120    avg_loss:0.079, val_acc:0.955]
Epoch [44/120    avg_loss:0.066, val_acc:0.952]
Epoch [45/120    avg_loss:0.134, val_acc:0.916]
Epoch [46/120    avg_loss:0.103, val_acc:0.955]
Epoch [47/120    avg_loss:0.074, val_acc:0.956]
Epoch [48/120    avg_loss:0.083, val_acc:0.952]
Epoch [49/120    avg_loss:0.075, val_acc:0.940]
Epoch [50/120    avg_loss:0.072, val_acc:0.961]
Epoch [51/120    avg_loss:0.071, val_acc:0.961]
Epoch [52/120    avg_loss:0.057, val_acc:0.970]
Epoch [53/120    avg_loss:0.036, val_acc:0.961]
Epoch [54/120    avg_loss:0.044, val_acc:0.955]
Epoch [55/120    avg_loss:0.055, val_acc:0.971]
Epoch [56/120    avg_loss:0.040, val_acc:0.970]
Epoch [57/120    avg_loss:0.029, val_acc:0.968]
Epoch [58/120    avg_loss:0.040, val_acc:0.967]
Epoch [59/120    avg_loss:0.035, val_acc:0.973]
Epoch [60/120    avg_loss:0.033, val_acc:0.978]
Epoch [61/120    avg_loss:0.046, val_acc:0.964]
Epoch [62/120    avg_loss:0.035, val_acc:0.977]
Epoch [63/120    avg_loss:0.061, val_acc:0.957]
Epoch [64/120    avg_loss:0.247, val_acc:0.949]
Epoch [65/120    avg_loss:0.070, val_acc:0.971]
Epoch [66/120    avg_loss:0.046, val_acc:0.974]
Epoch [67/120    avg_loss:0.029, val_acc:0.971]
Epoch [68/120    avg_loss:0.075, val_acc:0.968]
Epoch [69/120    avg_loss:0.035, val_acc:0.977]
Epoch [70/120    avg_loss:0.036, val_acc:0.970]
Epoch [71/120    avg_loss:0.037, val_acc:0.952]
Epoch [72/120    avg_loss:0.036, val_acc:0.964]
Epoch [73/120    avg_loss:0.033, val_acc:0.970]
Epoch [74/120    avg_loss:0.022, val_acc:0.975]
Epoch [75/120    avg_loss:0.023, val_acc:0.978]
Epoch [76/120    avg_loss:0.021, val_acc:0.978]
Epoch [77/120    avg_loss:0.020, val_acc:0.977]
Epoch [78/120    avg_loss:0.018, val_acc:0.980]
Epoch [79/120    avg_loss:0.015, val_acc:0.982]
Epoch [80/120    avg_loss:0.019, val_acc:0.983]
Epoch [81/120    avg_loss:0.019, val_acc:0.983]
Epoch [82/120    avg_loss:0.016, val_acc:0.982]
Epoch [83/120    avg_loss:0.016, val_acc:0.978]
Epoch [84/120    avg_loss:0.013, val_acc:0.979]
Epoch [85/120    avg_loss:0.015, val_acc:0.980]
Epoch [86/120    avg_loss:0.018, val_acc:0.982]
Epoch [87/120    avg_loss:0.018, val_acc:0.983]
Epoch [88/120    avg_loss:0.024, val_acc:0.982]
Epoch [89/120    avg_loss:0.015, val_acc:0.981]
Epoch [90/120    avg_loss:0.016, val_acc:0.979]
Epoch [91/120    avg_loss:0.014, val_acc:0.980]
Epoch [92/120    avg_loss:0.016, val_acc:0.981]
Epoch [93/120    avg_loss:0.015, val_acc:0.981]
Epoch [94/120    avg_loss:0.011, val_acc:0.981]
Epoch [95/120    avg_loss:0.014, val_acc:0.980]
Epoch [96/120    avg_loss:0.013, val_acc:0.981]
Epoch [97/120    avg_loss:0.011, val_acc:0.981]
Epoch [98/120    avg_loss:0.019, val_acc:0.983]
Epoch [99/120    avg_loss:0.013, val_acc:0.982]
Epoch [100/120    avg_loss:0.015, val_acc:0.981]
Epoch [101/120    avg_loss:0.011, val_acc:0.980]
Epoch [102/120    avg_loss:0.012, val_acc:0.981]
Epoch [103/120    avg_loss:0.011, val_acc:0.979]
Epoch [104/120    avg_loss:0.011, val_acc:0.980]
Epoch [105/120    avg_loss:0.011, val_acc:0.979]
Epoch [106/120    avg_loss:0.010, val_acc:0.980]
Epoch [107/120    avg_loss:0.014, val_acc:0.981]
Epoch [108/120    avg_loss:0.013, val_acc:0.980]
Epoch [109/120    avg_loss:0.011, val_acc:0.983]
Epoch [110/120    avg_loss:0.011, val_acc:0.982]
Epoch [111/120    avg_loss:0.015, val_acc:0.983]
Epoch [112/120    avg_loss:0.012, val_acc:0.984]
Epoch [113/120    avg_loss:0.012, val_acc:0.983]
Epoch [114/120    avg_loss:0.012, val_acc:0.984]
Epoch [115/120    avg_loss:0.010, val_acc:0.983]
Epoch [116/120    avg_loss:0.016, val_acc:0.984]
Epoch [117/120    avg_loss:0.011, val_acc:0.985]
Epoch [118/120    avg_loss:0.010, val_acc:0.985]
Epoch [119/120    avg_loss:0.010, val_acc:0.984]
Epoch [120/120    avg_loss:0.008, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1240    9    0    0    0    0    0    0   11   23    0    0
     0    2    0]
 [   0    0    0  717    0    2    0    0    0    5    0    5   16    1
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    2    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    1  852   18    0    0
     1    1    0]
 [   0    0    2    0    0    1    0    0    0    3    0 2192   10    0
     2    0    0]
 [   0    0    0    0    0    0    0    0    0    0    6    5  518    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1125   13    0]
 [   0    0    0    1    0    0   23    0    0    0    0    0    0    0
    35  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.64769647696477

F1 scores:
[       nan 0.975      0.98062475 0.97286296 1.         0.99078341
 0.97974494 0.96153846 1.         0.8        0.97538638 0.98362127
 0.96014829 0.99730458 0.97613883 0.87938931 0.98224852]

Kappa:
0.9731648415286922
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4f17868860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.457, val_acc:0.409]
Epoch [2/120    avg_loss:1.970, val_acc:0.515]
Epoch [3/120    avg_loss:1.710, val_acc:0.571]
Epoch [4/120    avg_loss:1.517, val_acc:0.630]
Epoch [5/120    avg_loss:1.315, val_acc:0.644]
Epoch [6/120    avg_loss:1.101, val_acc:0.733]
Epoch [7/120    avg_loss:0.913, val_acc:0.730]
Epoch [8/120    avg_loss:0.840, val_acc:0.765]
Epoch [9/120    avg_loss:0.836, val_acc:0.770]
Epoch [10/120    avg_loss:0.643, val_acc:0.757]
Epoch [11/120    avg_loss:0.647, val_acc:0.760]
Epoch [12/120    avg_loss:0.567, val_acc:0.797]
Epoch [13/120    avg_loss:0.560, val_acc:0.796]
Epoch [14/120    avg_loss:0.503, val_acc:0.790]
Epoch [15/120    avg_loss:0.471, val_acc:0.866]
Epoch [16/120    avg_loss:0.393, val_acc:0.864]
Epoch [17/120    avg_loss:0.432, val_acc:0.821]
Epoch [18/120    avg_loss:0.343, val_acc:0.861]
Epoch [19/120    avg_loss:0.272, val_acc:0.842]
Epoch [20/120    avg_loss:0.365, val_acc:0.887]
Epoch [21/120    avg_loss:0.311, val_acc:0.816]
Epoch [22/120    avg_loss:0.315, val_acc:0.863]
Epoch [23/120    avg_loss:0.308, val_acc:0.893]
Epoch [24/120    avg_loss:0.227, val_acc:0.887]
Epoch [25/120    avg_loss:0.208, val_acc:0.901]
Epoch [26/120    avg_loss:0.216, val_acc:0.935]
Epoch [27/120    avg_loss:0.244, val_acc:0.913]
Epoch [28/120    avg_loss:0.201, val_acc:0.894]
Epoch [29/120    avg_loss:0.235, val_acc:0.925]
Epoch [30/120    avg_loss:0.164, val_acc:0.902]
Epoch [31/120    avg_loss:0.144, val_acc:0.924]
Epoch [32/120    avg_loss:0.146, val_acc:0.888]
Epoch [33/120    avg_loss:0.143, val_acc:0.943]
Epoch [34/120    avg_loss:0.120, val_acc:0.948]
Epoch [35/120    avg_loss:0.152, val_acc:0.912]
Epoch [36/120    avg_loss:0.149, val_acc:0.940]
Epoch [37/120    avg_loss:0.119, val_acc:0.918]
Epoch [38/120    avg_loss:0.124, val_acc:0.938]
Epoch [39/120    avg_loss:0.145, val_acc:0.915]
Epoch [40/120    avg_loss:0.141, val_acc:0.939]
Epoch [41/120    avg_loss:0.093, val_acc:0.952]
Epoch [42/120    avg_loss:0.077, val_acc:0.957]
Epoch [43/120    avg_loss:0.137, val_acc:0.947]
Epoch [44/120    avg_loss:0.090, val_acc:0.954]
Epoch [45/120    avg_loss:0.071, val_acc:0.948]
Epoch [46/120    avg_loss:0.079, val_acc:0.956]
Epoch [47/120    avg_loss:0.115, val_acc:0.947]
Epoch [48/120    avg_loss:0.100, val_acc:0.948]
Epoch [49/120    avg_loss:0.102, val_acc:0.934]
Epoch [50/120    avg_loss:0.065, val_acc:0.956]
Epoch [51/120    avg_loss:0.116, val_acc:0.939]
Epoch [52/120    avg_loss:0.093, val_acc:0.910]
Epoch [53/120    avg_loss:0.186, val_acc:0.910]
Epoch [54/120    avg_loss:0.122, val_acc:0.937]
Epoch [55/120    avg_loss:0.144, val_acc:0.931]
Epoch [56/120    avg_loss:0.082, val_acc:0.955]
Epoch [57/120    avg_loss:0.056, val_acc:0.955]
Epoch [58/120    avg_loss:0.057, val_acc:0.956]
Epoch [59/120    avg_loss:0.048, val_acc:0.964]
Epoch [60/120    avg_loss:0.059, val_acc:0.964]
Epoch [61/120    avg_loss:0.042, val_acc:0.966]
Epoch [62/120    avg_loss:0.044, val_acc:0.967]
Epoch [63/120    avg_loss:0.044, val_acc:0.967]
Epoch [64/120    avg_loss:0.051, val_acc:0.965]
Epoch [65/120    avg_loss:0.039, val_acc:0.968]
Epoch [66/120    avg_loss:0.041, val_acc:0.969]
Epoch [67/120    avg_loss:0.044, val_acc:0.969]
Epoch [68/120    avg_loss:0.034, val_acc:0.969]
Epoch [69/120    avg_loss:0.033, val_acc:0.968]
Epoch [70/120    avg_loss:0.034, val_acc:0.965]
Epoch [71/120    avg_loss:0.034, val_acc:0.966]
Epoch [72/120    avg_loss:0.040, val_acc:0.969]
Epoch [73/120    avg_loss:0.032, val_acc:0.969]
Epoch [74/120    avg_loss:0.034, val_acc:0.968]
Epoch [75/120    avg_loss:0.035, val_acc:0.971]
Epoch [76/120    avg_loss:0.038, val_acc:0.966]
Epoch [77/120    avg_loss:0.032, val_acc:0.964]
Epoch [78/120    avg_loss:0.031, val_acc:0.965]
Epoch [79/120    avg_loss:0.031, val_acc:0.967]
Epoch [80/120    avg_loss:0.037, val_acc:0.964]
Epoch [81/120    avg_loss:0.032, val_acc:0.969]
Epoch [82/120    avg_loss:0.033, val_acc:0.968]
Epoch [83/120    avg_loss:0.032, val_acc:0.969]
Epoch [84/120    avg_loss:0.034, val_acc:0.969]
Epoch [85/120    avg_loss:0.030, val_acc:0.969]
Epoch [86/120    avg_loss:0.030, val_acc:0.970]
Epoch [87/120    avg_loss:0.026, val_acc:0.968]
Epoch [88/120    avg_loss:0.034, val_acc:0.971]
Epoch [89/120    avg_loss:0.031, val_acc:0.971]
Epoch [90/120    avg_loss:0.030, val_acc:0.968]
Epoch [91/120    avg_loss:0.032, val_acc:0.970]
Epoch [92/120    avg_loss:0.027, val_acc:0.969]
Epoch [93/120    avg_loss:0.031, val_acc:0.970]
Epoch [94/120    avg_loss:0.037, val_acc:0.971]
Epoch [95/120    avg_loss:0.030, val_acc:0.972]
Epoch [96/120    avg_loss:0.034, val_acc:0.973]
Epoch [97/120    avg_loss:0.031, val_acc:0.970]
Epoch [98/120    avg_loss:0.027, val_acc:0.970]
Epoch [99/120    avg_loss:0.033, val_acc:0.972]
Epoch [100/120    avg_loss:0.025, val_acc:0.970]
Epoch [101/120    avg_loss:0.032, val_acc:0.969]
Epoch [102/120    avg_loss:0.023, val_acc:0.972]
Epoch [103/120    avg_loss:0.026, val_acc:0.969]
Epoch [104/120    avg_loss:0.042, val_acc:0.969]
Epoch [105/120    avg_loss:0.026, val_acc:0.973]
Epoch [106/120    avg_loss:0.032, val_acc:0.972]
Epoch [107/120    avg_loss:0.026, val_acc:0.974]
Epoch [108/120    avg_loss:0.025, val_acc:0.972]
Epoch [109/120    avg_loss:0.033, val_acc:0.975]
Epoch [110/120    avg_loss:0.022, val_acc:0.972]
Epoch [111/120    avg_loss:0.020, val_acc:0.971]
Epoch [112/120    avg_loss:0.028, val_acc:0.971]
Epoch [113/120    avg_loss:0.022, val_acc:0.971]
Epoch [114/120    avg_loss:0.028, val_acc:0.971]
Epoch [115/120    avg_loss:0.021, val_acc:0.972]
Epoch [116/120    avg_loss:0.025, val_acc:0.970]
Epoch [117/120    avg_loss:0.032, val_acc:0.973]
Epoch [118/120    avg_loss:0.025, val_acc:0.972]
Epoch [119/120    avg_loss:0.025, val_acc:0.971]
Epoch [120/120    avg_loss:0.025, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1230    2    0    1    0    0    0    1   22   24    5    0
     0    0    0]
 [   0    0    0  717    9    6    0    0    0    2    0    0   12    0
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    2    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    4    0    0    0    0  841   26    0    0
     0    0    0]
 [   0    1    6    0    0    0    2    0    0    1   16 2178    1    4
     0    0    1]
 [   0    0    0    0    4    5    0    0    0    0    0    1  522    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    1    1    0    0    0
  1127    9    0]
 [   0    0    0    0    0    0    8    0    0    3    0    0    0    0
    64  272    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.1490514905149

F1 scores:
[       nan 0.95       0.97425743 0.97750511 0.96803653 0.97732426
 0.98866213 1.         0.99767442 0.8        0.95676906 0.98108108
 0.97026022 0.98930481 0.96572408 0.86624204 0.97647059]

Kappa:
0.9674822062023848
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f54524e1860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.432, val_acc:0.517]
Epoch [2/120    avg_loss:1.937, val_acc:0.558]
Epoch [3/120    avg_loss:1.618, val_acc:0.592]
Epoch [4/120    avg_loss:1.388, val_acc:0.651]
Epoch [5/120    avg_loss:1.199, val_acc:0.689]
Epoch [6/120    avg_loss:1.033, val_acc:0.702]
Epoch [7/120    avg_loss:0.918, val_acc:0.735]
Epoch [8/120    avg_loss:0.748, val_acc:0.790]
Epoch [9/120    avg_loss:0.825, val_acc:0.767]
Epoch [10/120    avg_loss:0.711, val_acc:0.778]
Epoch [11/120    avg_loss:0.635, val_acc:0.767]
Epoch [12/120    avg_loss:0.579, val_acc:0.821]
Epoch [13/120    avg_loss:0.522, val_acc:0.799]
Epoch [14/120    avg_loss:0.516, val_acc:0.848]
Epoch [15/120    avg_loss:0.477, val_acc:0.847]
Epoch [16/120    avg_loss:0.472, val_acc:0.855]
Epoch [17/120    avg_loss:0.428, val_acc:0.830]
Epoch [18/120    avg_loss:0.342, val_acc:0.845]
Epoch [19/120    avg_loss:0.328, val_acc:0.849]
Epoch [20/120    avg_loss:0.321, val_acc:0.821]
Epoch [21/120    avg_loss:0.293, val_acc:0.892]
Epoch [22/120    avg_loss:0.263, val_acc:0.875]
Epoch [23/120    avg_loss:0.286, val_acc:0.855]
Epoch [24/120    avg_loss:0.313, val_acc:0.870]
Epoch [25/120    avg_loss:0.325, val_acc:0.895]
Epoch [26/120    avg_loss:0.249, val_acc:0.887]
Epoch [27/120    avg_loss:0.262, val_acc:0.885]
Epoch [28/120    avg_loss:0.202, val_acc:0.901]
Epoch [29/120    avg_loss:0.226, val_acc:0.860]
Epoch [30/120    avg_loss:0.260, val_acc:0.904]
Epoch [31/120    avg_loss:0.171, val_acc:0.915]
Epoch [32/120    avg_loss:0.189, val_acc:0.900]
Epoch [33/120    avg_loss:0.159, val_acc:0.909]
Epoch [34/120    avg_loss:0.233, val_acc:0.897]
Epoch [35/120    avg_loss:0.137, val_acc:0.918]
Epoch [36/120    avg_loss:0.174, val_acc:0.906]
Epoch [37/120    avg_loss:0.125, val_acc:0.933]
Epoch [38/120    avg_loss:0.148, val_acc:0.928]
Epoch [39/120    avg_loss:0.104, val_acc:0.927]
Epoch [40/120    avg_loss:0.099, val_acc:0.909]
Epoch [41/120    avg_loss:0.130, val_acc:0.922]
Epoch [42/120    avg_loss:0.105, val_acc:0.924]
Epoch [43/120    avg_loss:0.079, val_acc:0.938]
Epoch [44/120    avg_loss:0.080, val_acc:0.947]
Epoch [45/120    avg_loss:0.100, val_acc:0.934]
Epoch [46/120    avg_loss:0.090, val_acc:0.938]
Epoch [47/120    avg_loss:0.093, val_acc:0.917]
Epoch [48/120    avg_loss:0.077, val_acc:0.932]
Epoch [49/120    avg_loss:0.118, val_acc:0.913]
Epoch [50/120    avg_loss:0.087, val_acc:0.939]
Epoch [51/120    avg_loss:0.091, val_acc:0.939]
Epoch [52/120    avg_loss:0.082, val_acc:0.932]
Epoch [53/120    avg_loss:0.106, val_acc:0.941]
Epoch [54/120    avg_loss:0.132, val_acc:0.951]
Epoch [55/120    avg_loss:0.061, val_acc:0.943]
Epoch [56/120    avg_loss:0.080, val_acc:0.925]
Epoch [57/120    avg_loss:0.074, val_acc:0.943]
Epoch [58/120    avg_loss:0.060, val_acc:0.941]
Epoch [59/120    avg_loss:0.056, val_acc:0.936]
Epoch [60/120    avg_loss:0.101, val_acc:0.928]
Epoch [61/120    avg_loss:0.087, val_acc:0.927]
Epoch [62/120    avg_loss:0.068, val_acc:0.937]
Epoch [63/120    avg_loss:0.056, val_acc:0.946]
Epoch [64/120    avg_loss:0.061, val_acc:0.950]
Epoch [65/120    avg_loss:0.053, val_acc:0.959]
Epoch [66/120    avg_loss:0.078, val_acc:0.946]
Epoch [67/120    avg_loss:0.047, val_acc:0.939]
Epoch [68/120    avg_loss:0.062, val_acc:0.953]
Epoch [69/120    avg_loss:0.059, val_acc:0.964]
Epoch [70/120    avg_loss:0.040, val_acc:0.962]
Epoch [71/120    avg_loss:0.043, val_acc:0.932]
Epoch [72/120    avg_loss:0.044, val_acc:0.965]
Epoch [73/120    avg_loss:0.042, val_acc:0.952]
Epoch [74/120    avg_loss:0.039, val_acc:0.964]
Epoch [75/120    avg_loss:0.055, val_acc:0.962]
Epoch [76/120    avg_loss:0.025, val_acc:0.962]
Epoch [77/120    avg_loss:0.039, val_acc:0.931]
Epoch [78/120    avg_loss:0.057, val_acc:0.955]
Epoch [79/120    avg_loss:0.060, val_acc:0.959]
Epoch [80/120    avg_loss:0.044, val_acc:0.969]
Epoch [81/120    avg_loss:0.023, val_acc:0.964]
Epoch [82/120    avg_loss:0.021, val_acc:0.958]
Epoch [83/120    avg_loss:0.050, val_acc:0.963]
Epoch [84/120    avg_loss:0.035, val_acc:0.972]
Epoch [85/120    avg_loss:0.024, val_acc:0.966]
Epoch [86/120    avg_loss:0.044, val_acc:0.966]
Epoch [87/120    avg_loss:0.030, val_acc:0.962]
Epoch [88/120    avg_loss:0.037, val_acc:0.955]
Epoch [89/120    avg_loss:0.055, val_acc:0.959]
Epoch [90/120    avg_loss:0.067, val_acc:0.950]
Epoch [91/120    avg_loss:0.079, val_acc:0.944]
Epoch [92/120    avg_loss:0.044, val_acc:0.951]
Epoch [93/120    avg_loss:0.027, val_acc:0.963]
Epoch [94/120    avg_loss:0.025, val_acc:0.968]
Epoch [95/120    avg_loss:0.028, val_acc:0.962]
Epoch [96/120    avg_loss:0.049, val_acc:0.958]
Epoch [97/120    avg_loss:0.030, val_acc:0.964]
Epoch [98/120    avg_loss:0.018, val_acc:0.965]
Epoch [99/120    avg_loss:0.017, val_acc:0.970]
Epoch [100/120    avg_loss:0.016, val_acc:0.969]
Epoch [101/120    avg_loss:0.019, val_acc:0.971]
Epoch [102/120    avg_loss:0.016, val_acc:0.970]
Epoch [103/120    avg_loss:0.017, val_acc:0.972]
Epoch [104/120    avg_loss:0.014, val_acc:0.974]
Epoch [105/120    avg_loss:0.012, val_acc:0.973]
Epoch [106/120    avg_loss:0.013, val_acc:0.973]
Epoch [107/120    avg_loss:0.011, val_acc:0.973]
Epoch [108/120    avg_loss:0.011, val_acc:0.973]
Epoch [109/120    avg_loss:0.025, val_acc:0.974]
Epoch [110/120    avg_loss:0.014, val_acc:0.972]
Epoch [111/120    avg_loss:0.010, val_acc:0.972]
Epoch [112/120    avg_loss:0.011, val_acc:0.975]
Epoch [113/120    avg_loss:0.009, val_acc:0.973]
Epoch [114/120    avg_loss:0.010, val_acc:0.973]
Epoch [115/120    avg_loss:0.016, val_acc:0.975]
Epoch [116/120    avg_loss:0.014, val_acc:0.973]
Epoch [117/120    avg_loss:0.013, val_acc:0.973]
Epoch [118/120    avg_loss:0.011, val_acc:0.972]
Epoch [119/120    avg_loss:0.012, val_acc:0.973]
Epoch [120/120    avg_loss:0.012, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1253    1    0    0    0    0    0    2    8   20    1    0
     0    0    0]
 [   0    0    1  689    0    5    0    0    0   13    0    4   35    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    1  655    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    3    0    0    0    0  854   13    0    0
     1    2    0]
 [   0    0    6    0    0    9    1    0    0    1    5 2187    0    1
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    8   12  507    0
     1    3    1]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1137    2    0]
 [   0    0    0    0    0    0    6    0    0    2    0    0    0    0
    50  289    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.48509485094851

F1 scores:
[       nan 0.94871795 0.98390263 0.95827538 0.99764706 0.97643098
 0.99317665 1.         0.99883586 0.66666667 0.97377423 0.98380567
 0.93888889 0.99459459 0.97638471 0.89891135 0.98203593]

Kappa:
0.971309261282517
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f309e32b898>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.396, val_acc:0.486]
Epoch [2/120    avg_loss:1.867, val_acc:0.589]
Epoch [3/120    avg_loss:1.559, val_acc:0.653]
Epoch [4/120    avg_loss:1.340, val_acc:0.631]
Epoch [5/120    avg_loss:1.149, val_acc:0.690]
Epoch [6/120    avg_loss:1.002, val_acc:0.700]
Epoch [7/120    avg_loss:0.983, val_acc:0.718]
Epoch [8/120    avg_loss:0.820, val_acc:0.699]
Epoch [9/120    avg_loss:0.753, val_acc:0.791]
Epoch [10/120    avg_loss:0.668, val_acc:0.810]
Epoch [11/120    avg_loss:0.539, val_acc:0.823]
Epoch [12/120    avg_loss:0.548, val_acc:0.827]
Epoch [13/120    avg_loss:0.484, val_acc:0.798]
Epoch [14/120    avg_loss:0.424, val_acc:0.834]
Epoch [15/120    avg_loss:0.431, val_acc:0.862]
Epoch [16/120    avg_loss:0.422, val_acc:0.857]
Epoch [17/120    avg_loss:0.363, val_acc:0.889]
Epoch [18/120    avg_loss:0.311, val_acc:0.829]
Epoch [19/120    avg_loss:0.444, val_acc:0.834]
Epoch [20/120    avg_loss:0.351, val_acc:0.859]
Epoch [21/120    avg_loss:0.331, val_acc:0.865]
Epoch [22/120    avg_loss:0.228, val_acc:0.891]
Epoch [23/120    avg_loss:0.244, val_acc:0.905]
Epoch [24/120    avg_loss:0.258, val_acc:0.901]
Epoch [25/120    avg_loss:0.308, val_acc:0.904]
Epoch [26/120    avg_loss:0.185, val_acc:0.920]
Epoch [27/120    avg_loss:0.151, val_acc:0.923]
Epoch [28/120    avg_loss:0.158, val_acc:0.894]
Epoch [29/120    avg_loss:0.163, val_acc:0.926]
Epoch [30/120    avg_loss:0.152, val_acc:0.939]
Epoch [31/120    avg_loss:0.180, val_acc:0.940]
Epoch [32/120    avg_loss:0.198, val_acc:0.898]
Epoch [33/120    avg_loss:0.201, val_acc:0.941]
Epoch [34/120    avg_loss:0.114, val_acc:0.952]
Epoch [35/120    avg_loss:0.123, val_acc:0.949]
Epoch [36/120    avg_loss:0.114, val_acc:0.956]
Epoch [37/120    avg_loss:0.115, val_acc:0.957]
Epoch [38/120    avg_loss:0.130, val_acc:0.919]
Epoch [39/120    avg_loss:0.108, val_acc:0.958]
Epoch [40/120    avg_loss:0.131, val_acc:0.935]
Epoch [41/120    avg_loss:0.119, val_acc:0.963]
Epoch [42/120    avg_loss:0.088, val_acc:0.961]
Epoch [43/120    avg_loss:0.078, val_acc:0.906]
Epoch [44/120    avg_loss:0.148, val_acc:0.962]
Epoch [45/120    avg_loss:0.104, val_acc:0.946]
Epoch [46/120    avg_loss:0.097, val_acc:0.960]
Epoch [47/120    avg_loss:0.078, val_acc:0.953]
Epoch [48/120    avg_loss:0.062, val_acc:0.964]
Epoch [49/120    avg_loss:0.071, val_acc:0.942]
Epoch [50/120    avg_loss:0.098, val_acc:0.916]
Epoch [51/120    avg_loss:0.093, val_acc:0.941]
Epoch [52/120    avg_loss:0.055, val_acc:0.960]
Epoch [53/120    avg_loss:0.056, val_acc:0.945]
Epoch [54/120    avg_loss:0.084, val_acc:0.953]
Epoch [55/120    avg_loss:0.160, val_acc:0.864]
Epoch [56/120    avg_loss:0.129, val_acc:0.952]
Epoch [57/120    avg_loss:0.058, val_acc:0.970]
Epoch [58/120    avg_loss:0.054, val_acc:0.949]
Epoch [59/120    avg_loss:0.067, val_acc:0.966]
Epoch [60/120    avg_loss:0.183, val_acc:0.860]
Epoch [61/120    avg_loss:0.192, val_acc:0.806]
Epoch [62/120    avg_loss:0.159, val_acc:0.963]
Epoch [63/120    avg_loss:0.071, val_acc:0.971]
Epoch [64/120    avg_loss:0.046, val_acc:0.970]
Epoch [65/120    avg_loss:0.042, val_acc:0.971]
Epoch [66/120    avg_loss:0.030, val_acc:0.960]
Epoch [67/120    avg_loss:0.043, val_acc:0.978]
Epoch [68/120    avg_loss:0.047, val_acc:0.974]
Epoch [69/120    avg_loss:0.031, val_acc:0.977]
Epoch [70/120    avg_loss:0.031, val_acc:0.964]
Epoch [71/120    avg_loss:0.031, val_acc:0.980]
Epoch [72/120    avg_loss:0.029, val_acc:0.953]
Epoch [73/120    avg_loss:0.040, val_acc:0.964]
Epoch [74/120    avg_loss:0.042, val_acc:0.975]
Epoch [75/120    avg_loss:0.032, val_acc:0.977]
Epoch [76/120    avg_loss:0.039, val_acc:0.981]
Epoch [77/120    avg_loss:0.029, val_acc:0.983]
Epoch [78/120    avg_loss:0.019, val_acc:0.984]
Epoch [79/120    avg_loss:0.021, val_acc:0.980]
Epoch [80/120    avg_loss:0.018, val_acc:0.981]
Epoch [81/120    avg_loss:0.035, val_acc:0.978]
Epoch [82/120    avg_loss:0.043, val_acc:0.959]
Epoch [83/120    avg_loss:0.031, val_acc:0.980]
Epoch [84/120    avg_loss:0.023, val_acc:0.982]
Epoch [85/120    avg_loss:0.032, val_acc:0.980]
Epoch [86/120    avg_loss:0.026, val_acc:0.982]
Epoch [87/120    avg_loss:0.017, val_acc:0.985]
Epoch [88/120    avg_loss:0.019, val_acc:0.987]
Epoch [89/120    avg_loss:0.021, val_acc:0.984]
Epoch [90/120    avg_loss:0.011, val_acc:0.985]
Epoch [91/120    avg_loss:0.017, val_acc:0.984]
Epoch [92/120    avg_loss:0.020, val_acc:0.980]
Epoch [93/120    avg_loss:0.011, val_acc:0.989]
Epoch [94/120    avg_loss:0.009, val_acc:0.988]
Epoch [95/120    avg_loss:0.012, val_acc:0.988]
Epoch [96/120    avg_loss:0.011, val_acc:0.984]
Epoch [97/120    avg_loss:0.012, val_acc:0.983]
Epoch [98/120    avg_loss:0.021, val_acc:0.974]
Epoch [99/120    avg_loss:0.025, val_acc:0.987]
Epoch [100/120    avg_loss:0.022, val_acc:0.982]
Epoch [101/120    avg_loss:0.033, val_acc:0.978]
Epoch [102/120    avg_loss:0.030, val_acc:0.986]
Epoch [103/120    avg_loss:0.012, val_acc:0.983]
Epoch [104/120    avg_loss:0.013, val_acc:0.983]
Epoch [105/120    avg_loss:0.041, val_acc:0.976]
Epoch [106/120    avg_loss:0.021, val_acc:0.986]
Epoch [107/120    avg_loss:0.013, val_acc:0.985]
Epoch [108/120    avg_loss:0.006, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.009, val_acc:0.986]
Epoch [111/120    avg_loss:0.009, val_acc:0.987]
Epoch [112/120    avg_loss:0.007, val_acc:0.987]
Epoch [113/120    avg_loss:0.007, val_acc:0.987]
Epoch [114/120    avg_loss:0.015, val_acc:0.986]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.007, val_acc:0.989]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.989]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1250    0    9    0    1    0    0    0   12   10    1    0
     0    2    0]
 [   0    0    2  723    0    0    1    0    0    6    0    7    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    4    7    0    0    0  850   13    0    0
     0    0    0]
 [   0    0    8    5    0    1    2    0    0    0    7 2186    0    1
     0    0    0]
 [   0    0    0    3    0    7    0    0    0    0   11    5  505    0
     0    0    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    1    0    0    0
  1125   11    0]
 [   0    0    0    0    0    0   27    0    0    0    0    0    0    0
    18  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.77777777777777

F1 scores:
[       nan 0.975      0.98193244 0.97834912 0.97931034 0.97954545
 0.97113249 1.         0.99883856 0.85714286 0.96700796 0.98646209
 0.96282173 0.99459459 0.98425197 0.91238671 0.97647059]

Kappa:
0.974661325646821
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9f6a8c87f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.404, val_acc:0.518]
Epoch [2/120    avg_loss:1.863, val_acc:0.593]
Epoch [3/120    avg_loss:1.620, val_acc:0.610]
Epoch [4/120    avg_loss:1.395, val_acc:0.696]
Epoch [5/120    avg_loss:1.222, val_acc:0.722]
Epoch [6/120    avg_loss:1.000, val_acc:0.707]
Epoch [7/120    avg_loss:0.898, val_acc:0.750]
Epoch [8/120    avg_loss:0.778, val_acc:0.786]
Epoch [9/120    avg_loss:0.684, val_acc:0.784]
Epoch [10/120    avg_loss:0.667, val_acc:0.802]
Epoch [11/120    avg_loss:0.581, val_acc:0.822]
Epoch [12/120    avg_loss:0.607, val_acc:0.837]
Epoch [13/120    avg_loss:0.533, val_acc:0.819]
Epoch [14/120    avg_loss:0.454, val_acc:0.849]
Epoch [15/120    avg_loss:0.445, val_acc:0.828]
Epoch [16/120    avg_loss:0.413, val_acc:0.853]
Epoch [17/120    avg_loss:0.429, val_acc:0.859]
Epoch [18/120    avg_loss:0.393, val_acc:0.822]
Epoch [19/120    avg_loss:0.434, val_acc:0.856]
Epoch [20/120    avg_loss:0.356, val_acc:0.852]
Epoch [21/120    avg_loss:0.339, val_acc:0.892]
Epoch [22/120    avg_loss:0.337, val_acc:0.867]
Epoch [23/120    avg_loss:0.338, val_acc:0.819]
Epoch [24/120    avg_loss:0.353, val_acc:0.893]
Epoch [25/120    avg_loss:0.277, val_acc:0.871]
Epoch [26/120    avg_loss:0.256, val_acc:0.864]
Epoch [27/120    avg_loss:0.294, val_acc:0.884]
Epoch [28/120    avg_loss:0.261, val_acc:0.888]
Epoch [29/120    avg_loss:0.197, val_acc:0.914]
Epoch [30/120    avg_loss:0.223, val_acc:0.916]
Epoch [31/120    avg_loss:0.173, val_acc:0.921]
Epoch [32/120    avg_loss:0.159, val_acc:0.940]
Epoch [33/120    avg_loss:0.152, val_acc:0.929]
Epoch [34/120    avg_loss:0.122, val_acc:0.935]
Epoch [35/120    avg_loss:0.163, val_acc:0.933]
Epoch [36/120    avg_loss:0.146, val_acc:0.928]
Epoch [37/120    avg_loss:0.135, val_acc:0.920]
Epoch [38/120    avg_loss:0.098, val_acc:0.853]
Epoch [39/120    avg_loss:0.217, val_acc:0.931]
Epoch [40/120    avg_loss:0.135, val_acc:0.933]
Epoch [41/120    avg_loss:0.100, val_acc:0.955]
Epoch [42/120    avg_loss:0.111, val_acc:0.927]
Epoch [43/120    avg_loss:0.140, val_acc:0.944]
Epoch [44/120    avg_loss:0.091, val_acc:0.920]
Epoch [45/120    avg_loss:0.076, val_acc:0.952]
Epoch [46/120    avg_loss:0.165, val_acc:0.886]
Epoch [47/120    avg_loss:0.114, val_acc:0.940]
Epoch [48/120    avg_loss:0.075, val_acc:0.953]
Epoch [49/120    avg_loss:0.086, val_acc:0.946]
Epoch [50/120    avg_loss:0.059, val_acc:0.966]
Epoch [51/120    avg_loss:0.052, val_acc:0.944]
Epoch [52/120    avg_loss:0.068, val_acc:0.958]
Epoch [53/120    avg_loss:0.052, val_acc:0.958]
Epoch [54/120    avg_loss:0.099, val_acc:0.953]
Epoch [55/120    avg_loss:0.057, val_acc:0.970]
Epoch [56/120    avg_loss:0.069, val_acc:0.946]
Epoch [57/120    avg_loss:0.054, val_acc:0.960]
Epoch [58/120    avg_loss:0.083, val_acc:0.956]
Epoch [59/120    avg_loss:0.072, val_acc:0.942]
Epoch [60/120    avg_loss:0.079, val_acc:0.943]
Epoch [61/120    avg_loss:0.079, val_acc:0.953]
Epoch [62/120    avg_loss:0.053, val_acc:0.959]
Epoch [63/120    avg_loss:0.058, val_acc:0.953]
Epoch [64/120    avg_loss:0.044, val_acc:0.970]
Epoch [65/120    avg_loss:0.044, val_acc:0.972]
Epoch [66/120    avg_loss:0.102, val_acc:0.947]
Epoch [67/120    avg_loss:0.059, val_acc:0.967]
Epoch [68/120    avg_loss:0.038, val_acc:0.970]
Epoch [69/120    avg_loss:0.054, val_acc:0.962]
Epoch [70/120    avg_loss:0.042, val_acc:0.969]
Epoch [71/120    avg_loss:0.053, val_acc:0.936]
Epoch [72/120    avg_loss:0.053, val_acc:0.973]
Epoch [73/120    avg_loss:0.053, val_acc:0.977]
Epoch [74/120    avg_loss:0.055, val_acc:0.976]
Epoch [75/120    avg_loss:0.035, val_acc:0.977]
Epoch [76/120    avg_loss:0.056, val_acc:0.952]
Epoch [77/120    avg_loss:0.060, val_acc:0.976]
Epoch [78/120    avg_loss:0.106, val_acc:0.914]
Epoch [79/120    avg_loss:0.214, val_acc:0.935]
Epoch [80/120    avg_loss:0.138, val_acc:0.945]
Epoch [81/120    avg_loss:0.074, val_acc:0.964]
Epoch [82/120    avg_loss:0.061, val_acc:0.971]
Epoch [83/120    avg_loss:0.041, val_acc:0.964]
Epoch [84/120    avg_loss:0.043, val_acc:0.958]
Epoch [85/120    avg_loss:0.064, val_acc:0.963]
Epoch [86/120    avg_loss:0.125, val_acc:0.966]
Epoch [87/120    avg_loss:0.061, val_acc:0.959]
Epoch [88/120    avg_loss:0.047, val_acc:0.975]
Epoch [89/120    avg_loss:0.028, val_acc:0.977]
Epoch [90/120    avg_loss:0.020, val_acc:0.980]
Epoch [91/120    avg_loss:0.031, val_acc:0.977]
Epoch [92/120    avg_loss:0.017, val_acc:0.980]
Epoch [93/120    avg_loss:0.018, val_acc:0.978]
Epoch [94/120    avg_loss:0.018, val_acc:0.981]
Epoch [95/120    avg_loss:0.015, val_acc:0.980]
Epoch [96/120    avg_loss:0.023, val_acc:0.980]
Epoch [97/120    avg_loss:0.020, val_acc:0.982]
Epoch [98/120    avg_loss:0.014, val_acc:0.984]
Epoch [99/120    avg_loss:0.018, val_acc:0.984]
Epoch [100/120    avg_loss:0.015, val_acc:0.985]
Epoch [101/120    avg_loss:0.015, val_acc:0.985]
Epoch [102/120    avg_loss:0.015, val_acc:0.984]
Epoch [103/120    avg_loss:0.014, val_acc:0.984]
Epoch [104/120    avg_loss:0.013, val_acc:0.983]
Epoch [105/120    avg_loss:0.013, val_acc:0.984]
Epoch [106/120    avg_loss:0.023, val_acc:0.984]
Epoch [107/120    avg_loss:0.013, val_acc:0.985]
Epoch [108/120    avg_loss:0.013, val_acc:0.984]
Epoch [109/120    avg_loss:0.016, val_acc:0.984]
Epoch [110/120    avg_loss:0.012, val_acc:0.984]
Epoch [111/120    avg_loss:0.017, val_acc:0.987]
Epoch [112/120    avg_loss:0.011, val_acc:0.985]
Epoch [113/120    avg_loss:0.013, val_acc:0.985]
Epoch [114/120    avg_loss:0.014, val_acc:0.985]
Epoch [115/120    avg_loss:0.012, val_acc:0.985]
Epoch [116/120    avg_loss:0.018, val_acc:0.985]
Epoch [117/120    avg_loss:0.016, val_acc:0.986]
Epoch [118/120    avg_loss:0.013, val_acc:0.986]
Epoch [119/120    avg_loss:0.010, val_acc:0.986]
Epoch [120/120    avg_loss:0.012, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1264    1    4    0    1    0    0    0   11    4    0    0
     0    0    0]
 [   0    0    0  694    0    7    0    0    0    5    0    0   41    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    1    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    6    0    0    3    0    0    0    1  858    2    5    0
     0    0    0]
 [   0    0    6    0    0    2    2    0    0    0   11 2167   17    5
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0  531    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    2    0    0    0
  1131    4    0]
 [   0    0    0    0    0    0   22    0    0    0    0    0    0    0
    15  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.8970189701897

F1 scores:
[       nan 0.96202532 0.98711441 0.96121884 0.98598131 0.98409091
 0.97907324 1.         0.99767981 0.77272727 0.975      0.98859489
 0.93899204 0.98666667 0.98993435 0.93797277 0.97619048]

Kappa:
0.9760456758827692
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe2721b3828>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.448, val_acc:0.473]
Epoch [2/120    avg_loss:1.903, val_acc:0.599]
Epoch [3/120    avg_loss:1.621, val_acc:0.639]
Epoch [4/120    avg_loss:1.426, val_acc:0.692]
Epoch [5/120    avg_loss:1.226, val_acc:0.710]
Epoch [6/120    avg_loss:0.941, val_acc:0.751]
Epoch [7/120    avg_loss:0.846, val_acc:0.767]
Epoch [8/120    avg_loss:0.762, val_acc:0.784]
Epoch [9/120    avg_loss:0.695, val_acc:0.745]
Epoch [10/120    avg_loss:0.625, val_acc:0.813]
Epoch [11/120    avg_loss:0.676, val_acc:0.804]
Epoch [12/120    avg_loss:0.574, val_acc:0.858]
Epoch [13/120    avg_loss:0.459, val_acc:0.841]
Epoch [14/120    avg_loss:0.461, val_acc:0.844]
Epoch [15/120    avg_loss:0.428, val_acc:0.851]
Epoch [16/120    avg_loss:0.358, val_acc:0.849]
Epoch [17/120    avg_loss:0.339, val_acc:0.839]
Epoch [18/120    avg_loss:0.388, val_acc:0.878]
Epoch [19/120    avg_loss:0.353, val_acc:0.884]
Epoch [20/120    avg_loss:0.332, val_acc:0.859]
Epoch [21/120    avg_loss:0.264, val_acc:0.889]
Epoch [22/120    avg_loss:0.230, val_acc:0.904]
Epoch [23/120    avg_loss:0.300, val_acc:0.913]
Epoch [24/120    avg_loss:0.213, val_acc:0.889]
Epoch [25/120    avg_loss:0.212, val_acc:0.928]
Epoch [26/120    avg_loss:0.209, val_acc:0.918]
Epoch [27/120    avg_loss:0.177, val_acc:0.926]
Epoch [28/120    avg_loss:0.224, val_acc:0.900]
Epoch [29/120    avg_loss:0.165, val_acc:0.928]
Epoch [30/120    avg_loss:0.168, val_acc:0.919]
Epoch [31/120    avg_loss:0.139, val_acc:0.937]
Epoch [32/120    avg_loss:0.177, val_acc:0.932]
Epoch [33/120    avg_loss:0.152, val_acc:0.935]
Epoch [34/120    avg_loss:0.128, val_acc:0.931]
Epoch [35/120    avg_loss:0.182, val_acc:0.886]
Epoch [36/120    avg_loss:0.216, val_acc:0.927]
Epoch [37/120    avg_loss:0.223, val_acc:0.929]
Epoch [38/120    avg_loss:0.107, val_acc:0.948]
Epoch [39/120    avg_loss:0.092, val_acc:0.942]
Epoch [40/120    avg_loss:0.096, val_acc:0.947]
Epoch [41/120    avg_loss:0.133, val_acc:0.926]
Epoch [42/120    avg_loss:0.111, val_acc:0.953]
Epoch [43/120    avg_loss:0.098, val_acc:0.944]
Epoch [44/120    avg_loss:0.126, val_acc:0.960]
Epoch [45/120    avg_loss:0.110, val_acc:0.957]
Epoch [46/120    avg_loss:0.108, val_acc:0.941]
Epoch [47/120    avg_loss:0.077, val_acc:0.942]
Epoch [48/120    avg_loss:0.086, val_acc:0.963]
Epoch [49/120    avg_loss:0.071, val_acc:0.959]
Epoch [50/120    avg_loss:0.067, val_acc:0.962]
Epoch [51/120    avg_loss:0.110, val_acc:0.953]
Epoch [52/120    avg_loss:0.093, val_acc:0.952]
Epoch [53/120    avg_loss:0.062, val_acc:0.959]
Epoch [54/120    avg_loss:0.073, val_acc:0.956]
Epoch [55/120    avg_loss:0.098, val_acc:0.967]
Epoch [56/120    avg_loss:0.059, val_acc:0.962]
Epoch [57/120    avg_loss:0.036, val_acc:0.966]
Epoch [58/120    avg_loss:0.046, val_acc:0.966]
Epoch [59/120    avg_loss:0.055, val_acc:0.966]
Epoch [60/120    avg_loss:0.042, val_acc:0.973]
Epoch [61/120    avg_loss:0.041, val_acc:0.950]
Epoch [62/120    avg_loss:0.054, val_acc:0.963]
Epoch [63/120    avg_loss:0.133, val_acc:0.925]
Epoch [64/120    avg_loss:0.052, val_acc:0.967]
Epoch [65/120    avg_loss:0.056, val_acc:0.972]
Epoch [66/120    avg_loss:0.044, val_acc:0.966]
Epoch [67/120    avg_loss:0.035, val_acc:0.972]
Epoch [68/120    avg_loss:0.039, val_acc:0.972]
Epoch [69/120    avg_loss:0.030, val_acc:0.969]
Epoch [70/120    avg_loss:0.042, val_acc:0.971]
Epoch [71/120    avg_loss:0.036, val_acc:0.960]
Epoch [72/120    avg_loss:0.035, val_acc:0.974]
Epoch [73/120    avg_loss:0.021, val_acc:0.975]
Epoch [74/120    avg_loss:0.028, val_acc:0.971]
Epoch [75/120    avg_loss:0.027, val_acc:0.974]
Epoch [76/120    avg_loss:0.046, val_acc:0.972]
Epoch [77/120    avg_loss:0.038, val_acc:0.957]
Epoch [78/120    avg_loss:0.043, val_acc:0.968]
Epoch [79/120    avg_loss:0.024, val_acc:0.972]
Epoch [80/120    avg_loss:0.027, val_acc:0.967]
Epoch [81/120    avg_loss:0.036, val_acc:0.965]
Epoch [82/120    avg_loss:0.088, val_acc:0.956]
Epoch [83/120    avg_loss:0.037, val_acc:0.980]
Epoch [84/120    avg_loss:0.038, val_acc:0.971]
Epoch [85/120    avg_loss:0.078, val_acc:0.968]
Epoch [86/120    avg_loss:0.055, val_acc:0.978]
Epoch [87/120    avg_loss:0.015, val_acc:0.980]
Epoch [88/120    avg_loss:0.027, val_acc:0.980]
Epoch [89/120    avg_loss:0.024, val_acc:0.982]
Epoch [90/120    avg_loss:0.022, val_acc:0.966]
Epoch [91/120    avg_loss:0.020, val_acc:0.974]
Epoch [92/120    avg_loss:0.013, val_acc:0.978]
Epoch [93/120    avg_loss:0.032, val_acc:0.967]
Epoch [94/120    avg_loss:0.025, val_acc:0.980]
Epoch [95/120    avg_loss:0.036, val_acc:0.973]
Epoch [96/120    avg_loss:0.025, val_acc:0.972]
Epoch [97/120    avg_loss:0.024, val_acc:0.981]
Epoch [98/120    avg_loss:0.021, val_acc:0.979]
Epoch [99/120    avg_loss:0.020, val_acc:0.983]
Epoch [100/120    avg_loss:0.026, val_acc:0.972]
Epoch [101/120    avg_loss:0.039, val_acc:0.949]
Epoch [102/120    avg_loss:0.037, val_acc:0.978]
Epoch [103/120    avg_loss:0.015, val_acc:0.983]
Epoch [104/120    avg_loss:0.014, val_acc:0.981]
Epoch [105/120    avg_loss:0.039, val_acc:0.964]
Epoch [106/120    avg_loss:0.049, val_acc:0.981]
Epoch [107/120    avg_loss:0.009, val_acc:0.985]
Epoch [108/120    avg_loss:0.009, val_acc:0.983]
Epoch [109/120    avg_loss:0.027, val_acc:0.978]
Epoch [110/120    avg_loss:0.031, val_acc:0.959]
Epoch [111/120    avg_loss:0.031, val_acc:0.986]
Epoch [112/120    avg_loss:0.023, val_acc:0.971]
Epoch [113/120    avg_loss:0.019, val_acc:0.981]
Epoch [114/120    avg_loss:0.019, val_acc:0.979]
Epoch [115/120    avg_loss:0.011, val_acc:0.974]
Epoch [116/120    avg_loss:0.018, val_acc:0.973]
Epoch [117/120    avg_loss:0.040, val_acc:0.981]
Epoch [118/120    avg_loss:0.011, val_acc:0.976]
Epoch [119/120    avg_loss:0.010, val_acc:0.982]
Epoch [120/120    avg_loss:0.005, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    1 1271    0    1    0    1    0    0    0    6    5    0    0
     0    0    0]
 [   0    0    0  713    0    3    0    0    0   10    0    3   17    0
     0    1    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    0    1    0    0    0  860    1    0    0
     1    3    0]
 [   0    0   15    1    0    0    3    0    0    0    5 2166   17    2
     0    1    0]
 [   0    0    2    0    0    2    0    0    0    0    9    0  516    0
     0    4    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1137    1    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    58  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.76693766937669

F1 scores:
[       nan 0.93670886 0.98450813 0.9747095  0.99294118 0.99082569
 0.98867925 1.         1.         0.7826087  0.97727273 0.98791334
 0.95027624 0.99462366 0.97179487 0.88087774 0.98203593]

Kappa:
0.974543555576548
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb4e7911860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.366, val_acc:0.506]
Epoch [2/120    avg_loss:1.895, val_acc:0.619]
Epoch [3/120    avg_loss:1.618, val_acc:0.655]
Epoch [4/120    avg_loss:1.310, val_acc:0.656]
Epoch [5/120    avg_loss:1.207, val_acc:0.685]
Epoch [6/120    avg_loss:0.999, val_acc:0.691]
Epoch [7/120    avg_loss:0.950, val_acc:0.768]
Epoch [8/120    avg_loss:0.749, val_acc:0.766]
Epoch [9/120    avg_loss:0.823, val_acc:0.763]
Epoch [10/120    avg_loss:0.674, val_acc:0.749]
Epoch [11/120    avg_loss:0.635, val_acc:0.792]
Epoch [12/120    avg_loss:0.520, val_acc:0.792]
Epoch [13/120    avg_loss:0.530, val_acc:0.817]
Epoch [14/120    avg_loss:0.463, val_acc:0.819]
Epoch [15/120    avg_loss:0.433, val_acc:0.832]
Epoch [16/120    avg_loss:0.392, val_acc:0.858]
Epoch [17/120    avg_loss:0.360, val_acc:0.821]
Epoch [18/120    avg_loss:0.455, val_acc:0.843]
Epoch [19/120    avg_loss:0.457, val_acc:0.849]
Epoch [20/120    avg_loss:0.328, val_acc:0.878]
Epoch [21/120    avg_loss:0.256, val_acc:0.858]
Epoch [22/120    avg_loss:0.256, val_acc:0.876]
Epoch [23/120    avg_loss:0.268, val_acc:0.911]
Epoch [24/120    avg_loss:0.204, val_acc:0.896]
Epoch [25/120    avg_loss:0.240, val_acc:0.903]
Epoch [26/120    avg_loss:0.177, val_acc:0.913]
Epoch [27/120    avg_loss:0.203, val_acc:0.900]
Epoch [28/120    avg_loss:0.163, val_acc:0.924]
Epoch [29/120    avg_loss:0.134, val_acc:0.924]
Epoch [30/120    avg_loss:0.158, val_acc:0.924]
Epoch [31/120    avg_loss:0.142, val_acc:0.931]
Epoch [32/120    avg_loss:0.137, val_acc:0.909]
Epoch [33/120    avg_loss:0.161, val_acc:0.934]
Epoch [34/120    avg_loss:0.097, val_acc:0.925]
Epoch [35/120    avg_loss:0.091, val_acc:0.941]
Epoch [36/120    avg_loss:0.092, val_acc:0.959]
Epoch [37/120    avg_loss:0.086, val_acc:0.953]
Epoch [38/120    avg_loss:0.107, val_acc:0.949]
Epoch [39/120    avg_loss:0.101, val_acc:0.943]
Epoch [40/120    avg_loss:0.088, val_acc:0.918]
Epoch [41/120    avg_loss:0.095, val_acc:0.936]
Epoch [42/120    avg_loss:0.097, val_acc:0.940]
Epoch [43/120    avg_loss:0.116, val_acc:0.943]
Epoch [44/120    avg_loss:0.089, val_acc:0.922]
Epoch [45/120    avg_loss:0.083, val_acc:0.938]
Epoch [46/120    avg_loss:0.081, val_acc:0.934]
Epoch [47/120    avg_loss:0.084, val_acc:0.957]
Epoch [48/120    avg_loss:0.076, val_acc:0.946]
Epoch [49/120    avg_loss:0.066, val_acc:0.923]
Epoch [50/120    avg_loss:0.094, val_acc:0.961]
Epoch [51/120    avg_loss:0.045, val_acc:0.963]
Epoch [52/120    avg_loss:0.039, val_acc:0.966]
Epoch [53/120    avg_loss:0.035, val_acc:0.968]
Epoch [54/120    avg_loss:0.033, val_acc:0.968]
Epoch [55/120    avg_loss:0.035, val_acc:0.969]
Epoch [56/120    avg_loss:0.031, val_acc:0.969]
Epoch [57/120    avg_loss:0.034, val_acc:0.971]
Epoch [58/120    avg_loss:0.038, val_acc:0.969]
Epoch [59/120    avg_loss:0.030, val_acc:0.970]
Epoch [60/120    avg_loss:0.026, val_acc:0.971]
Epoch [61/120    avg_loss:0.030, val_acc:0.971]
Epoch [62/120    avg_loss:0.030, val_acc:0.968]
Epoch [63/120    avg_loss:0.028, val_acc:0.970]
Epoch [64/120    avg_loss:0.028, val_acc:0.969]
Epoch [65/120    avg_loss:0.024, val_acc:0.970]
Epoch [66/120    avg_loss:0.031, val_acc:0.976]
Epoch [67/120    avg_loss:0.026, val_acc:0.976]
Epoch [68/120    avg_loss:0.030, val_acc:0.970]
Epoch [69/120    avg_loss:0.026, val_acc:0.974]
Epoch [70/120    avg_loss:0.028, val_acc:0.976]
Epoch [71/120    avg_loss:0.025, val_acc:0.974]
Epoch [72/120    avg_loss:0.026, val_acc:0.975]
Epoch [73/120    avg_loss:0.027, val_acc:0.974]
Epoch [74/120    avg_loss:0.024, val_acc:0.977]
Epoch [75/120    avg_loss:0.025, val_acc:0.976]
Epoch [76/120    avg_loss:0.025, val_acc:0.977]
Epoch [77/120    avg_loss:0.030, val_acc:0.976]
Epoch [78/120    avg_loss:0.027, val_acc:0.975]
Epoch [79/120    avg_loss:0.025, val_acc:0.977]
Epoch [80/120    avg_loss:0.030, val_acc:0.978]
Epoch [81/120    avg_loss:0.026, val_acc:0.975]
Epoch [82/120    avg_loss:0.028, val_acc:0.976]
Epoch [83/120    avg_loss:0.029, val_acc:0.976]
Epoch [84/120    avg_loss:0.028, val_acc:0.977]
Epoch [85/120    avg_loss:0.029, val_acc:0.975]
Epoch [86/120    avg_loss:0.024, val_acc:0.978]
Epoch [87/120    avg_loss:0.024, val_acc:0.978]
Epoch [88/120    avg_loss:0.024, val_acc:0.978]
Epoch [89/120    avg_loss:0.020, val_acc:0.978]
Epoch [90/120    avg_loss:0.024, val_acc:0.979]
Epoch [91/120    avg_loss:0.033, val_acc:0.979]
Epoch [92/120    avg_loss:0.025, val_acc:0.979]
Epoch [93/120    avg_loss:0.024, val_acc:0.978]
Epoch [94/120    avg_loss:0.024, val_acc:0.980]
Epoch [95/120    avg_loss:0.022, val_acc:0.979]
Epoch [96/120    avg_loss:0.025, val_acc:0.978]
Epoch [97/120    avg_loss:0.022, val_acc:0.975]
Epoch [98/120    avg_loss:0.026, val_acc:0.980]
Epoch [99/120    avg_loss:0.029, val_acc:0.980]
Epoch [100/120    avg_loss:0.018, val_acc:0.980]
Epoch [101/120    avg_loss:0.023, val_acc:0.978]
Epoch [102/120    avg_loss:0.022, val_acc:0.980]
Epoch [103/120    avg_loss:0.017, val_acc:0.982]
Epoch [104/120    avg_loss:0.020, val_acc:0.983]
Epoch [105/120    avg_loss:0.020, val_acc:0.980]
Epoch [106/120    avg_loss:0.026, val_acc:0.978]
Epoch [107/120    avg_loss:0.021, val_acc:0.982]
Epoch [108/120    avg_loss:0.026, val_acc:0.979]
Epoch [109/120    avg_loss:0.023, val_acc:0.980]
Epoch [110/120    avg_loss:0.019, val_acc:0.982]
Epoch [111/120    avg_loss:0.020, val_acc:0.981]
Epoch [112/120    avg_loss:0.021, val_acc:0.981]
Epoch [113/120    avg_loss:0.027, val_acc:0.978]
Epoch [114/120    avg_loss:0.020, val_acc:0.978]
Epoch [115/120    avg_loss:0.024, val_acc:0.980]
Epoch [116/120    avg_loss:0.021, val_acc:0.981]
Epoch [117/120    avg_loss:0.020, val_acc:0.982]
Epoch [118/120    avg_loss:0.018, val_acc:0.982]
Epoch [119/120    avg_loss:0.021, val_acc:0.981]
Epoch [120/120    avg_loss:0.019, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1255    3    0    1    1    0    0    0   11   14    0    0
     0    0    0]
 [   0    0    2  719    5    0    0    0    0    8    2    1    7    1
     2    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   11    1    3    2    4    0    0    0  832   19    0    0
     0    3    0]
 [   0    0    2    0    0    0    3    0    1    1   12 2188    1    2
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    3   10  512    0
     1    2    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1130    7    0]
 [   0    0    0    0    0    0   11    0    0    2    0    0    0    0
    36  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.72357723577235

F1 scores:
[       nan 0.94871795 0.98238748 0.97823129 0.98156682 0.992
 0.98422239 0.98039216 0.99767981 0.76595745 0.95632184 0.9849201
 0.971537   0.9919571  0.97877869 0.90715373 0.98245614]

Kappa:
0.9740339660252892
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f84c75757f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.394, val_acc:0.488]
Epoch [2/120    avg_loss:1.906, val_acc:0.562]
Epoch [3/120    avg_loss:1.628, val_acc:0.605]
Epoch [4/120    avg_loss:1.387, val_acc:0.578]
Epoch [5/120    avg_loss:1.166, val_acc:0.662]
Epoch [6/120    avg_loss:1.034, val_acc:0.696]
Epoch [7/120    avg_loss:0.850, val_acc:0.745]
Epoch [8/120    avg_loss:1.054, val_acc:0.645]
Epoch [9/120    avg_loss:0.781, val_acc:0.792]
Epoch [10/120    avg_loss:0.706, val_acc:0.800]
Epoch [11/120    avg_loss:0.675, val_acc:0.754]
Epoch [12/120    avg_loss:0.555, val_acc:0.812]
Epoch [13/120    avg_loss:0.482, val_acc:0.837]
Epoch [14/120    avg_loss:0.423, val_acc:0.836]
Epoch [15/120    avg_loss:0.437, val_acc:0.824]
Epoch [16/120    avg_loss:0.425, val_acc:0.860]
Epoch [17/120    avg_loss:0.341, val_acc:0.871]
Epoch [18/120    avg_loss:0.325, val_acc:0.890]
Epoch [19/120    avg_loss:0.339, val_acc:0.878]
Epoch [20/120    avg_loss:0.400, val_acc:0.877]
Epoch [21/120    avg_loss:0.350, val_acc:0.882]
Epoch [22/120    avg_loss:0.269, val_acc:0.907]
Epoch [23/120    avg_loss:0.265, val_acc:0.892]
Epoch [24/120    avg_loss:0.252, val_acc:0.877]
Epoch [25/120    avg_loss:0.222, val_acc:0.924]
Epoch [26/120    avg_loss:0.221, val_acc:0.895]
Epoch [27/120    avg_loss:0.228, val_acc:0.914]
Epoch [28/120    avg_loss:0.216, val_acc:0.911]
Epoch [29/120    avg_loss:0.213, val_acc:0.921]
Epoch [30/120    avg_loss:0.192, val_acc:0.918]
Epoch [31/120    avg_loss:0.172, val_acc:0.945]
Epoch [32/120    avg_loss:0.157, val_acc:0.941]
Epoch [33/120    avg_loss:0.190, val_acc:0.931]
Epoch [34/120    avg_loss:0.157, val_acc:0.936]
Epoch [35/120    avg_loss:0.164, val_acc:0.926]
Epoch [36/120    avg_loss:0.115, val_acc:0.954]
Epoch [37/120    avg_loss:0.101, val_acc:0.948]
Epoch [38/120    avg_loss:0.093, val_acc:0.954]
Epoch [39/120    avg_loss:0.110, val_acc:0.951]
Epoch [40/120    avg_loss:0.114, val_acc:0.935]
Epoch [41/120    avg_loss:0.114, val_acc:0.834]
Epoch [42/120    avg_loss:0.276, val_acc:0.889]
Epoch [43/120    avg_loss:0.127, val_acc:0.940]
Epoch [44/120    avg_loss:0.085, val_acc:0.954]
Epoch [45/120    avg_loss:0.093, val_acc:0.950]
Epoch [46/120    avg_loss:0.094, val_acc:0.933]
Epoch [47/120    avg_loss:0.120, val_acc:0.954]
Epoch [48/120    avg_loss:0.117, val_acc:0.956]
Epoch [49/120    avg_loss:0.070, val_acc:0.951]
Epoch [50/120    avg_loss:0.083, val_acc:0.938]
Epoch [51/120    avg_loss:0.085, val_acc:0.953]
Epoch [52/120    avg_loss:0.072, val_acc:0.945]
Epoch [53/120    avg_loss:0.128, val_acc:0.944]
Epoch [54/120    avg_loss:0.084, val_acc:0.925]
Epoch [55/120    avg_loss:0.098, val_acc:0.941]
Epoch [56/120    avg_loss:0.108, val_acc:0.939]
Epoch [57/120    avg_loss:0.068, val_acc:0.947]
Epoch [58/120    avg_loss:0.058, val_acc:0.971]
Epoch [59/120    avg_loss:0.080, val_acc:0.951]
Epoch [60/120    avg_loss:0.067, val_acc:0.953]
Epoch [61/120    avg_loss:0.075, val_acc:0.940]
Epoch [62/120    avg_loss:0.076, val_acc:0.958]
Epoch [63/120    avg_loss:0.055, val_acc:0.963]
Epoch [64/120    avg_loss:0.034, val_acc:0.972]
Epoch [65/120    avg_loss:0.051, val_acc:0.970]
Epoch [66/120    avg_loss:0.044, val_acc:0.964]
Epoch [67/120    avg_loss:0.037, val_acc:0.975]
Epoch [68/120    avg_loss:0.035, val_acc:0.983]
Epoch [69/120    avg_loss:0.092, val_acc:0.913]
Epoch [70/120    avg_loss:0.240, val_acc:0.912]
Epoch [71/120    avg_loss:0.094, val_acc:0.951]
Epoch [72/120    avg_loss:0.065, val_acc:0.953]
Epoch [73/120    avg_loss:0.047, val_acc:0.969]
Epoch [74/120    avg_loss:0.037, val_acc:0.976]
Epoch [75/120    avg_loss:0.048, val_acc:0.975]
Epoch [76/120    avg_loss:0.047, val_acc:0.965]
Epoch [77/120    avg_loss:0.083, val_acc:0.944]
Epoch [78/120    avg_loss:0.069, val_acc:0.962]
Epoch [79/120    avg_loss:0.063, val_acc:0.972]
Epoch [80/120    avg_loss:0.043, val_acc:0.969]
Epoch [81/120    avg_loss:0.033, val_acc:0.968]
Epoch [82/120    avg_loss:0.034, val_acc:0.975]
Epoch [83/120    avg_loss:0.027, val_acc:0.978]
Epoch [84/120    avg_loss:0.021, val_acc:0.976]
Epoch [85/120    avg_loss:0.018, val_acc:0.978]
Epoch [86/120    avg_loss:0.017, val_acc:0.977]
Epoch [87/120    avg_loss:0.023, val_acc:0.978]
Epoch [88/120    avg_loss:0.021, val_acc:0.976]
Epoch [89/120    avg_loss:0.025, val_acc:0.978]
Epoch [90/120    avg_loss:0.017, val_acc:0.977]
Epoch [91/120    avg_loss:0.016, val_acc:0.978]
Epoch [92/120    avg_loss:0.018, val_acc:0.978]
Epoch [93/120    avg_loss:0.021, val_acc:0.979]
Epoch [94/120    avg_loss:0.021, val_acc:0.979]
Epoch [95/120    avg_loss:0.018, val_acc:0.978]
Epoch [96/120    avg_loss:0.022, val_acc:0.980]
Epoch [97/120    avg_loss:0.019, val_acc:0.980]
Epoch [98/120    avg_loss:0.022, val_acc:0.979]
Epoch [99/120    avg_loss:0.017, val_acc:0.977]
Epoch [100/120    avg_loss:0.029, val_acc:0.978]
Epoch [101/120    avg_loss:0.020, val_acc:0.978]
Epoch [102/120    avg_loss:0.013, val_acc:0.978]
Epoch [103/120    avg_loss:0.018, val_acc:0.978]
Epoch [104/120    avg_loss:0.014, val_acc:0.978]
Epoch [105/120    avg_loss:0.017, val_acc:0.978]
Epoch [106/120    avg_loss:0.015, val_acc:0.978]
Epoch [107/120    avg_loss:0.015, val_acc:0.978]
Epoch [108/120    avg_loss:0.020, val_acc:0.978]
Epoch [109/120    avg_loss:0.019, val_acc:0.978]
Epoch [110/120    avg_loss:0.018, val_acc:0.978]
Epoch [111/120    avg_loss:0.013, val_acc:0.978]
Epoch [112/120    avg_loss:0.019, val_acc:0.978]
Epoch [113/120    avg_loss:0.022, val_acc:0.978]
Epoch [114/120    avg_loss:0.021, val_acc:0.978]
Epoch [115/120    avg_loss:0.018, val_acc:0.978]
Epoch [116/120    avg_loss:0.018, val_acc:0.978]
Epoch [117/120    avg_loss:0.018, val_acc:0.978]
Epoch [118/120    avg_loss:0.018, val_acc:0.978]
Epoch [119/120    avg_loss:0.017, val_acc:0.978]
Epoch [120/120    avg_loss:0.014, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    1 1251    1    0    0    0    0    0   10   10   11    1    0
     0    0    0]
 [   0    0    0  726    0    0    2    0    0   13    0    0    4    1
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    2    0    4    6    0    0    9  837    6    0    0
     0    3    0]
 [   0    0    8    1    0    0    7    0    0    4    3 2182    4    0
     0    1    0]
 [   0    0    0    0    1    7    0    0    0    0    3    0  518    0
     0    2    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    1    0    0
  1125    9    0]
 [   0    0    0    0    0    0   16    0    0    7    0    0    0    0
     2  322    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.95121951219512

F1 scores:
[       nan 0.92307692 0.98040752 0.9830738  0.99765808 0.98057143
 0.97542815 1.         0.99883856 0.4556962  0.96428571 0.98912058
 0.97643732 0.99730458 0.9903169  0.94014599 0.98245614]

Kappa:
0.9766597204170449
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcf83b6c780>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.419, val_acc:0.547]
Epoch [2/120    avg_loss:1.921, val_acc:0.584]
Epoch [3/120    avg_loss:1.719, val_acc:0.614]
Epoch [4/120    avg_loss:1.386, val_acc:0.712]
Epoch [5/120    avg_loss:1.184, val_acc:0.658]
Epoch [6/120    avg_loss:0.942, val_acc:0.756]
Epoch [7/120    avg_loss:0.874, val_acc:0.696]
Epoch [8/120    avg_loss:0.802, val_acc:0.800]
Epoch [9/120    avg_loss:0.695, val_acc:0.788]
Epoch [10/120    avg_loss:0.664, val_acc:0.829]
Epoch [11/120    avg_loss:0.555, val_acc:0.847]
Epoch [12/120    avg_loss:0.574, val_acc:0.858]
Epoch [13/120    avg_loss:0.456, val_acc:0.806]
Epoch [14/120    avg_loss:0.474, val_acc:0.797]
Epoch [15/120    avg_loss:0.457, val_acc:0.842]
Epoch [16/120    avg_loss:0.429, val_acc:0.856]
Epoch [17/120    avg_loss:0.360, val_acc:0.838]
Epoch [18/120    avg_loss:0.344, val_acc:0.903]
Epoch [19/120    avg_loss:0.304, val_acc:0.897]
Epoch [20/120    avg_loss:0.283, val_acc:0.913]
Epoch [21/120    avg_loss:0.213, val_acc:0.925]
Epoch [22/120    avg_loss:0.226, val_acc:0.895]
Epoch [23/120    avg_loss:0.271, val_acc:0.900]
Epoch [24/120    avg_loss:0.176, val_acc:0.917]
Epoch [25/120    avg_loss:0.261, val_acc:0.917]
Epoch [26/120    avg_loss:0.192, val_acc:0.930]
Epoch [27/120    avg_loss:0.175, val_acc:0.896]
Epoch [28/120    avg_loss:0.185, val_acc:0.934]
Epoch [29/120    avg_loss:0.131, val_acc:0.922]
Epoch [30/120    avg_loss:0.127, val_acc:0.941]
Epoch [31/120    avg_loss:0.162, val_acc:0.925]
Epoch [32/120    avg_loss:0.133, val_acc:0.949]
Epoch [33/120    avg_loss:0.122, val_acc:0.945]
Epoch [34/120    avg_loss:0.120, val_acc:0.965]
Epoch [35/120    avg_loss:0.093, val_acc:0.960]
Epoch [36/120    avg_loss:0.099, val_acc:0.940]
Epoch [37/120    avg_loss:0.093, val_acc:0.938]
Epoch [38/120    avg_loss:0.104, val_acc:0.952]
Epoch [39/120    avg_loss:0.132, val_acc:0.942]
Epoch [40/120    avg_loss:0.219, val_acc:0.923]
Epoch [41/120    avg_loss:0.114, val_acc:0.947]
Epoch [42/120    avg_loss:0.081, val_acc:0.945]
Epoch [43/120    avg_loss:0.069, val_acc:0.942]
Epoch [44/120    avg_loss:0.099, val_acc:0.918]
Epoch [45/120    avg_loss:0.105, val_acc:0.954]
Epoch [46/120    avg_loss:0.095, val_acc:0.959]
Epoch [47/120    avg_loss:0.125, val_acc:0.949]
Epoch [48/120    avg_loss:0.101, val_acc:0.962]
Epoch [49/120    avg_loss:0.075, val_acc:0.960]
Epoch [50/120    avg_loss:0.050, val_acc:0.961]
Epoch [51/120    avg_loss:0.058, val_acc:0.967]
Epoch [52/120    avg_loss:0.055, val_acc:0.968]
Epoch [53/120    avg_loss:0.043, val_acc:0.968]
Epoch [54/120    avg_loss:0.048, val_acc:0.968]
Epoch [55/120    avg_loss:0.044, val_acc:0.970]
Epoch [56/120    avg_loss:0.038, val_acc:0.968]
Epoch [57/120    avg_loss:0.043, val_acc:0.971]
Epoch [58/120    avg_loss:0.038, val_acc:0.971]
Epoch [59/120    avg_loss:0.040, val_acc:0.972]
Epoch [60/120    avg_loss:0.034, val_acc:0.970]
Epoch [61/120    avg_loss:0.038, val_acc:0.972]
Epoch [62/120    avg_loss:0.037, val_acc:0.974]
Epoch [63/120    avg_loss:0.027, val_acc:0.970]
Epoch [64/120    avg_loss:0.029, val_acc:0.972]
Epoch [65/120    avg_loss:0.031, val_acc:0.978]
Epoch [66/120    avg_loss:0.041, val_acc:0.974]
Epoch [67/120    avg_loss:0.032, val_acc:0.974]
Epoch [68/120    avg_loss:0.031, val_acc:0.976]
Epoch [69/120    avg_loss:0.026, val_acc:0.979]
Epoch [70/120    avg_loss:0.030, val_acc:0.978]
Epoch [71/120    avg_loss:0.034, val_acc:0.978]
Epoch [72/120    avg_loss:0.030, val_acc:0.975]
Epoch [73/120    avg_loss:0.027, val_acc:0.975]
Epoch [74/120    avg_loss:0.028, val_acc:0.975]
Epoch [75/120    avg_loss:0.028, val_acc:0.974]
Epoch [76/120    avg_loss:0.031, val_acc:0.979]
Epoch [77/120    avg_loss:0.025, val_acc:0.977]
Epoch [78/120    avg_loss:0.024, val_acc:0.977]
Epoch [79/120    avg_loss:0.024, val_acc:0.976]
Epoch [80/120    avg_loss:0.042, val_acc:0.976]
Epoch [81/120    avg_loss:0.025, val_acc:0.977]
Epoch [82/120    avg_loss:0.028, val_acc:0.976]
Epoch [83/120    avg_loss:0.028, val_acc:0.978]
Epoch [84/120    avg_loss:0.025, val_acc:0.979]
Epoch [85/120    avg_loss:0.025, val_acc:0.978]
Epoch [86/120    avg_loss:0.033, val_acc:0.971]
Epoch [87/120    avg_loss:0.027, val_acc:0.976]
Epoch [88/120    avg_loss:0.024, val_acc:0.978]
Epoch [89/120    avg_loss:0.022, val_acc:0.977]
Epoch [90/120    avg_loss:0.026, val_acc:0.974]
Epoch [91/120    avg_loss:0.025, val_acc:0.977]
Epoch [92/120    avg_loss:0.027, val_acc:0.976]
Epoch [93/120    avg_loss:0.024, val_acc:0.978]
Epoch [94/120    avg_loss:0.028, val_acc:0.977]
Epoch [95/120    avg_loss:0.022, val_acc:0.977]
Epoch [96/120    avg_loss:0.027, val_acc:0.976]
Epoch [97/120    avg_loss:0.022, val_acc:0.980]
Epoch [98/120    avg_loss:0.021, val_acc:0.976]
Epoch [99/120    avg_loss:0.020, val_acc:0.977]
Epoch [100/120    avg_loss:0.021, val_acc:0.977]
Epoch [101/120    avg_loss:0.038, val_acc:0.978]
Epoch [102/120    avg_loss:0.021, val_acc:0.981]
Epoch [103/120    avg_loss:0.022, val_acc:0.978]
Epoch [104/120    avg_loss:0.026, val_acc:0.978]
Epoch [105/120    avg_loss:0.018, val_acc:0.976]
Epoch [106/120    avg_loss:0.019, val_acc:0.980]
Epoch [107/120    avg_loss:0.022, val_acc:0.980]
Epoch [108/120    avg_loss:0.018, val_acc:0.980]
Epoch [109/120    avg_loss:0.025, val_acc:0.980]
Epoch [110/120    avg_loss:0.018, val_acc:0.979]
Epoch [111/120    avg_loss:0.021, val_acc:0.977]
Epoch [112/120    avg_loss:0.024, val_acc:0.977]
Epoch [113/120    avg_loss:0.018, val_acc:0.980]
Epoch [114/120    avg_loss:0.022, val_acc:0.976]
Epoch [115/120    avg_loss:0.021, val_acc:0.978]
Epoch [116/120    avg_loss:0.024, val_acc:0.977]
Epoch [117/120    avg_loss:0.024, val_acc:0.977]
Epoch [118/120    avg_loss:0.020, val_acc:0.977]
Epoch [119/120    avg_loss:0.016, val_acc:0.977]
Epoch [120/120    avg_loss:0.017, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1255    0    0    2    2    0    0    0   10   16    0    0
     0    0    0]
 [   0    0    0  686    0    6    0    0    0    7    8    0   40    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   17    0    0    5    0    0    0    1  841    8    0    0
     2    1    0]
 [   0    0    5    0    0    1    3    0    0    0   18 2176    6    1
     0    0    0]
 [   0    0    0    0    0    9    0    0    0    0    0    3  519    0
     2    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    3    0    0    0
  1128    6    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    42  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.31165311653116

F1 scores:
[       nan 0.94871795 0.97970336 0.9567643  0.99764706 0.97315436
 0.98496241 1.         0.99883856 0.79069767 0.95622513 0.9861772
 0.94363636 0.99730458 0.97451404 0.90402477 0.99408284]

Kappa:
0.9693498790219173
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff1b503c898>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.412, val_acc:0.466]
Epoch [2/120    avg_loss:1.937, val_acc:0.571]
Epoch [3/120    avg_loss:1.656, val_acc:0.630]
Epoch [4/120    avg_loss:1.335, val_acc:0.653]
Epoch [5/120    avg_loss:1.146, val_acc:0.661]
Epoch [6/120    avg_loss:1.000, val_acc:0.700]
Epoch [7/120    avg_loss:0.934, val_acc:0.731]
Epoch [8/120    avg_loss:0.753, val_acc:0.680]
Epoch [9/120    avg_loss:0.682, val_acc:0.768]
Epoch [10/120    avg_loss:0.655, val_acc:0.788]
Epoch [11/120    avg_loss:0.548, val_acc:0.811]
Epoch [12/120    avg_loss:0.525, val_acc:0.814]
Epoch [13/120    avg_loss:0.479, val_acc:0.811]
Epoch [14/120    avg_loss:0.522, val_acc:0.772]
Epoch [15/120    avg_loss:0.446, val_acc:0.815]
Epoch [16/120    avg_loss:0.400, val_acc:0.861]
Epoch [17/120    avg_loss:0.440, val_acc:0.844]
Epoch [18/120    avg_loss:0.393, val_acc:0.881]
Epoch [19/120    avg_loss:0.296, val_acc:0.855]
Epoch [20/120    avg_loss:0.369, val_acc:0.816]
Epoch [21/120    avg_loss:0.319, val_acc:0.870]
Epoch [22/120    avg_loss:0.310, val_acc:0.889]
Epoch [23/120    avg_loss:0.404, val_acc:0.828]
Epoch [24/120    avg_loss:0.264, val_acc:0.899]
Epoch [25/120    avg_loss:0.222, val_acc:0.899]
Epoch [26/120    avg_loss:0.197, val_acc:0.925]
Epoch [27/120    avg_loss:0.246, val_acc:0.941]
Epoch [28/120    avg_loss:0.239, val_acc:0.915]
Epoch [29/120    avg_loss:0.182, val_acc:0.749]
Epoch [30/120    avg_loss:0.262, val_acc:0.905]
Epoch [31/120    avg_loss:0.224, val_acc:0.943]
Epoch [32/120    avg_loss:0.138, val_acc:0.941]
Epoch [33/120    avg_loss:0.124, val_acc:0.938]
Epoch [34/120    avg_loss:0.128, val_acc:0.945]
Epoch [35/120    avg_loss:0.159, val_acc:0.924]
Epoch [36/120    avg_loss:0.109, val_acc:0.932]
Epoch [37/120    avg_loss:0.093, val_acc:0.947]
Epoch [38/120    avg_loss:0.149, val_acc:0.889]
Epoch [39/120    avg_loss:0.154, val_acc:0.939]
Epoch [40/120    avg_loss:0.118, val_acc:0.949]
Epoch [41/120    avg_loss:0.096, val_acc:0.948]
Epoch [42/120    avg_loss:0.069, val_acc:0.954]
Epoch [43/120    avg_loss:0.094, val_acc:0.927]
Epoch [44/120    avg_loss:0.727, val_acc:0.608]
Epoch [45/120    avg_loss:1.091, val_acc:0.763]
Epoch [46/120    avg_loss:0.780, val_acc:0.806]
Epoch [47/120    avg_loss:0.591, val_acc:0.858]
Epoch [48/120    avg_loss:0.438, val_acc:0.904]
Epoch [49/120    avg_loss:0.392, val_acc:0.870]
Epoch [50/120    avg_loss:0.286, val_acc:0.898]
Epoch [51/120    avg_loss:0.282, val_acc:0.890]
Epoch [52/120    avg_loss:0.226, val_acc:0.899]
Epoch [53/120    avg_loss:0.299, val_acc:0.914]
Epoch [54/120    avg_loss:0.180, val_acc:0.932]
Epoch [55/120    avg_loss:0.171, val_acc:0.931]
Epoch [56/120    avg_loss:0.109, val_acc:0.943]
Epoch [57/120    avg_loss:0.118, val_acc:0.947]
Epoch [58/120    avg_loss:0.101, val_acc:0.951]
Epoch [59/120    avg_loss:0.085, val_acc:0.950]
Epoch [60/120    avg_loss:0.083, val_acc:0.948]
Epoch [61/120    avg_loss:0.084, val_acc:0.951]
Epoch [62/120    avg_loss:0.078, val_acc:0.952]
Epoch [63/120    avg_loss:0.082, val_acc:0.948]
Epoch [64/120    avg_loss:0.081, val_acc:0.950]
Epoch [65/120    avg_loss:0.070, val_acc:0.954]
Epoch [66/120    avg_loss:0.077, val_acc:0.953]
Epoch [67/120    avg_loss:0.067, val_acc:0.956]
Epoch [68/120    avg_loss:0.065, val_acc:0.959]
Epoch [69/120    avg_loss:0.070, val_acc:0.957]
Epoch [70/120    avg_loss:0.063, val_acc:0.961]
Epoch [71/120    avg_loss:0.070, val_acc:0.952]
Epoch [72/120    avg_loss:0.068, val_acc:0.952]
Epoch [73/120    avg_loss:0.065, val_acc:0.954]
Epoch [74/120    avg_loss:0.063, val_acc:0.959]
Epoch [75/120    avg_loss:0.068, val_acc:0.960]
Epoch [76/120    avg_loss:0.065, val_acc:0.960]
Epoch [77/120    avg_loss:0.065, val_acc:0.964]
Epoch [78/120    avg_loss:0.053, val_acc:0.960]
Epoch [79/120    avg_loss:0.064, val_acc:0.957]
Epoch [80/120    avg_loss:0.055, val_acc:0.961]
Epoch [81/120    avg_loss:0.065, val_acc:0.962]
Epoch [82/120    avg_loss:0.057, val_acc:0.958]
Epoch [83/120    avg_loss:0.051, val_acc:0.956]
Epoch [84/120    avg_loss:0.058, val_acc:0.958]
Epoch [85/120    avg_loss:0.051, val_acc:0.960]
Epoch [86/120    avg_loss:0.063, val_acc:0.960]
Epoch [87/120    avg_loss:0.056, val_acc:0.957]
Epoch [88/120    avg_loss:0.071, val_acc:0.953]
Epoch [89/120    avg_loss:0.047, val_acc:0.959]
Epoch [90/120    avg_loss:0.047, val_acc:0.960]
Epoch [91/120    avg_loss:0.054, val_acc:0.960]
Epoch [92/120    avg_loss:0.045, val_acc:0.961]
Epoch [93/120    avg_loss:0.059, val_acc:0.960]
Epoch [94/120    avg_loss:0.055, val_acc:0.961]
Epoch [95/120    avg_loss:0.059, val_acc:0.960]
Epoch [96/120    avg_loss:0.056, val_acc:0.961]
Epoch [97/120    avg_loss:0.047, val_acc:0.960]
Epoch [98/120    avg_loss:0.051, val_acc:0.960]
Epoch [99/120    avg_loss:0.048, val_acc:0.961]
Epoch [100/120    avg_loss:0.040, val_acc:0.961]
Epoch [101/120    avg_loss:0.051, val_acc:0.961]
Epoch [102/120    avg_loss:0.051, val_acc:0.961]
Epoch [103/120    avg_loss:0.051, val_acc:0.961]
Epoch [104/120    avg_loss:0.049, val_acc:0.961]
Epoch [105/120    avg_loss:0.046, val_acc:0.961]
Epoch [106/120    avg_loss:0.050, val_acc:0.961]
Epoch [107/120    avg_loss:0.053, val_acc:0.961]
Epoch [108/120    avg_loss:0.045, val_acc:0.961]
Epoch [109/120    avg_loss:0.049, val_acc:0.961]
Epoch [110/120    avg_loss:0.043, val_acc:0.961]
Epoch [111/120    avg_loss:0.047, val_acc:0.961]
Epoch [112/120    avg_loss:0.058, val_acc:0.961]
Epoch [113/120    avg_loss:0.046, val_acc:0.962]
Epoch [114/120    avg_loss:0.048, val_acc:0.961]
Epoch [115/120    avg_loss:0.058, val_acc:0.961]
Epoch [116/120    avg_loss:0.054, val_acc:0.961]
Epoch [117/120    avg_loss:0.045, val_acc:0.961]
Epoch [118/120    avg_loss:0.052, val_acc:0.961]
Epoch [119/120    avg_loss:0.053, val_acc:0.961]
Epoch [120/120    avg_loss:0.050, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1224    0    0    0    0    0    0    8   10   40    0    0
     0    3    0]
 [   0    0    1  678   14    7   14    0    0    9    0    0   23    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   49    2    0   10    0    0    0    0  791   14    0    0
     0    9    0]
 [   0    0   11    1    0    2    4    0    0    6    8 2166    8    1
     0    3    0]
 [   0    0    5    0    1   11    1    0    0    2    0    0  499    0
     0    4   11]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   14    0    0    0    1    1    2    0    0
  1104   17    0]
 [   0    0    0    0    0    0    0    0    0    4    0    0    0    0
    36  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.91327913279133

F1 scores:
[       nan 0.94871795 0.95067961 0.94957983 0.96598639 0.9495614
 0.98345865 1.         1.         0.53731343 0.9366489  0.97721633
 0.93796992 0.99730458 0.9675723  0.88856729 0.93854749]

Kappa:
0.9534280507323406
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe0ed002860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.422, val_acc:0.525]
Epoch [2/120    avg_loss:1.880, val_acc:0.526]
Epoch [3/120    avg_loss:1.611, val_acc:0.577]
Epoch [4/120    avg_loss:1.303, val_acc:0.635]
Epoch [5/120    avg_loss:1.163, val_acc:0.702]
Epoch [6/120    avg_loss:0.961, val_acc:0.699]
Epoch [7/120    avg_loss:0.877, val_acc:0.710]
Epoch [8/120    avg_loss:0.804, val_acc:0.774]
Epoch [9/120    avg_loss:0.657, val_acc:0.828]
Epoch [10/120    avg_loss:0.605, val_acc:0.692]
Epoch [11/120    avg_loss:0.607, val_acc:0.775]
Epoch [12/120    avg_loss:0.540, val_acc:0.802]
Epoch [13/120    avg_loss:0.524, val_acc:0.843]
Epoch [14/120    avg_loss:0.394, val_acc:0.848]
Epoch [15/120    avg_loss:0.391, val_acc:0.793]
Epoch [16/120    avg_loss:0.396, val_acc:0.872]
Epoch [17/120    avg_loss:0.405, val_acc:0.849]
Epoch [18/120    avg_loss:0.362, val_acc:0.855]
Epoch [19/120    avg_loss:0.305, val_acc:0.894]
Epoch [20/120    avg_loss:0.368, val_acc:0.884]
Epoch [21/120    avg_loss:0.294, val_acc:0.876]
Epoch [22/120    avg_loss:0.275, val_acc:0.900]
Epoch [23/120    avg_loss:0.259, val_acc:0.902]
Epoch [24/120    avg_loss:0.313, val_acc:0.863]
Epoch [25/120    avg_loss:0.234, val_acc:0.893]
Epoch [26/120    avg_loss:0.215, val_acc:0.917]
Epoch [27/120    avg_loss:0.175, val_acc:0.898]
Epoch [28/120    avg_loss:0.213, val_acc:0.900]
Epoch [29/120    avg_loss:0.161, val_acc:0.893]
Epoch [30/120    avg_loss:0.223, val_acc:0.928]
Epoch [31/120    avg_loss:0.140, val_acc:0.921]
Epoch [32/120    avg_loss:0.178, val_acc:0.904]
Epoch [33/120    avg_loss:0.143, val_acc:0.922]
Epoch [34/120    avg_loss:0.116, val_acc:0.927]
Epoch [35/120    avg_loss:0.120, val_acc:0.948]
Epoch [36/120    avg_loss:0.123, val_acc:0.934]
Epoch [37/120    avg_loss:0.128, val_acc:0.870]
Epoch [38/120    avg_loss:0.200, val_acc:0.894]
Epoch [39/120    avg_loss:0.158, val_acc:0.940]
Epoch [40/120    avg_loss:0.119, val_acc:0.936]
Epoch [41/120    avg_loss:0.104, val_acc:0.939]
Epoch [42/120    avg_loss:0.096, val_acc:0.932]
Epoch [43/120    avg_loss:0.096, val_acc:0.947]
Epoch [44/120    avg_loss:0.076, val_acc:0.960]
Epoch [45/120    avg_loss:0.103, val_acc:0.930]
Epoch [46/120    avg_loss:0.087, val_acc:0.936]
Epoch [47/120    avg_loss:0.085, val_acc:0.952]
Epoch [48/120    avg_loss:0.131, val_acc:0.952]
Epoch [49/120    avg_loss:0.086, val_acc:0.956]
Epoch [50/120    avg_loss:0.070, val_acc:0.956]
Epoch [51/120    avg_loss:0.088, val_acc:0.956]
Epoch [52/120    avg_loss:0.073, val_acc:0.966]
Epoch [53/120    avg_loss:0.063, val_acc:0.953]
Epoch [54/120    avg_loss:0.044, val_acc:0.955]
Epoch [55/120    avg_loss:0.069, val_acc:0.955]
Epoch [56/120    avg_loss:0.051, val_acc:0.961]
Epoch [57/120    avg_loss:0.099, val_acc:0.959]
Epoch [58/120    avg_loss:0.102, val_acc:0.931]
Epoch [59/120    avg_loss:0.154, val_acc:0.925]
Epoch [60/120    avg_loss:0.123, val_acc:0.936]
Epoch [61/120    avg_loss:0.050, val_acc:0.953]
Epoch [62/120    avg_loss:0.080, val_acc:0.945]
Epoch [63/120    avg_loss:0.049, val_acc:0.964]
Epoch [64/120    avg_loss:0.055, val_acc:0.961]
Epoch [65/120    avg_loss:0.041, val_acc:0.962]
Epoch [66/120    avg_loss:0.028, val_acc:0.967]
Epoch [67/120    avg_loss:0.038, val_acc:0.967]
Epoch [68/120    avg_loss:0.032, val_acc:0.969]
Epoch [69/120    avg_loss:0.025, val_acc:0.971]
Epoch [70/120    avg_loss:0.034, val_acc:0.971]
Epoch [71/120    avg_loss:0.025, val_acc:0.969]
Epoch [72/120    avg_loss:0.031, val_acc:0.969]
Epoch [73/120    avg_loss:0.025, val_acc:0.971]
Epoch [74/120    avg_loss:0.020, val_acc:0.971]
Epoch [75/120    avg_loss:0.021, val_acc:0.973]
Epoch [76/120    avg_loss:0.022, val_acc:0.973]
Epoch [77/120    avg_loss:0.025, val_acc:0.975]
Epoch [78/120    avg_loss:0.032, val_acc:0.974]
Epoch [79/120    avg_loss:0.033, val_acc:0.972]
Epoch [80/120    avg_loss:0.024, val_acc:0.975]
Epoch [81/120    avg_loss:0.020, val_acc:0.973]
Epoch [82/120    avg_loss:0.017, val_acc:0.976]
Epoch [83/120    avg_loss:0.022, val_acc:0.978]
Epoch [84/120    avg_loss:0.022, val_acc:0.973]
Epoch [85/120    avg_loss:0.019, val_acc:0.978]
Epoch [86/120    avg_loss:0.020, val_acc:0.980]
Epoch [87/120    avg_loss:0.018, val_acc:0.978]
Epoch [88/120    avg_loss:0.021, val_acc:0.981]
Epoch [89/120    avg_loss:0.020, val_acc:0.977]
Epoch [90/120    avg_loss:0.014, val_acc:0.976]
Epoch [91/120    avg_loss:0.020, val_acc:0.978]
Epoch [92/120    avg_loss:0.020, val_acc:0.978]
Epoch [93/120    avg_loss:0.019, val_acc:0.978]
Epoch [94/120    avg_loss:0.019, val_acc:0.977]
Epoch [95/120    avg_loss:0.019, val_acc:0.977]
Epoch [96/120    avg_loss:0.016, val_acc:0.978]
Epoch [97/120    avg_loss:0.017, val_acc:0.976]
Epoch [98/120    avg_loss:0.023, val_acc:0.977]
Epoch [99/120    avg_loss:0.015, val_acc:0.977]
Epoch [100/120    avg_loss:0.019, val_acc:0.977]
Epoch [101/120    avg_loss:0.018, val_acc:0.977]
Epoch [102/120    avg_loss:0.019, val_acc:0.977]
Epoch [103/120    avg_loss:0.019, val_acc:0.977]
Epoch [104/120    avg_loss:0.015, val_acc:0.977]
Epoch [105/120    avg_loss:0.020, val_acc:0.977]
Epoch [106/120    avg_loss:0.020, val_acc:0.977]
Epoch [107/120    avg_loss:0.016, val_acc:0.977]
Epoch [108/120    avg_loss:0.023, val_acc:0.977]
Epoch [109/120    avg_loss:0.023, val_acc:0.977]
Epoch [110/120    avg_loss:0.021, val_acc:0.977]
Epoch [111/120    avg_loss:0.017, val_acc:0.977]
Epoch [112/120    avg_loss:0.017, val_acc:0.977]
Epoch [113/120    avg_loss:0.017, val_acc:0.977]
Epoch [114/120    avg_loss:0.019, val_acc:0.977]
Epoch [115/120    avg_loss:0.020, val_acc:0.977]
Epoch [116/120    avg_loss:0.017, val_acc:0.977]
Epoch [117/120    avg_loss:0.021, val_acc:0.977]
Epoch [118/120    avg_loss:0.021, val_acc:0.977]
Epoch [119/120    avg_loss:0.016, val_acc:0.977]
Epoch [120/120    avg_loss:0.018, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1244    3    0    0    2    0    0    0    7   21    7    0
     0    1    0]
 [   0    0    2  714    0    5    0    0    0    4    2    0   20    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    3    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    4    0    0    0    0  859    8    2    0
     0    0    0]
 [   0    0    5    0    0    0    2    0    0    0   19 2182    0    2
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0    0  529    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    0    0    0    0
  1124   10    0]
 [   0    0    0    0    0    0   26    0    0    0    0    0    0    0
    16  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.8970189701897

F1 scores:
[       nan 0.96202532 0.98029945 0.97474403 1.         0.98185941
 0.97386109 1.         0.9953271  0.87804878 0.9733711  0.98643761
 0.96532847 0.99462366 0.98510079 0.91729323 0.98823529]

Kappa:
0.9760276212596963
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f0aa41860>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.428, val_acc:0.481]
Epoch [2/120    avg_loss:1.951, val_acc:0.600]
Epoch [3/120    avg_loss:1.676, val_acc:0.615]
Epoch [4/120    avg_loss:1.470, val_acc:0.655]
Epoch [5/120    avg_loss:1.239, val_acc:0.696]
Epoch [6/120    avg_loss:1.056, val_acc:0.731]
Epoch [7/120    avg_loss:0.918, val_acc:0.767]
Epoch [8/120    avg_loss:0.794, val_acc:0.778]
Epoch [9/120    avg_loss:0.839, val_acc:0.779]
Epoch [10/120    avg_loss:0.661, val_acc:0.782]
Epoch [11/120    avg_loss:0.663, val_acc:0.804]
Epoch [12/120    avg_loss:0.649, val_acc:0.803]
Epoch [13/120    avg_loss:0.506, val_acc:0.815]
Epoch [14/120    avg_loss:0.542, val_acc:0.827]
Epoch [15/120    avg_loss:0.448, val_acc:0.847]
Epoch [16/120    avg_loss:0.471, val_acc:0.845]
Epoch [17/120    avg_loss:0.462, val_acc:0.860]
Epoch [18/120    avg_loss:0.339, val_acc:0.864]
Epoch [19/120    avg_loss:0.355, val_acc:0.885]
Epoch [20/120    avg_loss:0.303, val_acc:0.837]
Epoch [21/120    avg_loss:0.405, val_acc:0.870]
Epoch [22/120    avg_loss:0.340, val_acc:0.860]
Epoch [23/120    avg_loss:0.255, val_acc:0.867]
Epoch [24/120    avg_loss:0.253, val_acc:0.900]
Epoch [25/120    avg_loss:0.235, val_acc:0.870]
Epoch [26/120    avg_loss:0.278, val_acc:0.902]
Epoch [27/120    avg_loss:0.191, val_acc:0.912]
Epoch [28/120    avg_loss:0.189, val_acc:0.916]
Epoch [29/120    avg_loss:0.197, val_acc:0.913]
Epoch [30/120    avg_loss:0.201, val_acc:0.907]
Epoch [31/120    avg_loss:0.187, val_acc:0.913]
Epoch [32/120    avg_loss:0.223, val_acc:0.917]
Epoch [33/120    avg_loss:0.163, val_acc:0.917]
Epoch [34/120    avg_loss:0.172, val_acc:0.909]
Epoch [35/120    avg_loss:0.148, val_acc:0.918]
Epoch [36/120    avg_loss:0.135, val_acc:0.899]
Epoch [37/120    avg_loss:0.129, val_acc:0.905]
Epoch [38/120    avg_loss:0.170, val_acc:0.929]
Epoch [39/120    avg_loss:0.163, val_acc:0.888]
Epoch [40/120    avg_loss:0.143, val_acc:0.916]
Epoch [41/120    avg_loss:0.121, val_acc:0.944]
Epoch [42/120    avg_loss:0.135, val_acc:0.936]
Epoch [43/120    avg_loss:0.111, val_acc:0.938]
Epoch [44/120    avg_loss:0.118, val_acc:0.947]
Epoch [45/120    avg_loss:0.079, val_acc:0.939]
Epoch [46/120    avg_loss:0.103, val_acc:0.943]
Epoch [47/120    avg_loss:0.080, val_acc:0.956]
Epoch [48/120    avg_loss:0.155, val_acc:0.950]
Epoch [49/120    avg_loss:0.080, val_acc:0.933]
Epoch [50/120    avg_loss:0.082, val_acc:0.944]
Epoch [51/120    avg_loss:0.086, val_acc:0.939]
Epoch [52/120    avg_loss:0.064, val_acc:0.897]
Epoch [53/120    avg_loss:0.070, val_acc:0.954]
Epoch [54/120    avg_loss:0.067, val_acc:0.936]
Epoch [55/120    avg_loss:0.110, val_acc:0.943]
Epoch [56/120    avg_loss:0.090, val_acc:0.957]
Epoch [57/120    avg_loss:0.066, val_acc:0.954]
Epoch [58/120    avg_loss:0.062, val_acc:0.956]
Epoch [59/120    avg_loss:0.065, val_acc:0.960]
Epoch [60/120    avg_loss:0.051, val_acc:0.962]
Epoch [61/120    avg_loss:0.054, val_acc:0.960]
Epoch [62/120    avg_loss:0.055, val_acc:0.949]
Epoch [63/120    avg_loss:0.038, val_acc:0.956]
Epoch [64/120    avg_loss:0.047, val_acc:0.929]
Epoch [65/120    avg_loss:0.073, val_acc:0.941]
Epoch [66/120    avg_loss:0.069, val_acc:0.946]
Epoch [67/120    avg_loss:0.188, val_acc:0.918]
Epoch [68/120    avg_loss:0.115, val_acc:0.944]
Epoch [69/120    avg_loss:0.107, val_acc:0.943]
Epoch [70/120    avg_loss:0.081, val_acc:0.946]
Epoch [71/120    avg_loss:0.081, val_acc:0.931]
Epoch [72/120    avg_loss:0.088, val_acc:0.941]
Epoch [73/120    avg_loss:0.063, val_acc:0.953]
Epoch [74/120    avg_loss:0.053, val_acc:0.953]
Epoch [75/120    avg_loss:0.055, val_acc:0.959]
Epoch [76/120    avg_loss:0.038, val_acc:0.959]
Epoch [77/120    avg_loss:0.030, val_acc:0.960]
Epoch [78/120    avg_loss:0.035, val_acc:0.961]
Epoch [79/120    avg_loss:0.034, val_acc:0.963]
Epoch [80/120    avg_loss:0.033, val_acc:0.962]
Epoch [81/120    avg_loss:0.037, val_acc:0.964]
Epoch [82/120    avg_loss:0.027, val_acc:0.966]
Epoch [83/120    avg_loss:0.038, val_acc:0.962]
Epoch [84/120    avg_loss:0.025, val_acc:0.967]
Epoch [85/120    avg_loss:0.027, val_acc:0.966]
Epoch [86/120    avg_loss:0.019, val_acc:0.963]
Epoch [87/120    avg_loss:0.031, val_acc:0.970]
Epoch [88/120    avg_loss:0.035, val_acc:0.968]
Epoch [89/120    avg_loss:0.026, val_acc:0.969]
Epoch [90/120    avg_loss:0.033, val_acc:0.968]
Epoch [91/120    avg_loss:0.023, val_acc:0.971]
Epoch [92/120    avg_loss:0.025, val_acc:0.970]
Epoch [93/120    avg_loss:0.030, val_acc:0.969]
Epoch [94/120    avg_loss:0.022, val_acc:0.968]
Epoch [95/120    avg_loss:0.029, val_acc:0.971]
Epoch [96/120    avg_loss:0.021, val_acc:0.974]
Epoch [97/120    avg_loss:0.025, val_acc:0.971]
Epoch [98/120    avg_loss:0.023, val_acc:0.972]
Epoch [99/120    avg_loss:0.021, val_acc:0.971]
Epoch [100/120    avg_loss:0.032, val_acc:0.974]
Epoch [101/120    avg_loss:0.023, val_acc:0.974]
Epoch [102/120    avg_loss:0.029, val_acc:0.975]
Epoch [103/120    avg_loss:0.020, val_acc:0.973]
Epoch [104/120    avg_loss:0.023, val_acc:0.973]
Epoch [105/120    avg_loss:0.020, val_acc:0.973]
Epoch [106/120    avg_loss:0.020, val_acc:0.972]
Epoch [107/120    avg_loss:0.035, val_acc:0.970]
Epoch [108/120    avg_loss:0.023, val_acc:0.970]
Epoch [109/120    avg_loss:0.023, val_acc:0.972]
Epoch [110/120    avg_loss:0.019, val_acc:0.972]
Epoch [111/120    avg_loss:0.021, val_acc:0.974]
Epoch [112/120    avg_loss:0.024, val_acc:0.974]
Epoch [113/120    avg_loss:0.020, val_acc:0.972]
Epoch [114/120    avg_loss:0.021, val_acc:0.975]
Epoch [115/120    avg_loss:0.017, val_acc:0.974]
Epoch [116/120    avg_loss:0.023, val_acc:0.975]
Epoch [117/120    avg_loss:0.022, val_acc:0.975]
Epoch [118/120    avg_loss:0.019, val_acc:0.973]
Epoch [119/120    avg_loss:0.019, val_acc:0.973]
Epoch [120/120    avg_loss:0.024, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1254    2    0    0    0    0    0    2    4   23    0    0
     0    0    0]
 [   0    0    0  711    0   11    0    0    0   11    0    0   13    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    2    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   19   36    0   13    2    0    0    0  780   10    4    0
     1   10    0]
 [   0    0    5    0    0    0    3    0    0    0   11 2183    4    4
     0    0    0]
 [   0    0    3   22    9    3    0    0    0    0    0    0  489    0
     0    0    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    2    0    0
  1136    0    0]
 [   0    0    0    0    0    0   36    0    0    3    0    0    0    0
    69  239    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.16260162601625

F1 scores:
[       nan 0.98765432 0.97739673 0.93675889 0.97931034 0.96296296
 0.9689808  1.         1.         0.66666667 0.93301435 0.98577557
 0.93588517 0.98666667 0.96722009 0.80201342 0.94857143]

Kappa:
0.9562278102247809
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f68c06a3860>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.412, val_acc:0.470]
Epoch [2/120    avg_loss:1.976, val_acc:0.585]
Epoch [3/120    avg_loss:1.653, val_acc:0.615]
Epoch [4/120    avg_loss:1.419, val_acc:0.675]
Epoch [5/120    avg_loss:1.252, val_acc:0.690]
Epoch [6/120    avg_loss:1.050, val_acc:0.666]
Epoch [7/120    avg_loss:0.961, val_acc:0.727]
Epoch [8/120    avg_loss:0.812, val_acc:0.762]
Epoch [9/120    avg_loss:0.764, val_acc:0.746]
Epoch [10/120    avg_loss:0.638, val_acc:0.821]
Epoch [11/120    avg_loss:0.665, val_acc:0.811]
Epoch [12/120    avg_loss:0.585, val_acc:0.805]
Epoch [13/120    avg_loss:0.513, val_acc:0.804]
Epoch [14/120    avg_loss:0.468, val_acc:0.819]
Epoch [15/120    avg_loss:0.474, val_acc:0.888]
Epoch [16/120    avg_loss:0.386, val_acc:0.849]
Epoch [17/120    avg_loss:0.447, val_acc:0.873]
Epoch [18/120    avg_loss:0.329, val_acc:0.880]
Epoch [19/120    avg_loss:0.285, val_acc:0.870]
Epoch [20/120    avg_loss:0.322, val_acc:0.872]
Epoch [21/120    avg_loss:0.295, val_acc:0.873]
Epoch [22/120    avg_loss:0.279, val_acc:0.888]
Epoch [23/120    avg_loss:0.236, val_acc:0.837]
Epoch [24/120    avg_loss:0.277, val_acc:0.898]
Epoch [25/120    avg_loss:0.242, val_acc:0.900]
Epoch [26/120    avg_loss:0.233, val_acc:0.901]
Epoch [27/120    avg_loss:0.194, val_acc:0.909]
Epoch [28/120    avg_loss:0.187, val_acc:0.892]
Epoch [29/120    avg_loss:0.166, val_acc:0.913]
Epoch [30/120    avg_loss:0.167, val_acc:0.919]
Epoch [31/120    avg_loss:0.135, val_acc:0.931]
Epoch [32/120    avg_loss:0.143, val_acc:0.927]
Epoch [33/120    avg_loss:0.185, val_acc:0.913]
Epoch [34/120    avg_loss:0.157, val_acc:0.927]
Epoch [35/120    avg_loss:0.164, val_acc:0.912]
Epoch [36/120    avg_loss:0.101, val_acc:0.925]
Epoch [37/120    avg_loss:0.134, val_acc:0.907]
Epoch [38/120    avg_loss:0.122, val_acc:0.930]
Epoch [39/120    avg_loss:0.126, val_acc:0.929]
Epoch [40/120    avg_loss:0.094, val_acc:0.928]
Epoch [41/120    avg_loss:0.120, val_acc:0.897]
Epoch [42/120    avg_loss:0.107, val_acc:0.939]
Epoch [43/120    avg_loss:0.135, val_acc:0.897]
Epoch [44/120    avg_loss:0.086, val_acc:0.946]
Epoch [45/120    avg_loss:0.134, val_acc:0.918]
Epoch [46/120    avg_loss:0.108, val_acc:0.914]
Epoch [47/120    avg_loss:0.076, val_acc:0.946]
Epoch [48/120    avg_loss:0.087, val_acc:0.932]
Epoch [49/120    avg_loss:0.094, val_acc:0.952]
Epoch [50/120    avg_loss:0.087, val_acc:0.935]
Epoch [51/120    avg_loss:0.073, val_acc:0.941]
Epoch [52/120    avg_loss:0.115, val_acc:0.900]
Epoch [53/120    avg_loss:0.081, val_acc:0.947]
Epoch [54/120    avg_loss:0.063, val_acc:0.947]
Epoch [55/120    avg_loss:0.058, val_acc:0.955]
Epoch [56/120    avg_loss:0.086, val_acc:0.948]
Epoch [57/120    avg_loss:0.067, val_acc:0.947]
Epoch [58/120    avg_loss:0.035, val_acc:0.956]
Epoch [59/120    avg_loss:0.043, val_acc:0.949]
Epoch [60/120    avg_loss:0.079, val_acc:0.948]
Epoch [61/120    avg_loss:0.075, val_acc:0.944]
Epoch [62/120    avg_loss:0.111, val_acc:0.901]
Epoch [63/120    avg_loss:0.142, val_acc:0.919]
Epoch [64/120    avg_loss:0.115, val_acc:0.933]
Epoch [65/120    avg_loss:0.045, val_acc:0.955]
Epoch [66/120    avg_loss:0.069, val_acc:0.942]
Epoch [67/120    avg_loss:0.099, val_acc:0.949]
Epoch [68/120    avg_loss:0.054, val_acc:0.950]
Epoch [69/120    avg_loss:0.052, val_acc:0.954]
Epoch [70/120    avg_loss:0.055, val_acc:0.953]
Epoch [71/120    avg_loss:0.031, val_acc:0.964]
Epoch [72/120    avg_loss:0.027, val_acc:0.959]
Epoch [73/120    avg_loss:0.025, val_acc:0.968]
Epoch [74/120    avg_loss:0.030, val_acc:0.934]
Epoch [75/120    avg_loss:0.058, val_acc:0.954]
Epoch [76/120    avg_loss:0.036, val_acc:0.958]
Epoch [77/120    avg_loss:0.045, val_acc:0.959]
Epoch [78/120    avg_loss:0.046, val_acc:0.960]
Epoch [79/120    avg_loss:0.032, val_acc:0.959]
Epoch [80/120    avg_loss:0.045, val_acc:0.959]
Epoch [81/120    avg_loss:0.032, val_acc:0.966]
Epoch [82/120    avg_loss:0.061, val_acc:0.956]
Epoch [83/120    avg_loss:0.047, val_acc:0.959]
Epoch [84/120    avg_loss:0.025, val_acc:0.961]
Epoch [85/120    avg_loss:0.037, val_acc:0.960]
Epoch [86/120    avg_loss:0.056, val_acc:0.959]
Epoch [87/120    avg_loss:0.033, val_acc:0.964]
Epoch [88/120    avg_loss:0.033, val_acc:0.964]
Epoch [89/120    avg_loss:0.023, val_acc:0.970]
Epoch [90/120    avg_loss:0.021, val_acc:0.969]
Epoch [91/120    avg_loss:0.021, val_acc:0.969]
Epoch [92/120    avg_loss:0.013, val_acc:0.968]
Epoch [93/120    avg_loss:0.014, val_acc:0.968]
Epoch [94/120    avg_loss:0.015, val_acc:0.972]
Epoch [95/120    avg_loss:0.012, val_acc:0.971]
Epoch [96/120    avg_loss:0.019, val_acc:0.968]
Epoch [97/120    avg_loss:0.012, val_acc:0.970]
Epoch [98/120    avg_loss:0.013, val_acc:0.969]
Epoch [99/120    avg_loss:0.012, val_acc:0.970]
Epoch [100/120    avg_loss:0.014, val_acc:0.970]
Epoch [101/120    avg_loss:0.020, val_acc:0.970]
Epoch [102/120    avg_loss:0.012, val_acc:0.971]
Epoch [103/120    avg_loss:0.010, val_acc:0.970]
Epoch [104/120    avg_loss:0.010, val_acc:0.970]
Epoch [105/120    avg_loss:0.012, val_acc:0.972]
Epoch [106/120    avg_loss:0.014, val_acc:0.971]
Epoch [107/120    avg_loss:0.012, val_acc:0.972]
Epoch [108/120    avg_loss:0.011, val_acc:0.970]
Epoch [109/120    avg_loss:0.012, val_acc:0.972]
Epoch [110/120    avg_loss:0.012, val_acc:0.972]
Epoch [111/120    avg_loss:0.012, val_acc:0.971]
Epoch [112/120    avg_loss:0.013, val_acc:0.974]
Epoch [113/120    avg_loss:0.011, val_acc:0.969]
Epoch [114/120    avg_loss:0.011, val_acc:0.972]
Epoch [115/120    avg_loss:0.013, val_acc:0.971]
Epoch [116/120    avg_loss:0.012, val_acc:0.969]
Epoch [117/120    avg_loss:0.010, val_acc:0.968]
Epoch [118/120    avg_loss:0.013, val_acc:0.971]
Epoch [119/120    avg_loss:0.009, val_acc:0.970]
Epoch [120/120    avg_loss:0.011, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    0    0    2    0    0    0    0    6   15    0    0
     2    0    0]
 [   0    0    2  713    0   15    0    0    0    9    2    0    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8   10    0   10    0    0    0    0  818   15    3    0
     0   11    0]
 [   0    0    7    0    0    1    3    0    0    0   11 2185    1    2
     0    0    0]
 [   0    0    0   12   12    4    0    0    0    1    0    2  502    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1133    2    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    83  252    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.99728997289972

F1 scores:
[       nan 1.         0.98360656 0.96221323 0.97260274 0.96222222
 0.98795181 0.98039216 0.99883856 0.76595745 0.95393586 0.98690154
 0.95984704 0.99462366 0.9613916  0.82352941 0.99408284]

Kappa:
0.9657464169846671
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f62b2da3828>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.464, val_acc:0.485]
Epoch [2/120    avg_loss:1.968, val_acc:0.590]
Epoch [3/120    avg_loss:1.653, val_acc:0.621]
Epoch [4/120    avg_loss:1.346, val_acc:0.668]
Epoch [5/120    avg_loss:1.181, val_acc:0.735]
Epoch [6/120    avg_loss:0.936, val_acc:0.755]
Epoch [7/120    avg_loss:0.865, val_acc:0.761]
Epoch [8/120    avg_loss:0.716, val_acc:0.774]
Epoch [9/120    avg_loss:0.675, val_acc:0.788]
Epoch [10/120    avg_loss:0.671, val_acc:0.731]
Epoch [11/120    avg_loss:0.627, val_acc:0.783]
Epoch [12/120    avg_loss:0.547, val_acc:0.827]
Epoch [13/120    avg_loss:0.487, val_acc:0.854]
Epoch [14/120    avg_loss:0.555, val_acc:0.786]
Epoch [15/120    avg_loss:0.431, val_acc:0.831]
Epoch [16/120    avg_loss:0.429, val_acc:0.820]
Epoch [17/120    avg_loss:0.466, val_acc:0.796]
Epoch [18/120    avg_loss:0.383, val_acc:0.833]
Epoch [19/120    avg_loss:0.318, val_acc:0.898]
Epoch [20/120    avg_loss:0.301, val_acc:0.862]
Epoch [21/120    avg_loss:0.336, val_acc:0.888]
Epoch [22/120    avg_loss:0.261, val_acc:0.867]
Epoch [23/120    avg_loss:0.307, val_acc:0.889]
Epoch [24/120    avg_loss:0.265, val_acc:0.861]
Epoch [25/120    avg_loss:0.212, val_acc:0.885]
Epoch [26/120    avg_loss:0.231, val_acc:0.910]
Epoch [27/120    avg_loss:0.171, val_acc:0.902]
Epoch [28/120    avg_loss:0.275, val_acc:0.855]
Epoch [29/120    avg_loss:0.211, val_acc:0.920]
Epoch [30/120    avg_loss:0.206, val_acc:0.907]
Epoch [31/120    avg_loss:0.205, val_acc:0.895]
Epoch [32/120    avg_loss:0.261, val_acc:0.897]
Epoch [33/120    avg_loss:0.216, val_acc:0.935]
Epoch [34/120    avg_loss:0.235, val_acc:0.898]
Epoch [35/120    avg_loss:0.157, val_acc:0.943]
Epoch [36/120    avg_loss:0.132, val_acc:0.927]
Epoch [37/120    avg_loss:0.275, val_acc:0.931]
Epoch [38/120    avg_loss:0.157, val_acc:0.932]
Epoch [39/120    avg_loss:0.134, val_acc:0.943]
Epoch [40/120    avg_loss:0.108, val_acc:0.925]
Epoch [41/120    avg_loss:0.111, val_acc:0.908]
Epoch [42/120    avg_loss:0.131, val_acc:0.950]
Epoch [43/120    avg_loss:0.135, val_acc:0.942]
Epoch [44/120    avg_loss:0.101, val_acc:0.924]
Epoch [45/120    avg_loss:0.139, val_acc:0.939]
Epoch [46/120    avg_loss:0.127, val_acc:0.930]
Epoch [47/120    avg_loss:0.105, val_acc:0.945]
Epoch [48/120    avg_loss:0.073, val_acc:0.953]
Epoch [49/120    avg_loss:0.114, val_acc:0.956]
Epoch [50/120    avg_loss:0.076, val_acc:0.958]
Epoch [51/120    avg_loss:0.085, val_acc:0.942]
Epoch [52/120    avg_loss:0.069, val_acc:0.950]
Epoch [53/120    avg_loss:0.057, val_acc:0.958]
Epoch [54/120    avg_loss:0.081, val_acc:0.956]
Epoch [55/120    avg_loss:0.070, val_acc:0.952]
Epoch [56/120    avg_loss:0.084, val_acc:0.958]
Epoch [57/120    avg_loss:0.057, val_acc:0.962]
Epoch [58/120    avg_loss:0.064, val_acc:0.963]
Epoch [59/120    avg_loss:0.048, val_acc:0.958]
Epoch [60/120    avg_loss:0.083, val_acc:0.956]
Epoch [61/120    avg_loss:0.108, val_acc:0.950]
Epoch [62/120    avg_loss:0.065, val_acc:0.964]
Epoch [63/120    avg_loss:0.062, val_acc:0.967]
Epoch [64/120    avg_loss:0.074, val_acc:0.958]
Epoch [65/120    avg_loss:0.056, val_acc:0.974]
Epoch [66/120    avg_loss:0.044, val_acc:0.967]
Epoch [67/120    avg_loss:0.045, val_acc:0.958]
Epoch [68/120    avg_loss:0.049, val_acc:0.956]
Epoch [69/120    avg_loss:0.077, val_acc:0.963]
Epoch [70/120    avg_loss:0.060, val_acc:0.965]
Epoch [71/120    avg_loss:0.063, val_acc:0.970]
Epoch [72/120    avg_loss:0.041, val_acc:0.969]
Epoch [73/120    avg_loss:0.049, val_acc:0.966]
Epoch [74/120    avg_loss:0.039, val_acc:0.963]
Epoch [75/120    avg_loss:0.059, val_acc:0.970]
Epoch [76/120    avg_loss:0.027, val_acc:0.966]
Epoch [77/120    avg_loss:0.036, val_acc:0.965]
Epoch [78/120    avg_loss:0.045, val_acc:0.973]
Epoch [79/120    avg_loss:0.035, val_acc:0.970]
Epoch [80/120    avg_loss:0.029, val_acc:0.973]
Epoch [81/120    avg_loss:0.028, val_acc:0.977]
Epoch [82/120    avg_loss:0.023, val_acc:0.974]
Epoch [83/120    avg_loss:0.026, val_acc:0.977]
Epoch [84/120    avg_loss:0.026, val_acc:0.978]
Epoch [85/120    avg_loss:0.021, val_acc:0.977]
Epoch [86/120    avg_loss:0.015, val_acc:0.977]
Epoch [87/120    avg_loss:0.019, val_acc:0.977]
Epoch [88/120    avg_loss:0.018, val_acc:0.978]
Epoch [89/120    avg_loss:0.022, val_acc:0.974]
Epoch [90/120    avg_loss:0.017, val_acc:0.974]
Epoch [91/120    avg_loss:0.021, val_acc:0.976]
Epoch [92/120    avg_loss:0.017, val_acc:0.976]
Epoch [93/120    avg_loss:0.022, val_acc:0.975]
Epoch [94/120    avg_loss:0.018, val_acc:0.975]
Epoch [95/120    avg_loss:0.022, val_acc:0.975]
Epoch [96/120    avg_loss:0.016, val_acc:0.977]
Epoch [97/120    avg_loss:0.017, val_acc:0.978]
Epoch [98/120    avg_loss:0.017, val_acc:0.975]
Epoch [99/120    avg_loss:0.016, val_acc:0.977]
Epoch [100/120    avg_loss:0.015, val_acc:0.979]
Epoch [101/120    avg_loss:0.022, val_acc:0.977]
Epoch [102/120    avg_loss:0.014, val_acc:0.977]
Epoch [103/120    avg_loss:0.016, val_acc:0.977]
Epoch [104/120    avg_loss:0.016, val_acc:0.977]
Epoch [105/120    avg_loss:0.023, val_acc:0.977]
Epoch [106/120    avg_loss:0.016, val_acc:0.977]
Epoch [107/120    avg_loss:0.021, val_acc:0.979]
Epoch [108/120    avg_loss:0.017, val_acc:0.980]
Epoch [109/120    avg_loss:0.021, val_acc:0.977]
Epoch [110/120    avg_loss:0.014, val_acc:0.978]
Epoch [111/120    avg_loss:0.015, val_acc:0.978]
Epoch [112/120    avg_loss:0.019, val_acc:0.977]
Epoch [113/120    avg_loss:0.022, val_acc:0.979]
Epoch [114/120    avg_loss:0.018, val_acc:0.976]
Epoch [115/120    avg_loss:0.018, val_acc:0.979]
Epoch [116/120    avg_loss:0.018, val_acc:0.978]
Epoch [117/120    avg_loss:0.014, val_acc:0.977]
Epoch [118/120    avg_loss:0.012, val_acc:0.978]
Epoch [119/120    avg_loss:0.013, val_acc:0.977]
Epoch [120/120    avg_loss:0.013, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1246    3    4    0    2    0    0    3    6   20    0    0
     0    1    0]
 [   0    0    0  714    0   12    0    0    0    6    1    0   11    2
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   12   22    0    4    0    0    0    0  812   15    0    0
     0   10    0]
 [   0    0    3    0    0    0    3    0    0    0   13 2179   10    1
     1    0    0]
 [   0    0    0   16    8    9    0    0    0    0    6    2  491    0
     0    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    3    0    0    0
  1131    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    66  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.83468834688347

F1 scores:
[       nan 0.98765432 0.97879026 0.95073236 0.97260274 0.96098105
 0.99544765 1.         1.         0.8        0.94583576 0.98441382
 0.9370229  0.98924731 0.96625374 0.878125   0.97619048]

Kappa:
0.9639021805779178
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3bc173d860>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.474, val_acc:0.482]
Epoch [2/120    avg_loss:1.959, val_acc:0.522]
Epoch [3/120    avg_loss:1.733, val_acc:0.583]
Epoch [4/120    avg_loss:1.520, val_acc:0.657]
Epoch [5/120    avg_loss:1.278, val_acc:0.694]
Epoch [6/120    avg_loss:1.138, val_acc:0.727]
Epoch [7/120    avg_loss:0.972, val_acc:0.740]
Epoch [8/120    avg_loss:0.865, val_acc:0.764]
Epoch [9/120    avg_loss:0.805, val_acc:0.759]
Epoch [10/120    avg_loss:0.707, val_acc:0.814]
Epoch [11/120    avg_loss:0.583, val_acc:0.809]
Epoch [12/120    avg_loss:0.596, val_acc:0.738]
Epoch [13/120    avg_loss:0.636, val_acc:0.813]
Epoch [14/120    avg_loss:0.516, val_acc:0.833]
Epoch [15/120    avg_loss:0.437, val_acc:0.830]
Epoch [16/120    avg_loss:0.509, val_acc:0.860]
Epoch [17/120    avg_loss:0.407, val_acc:0.811]
Epoch [18/120    avg_loss:0.429, val_acc:0.856]
Epoch [19/120    avg_loss:0.359, val_acc:0.858]
Epoch [20/120    avg_loss:0.389, val_acc:0.873]
Epoch [21/120    avg_loss:0.385, val_acc:0.881]
Epoch [22/120    avg_loss:0.273, val_acc:0.879]
Epoch [23/120    avg_loss:0.224, val_acc:0.904]
Epoch [24/120    avg_loss:0.258, val_acc:0.897]
Epoch [25/120    avg_loss:0.242, val_acc:0.899]
Epoch [26/120    avg_loss:0.205, val_acc:0.897]
Epoch [27/120    avg_loss:0.182, val_acc:0.887]
Epoch [28/120    avg_loss:0.185, val_acc:0.916]
Epoch [29/120    avg_loss:0.204, val_acc:0.909]
Epoch [30/120    avg_loss:0.171, val_acc:0.928]
Epoch [31/120    avg_loss:0.182, val_acc:0.906]
Epoch [32/120    avg_loss:0.269, val_acc:0.854]
Epoch [33/120    avg_loss:0.222, val_acc:0.910]
Epoch [34/120    avg_loss:0.147, val_acc:0.907]
Epoch [35/120    avg_loss:0.152, val_acc:0.920]
Epoch [36/120    avg_loss:0.138, val_acc:0.936]
Epoch [37/120    avg_loss:0.113, val_acc:0.927]
Epoch [38/120    avg_loss:0.099, val_acc:0.931]
Epoch [39/120    avg_loss:0.114, val_acc:0.921]
Epoch [40/120    avg_loss:0.113, val_acc:0.936]
Epoch [41/120    avg_loss:0.102, val_acc:0.919]
Epoch [42/120    avg_loss:0.146, val_acc:0.945]
Epoch [43/120    avg_loss:0.169, val_acc:0.917]
Epoch [44/120    avg_loss:0.126, val_acc:0.944]
Epoch [45/120    avg_loss:0.107, val_acc:0.930]
Epoch [46/120    avg_loss:0.095, val_acc:0.948]
Epoch [47/120    avg_loss:0.072, val_acc:0.945]
Epoch [48/120    avg_loss:0.107, val_acc:0.930]
Epoch [49/120    avg_loss:0.111, val_acc:0.930]
Epoch [50/120    avg_loss:0.079, val_acc:0.923]
Epoch [51/120    avg_loss:0.106, val_acc:0.958]
Epoch [52/120    avg_loss:0.084, val_acc:0.931]
Epoch [53/120    avg_loss:0.112, val_acc:0.955]
Epoch [54/120    avg_loss:0.088, val_acc:0.903]
Epoch [55/120    avg_loss:0.119, val_acc:0.934]
Epoch [56/120    avg_loss:0.055, val_acc:0.958]
Epoch [57/120    avg_loss:0.065, val_acc:0.958]
Epoch [58/120    avg_loss:0.051, val_acc:0.957]
Epoch [59/120    avg_loss:0.052, val_acc:0.961]
Epoch [60/120    avg_loss:0.153, val_acc:0.941]
Epoch [61/120    avg_loss:0.083, val_acc:0.950]
Epoch [62/120    avg_loss:0.079, val_acc:0.951]
Epoch [63/120    avg_loss:0.060, val_acc:0.957]
Epoch [64/120    avg_loss:0.040, val_acc:0.961]
Epoch [65/120    avg_loss:0.051, val_acc:0.959]
Epoch [66/120    avg_loss:0.064, val_acc:0.959]
Epoch [67/120    avg_loss:0.057, val_acc:0.940]
Epoch [68/120    avg_loss:0.098, val_acc:0.950]
Epoch [69/120    avg_loss:0.179, val_acc:0.949]
Epoch [70/120    avg_loss:0.150, val_acc:0.949]
Epoch [71/120    avg_loss:0.065, val_acc:0.958]
Epoch [72/120    avg_loss:0.046, val_acc:0.958]
Epoch [73/120    avg_loss:0.043, val_acc:0.947]
Epoch [74/120    avg_loss:0.049, val_acc:0.949]
Epoch [75/120    avg_loss:0.036, val_acc:0.961]
Epoch [76/120    avg_loss:0.030, val_acc:0.941]
Epoch [77/120    avg_loss:0.043, val_acc:0.965]
Epoch [78/120    avg_loss:0.038, val_acc:0.966]
Epoch [79/120    avg_loss:0.030, val_acc:0.966]
Epoch [80/120    avg_loss:0.033, val_acc:0.956]
Epoch [81/120    avg_loss:0.028, val_acc:0.973]
Epoch [82/120    avg_loss:0.044, val_acc:0.940]
Epoch [83/120    avg_loss:0.044, val_acc:0.961]
Epoch [84/120    avg_loss:0.035, val_acc:0.968]
Epoch [85/120    avg_loss:0.076, val_acc:0.949]
Epoch [86/120    avg_loss:0.035, val_acc:0.966]
Epoch [87/120    avg_loss:0.022, val_acc:0.976]
Epoch [88/120    avg_loss:0.017, val_acc:0.976]
Epoch [89/120    avg_loss:0.032, val_acc:0.969]
Epoch [90/120    avg_loss:0.033, val_acc:0.976]
Epoch [91/120    avg_loss:0.028, val_acc:0.972]
Epoch [92/120    avg_loss:0.018, val_acc:0.977]
Epoch [93/120    avg_loss:0.033, val_acc:0.972]
Epoch [94/120    avg_loss:0.042, val_acc:0.967]
Epoch [95/120    avg_loss:0.036, val_acc:0.959]
Epoch [96/120    avg_loss:0.030, val_acc:0.969]
Epoch [97/120    avg_loss:0.029, val_acc:0.972]
Epoch [98/120    avg_loss:0.018, val_acc:0.976]
Epoch [99/120    avg_loss:0.033, val_acc:0.973]
Epoch [100/120    avg_loss:0.020, val_acc:0.976]
Epoch [101/120    avg_loss:0.018, val_acc:0.977]
Epoch [102/120    avg_loss:0.034, val_acc:0.961]
Epoch [103/120    avg_loss:0.028, val_acc:0.976]
Epoch [104/120    avg_loss:0.018, val_acc:0.978]
Epoch [105/120    avg_loss:0.023, val_acc:0.963]
Epoch [106/120    avg_loss:0.022, val_acc:0.975]
Epoch [107/120    avg_loss:0.017, val_acc:0.977]
Epoch [108/120    avg_loss:0.015, val_acc:0.980]
Epoch [109/120    avg_loss:0.015, val_acc:0.980]
Epoch [110/120    avg_loss:0.025, val_acc:0.977]
Epoch [111/120    avg_loss:0.027, val_acc:0.979]
Epoch [112/120    avg_loss:0.011, val_acc:0.975]
Epoch [113/120    avg_loss:0.021, val_acc:0.965]
Epoch [114/120    avg_loss:0.037, val_acc:0.959]
Epoch [115/120    avg_loss:0.035, val_acc:0.952]
Epoch [116/120    avg_loss:0.245, val_acc:0.950]
Epoch [117/120    avg_loss:0.073, val_acc:0.961]
Epoch [118/120    avg_loss:0.027, val_acc:0.969]
Epoch [119/120    avg_loss:0.027, val_acc:0.961]
Epoch [120/120    avg_loss:0.046, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    4 1224    0    0    0   10    0    0    1   18   11    3    0
     3   11    0]
 [   0    0    0  686    0    8    0    0    0    4    3    0   40    3
     1    2    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   14   48    0   15    0    0    0    0  748    1   14    0
     3   32    0]
 [   0    2    8    0    1    1   35    0    0    0   23 2115    3    7
     1   14    0]
 [   0    0    0    8    2    0    0    0    0    0    0   13  496    0
     1    2   12]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   13    0    0    0    0    2    0    0    0
  1113   11    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    70  261    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.60162601626016

F1 scores:
[       nan 0.93181818 0.96720664 0.92142377 0.99300699 0.95575221
 0.95487627 1.         1.         0.87804878 0.89634512 0.9721903
 0.91009174 0.97368421 0.95372751 0.76764706 0.93333333]

Kappa:
0.9385758337658204
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff6197e36d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.452, val_acc:0.420]
Epoch [2/120    avg_loss:1.962, val_acc:0.614]
Epoch [3/120    avg_loss:1.646, val_acc:0.654]
Epoch [4/120    avg_loss:1.352, val_acc:0.683]
Epoch [5/120    avg_loss:1.162, val_acc:0.719]
Epoch [6/120    avg_loss:0.993, val_acc:0.716]
Epoch [7/120    avg_loss:0.937, val_acc:0.768]
Epoch [8/120    avg_loss:0.788, val_acc:0.795]
Epoch [9/120    avg_loss:0.694, val_acc:0.828]
Epoch [10/120    avg_loss:0.679, val_acc:0.803]
Epoch [11/120    avg_loss:0.558, val_acc:0.837]
Epoch [12/120    avg_loss:0.577, val_acc:0.786]
Epoch [13/120    avg_loss:0.519, val_acc:0.855]
Epoch [14/120    avg_loss:0.453, val_acc:0.844]
Epoch [15/120    avg_loss:0.433, val_acc:0.880]
Epoch [16/120    avg_loss:0.485, val_acc:0.891]
Epoch [17/120    avg_loss:0.483, val_acc:0.843]
Epoch [18/120    avg_loss:0.384, val_acc:0.908]
Epoch [19/120    avg_loss:0.314, val_acc:0.911]
Epoch [20/120    avg_loss:0.282, val_acc:0.909]
Epoch [21/120    avg_loss:0.411, val_acc:0.884]
Epoch [22/120    avg_loss:0.285, val_acc:0.882]
Epoch [23/120    avg_loss:0.364, val_acc:0.909]
Epoch [24/120    avg_loss:0.230, val_acc:0.917]
Epoch [25/120    avg_loss:0.204, val_acc:0.898]
Epoch [26/120    avg_loss:0.222, val_acc:0.896]
Epoch [27/120    avg_loss:0.187, val_acc:0.909]
Epoch [28/120    avg_loss:0.237, val_acc:0.901]
Epoch [29/120    avg_loss:0.253, val_acc:0.942]
Epoch [30/120    avg_loss:0.201, val_acc:0.923]
Epoch [31/120    avg_loss:0.168, val_acc:0.923]
Epoch [32/120    avg_loss:0.173, val_acc:0.948]
Epoch [33/120    avg_loss:0.142, val_acc:0.922]
Epoch [34/120    avg_loss:0.180, val_acc:0.936]
Epoch [35/120    avg_loss:0.248, val_acc:0.924]
Epoch [36/120    avg_loss:0.130, val_acc:0.919]
Epoch [37/120    avg_loss:0.143, val_acc:0.934]
Epoch [38/120    avg_loss:0.171, val_acc:0.945]
Epoch [39/120    avg_loss:0.169, val_acc:0.938]
Epoch [40/120    avg_loss:0.120, val_acc:0.938]
Epoch [41/120    avg_loss:0.166, val_acc:0.934]
Epoch [42/120    avg_loss:0.160, val_acc:0.909]
Epoch [43/120    avg_loss:0.122, val_acc:0.931]
Epoch [44/120    avg_loss:0.103, val_acc:0.918]
Epoch [45/120    avg_loss:0.150, val_acc:0.948]
Epoch [46/120    avg_loss:0.103, val_acc:0.943]
Epoch [47/120    avg_loss:0.095, val_acc:0.947]
Epoch [48/120    avg_loss:0.091, val_acc:0.943]
Epoch [49/120    avg_loss:0.089, val_acc:0.956]
Epoch [50/120    avg_loss:0.086, val_acc:0.959]
Epoch [51/120    avg_loss:0.063, val_acc:0.935]
Epoch [52/120    avg_loss:0.086, val_acc:0.928]
Epoch [53/120    avg_loss:0.075, val_acc:0.957]
Epoch [54/120    avg_loss:0.061, val_acc:0.946]
Epoch [55/120    avg_loss:0.117, val_acc:0.952]
Epoch [56/120    avg_loss:0.073, val_acc:0.955]
Epoch [57/120    avg_loss:0.071, val_acc:0.952]
Epoch [58/120    avg_loss:0.100, val_acc:0.945]
Epoch [59/120    avg_loss:0.090, val_acc:0.955]
Epoch [60/120    avg_loss:0.073, val_acc:0.950]
Epoch [61/120    avg_loss:0.078, val_acc:0.959]
Epoch [62/120    avg_loss:0.048, val_acc:0.961]
Epoch [63/120    avg_loss:0.074, val_acc:0.950]
Epoch [64/120    avg_loss:0.067, val_acc:0.947]
Epoch [65/120    avg_loss:0.097, val_acc:0.866]
Epoch [66/120    avg_loss:0.090, val_acc:0.953]
Epoch [67/120    avg_loss:0.077, val_acc:0.958]
Epoch [68/120    avg_loss:0.059, val_acc:0.958]
Epoch [69/120    avg_loss:0.042, val_acc:0.958]
Epoch [70/120    avg_loss:0.062, val_acc:0.959]
Epoch [71/120    avg_loss:0.043, val_acc:0.961]
Epoch [72/120    avg_loss:0.045, val_acc:0.878]
Epoch [73/120    avg_loss:0.048, val_acc:0.962]
Epoch [74/120    avg_loss:0.069, val_acc:0.968]
Epoch [75/120    avg_loss:0.030, val_acc:0.967]
Epoch [76/120    avg_loss:0.048, val_acc:0.963]
Epoch [77/120    avg_loss:0.036, val_acc:0.974]
Epoch [78/120    avg_loss:0.055, val_acc:0.965]
Epoch [79/120    avg_loss:0.037, val_acc:0.963]
Epoch [80/120    avg_loss:0.036, val_acc:0.969]
Epoch [81/120    avg_loss:0.032, val_acc:0.974]
Epoch [82/120    avg_loss:0.035, val_acc:0.976]
Epoch [83/120    avg_loss:1.385, val_acc:0.561]
Epoch [84/120    avg_loss:1.453, val_acc:0.641]
Epoch [85/120    avg_loss:1.257, val_acc:0.671]
Epoch [86/120    avg_loss:1.070, val_acc:0.730]
Epoch [87/120    avg_loss:1.051, val_acc:0.759]
Epoch [88/120    avg_loss:0.859, val_acc:0.758]
Epoch [89/120    avg_loss:0.866, val_acc:0.749]
Epoch [90/120    avg_loss:0.772, val_acc:0.786]
Epoch [91/120    avg_loss:0.756, val_acc:0.796]
Epoch [92/120    avg_loss:0.694, val_acc:0.802]
Epoch [93/120    avg_loss:0.632, val_acc:0.803]
Epoch [94/120    avg_loss:0.609, val_acc:0.811]
Epoch [95/120    avg_loss:0.574, val_acc:0.831]
Epoch [96/120    avg_loss:0.416, val_acc:0.854]
Epoch [97/120    avg_loss:0.386, val_acc:0.850]
Epoch [98/120    avg_loss:0.417, val_acc:0.856]
Epoch [99/120    avg_loss:0.442, val_acc:0.855]
Epoch [100/120    avg_loss:0.393, val_acc:0.860]
Epoch [101/120    avg_loss:0.395, val_acc:0.857]
Epoch [102/120    avg_loss:0.373, val_acc:0.862]
Epoch [103/120    avg_loss:0.392, val_acc:0.863]
Epoch [104/120    avg_loss:0.356, val_acc:0.872]
Epoch [105/120    avg_loss:0.391, val_acc:0.867]
Epoch [106/120    avg_loss:0.382, val_acc:0.870]
Epoch [107/120    avg_loss:0.354, val_acc:0.870]
Epoch [108/120    avg_loss:0.350, val_acc:0.871]
Epoch [109/120    avg_loss:0.354, val_acc:0.874]
Epoch [110/120    avg_loss:0.351, val_acc:0.875]
Epoch [111/120    avg_loss:0.329, val_acc:0.872]
Epoch [112/120    avg_loss:0.341, val_acc:0.873]
Epoch [113/120    avg_loss:0.346, val_acc:0.874]
Epoch [114/120    avg_loss:0.342, val_acc:0.873]
Epoch [115/120    avg_loss:0.379, val_acc:0.873]
Epoch [116/120    avg_loss:0.360, val_acc:0.874]
Epoch [117/120    avg_loss:0.330, val_acc:0.872]
Epoch [118/120    avg_loss:0.361, val_acc:0.873]
Epoch [119/120    avg_loss:0.342, val_acc:0.874]
Epoch [120/120    avg_loss:0.331, val_acc:0.872]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    4 1087   47    5    1   14    0    0    0   15   82    2    0
     4   24    0]
 [   0    0   26  588   21   48    0    0    0    2    0   25   29    3
     4    1    0]
 [   0    0    0    0  208    0    0    0    0    0    0    0    0    0
     0    5    0]
 [   0    0    0    0    0  374    0   31    0    0    0    1    0    0
    29    0    0]
 [   0    0    2    0    3    0  635    0    0    0    0   12    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    7    0    0    0    0    0    0  408    0    0    0   12    0
     0    3    0]
 [   0    0    0    0    4    0   11    0    0    2    0    1    0    0
     0    0    0]
 [   0    0   37    3    0   25    7    0    0    0  690   51   11    0
     2   49    0]
 [   0    3   37   20    1   38   37    0    0    0   50 1903   39   17
    21   44    0]
 [   0    0    3   28   32   16    0    0    0    0    0    1  407    0
     0   21   26]
 [   0    0    0    0    0    6    0    0    0    0    0    1    0  177
     1    0    0]
 [   0    2    1    1    0   41    0    0    0    0    1    2    0    0
  1063   28    0]
 [   0    0    0    9    0    0    3    0    0   17   15   15    0    0
    75  213    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
85.65853658536585

F1 scores:
[       nan 0.8125     0.87732042 0.81496881 0.85420945 0.7601626
 0.93108504 0.61728395 0.97374702 0.1025641  0.83737864 0.88429368
 0.78647343 0.92670157 0.9073837  0.57959184 0.86010363]

Kappa:
0.8372347945076902
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbdbc3f08d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.444, val_acc:0.524]
Epoch [2/120    avg_loss:1.987, val_acc:0.518]
Epoch [3/120    avg_loss:1.715, val_acc:0.598]
Epoch [4/120    avg_loss:1.446, val_acc:0.648]
Epoch [5/120    avg_loss:1.221, val_acc:0.699]
Epoch [6/120    avg_loss:1.004, val_acc:0.703]
Epoch [7/120    avg_loss:0.917, val_acc:0.762]
Epoch [8/120    avg_loss:0.818, val_acc:0.712]
Epoch [9/120    avg_loss:0.716, val_acc:0.815]
Epoch [10/120    avg_loss:0.660, val_acc:0.765]
Epoch [11/120    avg_loss:0.572, val_acc:0.792]
Epoch [12/120    avg_loss:0.530, val_acc:0.799]
Epoch [13/120    avg_loss:0.538, val_acc:0.804]
Epoch [14/120    avg_loss:0.460, val_acc:0.835]
Epoch [15/120    avg_loss:0.418, val_acc:0.850]
Epoch [16/120    avg_loss:0.435, val_acc:0.874]
Epoch [17/120    avg_loss:0.416, val_acc:0.862]
Epoch [18/120    avg_loss:0.373, val_acc:0.855]
Epoch [19/120    avg_loss:0.399, val_acc:0.847]
Epoch [20/120    avg_loss:0.331, val_acc:0.874]
Epoch [21/120    avg_loss:0.318, val_acc:0.882]
Epoch [22/120    avg_loss:0.277, val_acc:0.887]
Epoch [23/120    avg_loss:0.292, val_acc:0.878]
Epoch [24/120    avg_loss:0.253, val_acc:0.879]
Epoch [25/120    avg_loss:0.242, val_acc:0.892]
Epoch [26/120    avg_loss:0.229, val_acc:0.898]
Epoch [27/120    avg_loss:0.187, val_acc:0.908]
Epoch [28/120    avg_loss:0.253, val_acc:0.900]
Epoch [29/120    avg_loss:0.210, val_acc:0.873]
Epoch [30/120    avg_loss:0.185, val_acc:0.920]
Epoch [31/120    avg_loss:0.192, val_acc:0.907]
Epoch [32/120    avg_loss:0.139, val_acc:0.937]
Epoch [33/120    avg_loss:0.207, val_acc:0.897]
Epoch [34/120    avg_loss:0.153, val_acc:0.926]
Epoch [35/120    avg_loss:0.150, val_acc:0.918]
Epoch [36/120    avg_loss:0.135, val_acc:0.939]
Epoch [37/120    avg_loss:0.196, val_acc:0.915]
Epoch [38/120    avg_loss:0.114, val_acc:0.939]
Epoch [39/120    avg_loss:0.156, val_acc:0.924]
Epoch [40/120    avg_loss:0.131, val_acc:0.914]
Epoch [41/120    avg_loss:0.117, val_acc:0.939]
Epoch [42/120    avg_loss:0.136, val_acc:0.945]
Epoch [43/120    avg_loss:0.109, val_acc:0.943]
Epoch [44/120    avg_loss:0.096, val_acc:0.934]
Epoch [45/120    avg_loss:0.132, val_acc:0.917]
Epoch [46/120    avg_loss:0.087, val_acc:0.947]
Epoch [47/120    avg_loss:0.100, val_acc:0.935]
Epoch [48/120    avg_loss:0.165, val_acc:0.896]
Epoch [49/120    avg_loss:0.166, val_acc:0.914]
Epoch [50/120    avg_loss:0.116, val_acc:0.941]
Epoch [51/120    avg_loss:0.077, val_acc:0.939]
Epoch [52/120    avg_loss:0.076, val_acc:0.949]
Epoch [53/120    avg_loss:0.078, val_acc:0.953]
Epoch [54/120    avg_loss:0.057, val_acc:0.964]
Epoch [55/120    avg_loss:0.059, val_acc:0.955]
Epoch [56/120    avg_loss:0.060, val_acc:0.943]
Epoch [57/120    avg_loss:0.085, val_acc:0.950]
Epoch [58/120    avg_loss:0.059, val_acc:0.946]
Epoch [59/120    avg_loss:0.074, val_acc:0.960]
Epoch [60/120    avg_loss:0.074, val_acc:0.949]
Epoch [61/120    avg_loss:0.076, val_acc:0.948]
Epoch [62/120    avg_loss:0.052, val_acc:0.954]
Epoch [63/120    avg_loss:0.083, val_acc:0.938]
Epoch [64/120    avg_loss:0.050, val_acc:0.948]
Epoch [65/120    avg_loss:0.045, val_acc:0.957]
Epoch [66/120    avg_loss:0.060, val_acc:0.934]
Epoch [67/120    avg_loss:0.140, val_acc:0.957]
Epoch [68/120    avg_loss:0.054, val_acc:0.964]
Epoch [69/120    avg_loss:0.040, val_acc:0.963]
Epoch [70/120    avg_loss:0.039, val_acc:0.964]
Epoch [71/120    avg_loss:0.037, val_acc:0.970]
Epoch [72/120    avg_loss:0.030, val_acc:0.968]
Epoch [73/120    avg_loss:0.040, val_acc:0.970]
Epoch [74/120    avg_loss:0.036, val_acc:0.967]
Epoch [75/120    avg_loss:0.042, val_acc:0.968]
Epoch [76/120    avg_loss:0.032, val_acc:0.967]
Epoch [77/120    avg_loss:0.030, val_acc:0.966]
Epoch [78/120    avg_loss:0.027, val_acc:0.968]
Epoch [79/120    avg_loss:0.027, val_acc:0.968]
Epoch [80/120    avg_loss:0.032, val_acc:0.971]
Epoch [81/120    avg_loss:0.028, val_acc:0.970]
Epoch [82/120    avg_loss:0.027, val_acc:0.972]
Epoch [83/120    avg_loss:0.031, val_acc:0.972]
Epoch [84/120    avg_loss:0.027, val_acc:0.970]
Epoch [85/120    avg_loss:0.027, val_acc:0.971]
Epoch [86/120    avg_loss:0.028, val_acc:0.970]
Epoch [87/120    avg_loss:0.023, val_acc:0.972]
Epoch [88/120    avg_loss:0.026, val_acc:0.967]
Epoch [89/120    avg_loss:0.025, val_acc:0.972]
Epoch [90/120    avg_loss:0.022, val_acc:0.972]
Epoch [91/120    avg_loss:0.024, val_acc:0.971]
Epoch [92/120    avg_loss:0.019, val_acc:0.972]
Epoch [93/120    avg_loss:0.027, val_acc:0.970]
Epoch [94/120    avg_loss:0.026, val_acc:0.971]
Epoch [95/120    avg_loss:0.023, val_acc:0.973]
Epoch [96/120    avg_loss:0.027, val_acc:0.972]
Epoch [97/120    avg_loss:0.024, val_acc:0.972]
Epoch [98/120    avg_loss:0.022, val_acc:0.971]
Epoch [99/120    avg_loss:0.019, val_acc:0.973]
Epoch [100/120    avg_loss:0.022, val_acc:0.974]
Epoch [101/120    avg_loss:0.023, val_acc:0.972]
Epoch [102/120    avg_loss:0.023, val_acc:0.974]
Epoch [103/120    avg_loss:0.020, val_acc:0.974]
Epoch [104/120    avg_loss:0.024, val_acc:0.973]
Epoch [105/120    avg_loss:0.021, val_acc:0.973]
Epoch [106/120    avg_loss:0.020, val_acc:0.975]
Epoch [107/120    avg_loss:0.023, val_acc:0.973]
Epoch [108/120    avg_loss:0.021, val_acc:0.973]
Epoch [109/120    avg_loss:0.020, val_acc:0.975]
Epoch [110/120    avg_loss:0.020, val_acc:0.972]
Epoch [111/120    avg_loss:0.024, val_acc:0.973]
Epoch [112/120    avg_loss:0.020, val_acc:0.973]
Epoch [113/120    avg_loss:0.019, val_acc:0.972]
Epoch [114/120    avg_loss:0.022, val_acc:0.974]
Epoch [115/120    avg_loss:0.019, val_acc:0.972]
Epoch [116/120    avg_loss:0.016, val_acc:0.973]
Epoch [117/120    avg_loss:0.024, val_acc:0.974]
Epoch [118/120    avg_loss:0.017, val_acc:0.973]
Epoch [119/120    avg_loss:0.016, val_acc:0.973]
Epoch [120/120    avg_loss:0.017, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    1 1230    3    1    9    5    0    0    0    7   27    2    0
     0    0    0]
 [   0    0    2  707    1   22    0    0    0    3    0    1    9    1
     1    0    0]
 [   0    0    0    4  209    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0    3   17    0    7    0    0    0    0  818   22    4    0
     1    3    0]
 [   0    0    8    0    0    0    5    0    0    0   15 2161   19    2
     0    0    0]
 [   0    0    0    5    7    8    0    0    0    0    0    3  509    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    3    0    0    0
  1129    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    71  276    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.50948509485094

F1 scores:
[       nan 0.96296296 0.97310127 0.95347269 0.96983759 0.94347826
 0.99092284 1.         0.995338   0.84210526 0.95116279 0.97650249
 0.93998153 0.9919571  0.96454507 0.88038278 0.98224852]

Kappa:
0.9602000628663351
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f62d8fbe7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.483, val_acc:0.512]
Epoch [2/120    avg_loss:1.971, val_acc:0.560]
Epoch [3/120    avg_loss:1.759, val_acc:0.627]
Epoch [4/120    avg_loss:1.437, val_acc:0.655]
Epoch [5/120    avg_loss:1.232, val_acc:0.707]
Epoch [6/120    avg_loss:1.030, val_acc:0.736]
Epoch [7/120    avg_loss:0.927, val_acc:0.768]
Epoch [8/120    avg_loss:0.964, val_acc:0.761]
Epoch [9/120    avg_loss:0.805, val_acc:0.795]
Epoch [10/120    avg_loss:0.713, val_acc:0.816]
Epoch [11/120    avg_loss:0.577, val_acc:0.802]
Epoch [12/120    avg_loss:0.656, val_acc:0.817]
Epoch [13/120    avg_loss:0.482, val_acc:0.842]
Epoch [14/120    avg_loss:0.492, val_acc:0.859]
Epoch [15/120    avg_loss:0.453, val_acc:0.849]
Epoch [16/120    avg_loss:0.389, val_acc:0.861]
Epoch [17/120    avg_loss:0.420, val_acc:0.832]
Epoch [18/120    avg_loss:0.338, val_acc:0.877]
Epoch [19/120    avg_loss:0.313, val_acc:0.854]
Epoch [20/120    avg_loss:0.355, val_acc:0.888]
Epoch [21/120    avg_loss:0.303, val_acc:0.885]
Epoch [22/120    avg_loss:0.340, val_acc:0.903]
Epoch [23/120    avg_loss:0.240, val_acc:0.875]
Epoch [24/120    avg_loss:0.257, val_acc:0.889]
Epoch [25/120    avg_loss:0.259, val_acc:0.917]
Epoch [26/120    avg_loss:0.232, val_acc:0.907]
Epoch [27/120    avg_loss:0.195, val_acc:0.909]
Epoch [28/120    avg_loss:0.238, val_acc:0.875]
Epoch [29/120    avg_loss:0.206, val_acc:0.914]
Epoch [30/120    avg_loss:0.240, val_acc:0.903]
Epoch [31/120    avg_loss:0.237, val_acc:0.909]
Epoch [32/120    avg_loss:0.210, val_acc:0.921]
Epoch [33/120    avg_loss:0.147, val_acc:0.917]
Epoch [34/120    avg_loss:0.132, val_acc:0.928]
Epoch [35/120    avg_loss:0.140, val_acc:0.911]
Epoch [36/120    avg_loss:0.123, val_acc:0.926]
Epoch [37/120    avg_loss:0.146, val_acc:0.920]
Epoch [38/120    avg_loss:0.218, val_acc:0.886]
Epoch [39/120    avg_loss:0.168, val_acc:0.941]
Epoch [40/120    avg_loss:0.125, val_acc:0.909]
Epoch [41/120    avg_loss:0.110, val_acc:0.924]
Epoch [42/120    avg_loss:0.112, val_acc:0.950]
Epoch [43/120    avg_loss:0.103, val_acc:0.940]
Epoch [44/120    avg_loss:0.104, val_acc:0.918]
Epoch [45/120    avg_loss:0.123, val_acc:0.930]
Epoch [46/120    avg_loss:0.101, val_acc:0.926]
Epoch [47/120    avg_loss:0.145, val_acc:0.848]
Epoch [48/120    avg_loss:0.244, val_acc:0.911]
Epoch [49/120    avg_loss:0.168, val_acc:0.925]
Epoch [50/120    avg_loss:0.083, val_acc:0.933]
Epoch [51/120    avg_loss:0.109, val_acc:0.952]
Epoch [52/120    avg_loss:0.120, val_acc:0.913]
Epoch [53/120    avg_loss:0.074, val_acc:0.910]
Epoch [54/120    avg_loss:0.108, val_acc:0.938]
Epoch [55/120    avg_loss:0.060, val_acc:0.955]
Epoch [56/120    avg_loss:0.091, val_acc:0.951]
Epoch [57/120    avg_loss:0.079, val_acc:0.948]
Epoch [58/120    avg_loss:0.092, val_acc:0.948]
Epoch [59/120    avg_loss:0.087, val_acc:0.957]
Epoch [60/120    avg_loss:0.055, val_acc:0.962]
Epoch [61/120    avg_loss:0.057, val_acc:0.945]
Epoch [62/120    avg_loss:0.052, val_acc:0.954]
Epoch [63/120    avg_loss:0.040, val_acc:0.962]
Epoch [64/120    avg_loss:0.051, val_acc:0.964]
Epoch [65/120    avg_loss:0.038, val_acc:0.964]
Epoch [66/120    avg_loss:0.050, val_acc:0.962]
Epoch [67/120    avg_loss:0.052, val_acc:0.902]
Epoch [68/120    avg_loss:0.050, val_acc:0.958]
Epoch [69/120    avg_loss:0.051, val_acc:0.964]
Epoch [70/120    avg_loss:0.040, val_acc:0.955]
Epoch [71/120    avg_loss:0.066, val_acc:0.957]
Epoch [72/120    avg_loss:0.045, val_acc:0.962]
Epoch [73/120    avg_loss:0.048, val_acc:0.963]
Epoch [74/120    avg_loss:0.066, val_acc:0.936]
Epoch [75/120    avg_loss:0.063, val_acc:0.964]
Epoch [76/120    avg_loss:0.042, val_acc:0.964]
Epoch [77/120    avg_loss:0.128, val_acc:0.950]
Epoch [78/120    avg_loss:0.056, val_acc:0.963]
Epoch [79/120    avg_loss:0.056, val_acc:0.965]
Epoch [80/120    avg_loss:0.035, val_acc:0.965]
Epoch [81/120    avg_loss:0.057, val_acc:0.966]
Epoch [82/120    avg_loss:0.057, val_acc:0.960]
Epoch [83/120    avg_loss:0.047, val_acc:0.963]
Epoch [84/120    avg_loss:0.040, val_acc:0.963]
Epoch [85/120    avg_loss:0.032, val_acc:0.970]
Epoch [86/120    avg_loss:0.059, val_acc:0.949]
Epoch [87/120    avg_loss:0.035, val_acc:0.973]
Epoch [88/120    avg_loss:0.032, val_acc:0.965]
Epoch [89/120    avg_loss:0.045, val_acc:0.933]
Epoch [90/120    avg_loss:0.065, val_acc:0.966]
Epoch [91/120    avg_loss:0.031, val_acc:0.964]
Epoch [92/120    avg_loss:0.033, val_acc:0.960]
Epoch [93/120    avg_loss:0.021, val_acc:0.971]
Epoch [94/120    avg_loss:0.026, val_acc:0.967]
Epoch [95/120    avg_loss:0.016, val_acc:0.978]
Epoch [96/120    avg_loss:0.027, val_acc:0.962]
Epoch [97/120    avg_loss:0.031, val_acc:0.972]
Epoch [98/120    avg_loss:0.067, val_acc:0.935]
Epoch [99/120    avg_loss:0.072, val_acc:0.966]
Epoch [100/120    avg_loss:0.045, val_acc:0.964]
Epoch [101/120    avg_loss:0.024, val_acc:0.973]
Epoch [102/120    avg_loss:0.034, val_acc:0.967]
Epoch [103/120    avg_loss:0.030, val_acc:0.972]
Epoch [104/120    avg_loss:0.020, val_acc:0.968]
Epoch [105/120    avg_loss:0.030, val_acc:0.972]
Epoch [106/120    avg_loss:0.021, val_acc:0.962]
Epoch [107/120    avg_loss:0.027, val_acc:0.964]
Epoch [108/120    avg_loss:0.019, val_acc:0.970]
Epoch [109/120    avg_loss:0.016, val_acc:0.974]
Epoch [110/120    avg_loss:0.012, val_acc:0.974]
Epoch [111/120    avg_loss:0.012, val_acc:0.975]
Epoch [112/120    avg_loss:0.009, val_acc:0.974]
Epoch [113/120    avg_loss:0.010, val_acc:0.976]
Epoch [114/120    avg_loss:0.010, val_acc:0.974]
Epoch [115/120    avg_loss:0.012, val_acc:0.976]
Epoch [116/120    avg_loss:0.010, val_acc:0.977]
Epoch [117/120    avg_loss:0.009, val_acc:0.975]
Epoch [118/120    avg_loss:0.009, val_acc:0.977]
Epoch [119/120    avg_loss:0.010, val_acc:0.976]
Epoch [120/120    avg_loss:0.010, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1259    6    1    0    0    0    0    0    8   11    0    0
     0    0    0]
 [   0    0    0  724    0   10    0    0    0    7    0    0    5    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    1    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10   41    0    5    0    0    0    0  799   14    0    0
     1    5    0]
 [   0    0    4    0    0    0    3    0    0    0   10 2184    6    3
     0    0    0]
 [   0    0    0    0    5   12    0    0    0    1    1    0  510    0
     4    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    3    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
   121  214    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.53116531165311

F1 scores:
[       nan 0.98765432 0.98436278 0.95388669 0.98611111 0.96644295
 0.98642534 1.         0.99767981 0.7826087  0.94166176 0.98823529
 0.96682464 0.98930481 0.94421316 0.75618375 0.99408284]

Kappa:
0.960416373622233
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f15837c5860>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.468, val_acc:0.511]
Epoch [2/120    avg_loss:1.954, val_acc:0.588]
Epoch [3/120    avg_loss:1.675, val_acc:0.623]
Epoch [4/120    avg_loss:1.410, val_acc:0.673]
Epoch [5/120    avg_loss:1.229, val_acc:0.713]
Epoch [6/120    avg_loss:1.079, val_acc:0.735]
Epoch [7/120    avg_loss:0.911, val_acc:0.741]
Epoch [8/120    avg_loss:0.746, val_acc:0.767]
Epoch [9/120    avg_loss:0.752, val_acc:0.785]
Epoch [10/120    avg_loss:0.630, val_acc:0.796]
Epoch [11/120    avg_loss:0.553, val_acc:0.749]
Epoch [12/120    avg_loss:0.507, val_acc:0.827]
Epoch [13/120    avg_loss:0.534, val_acc:0.826]
Epoch [14/120    avg_loss:0.455, val_acc:0.873]
Epoch [15/120    avg_loss:0.434, val_acc:0.816]
Epoch [16/120    avg_loss:0.417, val_acc:0.863]
Epoch [17/120    avg_loss:0.421, val_acc:0.876]
Epoch [18/120    avg_loss:0.385, val_acc:0.895]
Epoch [19/120    avg_loss:0.313, val_acc:0.876]
Epoch [20/120    avg_loss:0.354, val_acc:0.885]
Epoch [21/120    avg_loss:0.314, val_acc:0.899]
Epoch [22/120    avg_loss:0.298, val_acc:0.899]
Epoch [23/120    avg_loss:0.322, val_acc:0.889]
Epoch [24/120    avg_loss:0.273, val_acc:0.866]
Epoch [25/120    avg_loss:0.249, val_acc:0.886]
Epoch [26/120    avg_loss:0.267, val_acc:0.916]
Epoch [27/120    avg_loss:0.282, val_acc:0.904]
Epoch [28/120    avg_loss:0.182, val_acc:0.947]
Epoch [29/120    avg_loss:0.192, val_acc:0.916]
Epoch [30/120    avg_loss:0.166, val_acc:0.923]
Epoch [31/120    avg_loss:0.175, val_acc:0.939]
Epoch [32/120    avg_loss:0.157, val_acc:0.928]
Epoch [33/120    avg_loss:0.171, val_acc:0.920]
Epoch [34/120    avg_loss:0.187, val_acc:0.917]
Epoch [35/120    avg_loss:0.150, val_acc:0.929]
Epoch [36/120    avg_loss:0.118, val_acc:0.946]
Epoch [37/120    avg_loss:0.124, val_acc:0.925]
Epoch [38/120    avg_loss:0.158, val_acc:0.936]
Epoch [39/120    avg_loss:0.125, val_acc:0.949]
Epoch [40/120    avg_loss:0.133, val_acc:0.947]
Epoch [41/120    avg_loss:0.134, val_acc:0.921]
Epoch [42/120    avg_loss:0.193, val_acc:0.936]
Epoch [43/120    avg_loss:0.113, val_acc:0.944]
Epoch [44/120    avg_loss:0.100, val_acc:0.949]
Epoch [45/120    avg_loss:0.086, val_acc:0.955]
Epoch [46/120    avg_loss:0.105, val_acc:0.948]
Epoch [47/120    avg_loss:0.068, val_acc:0.970]
Epoch [48/120    avg_loss:0.110, val_acc:0.935]
Epoch [49/120    avg_loss:0.108, val_acc:0.961]
Epoch [50/120    avg_loss:0.077, val_acc:0.965]
Epoch [51/120    avg_loss:0.087, val_acc:0.938]
Epoch [52/120    avg_loss:0.066, val_acc:0.965]
Epoch [53/120    avg_loss:0.055, val_acc:0.955]
Epoch [54/120    avg_loss:0.087, val_acc:0.947]
Epoch [55/120    avg_loss:0.068, val_acc:0.959]
Epoch [56/120    avg_loss:0.083, val_acc:0.952]
Epoch [57/120    avg_loss:0.066, val_acc:0.959]
Epoch [58/120    avg_loss:0.074, val_acc:0.959]
Epoch [59/120    avg_loss:0.085, val_acc:0.917]
Epoch [60/120    avg_loss:0.120, val_acc:0.957]
Epoch [61/120    avg_loss:0.068, val_acc:0.960]
Epoch [62/120    avg_loss:0.053, val_acc:0.961]
Epoch [63/120    avg_loss:0.040, val_acc:0.963]
Epoch [64/120    avg_loss:0.047, val_acc:0.968]
Epoch [65/120    avg_loss:0.044, val_acc:0.972]
Epoch [66/120    avg_loss:0.038, val_acc:0.971]
Epoch [67/120    avg_loss:0.043, val_acc:0.976]
Epoch [68/120    avg_loss:0.038, val_acc:0.969]
Epoch [69/120    avg_loss:0.035, val_acc:0.971]
Epoch [70/120    avg_loss:0.033, val_acc:0.976]
Epoch [71/120    avg_loss:0.025, val_acc:0.977]
Epoch [72/120    avg_loss:0.026, val_acc:0.973]
Epoch [73/120    avg_loss:0.027, val_acc:0.976]
Epoch [74/120    avg_loss:0.029, val_acc:0.976]
Epoch [75/120    avg_loss:0.025, val_acc:0.975]
Epoch [76/120    avg_loss:0.028, val_acc:0.977]
Epoch [77/120    avg_loss:0.028, val_acc:0.975]
Epoch [78/120    avg_loss:0.030, val_acc:0.977]
Epoch [79/120    avg_loss:0.026, val_acc:0.978]
Epoch [80/120    avg_loss:0.032, val_acc:0.980]
Epoch [81/120    avg_loss:0.031, val_acc:0.980]
Epoch [82/120    avg_loss:0.020, val_acc:0.980]
Epoch [83/120    avg_loss:0.022, val_acc:0.981]
Epoch [84/120    avg_loss:0.026, val_acc:0.978]
Epoch [85/120    avg_loss:0.035, val_acc:0.979]
Epoch [86/120    avg_loss:0.021, val_acc:0.980]
Epoch [87/120    avg_loss:0.025, val_acc:0.979]
Epoch [88/120    avg_loss:0.021, val_acc:0.979]
Epoch [89/120    avg_loss:0.025, val_acc:0.979]
Epoch [90/120    avg_loss:0.025, val_acc:0.980]
Epoch [91/120    avg_loss:0.024, val_acc:0.980]
Epoch [92/120    avg_loss:0.018, val_acc:0.979]
Epoch [93/120    avg_loss:0.017, val_acc:0.983]
Epoch [94/120    avg_loss:0.020, val_acc:0.980]
Epoch [95/120    avg_loss:0.021, val_acc:0.982]
Epoch [96/120    avg_loss:0.020, val_acc:0.982]
Epoch [97/120    avg_loss:0.024, val_acc:0.983]
Epoch [98/120    avg_loss:0.017, val_acc:0.980]
Epoch [99/120    avg_loss:0.020, val_acc:0.981]
Epoch [100/120    avg_loss:0.017, val_acc:0.982]
Epoch [101/120    avg_loss:0.020, val_acc:0.982]
Epoch [102/120    avg_loss:0.019, val_acc:0.981]
Epoch [103/120    avg_loss:0.018, val_acc:0.982]
Epoch [104/120    avg_loss:0.018, val_acc:0.979]
Epoch [105/120    avg_loss:0.030, val_acc:0.982]
Epoch [106/120    avg_loss:0.019, val_acc:0.979]
Epoch [107/120    avg_loss:0.026, val_acc:0.979]
Epoch [108/120    avg_loss:0.018, val_acc:0.981]
Epoch [109/120    avg_loss:0.018, val_acc:0.980]
Epoch [110/120    avg_loss:0.021, val_acc:0.979]
Epoch [111/120    avg_loss:0.018, val_acc:0.980]
Epoch [112/120    avg_loss:0.020, val_acc:0.980]
Epoch [113/120    avg_loss:0.021, val_acc:0.980]
Epoch [114/120    avg_loss:0.022, val_acc:0.980]
Epoch [115/120    avg_loss:0.021, val_acc:0.980]
Epoch [116/120    avg_loss:0.018, val_acc:0.981]
Epoch [117/120    avg_loss:0.019, val_acc:0.981]
Epoch [118/120    avg_loss:0.020, val_acc:0.981]
Epoch [119/120    avg_loss:0.017, val_acc:0.980]
Epoch [120/120    avg_loss:0.018, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1246    0    4    0    2    0    0    0   17   16    0    0
     0    0    0]
 [   0    0    0  717    1   13    0    0    0    4    0    0    9    2
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9   19    0    9    1    0    0    0  826    8    0    0
     1    2    0]
 [   0    0    6    0    0    1    4    0    0    0   11 2185    1    2
     0    0    0]
 [   0    0    0   11    4   14    0    0    0    0    0    0  500    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    4    0    3    0    0    0
  1121    7    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    47  286    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.13821138211382

F1 scores:
[       nan 0.98765432 0.97879026 0.95983936 0.97931034 0.95384615
 0.98121713 0.98039216 0.99303944 0.9        0.95326024 0.98757062
 0.95785441 0.98930481 0.97098311 0.89096573 0.97109827]

Kappa:
0.9673719858877208
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f59f6e1e898>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.389, val_acc:0.436]
Epoch [2/120    avg_loss:1.914, val_acc:0.565]
Epoch [3/120    avg_loss:1.669, val_acc:0.588]
Epoch [4/120    avg_loss:1.484, val_acc:0.626]
Epoch [5/120    avg_loss:1.320, val_acc:0.674]
Epoch [6/120    avg_loss:1.068, val_acc:0.722]
Epoch [7/120    avg_loss:0.928, val_acc:0.646]
Epoch [8/120    avg_loss:0.822, val_acc:0.771]
Epoch [9/120    avg_loss:0.722, val_acc:0.773]
Epoch [10/120    avg_loss:0.664, val_acc:0.790]
Epoch [11/120    avg_loss:0.556, val_acc:0.792]
Epoch [12/120    avg_loss:0.553, val_acc:0.836]
Epoch [13/120    avg_loss:0.452, val_acc:0.817]
Epoch [14/120    avg_loss:0.468, val_acc:0.861]
Epoch [15/120    avg_loss:0.431, val_acc:0.864]
Epoch [16/120    avg_loss:0.400, val_acc:0.808]
Epoch [17/120    avg_loss:0.389, val_acc:0.878]
Epoch [18/120    avg_loss:0.270, val_acc:0.890]
Epoch [19/120    avg_loss:0.294, val_acc:0.900]
Epoch [20/120    avg_loss:0.425, val_acc:0.842]
Epoch [21/120    avg_loss:0.287, val_acc:0.915]
Epoch [22/120    avg_loss:0.275, val_acc:0.878]
Epoch [23/120    avg_loss:0.250, val_acc:0.904]
Epoch [24/120    avg_loss:0.256, val_acc:0.895]
Epoch [25/120    avg_loss:0.197, val_acc:0.908]
Epoch [26/120    avg_loss:0.174, val_acc:0.925]
Epoch [27/120    avg_loss:0.150, val_acc:0.927]
Epoch [28/120    avg_loss:0.197, val_acc:0.916]
Epoch [29/120    avg_loss:0.264, val_acc:0.845]
Epoch [30/120    avg_loss:0.313, val_acc:0.909]
Epoch [31/120    avg_loss:0.152, val_acc:0.912]
Epoch [32/120    avg_loss:0.179, val_acc:0.890]
Epoch [33/120    avg_loss:0.164, val_acc:0.938]
Epoch [34/120    avg_loss:0.139, val_acc:0.943]
Epoch [35/120    avg_loss:0.159, val_acc:0.930]
Epoch [36/120    avg_loss:0.163, val_acc:0.921]
Epoch [37/120    avg_loss:0.111, val_acc:0.939]
Epoch [38/120    avg_loss:0.204, val_acc:0.928]
Epoch [39/120    avg_loss:0.146, val_acc:0.933]
Epoch [40/120    avg_loss:0.144, val_acc:0.922]
Epoch [41/120    avg_loss:0.106, val_acc:0.955]
Epoch [42/120    avg_loss:0.088, val_acc:0.930]
Epoch [43/120    avg_loss:0.083, val_acc:0.939]
Epoch [44/120    avg_loss:0.090, val_acc:0.946]
Epoch [45/120    avg_loss:0.095, val_acc:0.950]
Epoch [46/120    avg_loss:0.095, val_acc:0.954]
Epoch [47/120    avg_loss:0.101, val_acc:0.945]
Epoch [48/120    avg_loss:0.076, val_acc:0.961]
Epoch [49/120    avg_loss:0.063, val_acc:0.962]
Epoch [50/120    avg_loss:0.072, val_acc:0.938]
Epoch [51/120    avg_loss:0.068, val_acc:0.954]
Epoch [52/120    avg_loss:0.056, val_acc:0.964]
Epoch [53/120    avg_loss:0.135, val_acc:0.829]
Epoch [54/120    avg_loss:0.178, val_acc:0.874]
Epoch [55/120    avg_loss:0.104, val_acc:0.959]
Epoch [56/120    avg_loss:0.073, val_acc:0.962]
Epoch [57/120    avg_loss:0.062, val_acc:0.965]
Epoch [58/120    avg_loss:0.044, val_acc:0.966]
Epoch [59/120    avg_loss:0.032, val_acc:0.965]
Epoch [60/120    avg_loss:0.036, val_acc:0.973]
Epoch [61/120    avg_loss:0.029, val_acc:0.979]
Epoch [62/120    avg_loss:0.026, val_acc:0.974]
Epoch [63/120    avg_loss:0.037, val_acc:0.964]
Epoch [64/120    avg_loss:0.048, val_acc:0.954]
Epoch [65/120    avg_loss:0.041, val_acc:0.946]
Epoch [66/120    avg_loss:0.045, val_acc:0.968]
Epoch [67/120    avg_loss:0.024, val_acc:0.972]
Epoch [68/120    avg_loss:0.040, val_acc:0.954]
Epoch [69/120    avg_loss:0.048, val_acc:0.954]
Epoch [70/120    avg_loss:0.097, val_acc:0.932]
Epoch [71/120    avg_loss:0.092, val_acc:0.934]
Epoch [72/120    avg_loss:0.065, val_acc:0.977]
Epoch [73/120    avg_loss:0.040, val_acc:0.958]
Epoch [74/120    avg_loss:0.041, val_acc:0.973]
Epoch [75/120    avg_loss:0.023, val_acc:0.976]
Epoch [76/120    avg_loss:0.020, val_acc:0.979]
Epoch [77/120    avg_loss:0.021, val_acc:0.980]
Epoch [78/120    avg_loss:0.020, val_acc:0.979]
Epoch [79/120    avg_loss:0.021, val_acc:0.979]
Epoch [80/120    avg_loss:0.018, val_acc:0.979]
Epoch [81/120    avg_loss:0.019, val_acc:0.979]
Epoch [82/120    avg_loss:0.015, val_acc:0.980]
Epoch [83/120    avg_loss:0.019, val_acc:0.978]
Epoch [84/120    avg_loss:0.014, val_acc:0.979]
Epoch [85/120    avg_loss:0.017, val_acc:0.979]
Epoch [86/120    avg_loss:0.016, val_acc:0.980]
Epoch [87/120    avg_loss:0.015, val_acc:0.982]
Epoch [88/120    avg_loss:0.017, val_acc:0.982]
Epoch [89/120    avg_loss:0.015, val_acc:0.980]
Epoch [90/120    avg_loss:0.015, val_acc:0.980]
Epoch [91/120    avg_loss:0.019, val_acc:0.979]
Epoch [92/120    avg_loss:0.016, val_acc:0.980]
Epoch [93/120    avg_loss:0.015, val_acc:0.980]
Epoch [94/120    avg_loss:0.013, val_acc:0.983]
Epoch [95/120    avg_loss:0.016, val_acc:0.983]
Epoch [96/120    avg_loss:0.018, val_acc:0.979]
Epoch [97/120    avg_loss:0.012, val_acc:0.978]
Epoch [98/120    avg_loss:0.015, val_acc:0.980]
Epoch [99/120    avg_loss:0.015, val_acc:0.979]
Epoch [100/120    avg_loss:0.012, val_acc:0.980]
Epoch [101/120    avg_loss:0.014, val_acc:0.980]
Epoch [102/120    avg_loss:0.010, val_acc:0.980]
Epoch [103/120    avg_loss:0.019, val_acc:0.980]
Epoch [104/120    avg_loss:0.018, val_acc:0.982]
Epoch [105/120    avg_loss:0.016, val_acc:0.980]
Epoch [106/120    avg_loss:0.012, val_acc:0.980]
Epoch [107/120    avg_loss:0.015, val_acc:0.982]
Epoch [108/120    avg_loss:0.012, val_acc:0.983]
Epoch [109/120    avg_loss:0.015, val_acc:0.984]
Epoch [110/120    avg_loss:0.013, val_acc:0.984]
Epoch [111/120    avg_loss:0.012, val_acc:0.984]
Epoch [112/120    avg_loss:0.012, val_acc:0.986]
Epoch [113/120    avg_loss:0.012, val_acc:0.984]
Epoch [114/120    avg_loss:0.012, val_acc:0.984]
Epoch [115/120    avg_loss:0.013, val_acc:0.982]
Epoch [116/120    avg_loss:0.012, val_acc:0.983]
Epoch [117/120    avg_loss:0.014, val_acc:0.985]
Epoch [118/120    avg_loss:0.011, val_acc:0.984]
Epoch [119/120    avg_loss:0.012, val_acc:0.984]
Epoch [120/120    avg_loss:0.011, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1256    1    2    0    1    0    0    0   12   12    1    0
     0    0    0]
 [   0    0    0  731    0    3    0    0    0    3    2    0    4    3
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3   18    0    8    1    0    0    0  827   14    0    0
     3    1    0]
 [   0    0    6    0    0    0    3    0    0    0    9 2189    1    2
     0    0    0]
 [   0    0    1    4    4    9    0    0    0    0    5   15  492    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1135    2    0]
 [   0    0    0    0    0    0   28    0    0    0    0    0    0    0
    64  255    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.24661246612466

F1 scores:
[       nan 1.         0.98471188 0.97401732 0.98611111 0.97522523
 0.97550111 0.98039216 0.99883856 0.92307692 0.95551704 0.98603604
 0.95348837 0.98666667 0.96884336 0.84297521 0.97674419]

Kappa:
0.9685815750712166
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f25e6b05828>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.388, val_acc:0.514]
Epoch [2/120    avg_loss:1.904, val_acc:0.557]
Epoch [3/120    avg_loss:1.674, val_acc:0.618]
Epoch [4/120    avg_loss:1.440, val_acc:0.689]
Epoch [5/120    avg_loss:1.252, val_acc:0.679]
Epoch [6/120    avg_loss:1.170, val_acc:0.720]
Epoch [7/120    avg_loss:0.942, val_acc:0.711]
Epoch [8/120    avg_loss:0.875, val_acc:0.721]
Epoch [9/120    avg_loss:0.805, val_acc:0.724]
Epoch [10/120    avg_loss:0.752, val_acc:0.759]
Epoch [11/120    avg_loss:0.655, val_acc:0.796]
Epoch [12/120    avg_loss:0.563, val_acc:0.828]
Epoch [13/120    avg_loss:0.491, val_acc:0.832]
Epoch [14/120    avg_loss:0.523, val_acc:0.852]
Epoch [15/120    avg_loss:0.465, val_acc:0.857]
Epoch [16/120    avg_loss:0.375, val_acc:0.861]
Epoch [17/120    avg_loss:0.347, val_acc:0.851]
Epoch [18/120    avg_loss:0.345, val_acc:0.861]
Epoch [19/120    avg_loss:0.320, val_acc:0.888]
Epoch [20/120    avg_loss:0.385, val_acc:0.868]
Epoch [21/120    avg_loss:0.345, val_acc:0.864]
Epoch [22/120    avg_loss:0.368, val_acc:0.840]
Epoch [23/120    avg_loss:0.289, val_acc:0.890]
Epoch [24/120    avg_loss:0.230, val_acc:0.910]
Epoch [25/120    avg_loss:0.205, val_acc:0.885]
Epoch [26/120    avg_loss:0.250, val_acc:0.897]
Epoch [27/120    avg_loss:0.262, val_acc:0.856]
Epoch [28/120    avg_loss:0.184, val_acc:0.919]
Epoch [29/120    avg_loss:0.156, val_acc:0.908]
Epoch [30/120    avg_loss:0.173, val_acc:0.893]
Epoch [31/120    avg_loss:0.178, val_acc:0.879]
Epoch [32/120    avg_loss:0.152, val_acc:0.914]
Epoch [33/120    avg_loss:0.167, val_acc:0.901]
Epoch [34/120    avg_loss:0.136, val_acc:0.904]
Epoch [35/120    avg_loss:0.115, val_acc:0.929]
Epoch [36/120    avg_loss:0.148, val_acc:0.931]
Epoch [37/120    avg_loss:0.112, val_acc:0.947]
Epoch [38/120    avg_loss:0.115, val_acc:0.921]
Epoch [39/120    avg_loss:0.108, val_acc:0.940]
Epoch [40/120    avg_loss:0.182, val_acc:0.874]
Epoch [41/120    avg_loss:0.166, val_acc:0.935]
Epoch [42/120    avg_loss:0.138, val_acc:0.921]
Epoch [43/120    avg_loss:0.123, val_acc:0.911]
Epoch [44/120    avg_loss:0.112, val_acc:0.928]
Epoch [45/120    avg_loss:0.159, val_acc:0.906]
Epoch [46/120    avg_loss:0.108, val_acc:0.935]
Epoch [47/120    avg_loss:0.122, val_acc:0.928]
Epoch [48/120    avg_loss:0.092, val_acc:0.941]
Epoch [49/120    avg_loss:0.073, val_acc:0.941]
Epoch [50/120    avg_loss:0.077, val_acc:0.935]
Epoch [51/120    avg_loss:0.088, val_acc:0.945]
Epoch [52/120    avg_loss:0.054, val_acc:0.951]
Epoch [53/120    avg_loss:0.043, val_acc:0.945]
Epoch [54/120    avg_loss:0.041, val_acc:0.945]
Epoch [55/120    avg_loss:0.040, val_acc:0.948]
Epoch [56/120    avg_loss:0.040, val_acc:0.950]
Epoch [57/120    avg_loss:0.034, val_acc:0.948]
Epoch [58/120    avg_loss:0.033, val_acc:0.950]
Epoch [59/120    avg_loss:0.039, val_acc:0.950]
Epoch [60/120    avg_loss:0.034, val_acc:0.950]
Epoch [61/120    avg_loss:0.037, val_acc:0.950]
Epoch [62/120    avg_loss:0.033, val_acc:0.951]
Epoch [63/120    avg_loss:0.035, val_acc:0.953]
Epoch [64/120    avg_loss:0.038, val_acc:0.953]
Epoch [65/120    avg_loss:0.037, val_acc:0.953]
Epoch [66/120    avg_loss:0.031, val_acc:0.956]
Epoch [67/120    avg_loss:0.033, val_acc:0.954]
Epoch [68/120    avg_loss:0.032, val_acc:0.955]
Epoch [69/120    avg_loss:0.033, val_acc:0.956]
Epoch [70/120    avg_loss:0.030, val_acc:0.954]
Epoch [71/120    avg_loss:0.035, val_acc:0.953]
Epoch [72/120    avg_loss:0.040, val_acc:0.954]
Epoch [73/120    avg_loss:0.036, val_acc:0.953]
Epoch [74/120    avg_loss:0.029, val_acc:0.952]
Epoch [75/120    avg_loss:0.034, val_acc:0.951]
Epoch [76/120    avg_loss:0.031, val_acc:0.953]
Epoch [77/120    avg_loss:0.027, val_acc:0.954]
Epoch [78/120    avg_loss:0.029, val_acc:0.952]
Epoch [79/120    avg_loss:0.029, val_acc:0.952]
Epoch [80/120    avg_loss:0.034, val_acc:0.954]
Epoch [81/120    avg_loss:0.033, val_acc:0.957]
Epoch [82/120    avg_loss:0.026, val_acc:0.958]
Epoch [83/120    avg_loss:0.033, val_acc:0.956]
Epoch [84/120    avg_loss:0.026, val_acc:0.955]
Epoch [85/120    avg_loss:0.028, val_acc:0.959]
Epoch [86/120    avg_loss:0.028, val_acc:0.954]
Epoch [87/120    avg_loss:0.028, val_acc:0.959]
Epoch [88/120    avg_loss:0.022, val_acc:0.958]
Epoch [89/120    avg_loss:0.026, val_acc:0.958]
Epoch [90/120    avg_loss:0.026, val_acc:0.957]
Epoch [91/120    avg_loss:0.025, val_acc:0.957]
Epoch [92/120    avg_loss:0.026, val_acc:0.962]
Epoch [93/120    avg_loss:0.027, val_acc:0.959]
Epoch [94/120    avg_loss:0.028, val_acc:0.962]
Epoch [95/120    avg_loss:0.025, val_acc:0.961]
Epoch [96/120    avg_loss:0.029, val_acc:0.963]
Epoch [97/120    avg_loss:0.029, val_acc:0.957]
Epoch [98/120    avg_loss:0.027, val_acc:0.961]
Epoch [99/120    avg_loss:0.022, val_acc:0.961]
Epoch [100/120    avg_loss:0.026, val_acc:0.959]
Epoch [101/120    avg_loss:0.025, val_acc:0.959]
Epoch [102/120    avg_loss:0.030, val_acc:0.961]
Epoch [103/120    avg_loss:0.023, val_acc:0.961]
Epoch [104/120    avg_loss:0.019, val_acc:0.961]
Epoch [105/120    avg_loss:0.027, val_acc:0.962]
Epoch [106/120    avg_loss:0.025, val_acc:0.963]
Epoch [107/120    avg_loss:0.025, val_acc:0.962]
Epoch [108/120    avg_loss:0.022, val_acc:0.963]
Epoch [109/120    avg_loss:0.026, val_acc:0.961]
Epoch [110/120    avg_loss:0.027, val_acc:0.961]
Epoch [111/120    avg_loss:0.023, val_acc:0.962]
Epoch [112/120    avg_loss:0.028, val_acc:0.959]
Epoch [113/120    avg_loss:0.031, val_acc:0.961]
Epoch [114/120    avg_loss:0.022, val_acc:0.963]
Epoch [115/120    avg_loss:0.018, val_acc:0.963]
Epoch [116/120    avg_loss:0.025, val_acc:0.964]
Epoch [117/120    avg_loss:0.021, val_acc:0.963]
Epoch [118/120    avg_loss:0.021, val_acc:0.961]
Epoch [119/120    avg_loss:0.023, val_acc:0.963]
Epoch [120/120    avg_loss:0.023, val_acc:0.959]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    3    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1252    3   11    0    1    0    0    1    6    7    0    0
     0    4    0]
 [   0    0    0  717    0    4    0    0    0   23    1    0    0    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0   23   63    0    0    0    0    0    0  777    6    0    0
     1    5    0]
 [   0    1   42    0    0    0    7    0    0    0   19 2129    8    3
     1    0    0]
 [   0    0    0   28    9    0    0    0    0    1    5    4  481    0
     0    2    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    3    0    0    0    4    0    0    2    0    1    0    0    0
  1128    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    93  254    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.57723577235772

F1 scores:
[       nan 0.91566265 0.96122841 0.92041078 0.95515695 0.98858447
 0.99242424 0.98039216 0.99767981 0.55737705 0.92280285 0.97705369
 0.93945312 0.98666667 0.95471858 0.82871126 0.97076023]

Kappa:
0.9496057795013053
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3f207c9860>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.480, val_acc:0.460]
Epoch [2/120    avg_loss:1.926, val_acc:0.569]
Epoch [3/120    avg_loss:1.677, val_acc:0.615]
Epoch [4/120    avg_loss:1.460, val_acc:0.683]
Epoch [5/120    avg_loss:1.209, val_acc:0.690]
Epoch [6/120    avg_loss:1.091, val_acc:0.715]
Epoch [7/120    avg_loss:0.945, val_acc:0.766]
Epoch [8/120    avg_loss:0.849, val_acc:0.780]
Epoch [9/120    avg_loss:0.737, val_acc:0.789]
Epoch [10/120    avg_loss:0.640, val_acc:0.781]
Epoch [11/120    avg_loss:0.622, val_acc:0.799]
Epoch [12/120    avg_loss:0.586, val_acc:0.832]
Epoch [13/120    avg_loss:0.534, val_acc:0.766]
Epoch [14/120    avg_loss:0.499, val_acc:0.839]
Epoch [15/120    avg_loss:0.472, val_acc:0.853]
Epoch [16/120    avg_loss:0.466, val_acc:0.887]
Epoch [17/120    avg_loss:0.361, val_acc:0.849]
Epoch [18/120    avg_loss:0.365, val_acc:0.843]
Epoch [19/120    avg_loss:0.347, val_acc:0.881]
Epoch [20/120    avg_loss:0.394, val_acc:0.866]
Epoch [21/120    avg_loss:0.443, val_acc:0.861]
Epoch [22/120    avg_loss:0.362, val_acc:0.889]
Epoch [23/120    avg_loss:0.310, val_acc:0.895]
Epoch [24/120    avg_loss:0.254, val_acc:0.887]
Epoch [25/120    avg_loss:0.283, val_acc:0.922]
Epoch [26/120    avg_loss:0.271, val_acc:0.869]
Epoch [27/120    avg_loss:0.275, val_acc:0.912]
Epoch [28/120    avg_loss:0.331, val_acc:0.876]
Epoch [29/120    avg_loss:0.259, val_acc:0.882]
Epoch [30/120    avg_loss:0.278, val_acc:0.911]
Epoch [31/120    avg_loss:0.209, val_acc:0.908]
Epoch [32/120    avg_loss:0.203, val_acc:0.889]
Epoch [33/120    avg_loss:0.235, val_acc:0.939]
Epoch [34/120    avg_loss:0.157, val_acc:0.919]
Epoch [35/120    avg_loss:0.206, val_acc:0.899]
Epoch [36/120    avg_loss:0.164, val_acc:0.930]
Epoch [37/120    avg_loss:0.156, val_acc:0.902]
Epoch [38/120    avg_loss:0.168, val_acc:0.942]
Epoch [39/120    avg_loss:0.175, val_acc:0.920]
Epoch [40/120    avg_loss:0.163, val_acc:0.926]
Epoch [41/120    avg_loss:0.132, val_acc:0.932]
Epoch [42/120    avg_loss:0.138, val_acc:0.924]
Epoch [43/120    avg_loss:0.114, val_acc:0.941]
Epoch [44/120    avg_loss:0.077, val_acc:0.941]
Epoch [45/120    avg_loss:0.114, val_acc:0.932]
Epoch [46/120    avg_loss:0.168, val_acc:0.906]
Epoch [47/120    avg_loss:0.198, val_acc:0.947]
Epoch [48/120    avg_loss:0.098, val_acc:0.964]
Epoch [49/120    avg_loss:0.114, val_acc:0.933]
Epoch [50/120    avg_loss:0.134, val_acc:0.941]
Epoch [51/120    avg_loss:0.119, val_acc:0.942]
Epoch [52/120    avg_loss:0.086, val_acc:0.948]
Epoch [53/120    avg_loss:0.153, val_acc:0.930]
Epoch [54/120    avg_loss:0.118, val_acc:0.914]
Epoch [55/120    avg_loss:0.113, val_acc:0.963]
Epoch [56/120    avg_loss:0.085, val_acc:0.949]
Epoch [57/120    avg_loss:0.082, val_acc:0.948]
Epoch [58/120    avg_loss:0.126, val_acc:0.950]
Epoch [59/120    avg_loss:0.111, val_acc:0.951]
Epoch [60/120    avg_loss:0.079, val_acc:0.944]
Epoch [61/120    avg_loss:0.080, val_acc:0.958]
Epoch [62/120    avg_loss:0.054, val_acc:0.960]
Epoch [63/120    avg_loss:0.048, val_acc:0.968]
Epoch [64/120    avg_loss:0.040, val_acc:0.967]
Epoch [65/120    avg_loss:0.043, val_acc:0.969]
Epoch [66/120    avg_loss:0.050, val_acc:0.972]
Epoch [67/120    avg_loss:0.039, val_acc:0.973]
Epoch [68/120    avg_loss:0.043, val_acc:0.974]
Epoch [69/120    avg_loss:0.037, val_acc:0.972]
Epoch [70/120    avg_loss:0.035, val_acc:0.974]
Epoch [71/120    avg_loss:0.034, val_acc:0.974]
Epoch [72/120    avg_loss:0.039, val_acc:0.970]
Epoch [73/120    avg_loss:0.035, val_acc:0.973]
Epoch [74/120    avg_loss:0.035, val_acc:0.973]
Epoch [75/120    avg_loss:0.050, val_acc:0.974]
Epoch [76/120    avg_loss:0.033, val_acc:0.974]
Epoch [77/120    avg_loss:0.035, val_acc:0.975]
Epoch [78/120    avg_loss:0.033, val_acc:0.973]
Epoch [79/120    avg_loss:0.031, val_acc:0.975]
Epoch [80/120    avg_loss:0.033, val_acc:0.973]
Epoch [81/120    avg_loss:0.035, val_acc:0.975]
Epoch [82/120    avg_loss:0.031, val_acc:0.973]
Epoch [83/120    avg_loss:0.028, val_acc:0.974]
Epoch [84/120    avg_loss:0.028, val_acc:0.974]
Epoch [85/120    avg_loss:0.034, val_acc:0.969]
Epoch [86/120    avg_loss:0.027, val_acc:0.972]
Epoch [87/120    avg_loss:0.026, val_acc:0.974]
Epoch [88/120    avg_loss:0.026, val_acc:0.975]
Epoch [89/120    avg_loss:0.032, val_acc:0.975]
Epoch [90/120    avg_loss:0.028, val_acc:0.972]
Epoch [91/120    avg_loss:0.038, val_acc:0.974]
Epoch [92/120    avg_loss:0.034, val_acc:0.974]
Epoch [93/120    avg_loss:0.024, val_acc:0.973]
Epoch [94/120    avg_loss:0.029, val_acc:0.974]
Epoch [95/120    avg_loss:0.038, val_acc:0.973]
Epoch [96/120    avg_loss:0.030, val_acc:0.975]
Epoch [97/120    avg_loss:0.033, val_acc:0.975]
Epoch [98/120    avg_loss:0.027, val_acc:0.978]
Epoch [99/120    avg_loss:0.030, val_acc:0.977]
Epoch [100/120    avg_loss:0.030, val_acc:0.978]
Epoch [101/120    avg_loss:0.025, val_acc:0.980]
Epoch [102/120    avg_loss:0.024, val_acc:0.976]
Epoch [103/120    avg_loss:0.024, val_acc:0.976]
Epoch [104/120    avg_loss:0.028, val_acc:0.976]
Epoch [105/120    avg_loss:0.026, val_acc:0.977]
Epoch [106/120    avg_loss:0.026, val_acc:0.976]
Epoch [107/120    avg_loss:0.027, val_acc:0.977]
Epoch [108/120    avg_loss:0.033, val_acc:0.976]
Epoch [109/120    avg_loss:0.036, val_acc:0.976]
Epoch [110/120    avg_loss:0.028, val_acc:0.974]
Epoch [111/120    avg_loss:0.028, val_acc:0.982]
Epoch [112/120    avg_loss:0.026, val_acc:0.976]
Epoch [113/120    avg_loss:0.030, val_acc:0.974]
Epoch [114/120    avg_loss:0.028, val_acc:0.975]
Epoch [115/120    avg_loss:0.027, val_acc:0.977]
Epoch [116/120    avg_loss:0.034, val_acc:0.967]
Epoch [117/120    avg_loss:0.030, val_acc:0.972]
Epoch [118/120    avg_loss:0.026, val_acc:0.974]
Epoch [119/120    avg_loss:0.031, val_acc:0.981]
Epoch [120/120    avg_loss:0.024, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    6 1240    1    0    0    0    0    0    0   14   24    0    0
     0    0    0]
 [   0    0    3  737    0    0    0    0    0    2    0    0    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    1    0    3    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   18    0    0    0    0    0    0  412    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5   90    0    5    0    0    0    0  771    1    0    0
     1    2    0]
 [   0    2    9    0    0    1    2    0    0    0    3 2191    0    2
     0    0    0]
 [   0    0    0   16    1    3    0    0    0    0   11    5  496    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    0    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0    0    0    0    6    0    0    0    0
   121  220    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.94579945799458

F1 scores:
[       nan 0.74766355 0.97560976 0.92646135 0.99765808 0.97931034
 0.99848024 0.98039216 0.97630332 0.76595745 0.92059701 0.98894155
 0.95752896 0.99462366 0.9463171  0.77328647 0.98224852]

Kappa:
0.9537329210995139
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe4c696c898>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.444, val_acc:0.486]
Epoch [2/120    avg_loss:1.970, val_acc:0.580]
Epoch [3/120    avg_loss:1.703, val_acc:0.598]
Epoch [4/120    avg_loss:1.457, val_acc:0.655]
Epoch [5/120    avg_loss:1.214, val_acc:0.693]
Epoch [6/120    avg_loss:1.000, val_acc:0.714]
Epoch [7/120    avg_loss:0.935, val_acc:0.722]
Epoch [8/120    avg_loss:0.788, val_acc:0.706]
Epoch [9/120    avg_loss:0.761, val_acc:0.741]
Epoch [10/120    avg_loss:0.687, val_acc:0.815]
Epoch [11/120    avg_loss:0.630, val_acc:0.815]
Epoch [12/120    avg_loss:0.553, val_acc:0.826]
Epoch [13/120    avg_loss:0.563, val_acc:0.842]
Epoch [14/120    avg_loss:0.501, val_acc:0.822]
Epoch [15/120    avg_loss:0.450, val_acc:0.845]
Epoch [16/120    avg_loss:0.522, val_acc:0.835]
Epoch [17/120    avg_loss:0.423, val_acc:0.863]
Epoch [18/120    avg_loss:0.359, val_acc:0.844]
Epoch [19/120    avg_loss:0.354, val_acc:0.872]
Epoch [20/120    avg_loss:0.324, val_acc:0.887]
Epoch [21/120    avg_loss:0.286, val_acc:0.880]
Epoch [22/120    avg_loss:0.350, val_acc:0.863]
Epoch [23/120    avg_loss:0.307, val_acc:0.863]
Epoch [24/120    avg_loss:0.264, val_acc:0.856]
Epoch [25/120    avg_loss:0.312, val_acc:0.839]
Epoch [26/120    avg_loss:0.276, val_acc:0.893]
Epoch [27/120    avg_loss:0.233, val_acc:0.910]
Epoch [28/120    avg_loss:0.261, val_acc:0.885]
Epoch [29/120    avg_loss:0.208, val_acc:0.905]
Epoch [30/120    avg_loss:0.226, val_acc:0.884]
Epoch [31/120    avg_loss:0.289, val_acc:0.900]
Epoch [32/120    avg_loss:0.183, val_acc:0.922]
Epoch [33/120    avg_loss:0.155, val_acc:0.926]
Epoch [34/120    avg_loss:0.259, val_acc:0.917]
Epoch [35/120    avg_loss:0.156, val_acc:0.912]
Epoch [36/120    avg_loss:0.165, val_acc:0.931]
Epoch [37/120    avg_loss:0.123, val_acc:0.933]
Epoch [38/120    avg_loss:0.137, val_acc:0.945]
Epoch [39/120    avg_loss:0.116, val_acc:0.945]
Epoch [40/120    avg_loss:0.126, val_acc:0.920]
Epoch [41/120    avg_loss:0.123, val_acc:0.945]
Epoch [42/120    avg_loss:0.123, val_acc:0.925]
Epoch [43/120    avg_loss:0.137, val_acc:0.907]
Epoch [44/120    avg_loss:0.126, val_acc:0.961]
Epoch [45/120    avg_loss:0.106, val_acc:0.944]
Epoch [46/120    avg_loss:0.102, val_acc:0.936]
Epoch [47/120    avg_loss:0.230, val_acc:0.865]
Epoch [48/120    avg_loss:0.186, val_acc:0.948]
Epoch [49/120    avg_loss:0.125, val_acc:0.940]
Epoch [50/120    avg_loss:0.092, val_acc:0.944]
Epoch [51/120    avg_loss:0.091, val_acc:0.944]
Epoch [52/120    avg_loss:0.119, val_acc:0.919]
Epoch [53/120    avg_loss:0.172, val_acc:0.915]
Epoch [54/120    avg_loss:0.115, val_acc:0.943]
Epoch [55/120    avg_loss:0.098, val_acc:0.943]
Epoch [56/120    avg_loss:0.081, val_acc:0.941]
Epoch [57/120    avg_loss:0.089, val_acc:0.944]
Epoch [58/120    avg_loss:0.068, val_acc:0.956]
Epoch [59/120    avg_loss:0.055, val_acc:0.959]
Epoch [60/120    avg_loss:0.047, val_acc:0.960]
Epoch [61/120    avg_loss:0.053, val_acc:0.961]
Epoch [62/120    avg_loss:0.048, val_acc:0.965]
Epoch [63/120    avg_loss:0.047, val_acc:0.964]
Epoch [64/120    avg_loss:0.047, val_acc:0.960]
Epoch [65/120    avg_loss:0.053, val_acc:0.959]
Epoch [66/120    avg_loss:0.044, val_acc:0.961]
Epoch [67/120    avg_loss:0.065, val_acc:0.965]
Epoch [68/120    avg_loss:0.048, val_acc:0.963]
Epoch [69/120    avg_loss:0.046, val_acc:0.965]
Epoch [70/120    avg_loss:0.043, val_acc:0.966]
Epoch [71/120    avg_loss:0.039, val_acc:0.968]
Epoch [72/120    avg_loss:0.043, val_acc:0.966]
Epoch [73/120    avg_loss:0.040, val_acc:0.969]
Epoch [74/120    avg_loss:0.040, val_acc:0.967]
Epoch [75/120    avg_loss:0.043, val_acc:0.970]
Epoch [76/120    avg_loss:0.044, val_acc:0.973]
Epoch [77/120    avg_loss:0.047, val_acc:0.970]
Epoch [78/120    avg_loss:0.041, val_acc:0.973]
Epoch [79/120    avg_loss:0.038, val_acc:0.973]
Epoch [80/120    avg_loss:0.041, val_acc:0.974]
Epoch [81/120    avg_loss:0.039, val_acc:0.973]
Epoch [82/120    avg_loss:0.039, val_acc:0.978]
Epoch [83/120    avg_loss:0.039, val_acc:0.980]
Epoch [84/120    avg_loss:0.042, val_acc:0.976]
Epoch [85/120    avg_loss:0.038, val_acc:0.974]
Epoch [86/120    avg_loss:0.044, val_acc:0.974]
Epoch [87/120    avg_loss:0.035, val_acc:0.975]
Epoch [88/120    avg_loss:0.034, val_acc:0.975]
Epoch [89/120    avg_loss:0.031, val_acc:0.970]
Epoch [90/120    avg_loss:0.041, val_acc:0.978]
Epoch [91/120    avg_loss:0.037, val_acc:0.977]
Epoch [92/120    avg_loss:0.039, val_acc:0.978]
Epoch [93/120    avg_loss:0.035, val_acc:0.975]
Epoch [94/120    avg_loss:0.036, val_acc:0.977]
Epoch [95/120    avg_loss:0.030, val_acc:0.980]
Epoch [96/120    avg_loss:0.035, val_acc:0.977]
Epoch [97/120    avg_loss:0.030, val_acc:0.977]
Epoch [98/120    avg_loss:0.027, val_acc:0.978]
Epoch [99/120    avg_loss:0.035, val_acc:0.981]
Epoch [100/120    avg_loss:0.033, val_acc:0.980]
Epoch [101/120    avg_loss:0.032, val_acc:0.977]
Epoch [102/120    avg_loss:0.027, val_acc:0.982]
Epoch [103/120    avg_loss:0.035, val_acc:0.981]
Epoch [104/120    avg_loss:0.038, val_acc:0.983]
Epoch [105/120    avg_loss:0.029, val_acc:0.982]
Epoch [106/120    avg_loss:0.029, val_acc:0.981]
Epoch [107/120    avg_loss:0.028, val_acc:0.980]
Epoch [108/120    avg_loss:0.033, val_acc:0.981]
Epoch [109/120    avg_loss:0.026, val_acc:0.977]
Epoch [110/120    avg_loss:0.029, val_acc:0.982]
Epoch [111/120    avg_loss:0.031, val_acc:0.980]
Epoch [112/120    avg_loss:0.033, val_acc:0.977]
Epoch [113/120    avg_loss:0.034, val_acc:0.978]
Epoch [114/120    avg_loss:0.036, val_acc:0.982]
Epoch [115/120    avg_loss:0.026, val_acc:0.984]
Epoch [116/120    avg_loss:0.028, val_acc:0.982]
Epoch [117/120    avg_loss:0.026, val_acc:0.980]
Epoch [118/120    avg_loss:0.034, val_acc:0.981]
Epoch [119/120    avg_loss:0.033, val_acc:0.982]
Epoch [120/120    avg_loss:0.026, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1254    0    5    0    1    0    0    1   15    5    4    0
     0    0    0]
 [   0    0    5  711    3    0    0    0    0    3    0    0   25    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    7    0    0    0    0    0    0  423    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0    9   89    0    4    0    0    0    0  770    2    0    0
     1    0    0]
 [   0    0   37    0    0    8    5    0    0    0   10 2112   34    4
     0    0    0]
 [   0    0    0   24    3    6    0    0    0    0    5    0  490    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1137    1    0]
 [   0    0    0    0    0    0   24    0    0    0    0    0    0    0
    76  247    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.32791327913279

F1 scores:
[       nan 0.87058824 0.96833977 0.90515595 0.97482838 0.97857948
 0.97767857 1.         0.99063232 0.81081081 0.91721263 0.97574498
 0.89908257 0.98930481 0.96601529 0.8302521  0.96551724]

Kappa:
0.9467965529409073
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe7db771860>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.461, val_acc:0.421]
Epoch [2/120    avg_loss:1.953, val_acc:0.561]
Epoch [3/120    avg_loss:1.716, val_acc:0.593]
Epoch [4/120    avg_loss:1.478, val_acc:0.631]
Epoch [5/120    avg_loss:1.300, val_acc:0.659]
Epoch [6/120    avg_loss:1.085, val_acc:0.700]
Epoch [7/120    avg_loss:0.953, val_acc:0.716]
Epoch [8/120    avg_loss:0.826, val_acc:0.761]
Epoch [9/120    avg_loss:0.724, val_acc:0.781]
Epoch [10/120    avg_loss:0.640, val_acc:0.796]
Epoch [11/120    avg_loss:0.597, val_acc:0.769]
Epoch [12/120    avg_loss:0.627, val_acc:0.790]
Epoch [13/120    avg_loss:0.551, val_acc:0.790]
Epoch [14/120    avg_loss:0.471, val_acc:0.857]
Epoch [15/120    avg_loss:0.453, val_acc:0.810]
Epoch [16/120    avg_loss:0.493, val_acc:0.820]
Epoch [17/120    avg_loss:0.404, val_acc:0.864]
Epoch [18/120    avg_loss:0.359, val_acc:0.856]
Epoch [19/120    avg_loss:0.400, val_acc:0.860]
Epoch [20/120    avg_loss:0.344, val_acc:0.869]
Epoch [21/120    avg_loss:0.304, val_acc:0.862]
Epoch [22/120    avg_loss:0.318, val_acc:0.872]
Epoch [23/120    avg_loss:0.296, val_acc:0.857]
Epoch [24/120    avg_loss:0.278, val_acc:0.893]
Epoch [25/120    avg_loss:0.269, val_acc:0.899]
Epoch [26/120    avg_loss:0.235, val_acc:0.903]
Epoch [27/120    avg_loss:0.241, val_acc:0.873]
Epoch [28/120    avg_loss:0.204, val_acc:0.902]
Epoch [29/120    avg_loss:0.251, val_acc:0.877]
Epoch [30/120    avg_loss:0.207, val_acc:0.909]
Epoch [31/120    avg_loss:0.254, val_acc:0.900]
Epoch [32/120    avg_loss:0.186, val_acc:0.899]
Epoch [33/120    avg_loss:0.202, val_acc:0.918]
Epoch [34/120    avg_loss:0.129, val_acc:0.929]
Epoch [35/120    avg_loss:0.164, val_acc:0.939]
Epoch [36/120    avg_loss:0.157, val_acc:0.905]
Epoch [37/120    avg_loss:0.139, val_acc:0.929]
Epoch [38/120    avg_loss:0.192, val_acc:0.896]
Epoch [39/120    avg_loss:0.228, val_acc:0.905]
Epoch [40/120    avg_loss:0.182, val_acc:0.931]
Epoch [41/120    avg_loss:0.145, val_acc:0.941]
Epoch [42/120    avg_loss:0.239, val_acc:0.925]
Epoch [43/120    avg_loss:0.139, val_acc:0.947]
Epoch [44/120    avg_loss:0.130, val_acc:0.934]
Epoch [45/120    avg_loss:0.132, val_acc:0.951]
Epoch [46/120    avg_loss:0.127, val_acc:0.925]
Epoch [47/120    avg_loss:0.100, val_acc:0.916]
Epoch [48/120    avg_loss:0.116, val_acc:0.952]
Epoch [49/120    avg_loss:0.121, val_acc:0.942]
Epoch [50/120    avg_loss:0.112, val_acc:0.934]
Epoch [51/120    avg_loss:0.120, val_acc:0.942]
Epoch [52/120    avg_loss:0.073, val_acc:0.946]
Epoch [53/120    avg_loss:0.092, val_acc:0.931]
Epoch [54/120    avg_loss:0.114, val_acc:0.954]
Epoch [55/120    avg_loss:0.194, val_acc:0.897]
Epoch [56/120    avg_loss:0.125, val_acc:0.956]
Epoch [57/120    avg_loss:0.086, val_acc:0.945]
Epoch [58/120    avg_loss:0.072, val_acc:0.962]
Epoch [59/120    avg_loss:0.087, val_acc:0.948]
Epoch [60/120    avg_loss:0.068, val_acc:0.963]
Epoch [61/120    avg_loss:0.089, val_acc:0.948]
Epoch [62/120    avg_loss:0.098, val_acc:0.927]
Epoch [63/120    avg_loss:0.076, val_acc:0.927]
Epoch [64/120    avg_loss:0.068, val_acc:0.957]
Epoch [65/120    avg_loss:0.056, val_acc:0.961]
Epoch [66/120    avg_loss:0.085, val_acc:0.954]
Epoch [67/120    avg_loss:0.051, val_acc:0.960]
Epoch [68/120    avg_loss:0.039, val_acc:0.967]
Epoch [69/120    avg_loss:0.044, val_acc:0.962]
Epoch [70/120    avg_loss:0.055, val_acc:0.961]
Epoch [71/120    avg_loss:0.079, val_acc:0.939]
Epoch [72/120    avg_loss:0.105, val_acc:0.958]
Epoch [73/120    avg_loss:0.045, val_acc:0.955]
Epoch [74/120    avg_loss:0.047, val_acc:0.954]
Epoch [75/120    avg_loss:0.042, val_acc:0.962]
Epoch [76/120    avg_loss:0.059, val_acc:0.968]
Epoch [77/120    avg_loss:0.053, val_acc:0.944]
Epoch [78/120    avg_loss:0.040, val_acc:0.961]
Epoch [79/120    avg_loss:0.043, val_acc:0.970]
Epoch [80/120    avg_loss:0.065, val_acc:0.956]
Epoch [81/120    avg_loss:0.057, val_acc:0.956]
Epoch [82/120    avg_loss:0.054, val_acc:0.970]
Epoch [83/120    avg_loss:0.069, val_acc:0.944]
Epoch [84/120    avg_loss:0.042, val_acc:0.970]
Epoch [85/120    avg_loss:0.099, val_acc:0.966]
Epoch [86/120    avg_loss:0.066, val_acc:0.954]
Epoch [87/120    avg_loss:0.044, val_acc:0.966]
Epoch [88/120    avg_loss:0.079, val_acc:0.948]
Epoch [89/120    avg_loss:0.053, val_acc:0.980]
Epoch [90/120    avg_loss:0.064, val_acc:0.956]
Epoch [91/120    avg_loss:0.041, val_acc:0.968]
Epoch [92/120    avg_loss:0.029, val_acc:0.970]
Epoch [93/120    avg_loss:0.035, val_acc:0.973]
Epoch [94/120    avg_loss:0.026, val_acc:0.979]
Epoch [95/120    avg_loss:0.023, val_acc:0.983]
Epoch [96/120    avg_loss:0.022, val_acc:0.977]
Epoch [97/120    avg_loss:0.017, val_acc:0.981]
Epoch [98/120    avg_loss:0.024, val_acc:0.976]
Epoch [99/120    avg_loss:0.026, val_acc:0.981]
Epoch [100/120    avg_loss:0.049, val_acc:0.982]
Epoch [101/120    avg_loss:0.037, val_acc:0.973]
Epoch [102/120    avg_loss:0.019, val_acc:0.977]
Epoch [103/120    avg_loss:0.016, val_acc:0.981]
Epoch [104/120    avg_loss:0.018, val_acc:0.969]
Epoch [105/120    avg_loss:0.028, val_acc:0.972]
Epoch [106/120    avg_loss:0.024, val_acc:0.982]
Epoch [107/120    avg_loss:0.019, val_acc:0.985]
Epoch [108/120    avg_loss:0.015, val_acc:0.978]
Epoch [109/120    avg_loss:0.018, val_acc:0.981]
Epoch [110/120    avg_loss:0.010, val_acc:0.985]
Epoch [111/120    avg_loss:0.015, val_acc:0.979]
Epoch [112/120    avg_loss:0.016, val_acc:0.971]
Epoch [113/120    avg_loss:0.012, val_acc:0.981]
Epoch [114/120    avg_loss:0.013, val_acc:0.985]
Epoch [115/120    avg_loss:0.015, val_acc:0.976]
Epoch [116/120    avg_loss:0.011, val_acc:0.979]
Epoch [117/120    avg_loss:0.013, val_acc:0.981]
Epoch [118/120    avg_loss:0.031, val_acc:0.973]
Epoch [119/120    avg_loss:0.024, val_acc:0.978]
Epoch [120/120    avg_loss:0.048, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    1 1217    3    4    5    3    0    0    0   11   41    0    0
     0    0    0]
 [   0    0    0  720    0   16    0    0    0    9    0    0    0    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  418    0    6    0    3    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0   32    0    5    0    0    0    0  816   15    0    0
     0    7    0]
 [   0    0    1    0    0    0   15    0    0    0    1 2172   10    1
    10    0    0]
 [   0    0    0   18   13   10    0    0    0    0   11    0  477    0
     0    5    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0  183
     0    0    0]
 [   0    0    0    0    0   17    0    0    1    0    1    1    0    0
  1119    0    0]
 [   0    0    0    0    0    0   42    0    0    4    0    0    0    0
    75  226    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.43631436314364

F1 scores:
[       nan 0.91764706 0.97204473 0.94736842 0.96162528 0.92070485
 0.95328467 0.89285714 0.99416569 0.69230769 0.95104895 0.97859878
 0.93255132 0.98652291 0.95031847 0.77264957 0.98795181]

Kappa:
0.9479496646194542
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f85d7353898>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.456, val_acc:0.520]
Epoch [2/120    avg_loss:2.002, val_acc:0.576]
Epoch [3/120    avg_loss:1.703, val_acc:0.607]
Epoch [4/120    avg_loss:1.449, val_acc:0.645]
Epoch [5/120    avg_loss:1.174, val_acc:0.696]
Epoch [6/120    avg_loss:1.023, val_acc:0.720]
Epoch [7/120    avg_loss:0.934, val_acc:0.759]
Epoch [8/120    avg_loss:0.819, val_acc:0.771]
Epoch [9/120    avg_loss:0.715, val_acc:0.831]
Epoch [10/120    avg_loss:0.658, val_acc:0.839]
Epoch [11/120    avg_loss:0.605, val_acc:0.847]
Epoch [12/120    avg_loss:0.540, val_acc:0.833]
Epoch [13/120    avg_loss:0.534, val_acc:0.854]
Epoch [14/120    avg_loss:0.475, val_acc:0.825]
Epoch [15/120    avg_loss:0.501, val_acc:0.852]
Epoch [16/120    avg_loss:0.415, val_acc:0.842]
Epoch [17/120    avg_loss:0.514, val_acc:0.838]
Epoch [18/120    avg_loss:0.418, val_acc:0.865]
Epoch [19/120    avg_loss:0.316, val_acc:0.900]
Epoch [20/120    avg_loss:0.362, val_acc:0.882]
Epoch [21/120    avg_loss:0.364, val_acc:0.897]
Epoch [22/120    avg_loss:0.331, val_acc:0.894]
Epoch [23/120    avg_loss:0.309, val_acc:0.911]
Epoch [24/120    avg_loss:0.329, val_acc:0.910]
Epoch [25/120    avg_loss:0.261, val_acc:0.903]
Epoch [26/120    avg_loss:0.253, val_acc:0.908]
Epoch [27/120    avg_loss:0.242, val_acc:0.875]
Epoch [28/120    avg_loss:0.227, val_acc:0.927]
Epoch [29/120    avg_loss:0.312, val_acc:0.914]
Epoch [30/120    avg_loss:0.195, val_acc:0.915]
Epoch [31/120    avg_loss:0.212, val_acc:0.894]
Epoch [32/120    avg_loss:0.226, val_acc:0.934]
Epoch [33/120    avg_loss:0.210, val_acc:0.917]
Epoch [34/120    avg_loss:0.213, val_acc:0.900]
Epoch [35/120    avg_loss:0.183, val_acc:0.931]
Epoch [36/120    avg_loss:0.252, val_acc:0.929]
Epoch [37/120    avg_loss:0.257, val_acc:0.931]
Epoch [38/120    avg_loss:0.231, val_acc:0.930]
Epoch [39/120    avg_loss:0.182, val_acc:0.940]
Epoch [40/120    avg_loss:0.125, val_acc:0.944]
Epoch [41/120    avg_loss:0.143, val_acc:0.931]
Epoch [42/120    avg_loss:0.131, val_acc:0.940]
Epoch [43/120    avg_loss:0.157, val_acc:0.921]
Epoch [44/120    avg_loss:0.145, val_acc:0.950]
Epoch [45/120    avg_loss:0.113, val_acc:0.946]
Epoch [46/120    avg_loss:0.109, val_acc:0.924]
Epoch [47/120    avg_loss:0.096, val_acc:0.942]
Epoch [48/120    avg_loss:0.151, val_acc:0.941]
Epoch [49/120    avg_loss:0.141, val_acc:0.932]
Epoch [50/120    avg_loss:0.140, val_acc:0.940]
Epoch [51/120    avg_loss:0.113, val_acc:0.916]
Epoch [52/120    avg_loss:0.108, val_acc:0.952]
Epoch [53/120    avg_loss:0.087, val_acc:0.927]
Epoch [54/120    avg_loss:0.075, val_acc:0.945]
Epoch [55/120    avg_loss:0.096, val_acc:0.943]
Epoch [56/120    avg_loss:0.077, val_acc:0.944]
Epoch [57/120    avg_loss:0.075, val_acc:0.953]
Epoch [58/120    avg_loss:0.072, val_acc:0.926]
Epoch [59/120    avg_loss:0.082, val_acc:0.953]
Epoch [60/120    avg_loss:0.067, val_acc:0.943]
Epoch [61/120    avg_loss:0.076, val_acc:0.950]
Epoch [62/120    avg_loss:0.067, val_acc:0.951]
Epoch [63/120    avg_loss:0.083, val_acc:0.950]
Epoch [64/120    avg_loss:0.079, val_acc:0.943]
Epoch [65/120    avg_loss:0.070, val_acc:0.912]
Epoch [66/120    avg_loss:0.050, val_acc:0.959]
Epoch [67/120    avg_loss:0.049, val_acc:0.960]
Epoch [68/120    avg_loss:0.057, val_acc:0.961]
Epoch [69/120    avg_loss:0.066, val_acc:0.940]
Epoch [70/120    avg_loss:0.058, val_acc:0.961]
Epoch [71/120    avg_loss:0.058, val_acc:0.959]
Epoch [72/120    avg_loss:0.056, val_acc:0.960]
Epoch [73/120    avg_loss:0.045, val_acc:0.962]
Epoch [74/120    avg_loss:0.046, val_acc:0.944]
Epoch [75/120    avg_loss:0.075, val_acc:0.962]
Epoch [76/120    avg_loss:0.046, val_acc:0.968]
Epoch [77/120    avg_loss:0.052, val_acc:0.963]
Epoch [78/120    avg_loss:0.063, val_acc:0.948]
Epoch [79/120    avg_loss:0.119, val_acc:0.955]
Epoch [80/120    avg_loss:0.086, val_acc:0.962]
Epoch [81/120    avg_loss:0.064, val_acc:0.959]
Epoch [82/120    avg_loss:0.052, val_acc:0.959]
Epoch [83/120    avg_loss:0.058, val_acc:0.969]
Epoch [84/120    avg_loss:0.056, val_acc:0.964]
Epoch [85/120    avg_loss:0.077, val_acc:0.962]
Epoch [86/120    avg_loss:0.077, val_acc:0.946]
Epoch [87/120    avg_loss:0.056, val_acc:0.959]
Epoch [88/120    avg_loss:0.049, val_acc:0.967]
Epoch [89/120    avg_loss:0.036, val_acc:0.958]
Epoch [90/120    avg_loss:0.041, val_acc:0.959]
Epoch [91/120    avg_loss:0.065, val_acc:0.929]
Epoch [92/120    avg_loss:0.070, val_acc:0.952]
Epoch [93/120    avg_loss:0.044, val_acc:0.974]
Epoch [94/120    avg_loss:0.037, val_acc:0.978]
Epoch [95/120    avg_loss:0.033, val_acc:0.980]
Epoch [96/120    avg_loss:0.030, val_acc:0.967]
Epoch [97/120    avg_loss:0.033, val_acc:0.971]
Epoch [98/120    avg_loss:0.066, val_acc:0.961]
Epoch [99/120    avg_loss:0.081, val_acc:0.951]
Epoch [100/120    avg_loss:0.050, val_acc:0.965]
Epoch [101/120    avg_loss:0.036, val_acc:0.969]
Epoch [102/120    avg_loss:0.035, val_acc:0.972]
Epoch [103/120    avg_loss:0.034, val_acc:0.973]
Epoch [104/120    avg_loss:0.022, val_acc:0.978]
Epoch [105/120    avg_loss:0.035, val_acc:0.969]
Epoch [106/120    avg_loss:0.025, val_acc:0.979]
Epoch [107/120    avg_loss:0.026, val_acc:0.969]
Epoch [108/120    avg_loss:0.031, val_acc:0.971]
Epoch [109/120    avg_loss:0.033, val_acc:0.984]
Epoch [110/120    avg_loss:0.020, val_acc:0.988]
Epoch [111/120    avg_loss:0.020, val_acc:0.982]
Epoch [112/120    avg_loss:0.013, val_acc:0.982]
Epoch [113/120    avg_loss:0.014, val_acc:0.982]
Epoch [114/120    avg_loss:0.011, val_acc:0.983]
Epoch [115/120    avg_loss:0.013, val_acc:0.982]
Epoch [116/120    avg_loss:0.015, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.981]
Epoch [118/120    avg_loss:0.011, val_acc:0.981]
Epoch [119/120    avg_loss:0.012, val_acc:0.981]
Epoch [120/120    avg_loss:0.013, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    6 1254    0    0    0    3    0    0    1    7   13    1    0
     0    0    0]
 [   0    0    0  739    1    1    0    0    0    3    0    0    2    0
     1    0    0]
 [   0    0    0    3  210    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    6    0    0    0  424    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7   76    0    4    0    0    0    0  776    2    6    0
     0    4    0]
 [   0    0    6    0    0    0    3    0    0    0   19 2169   11    2
     0    0    0]
 [   0    0    0   28    0    0    0    0    0    0    0    0  503    0
     2    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    1    0    0
  1135    0    0]
 [   0    0    0    0    0    0    9    0    0    1    0    0    0    0
   111  226    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.18428184281842

F1 scores:
[       nan 0.91954023 0.98275862 0.92780917 0.97674419 0.98734177
 0.98871332 0.98039216 0.99181287 0.85714286 0.92380952 0.98703072
 0.95085066 0.99462366 0.94899666 0.78336222 0.98809524]

Kappa:
0.9564815720927548
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcf206627f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.435, val_acc:0.460]
Epoch [2/120    avg_loss:1.940, val_acc:0.595]
Epoch [3/120    avg_loss:1.628, val_acc:0.638]
Epoch [4/120    avg_loss:1.405, val_acc:0.646]
Epoch [5/120    avg_loss:1.155, val_acc:0.694]
Epoch [6/120    avg_loss:1.035, val_acc:0.701]
Epoch [7/120    avg_loss:0.935, val_acc:0.714]
Epoch [8/120    avg_loss:0.812, val_acc:0.794]
Epoch [9/120    avg_loss:0.708, val_acc:0.814]
Epoch [10/120    avg_loss:0.663, val_acc:0.800]
Epoch [11/120    avg_loss:0.560, val_acc:0.829]
Epoch [12/120    avg_loss:0.586, val_acc:0.819]
Epoch [13/120    avg_loss:0.506, val_acc:0.834]
Epoch [14/120    avg_loss:0.519, val_acc:0.817]
Epoch [15/120    avg_loss:0.466, val_acc:0.801]
Epoch [16/120    avg_loss:0.484, val_acc:0.810]
Epoch [17/120    avg_loss:0.367, val_acc:0.885]
Epoch [18/120    avg_loss:0.384, val_acc:0.864]
Epoch [19/120    avg_loss:0.374, val_acc:0.868]
Epoch [20/120    avg_loss:0.332, val_acc:0.884]
Epoch [21/120    avg_loss:0.293, val_acc:0.858]
Epoch [22/120    avg_loss:0.301, val_acc:0.896]
Epoch [23/120    avg_loss:0.252, val_acc:0.883]
Epoch [24/120    avg_loss:0.313, val_acc:0.848]
Epoch [25/120    avg_loss:0.289, val_acc:0.907]
Epoch [26/120    avg_loss:0.227, val_acc:0.902]
Epoch [27/120    avg_loss:0.224, val_acc:0.896]
Epoch [28/120    avg_loss:0.274, val_acc:0.919]
Epoch [29/120    avg_loss:0.235, val_acc:0.907]
Epoch [30/120    avg_loss:0.184, val_acc:0.914]
Epoch [31/120    avg_loss:0.159, val_acc:0.919]
Epoch [32/120    avg_loss:0.178, val_acc:0.929]
Epoch [33/120    avg_loss:0.198, val_acc:0.915]
Epoch [34/120    avg_loss:0.171, val_acc:0.925]
Epoch [35/120    avg_loss:0.163, val_acc:0.916]
Epoch [36/120    avg_loss:0.169, val_acc:0.941]
Epoch [37/120    avg_loss:0.147, val_acc:0.931]
Epoch [38/120    avg_loss:0.180, val_acc:0.920]
Epoch [39/120    avg_loss:0.175, val_acc:0.907]
Epoch [40/120    avg_loss:0.185, val_acc:0.921]
Epoch [41/120    avg_loss:0.165, val_acc:0.913]
Epoch [42/120    avg_loss:0.110, val_acc:0.950]
Epoch [43/120    avg_loss:0.159, val_acc:0.919]
Epoch [44/120    avg_loss:0.143, val_acc:0.942]
Epoch [45/120    avg_loss:0.116, val_acc:0.921]
Epoch [46/120    avg_loss:0.207, val_acc:0.939]
Epoch [47/120    avg_loss:0.155, val_acc:0.941]
Epoch [48/120    avg_loss:0.126, val_acc:0.935]
Epoch [49/120    avg_loss:0.112, val_acc:0.930]
Epoch [50/120    avg_loss:0.153, val_acc:0.940]
Epoch [51/120    avg_loss:0.121, val_acc:0.940]
Epoch [52/120    avg_loss:0.094, val_acc:0.961]
Epoch [53/120    avg_loss:0.080, val_acc:0.960]
Epoch [54/120    avg_loss:0.168, val_acc:0.949]
Epoch [55/120    avg_loss:0.113, val_acc:0.953]
Epoch [56/120    avg_loss:0.090, val_acc:0.949]
Epoch [57/120    avg_loss:0.093, val_acc:0.943]
Epoch [58/120    avg_loss:0.094, val_acc:0.958]
Epoch [59/120    avg_loss:0.069, val_acc:0.962]
Epoch [60/120    avg_loss:0.074, val_acc:0.939]
Epoch [61/120    avg_loss:0.082, val_acc:0.967]
Epoch [62/120    avg_loss:0.071, val_acc:0.967]
Epoch [63/120    avg_loss:0.093, val_acc:0.945]
Epoch [64/120    avg_loss:0.106, val_acc:0.949]
Epoch [65/120    avg_loss:0.085, val_acc:0.940]
Epoch [66/120    avg_loss:0.082, val_acc:0.970]
Epoch [67/120    avg_loss:0.052, val_acc:0.968]
Epoch [68/120    avg_loss:0.057, val_acc:0.967]
Epoch [69/120    avg_loss:0.062, val_acc:0.921]
Epoch [70/120    avg_loss:0.059, val_acc:0.959]
Epoch [71/120    avg_loss:0.069, val_acc:0.955]
Epoch [72/120    avg_loss:0.053, val_acc:0.953]
Epoch [73/120    avg_loss:0.058, val_acc:0.965]
Epoch [74/120    avg_loss:0.049, val_acc:0.953]
Epoch [75/120    avg_loss:0.058, val_acc:0.965]
Epoch [76/120    avg_loss:0.056, val_acc:0.967]
Epoch [77/120    avg_loss:0.061, val_acc:0.961]
Epoch [78/120    avg_loss:0.078, val_acc:0.971]
Epoch [79/120    avg_loss:0.046, val_acc:0.980]
Epoch [80/120    avg_loss:0.031, val_acc:0.967]
Epoch [81/120    avg_loss:0.057, val_acc:0.973]
Epoch [82/120    avg_loss:0.037, val_acc:0.979]
Epoch [83/120    avg_loss:0.034, val_acc:0.978]
Epoch [84/120    avg_loss:0.045, val_acc:0.980]
Epoch [85/120    avg_loss:0.065, val_acc:0.950]
Epoch [86/120    avg_loss:0.047, val_acc:0.971]
Epoch [87/120    avg_loss:0.033, val_acc:0.970]
Epoch [88/120    avg_loss:0.031, val_acc:0.965]
Epoch [89/120    avg_loss:0.049, val_acc:0.965]
Epoch [90/120    avg_loss:0.051, val_acc:0.972]
Epoch [91/120    avg_loss:0.079, val_acc:0.960]
Epoch [92/120    avg_loss:0.066, val_acc:0.970]
Epoch [93/120    avg_loss:0.034, val_acc:0.974]
Epoch [94/120    avg_loss:0.029, val_acc:0.981]
Epoch [95/120    avg_loss:0.026, val_acc:0.979]
Epoch [96/120    avg_loss:0.028, val_acc:0.978]
Epoch [97/120    avg_loss:0.021, val_acc:0.977]
Epoch [98/120    avg_loss:0.030, val_acc:0.980]
Epoch [99/120    avg_loss:0.024, val_acc:0.984]
Epoch [100/120    avg_loss:0.031, val_acc:0.980]
Epoch [101/120    avg_loss:0.025, val_acc:0.975]
Epoch [102/120    avg_loss:0.028, val_acc:0.982]
Epoch [103/120    avg_loss:0.018, val_acc:0.984]
Epoch [104/120    avg_loss:0.016, val_acc:0.984]
Epoch [105/120    avg_loss:0.025, val_acc:0.941]
Epoch [106/120    avg_loss:0.030, val_acc:0.968]
Epoch [107/120    avg_loss:0.032, val_acc:0.969]
Epoch [108/120    avg_loss:0.039, val_acc:0.967]
Epoch [109/120    avg_loss:0.039, val_acc:0.975]
Epoch [110/120    avg_loss:0.024, val_acc:0.979]
Epoch [111/120    avg_loss:0.020, val_acc:0.991]
Epoch [112/120    avg_loss:0.015, val_acc:0.984]
Epoch [113/120    avg_loss:0.017, val_acc:0.987]
Epoch [114/120    avg_loss:0.041, val_acc:0.965]
Epoch [115/120    avg_loss:0.036, val_acc:0.977]
Epoch [116/120    avg_loss:0.038, val_acc:0.988]
Epoch [117/120    avg_loss:0.040, val_acc:0.962]
Epoch [118/120    avg_loss:0.091, val_acc:0.959]
Epoch [119/120    avg_loss:0.036, val_acc:0.977]
Epoch [120/120    avg_loss:0.069, val_acc:0.955]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    4    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1281    0    0    3    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    3  714    0   23    0    0    0    5    0    0    1    1
     0    0    0]
 [   0    0    0   18  195    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  414    0    8    0    2    1    0    0    0
    10    0    0]
 [   0    0    9    0    0    0  648    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    3    0    0   13    0    0    1    0
     0    0    0]
 [   0    0   67   77    0    0    0    0    0    0  723    5    0    0
     3    0    0]
 [   0    0  174    0    0    1    6    0    7    0   17 2000    0    5
     0    0    0]
 [   0    0    0   39    4    0    0    0    0    0   20    0  469    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    5    0    1    0    4    1    0    0
  1128    0    0]
 [   0    0    0    0    0    0   34    0    0    0    0    0    0    0
   144  169    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
92.23848238482385

F1 scores:
[       nan 0.94871795 0.90754516 0.89473684 0.94660194 0.94520548
 0.9578714  0.86206897 0.99078341 0.68421053 0.88170732 0.94854162
 0.92779426 0.98404255 0.93069307 0.65503876 0.95121951]

Kappa:
0.9115589912012906
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe0411e58d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.403, val_acc:0.476]
Epoch [2/120    avg_loss:1.946, val_acc:0.563]
Epoch [3/120    avg_loss:1.700, val_acc:0.618]
Epoch [4/120    avg_loss:1.490, val_acc:0.591]
Epoch [5/120    avg_loss:1.318, val_acc:0.667]
Epoch [6/120    avg_loss:1.060, val_acc:0.645]
Epoch [7/120    avg_loss:1.023, val_acc:0.661]
Epoch [8/120    avg_loss:0.973, val_acc:0.693]
Epoch [9/120    avg_loss:0.838, val_acc:0.770]
Epoch [10/120    avg_loss:0.736, val_acc:0.774]
Epoch [11/120    avg_loss:0.731, val_acc:0.767]
Epoch [12/120    avg_loss:0.620, val_acc:0.787]
Epoch [13/120    avg_loss:0.567, val_acc:0.783]
Epoch [14/120    avg_loss:0.531, val_acc:0.802]
Epoch [15/120    avg_loss:0.485, val_acc:0.815]
Epoch [16/120    avg_loss:0.503, val_acc:0.814]
Epoch [17/120    avg_loss:0.481, val_acc:0.821]
Epoch [18/120    avg_loss:0.504, val_acc:0.821]
Epoch [19/120    avg_loss:0.563, val_acc:0.818]
Epoch [20/120    avg_loss:0.390, val_acc:0.868]
Epoch [21/120    avg_loss:0.398, val_acc:0.880]
Epoch [22/120    avg_loss:0.343, val_acc:0.843]
Epoch [23/120    avg_loss:0.392, val_acc:0.863]
Epoch [24/120    avg_loss:0.336, val_acc:0.874]
Epoch [25/120    avg_loss:0.302, val_acc:0.887]
Epoch [26/120    avg_loss:0.302, val_acc:0.877]
Epoch [27/120    avg_loss:0.291, val_acc:0.896]
Epoch [28/120    avg_loss:0.271, val_acc:0.845]
Epoch [29/120    avg_loss:0.350, val_acc:0.857]
Epoch [30/120    avg_loss:0.293, val_acc:0.880]
Epoch [31/120    avg_loss:0.246, val_acc:0.901]
Epoch [32/120    avg_loss:0.317, val_acc:0.875]
Epoch [33/120    avg_loss:0.226, val_acc:0.926]
Epoch [34/120    avg_loss:0.195, val_acc:0.908]
Epoch [35/120    avg_loss:0.228, val_acc:0.892]
Epoch [36/120    avg_loss:0.192, val_acc:0.908]
Epoch [37/120    avg_loss:0.216, val_acc:0.916]
Epoch [38/120    avg_loss:0.184, val_acc:0.896]
Epoch [39/120    avg_loss:0.173, val_acc:0.926]
Epoch [40/120    avg_loss:0.178, val_acc:0.892]
Epoch [41/120    avg_loss:0.192, val_acc:0.910]
Epoch [42/120    avg_loss:0.204, val_acc:0.930]
Epoch [43/120    avg_loss:0.178, val_acc:0.919]
Epoch [44/120    avg_loss:0.243, val_acc:0.892]
Epoch [45/120    avg_loss:0.191, val_acc:0.885]
Epoch [46/120    avg_loss:0.174, val_acc:0.927]
Epoch [47/120    avg_loss:0.112, val_acc:0.950]
Epoch [48/120    avg_loss:0.122, val_acc:0.938]
Epoch [49/120    avg_loss:0.135, val_acc:0.932]
Epoch [50/120    avg_loss:0.112, val_acc:0.943]
Epoch [51/120    avg_loss:0.102, val_acc:0.935]
Epoch [52/120    avg_loss:0.108, val_acc:0.944]
Epoch [53/120    avg_loss:0.110, val_acc:0.946]
Epoch [54/120    avg_loss:0.109, val_acc:0.941]
Epoch [55/120    avg_loss:0.147, val_acc:0.921]
Epoch [56/120    avg_loss:0.156, val_acc:0.895]
Epoch [57/120    avg_loss:0.137, val_acc:0.916]
Epoch [58/120    avg_loss:0.110, val_acc:0.925]
Epoch [59/120    avg_loss:0.131, val_acc:0.944]
Epoch [60/120    avg_loss:0.112, val_acc:0.932]
Epoch [61/120    avg_loss:0.084, val_acc:0.949]
Epoch [62/120    avg_loss:0.064, val_acc:0.950]
Epoch [63/120    avg_loss:0.062, val_acc:0.955]
Epoch [64/120    avg_loss:0.052, val_acc:0.955]
Epoch [65/120    avg_loss:0.056, val_acc:0.955]
Epoch [66/120    avg_loss:0.054, val_acc:0.961]
Epoch [67/120    avg_loss:0.057, val_acc:0.961]
Epoch [68/120    avg_loss:0.061, val_acc:0.962]
Epoch [69/120    avg_loss:0.052, val_acc:0.961]
Epoch [70/120    avg_loss:0.057, val_acc:0.961]
Epoch [71/120    avg_loss:0.053, val_acc:0.964]
Epoch [72/120    avg_loss:0.048, val_acc:0.965]
Epoch [73/120    avg_loss:0.046, val_acc:0.971]
Epoch [74/120    avg_loss:0.047, val_acc:0.965]
Epoch [75/120    avg_loss:0.054, val_acc:0.966]
Epoch [76/120    avg_loss:0.080, val_acc:0.962]
Epoch [77/120    avg_loss:0.042, val_acc:0.962]
Epoch [78/120    avg_loss:0.040, val_acc:0.967]
Epoch [79/120    avg_loss:0.044, val_acc:0.968]
Epoch [80/120    avg_loss:0.044, val_acc:0.961]
Epoch [81/120    avg_loss:0.048, val_acc:0.970]
Epoch [82/120    avg_loss:0.046, val_acc:0.970]
Epoch [83/120    avg_loss:0.056, val_acc:0.966]
Epoch [84/120    avg_loss:0.042, val_acc:0.968]
Epoch [85/120    avg_loss:0.043, val_acc:0.970]
Epoch [86/120    avg_loss:0.039, val_acc:0.965]
Epoch [87/120    avg_loss:0.040, val_acc:0.965]
Epoch [88/120    avg_loss:0.040, val_acc:0.966]
Epoch [89/120    avg_loss:0.041, val_acc:0.965]
Epoch [90/120    avg_loss:0.043, val_acc:0.966]
Epoch [91/120    avg_loss:0.044, val_acc:0.967]
Epoch [92/120    avg_loss:0.036, val_acc:0.968]
Epoch [93/120    avg_loss:0.040, val_acc:0.968]
Epoch [94/120    avg_loss:0.037, val_acc:0.970]
Epoch [95/120    avg_loss:0.040, val_acc:0.970]
Epoch [96/120    avg_loss:0.042, val_acc:0.970]
Epoch [97/120    avg_loss:0.047, val_acc:0.970]
Epoch [98/120    avg_loss:0.041, val_acc:0.970]
Epoch [99/120    avg_loss:0.038, val_acc:0.971]
Epoch [100/120    avg_loss:0.037, val_acc:0.971]
Epoch [101/120    avg_loss:0.043, val_acc:0.971]
Epoch [102/120    avg_loss:0.039, val_acc:0.971]
Epoch [103/120    avg_loss:0.037, val_acc:0.970]
Epoch [104/120    avg_loss:0.046, val_acc:0.966]
Epoch [105/120    avg_loss:0.045, val_acc:0.966]
Epoch [106/120    avg_loss:0.044, val_acc:0.966]
Epoch [107/120    avg_loss:0.035, val_acc:0.968]
Epoch [108/120    avg_loss:0.043, val_acc:0.968]
Epoch [109/120    avg_loss:0.040, val_acc:0.967]
Epoch [110/120    avg_loss:0.041, val_acc:0.968]
Epoch [111/120    avg_loss:0.039, val_acc:0.967]
Epoch [112/120    avg_loss:0.042, val_acc:0.967]
Epoch [113/120    avg_loss:0.039, val_acc:0.967]
Epoch [114/120    avg_loss:0.035, val_acc:0.967]
Epoch [115/120    avg_loss:0.036, val_acc:0.968]
Epoch [116/120    avg_loss:0.037, val_acc:0.968]
Epoch [117/120    avg_loss:0.042, val_acc:0.968]
Epoch [118/120    avg_loss:0.040, val_acc:0.968]
Epoch [119/120    avg_loss:0.036, val_acc:0.968]
Epoch [120/120    avg_loss:0.038, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    5 1228    8    0    0    5    0    0    5    8   26    0    0
     0    0    0]
 [   0    0    2  712    1    2    0    0    0   16    0    0   14    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    6    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    2    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10   79    0    9   11    0    0    0  758    4    0    0
     0    4    0]
 [   0    2   18    0    0    1   17    0    0    0   21 2133   14    4
     0    0    0]
 [   0    0    0   37    8   12    0    0    0    1    3    0  468    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    3    0    0    0    2    0    1    0    2    0    0    0
  1131    0    0]
 [   0    0    0    0    0    0   37    0    0    4    0    0    0    0
   109  197    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.26558265582656

F1 scores:
[       nan 0.88372093 0.96465043 0.8995578  0.97931034 0.96388262
 0.94271211 0.89285714 0.99883856 0.5625     0.90778443 0.97508571
 0.90785645 0.98930481 0.9488255  0.7189781  0.96511628]

Kappa:
0.934648339494459
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f91e5c6c7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.456, val_acc:0.508]
Epoch [2/120    avg_loss:1.941, val_acc:0.537]
Epoch [3/120    avg_loss:1.700, val_acc:0.648]
Epoch [4/120    avg_loss:1.403, val_acc:0.679]
Epoch [5/120    avg_loss:1.194, val_acc:0.724]
Epoch [6/120    avg_loss:1.045, val_acc:0.747]
Epoch [7/120    avg_loss:0.991, val_acc:0.748]
Epoch [8/120    avg_loss:0.857, val_acc:0.777]
Epoch [9/120    avg_loss:0.789, val_acc:0.765]
Epoch [10/120    avg_loss:0.697, val_acc:0.809]
Epoch [11/120    avg_loss:0.620, val_acc:0.797]
Epoch [12/120    avg_loss:0.648, val_acc:0.780]
Epoch [13/120    avg_loss:0.565, val_acc:0.800]
Epoch [14/120    avg_loss:0.485, val_acc:0.834]
Epoch [15/120    avg_loss:0.594, val_acc:0.812]
Epoch [16/120    avg_loss:0.484, val_acc:0.840]
Epoch [17/120    avg_loss:0.448, val_acc:0.831]
Epoch [18/120    avg_loss:0.472, val_acc:0.822]
Epoch [19/120    avg_loss:0.421, val_acc:0.873]
Epoch [20/120    avg_loss:0.368, val_acc:0.864]
Epoch [21/120    avg_loss:0.342, val_acc:0.860]
Epoch [22/120    avg_loss:0.323, val_acc:0.865]
Epoch [23/120    avg_loss:0.310, val_acc:0.882]
Epoch [24/120    avg_loss:0.279, val_acc:0.877]
Epoch [25/120    avg_loss:0.259, val_acc:0.884]
Epoch [26/120    avg_loss:0.351, val_acc:0.876]
Epoch [27/120    avg_loss:0.297, val_acc:0.894]
Epoch [28/120    avg_loss:0.337, val_acc:0.859]
Epoch [29/120    avg_loss:0.366, val_acc:0.858]
Epoch [30/120    avg_loss:0.361, val_acc:0.891]
Epoch [31/120    avg_loss:0.262, val_acc:0.903]
Epoch [32/120    avg_loss:0.234, val_acc:0.884]
Epoch [33/120    avg_loss:0.299, val_acc:0.881]
Epoch [34/120    avg_loss:0.213, val_acc:0.885]
Epoch [35/120    avg_loss:0.207, val_acc:0.917]
Epoch [36/120    avg_loss:0.191, val_acc:0.916]
Epoch [37/120    avg_loss:0.199, val_acc:0.889]
Epoch [38/120    avg_loss:0.192, val_acc:0.912]
Epoch [39/120    avg_loss:0.167, val_acc:0.909]
Epoch [40/120    avg_loss:0.165, val_acc:0.917]
Epoch [41/120    avg_loss:0.243, val_acc:0.896]
Epoch [42/120    avg_loss:0.205, val_acc:0.927]
Epoch [43/120    avg_loss:0.161, val_acc:0.916]
Epoch [44/120    avg_loss:0.153, val_acc:0.873]
Epoch [45/120    avg_loss:0.184, val_acc:0.921]
Epoch [46/120    avg_loss:0.212, val_acc:0.918]
Epoch [47/120    avg_loss:0.160, val_acc:0.925]
Epoch [48/120    avg_loss:0.110, val_acc:0.946]
Epoch [49/120    avg_loss:0.114, val_acc:0.932]
Epoch [50/120    avg_loss:0.137, val_acc:0.927]
Epoch [51/120    avg_loss:0.120, val_acc:0.937]
Epoch [52/120    avg_loss:0.113, val_acc:0.931]
Epoch [53/120    avg_loss:0.108, val_acc:0.923]
Epoch [54/120    avg_loss:0.191, val_acc:0.921]
Epoch [55/120    avg_loss:0.117, val_acc:0.937]
Epoch [56/120    avg_loss:0.090, val_acc:0.930]
Epoch [57/120    avg_loss:0.101, val_acc:0.945]
Epoch [58/120    avg_loss:0.112, val_acc:0.938]
Epoch [59/120    avg_loss:0.075, val_acc:0.944]
Epoch [60/120    avg_loss:0.064, val_acc:0.947]
Epoch [61/120    avg_loss:0.055, val_acc:0.945]
Epoch [62/120    avg_loss:0.075, val_acc:0.941]
Epoch [63/120    avg_loss:0.107, val_acc:0.930]
Epoch [64/120    avg_loss:0.234, val_acc:0.860]
Epoch [65/120    avg_loss:0.306, val_acc:0.907]
Epoch [66/120    avg_loss:0.126, val_acc:0.925]
Epoch [67/120    avg_loss:0.113, val_acc:0.935]
Epoch [68/120    avg_loss:0.080, val_acc:0.935]
Epoch [69/120    avg_loss:0.087, val_acc:0.955]
Epoch [70/120    avg_loss:0.071, val_acc:0.944]
Epoch [71/120    avg_loss:0.070, val_acc:0.953]
Epoch [72/120    avg_loss:0.056, val_acc:0.957]
Epoch [73/120    avg_loss:0.083, val_acc:0.941]
Epoch [74/120    avg_loss:0.067, val_acc:0.948]
Epoch [75/120    avg_loss:0.137, val_acc:0.946]
Epoch [76/120    avg_loss:0.061, val_acc:0.941]
Epoch [77/120    avg_loss:0.067, val_acc:0.946]
Epoch [78/120    avg_loss:0.085, val_acc:0.932]
Epoch [79/120    avg_loss:0.090, val_acc:0.946]
Epoch [80/120    avg_loss:0.076, val_acc:0.946]
Epoch [81/120    avg_loss:0.052, val_acc:0.956]
Epoch [82/120    avg_loss:0.062, val_acc:0.947]
Epoch [83/120    avg_loss:0.055, val_acc:0.937]
Epoch [84/120    avg_loss:0.062, val_acc:0.922]
Epoch [85/120    avg_loss:0.046, val_acc:0.957]
Epoch [86/120    avg_loss:0.041, val_acc:0.957]
Epoch [87/120    avg_loss:0.088, val_acc:0.946]
Epoch [88/120    avg_loss:0.073, val_acc:0.929]
Epoch [89/120    avg_loss:0.054, val_acc:0.948]
Epoch [90/120    avg_loss:0.052, val_acc:0.954]
Epoch [91/120    avg_loss:0.050, val_acc:0.950]
Epoch [92/120    avg_loss:0.057, val_acc:0.938]
Epoch [93/120    avg_loss:0.063, val_acc:0.959]
Epoch [94/120    avg_loss:0.044, val_acc:0.963]
Epoch [95/120    avg_loss:0.031, val_acc:0.966]
Epoch [96/120    avg_loss:0.038, val_acc:0.972]
Epoch [97/120    avg_loss:0.040, val_acc:0.964]
Epoch [98/120    avg_loss:0.038, val_acc:0.962]
Epoch [99/120    avg_loss:0.028, val_acc:0.964]
Epoch [100/120    avg_loss:0.026, val_acc:0.963]
Epoch [101/120    avg_loss:0.026, val_acc:0.959]
Epoch [102/120    avg_loss:0.029, val_acc:0.965]
Epoch [103/120    avg_loss:0.047, val_acc:0.959]
Epoch [104/120    avg_loss:0.046, val_acc:0.963]
Epoch [105/120    avg_loss:0.036, val_acc:0.957]
Epoch [106/120    avg_loss:0.032, val_acc:0.962]
Epoch [107/120    avg_loss:0.035, val_acc:0.962]
Epoch [108/120    avg_loss:0.045, val_acc:0.967]
Epoch [109/120    avg_loss:0.027, val_acc:0.964]
Epoch [110/120    avg_loss:0.024, val_acc:0.967]
Epoch [111/120    avg_loss:0.020, val_acc:0.972]
Epoch [112/120    avg_loss:0.023, val_acc:0.971]
Epoch [113/120    avg_loss:0.020, val_acc:0.970]
Epoch [114/120    avg_loss:0.020, val_acc:0.970]
Epoch [115/120    avg_loss:0.016, val_acc:0.970]
Epoch [116/120    avg_loss:0.015, val_acc:0.971]
Epoch [117/120    avg_loss:0.014, val_acc:0.970]
Epoch [118/120    avg_loss:0.015, val_acc:0.971]
Epoch [119/120    avg_loss:0.015, val_acc:0.971]
Epoch [120/120    avg_loss:0.022, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1256    2    7    0    0    0    0    1    3   16    0    0
     0    0    0]
 [   0    0    3  711    0    0    0    0    0   10    0    1   22    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  413    0    0    0    1    0    0    0    0
    21    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   10   84    0    1    1    0    0    0  765    7    0    0
     0    7    0]
 [   0    0    6    0    0    0    3    0    5    0   24 2169    1    2
     0    0    0]
 [   0    0    1   36    0    7    0    0    0    1    8    0  478    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    3    0    3    1    0    0
  1132    0    0]
 [   0    0    0    0    0    1    1    0    0    0    0    0    0    0
    50  295    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.05420054200542

F1 scores:
[       nan 0.93506494 0.98086685 0.8988622  0.97911833 0.9638273
 0.99544765 1.         0.99078341 0.70833333 0.90909091 0.98479001
 0.92100193 0.99462366 0.96669513 0.90909091 0.9704142 ]

Kappa:
0.9550109283290419
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efd78a09898>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.457, val_acc:0.532]
Epoch [2/120    avg_loss:1.921, val_acc:0.567]
Epoch [3/120    avg_loss:1.677, val_acc:0.619]
Epoch [4/120    avg_loss:1.490, val_acc:0.639]
Epoch [5/120    avg_loss:1.297, val_acc:0.681]
Epoch [6/120    avg_loss:1.128, val_acc:0.709]
Epoch [7/120    avg_loss:0.952, val_acc:0.753]
Epoch [8/120    avg_loss:0.870, val_acc:0.720]
Epoch [9/120    avg_loss:0.751, val_acc:0.789]
Epoch [10/120    avg_loss:0.695, val_acc:0.810]
Epoch [11/120    avg_loss:0.585, val_acc:0.793]
Epoch [12/120    avg_loss:0.701, val_acc:0.824]
Epoch [13/120    avg_loss:0.578, val_acc:0.810]
Epoch [14/120    avg_loss:0.467, val_acc:0.791]
Epoch [15/120    avg_loss:0.469, val_acc:0.841]
Epoch [16/120    avg_loss:0.520, val_acc:0.868]
Epoch [17/120    avg_loss:0.390, val_acc:0.793]
Epoch [18/120    avg_loss:0.368, val_acc:0.864]
Epoch [19/120    avg_loss:0.358, val_acc:0.858]
Epoch [20/120    avg_loss:0.471, val_acc:0.850]
Epoch [21/120    avg_loss:0.346, val_acc:0.869]
Epoch [22/120    avg_loss:0.328, val_acc:0.869]
Epoch [23/120    avg_loss:0.309, val_acc:0.893]
Epoch [24/120    avg_loss:0.264, val_acc:0.895]
Epoch [25/120    avg_loss:0.238, val_acc:0.860]
Epoch [26/120    avg_loss:0.269, val_acc:0.889]
Epoch [27/120    avg_loss:0.271, val_acc:0.905]
Epoch [28/120    avg_loss:0.308, val_acc:0.886]
Epoch [29/120    avg_loss:0.241, val_acc:0.909]
Epoch [30/120    avg_loss:0.194, val_acc:0.923]
Epoch [31/120    avg_loss:0.177, val_acc:0.915]
Epoch [32/120    avg_loss:0.211, val_acc:0.888]
Epoch [33/120    avg_loss:0.225, val_acc:0.900]
Epoch [34/120    avg_loss:0.232, val_acc:0.937]
Epoch [35/120    avg_loss:0.217, val_acc:0.904]
Epoch [36/120    avg_loss:0.197, val_acc:0.933]
Epoch [37/120    avg_loss:0.157, val_acc:0.919]
Epoch [38/120    avg_loss:0.135, val_acc:0.915]
Epoch [39/120    avg_loss:0.148, val_acc:0.934]
Epoch [40/120    avg_loss:0.122, val_acc:0.928]
Epoch [41/120    avg_loss:0.146, val_acc:0.934]
Epoch [42/120    avg_loss:0.118, val_acc:0.940]
Epoch [43/120    avg_loss:0.121, val_acc:0.928]
Epoch [44/120    avg_loss:0.124, val_acc:0.919]
Epoch [45/120    avg_loss:0.120, val_acc:0.952]
Epoch [46/120    avg_loss:0.132, val_acc:0.933]
Epoch [47/120    avg_loss:0.123, val_acc:0.936]
Epoch [48/120    avg_loss:0.103, val_acc:0.942]
Epoch [49/120    avg_loss:0.080, val_acc:0.948]
Epoch [50/120    avg_loss:0.079, val_acc:0.942]
Epoch [51/120    avg_loss:0.114, val_acc:0.954]
Epoch [52/120    avg_loss:0.074, val_acc:0.923]
Epoch [53/120    avg_loss:0.109, val_acc:0.950]
Epoch [54/120    avg_loss:0.088, val_acc:0.945]
Epoch [55/120    avg_loss:0.102, val_acc:0.954]
Epoch [56/120    avg_loss:0.060, val_acc:0.944]
Epoch [57/120    avg_loss:0.078, val_acc:0.950]
Epoch [58/120    avg_loss:0.042, val_acc:0.947]
Epoch [59/120    avg_loss:0.046, val_acc:0.945]
Epoch [60/120    avg_loss:0.071, val_acc:0.955]
Epoch [61/120    avg_loss:0.092, val_acc:0.936]
Epoch [62/120    avg_loss:0.074, val_acc:0.931]
Epoch [63/120    avg_loss:0.060, val_acc:0.927]
Epoch [64/120    avg_loss:0.058, val_acc:0.938]
Epoch [65/120    avg_loss:0.068, val_acc:0.956]
Epoch [66/120    avg_loss:0.055, val_acc:0.939]
Epoch [67/120    avg_loss:0.052, val_acc:0.939]
Epoch [68/120    avg_loss:0.088, val_acc:0.960]
Epoch [69/120    avg_loss:0.069, val_acc:0.931]
Epoch [70/120    avg_loss:0.052, val_acc:0.956]
Epoch [71/120    avg_loss:0.046, val_acc:0.966]
Epoch [72/120    avg_loss:0.049, val_acc:0.919]
Epoch [73/120    avg_loss:0.069, val_acc:0.946]
Epoch [74/120    avg_loss:0.116, val_acc:0.960]
Epoch [75/120    avg_loss:0.048, val_acc:0.962]
Epoch [76/120    avg_loss:0.079, val_acc:0.935]
Epoch [77/120    avg_loss:0.059, val_acc:0.963]
Epoch [78/120    avg_loss:0.044, val_acc:0.955]
Epoch [79/120    avg_loss:0.057, val_acc:0.936]
Epoch [80/120    avg_loss:0.066, val_acc:0.937]
Epoch [81/120    avg_loss:0.091, val_acc:0.966]
Epoch [82/120    avg_loss:0.038, val_acc:0.970]
Epoch [83/120    avg_loss:0.038, val_acc:0.968]
Epoch [84/120    avg_loss:0.045, val_acc:0.965]
Epoch [85/120    avg_loss:0.061, val_acc:0.952]
Epoch [86/120    avg_loss:0.043, val_acc:0.965]
Epoch [87/120    avg_loss:0.189, val_acc:0.867]
Epoch [88/120    avg_loss:0.242, val_acc:0.913]
Epoch [89/120    avg_loss:0.139, val_acc:0.923]
Epoch [90/120    avg_loss:0.068, val_acc:0.958]
Epoch [91/120    avg_loss:0.036, val_acc:0.955]
Epoch [92/120    avg_loss:0.056, val_acc:0.965]
Epoch [93/120    avg_loss:0.035, val_acc:0.972]
Epoch [94/120    avg_loss:0.038, val_acc:0.963]
Epoch [95/120    avg_loss:0.024, val_acc:0.962]
Epoch [96/120    avg_loss:0.028, val_acc:0.979]
Epoch [97/120    avg_loss:0.030, val_acc:0.968]
Epoch [98/120    avg_loss:0.067, val_acc:0.951]
Epoch [99/120    avg_loss:0.039, val_acc:0.970]
Epoch [100/120    avg_loss:0.036, val_acc:0.958]
Epoch [101/120    avg_loss:0.041, val_acc:0.967]
Epoch [102/120    avg_loss:0.037, val_acc:0.962]
Epoch [103/120    avg_loss:0.035, val_acc:0.969]
Epoch [104/120    avg_loss:0.022, val_acc:0.960]
Epoch [105/120    avg_loss:0.029, val_acc:0.963]
Epoch [106/120    avg_loss:0.023, val_acc:0.967]
Epoch [107/120    avg_loss:0.021, val_acc:0.976]
Epoch [108/120    avg_loss:0.020, val_acc:0.969]
Epoch [109/120    avg_loss:0.021, val_acc:0.966]
Epoch [110/120    avg_loss:0.016, val_acc:0.969]
Epoch [111/120    avg_loss:0.015, val_acc:0.969]
Epoch [112/120    avg_loss:0.012, val_acc:0.972]
Epoch [113/120    avg_loss:0.010, val_acc:0.971]
Epoch [114/120    avg_loss:0.012, val_acc:0.970]
Epoch [115/120    avg_loss:0.009, val_acc:0.972]
Epoch [116/120    avg_loss:0.012, val_acc:0.972]
Epoch [117/120    avg_loss:0.013, val_acc:0.973]
Epoch [118/120    avg_loss:0.011, val_acc:0.975]
Epoch [119/120    avg_loss:0.010, val_acc:0.975]
Epoch [120/120    avg_loss:0.015, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    5 1253    0    0    0    2    0    0    0    6    4   14    0
     0    1    0]
 [   0    0    3  727    0   12    0    0    0    4    0    0    1    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9   91    0    4    2    0    0    0  768    1    0    0
     0    0    0]
 [   0    0    5    4    0    1    4    0    2    0    5 2154   29    3
     3    0    0]
 [   0    0    0   21    3    8    0    0    0    0    3    0  490    0
     0    0    9]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    3    0    1    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0    1    0    0    3    0    0    0    0
   140  203    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.5230352303523

F1 scores:
[       nan 0.91764706 0.98082192 0.91389063 0.99065421 0.9698324
 0.9924357  1.         0.99421965 0.81818182 0.9253012  0.98581236
 0.917603   0.98924731 0.9391808  0.73684211 0.94915254]

Kappa:
0.9489635047314575
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd593fef860>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.435, val_acc:0.532]
Epoch [2/120    avg_loss:1.942, val_acc:0.600]
Epoch [3/120    avg_loss:1.665, val_acc:0.634]
Epoch [4/120    avg_loss:1.467, val_acc:0.642]
Epoch [5/120    avg_loss:1.301, val_acc:0.677]
Epoch [6/120    avg_loss:1.144, val_acc:0.721]
Epoch [7/120    avg_loss:1.014, val_acc:0.759]
Epoch [8/120    avg_loss:0.816, val_acc:0.758]
Epoch [9/120    avg_loss:0.786, val_acc:0.804]
Epoch [10/120    avg_loss:0.722, val_acc:0.781]
Epoch [11/120    avg_loss:0.630, val_acc:0.811]
Epoch [12/120    avg_loss:0.545, val_acc:0.818]
Epoch [13/120    avg_loss:0.498, val_acc:0.796]
Epoch [14/120    avg_loss:0.524, val_acc:0.811]
Epoch [15/120    avg_loss:0.483, val_acc:0.816]
Epoch [16/120    avg_loss:0.444, val_acc:0.757]
Epoch [17/120    avg_loss:0.438, val_acc:0.835]
Epoch [18/120    avg_loss:0.412, val_acc:0.816]
Epoch [19/120    avg_loss:0.415, val_acc:0.860]
Epoch [20/120    avg_loss:0.361, val_acc:0.866]
Epoch [21/120    avg_loss:0.317, val_acc:0.876]
Epoch [22/120    avg_loss:0.284, val_acc:0.873]
Epoch [23/120    avg_loss:0.301, val_acc:0.891]
Epoch [24/120    avg_loss:0.295, val_acc:0.873]
Epoch [25/120    avg_loss:0.289, val_acc:0.898]
Epoch [26/120    avg_loss:0.243, val_acc:0.849]
Epoch [27/120    avg_loss:0.245, val_acc:0.885]
Epoch [28/120    avg_loss:0.228, val_acc:0.887]
Epoch [29/120    avg_loss:0.265, val_acc:0.905]
Epoch [30/120    avg_loss:0.226, val_acc:0.865]
Epoch [31/120    avg_loss:0.274, val_acc:0.910]
Epoch [32/120    avg_loss:0.198, val_acc:0.887]
Epoch [33/120    avg_loss:0.206, val_acc:0.911]
Epoch [34/120    avg_loss:0.172, val_acc:0.907]
Epoch [35/120    avg_loss:0.163, val_acc:0.905]
Epoch [36/120    avg_loss:0.150, val_acc:0.933]
Epoch [37/120    avg_loss:0.137, val_acc:0.927]
Epoch [38/120    avg_loss:0.150, val_acc:0.891]
Epoch [39/120    avg_loss:0.150, val_acc:0.900]
Epoch [40/120    avg_loss:0.155, val_acc:0.934]
Epoch [41/120    avg_loss:0.123, val_acc:0.935]
Epoch [42/120    avg_loss:0.109, val_acc:0.934]
Epoch [43/120    avg_loss:0.097, val_acc:0.943]
Epoch [44/120    avg_loss:0.103, val_acc:0.939]
Epoch [45/120    avg_loss:0.095, val_acc:0.941]
Epoch [46/120    avg_loss:0.122, val_acc:0.923]
Epoch [47/120    avg_loss:0.232, val_acc:0.905]
Epoch [48/120    avg_loss:0.181, val_acc:0.923]
Epoch [49/120    avg_loss:0.131, val_acc:0.920]
Epoch [50/120    avg_loss:0.105, val_acc:0.938]
Epoch [51/120    avg_loss:0.083, val_acc:0.924]
Epoch [52/120    avg_loss:0.120, val_acc:0.930]
Epoch [53/120    avg_loss:0.141, val_acc:0.915]
Epoch [54/120    avg_loss:0.091, val_acc:0.943]
Epoch [55/120    avg_loss:0.091, val_acc:0.953]
Epoch [56/120    avg_loss:0.088, val_acc:0.948]
Epoch [57/120    avg_loss:0.071, val_acc:0.951]
Epoch [58/120    avg_loss:0.063, val_acc:0.944]
Epoch [59/120    avg_loss:0.061, val_acc:0.949]
Epoch [60/120    avg_loss:0.054, val_acc:0.952]
Epoch [61/120    avg_loss:0.061, val_acc:0.942]
Epoch [62/120    avg_loss:0.069, val_acc:0.954]
Epoch [63/120    avg_loss:0.064, val_acc:0.960]
Epoch [64/120    avg_loss:0.043, val_acc:0.954]
Epoch [65/120    avg_loss:0.066, val_acc:0.946]
Epoch [66/120    avg_loss:0.048, val_acc:0.954]
Epoch [67/120    avg_loss:0.050, val_acc:0.964]
Epoch [68/120    avg_loss:0.064, val_acc:0.932]
Epoch [69/120    avg_loss:0.062, val_acc:0.948]
Epoch [70/120    avg_loss:0.053, val_acc:0.952]
Epoch [71/120    avg_loss:0.055, val_acc:0.960]
Epoch [72/120    avg_loss:0.066, val_acc:0.943]
Epoch [73/120    avg_loss:0.046, val_acc:0.961]
Epoch [74/120    avg_loss:0.102, val_acc:0.956]
Epoch [75/120    avg_loss:0.052, val_acc:0.963]
Epoch [76/120    avg_loss:0.049, val_acc:0.962]
Epoch [77/120    avg_loss:0.054, val_acc:0.954]
Epoch [78/120    avg_loss:0.034, val_acc:0.965]
Epoch [79/120    avg_loss:0.034, val_acc:0.952]
Epoch [80/120    avg_loss:0.045, val_acc:0.965]
Epoch [81/120    avg_loss:0.041, val_acc:0.968]
Epoch [82/120    avg_loss:0.032, val_acc:0.971]
Epoch [83/120    avg_loss:0.028, val_acc:0.968]
Epoch [84/120    avg_loss:0.026, val_acc:0.964]
Epoch [85/120    avg_loss:0.079, val_acc:0.945]
Epoch [86/120    avg_loss:0.084, val_acc:0.963]
Epoch [87/120    avg_loss:0.039, val_acc:0.965]
Epoch [88/120    avg_loss:0.051, val_acc:0.959]
Epoch [89/120    avg_loss:0.031, val_acc:0.962]
Epoch [90/120    avg_loss:0.031, val_acc:0.953]
Epoch [91/120    avg_loss:0.061, val_acc:0.963]
Epoch [92/120    avg_loss:0.045, val_acc:0.964]
Epoch [93/120    avg_loss:0.031, val_acc:0.962]
Epoch [94/120    avg_loss:0.023, val_acc:0.969]
Epoch [95/120    avg_loss:0.026, val_acc:0.964]
Epoch [96/120    avg_loss:0.027, val_acc:0.969]
Epoch [97/120    avg_loss:0.021, val_acc:0.965]
Epoch [98/120    avg_loss:0.017, val_acc:0.968]
Epoch [99/120    avg_loss:0.019, val_acc:0.968]
Epoch [100/120    avg_loss:0.012, val_acc:0.968]
Epoch [101/120    avg_loss:0.015, val_acc:0.970]
Epoch [102/120    avg_loss:0.016, val_acc:0.969]
Epoch [103/120    avg_loss:0.014, val_acc:0.970]
Epoch [104/120    avg_loss:0.014, val_acc:0.971]
Epoch [105/120    avg_loss:0.015, val_acc:0.968]
Epoch [106/120    avg_loss:0.014, val_acc:0.973]
Epoch [107/120    avg_loss:0.017, val_acc:0.973]
Epoch [108/120    avg_loss:0.014, val_acc:0.973]
Epoch [109/120    avg_loss:0.011, val_acc:0.973]
Epoch [110/120    avg_loss:0.016, val_acc:0.972]
Epoch [111/120    avg_loss:0.016, val_acc:0.973]
Epoch [112/120    avg_loss:0.014, val_acc:0.977]
Epoch [113/120    avg_loss:0.014, val_acc:0.975]
Epoch [114/120    avg_loss:0.023, val_acc:0.972]
Epoch [115/120    avg_loss:0.012, val_acc:0.973]
Epoch [116/120    avg_loss:0.014, val_acc:0.974]
Epoch [117/120    avg_loss:0.016, val_acc:0.977]
Epoch [118/120    avg_loss:0.013, val_acc:0.973]
Epoch [119/120    avg_loss:0.014, val_acc:0.975]
Epoch [120/120    avg_loss:0.014, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1257    2    0    0    2    0    0    4   10   10    0    0
     0    0    0]
 [   0    0    7  723    1    8    0    0    0    5    0    0    3    0
     0    0    0]
 [   0    0    0    2  207    0    0    0    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    3    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    4   90    0    2    0    0    0    0  772    0    0    0
     0    7    0]
 [   0    0   13    0    0    0    8    0    0    0   18 2170    0    1
     0    0    0]
 [   0    0    8   33    0    6    0    0    0    0    9    0  472    0
     1    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    1    0    0
  1135    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   140  207    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.42547425474255

F1 scores:
[       nan 0.96296296 0.97668998 0.90544771 0.98337292 0.97149373
 0.99092284 1.         0.99767442 0.66666667 0.91469194 0.98793535
 0.93465347 0.99730458 0.93762908 0.73796791 0.97109827]

Kappa:
0.9478029079530337
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f40b61978d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.451, val_acc:0.466]
Epoch [2/120    avg_loss:2.021, val_acc:0.505]
Epoch [3/120    avg_loss:1.758, val_acc:0.569]
Epoch [4/120    avg_loss:1.570, val_acc:0.628]
Epoch [5/120    avg_loss:1.394, val_acc:0.684]
Epoch [6/120    avg_loss:1.252, val_acc:0.721]
Epoch [7/120    avg_loss:1.036, val_acc:0.704]
Epoch [8/120    avg_loss:0.942, val_acc:0.734]
Epoch [9/120    avg_loss:0.871, val_acc:0.743]
Epoch [10/120    avg_loss:0.778, val_acc:0.806]
Epoch [11/120    avg_loss:0.758, val_acc:0.804]
Epoch [12/120    avg_loss:0.671, val_acc:0.795]
Epoch [13/120    avg_loss:0.666, val_acc:0.797]
Epoch [14/120    avg_loss:0.555, val_acc:0.847]
Epoch [15/120    avg_loss:0.507, val_acc:0.806]
Epoch [16/120    avg_loss:0.521, val_acc:0.852]
Epoch [17/120    avg_loss:0.408, val_acc:0.864]
Epoch [18/120    avg_loss:0.405, val_acc:0.834]
Epoch [19/120    avg_loss:0.421, val_acc:0.841]
Epoch [20/120    avg_loss:0.358, val_acc:0.854]
Epoch [21/120    avg_loss:0.301, val_acc:0.858]
Epoch [22/120    avg_loss:0.316, val_acc:0.878]
Epoch [23/120    avg_loss:0.302, val_acc:0.886]
Epoch [24/120    avg_loss:0.310, val_acc:0.857]
Epoch [25/120    avg_loss:0.326, val_acc:0.860]
Epoch [26/120    avg_loss:0.317, val_acc:0.883]
Epoch [27/120    avg_loss:0.288, val_acc:0.882]
Epoch [28/120    avg_loss:0.306, val_acc:0.893]
Epoch [29/120    avg_loss:0.255, val_acc:0.914]
Epoch [30/120    avg_loss:0.262, val_acc:0.903]
Epoch [31/120    avg_loss:0.222, val_acc:0.911]
Epoch [32/120    avg_loss:0.202, val_acc:0.903]
Epoch [33/120    avg_loss:0.228, val_acc:0.914]
Epoch [34/120    avg_loss:0.181, val_acc:0.905]
Epoch [35/120    avg_loss:0.264, val_acc:0.895]
Epoch [36/120    avg_loss:0.191, val_acc:0.921]
Epoch [37/120    avg_loss:0.199, val_acc:0.911]
Epoch [38/120    avg_loss:0.209, val_acc:0.887]
Epoch [39/120    avg_loss:0.183, val_acc:0.883]
Epoch [40/120    avg_loss:0.200, val_acc:0.912]
Epoch [41/120    avg_loss:0.178, val_acc:0.919]
Epoch [42/120    avg_loss:0.148, val_acc:0.940]
Epoch [43/120    avg_loss:0.120, val_acc:0.936]
Epoch [44/120    avg_loss:0.122, val_acc:0.930]
Epoch [45/120    avg_loss:0.128, val_acc:0.932]
Epoch [46/120    avg_loss:0.120, val_acc:0.952]
Epoch [47/120    avg_loss:0.107, val_acc:0.927]
Epoch [48/120    avg_loss:0.143, val_acc:0.941]
Epoch [49/120    avg_loss:0.101, val_acc:0.944]
Epoch [50/120    avg_loss:0.136, val_acc:0.937]
Epoch [51/120    avg_loss:0.112, val_acc:0.926]
Epoch [52/120    avg_loss:0.138, val_acc:0.920]
Epoch [53/120    avg_loss:0.097, val_acc:0.952]
Epoch [54/120    avg_loss:0.122, val_acc:0.929]
Epoch [55/120    avg_loss:0.143, val_acc:0.935]
Epoch [56/120    avg_loss:0.111, val_acc:0.953]
Epoch [57/120    avg_loss:0.095, val_acc:0.932]
Epoch [58/120    avg_loss:0.100, val_acc:0.941]
Epoch [59/120    avg_loss:0.079, val_acc:0.952]
Epoch [60/120    avg_loss:0.091, val_acc:0.926]
Epoch [61/120    avg_loss:0.077, val_acc:0.947]
Epoch [62/120    avg_loss:0.080, val_acc:0.945]
Epoch [63/120    avg_loss:0.069, val_acc:0.959]
Epoch [64/120    avg_loss:0.060, val_acc:0.958]
Epoch [65/120    avg_loss:0.060, val_acc:0.945]
Epoch [66/120    avg_loss:0.069, val_acc:0.926]
Epoch [67/120    avg_loss:0.085, val_acc:0.954]
Epoch [68/120    avg_loss:0.079, val_acc:0.954]
Epoch [69/120    avg_loss:0.056, val_acc:0.958]
Epoch [70/120    avg_loss:0.127, val_acc:0.946]
Epoch [71/120    avg_loss:0.116, val_acc:0.962]
Epoch [72/120    avg_loss:0.082, val_acc:0.947]
Epoch [73/120    avg_loss:0.094, val_acc:0.941]
Epoch [74/120    avg_loss:0.076, val_acc:0.952]
Epoch [75/120    avg_loss:0.048, val_acc:0.965]
Epoch [76/120    avg_loss:0.047, val_acc:0.959]
Epoch [77/120    avg_loss:0.041, val_acc:0.965]
Epoch [78/120    avg_loss:0.058, val_acc:0.950]
Epoch [79/120    avg_loss:0.084, val_acc:0.901]
Epoch [80/120    avg_loss:0.108, val_acc:0.952]
Epoch [81/120    avg_loss:0.063, val_acc:0.959]
Epoch [82/120    avg_loss:0.058, val_acc:0.963]
Epoch [83/120    avg_loss:0.078, val_acc:0.964]
Epoch [84/120    avg_loss:0.035, val_acc:0.964]
Epoch [85/120    avg_loss:0.036, val_acc:0.966]
Epoch [86/120    avg_loss:0.030, val_acc:0.972]
Epoch [87/120    avg_loss:0.042, val_acc:0.974]
Epoch [88/120    avg_loss:0.148, val_acc:0.873]
Epoch [89/120    avg_loss:0.109, val_acc:0.957]
Epoch [90/120    avg_loss:0.061, val_acc:0.954]
Epoch [91/120    avg_loss:0.044, val_acc:0.971]
Epoch [92/120    avg_loss:0.039, val_acc:0.956]
Epoch [93/120    avg_loss:0.043, val_acc:0.971]
Epoch [94/120    avg_loss:0.053, val_acc:0.966]
Epoch [95/120    avg_loss:0.046, val_acc:0.955]
Epoch [96/120    avg_loss:0.037, val_acc:0.971]
Epoch [97/120    avg_loss:0.050, val_acc:0.967]
Epoch [98/120    avg_loss:0.030, val_acc:0.967]
Epoch [99/120    avg_loss:0.034, val_acc:0.963]
Epoch [100/120    avg_loss:0.031, val_acc:0.970]
Epoch [101/120    avg_loss:0.023, val_acc:0.975]
Epoch [102/120    avg_loss:0.019, val_acc:0.976]
Epoch [103/120    avg_loss:0.015, val_acc:0.976]
Epoch [104/120    avg_loss:0.016, val_acc:0.976]
Epoch [105/120    avg_loss:0.019, val_acc:0.976]
Epoch [106/120    avg_loss:0.014, val_acc:0.979]
Epoch [107/120    avg_loss:0.025, val_acc:0.977]
Epoch [108/120    avg_loss:0.017, val_acc:0.980]
Epoch [109/120    avg_loss:0.021, val_acc:0.977]
Epoch [110/120    avg_loss:0.015, val_acc:0.979]
Epoch [111/120    avg_loss:0.015, val_acc:0.979]
Epoch [112/120    avg_loss:0.020, val_acc:0.976]
Epoch [113/120    avg_loss:0.017, val_acc:0.979]
Epoch [114/120    avg_loss:0.017, val_acc:0.980]
Epoch [115/120    avg_loss:0.017, val_acc:0.980]
Epoch [116/120    avg_loss:0.026, val_acc:0.979]
Epoch [117/120    avg_loss:0.016, val_acc:0.979]
Epoch [118/120    avg_loss:0.018, val_acc:0.980]
Epoch [119/120    avg_loss:0.013, val_acc:0.981]
Epoch [120/120    avg_loss:0.014, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0   12 1233    2    0    0    6    0    0    1    2   23    5    0
     0    1    0]
 [   0    0    1  728    0    9    0    0    0    6    0    0    2    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    0    0    2    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    2    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   12    0    0    0    0    0    0  418    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   18   90    0    4    0    0    0    0  756    1    0    0
     0    6    0]
 [   0    0    6    0    0    1    4    0    0    0   15 2183    0    1
     0    0    0]
 [   0    0    0   16    0    0    0    0    0    0   20    1  494    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    1    0    0
  1135    1    0]
 [   0    0    0    0    0    0   46    0    0    0    0    0    0    0
    91  210    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
95.2520325203252

F1 scores:
[       nan 0.7254902  0.9697208  0.91977258 1.         0.97031963
 0.95377843 1.         0.98584906 0.77272727 0.90430622 0.98755938
 0.94817658 0.99462366 0.95458368 0.74336283 0.94545455]

Kappa:
0.9458313223938224
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f32c10f2898>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.559, val_acc:0.417]
Epoch [2/120    avg_loss:2.175, val_acc:0.520]
Epoch [3/120    avg_loss:1.923, val_acc:0.576]
Epoch [4/120    avg_loss:1.685, val_acc:0.592]
Epoch [5/120    avg_loss:1.505, val_acc:0.640]
Epoch [6/120    avg_loss:1.318, val_acc:0.627]
Epoch [7/120    avg_loss:1.126, val_acc:0.618]
Epoch [8/120    avg_loss:0.972, val_acc:0.693]
Epoch [9/120    avg_loss:0.860, val_acc:0.642]
Epoch [10/120    avg_loss:0.848, val_acc:0.765]
Epoch [11/120    avg_loss:0.699, val_acc:0.801]
Epoch [12/120    avg_loss:0.603, val_acc:0.733]
Epoch [13/120    avg_loss:0.594, val_acc:0.806]
Epoch [14/120    avg_loss:0.443, val_acc:0.832]
Epoch [15/120    avg_loss:0.485, val_acc:0.845]
Epoch [16/120    avg_loss:0.440, val_acc:0.844]
Epoch [17/120    avg_loss:0.373, val_acc:0.861]
Epoch [18/120    avg_loss:0.329, val_acc:0.856]
Epoch [19/120    avg_loss:0.247, val_acc:0.887]
Epoch [20/120    avg_loss:0.255, val_acc:0.894]
Epoch [21/120    avg_loss:0.233, val_acc:0.912]
Epoch [22/120    avg_loss:0.218, val_acc:0.917]
Epoch [23/120    avg_loss:0.238, val_acc:0.869]
Epoch [24/120    avg_loss:0.241, val_acc:0.896]
Epoch [25/120    avg_loss:0.181, val_acc:0.926]
Epoch [26/120    avg_loss:0.168, val_acc:0.923]
Epoch [27/120    avg_loss:0.173, val_acc:0.925]
Epoch [28/120    avg_loss:0.120, val_acc:0.923]
Epoch [29/120    avg_loss:0.137, val_acc:0.933]
Epoch [30/120    avg_loss:0.164, val_acc:0.938]
Epoch [31/120    avg_loss:0.126, val_acc:0.928]
Epoch [32/120    avg_loss:0.121, val_acc:0.949]
Epoch [33/120    avg_loss:0.099, val_acc:0.967]
Epoch [34/120    avg_loss:0.098, val_acc:0.939]
Epoch [35/120    avg_loss:0.083, val_acc:0.954]
Epoch [36/120    avg_loss:0.091, val_acc:0.958]
Epoch [37/120    avg_loss:0.084, val_acc:0.959]
Epoch [38/120    avg_loss:0.062, val_acc:0.957]
Epoch [39/120    avg_loss:0.066, val_acc:0.948]
Epoch [40/120    avg_loss:0.078, val_acc:0.960]
Epoch [41/120    avg_loss:0.067, val_acc:0.962]
Epoch [42/120    avg_loss:0.050, val_acc:0.963]
Epoch [43/120    avg_loss:0.043, val_acc:0.952]
Epoch [44/120    avg_loss:0.045, val_acc:0.962]
Epoch [45/120    avg_loss:0.053, val_acc:0.961]
Epoch [46/120    avg_loss:0.047, val_acc:0.965]
Epoch [47/120    avg_loss:0.029, val_acc:0.970]
Epoch [48/120    avg_loss:0.027, val_acc:0.977]
Epoch [49/120    avg_loss:0.029, val_acc:0.977]
Epoch [50/120    avg_loss:0.024, val_acc:0.974]
Epoch [51/120    avg_loss:0.025, val_acc:0.976]
Epoch [52/120    avg_loss:0.031, val_acc:0.977]
Epoch [53/120    avg_loss:0.024, val_acc:0.977]
Epoch [54/120    avg_loss:0.023, val_acc:0.978]
Epoch [55/120    avg_loss:0.023, val_acc:0.977]
Epoch [56/120    avg_loss:0.024, val_acc:0.979]
Epoch [57/120    avg_loss:0.024, val_acc:0.977]
Epoch [58/120    avg_loss:0.025, val_acc:0.978]
Epoch [59/120    avg_loss:0.023, val_acc:0.974]
Epoch [60/120    avg_loss:0.023, val_acc:0.978]
Epoch [61/120    avg_loss:0.029, val_acc:0.979]
Epoch [62/120    avg_loss:0.023, val_acc:0.974]
Epoch [63/120    avg_loss:0.021, val_acc:0.979]
Epoch [64/120    avg_loss:0.022, val_acc:0.975]
Epoch [65/120    avg_loss:0.025, val_acc:0.975]
Epoch [66/120    avg_loss:0.020, val_acc:0.975]
Epoch [67/120    avg_loss:0.021, val_acc:0.976]
Epoch [68/120    avg_loss:0.020, val_acc:0.974]
Epoch [69/120    avg_loss:0.021, val_acc:0.976]
Epoch [70/120    avg_loss:0.029, val_acc:0.979]
Epoch [71/120    avg_loss:0.025, val_acc:0.976]
Epoch [72/120    avg_loss:0.021, val_acc:0.977]
Epoch [73/120    avg_loss:0.025, val_acc:0.978]
Epoch [74/120    avg_loss:0.025, val_acc:0.981]
Epoch [75/120    avg_loss:0.017, val_acc:0.978]
Epoch [76/120    avg_loss:0.019, val_acc:0.978]
Epoch [77/120    avg_loss:0.020, val_acc:0.976]
Epoch [78/120    avg_loss:0.018, val_acc:0.976]
Epoch [79/120    avg_loss:0.020, val_acc:0.975]
Epoch [80/120    avg_loss:0.021, val_acc:0.976]
Epoch [81/120    avg_loss:0.021, val_acc:0.978]
Epoch [82/120    avg_loss:0.020, val_acc:0.977]
Epoch [83/120    avg_loss:0.018, val_acc:0.977]
Epoch [84/120    avg_loss:0.022, val_acc:0.978]
Epoch [85/120    avg_loss:0.018, val_acc:0.977]
Epoch [86/120    avg_loss:0.018, val_acc:0.978]
Epoch [87/120    avg_loss:0.019, val_acc:0.976]
Epoch [88/120    avg_loss:0.019, val_acc:0.977]
Epoch [89/120    avg_loss:0.018, val_acc:0.977]
Epoch [90/120    avg_loss:0.019, val_acc:0.978]
Epoch [91/120    avg_loss:0.019, val_acc:0.978]
Epoch [92/120    avg_loss:0.015, val_acc:0.978]
Epoch [93/120    avg_loss:0.019, val_acc:0.978]
Epoch [94/120    avg_loss:0.017, val_acc:0.978]
Epoch [95/120    avg_loss:0.015, val_acc:0.978]
Epoch [96/120    avg_loss:0.021, val_acc:0.978]
Epoch [97/120    avg_loss:0.020, val_acc:0.978]
Epoch [98/120    avg_loss:0.017, val_acc:0.978]
Epoch [99/120    avg_loss:0.018, val_acc:0.978]
Epoch [100/120    avg_loss:0.017, val_acc:0.977]
Epoch [101/120    avg_loss:0.023, val_acc:0.977]
Epoch [102/120    avg_loss:0.015, val_acc:0.977]
Epoch [103/120    avg_loss:0.017, val_acc:0.978]
Epoch [104/120    avg_loss:0.019, val_acc:0.978]
Epoch [105/120    avg_loss:0.016, val_acc:0.977]
Epoch [106/120    avg_loss:0.020, val_acc:0.978]
Epoch [107/120    avg_loss:0.018, val_acc:0.978]
Epoch [108/120    avg_loss:0.017, val_acc:0.978]
Epoch [109/120    avg_loss:0.016, val_acc:0.978]
Epoch [110/120    avg_loss:0.016, val_acc:0.978]
Epoch [111/120    avg_loss:0.017, val_acc:0.978]
Epoch [112/120    avg_loss:0.021, val_acc:0.978]
Epoch [113/120    avg_loss:0.022, val_acc:0.978]
Epoch [114/120    avg_loss:0.022, val_acc:0.978]
Epoch [115/120    avg_loss:0.017, val_acc:0.978]
Epoch [116/120    avg_loss:0.018, val_acc:0.978]
Epoch [117/120    avg_loss:0.017, val_acc:0.978]
Epoch [118/120    avg_loss:0.017, val_acc:0.978]
Epoch [119/120    avg_loss:0.016, val_acc:0.978]
Epoch [120/120    avg_loss:0.020, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    3    0    3    0    0    0    1    6   11    0    0
     0    0    0]
 [   0    0    0  719    9    2    0    0    0    1    1    8    2    3
     2    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  850   19    0    0
     1    1    0]
 [   0    0    7    0    0    2    0    0    0    6   16 2160   17    0
     0    2    0]
 [   0    0    3    5    2    1    0    0    0    0    5    1  508    0
     0    6    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1121   17    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    55  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.33333333333333

F1 scores:
[       nan 1.         0.98515625 0.97557666 0.97482838 0.98394495
 0.99017385 1.         1.         0.8        0.96976612 0.97936976
 0.9566855  0.9919571  0.96471601 0.85932722 0.97647059]

Kappa:
0.9695974704280858
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3fd16eb898>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.594, val_acc:0.415]
Epoch [2/120    avg_loss:2.145, val_acc:0.470]
Epoch [3/120    avg_loss:1.874, val_acc:0.472]
Epoch [4/120    avg_loss:1.640, val_acc:0.568]
Epoch [5/120    avg_loss:1.446, val_acc:0.606]
Epoch [6/120    avg_loss:1.261, val_acc:0.663]
Epoch [7/120    avg_loss:1.099, val_acc:0.683]
Epoch [8/120    avg_loss:0.963, val_acc:0.719]
Epoch [9/120    avg_loss:0.884, val_acc:0.748]
Epoch [10/120    avg_loss:0.764, val_acc:0.725]
Epoch [11/120    avg_loss:0.665, val_acc:0.754]
Epoch [12/120    avg_loss:0.553, val_acc:0.793]
Epoch [13/120    avg_loss:0.559, val_acc:0.747]
Epoch [14/120    avg_loss:0.500, val_acc:0.787]
Epoch [15/120    avg_loss:0.423, val_acc:0.838]
Epoch [16/120    avg_loss:0.424, val_acc:0.865]
Epoch [17/120    avg_loss:0.308, val_acc:0.871]
Epoch [18/120    avg_loss:0.290, val_acc:0.840]
Epoch [19/120    avg_loss:0.321, val_acc:0.851]
Epoch [20/120    avg_loss:0.277, val_acc:0.834]
Epoch [21/120    avg_loss:0.303, val_acc:0.877]
Epoch [22/120    avg_loss:0.211, val_acc:0.888]
Epoch [23/120    avg_loss:0.234, val_acc:0.885]
Epoch [24/120    avg_loss:0.173, val_acc:0.906]
Epoch [25/120    avg_loss:0.165, val_acc:0.893]
Epoch [26/120    avg_loss:0.160, val_acc:0.908]
Epoch [27/120    avg_loss:0.113, val_acc:0.922]
Epoch [28/120    avg_loss:0.157, val_acc:0.904]
Epoch [29/120    avg_loss:0.128, val_acc:0.939]
Epoch [30/120    avg_loss:0.102, val_acc:0.928]
Epoch [31/120    avg_loss:0.088, val_acc:0.938]
Epoch [32/120    avg_loss:0.098, val_acc:0.923]
Epoch [33/120    avg_loss:0.107, val_acc:0.923]
Epoch [34/120    avg_loss:0.108, val_acc:0.930]
Epoch [35/120    avg_loss:0.078, val_acc:0.939]
Epoch [36/120    avg_loss:0.106, val_acc:0.914]
Epoch [37/120    avg_loss:0.136, val_acc:0.930]
Epoch [38/120    avg_loss:0.113, val_acc:0.935]
Epoch [39/120    avg_loss:0.080, val_acc:0.943]
Epoch [40/120    avg_loss:0.062, val_acc:0.957]
Epoch [41/120    avg_loss:0.059, val_acc:0.957]
Epoch [42/120    avg_loss:0.064, val_acc:0.956]
Epoch [43/120    avg_loss:0.049, val_acc:0.953]
Epoch [44/120    avg_loss:0.115, val_acc:0.760]
Epoch [45/120    avg_loss:0.308, val_acc:0.850]
Epoch [46/120    avg_loss:0.123, val_acc:0.932]
Epoch [47/120    avg_loss:0.096, val_acc:0.913]
Epoch [48/120    avg_loss:0.134, val_acc:0.907]
Epoch [49/120    avg_loss:0.142, val_acc:0.939]
Epoch [50/120    avg_loss:0.072, val_acc:0.943]
Epoch [51/120    avg_loss:0.060, val_acc:0.947]
Epoch [52/120    avg_loss:0.050, val_acc:0.948]
Epoch [53/120    avg_loss:0.068, val_acc:0.955]
Epoch [54/120    avg_loss:0.061, val_acc:0.966]
Epoch [55/120    avg_loss:0.049, val_acc:0.955]
Epoch [56/120    avg_loss:0.051, val_acc:0.954]
Epoch [57/120    avg_loss:0.033, val_acc:0.963]
Epoch [58/120    avg_loss:0.035, val_acc:0.962]
Epoch [59/120    avg_loss:0.044, val_acc:0.962]
Epoch [60/120    avg_loss:0.025, val_acc:0.966]
Epoch [61/120    avg_loss:0.029, val_acc:0.946]
Epoch [62/120    avg_loss:0.052, val_acc:0.961]
Epoch [63/120    avg_loss:0.028, val_acc:0.967]
Epoch [64/120    avg_loss:0.036, val_acc:0.973]
Epoch [65/120    avg_loss:0.032, val_acc:0.965]
Epoch [66/120    avg_loss:0.031, val_acc:0.956]
Epoch [67/120    avg_loss:0.039, val_acc:0.967]
Epoch [68/120    avg_loss:0.030, val_acc:0.958]
Epoch [69/120    avg_loss:0.031, val_acc:0.965]
Epoch [70/120    avg_loss:0.043, val_acc:0.952]
Epoch [71/120    avg_loss:0.025, val_acc:0.971]
Epoch [72/120    avg_loss:0.018, val_acc:0.962]
Epoch [73/120    avg_loss:0.043, val_acc:0.961]
Epoch [74/120    avg_loss:0.032, val_acc:0.965]
Epoch [75/120    avg_loss:0.036, val_acc:0.971]
Epoch [76/120    avg_loss:0.019, val_acc:0.969]
Epoch [77/120    avg_loss:0.012, val_acc:0.973]
Epoch [78/120    avg_loss:0.017, val_acc:0.962]
Epoch [79/120    avg_loss:0.020, val_acc:0.975]
Epoch [80/120    avg_loss:0.018, val_acc:0.972]
Epoch [81/120    avg_loss:0.014, val_acc:0.966]
Epoch [82/120    avg_loss:0.023, val_acc:0.968]
Epoch [83/120    avg_loss:0.011, val_acc:0.969]
Epoch [84/120    avg_loss:0.016, val_acc:0.972]
Epoch [85/120    avg_loss:0.016, val_acc:0.969]
Epoch [86/120    avg_loss:0.008, val_acc:0.970]
Epoch [87/120    avg_loss:0.017, val_acc:0.952]
Epoch [88/120    avg_loss:0.030, val_acc:0.967]
Epoch [89/120    avg_loss:0.024, val_acc:0.951]
Epoch [90/120    avg_loss:0.023, val_acc:0.969]
Epoch [91/120    avg_loss:0.010, val_acc:0.974]
Epoch [92/120    avg_loss:0.010, val_acc:0.973]
Epoch [93/120    avg_loss:0.009, val_acc:0.974]
Epoch [94/120    avg_loss:0.010, val_acc:0.974]
Epoch [95/120    avg_loss:0.007, val_acc:0.975]
Epoch [96/120    avg_loss:0.007, val_acc:0.976]
Epoch [97/120    avg_loss:0.008, val_acc:0.976]
Epoch [98/120    avg_loss:0.007, val_acc:0.973]
Epoch [99/120    avg_loss:0.006, val_acc:0.972]
Epoch [100/120    avg_loss:0.008, val_acc:0.973]
Epoch [101/120    avg_loss:0.008, val_acc:0.976]
Epoch [102/120    avg_loss:0.008, val_acc:0.974]
Epoch [103/120    avg_loss:0.007, val_acc:0.972]
Epoch [104/120    avg_loss:0.008, val_acc:0.974]
Epoch [105/120    avg_loss:0.008, val_acc:0.976]
Epoch [106/120    avg_loss:0.006, val_acc:0.976]
Epoch [107/120    avg_loss:0.005, val_acc:0.976]
Epoch [108/120    avg_loss:0.007, val_acc:0.976]
Epoch [109/120    avg_loss:0.005, val_acc:0.977]
Epoch [110/120    avg_loss:0.007, val_acc:0.976]
Epoch [111/120    avg_loss:0.007, val_acc:0.976]
Epoch [112/120    avg_loss:0.006, val_acc:0.978]
Epoch [113/120    avg_loss:0.007, val_acc:0.977]
Epoch [114/120    avg_loss:0.009, val_acc:0.977]
Epoch [115/120    avg_loss:0.005, val_acc:0.977]
Epoch [116/120    avg_loss:0.005, val_acc:0.977]
Epoch [117/120    avg_loss:0.007, val_acc:0.977]
Epoch [118/120    avg_loss:0.007, val_acc:0.978]
Epoch [119/120    avg_loss:0.006, val_acc:0.980]
Epoch [120/120    avg_loss:0.006, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    1    0    0    3    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0 1276    1    0    0    0    0    0    0    1    6    1    0
     0    0    0]
 [   0    0    0  726    3    0    0    0    0    1    5    6    3    0
     2    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    0    0    1    0    0
     8    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    1    0    0    0    1    0    0    0  863   10    0    0
     0    0    0]
 [   0    0    4    0    0    0    1    0    0    0   11 2179   15    0
     0    0    0]
 [   0    0    5    4    0    0    0    0    0    0    0    0  523    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    0    0    0    0    0    0    0    0
  1115   15    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
    99  247    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.51761517615176

F1 scores:
[       nan 0.93506494 0.99222395 0.98240866 0.99300699 0.97482838
 0.99695586 1.         0.99883586 0.94444444 0.98347578 0.98731309
 0.96762257 1.         0.94331641 0.80983607 0.97005988]

Kappa:
0.9716761177929729
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f29f15d7828>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.606, val_acc:0.399]
Epoch [2/120    avg_loss:2.210, val_acc:0.423]
Epoch [3/120    avg_loss:1.932, val_acc:0.486]
Epoch [4/120    avg_loss:1.719, val_acc:0.568]
Epoch [5/120    avg_loss:1.532, val_acc:0.561]
Epoch [6/120    avg_loss:1.372, val_acc:0.611]
Epoch [7/120    avg_loss:1.158, val_acc:0.663]
Epoch [8/120    avg_loss:1.031, val_acc:0.671]
Epoch [9/120    avg_loss:0.948, val_acc:0.716]
Epoch [10/120    avg_loss:0.980, val_acc:0.677]
Epoch [11/120    avg_loss:0.856, val_acc:0.755]
Epoch [12/120    avg_loss:0.674, val_acc:0.773]
Epoch [13/120    avg_loss:0.705, val_acc:0.766]
Epoch [14/120    avg_loss:0.634, val_acc:0.773]
Epoch [15/120    avg_loss:0.554, val_acc:0.805]
Epoch [16/120    avg_loss:0.482, val_acc:0.835]
Epoch [17/120    avg_loss:0.440, val_acc:0.846]
Epoch [18/120    avg_loss:0.336, val_acc:0.848]
Epoch [19/120    avg_loss:0.322, val_acc:0.866]
Epoch [20/120    avg_loss:0.317, val_acc:0.844]
Epoch [21/120    avg_loss:0.348, val_acc:0.829]
Epoch [22/120    avg_loss:0.283, val_acc:0.881]
Epoch [23/120    avg_loss:0.258, val_acc:0.892]
Epoch [24/120    avg_loss:0.220, val_acc:0.894]
Epoch [25/120    avg_loss:0.198, val_acc:0.899]
Epoch [26/120    avg_loss:0.181, val_acc:0.909]
Epoch [27/120    avg_loss:0.196, val_acc:0.862]
Epoch [28/120    avg_loss:0.183, val_acc:0.882]
Epoch [29/120    avg_loss:0.164, val_acc:0.905]
Epoch [30/120    avg_loss:0.146, val_acc:0.904]
Epoch [31/120    avg_loss:0.141, val_acc:0.912]
Epoch [32/120    avg_loss:0.137, val_acc:0.903]
Epoch [33/120    avg_loss:0.118, val_acc:0.930]
Epoch [34/120    avg_loss:0.098, val_acc:0.905]
Epoch [35/120    avg_loss:0.088, val_acc:0.944]
Epoch [36/120    avg_loss:0.081, val_acc:0.947]
Epoch [37/120    avg_loss:0.078, val_acc:0.944]
Epoch [38/120    avg_loss:0.076, val_acc:0.910]
Epoch [39/120    avg_loss:0.070, val_acc:0.942]
Epoch [40/120    avg_loss:0.080, val_acc:0.940]
Epoch [41/120    avg_loss:0.079, val_acc:0.922]
Epoch [42/120    avg_loss:0.093, val_acc:0.923]
Epoch [43/120    avg_loss:0.083, val_acc:0.949]
Epoch [44/120    avg_loss:0.065, val_acc:0.942]
Epoch [45/120    avg_loss:0.046, val_acc:0.947]
Epoch [46/120    avg_loss:0.050, val_acc:0.947]
Epoch [47/120    avg_loss:0.052, val_acc:0.956]
Epoch [48/120    avg_loss:0.057, val_acc:0.955]
Epoch [49/120    avg_loss:0.053, val_acc:0.927]
Epoch [50/120    avg_loss:0.081, val_acc:0.952]
Epoch [51/120    avg_loss:0.050, val_acc:0.954]
Epoch [52/120    avg_loss:0.047, val_acc:0.951]
Epoch [53/120    avg_loss:0.044, val_acc:0.954]
Epoch [54/120    avg_loss:0.039, val_acc:0.938]
Epoch [55/120    avg_loss:0.037, val_acc:0.953]
Epoch [56/120    avg_loss:0.024, val_acc:0.961]
Epoch [57/120    avg_loss:0.026, val_acc:0.964]
Epoch [58/120    avg_loss:0.024, val_acc:0.955]
Epoch [59/120    avg_loss:0.025, val_acc:0.951]
Epoch [60/120    avg_loss:0.026, val_acc:0.947]
Epoch [61/120    avg_loss:0.032, val_acc:0.959]
Epoch [62/120    avg_loss:0.029, val_acc:0.956]
Epoch [63/120    avg_loss:0.026, val_acc:0.966]
Epoch [64/120    avg_loss:0.036, val_acc:0.954]
Epoch [65/120    avg_loss:0.036, val_acc:0.970]
Epoch [66/120    avg_loss:0.028, val_acc:0.961]
Epoch [67/120    avg_loss:0.033, val_acc:0.958]
Epoch [68/120    avg_loss:0.023, val_acc:0.960]
Epoch [69/120    avg_loss:0.019, val_acc:0.962]
Epoch [70/120    avg_loss:0.036, val_acc:0.947]
Epoch [71/120    avg_loss:0.039, val_acc:0.960]
Epoch [72/120    avg_loss:0.018, val_acc:0.963]
Epoch [73/120    avg_loss:0.021, val_acc:0.963]
Epoch [74/120    avg_loss:0.026, val_acc:0.965]
Epoch [75/120    avg_loss:0.021, val_acc:0.965]
Epoch [76/120    avg_loss:0.025, val_acc:0.964]
Epoch [77/120    avg_loss:0.016, val_acc:0.965]
Epoch [78/120    avg_loss:0.017, val_acc:0.971]
Epoch [79/120    avg_loss:0.014, val_acc:0.966]
Epoch [80/120    avg_loss:0.029, val_acc:0.960]
Epoch [81/120    avg_loss:0.017, val_acc:0.963]
Epoch [82/120    avg_loss:0.020, val_acc:0.971]
Epoch [83/120    avg_loss:0.015, val_acc:0.971]
Epoch [84/120    avg_loss:0.015, val_acc:0.968]
Epoch [85/120    avg_loss:0.012, val_acc:0.960]
Epoch [86/120    avg_loss:0.018, val_acc:0.969]
Epoch [87/120    avg_loss:0.013, val_acc:0.973]
Epoch [88/120    avg_loss:0.012, val_acc:0.964]
Epoch [89/120    avg_loss:0.008, val_acc:0.970]
Epoch [90/120    avg_loss:0.007, val_acc:0.971]
Epoch [91/120    avg_loss:0.014, val_acc:0.966]
Epoch [92/120    avg_loss:0.010, val_acc:0.980]
Epoch [93/120    avg_loss:0.014, val_acc:0.969]
Epoch [94/120    avg_loss:0.010, val_acc:0.966]
Epoch [95/120    avg_loss:0.009, val_acc:0.969]
Epoch [96/120    avg_loss:0.009, val_acc:0.973]
Epoch [97/120    avg_loss:0.010, val_acc:0.968]
Epoch [98/120    avg_loss:0.009, val_acc:0.974]
Epoch [99/120    avg_loss:0.012, val_acc:0.971]
Epoch [100/120    avg_loss:0.010, val_acc:0.974]
Epoch [101/120    avg_loss:0.005, val_acc:0.977]
Epoch [102/120    avg_loss:0.008, val_acc:0.972]
Epoch [103/120    avg_loss:0.011, val_acc:0.970]
Epoch [104/120    avg_loss:0.007, val_acc:0.971]
Epoch [105/120    avg_loss:0.007, val_acc:0.969]
Epoch [106/120    avg_loss:0.007, val_acc:0.972]
Epoch [107/120    avg_loss:0.006, val_acc:0.973]
Epoch [108/120    avg_loss:0.005, val_acc:0.972]
Epoch [109/120    avg_loss:0.005, val_acc:0.971]
Epoch [110/120    avg_loss:0.008, val_acc:0.973]
Epoch [111/120    avg_loss:0.005, val_acc:0.975]
Epoch [112/120    avg_loss:0.007, val_acc:0.974]
Epoch [113/120    avg_loss:0.005, val_acc:0.975]
Epoch [114/120    avg_loss:0.007, val_acc:0.974]
Epoch [115/120    avg_loss:0.007, val_acc:0.976]
Epoch [116/120    avg_loss:0.005, val_acc:0.976]
Epoch [117/120    avg_loss:0.005, val_acc:0.978]
Epoch [118/120    avg_loss:0.005, val_acc:0.978]
Epoch [119/120    avg_loss:0.007, val_acc:0.978]
Epoch [120/120    avg_loss:0.005, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1260    5    0    4    0    0    0    0    2   14    0    0
     0    0    0]
 [   0    0    0  728    6    0    3    0    0    2    2    2    2    0
     0    2    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    1    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0  845   25    2    0
     0    1    0]
 [   0    0   10    0    0    2    0    0    0    0    7 2181    0    0
     1    9    0]
 [   0    0    0    1    8    0    0    0    0    0    0    0  522    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1120   18    0]
 [   0    0    0    0    0    0   18    0    0    0    0    0    0    0
    53  276    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.71273712737127

F1 scores:
[       nan 0.98765432 0.98552992 0.98245614 0.96818182 0.98969072
 0.98203593 1.         1.         0.91891892 0.97575058 0.98420578
 0.98490566 1.         0.9671848  0.8440367  0.98823529]

Kappa:
0.9739129504786388
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fef3ce8f828>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.598, val_acc:0.419]
Epoch [2/120    avg_loss:2.167, val_acc:0.536]
Epoch [3/120    avg_loss:1.883, val_acc:0.560]
Epoch [4/120    avg_loss:1.677, val_acc:0.613]
Epoch [5/120    avg_loss:1.484, val_acc:0.622]
Epoch [6/120    avg_loss:1.267, val_acc:0.625]
Epoch [7/120    avg_loss:1.184, val_acc:0.724]
Epoch [8/120    avg_loss:0.942, val_acc:0.761]
Epoch [9/120    avg_loss:0.819, val_acc:0.767]
Epoch [10/120    avg_loss:0.716, val_acc:0.769]
Epoch [11/120    avg_loss:0.815, val_acc:0.736]
Epoch [12/120    avg_loss:0.695, val_acc:0.773]
Epoch [13/120    avg_loss:0.590, val_acc:0.757]
Epoch [14/120    avg_loss:0.571, val_acc:0.781]
Epoch [15/120    avg_loss:0.441, val_acc:0.757]
Epoch [16/120    avg_loss:0.370, val_acc:0.816]
Epoch [17/120    avg_loss:0.381, val_acc:0.825]
Epoch [18/120    avg_loss:0.341, val_acc:0.843]
Epoch [19/120    avg_loss:0.373, val_acc:0.827]
Epoch [20/120    avg_loss:0.407, val_acc:0.845]
Epoch [21/120    avg_loss:0.305, val_acc:0.861]
Epoch [22/120    avg_loss:0.239, val_acc:0.895]
Epoch [23/120    avg_loss:0.224, val_acc:0.902]
Epoch [24/120    avg_loss:0.205, val_acc:0.894]
Epoch [25/120    avg_loss:0.220, val_acc:0.901]
Epoch [26/120    avg_loss:0.204, val_acc:0.902]
Epoch [27/120    avg_loss:0.280, val_acc:0.839]
Epoch [28/120    avg_loss:0.303, val_acc:0.880]
Epoch [29/120    avg_loss:0.198, val_acc:0.887]
Epoch [30/120    avg_loss:0.165, val_acc:0.904]
Epoch [31/120    avg_loss:0.147, val_acc:0.880]
Epoch [32/120    avg_loss:0.158, val_acc:0.920]
Epoch [33/120    avg_loss:0.154, val_acc:0.927]
Epoch [34/120    avg_loss:0.122, val_acc:0.931]
Epoch [35/120    avg_loss:0.123, val_acc:0.910]
Epoch [36/120    avg_loss:0.101, val_acc:0.947]
Epoch [37/120    avg_loss:0.090, val_acc:0.935]
Epoch [38/120    avg_loss:0.109, val_acc:0.936]
Epoch [39/120    avg_loss:0.088, val_acc:0.949]
Epoch [40/120    avg_loss:0.359, val_acc:0.860]
Epoch [41/120    avg_loss:0.234, val_acc:0.916]
Epoch [42/120    avg_loss:0.137, val_acc:0.947]
Epoch [43/120    avg_loss:0.111, val_acc:0.932]
Epoch [44/120    avg_loss:0.088, val_acc:0.907]
Epoch [45/120    avg_loss:0.105, val_acc:0.943]
Epoch [46/120    avg_loss:0.094, val_acc:0.927]
Epoch [47/120    avg_loss:0.075, val_acc:0.944]
Epoch [48/120    avg_loss:0.063, val_acc:0.947]
Epoch [49/120    avg_loss:0.096, val_acc:0.936]
Epoch [50/120    avg_loss:0.113, val_acc:0.922]
Epoch [51/120    avg_loss:0.085, val_acc:0.936]
Epoch [52/120    avg_loss:0.068, val_acc:0.954]
Epoch [53/120    avg_loss:0.050, val_acc:0.970]
Epoch [54/120    avg_loss:0.069, val_acc:0.957]
Epoch [55/120    avg_loss:0.045, val_acc:0.960]
Epoch [56/120    avg_loss:0.038, val_acc:0.963]
Epoch [57/120    avg_loss:0.040, val_acc:0.954]
Epoch [58/120    avg_loss:0.046, val_acc:0.960]
Epoch [59/120    avg_loss:0.051, val_acc:0.946]
Epoch [60/120    avg_loss:0.044, val_acc:0.960]
Epoch [61/120    avg_loss:0.040, val_acc:0.961]
Epoch [62/120    avg_loss:0.038, val_acc:0.957]
Epoch [63/120    avg_loss:0.031, val_acc:0.958]
Epoch [64/120    avg_loss:0.024, val_acc:0.963]
Epoch [65/120    avg_loss:0.025, val_acc:0.959]
Epoch [66/120    avg_loss:0.023, val_acc:0.970]
Epoch [67/120    avg_loss:0.073, val_acc:0.946]
Epoch [68/120    avg_loss:0.042, val_acc:0.959]
Epoch [69/120    avg_loss:0.049, val_acc:0.963]
Epoch [70/120    avg_loss:0.044, val_acc:0.961]
Epoch [71/120    avg_loss:0.044, val_acc:0.955]
Epoch [72/120    avg_loss:0.028, val_acc:0.964]
Epoch [73/120    avg_loss:0.029, val_acc:0.964]
Epoch [74/120    avg_loss:0.025, val_acc:0.967]
Epoch [75/120    avg_loss:0.020, val_acc:0.970]
Epoch [76/120    avg_loss:0.019, val_acc:0.969]
Epoch [77/120    avg_loss:0.025, val_acc:0.967]
Epoch [78/120    avg_loss:0.018, val_acc:0.967]
Epoch [79/120    avg_loss:0.023, val_acc:0.969]
Epoch [80/120    avg_loss:0.018, val_acc:0.969]
Epoch [81/120    avg_loss:0.018, val_acc:0.967]
Epoch [82/120    avg_loss:0.015, val_acc:0.967]
Epoch [83/120    avg_loss:0.013, val_acc:0.940]
Epoch [84/120    avg_loss:0.037, val_acc:0.958]
Epoch [85/120    avg_loss:0.080, val_acc:0.920]
Epoch [86/120    avg_loss:0.062, val_acc:0.952]
Epoch [87/120    avg_loss:0.049, val_acc:0.963]
Epoch [88/120    avg_loss:0.023, val_acc:0.964]
Epoch [89/120    avg_loss:0.017, val_acc:0.966]
Epoch [90/120    avg_loss:0.020, val_acc:0.965]
Epoch [91/120    avg_loss:0.014, val_acc:0.968]
Epoch [92/120    avg_loss:0.016, val_acc:0.969]
Epoch [93/120    avg_loss:0.016, val_acc:0.972]
Epoch [94/120    avg_loss:0.011, val_acc:0.971]
Epoch [95/120    avg_loss:0.012, val_acc:0.971]
Epoch [96/120    avg_loss:0.012, val_acc:0.970]
Epoch [97/120    avg_loss:0.014, val_acc:0.970]
Epoch [98/120    avg_loss:0.010, val_acc:0.972]
Epoch [99/120    avg_loss:0.013, val_acc:0.975]
Epoch [100/120    avg_loss:0.012, val_acc:0.974]
Epoch [101/120    avg_loss:0.011, val_acc:0.975]
Epoch [102/120    avg_loss:0.010, val_acc:0.975]
Epoch [103/120    avg_loss:0.013, val_acc:0.973]
Epoch [104/120    avg_loss:0.011, val_acc:0.975]
Epoch [105/120    avg_loss:0.009, val_acc:0.974]
Epoch [106/120    avg_loss:0.010, val_acc:0.973]
Epoch [107/120    avg_loss:0.009, val_acc:0.974]
Epoch [108/120    avg_loss:0.011, val_acc:0.974]
Epoch [109/120    avg_loss:0.009, val_acc:0.974]
Epoch [110/120    avg_loss:0.010, val_acc:0.975]
Epoch [111/120    avg_loss:0.011, val_acc:0.975]
Epoch [112/120    avg_loss:0.010, val_acc:0.976]
Epoch [113/120    avg_loss:0.014, val_acc:0.975]
Epoch [114/120    avg_loss:0.011, val_acc:0.974]
Epoch [115/120    avg_loss:0.011, val_acc:0.975]
Epoch [116/120    avg_loss:0.014, val_acc:0.972]
Epoch [117/120    avg_loss:0.010, val_acc:0.975]
Epoch [118/120    avg_loss:0.014, val_acc:0.976]
Epoch [119/120    avg_loss:0.008, val_acc:0.974]
Epoch [120/120    avg_loss:0.009, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1264    1    0    0    0    0    0    1    2   16    1    0
     0    0    0]
 [   0    0    0  737    0    0    0    0    0    1    0    2    4    3
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    2    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  855   15    0    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    2   11 2166   24    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    0    3  525    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1121   18    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    72  275    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.7560975609756

F1 scores:
[       nan 0.98765432 0.98711441 0.9905914  0.99764706 0.99071926
 0.99847561 1.         1.         0.85714286 0.98050459 0.98142275
 0.96330275 0.9919571  0.95893926 0.85803432 0.98224852]

Kappa:
0.9744096448491694
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fec8a419828>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.605, val_acc:0.392]
Epoch [2/120    avg_loss:2.198, val_acc:0.494]
Epoch [3/120    avg_loss:1.918, val_acc:0.555]
Epoch [4/120    avg_loss:1.706, val_acc:0.597]
Epoch [5/120    avg_loss:1.606, val_acc:0.591]
Epoch [6/120    avg_loss:1.458, val_acc:0.645]
Epoch [7/120    avg_loss:1.300, val_acc:0.665]
Epoch [8/120    avg_loss:1.170, val_acc:0.660]
Epoch [9/120    avg_loss:1.103, val_acc:0.717]
Epoch [10/120    avg_loss:0.870, val_acc:0.719]
Epoch [11/120    avg_loss:0.804, val_acc:0.755]
Epoch [12/120    avg_loss:0.660, val_acc:0.776]
Epoch [13/120    avg_loss:0.594, val_acc:0.780]
Epoch [14/120    avg_loss:0.562, val_acc:0.778]
Epoch [15/120    avg_loss:0.575, val_acc:0.791]
Epoch [16/120    avg_loss:0.508, val_acc:0.836]
Epoch [17/120    avg_loss:0.357, val_acc:0.860]
Epoch [18/120    avg_loss:0.404, val_acc:0.794]
Epoch [19/120    avg_loss:0.417, val_acc:0.847]
Epoch [20/120    avg_loss:0.328, val_acc:0.887]
Epoch [21/120    avg_loss:0.345, val_acc:0.839]
Epoch [22/120    avg_loss:0.256, val_acc:0.899]
Epoch [23/120    avg_loss:0.262, val_acc:0.906]
Epoch [24/120    avg_loss:0.235, val_acc:0.876]
Epoch [25/120    avg_loss:0.177, val_acc:0.917]
Epoch [26/120    avg_loss:0.177, val_acc:0.919]
Epoch [27/120    avg_loss:0.161, val_acc:0.908]
Epoch [28/120    avg_loss:0.233, val_acc:0.892]
Epoch [29/120    avg_loss:0.215, val_acc:0.908]
Epoch [30/120    avg_loss:0.155, val_acc:0.936]
Epoch [31/120    avg_loss:0.170, val_acc:0.919]
Epoch [32/120    avg_loss:0.120, val_acc:0.920]
Epoch [33/120    avg_loss:0.143, val_acc:0.905]
Epoch [34/120    avg_loss:0.122, val_acc:0.932]
Epoch [35/120    avg_loss:0.099, val_acc:0.941]
Epoch [36/120    avg_loss:0.100, val_acc:0.928]
Epoch [37/120    avg_loss:0.110, val_acc:0.928]
Epoch [38/120    avg_loss:0.075, val_acc:0.928]
Epoch [39/120    avg_loss:0.084, val_acc:0.943]
Epoch [40/120    avg_loss:0.074, val_acc:0.955]
Epoch [41/120    avg_loss:0.077, val_acc:0.953]
Epoch [42/120    avg_loss:0.070, val_acc:0.942]
Epoch [43/120    avg_loss:0.084, val_acc:0.949]
Epoch [44/120    avg_loss:0.066, val_acc:0.958]
Epoch [45/120    avg_loss:0.049, val_acc:0.956]
Epoch [46/120    avg_loss:0.069, val_acc:0.954]
Epoch [47/120    avg_loss:0.080, val_acc:0.940]
Epoch [48/120    avg_loss:0.064, val_acc:0.957]
Epoch [49/120    avg_loss:0.055, val_acc:0.961]
Epoch [50/120    avg_loss:0.047, val_acc:0.934]
Epoch [51/120    avg_loss:0.059, val_acc:0.947]
Epoch [52/120    avg_loss:0.078, val_acc:0.945]
Epoch [53/120    avg_loss:0.047, val_acc:0.969]
Epoch [54/120    avg_loss:0.029, val_acc:0.961]
Epoch [55/120    avg_loss:0.034, val_acc:0.958]
Epoch [56/120    avg_loss:0.035, val_acc:0.963]
Epoch [57/120    avg_loss:0.056, val_acc:0.939]
Epoch [58/120    avg_loss:0.040, val_acc:0.957]
Epoch [59/120    avg_loss:0.028, val_acc:0.961]
Epoch [60/120    avg_loss:0.031, val_acc:0.966]
Epoch [61/120    avg_loss:0.029, val_acc:0.968]
Epoch [62/120    avg_loss:0.024, val_acc:0.973]
Epoch [63/120    avg_loss:0.031, val_acc:0.962]
Epoch [64/120    avg_loss:0.029, val_acc:0.966]
Epoch [65/120    avg_loss:0.027, val_acc:0.967]
Epoch [66/120    avg_loss:0.022, val_acc:0.967]
Epoch [67/120    avg_loss:0.016, val_acc:0.973]
Epoch [68/120    avg_loss:0.018, val_acc:0.965]
Epoch [69/120    avg_loss:0.030, val_acc:0.969]
Epoch [70/120    avg_loss:0.037, val_acc:0.963]
Epoch [71/120    avg_loss:0.024, val_acc:0.964]
Epoch [72/120    avg_loss:0.025, val_acc:0.965]
Epoch [73/120    avg_loss:0.017, val_acc:0.972]
Epoch [74/120    avg_loss:0.024, val_acc:0.965]
Epoch [75/120    avg_loss:0.020, val_acc:0.969]
Epoch [76/120    avg_loss:0.014, val_acc:0.974]
Epoch [77/120    avg_loss:0.016, val_acc:0.971]
Epoch [78/120    avg_loss:0.018, val_acc:0.976]
Epoch [79/120    avg_loss:0.031, val_acc:0.966]
Epoch [80/120    avg_loss:0.026, val_acc:0.976]
Epoch [81/120    avg_loss:0.036, val_acc:0.969]
Epoch [82/120    avg_loss:0.024, val_acc:0.969]
Epoch [83/120    avg_loss:0.019, val_acc:0.974]
Epoch [84/120    avg_loss:0.026, val_acc:0.972]
Epoch [85/120    avg_loss:0.018, val_acc:0.968]
Epoch [86/120    avg_loss:0.029, val_acc:0.971]
Epoch [87/120    avg_loss:0.020, val_acc:0.976]
Epoch [88/120    avg_loss:0.025, val_acc:0.954]
Epoch [89/120    avg_loss:0.043, val_acc:0.963]
Epoch [90/120    avg_loss:0.018, val_acc:0.976]
Epoch [91/120    avg_loss:0.022, val_acc:0.977]
Epoch [92/120    avg_loss:0.020, val_acc:0.976]
Epoch [93/120    avg_loss:0.020, val_acc:0.970]
Epoch [94/120    avg_loss:0.022, val_acc:0.961]
Epoch [95/120    avg_loss:0.032, val_acc:0.977]
Epoch [96/120    avg_loss:0.013, val_acc:0.980]
Epoch [97/120    avg_loss:0.015, val_acc:0.972]
Epoch [98/120    avg_loss:0.010, val_acc:0.980]
Epoch [99/120    avg_loss:0.009, val_acc:0.980]
Epoch [100/120    avg_loss:0.011, val_acc:0.978]
Epoch [101/120    avg_loss:0.020, val_acc:0.972]
Epoch [102/120    avg_loss:0.033, val_acc:0.963]
Epoch [103/120    avg_loss:0.016, val_acc:0.978]
Epoch [104/120    avg_loss:0.014, val_acc:0.976]
Epoch [105/120    avg_loss:0.011, val_acc:0.984]
Epoch [106/120    avg_loss:0.012, val_acc:0.980]
Epoch [107/120    avg_loss:0.015, val_acc:0.966]
Epoch [108/120    avg_loss:0.023, val_acc:0.975]
Epoch [109/120    avg_loss:0.011, val_acc:0.978]
Epoch [110/120    avg_loss:0.008, val_acc:0.980]
Epoch [111/120    avg_loss:0.006, val_acc:0.981]
Epoch [112/120    avg_loss:0.008, val_acc:0.983]
Epoch [113/120    avg_loss:0.026, val_acc:0.980]
Epoch [114/120    avg_loss:0.034, val_acc:0.967]
Epoch [115/120    avg_loss:0.044, val_acc:0.934]
Epoch [116/120    avg_loss:0.068, val_acc:0.970]
Epoch [117/120    avg_loss:0.038, val_acc:0.980]
Epoch [118/120    avg_loss:0.012, val_acc:0.971]
Epoch [119/120    avg_loss:0.009, val_acc:0.973]
Epoch [120/120    avg_loss:0.009, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0 1253    6    6    1    0    0    0    0    5   13    1    0
     0    0    0]
 [   0    0    0  720    2    0    1    0    0    1    0   14    9    0
     0    0    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    1    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    0    0    0    0    0  853   12    0    0
     0    0    0]
 [   0    0    5    0    0    1    0    0    0    1   23 2159   21    0
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    4  525    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1124   14    0]
 [   0    0    0    0    0    3   15    0    0    0    0    0    0    0
    94  235    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.97560975609755

F1 scores:
[       nan 0.975      0.98120595 0.97693351 0.97685185 0.98737084
 0.98574644 1.         1.         0.86486486 0.9715262  0.97847269
 0.96153846 1.         0.95254237 0.78726968 0.98245614]

Kappa:
0.9655001540363783
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3450ee0780>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.573, val_acc:0.383]
Epoch [2/120    avg_loss:2.163, val_acc:0.447]
Epoch [3/120    avg_loss:1.897, val_acc:0.554]
Epoch [4/120    avg_loss:1.667, val_acc:0.579]
Epoch [5/120    avg_loss:1.449, val_acc:0.637]
Epoch [6/120    avg_loss:1.354, val_acc:0.645]
Epoch [7/120    avg_loss:1.081, val_acc:0.670]
Epoch [8/120    avg_loss:1.016, val_acc:0.740]
Epoch [9/120    avg_loss:0.842, val_acc:0.800]
Epoch [10/120    avg_loss:0.708, val_acc:0.792]
Epoch [11/120    avg_loss:0.612, val_acc:0.785]
Epoch [12/120    avg_loss:0.650, val_acc:0.795]
Epoch [13/120    avg_loss:0.557, val_acc:0.834]
Epoch [14/120    avg_loss:0.512, val_acc:0.787]
Epoch [15/120    avg_loss:0.451, val_acc:0.827]
Epoch [16/120    avg_loss:0.400, val_acc:0.861]
Epoch [17/120    avg_loss:0.396, val_acc:0.858]
Epoch [18/120    avg_loss:0.329, val_acc:0.879]
Epoch [19/120    avg_loss:0.289, val_acc:0.875]
Epoch [20/120    avg_loss:0.306, val_acc:0.854]
Epoch [21/120    avg_loss:0.280, val_acc:0.882]
Epoch [22/120    avg_loss:0.299, val_acc:0.849]
Epoch [23/120    avg_loss:0.218, val_acc:0.902]
Epoch [24/120    avg_loss:0.187, val_acc:0.920]
Epoch [25/120    avg_loss:0.197, val_acc:0.883]
Epoch [26/120    avg_loss:0.223, val_acc:0.881]
Epoch [27/120    avg_loss:0.163, val_acc:0.907]
Epoch [28/120    avg_loss:0.185, val_acc:0.895]
Epoch [29/120    avg_loss:0.151, val_acc:0.909]
Epoch [30/120    avg_loss:0.124, val_acc:0.929]
Epoch [31/120    avg_loss:0.114, val_acc:0.907]
Epoch [32/120    avg_loss:0.110, val_acc:0.935]
Epoch [33/120    avg_loss:0.110, val_acc:0.931]
Epoch [34/120    avg_loss:0.080, val_acc:0.932]
Epoch [35/120    avg_loss:0.087, val_acc:0.931]
Epoch [36/120    avg_loss:0.093, val_acc:0.933]
Epoch [37/120    avg_loss:0.092, val_acc:0.951]
Epoch [38/120    avg_loss:0.071, val_acc:0.933]
Epoch [39/120    avg_loss:0.058, val_acc:0.943]
Epoch [40/120    avg_loss:0.068, val_acc:0.953]
Epoch [41/120    avg_loss:0.064, val_acc:0.954]
Epoch [42/120    avg_loss:0.052, val_acc:0.953]
Epoch [43/120    avg_loss:0.049, val_acc:0.946]
Epoch [44/120    avg_loss:0.157, val_acc:0.799]
Epoch [45/120    avg_loss:0.280, val_acc:0.822]
Epoch [46/120    avg_loss:0.263, val_acc:0.876]
Epoch [47/120    avg_loss:0.180, val_acc:0.925]
Epoch [48/120    avg_loss:0.107, val_acc:0.926]
Epoch [49/120    avg_loss:0.072, val_acc:0.940]
Epoch [50/120    avg_loss:0.059, val_acc:0.946]
Epoch [51/120    avg_loss:0.058, val_acc:0.952]
Epoch [52/120    avg_loss:0.046, val_acc:0.947]
Epoch [53/120    avg_loss:0.061, val_acc:0.943]
Epoch [54/120    avg_loss:0.049, val_acc:0.946]
Epoch [55/120    avg_loss:0.047, val_acc:0.951]
Epoch [56/120    avg_loss:0.031, val_acc:0.960]
Epoch [57/120    avg_loss:0.034, val_acc:0.958]
Epoch [58/120    avg_loss:0.030, val_acc:0.964]
Epoch [59/120    avg_loss:0.032, val_acc:0.961]
Epoch [60/120    avg_loss:0.031, val_acc:0.966]
Epoch [61/120    avg_loss:0.026, val_acc:0.964]
Epoch [62/120    avg_loss:0.027, val_acc:0.964]
Epoch [63/120    avg_loss:0.031, val_acc:0.964]
Epoch [64/120    avg_loss:0.023, val_acc:0.965]
Epoch [65/120    avg_loss:0.023, val_acc:0.969]
Epoch [66/120    avg_loss:0.023, val_acc:0.969]
Epoch [67/120    avg_loss:0.029, val_acc:0.969]
Epoch [68/120    avg_loss:0.024, val_acc:0.967]
Epoch [69/120    avg_loss:0.028, val_acc:0.968]
Epoch [70/120    avg_loss:0.022, val_acc:0.967]
Epoch [71/120    avg_loss:0.027, val_acc:0.970]
Epoch [72/120    avg_loss:0.024, val_acc:0.969]
Epoch [73/120    avg_loss:0.026, val_acc:0.968]
Epoch [74/120    avg_loss:0.024, val_acc:0.971]
Epoch [75/120    avg_loss:0.027, val_acc:0.971]
Epoch [76/120    avg_loss:0.024, val_acc:0.969]
Epoch [77/120    avg_loss:0.025, val_acc:0.970]
Epoch [78/120    avg_loss:0.023, val_acc:0.970]
Epoch [79/120    avg_loss:0.025, val_acc:0.970]
Epoch [80/120    avg_loss:0.018, val_acc:0.969]
Epoch [81/120    avg_loss:0.026, val_acc:0.970]
Epoch [82/120    avg_loss:0.023, val_acc:0.970]
Epoch [83/120    avg_loss:0.022, val_acc:0.969]
Epoch [84/120    avg_loss:0.022, val_acc:0.970]
Epoch [85/120    avg_loss:0.023, val_acc:0.970]
Epoch [86/120    avg_loss:0.019, val_acc:0.970]
Epoch [87/120    avg_loss:0.021, val_acc:0.972]
Epoch [88/120    avg_loss:0.021, val_acc:0.971]
Epoch [89/120    avg_loss:0.021, val_acc:0.968]
Epoch [90/120    avg_loss:0.021, val_acc:0.970]
Epoch [91/120    avg_loss:0.017, val_acc:0.970]
Epoch [92/120    avg_loss:0.023, val_acc:0.970]
Epoch [93/120    avg_loss:0.022, val_acc:0.971]
Epoch [94/120    avg_loss:0.019, val_acc:0.970]
Epoch [95/120    avg_loss:0.017, val_acc:0.969]
Epoch [96/120    avg_loss:0.031, val_acc:0.969]
Epoch [97/120    avg_loss:0.019, val_acc:0.970]
Epoch [98/120    avg_loss:0.018, val_acc:0.971]
Epoch [99/120    avg_loss:0.019, val_acc:0.970]
Epoch [100/120    avg_loss:0.020, val_acc:0.971]
Epoch [101/120    avg_loss:0.018, val_acc:0.971]
Epoch [102/120    avg_loss:0.018, val_acc:0.971]
Epoch [103/120    avg_loss:0.024, val_acc:0.971]
Epoch [104/120    avg_loss:0.018, val_acc:0.971]
Epoch [105/120    avg_loss:0.019, val_acc:0.971]
Epoch [106/120    avg_loss:0.019, val_acc:0.971]
Epoch [107/120    avg_loss:0.022, val_acc:0.972]
Epoch [108/120    avg_loss:0.016, val_acc:0.971]
Epoch [109/120    avg_loss:0.018, val_acc:0.971]
Epoch [110/120    avg_loss:0.021, val_acc:0.972]
Epoch [111/120    avg_loss:0.019, val_acc:0.972]
Epoch [112/120    avg_loss:0.023, val_acc:0.972]
Epoch [113/120    avg_loss:0.018, val_acc:0.971]
Epoch [114/120    avg_loss:0.018, val_acc:0.972]
Epoch [115/120    avg_loss:0.015, val_acc:0.972]
Epoch [116/120    avg_loss:0.019, val_acc:0.972]
Epoch [117/120    avg_loss:0.018, val_acc:0.972]
Epoch [118/120    avg_loss:0.020, val_acc:0.972]
Epoch [119/120    avg_loss:0.022, val_acc:0.971]
Epoch [120/120    avg_loss:0.014, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    2    0    1    0    0    0
     0    0    0]
 [   0    0 1252    4    0    3    0    0    0    0    2   24    0    0
     0    0    0]
 [   0    0    0  723    3    0    0    0    0    0    1   16    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    0    0    0    0    0    0    0
    10    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    1    0    0    0    0  841   26    4    0
     0    0    0]
 [   0    0    8    0    0    1    0    0    0    0    6 2190    3    0
     2    0    0]
 [   0    0    0    3    0    0    0    0    0    0    0    7  518    0
     0    3    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1132    7    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    69  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.6260162601626

F1 scores:
[       nan 0.96202532 0.98273155 0.97901151 0.99300699 0.98265896
 0.99923839 1.         0.99767981 1.         0.97450753 0.97898972
 0.97276995 1.         0.96258503 0.87559055 0.9704142 ]

Kappa:
0.9728914993356101
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f685915c7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.621, val_acc:0.446]
Epoch [2/120    avg_loss:2.186, val_acc:0.515]
Epoch [3/120    avg_loss:1.944, val_acc:0.555]
Epoch [4/120    avg_loss:1.694, val_acc:0.640]
Epoch [5/120    avg_loss:1.502, val_acc:0.658]
Epoch [6/120    avg_loss:1.313, val_acc:0.693]
Epoch [7/120    avg_loss:1.195, val_acc:0.699]
Epoch [8/120    avg_loss:1.019, val_acc:0.747]
Epoch [9/120    avg_loss:0.888, val_acc:0.745]
Epoch [10/120    avg_loss:0.737, val_acc:0.742]
Epoch [11/120    avg_loss:0.733, val_acc:0.754]
Epoch [12/120    avg_loss:0.682, val_acc:0.798]
Epoch [13/120    avg_loss:0.582, val_acc:0.812]
Epoch [14/120    avg_loss:0.461, val_acc:0.817]
Epoch [15/120    avg_loss:0.445, val_acc:0.840]
Epoch [16/120    avg_loss:0.391, val_acc:0.844]
Epoch [17/120    avg_loss:0.361, val_acc:0.872]
Epoch [18/120    avg_loss:0.346, val_acc:0.865]
Epoch [19/120    avg_loss:0.247, val_acc:0.854]
Epoch [20/120    avg_loss:0.267, val_acc:0.893]
Epoch [21/120    avg_loss:0.288, val_acc:0.887]
Epoch [22/120    avg_loss:0.264, val_acc:0.895]
Epoch [23/120    avg_loss:0.179, val_acc:0.914]
Epoch [24/120    avg_loss:0.161, val_acc:0.879]
Epoch [25/120    avg_loss:0.174, val_acc:0.906]
Epoch [26/120    avg_loss:0.152, val_acc:0.889]
Epoch [27/120    avg_loss:0.173, val_acc:0.902]
Epoch [28/120    avg_loss:0.176, val_acc:0.926]
Epoch [29/120    avg_loss:0.156, val_acc:0.923]
Epoch [30/120    avg_loss:0.138, val_acc:0.912]
Epoch [31/120    avg_loss:0.275, val_acc:0.916]
Epoch [32/120    avg_loss:0.191, val_acc:0.883]
Epoch [33/120    avg_loss:0.150, val_acc:0.936]
Epoch [34/120    avg_loss:0.127, val_acc:0.930]
Epoch [35/120    avg_loss:0.108, val_acc:0.943]
Epoch [36/120    avg_loss:0.097, val_acc:0.944]
Epoch [37/120    avg_loss:0.101, val_acc:0.941]
Epoch [38/120    avg_loss:0.083, val_acc:0.935]
Epoch [39/120    avg_loss:0.076, val_acc:0.950]
Epoch [40/120    avg_loss:0.069, val_acc:0.961]
Epoch [41/120    avg_loss:0.066, val_acc:0.956]
Epoch [42/120    avg_loss:0.054, val_acc:0.950]
Epoch [43/120    avg_loss:0.074, val_acc:0.953]
Epoch [44/120    avg_loss:0.073, val_acc:0.947]
Epoch [45/120    avg_loss:0.051, val_acc:0.952]
Epoch [46/120    avg_loss:0.053, val_acc:0.960]
Epoch [47/120    avg_loss:0.065, val_acc:0.954]
Epoch [48/120    avg_loss:0.056, val_acc:0.957]
Epoch [49/120    avg_loss:0.052, val_acc:0.961]
Epoch [50/120    avg_loss:0.067, val_acc:0.947]
Epoch [51/120    avg_loss:0.052, val_acc:0.967]
Epoch [52/120    avg_loss:0.033, val_acc:0.959]
Epoch [53/120    avg_loss:0.035, val_acc:0.969]
Epoch [54/120    avg_loss:0.030, val_acc:0.970]
Epoch [55/120    avg_loss:0.035, val_acc:0.967]
Epoch [56/120    avg_loss:0.024, val_acc:0.959]
Epoch [57/120    avg_loss:0.030, val_acc:0.969]
Epoch [58/120    avg_loss:0.031, val_acc:0.972]
Epoch [59/120    avg_loss:0.034, val_acc:0.964]
Epoch [60/120    avg_loss:0.039, val_acc:0.963]
Epoch [61/120    avg_loss:0.031, val_acc:0.965]
Epoch [62/120    avg_loss:0.025, val_acc:0.975]
Epoch [63/120    avg_loss:0.019, val_acc:0.969]
Epoch [64/120    avg_loss:0.020, val_acc:0.974]
Epoch [65/120    avg_loss:0.023, val_acc:0.973]
Epoch [66/120    avg_loss:0.022, val_acc:0.977]
Epoch [67/120    avg_loss:0.027, val_acc:0.958]
Epoch [68/120    avg_loss:0.030, val_acc:0.974]
Epoch [69/120    avg_loss:0.023, val_acc:0.975]
Epoch [70/120    avg_loss:0.018, val_acc:0.975]
Epoch [71/120    avg_loss:0.021, val_acc:0.973]
Epoch [72/120    avg_loss:0.028, val_acc:0.950]
Epoch [73/120    avg_loss:0.058, val_acc:0.969]
Epoch [74/120    avg_loss:0.024, val_acc:0.974]
Epoch [75/120    avg_loss:0.017, val_acc:0.973]
Epoch [76/120    avg_loss:0.021, val_acc:0.961]
Epoch [77/120    avg_loss:0.019, val_acc:0.979]
Epoch [78/120    avg_loss:0.022, val_acc:0.963]
Epoch [79/120    avg_loss:0.018, val_acc:0.975]
Epoch [80/120    avg_loss:0.023, val_acc:0.976]
Epoch [81/120    avg_loss:0.054, val_acc:0.948]
Epoch [82/120    avg_loss:0.039, val_acc:0.974]
Epoch [83/120    avg_loss:0.038, val_acc:0.966]
Epoch [84/120    avg_loss:0.023, val_acc:0.977]
Epoch [85/120    avg_loss:0.017, val_acc:0.966]
Epoch [86/120    avg_loss:0.017, val_acc:0.977]
Epoch [87/120    avg_loss:0.014, val_acc:0.973]
Epoch [88/120    avg_loss:0.014, val_acc:0.978]
Epoch [89/120    avg_loss:0.013, val_acc:0.974]
Epoch [90/120    avg_loss:0.023, val_acc:0.972]
Epoch [91/120    avg_loss:0.018, val_acc:0.974]
Epoch [92/120    avg_loss:0.014, val_acc:0.977]
Epoch [93/120    avg_loss:0.013, val_acc:0.976]
Epoch [94/120    avg_loss:0.015, val_acc:0.978]
Epoch [95/120    avg_loss:0.011, val_acc:0.981]
Epoch [96/120    avg_loss:0.010, val_acc:0.980]
Epoch [97/120    avg_loss:0.009, val_acc:0.978]
Epoch [98/120    avg_loss:0.009, val_acc:0.980]
Epoch [99/120    avg_loss:0.007, val_acc:0.980]
Epoch [100/120    avg_loss:0.010, val_acc:0.979]
Epoch [101/120    avg_loss:0.012, val_acc:0.978]
Epoch [102/120    avg_loss:0.009, val_acc:0.978]
Epoch [103/120    avg_loss:0.008, val_acc:0.980]
Epoch [104/120    avg_loss:0.009, val_acc:0.978]
Epoch [105/120    avg_loss:0.011, val_acc:0.978]
Epoch [106/120    avg_loss:0.009, val_acc:0.979]
Epoch [107/120    avg_loss:0.009, val_acc:0.980]
Epoch [108/120    avg_loss:0.011, val_acc:0.980]
Epoch [109/120    avg_loss:0.007, val_acc:0.980]
Epoch [110/120    avg_loss:0.009, val_acc:0.980]
Epoch [111/120    avg_loss:0.007, val_acc:0.980]
Epoch [112/120    avg_loss:0.009, val_acc:0.980]
Epoch [113/120    avg_loss:0.009, val_acc:0.979]
Epoch [114/120    avg_loss:0.008, val_acc:0.980]
Epoch [115/120    avg_loss:0.008, val_acc:0.979]
Epoch [116/120    avg_loss:0.007, val_acc:0.979]
Epoch [117/120    avg_loss:0.010, val_acc:0.979]
Epoch [118/120    avg_loss:0.008, val_acc:0.979]
Epoch [119/120    avg_loss:0.009, val_acc:0.979]
Epoch [120/120    avg_loss:0.008, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1255    4    5    3    2    0    0    0    0   16    0    0
     0    0    0]
 [   0    0    2  721    6    0    0    0    0    0    1   10    6    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    1    0    0  431    0    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    2    0    0    0    0  846   20    2    0
     2    1    0]
 [   0    0    7    0    0    0    0    0    0    0   12 2178    6    0
     0    7    0]
 [   0    0    1    3    0    1    0    0    0    0    0    3  522    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    94  253    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.39837398373983

F1 scores:
[       nan 0.98765432 0.98277212 0.97762712 0.97482838 0.98853211
 0.99771863 1.         1.         1.         0.97577855 0.98152321
 0.97570093 1.         0.95350803 0.81350482 0.98823529]

Kappa:
0.9703107260105638
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f444fbec8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.580, val_acc:0.395]
Epoch [2/120    avg_loss:2.170, val_acc:0.507]
Epoch [3/120    avg_loss:1.904, val_acc:0.542]
Epoch [4/120    avg_loss:1.687, val_acc:0.591]
Epoch [5/120    avg_loss:1.525, val_acc:0.645]
Epoch [6/120    avg_loss:1.345, val_acc:0.649]
Epoch [7/120    avg_loss:1.185, val_acc:0.652]
Epoch [8/120    avg_loss:1.039, val_acc:0.713]
Epoch [9/120    avg_loss:0.879, val_acc:0.779]
Epoch [10/120    avg_loss:0.749, val_acc:0.794]
Epoch [11/120    avg_loss:0.706, val_acc:0.801]
Epoch [12/120    avg_loss:0.566, val_acc:0.829]
Epoch [13/120    avg_loss:0.503, val_acc:0.795]
Epoch [14/120    avg_loss:0.473, val_acc:0.873]
Epoch [15/120    avg_loss:0.486, val_acc:0.856]
Epoch [16/120    avg_loss:0.445, val_acc:0.855]
Epoch [17/120    avg_loss:0.370, val_acc:0.881]
Epoch [18/120    avg_loss:0.361, val_acc:0.847]
Epoch [19/120    avg_loss:0.363, val_acc:0.864]
Epoch [20/120    avg_loss:0.293, val_acc:0.858]
Epoch [21/120    avg_loss:0.290, val_acc:0.910]
Epoch [22/120    avg_loss:0.265, val_acc:0.907]
Epoch [23/120    avg_loss:0.182, val_acc:0.925]
Epoch [24/120    avg_loss:0.184, val_acc:0.935]
Epoch [25/120    avg_loss:0.144, val_acc:0.923]
Epoch [26/120    avg_loss:0.130, val_acc:0.928]
Epoch [27/120    avg_loss:0.133, val_acc:0.933]
Epoch [28/120    avg_loss:0.101, val_acc:0.934]
Epoch [29/120    avg_loss:0.108, val_acc:0.922]
Epoch [30/120    avg_loss:0.266, val_acc:0.870]
Epoch [31/120    avg_loss:0.182, val_acc:0.934]
Epoch [32/120    avg_loss:0.153, val_acc:0.920]
Epoch [33/120    avg_loss:0.133, val_acc:0.919]
Epoch [34/120    avg_loss:0.108, val_acc:0.940]
Epoch [35/120    avg_loss:0.094, val_acc:0.942]
Epoch [36/120    avg_loss:0.090, val_acc:0.954]
Epoch [37/120    avg_loss:0.094, val_acc:0.942]
Epoch [38/120    avg_loss:0.068, val_acc:0.957]
Epoch [39/120    avg_loss:0.065, val_acc:0.960]
Epoch [40/120    avg_loss:0.049, val_acc:0.957]
Epoch [41/120    avg_loss:0.065, val_acc:0.955]
Epoch [42/120    avg_loss:0.113, val_acc:0.956]
Epoch [43/120    avg_loss:0.056, val_acc:0.952]
Epoch [44/120    avg_loss:0.063, val_acc:0.961]
Epoch [45/120    avg_loss:0.048, val_acc:0.954]
Epoch [46/120    avg_loss:0.041, val_acc:0.954]
Epoch [47/120    avg_loss:0.046, val_acc:0.958]
Epoch [48/120    avg_loss:0.043, val_acc:0.956]
Epoch [49/120    avg_loss:0.036, val_acc:0.965]
Epoch [50/120    avg_loss:0.039, val_acc:0.949]
Epoch [51/120    avg_loss:0.060, val_acc:0.965]
Epoch [52/120    avg_loss:0.037, val_acc:0.967]
Epoch [53/120    avg_loss:0.032, val_acc:0.960]
Epoch [54/120    avg_loss:0.026, val_acc:0.971]
Epoch [55/120    avg_loss:0.026, val_acc:0.969]
Epoch [56/120    avg_loss:0.021, val_acc:0.959]
Epoch [57/120    avg_loss:0.047, val_acc:0.969]
Epoch [58/120    avg_loss:0.033, val_acc:0.965]
Epoch [59/120    avg_loss:0.041, val_acc:0.972]
Epoch [60/120    avg_loss:0.033, val_acc:0.970]
Epoch [61/120    avg_loss:0.032, val_acc:0.953]
Epoch [62/120    avg_loss:0.050, val_acc:0.965]
Epoch [63/120    avg_loss:0.033, val_acc:0.968]
Epoch [64/120    avg_loss:0.023, val_acc:0.969]
Epoch [65/120    avg_loss:0.032, val_acc:0.966]
Epoch [66/120    avg_loss:0.029, val_acc:0.949]
Epoch [67/120    avg_loss:0.046, val_acc:0.977]
Epoch [68/120    avg_loss:0.033, val_acc:0.972]
Epoch [69/120    avg_loss:0.026, val_acc:0.959]
Epoch [70/120    avg_loss:0.016, val_acc:0.970]
Epoch [71/120    avg_loss:0.022, val_acc:0.972]
Epoch [72/120    avg_loss:0.012, val_acc:0.968]
Epoch [73/120    avg_loss:0.031, val_acc:0.963]
Epoch [74/120    avg_loss:0.046, val_acc:0.971]
Epoch [75/120    avg_loss:0.022, val_acc:0.969]
Epoch [76/120    avg_loss:0.033, val_acc:0.952]
Epoch [77/120    avg_loss:0.024, val_acc:0.972]
Epoch [78/120    avg_loss:0.013, val_acc:0.972]
Epoch [79/120    avg_loss:0.018, val_acc:0.972]
Epoch [80/120    avg_loss:0.021, val_acc:0.968]
Epoch [81/120    avg_loss:0.014, val_acc:0.975]
Epoch [82/120    avg_loss:0.013, val_acc:0.977]
Epoch [83/120    avg_loss:0.008, val_acc:0.979]
Epoch [84/120    avg_loss:0.011, val_acc:0.978]
Epoch [85/120    avg_loss:0.009, val_acc:0.979]
Epoch [86/120    avg_loss:0.010, val_acc:0.978]
Epoch [87/120    avg_loss:0.009, val_acc:0.977]
Epoch [88/120    avg_loss:0.008, val_acc:0.979]
Epoch [89/120    avg_loss:0.008, val_acc:0.977]
Epoch [90/120    avg_loss:0.009, val_acc:0.978]
Epoch [91/120    avg_loss:0.009, val_acc:0.978]
Epoch [92/120    avg_loss:0.007, val_acc:0.978]
Epoch [93/120    avg_loss:0.009, val_acc:0.980]
Epoch [94/120    avg_loss:0.009, val_acc:0.980]
Epoch [95/120    avg_loss:0.009, val_acc:0.977]
Epoch [96/120    avg_loss:0.009, val_acc:0.978]
Epoch [97/120    avg_loss:0.009, val_acc:0.979]
Epoch [98/120    avg_loss:0.008, val_acc:0.979]
Epoch [99/120    avg_loss:0.009, val_acc:0.979]
Epoch [100/120    avg_loss:0.008, val_acc:0.979]
Epoch [101/120    avg_loss:0.014, val_acc:0.975]
Epoch [102/120    avg_loss:0.007, val_acc:0.975]
Epoch [103/120    avg_loss:0.007, val_acc:0.975]
Epoch [104/120    avg_loss:0.012, val_acc:0.975]
Epoch [105/120    avg_loss:0.008, val_acc:0.977]
Epoch [106/120    avg_loss:0.007, val_acc:0.976]
Epoch [107/120    avg_loss:0.007, val_acc:0.976]
Epoch [108/120    avg_loss:0.007, val_acc:0.976]
Epoch [109/120    avg_loss:0.009, val_acc:0.976]
Epoch [110/120    avg_loss:0.009, val_acc:0.977]
Epoch [111/120    avg_loss:0.007, val_acc:0.977]
Epoch [112/120    avg_loss:0.012, val_acc:0.977]
Epoch [113/120    avg_loss:0.006, val_acc:0.977]
Epoch [114/120    avg_loss:0.007, val_acc:0.977]
Epoch [115/120    avg_loss:0.008, val_acc:0.977]
Epoch [116/120    avg_loss:0.008, val_acc:0.977]
Epoch [117/120    avg_loss:0.006, val_acc:0.977]
Epoch [118/120    avg_loss:0.007, val_acc:0.977]
Epoch [119/120    avg_loss:0.007, val_acc:0.977]
Epoch [120/120    avg_loss:0.006, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1268    2    5    0    0    0    0    0    0    6    4    0
     0    0    0]
 [   0    0    0  738    1    0    2    0    0    1    0    2    3    0
     0    0    0]
 [   0    0    0    0  210    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    3    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    3    0    2    0    0    0    0  853   13    2    0
     0    0    0]
 [   0    0    6    2    0    0    3    0    0    1    5 2176   17    0
     0    0    0]
 [   0    0    2    2    3    0    0    0    0    0    0    5  517    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1125   13    0]
 [   0    0    0    0    0    0   12    0    0    1    0    0    0    0
    89  245    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.53929539295393

F1 scores:
[       nan 1.         0.98946547 0.98795181 0.97222222 0.99424626
 0.98266767 1.         0.99883586 0.92307692 0.98442008 0.98573046
 0.95652174 1.         0.95419847 0.80592105 0.98823529]

Kappa:
0.9719297598900822
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd776b68908>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.571, val_acc:0.502]
Epoch [2/120    avg_loss:2.139, val_acc:0.557]
Epoch [3/120    avg_loss:1.876, val_acc:0.603]
Epoch [4/120    avg_loss:1.670, val_acc:0.616]
Epoch [5/120    avg_loss:1.524, val_acc:0.607]
Epoch [6/120    avg_loss:1.369, val_acc:0.635]
Epoch [7/120    avg_loss:1.197, val_acc:0.675]
Epoch [8/120    avg_loss:1.073, val_acc:0.713]
Epoch [9/120    avg_loss:0.876, val_acc:0.731]
Epoch [10/120    avg_loss:0.951, val_acc:0.752]
Epoch [11/120    avg_loss:0.829, val_acc:0.778]
Epoch [12/120    avg_loss:0.650, val_acc:0.809]
Epoch [13/120    avg_loss:0.593, val_acc:0.802]
Epoch [14/120    avg_loss:0.505, val_acc:0.825]
Epoch [15/120    avg_loss:0.536, val_acc:0.823]
Epoch [16/120    avg_loss:0.413, val_acc:0.807]
Epoch [17/120    avg_loss:0.407, val_acc:0.864]
Epoch [18/120    avg_loss:0.400, val_acc:0.848]
Epoch [19/120    avg_loss:0.398, val_acc:0.869]
Epoch [20/120    avg_loss:0.342, val_acc:0.872]
Epoch [21/120    avg_loss:0.378, val_acc:0.822]
Epoch [22/120    avg_loss:0.352, val_acc:0.873]
Epoch [23/120    avg_loss:0.264, val_acc:0.866]
Epoch [24/120    avg_loss:0.273, val_acc:0.843]
Epoch [25/120    avg_loss:0.627, val_acc:0.803]
Epoch [26/120    avg_loss:0.389, val_acc:0.859]
Epoch [27/120    avg_loss:0.270, val_acc:0.889]
Epoch [28/120    avg_loss:0.198, val_acc:0.915]
Epoch [29/120    avg_loss:0.193, val_acc:0.903]
Epoch [30/120    avg_loss:0.166, val_acc:0.924]
Epoch [31/120    avg_loss:0.176, val_acc:0.901]
Epoch [32/120    avg_loss:0.156, val_acc:0.929]
Epoch [33/120    avg_loss:0.118, val_acc:0.915]
Epoch [34/120    avg_loss:0.153, val_acc:0.909]
Epoch [35/120    avg_loss:0.139, val_acc:0.914]
Epoch [36/120    avg_loss:0.097, val_acc:0.926]
Epoch [37/120    avg_loss:0.107, val_acc:0.933]
Epoch [38/120    avg_loss:0.115, val_acc:0.945]
Epoch [39/120    avg_loss:0.090, val_acc:0.942]
Epoch [40/120    avg_loss:0.093, val_acc:0.943]
Epoch [41/120    avg_loss:0.085, val_acc:0.946]
Epoch [42/120    avg_loss:0.070, val_acc:0.949]
Epoch [43/120    avg_loss:0.087, val_acc:0.934]
Epoch [44/120    avg_loss:0.127, val_acc:0.940]
Epoch [45/120    avg_loss:0.085, val_acc:0.955]
Epoch [46/120    avg_loss:0.057, val_acc:0.960]
Epoch [47/120    avg_loss:0.055, val_acc:0.948]
Epoch [48/120    avg_loss:0.059, val_acc:0.950]
Epoch [49/120    avg_loss:0.055, val_acc:0.955]
Epoch [50/120    avg_loss:0.051, val_acc:0.957]
Epoch [51/120    avg_loss:0.077, val_acc:0.952]
Epoch [52/120    avg_loss:0.087, val_acc:0.950]
Epoch [53/120    avg_loss:0.070, val_acc:0.947]
Epoch [54/120    avg_loss:0.064, val_acc:0.949]
Epoch [55/120    avg_loss:0.066, val_acc:0.949]
Epoch [56/120    avg_loss:0.056, val_acc:0.959]
Epoch [57/120    avg_loss:0.063, val_acc:0.947]
Epoch [58/120    avg_loss:0.050, val_acc:0.959]
Epoch [59/120    avg_loss:0.042, val_acc:0.954]
Epoch [60/120    avg_loss:0.037, val_acc:0.971]
Epoch [61/120    avg_loss:0.031, val_acc:0.973]
Epoch [62/120    avg_loss:0.025, val_acc:0.973]
Epoch [63/120    avg_loss:0.023, val_acc:0.973]
Epoch [64/120    avg_loss:0.022, val_acc:0.975]
Epoch [65/120    avg_loss:0.023, val_acc:0.975]
Epoch [66/120    avg_loss:0.023, val_acc:0.975]
Epoch [67/120    avg_loss:0.024, val_acc:0.975]
Epoch [68/120    avg_loss:0.022, val_acc:0.975]
Epoch [69/120    avg_loss:0.024, val_acc:0.971]
Epoch [70/120    avg_loss:0.021, val_acc:0.973]
Epoch [71/120    avg_loss:0.020, val_acc:0.974]
Epoch [72/120    avg_loss:0.020, val_acc:0.974]
Epoch [73/120    avg_loss:0.019, val_acc:0.974]
Epoch [74/120    avg_loss:0.024, val_acc:0.974]
Epoch [75/120    avg_loss:0.018, val_acc:0.974]
Epoch [76/120    avg_loss:0.017, val_acc:0.974]
Epoch [77/120    avg_loss:0.021, val_acc:0.976]
Epoch [78/120    avg_loss:0.019, val_acc:0.976]
Epoch [79/120    avg_loss:0.019, val_acc:0.977]
Epoch [80/120    avg_loss:0.019, val_acc:0.980]
Epoch [81/120    avg_loss:0.018, val_acc:0.978]
Epoch [82/120    avg_loss:0.019, val_acc:0.976]
Epoch [83/120    avg_loss:0.017, val_acc:0.978]
Epoch [84/120    avg_loss:0.020, val_acc:0.978]
Epoch [85/120    avg_loss:0.016, val_acc:0.980]
Epoch [86/120    avg_loss:0.018, val_acc:0.977]
Epoch [87/120    avg_loss:0.018, val_acc:0.976]
Epoch [88/120    avg_loss:0.015, val_acc:0.978]
Epoch [89/120    avg_loss:0.014, val_acc:0.978]
Epoch [90/120    avg_loss:0.021, val_acc:0.975]
Epoch [91/120    avg_loss:0.027, val_acc:0.977]
Epoch [92/120    avg_loss:0.016, val_acc:0.977]
Epoch [93/120    avg_loss:0.015, val_acc:0.978]
Epoch [94/120    avg_loss:0.022, val_acc:0.977]
Epoch [95/120    avg_loss:0.018, val_acc:0.977]
Epoch [96/120    avg_loss:0.018, val_acc:0.977]
Epoch [97/120    avg_loss:0.015, val_acc:0.979]
Epoch [98/120    avg_loss:0.014, val_acc:0.978]
Epoch [99/120    avg_loss:0.015, val_acc:0.978]
Epoch [100/120    avg_loss:0.017, val_acc:0.978]
Epoch [101/120    avg_loss:0.016, val_acc:0.978]
Epoch [102/120    avg_loss:0.017, val_acc:0.978]
Epoch [103/120    avg_loss:0.018, val_acc:0.978]
Epoch [104/120    avg_loss:0.017, val_acc:0.979]
Epoch [105/120    avg_loss:0.015, val_acc:0.979]
Epoch [106/120    avg_loss:0.012, val_acc:0.979]
Epoch [107/120    avg_loss:0.013, val_acc:0.979]
Epoch [108/120    avg_loss:0.014, val_acc:0.979]
Epoch [109/120    avg_loss:0.015, val_acc:0.979]
Epoch [110/120    avg_loss:0.015, val_acc:0.979]
Epoch [111/120    avg_loss:0.018, val_acc:0.979]
Epoch [112/120    avg_loss:0.018, val_acc:0.979]
Epoch [113/120    avg_loss:0.015, val_acc:0.979]
Epoch [114/120    avg_loss:0.018, val_acc:0.979]
Epoch [115/120    avg_loss:0.016, val_acc:0.979]
Epoch [116/120    avg_loss:0.019, val_acc:0.979]
Epoch [117/120    avg_loss:0.015, val_acc:0.979]
Epoch [118/120    avg_loss:0.014, val_acc:0.979]
Epoch [119/120    avg_loss:0.013, val_acc:0.979]
Epoch [120/120    avg_loss:0.015, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1223   19    2    0    0    0    0    0    7   34    0    0
     0    0    0]
 [   0    0    0  721    2    0    0    0    0    4    0   14    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    1    0    0  433    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    1  839   29    0    0
     1    0    0]
 [   0    0   11    0    0    0    1    0    0    2    8 2170   14    1
     0    3    0]
 [   0    0    0    4    0    0    0    0    0    0    0    6  520    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1119   19    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    71  268    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.91056910569105

F1 scores:
[       nan 0.975      0.96871287 0.96713615 0.99069767 0.99654776
 0.99167298 1.         1.         0.8372093  0.96938186 0.97222222
 0.96654275 0.99730458 0.95969125 0.83881064 0.97619048]

Kappa:
0.9647434448554623
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ad9c1d860>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.556, val_acc:0.407]
Epoch [2/120    avg_loss:2.149, val_acc:0.432]
Epoch [3/120    avg_loss:1.903, val_acc:0.572]
Epoch [4/120    avg_loss:1.692, val_acc:0.593]
Epoch [5/120    avg_loss:1.471, val_acc:0.645]
Epoch [6/120    avg_loss:1.260, val_acc:0.698]
Epoch [7/120    avg_loss:1.057, val_acc:0.692]
Epoch [8/120    avg_loss:0.938, val_acc:0.752]
Epoch [9/120    avg_loss:0.824, val_acc:0.723]
Epoch [10/120    avg_loss:0.775, val_acc:0.750]
Epoch [11/120    avg_loss:0.637, val_acc:0.778]
Epoch [12/120    avg_loss:0.574, val_acc:0.814]
Epoch [13/120    avg_loss:0.509, val_acc:0.799]
Epoch [14/120    avg_loss:0.455, val_acc:0.793]
Epoch [15/120    avg_loss:0.426, val_acc:0.811]
Epoch [16/120    avg_loss:0.368, val_acc:0.860]
Epoch [17/120    avg_loss:0.325, val_acc:0.841]
Epoch [18/120    avg_loss:0.275, val_acc:0.857]
Epoch [19/120    avg_loss:0.370, val_acc:0.856]
Epoch [20/120    avg_loss:0.313, val_acc:0.826]
Epoch [21/120    avg_loss:0.353, val_acc:0.838]
Epoch [22/120    avg_loss:0.308, val_acc:0.838]
Epoch [23/120    avg_loss:0.272, val_acc:0.849]
Epoch [24/120    avg_loss:0.231, val_acc:0.867]
Epoch [25/120    avg_loss:0.222, val_acc:0.896]
Epoch [26/120    avg_loss:0.177, val_acc:0.896]
Epoch [27/120    avg_loss:0.158, val_acc:0.907]
Epoch [28/120    avg_loss:0.156, val_acc:0.881]
Epoch [29/120    avg_loss:0.134, val_acc:0.910]
Epoch [30/120    avg_loss:0.222, val_acc:0.841]
Epoch [31/120    avg_loss:0.211, val_acc:0.839]
Epoch [32/120    avg_loss:0.218, val_acc:0.871]
Epoch [33/120    avg_loss:0.306, val_acc:0.917]
Epoch [34/120    avg_loss:0.159, val_acc:0.901]
Epoch [35/120    avg_loss:0.134, val_acc:0.914]
Epoch [36/120    avg_loss:0.135, val_acc:0.919]
Epoch [37/120    avg_loss:0.135, val_acc:0.922]
Epoch [38/120    avg_loss:0.119, val_acc:0.926]
Epoch [39/120    avg_loss:0.071, val_acc:0.944]
Epoch [40/120    avg_loss:0.077, val_acc:0.935]
Epoch [41/120    avg_loss:0.095, val_acc:0.943]
Epoch [42/120    avg_loss:0.077, val_acc:0.927]
Epoch [43/120    avg_loss:0.066, val_acc:0.932]
Epoch [44/120    avg_loss:0.076, val_acc:0.942]
Epoch [45/120    avg_loss:0.057, val_acc:0.942]
Epoch [46/120    avg_loss:0.059, val_acc:0.965]
Epoch [47/120    avg_loss:0.060, val_acc:0.947]
Epoch [48/120    avg_loss:0.062, val_acc:0.953]
Epoch [49/120    avg_loss:0.045, val_acc:0.945]
Epoch [50/120    avg_loss:0.050, val_acc:0.964]
Epoch [51/120    avg_loss:0.061, val_acc:0.931]
Epoch [52/120    avg_loss:0.065, val_acc:0.945]
Epoch [53/120    avg_loss:0.051, val_acc:0.890]
Epoch [54/120    avg_loss:0.063, val_acc:0.954]
Epoch [55/120    avg_loss:0.050, val_acc:0.958]
Epoch [56/120    avg_loss:0.040, val_acc:0.965]
Epoch [57/120    avg_loss:0.047, val_acc:0.954]
Epoch [58/120    avg_loss:0.052, val_acc:0.952]
Epoch [59/120    avg_loss:0.040, val_acc:0.960]
Epoch [60/120    avg_loss:0.033, val_acc:0.966]
Epoch [61/120    avg_loss:0.052, val_acc:0.944]
Epoch [62/120    avg_loss:0.031, val_acc:0.965]
Epoch [63/120    avg_loss:0.031, val_acc:0.952]
Epoch [64/120    avg_loss:0.038, val_acc:0.954]
Epoch [65/120    avg_loss:0.032, val_acc:0.970]
Epoch [66/120    avg_loss:0.031, val_acc:0.957]
Epoch [67/120    avg_loss:0.044, val_acc:0.964]
Epoch [68/120    avg_loss:0.039, val_acc:0.950]
Epoch [69/120    avg_loss:0.044, val_acc:0.961]
Epoch [70/120    avg_loss:0.029, val_acc:0.971]
Epoch [71/120    avg_loss:0.025, val_acc:0.967]
Epoch [72/120    avg_loss:0.024, val_acc:0.968]
Epoch [73/120    avg_loss:0.025, val_acc:0.969]
Epoch [74/120    avg_loss:0.021, val_acc:0.953]
Epoch [75/120    avg_loss:0.022, val_acc:0.968]
Epoch [76/120    avg_loss:0.028, val_acc:0.954]
Epoch [77/120    avg_loss:0.035, val_acc:0.958]
Epoch [78/120    avg_loss:0.022, val_acc:0.976]
Epoch [79/120    avg_loss:0.038, val_acc:0.967]
Epoch [80/120    avg_loss:0.019, val_acc:0.973]
Epoch [81/120    avg_loss:0.018, val_acc:0.971]
Epoch [82/120    avg_loss:0.023, val_acc:0.963]
Epoch [83/120    avg_loss:0.019, val_acc:0.968]
Epoch [84/120    avg_loss:0.024, val_acc:0.970]
Epoch [85/120    avg_loss:0.015, val_acc:0.966]
Epoch [86/120    avg_loss:0.016, val_acc:0.965]
Epoch [87/120    avg_loss:0.021, val_acc:0.967]
Epoch [88/120    avg_loss:0.015, val_acc:0.967]
Epoch [89/120    avg_loss:0.017, val_acc:0.966]
Epoch [90/120    avg_loss:0.016, val_acc:0.971]
Epoch [91/120    avg_loss:0.014, val_acc:0.973]
Epoch [92/120    avg_loss:0.011, val_acc:0.977]
Epoch [93/120    avg_loss:0.012, val_acc:0.977]
Epoch [94/120    avg_loss:0.012, val_acc:0.979]
Epoch [95/120    avg_loss:0.017, val_acc:0.979]
Epoch [96/120    avg_loss:0.008, val_acc:0.983]
Epoch [97/120    avg_loss:0.009, val_acc:0.983]
Epoch [98/120    avg_loss:0.012, val_acc:0.984]
Epoch [99/120    avg_loss:0.009, val_acc:0.984]
Epoch [100/120    avg_loss:0.009, val_acc:0.983]
Epoch [101/120    avg_loss:0.009, val_acc:0.982]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.010, val_acc:0.982]
Epoch [104/120    avg_loss:0.007, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.982]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.009, val_acc:0.984]
Epoch [108/120    avg_loss:0.009, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.985]
Epoch [110/120    avg_loss:0.008, val_acc:0.985]
Epoch [111/120    avg_loss:0.006, val_acc:0.983]
Epoch [112/120    avg_loss:0.009, val_acc:0.985]
Epoch [113/120    avg_loss:0.008, val_acc:0.985]
Epoch [114/120    avg_loss:0.008, val_acc:0.984]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.006, val_acc:0.985]
Epoch [117/120    avg_loss:0.008, val_acc:0.984]
Epoch [118/120    avg_loss:0.006, val_acc:0.984]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1257    2    2    3    0    0    0    0    5   16    0    0
     0    0    0]
 [   0    0    0  724    6    2    7    0    0    0    0    5    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    0    0    0    0    0    0    0
    10    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0    9    0    0    0    0    0    0    0  831   35    0    0
     0    0    0]
 [   0    0   12    0    0    0    0    0    0    0    6 2177   14    0
     0    1    0]
 [   0    0    0    1    1    0    0    0    0    0    5    6  518    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1118   20    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   105  242    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.88888888888889

F1 scores:
[       nan 1.         0.98088178 0.98236092 0.97931034 0.98265896
 0.99317665 1.         0.997669   0.91428571 0.96515679 0.97820714
 0.96461825 1.         0.94266442 0.79084967 0.98809524]

Kappa:
0.9644810014359331
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdb708f68d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.607, val_acc:0.440]
Epoch [2/120    avg_loss:2.225, val_acc:0.450]
Epoch [3/120    avg_loss:1.961, val_acc:0.489]
Epoch [4/120    avg_loss:1.800, val_acc:0.599]
Epoch [5/120    avg_loss:1.585, val_acc:0.622]
Epoch [6/120    avg_loss:1.457, val_acc:0.628]
Epoch [7/120    avg_loss:1.245, val_acc:0.661]
Epoch [8/120    avg_loss:1.104, val_acc:0.742]
Epoch [9/120    avg_loss:0.946, val_acc:0.757]
Epoch [10/120    avg_loss:0.889, val_acc:0.745]
Epoch [11/120    avg_loss:0.807, val_acc:0.746]
Epoch [12/120    avg_loss:0.674, val_acc:0.800]
Epoch [13/120    avg_loss:0.633, val_acc:0.806]
Epoch [14/120    avg_loss:0.521, val_acc:0.758]
Epoch [15/120    avg_loss:0.529, val_acc:0.825]
Epoch [16/120    avg_loss:0.442, val_acc:0.863]
Epoch [17/120    avg_loss:0.373, val_acc:0.776]
Epoch [18/120    avg_loss:0.335, val_acc:0.848]
Epoch [19/120    avg_loss:0.466, val_acc:0.820]
Epoch [20/120    avg_loss:0.416, val_acc:0.866]
Epoch [21/120    avg_loss:0.273, val_acc:0.877]
Epoch [22/120    avg_loss:0.304, val_acc:0.847]
Epoch [23/120    avg_loss:0.308, val_acc:0.871]
Epoch [24/120    avg_loss:0.255, val_acc:0.872]
Epoch [25/120    avg_loss:0.255, val_acc:0.886]
Epoch [26/120    avg_loss:0.420, val_acc:0.835]
Epoch [27/120    avg_loss:0.348, val_acc:0.876]
Epoch [28/120    avg_loss:0.208, val_acc:0.889]
Epoch [29/120    avg_loss:0.171, val_acc:0.910]
Epoch [30/120    avg_loss:0.153, val_acc:0.919]
Epoch [31/120    avg_loss:0.152, val_acc:0.909]
Epoch [32/120    avg_loss:0.153, val_acc:0.919]
Epoch [33/120    avg_loss:0.124, val_acc:0.930]
Epoch [34/120    avg_loss:0.118, val_acc:0.935]
Epoch [35/120    avg_loss:0.117, val_acc:0.935]
Epoch [36/120    avg_loss:0.104, val_acc:0.932]
Epoch [37/120    avg_loss:0.094, val_acc:0.931]
Epoch [38/120    avg_loss:0.103, val_acc:0.912]
Epoch [39/120    avg_loss:0.103, val_acc:0.936]
Epoch [40/120    avg_loss:0.090, val_acc:0.923]
Epoch [41/120    avg_loss:0.083, val_acc:0.948]
Epoch [42/120    avg_loss:0.074, val_acc:0.942]
Epoch [43/120    avg_loss:0.077, val_acc:0.939]
Epoch [44/120    avg_loss:0.088, val_acc:0.942]
Epoch [45/120    avg_loss:0.094, val_acc:0.940]
Epoch [46/120    avg_loss:0.110, val_acc:0.940]
Epoch [47/120    avg_loss:0.087, val_acc:0.942]
Epoch [48/120    avg_loss:0.100, val_acc:0.929]
Epoch [49/120    avg_loss:0.073, val_acc:0.932]
Epoch [50/120    avg_loss:0.074, val_acc:0.941]
Epoch [51/120    avg_loss:0.056, val_acc:0.946]
Epoch [52/120    avg_loss:0.065, val_acc:0.941]
Epoch [53/120    avg_loss:0.072, val_acc:0.949]
Epoch [54/120    avg_loss:0.077, val_acc:0.928]
Epoch [55/120    avg_loss:0.133, val_acc:0.877]
Epoch [56/120    avg_loss:0.159, val_acc:0.940]
Epoch [57/120    avg_loss:0.123, val_acc:0.903]
Epoch [58/120    avg_loss:0.161, val_acc:0.952]
Epoch [59/120    avg_loss:0.094, val_acc:0.946]
Epoch [60/120    avg_loss:0.081, val_acc:0.950]
Epoch [61/120    avg_loss:0.059, val_acc:0.940]
Epoch [62/120    avg_loss:0.065, val_acc:0.949]
Epoch [63/120    avg_loss:0.057, val_acc:0.958]
Epoch [64/120    avg_loss:0.036, val_acc:0.958]
Epoch [65/120    avg_loss:0.036, val_acc:0.953]
Epoch [66/120    avg_loss:0.040, val_acc:0.963]
Epoch [67/120    avg_loss:0.043, val_acc:0.966]
Epoch [68/120    avg_loss:0.034, val_acc:0.956]
Epoch [69/120    avg_loss:0.039, val_acc:0.968]
Epoch [70/120    avg_loss:0.033, val_acc:0.967]
Epoch [71/120    avg_loss:0.027, val_acc:0.952]
Epoch [72/120    avg_loss:0.046, val_acc:0.956]
Epoch [73/120    avg_loss:0.032, val_acc:0.963]
Epoch [74/120    avg_loss:0.033, val_acc:0.957]
Epoch [75/120    avg_loss:0.037, val_acc:0.954]
Epoch [76/120    avg_loss:0.031, val_acc:0.966]
Epoch [77/120    avg_loss:0.023, val_acc:0.965]
Epoch [78/120    avg_loss:0.031, val_acc:0.963]
Epoch [79/120    avg_loss:0.031, val_acc:0.952]
Epoch [80/120    avg_loss:0.040, val_acc:0.964]
Epoch [81/120    avg_loss:0.030, val_acc:0.954]
Epoch [82/120    avg_loss:0.026, val_acc:0.966]
Epoch [83/120    avg_loss:0.020, val_acc:0.966]
Epoch [84/120    avg_loss:0.016, val_acc:0.965]
Epoch [85/120    avg_loss:0.019, val_acc:0.967]
Epoch [86/120    avg_loss:0.014, val_acc:0.968]
Epoch [87/120    avg_loss:0.016, val_acc:0.968]
Epoch [88/120    avg_loss:0.016, val_acc:0.971]
Epoch [89/120    avg_loss:0.013, val_acc:0.970]
Epoch [90/120    avg_loss:0.016, val_acc:0.971]
Epoch [91/120    avg_loss:0.015, val_acc:0.971]
Epoch [92/120    avg_loss:0.014, val_acc:0.970]
Epoch [93/120    avg_loss:0.014, val_acc:0.971]
Epoch [94/120    avg_loss:0.015, val_acc:0.970]
Epoch [95/120    avg_loss:0.012, val_acc:0.970]
Epoch [96/120    avg_loss:0.011, val_acc:0.970]
Epoch [97/120    avg_loss:0.014, val_acc:0.969]
Epoch [98/120    avg_loss:0.011, val_acc:0.969]
Epoch [99/120    avg_loss:0.014, val_acc:0.971]
Epoch [100/120    avg_loss:0.014, val_acc:0.971]
Epoch [101/120    avg_loss:0.016, val_acc:0.969]
Epoch [102/120    avg_loss:0.014, val_acc:0.971]
Epoch [103/120    avg_loss:0.014, val_acc:0.968]
Epoch [104/120    avg_loss:0.012, val_acc:0.970]
Epoch [105/120    avg_loss:0.013, val_acc:0.971]
Epoch [106/120    avg_loss:0.011, val_acc:0.969]
Epoch [107/120    avg_loss:0.014, val_acc:0.968]
Epoch [108/120    avg_loss:0.015, val_acc:0.969]
Epoch [109/120    avg_loss:0.011, val_acc:0.968]
Epoch [110/120    avg_loss:0.013, val_acc:0.967]
Epoch [111/120    avg_loss:0.015, val_acc:0.968]
Epoch [112/120    avg_loss:0.017, val_acc:0.967]
Epoch [113/120    avg_loss:0.010, val_acc:0.970]
Epoch [114/120    avg_loss:0.014, val_acc:0.972]
Epoch [115/120    avg_loss:0.011, val_acc:0.970]
Epoch [116/120    avg_loss:0.010, val_acc:0.969]
Epoch [117/120    avg_loss:0.012, val_acc:0.966]
Epoch [118/120    avg_loss:0.011, val_acc:0.966]
Epoch [119/120    avg_loss:0.011, val_acc:0.968]
Epoch [120/120    avg_loss:0.012, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1253    0    0    7    0    0    0    1    3   21    0    0
     0    0    0]
 [   0    0    0  715    0    0    0    0    0    7    0    5   20    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0    2    0    0    2    0    0    0    1  858    4    0    0
     0    8    0]
 [   0    0    1    0    0    1    0    0    0    1   29 2176    0    1
     0    1    0]
 [   0    0    0    3    0    0    0    0    0    0    0    2  525    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0   21    0    0    0    0    0    0    0    0
  1105   13    0]
 [   0    0    0    0    0    0   29    0    0    0    0    0    0    0
    15  303    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.6910569105691

F1 scores:
[       nan 0.96202532 0.98583792 0.97610922 1.         0.96329255
 0.97840655 1.         1.         0.71111111 0.97113752 0.9848382
 0.96863469 0.99459459 0.97701149 0.90178571 0.97005988]

Kappa:
0.9736867637159966
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faea1e89898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.619, val_acc:0.467]
Epoch [2/120    avg_loss:2.209, val_acc:0.533]
Epoch [3/120    avg_loss:1.954, val_acc:0.515]
Epoch [4/120    avg_loss:1.740, val_acc:0.599]
Epoch [5/120    avg_loss:1.597, val_acc:0.621]
Epoch [6/120    avg_loss:1.437, val_acc:0.649]
Epoch [7/120    avg_loss:1.252, val_acc:0.677]
Epoch [8/120    avg_loss:1.100, val_acc:0.686]
Epoch [9/120    avg_loss:1.001, val_acc:0.720]
Epoch [10/120    avg_loss:0.896, val_acc:0.738]
Epoch [11/120    avg_loss:0.765, val_acc:0.754]
Epoch [12/120    avg_loss:0.730, val_acc:0.773]
Epoch [13/120    avg_loss:0.683, val_acc:0.809]
Epoch [14/120    avg_loss:0.532, val_acc:0.797]
Epoch [15/120    avg_loss:0.535, val_acc:0.835]
Epoch [16/120    avg_loss:0.531, val_acc:0.825]
Epoch [17/120    avg_loss:0.414, val_acc:0.851]
Epoch [18/120    avg_loss:0.432, val_acc:0.826]
Epoch [19/120    avg_loss:0.370, val_acc:0.850]
Epoch [20/120    avg_loss:0.334, val_acc:0.874]
Epoch [21/120    avg_loss:0.316, val_acc:0.887]
Epoch [22/120    avg_loss:0.237, val_acc:0.862]
Epoch [23/120    avg_loss:0.227, val_acc:0.904]
Epoch [24/120    avg_loss:0.220, val_acc:0.916]
Epoch [25/120    avg_loss:0.206, val_acc:0.891]
Epoch [26/120    avg_loss:0.202, val_acc:0.901]
Epoch [27/120    avg_loss:0.154, val_acc:0.923]
Epoch [28/120    avg_loss:0.168, val_acc:0.907]
Epoch [29/120    avg_loss:0.441, val_acc:0.844]
Epoch [30/120    avg_loss:0.387, val_acc:0.858]
Epoch [31/120    avg_loss:0.319, val_acc:0.901]
Epoch [32/120    avg_loss:0.182, val_acc:0.916]
Epoch [33/120    avg_loss:0.173, val_acc:0.915]
Epoch [34/120    avg_loss:0.124, val_acc:0.914]
Epoch [35/120    avg_loss:0.138, val_acc:0.904]
Epoch [36/120    avg_loss:0.117, val_acc:0.936]
Epoch [37/120    avg_loss:0.097, val_acc:0.924]
Epoch [38/120    avg_loss:0.121, val_acc:0.932]
Epoch [39/120    avg_loss:0.086, val_acc:0.947]
Epoch [40/120    avg_loss:0.085, val_acc:0.924]
Epoch [41/120    avg_loss:0.090, val_acc:0.919]
Epoch [42/120    avg_loss:0.082, val_acc:0.949]
Epoch [43/120    avg_loss:0.092, val_acc:0.929]
Epoch [44/120    avg_loss:0.099, val_acc:0.934]
Epoch [45/120    avg_loss:0.088, val_acc:0.938]
Epoch [46/120    avg_loss:0.068, val_acc:0.936]
Epoch [47/120    avg_loss:0.088, val_acc:0.928]
Epoch [48/120    avg_loss:0.063, val_acc:0.953]
Epoch [49/120    avg_loss:0.063, val_acc:0.938]
Epoch [50/120    avg_loss:0.111, val_acc:0.949]
Epoch [51/120    avg_loss:0.181, val_acc:0.942]
Epoch [52/120    avg_loss:0.075, val_acc:0.950]
Epoch [53/120    avg_loss:0.057, val_acc:0.952]
Epoch [54/120    avg_loss:0.095, val_acc:0.938]
Epoch [55/120    avg_loss:0.092, val_acc:0.943]
Epoch [56/120    avg_loss:0.061, val_acc:0.956]
Epoch [57/120    avg_loss:0.070, val_acc:0.944]
Epoch [58/120    avg_loss:0.043, val_acc:0.969]
Epoch [59/120    avg_loss:0.041, val_acc:0.964]
Epoch [60/120    avg_loss:0.033, val_acc:0.962]
Epoch [61/120    avg_loss:0.037, val_acc:0.965]
Epoch [62/120    avg_loss:0.054, val_acc:0.958]
Epoch [63/120    avg_loss:0.045, val_acc:0.965]
Epoch [64/120    avg_loss:0.031, val_acc:0.966]
Epoch [65/120    avg_loss:0.062, val_acc:0.953]
Epoch [66/120    avg_loss:0.078, val_acc:0.964]
Epoch [67/120    avg_loss:0.082, val_acc:0.951]
Epoch [68/120    avg_loss:0.033, val_acc:0.959]
Epoch [69/120    avg_loss:0.031, val_acc:0.956]
Epoch [70/120    avg_loss:0.026, val_acc:0.968]
Epoch [71/120    avg_loss:0.028, val_acc:0.968]
Epoch [72/120    avg_loss:0.022, val_acc:0.975]
Epoch [73/120    avg_loss:0.015, val_acc:0.976]
Epoch [74/120    avg_loss:0.016, val_acc:0.977]
Epoch [75/120    avg_loss:0.015, val_acc:0.977]
Epoch [76/120    avg_loss:0.014, val_acc:0.975]
Epoch [77/120    avg_loss:0.018, val_acc:0.975]
Epoch [78/120    avg_loss:0.015, val_acc:0.976]
Epoch [79/120    avg_loss:0.015, val_acc:0.976]
Epoch [80/120    avg_loss:0.014, val_acc:0.976]
Epoch [81/120    avg_loss:0.011, val_acc:0.974]
Epoch [82/120    avg_loss:0.016, val_acc:0.974]
Epoch [83/120    avg_loss:0.013, val_acc:0.975]
Epoch [84/120    avg_loss:0.017, val_acc:0.972]
Epoch [85/120    avg_loss:0.013, val_acc:0.974]
Epoch [86/120    avg_loss:0.015, val_acc:0.976]
Epoch [87/120    avg_loss:0.012, val_acc:0.976]
Epoch [88/120    avg_loss:0.012, val_acc:0.974]
Epoch [89/120    avg_loss:0.018, val_acc:0.974]
Epoch [90/120    avg_loss:0.013, val_acc:0.974]
Epoch [91/120    avg_loss:0.011, val_acc:0.974]
Epoch [92/120    avg_loss:0.011, val_acc:0.974]
Epoch [93/120    avg_loss:0.013, val_acc:0.974]
Epoch [94/120    avg_loss:0.014, val_acc:0.974]
Epoch [95/120    avg_loss:0.011, val_acc:0.974]
Epoch [96/120    avg_loss:0.010, val_acc:0.974]
Epoch [97/120    avg_loss:0.012, val_acc:0.974]
Epoch [98/120    avg_loss:0.014, val_acc:0.974]
Epoch [99/120    avg_loss:0.013, val_acc:0.974]
Epoch [100/120    avg_loss:0.011, val_acc:0.974]
Epoch [101/120    avg_loss:0.012, val_acc:0.974]
Epoch [102/120    avg_loss:0.015, val_acc:0.974]
Epoch [103/120    avg_loss:0.013, val_acc:0.974]
Epoch [104/120    avg_loss:0.013, val_acc:0.974]
Epoch [105/120    avg_loss:0.010, val_acc:0.974]
Epoch [106/120    avg_loss:0.012, val_acc:0.974]
Epoch [107/120    avg_loss:0.012, val_acc:0.974]
Epoch [108/120    avg_loss:0.013, val_acc:0.974]
Epoch [109/120    avg_loss:0.011, val_acc:0.974]
Epoch [110/120    avg_loss:0.012, val_acc:0.974]
Epoch [111/120    avg_loss:0.014, val_acc:0.974]
Epoch [112/120    avg_loss:0.013, val_acc:0.974]
Epoch [113/120    avg_loss:0.014, val_acc:0.974]
Epoch [114/120    avg_loss:0.015, val_acc:0.974]
Epoch [115/120    avg_loss:0.011, val_acc:0.974]
Epoch [116/120    avg_loss:0.013, val_acc:0.974]
Epoch [117/120    avg_loss:0.017, val_acc:0.974]
Epoch [118/120    avg_loss:0.013, val_acc:0.974]
Epoch [119/120    avg_loss:0.012, val_acc:0.974]
Epoch [120/120    avg_loss:0.011, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    4 1246    4    0    1    4    0    0    0    4   22    0    0
     0    0    0]
 [   0    0    0  726    0   11    0    0    0    2    0    4    4    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    0    0    0    0    0  842   21    0    0
     0    2    0]
 [   0    0    3    0    0    2    2    0    0    0    7 2177   18    0
     1    0    0]
 [   0    0    0    5    0    0    0    0    0    0    9    5  511    0
     1    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1131    6    0]
 [   0    0    0    0    0    0   21    0    0    0    0    0    0    0
    18  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.78861788617886

F1 scores:
[       nan 0.95348837 0.97955975 0.97909643 0.99764706 0.98070375
 0.97834205 0.98039216 0.99883856 0.94736842 0.9689298  0.98063063
 0.95603368 1.         0.98648059 0.92771084 0.97619048]

Kappa:
0.9747811633743214
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9fe58dd898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.598, val_acc:0.432]
Epoch [2/120    avg_loss:2.172, val_acc:0.496]
Epoch [3/120    avg_loss:1.899, val_acc:0.552]
Epoch [4/120    avg_loss:1.694, val_acc:0.576]
Epoch [5/120    avg_loss:1.564, val_acc:0.626]
Epoch [6/120    avg_loss:1.418, val_acc:0.652]
Epoch [7/120    avg_loss:1.305, val_acc:0.671]
Epoch [8/120    avg_loss:1.161, val_acc:0.726]
Epoch [9/120    avg_loss:0.978, val_acc:0.714]
Epoch [10/120    avg_loss:0.909, val_acc:0.752]
Epoch [11/120    avg_loss:0.764, val_acc:0.771]
Epoch [12/120    avg_loss:0.630, val_acc:0.833]
Epoch [13/120    avg_loss:0.629, val_acc:0.799]
Epoch [14/120    avg_loss:0.647, val_acc:0.802]
Epoch [15/120    avg_loss:0.480, val_acc:0.854]
Epoch [16/120    avg_loss:0.407, val_acc:0.861]
Epoch [17/120    avg_loss:0.499, val_acc:0.852]
Epoch [18/120    avg_loss:0.370, val_acc:0.880]
Epoch [19/120    avg_loss:0.325, val_acc:0.893]
Epoch [20/120    avg_loss:0.271, val_acc:0.898]
Epoch [21/120    avg_loss:0.268, val_acc:0.874]
Epoch [22/120    avg_loss:0.248, val_acc:0.907]
Epoch [23/120    avg_loss:0.201, val_acc:0.900]
Epoch [24/120    avg_loss:0.215, val_acc:0.901]
Epoch [25/120    avg_loss:0.203, val_acc:0.912]
Epoch [26/120    avg_loss:0.191, val_acc:0.907]
Epoch [27/120    avg_loss:0.155, val_acc:0.925]
Epoch [28/120    avg_loss:0.165, val_acc:0.905]
Epoch [29/120    avg_loss:0.137, val_acc:0.931]
Epoch [30/120    avg_loss:0.100, val_acc:0.938]
Epoch [31/120    avg_loss:0.121, val_acc:0.927]
Epoch [32/120    avg_loss:0.092, val_acc:0.952]
Epoch [33/120    avg_loss:0.118, val_acc:0.953]
Epoch [34/120    avg_loss:0.123, val_acc:0.902]
Epoch [35/120    avg_loss:0.196, val_acc:0.856]
Epoch [36/120    avg_loss:0.339, val_acc:0.901]
Epoch [37/120    avg_loss:0.148, val_acc:0.948]
Epoch [38/120    avg_loss:0.115, val_acc:0.943]
Epoch [39/120    avg_loss:0.098, val_acc:0.958]
Epoch [40/120    avg_loss:0.097, val_acc:0.945]
Epoch [41/120    avg_loss:0.080, val_acc:0.948]
Epoch [42/120    avg_loss:0.084, val_acc:0.935]
Epoch [43/120    avg_loss:0.096, val_acc:0.930]
Epoch [44/120    avg_loss:0.074, val_acc:0.958]
Epoch [45/120    avg_loss:0.073, val_acc:0.955]
Epoch [46/120    avg_loss:0.096, val_acc:0.889]
Epoch [47/120    avg_loss:0.154, val_acc:0.936]
Epoch [48/120    avg_loss:0.096, val_acc:0.963]
Epoch [49/120    avg_loss:0.069, val_acc:0.968]
Epoch [50/120    avg_loss:0.087, val_acc:0.920]
Epoch [51/120    avg_loss:0.067, val_acc:0.949]
Epoch [52/120    avg_loss:0.056, val_acc:0.960]
Epoch [53/120    avg_loss:0.045, val_acc:0.970]
Epoch [54/120    avg_loss:0.053, val_acc:0.966]
Epoch [55/120    avg_loss:0.039, val_acc:0.958]
Epoch [56/120    avg_loss:0.040, val_acc:0.968]
Epoch [57/120    avg_loss:0.041, val_acc:0.963]
Epoch [58/120    avg_loss:0.051, val_acc:0.931]
Epoch [59/120    avg_loss:0.075, val_acc:0.961]
Epoch [60/120    avg_loss:0.097, val_acc:0.961]
Epoch [61/120    avg_loss:0.051, val_acc:0.961]
Epoch [62/120    avg_loss:0.044, val_acc:0.964]
Epoch [63/120    avg_loss:0.030, val_acc:0.967]
Epoch [64/120    avg_loss:0.043, val_acc:0.952]
Epoch [65/120    avg_loss:0.033, val_acc:0.978]
Epoch [66/120    avg_loss:0.027, val_acc:0.971]
Epoch [67/120    avg_loss:0.029, val_acc:0.972]
Epoch [68/120    avg_loss:0.023, val_acc:0.978]
Epoch [69/120    avg_loss:0.026, val_acc:0.977]
Epoch [70/120    avg_loss:0.028, val_acc:0.974]
Epoch [71/120    avg_loss:0.036, val_acc:0.976]
Epoch [72/120    avg_loss:0.023, val_acc:0.980]
Epoch [73/120    avg_loss:0.025, val_acc:0.966]
Epoch [74/120    avg_loss:0.022, val_acc:0.983]
Epoch [75/120    avg_loss:0.018, val_acc:0.981]
Epoch [76/120    avg_loss:0.016, val_acc:0.980]
Epoch [77/120    avg_loss:0.019, val_acc:0.982]
Epoch [78/120    avg_loss:0.014, val_acc:0.980]
Epoch [79/120    avg_loss:0.019, val_acc:0.983]
Epoch [80/120    avg_loss:0.022, val_acc:0.984]
Epoch [81/120    avg_loss:0.017, val_acc:0.981]
Epoch [82/120    avg_loss:0.019, val_acc:0.984]
Epoch [83/120    avg_loss:0.015, val_acc:0.983]
Epoch [84/120    avg_loss:0.019, val_acc:0.978]
Epoch [85/120    avg_loss:0.015, val_acc:0.976]
Epoch [86/120    avg_loss:0.015, val_acc:0.977]
Epoch [87/120    avg_loss:0.017, val_acc:0.961]
Epoch [88/120    avg_loss:0.027, val_acc:0.983]
Epoch [89/120    avg_loss:0.015, val_acc:0.985]
Epoch [90/120    avg_loss:0.022, val_acc:0.975]
Epoch [91/120    avg_loss:0.014, val_acc:0.991]
Epoch [92/120    avg_loss:0.010, val_acc:0.984]
Epoch [93/120    avg_loss:0.012, val_acc:0.979]
Epoch [94/120    avg_loss:0.026, val_acc:0.971]
Epoch [95/120    avg_loss:0.160, val_acc:0.926]
Epoch [96/120    avg_loss:0.137, val_acc:0.975]
Epoch [97/120    avg_loss:0.039, val_acc:0.975]
Epoch [98/120    avg_loss:0.033, val_acc:0.965]
Epoch [99/120    avg_loss:0.024, val_acc:0.982]
Epoch [100/120    avg_loss:0.016, val_acc:0.979]
Epoch [101/120    avg_loss:0.012, val_acc:0.986]
Epoch [102/120    avg_loss:0.016, val_acc:0.984]
Epoch [103/120    avg_loss:0.050, val_acc:0.978]
Epoch [104/120    avg_loss:0.020, val_acc:0.982]
Epoch [105/120    avg_loss:0.014, val_acc:0.983]
Epoch [106/120    avg_loss:0.011, val_acc:0.979]
Epoch [107/120    avg_loss:0.011, val_acc:0.980]
Epoch [108/120    avg_loss:0.014, val_acc:0.982]
Epoch [109/120    avg_loss:0.011, val_acc:0.981]
Epoch [110/120    avg_loss:0.014, val_acc:0.981]
Epoch [111/120    avg_loss:0.012, val_acc:0.983]
Epoch [112/120    avg_loss:0.013, val_acc:0.982]
Epoch [113/120    avg_loss:0.014, val_acc:0.983]
Epoch [114/120    avg_loss:0.014, val_acc:0.983]
Epoch [115/120    avg_loss:0.010, val_acc:0.982]
Epoch [116/120    avg_loss:0.012, val_acc:0.981]
Epoch [117/120    avg_loss:0.009, val_acc:0.981]
Epoch [118/120    avg_loss:0.014, val_acc:0.981]
Epoch [119/120    avg_loss:0.013, val_acc:0.981]
Epoch [120/120    avg_loss:0.012, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1261    0    0    2    1    0    0    0    7   12    0    0
     0    2    0]
 [   0    0    0  730    3    0    0    0    0    1    0    0    8    5
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    3    0    0    0  850   20    0    0
     0    1    0]
 [   0    0    5    0    0    0    1    0    0    1    3 2194    2    2
     1    1    0]
 [   0    0    0    2    0    0    0    0    0    0    0    0  530    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    21  315    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.49322493224932

F1 scores:
[       nan 0.96202532 0.98785742 0.98715348 0.99300699 0.99424626
 0.98568199 1.         1.         0.9        0.97869891 0.9889565
 0.98604651 0.98143236 0.98429319 0.93057607 0.98224852]

Kappa:
0.9828174821504624
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1fb49a5898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.627, val_acc:0.423]
Epoch [2/120    avg_loss:2.199, val_acc:0.459]
Epoch [3/120    avg_loss:1.891, val_acc:0.548]
Epoch [4/120    avg_loss:1.721, val_acc:0.601]
Epoch [5/120    avg_loss:1.485, val_acc:0.603]
Epoch [6/120    avg_loss:1.280, val_acc:0.632]
Epoch [7/120    avg_loss:1.142, val_acc:0.699]
Epoch [8/120    avg_loss:1.064, val_acc:0.695]
Epoch [9/120    avg_loss:0.927, val_acc:0.718]
Epoch [10/120    avg_loss:0.789, val_acc:0.779]
Epoch [11/120    avg_loss:0.759, val_acc:0.784]
Epoch [12/120    avg_loss:0.744, val_acc:0.744]
Epoch [13/120    avg_loss:0.648, val_acc:0.800]
Epoch [14/120    avg_loss:0.530, val_acc:0.820]
Epoch [15/120    avg_loss:0.429, val_acc:0.847]
Epoch [16/120    avg_loss:0.401, val_acc:0.818]
Epoch [17/120    avg_loss:0.362, val_acc:0.834]
Epoch [18/120    avg_loss:0.376, val_acc:0.730]
Epoch [19/120    avg_loss:0.393, val_acc:0.851]
Epoch [20/120    avg_loss:0.298, val_acc:0.868]
Epoch [21/120    avg_loss:0.236, val_acc:0.863]
Epoch [22/120    avg_loss:0.268, val_acc:0.907]
Epoch [23/120    avg_loss:0.257, val_acc:0.849]
Epoch [24/120    avg_loss:0.266, val_acc:0.881]
Epoch [25/120    avg_loss:0.205, val_acc:0.890]
Epoch [26/120    avg_loss:0.215, val_acc:0.847]
Epoch [27/120    avg_loss:0.228, val_acc:0.870]
Epoch [28/120    avg_loss:0.196, val_acc:0.882]
Epoch [29/120    avg_loss:0.209, val_acc:0.900]
Epoch [30/120    avg_loss:0.221, val_acc:0.907]
Epoch [31/120    avg_loss:0.178, val_acc:0.901]
Epoch [32/120    avg_loss:0.129, val_acc:0.918]
Epoch [33/120    avg_loss:0.116, val_acc:0.938]
Epoch [34/120    avg_loss:0.096, val_acc:0.933]
Epoch [35/120    avg_loss:0.112, val_acc:0.896]
Epoch [36/120    avg_loss:0.095, val_acc:0.938]
Epoch [37/120    avg_loss:0.086, val_acc:0.938]
Epoch [38/120    avg_loss:0.109, val_acc:0.926]
Epoch [39/120    avg_loss:0.117, val_acc:0.926]
Epoch [40/120    avg_loss:0.113, val_acc:0.928]
Epoch [41/120    avg_loss:0.098, val_acc:0.944]
Epoch [42/120    avg_loss:0.088, val_acc:0.943]
Epoch [43/120    avg_loss:0.068, val_acc:0.940]
Epoch [44/120    avg_loss:0.062, val_acc:0.951]
Epoch [45/120    avg_loss:0.055, val_acc:0.955]
Epoch [46/120    avg_loss:0.070, val_acc:0.948]
Epoch [47/120    avg_loss:0.054, val_acc:0.954]
Epoch [48/120    avg_loss:0.050, val_acc:0.949]
Epoch [49/120    avg_loss:0.049, val_acc:0.952]
Epoch [50/120    avg_loss:0.052, val_acc:0.959]
Epoch [51/120    avg_loss:0.041, val_acc:0.951]
Epoch [52/120    avg_loss:0.057, val_acc:0.949]
Epoch [53/120    avg_loss:0.046, val_acc:0.955]
Epoch [54/120    avg_loss:0.046, val_acc:0.959]
Epoch [55/120    avg_loss:0.034, val_acc:0.960]
Epoch [56/120    avg_loss:0.032, val_acc:0.952]
Epoch [57/120    avg_loss:0.076, val_acc:0.953]
Epoch [58/120    avg_loss:0.069, val_acc:0.945]
Epoch [59/120    avg_loss:0.054, val_acc:0.965]
Epoch [60/120    avg_loss:0.037, val_acc:0.958]
Epoch [61/120    avg_loss:0.032, val_acc:0.961]
Epoch [62/120    avg_loss:0.027, val_acc:0.960]
Epoch [63/120    avg_loss:0.031, val_acc:0.959]
Epoch [64/120    avg_loss:0.030, val_acc:0.968]
Epoch [65/120    avg_loss:0.039, val_acc:0.970]
Epoch [66/120    avg_loss:0.038, val_acc:0.964]
Epoch [67/120    avg_loss:0.067, val_acc:0.953]
Epoch [68/120    avg_loss:0.045, val_acc:0.961]
Epoch [69/120    avg_loss:0.042, val_acc:0.927]
Epoch [70/120    avg_loss:0.065, val_acc:0.940]
Epoch [71/120    avg_loss:0.081, val_acc:0.921]
Epoch [72/120    avg_loss:0.114, val_acc:0.950]
Epoch [73/120    avg_loss:0.046, val_acc:0.953]
Epoch [74/120    avg_loss:0.051, val_acc:0.967]
Epoch [75/120    avg_loss:0.033, val_acc:0.969]
Epoch [76/120    avg_loss:0.056, val_acc:0.924]
Epoch [77/120    avg_loss:0.117, val_acc:0.933]
Epoch [78/120    avg_loss:0.052, val_acc:0.961]
Epoch [79/120    avg_loss:0.035, val_acc:0.971]
Epoch [80/120    avg_loss:0.028, val_acc:0.971]
Epoch [81/120    avg_loss:0.024, val_acc:0.972]
Epoch [82/120    avg_loss:0.026, val_acc:0.969]
Epoch [83/120    avg_loss:0.033, val_acc:0.970]
Epoch [84/120    avg_loss:0.024, val_acc:0.970]
Epoch [85/120    avg_loss:0.023, val_acc:0.975]
Epoch [86/120    avg_loss:0.024, val_acc:0.977]
Epoch [87/120    avg_loss:0.018, val_acc:0.976]
Epoch [88/120    avg_loss:0.028, val_acc:0.976]
Epoch [89/120    avg_loss:0.026, val_acc:0.974]
Epoch [90/120    avg_loss:0.024, val_acc:0.973]
Epoch [91/120    avg_loss:0.018, val_acc:0.974]
Epoch [92/120    avg_loss:0.022, val_acc:0.973]
Epoch [93/120    avg_loss:0.022, val_acc:0.973]
Epoch [94/120    avg_loss:0.021, val_acc:0.973]
Epoch [95/120    avg_loss:0.018, val_acc:0.974]
Epoch [96/120    avg_loss:0.018, val_acc:0.973]
Epoch [97/120    avg_loss:0.018, val_acc:0.973]
Epoch [98/120    avg_loss:0.017, val_acc:0.975]
Epoch [99/120    avg_loss:0.021, val_acc:0.975]
Epoch [100/120    avg_loss:0.016, val_acc:0.976]
Epoch [101/120    avg_loss:0.015, val_acc:0.976]
Epoch [102/120    avg_loss:0.016, val_acc:0.976]
Epoch [103/120    avg_loss:0.016, val_acc:0.976]
Epoch [104/120    avg_loss:0.019, val_acc:0.976]
Epoch [105/120    avg_loss:0.018, val_acc:0.976]
Epoch [106/120    avg_loss:0.013, val_acc:0.976]
Epoch [107/120    avg_loss:0.013, val_acc:0.976]
Epoch [108/120    avg_loss:0.015, val_acc:0.975]
Epoch [109/120    avg_loss:0.014, val_acc:0.976]
Epoch [110/120    avg_loss:0.014, val_acc:0.976]
Epoch [111/120    avg_loss:0.018, val_acc:0.976]
Epoch [112/120    avg_loss:0.019, val_acc:0.976]
Epoch [113/120    avg_loss:0.016, val_acc:0.976]
Epoch [114/120    avg_loss:0.017, val_acc:0.975]
Epoch [115/120    avg_loss:0.016, val_acc:0.975]
Epoch [116/120    avg_loss:0.019, val_acc:0.975]
Epoch [117/120    avg_loss:0.015, val_acc:0.975]
Epoch [118/120    avg_loss:0.022, val_acc:0.975]
Epoch [119/120    avg_loss:0.018, val_acc:0.975]
Epoch [120/120    avg_loss:0.015, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1256    1    0    0    0    0    0    0    8   20    0    0
     0    0    0]
 [   0    0    1  726    4    0    0    0    0    2    0    3    8    2
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    1    4    0    0    0    2    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    2    0    0    0    3  849   16    0    0
     0    1    0]
 [   0    0    4    0    0    0    0    0    0    0   16 2176   12    2
     0    0    0]
 [   0    0    0    7    4    1    0    0    0    0    0    0  520    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1121   17    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    56  290    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.65853658536585

F1 scores:
[       nan 0.96385542 0.98509804 0.98041864 0.98156682 0.98493627
 0.99619193 0.92592593 0.997669   0.87804878 0.97028571 0.98261459
 0.96744186 0.98930481 0.96554694 0.88549618 0.98809524]

Kappa:
0.9732969942313547
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff942a728d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.644, val_acc:0.370]
Epoch [2/120    avg_loss:2.218, val_acc:0.445]
Epoch [3/120    avg_loss:1.928, val_acc:0.558]
Epoch [4/120    avg_loss:1.743, val_acc:0.542]
Epoch [5/120    avg_loss:1.509, val_acc:0.599]
Epoch [6/120    avg_loss:1.282, val_acc:0.658]
Epoch [7/120    avg_loss:1.086, val_acc:0.633]
Epoch [8/120    avg_loss:0.915, val_acc:0.726]
Epoch [9/120    avg_loss:0.859, val_acc:0.731]
Epoch [10/120    avg_loss:0.708, val_acc:0.779]
Epoch [11/120    avg_loss:0.654, val_acc:0.811]
Epoch [12/120    avg_loss:0.679, val_acc:0.804]
Epoch [13/120    avg_loss:0.620, val_acc:0.810]
Epoch [14/120    avg_loss:0.458, val_acc:0.807]
Epoch [15/120    avg_loss:0.403, val_acc:0.878]
Epoch [16/120    avg_loss:0.367, val_acc:0.865]
Epoch [17/120    avg_loss:0.387, val_acc:0.855]
Epoch [18/120    avg_loss:0.313, val_acc:0.888]
Epoch [19/120    avg_loss:0.264, val_acc:0.881]
Epoch [20/120    avg_loss:0.279, val_acc:0.910]
Epoch [21/120    avg_loss:0.338, val_acc:0.897]
Epoch [22/120    avg_loss:0.220, val_acc:0.910]
Epoch [23/120    avg_loss:0.173, val_acc:0.892]
Epoch [24/120    avg_loss:0.226, val_acc:0.891]
Epoch [25/120    avg_loss:0.190, val_acc:0.904]
Epoch [26/120    avg_loss:0.186, val_acc:0.904]
Epoch [27/120    avg_loss:0.173, val_acc:0.897]
Epoch [28/120    avg_loss:0.207, val_acc:0.922]
Epoch [29/120    avg_loss:0.179, val_acc:0.915]
Epoch [30/120    avg_loss:0.122, val_acc:0.906]
Epoch [31/120    avg_loss:0.150, val_acc:0.921]
Epoch [32/120    avg_loss:0.093, val_acc:0.930]
Epoch [33/120    avg_loss:0.102, val_acc:0.951]
Epoch [34/120    avg_loss:0.088, val_acc:0.946]
Epoch [35/120    avg_loss:0.090, val_acc:0.961]
Epoch [36/120    avg_loss:0.084, val_acc:0.957]
Epoch [37/120    avg_loss:0.073, val_acc:0.940]
Epoch [38/120    avg_loss:0.080, val_acc:0.958]
Epoch [39/120    avg_loss:0.067, val_acc:0.958]
Epoch [40/120    avg_loss:0.095, val_acc:0.939]
Epoch [41/120    avg_loss:0.069, val_acc:0.972]
Epoch [42/120    avg_loss:0.062, val_acc:0.967]
Epoch [43/120    avg_loss:0.060, val_acc:0.959]
Epoch [44/120    avg_loss:0.112, val_acc:0.924]
Epoch [45/120    avg_loss:0.093, val_acc:0.958]
Epoch [46/120    avg_loss:0.061, val_acc:0.948]
Epoch [47/120    avg_loss:0.361, val_acc:0.914]
Epoch [48/120    avg_loss:0.127, val_acc:0.935]
Epoch [49/120    avg_loss:0.090, val_acc:0.957]
Epoch [50/120    avg_loss:0.080, val_acc:0.957]
Epoch [51/120    avg_loss:0.072, val_acc:0.965]
Epoch [52/120    avg_loss:0.052, val_acc:0.970]
Epoch [53/120    avg_loss:0.083, val_acc:0.951]
Epoch [54/120    avg_loss:0.057, val_acc:0.964]
Epoch [55/120    avg_loss:0.046, val_acc:0.975]
Epoch [56/120    avg_loss:0.036, val_acc:0.979]
Epoch [57/120    avg_loss:0.029, val_acc:0.979]
Epoch [58/120    avg_loss:0.033, val_acc:0.976]
Epoch [59/120    avg_loss:0.039, val_acc:0.978]
Epoch [60/120    avg_loss:0.031, val_acc:0.977]
Epoch [61/120    avg_loss:0.029, val_acc:0.980]
Epoch [62/120    avg_loss:0.028, val_acc:0.980]
Epoch [63/120    avg_loss:0.029, val_acc:0.977]
Epoch [64/120    avg_loss:0.026, val_acc:0.981]
Epoch [65/120    avg_loss:0.030, val_acc:0.982]
Epoch [66/120    avg_loss:0.027, val_acc:0.981]
Epoch [67/120    avg_loss:0.026, val_acc:0.981]
Epoch [68/120    avg_loss:0.027, val_acc:0.981]
Epoch [69/120    avg_loss:0.027, val_acc:0.981]
Epoch [70/120    avg_loss:0.024, val_acc:0.980]
Epoch [71/120    avg_loss:0.030, val_acc:0.982]
Epoch [72/120    avg_loss:0.023, val_acc:0.982]
Epoch [73/120    avg_loss:0.023, val_acc:0.982]
Epoch [74/120    avg_loss:0.028, val_acc:0.981]
Epoch [75/120    avg_loss:0.021, val_acc:0.979]
Epoch [76/120    avg_loss:0.026, val_acc:0.980]
Epoch [77/120    avg_loss:0.029, val_acc:0.980]
Epoch [78/120    avg_loss:0.024, val_acc:0.981]
Epoch [79/120    avg_loss:0.024, val_acc:0.982]
Epoch [80/120    avg_loss:0.023, val_acc:0.981]
Epoch [81/120    avg_loss:0.024, val_acc:0.982]
Epoch [82/120    avg_loss:0.023, val_acc:0.982]
Epoch [83/120    avg_loss:0.023, val_acc:0.980]
Epoch [84/120    avg_loss:0.024, val_acc:0.983]
Epoch [85/120    avg_loss:0.023, val_acc:0.982]
Epoch [86/120    avg_loss:0.025, val_acc:0.981]
Epoch [87/120    avg_loss:0.025, val_acc:0.982]
Epoch [88/120    avg_loss:0.023, val_acc:0.980]
Epoch [89/120    avg_loss:0.022, val_acc:0.981]
Epoch [90/120    avg_loss:0.021, val_acc:0.981]
Epoch [91/120    avg_loss:0.024, val_acc:0.982]
Epoch [92/120    avg_loss:0.022, val_acc:0.982]
Epoch [93/120    avg_loss:0.021, val_acc:0.979]
Epoch [94/120    avg_loss:0.020, val_acc:0.979]
Epoch [95/120    avg_loss:0.020, val_acc:0.981]
Epoch [96/120    avg_loss:0.019, val_acc:0.981]
Epoch [97/120    avg_loss:0.019, val_acc:0.980]
Epoch [98/120    avg_loss:0.022, val_acc:0.980]
Epoch [99/120    avg_loss:0.024, val_acc:0.980]
Epoch [100/120    avg_loss:0.017, val_acc:0.981]
Epoch [101/120    avg_loss:0.022, val_acc:0.980]
Epoch [102/120    avg_loss:0.020, val_acc:0.982]
Epoch [103/120    avg_loss:0.020, val_acc:0.981]
Epoch [104/120    avg_loss:0.018, val_acc:0.981]
Epoch [105/120    avg_loss:0.017, val_acc:0.980]
Epoch [106/120    avg_loss:0.021, val_acc:0.981]
Epoch [107/120    avg_loss:0.020, val_acc:0.981]
Epoch [108/120    avg_loss:0.019, val_acc:0.981]
Epoch [109/120    avg_loss:0.022, val_acc:0.980]
Epoch [110/120    avg_loss:0.019, val_acc:0.980]
Epoch [111/120    avg_loss:0.022, val_acc:0.980]
Epoch [112/120    avg_loss:0.022, val_acc:0.980]
Epoch [113/120    avg_loss:0.022, val_acc:0.980]
Epoch [114/120    avg_loss:0.018, val_acc:0.980]
Epoch [115/120    avg_loss:0.019, val_acc:0.980]
Epoch [116/120    avg_loss:0.019, val_acc:0.980]
Epoch [117/120    avg_loss:0.015, val_acc:0.980]
Epoch [118/120    avg_loss:0.019, val_acc:0.980]
Epoch [119/120    avg_loss:0.019, val_acc:0.980]
Epoch [120/120    avg_loss:0.017, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    1    0    0    1    0    0    0    9   13    0    0
     0    0    0]
 [   0    0    0  732    1    3    1    0    0    5    1    1    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    2    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    4    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    1    0    0
     0    0    0]
 [   0    0    1    0    0    2    2    0    0    0  854   15    0    0
     0    1    0]
 [   0    0    8    0    0    0    0    0    0    0    7 2161   12    0
    17    5    0]
 [   0    0    0    0    0    0    0    0    0    0    0    9  523    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    0   18    0    0    0    0    0    0    0
    35  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.8970189701897

F1 scores:
[       nan 0.97619048 0.98708415 0.98852127 0.99765808 0.99198167
 0.9797145  0.96153846 0.997669   0.82051282 0.97823597 0.97915723
 0.97483691 1.         0.97329888 0.89634146 0.98224852]

Kappa:
0.9760221951953953
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1c7ba63908>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.569, val_acc:0.465]
Epoch [2/120    avg_loss:2.204, val_acc:0.523]
Epoch [3/120    avg_loss:1.946, val_acc:0.548]
Epoch [4/120    avg_loss:1.744, val_acc:0.531]
Epoch [5/120    avg_loss:1.594, val_acc:0.584]
Epoch [6/120    avg_loss:1.457, val_acc:0.623]
Epoch [7/120    avg_loss:1.344, val_acc:0.624]
Epoch [8/120    avg_loss:1.142, val_acc:0.648]
Epoch [9/120    avg_loss:1.038, val_acc:0.667]
Epoch [10/120    avg_loss:0.873, val_acc:0.721]
Epoch [11/120    avg_loss:0.838, val_acc:0.721]
Epoch [12/120    avg_loss:0.760, val_acc:0.741]
Epoch [13/120    avg_loss:0.620, val_acc:0.814]
Epoch [14/120    avg_loss:0.527, val_acc:0.812]
Epoch [15/120    avg_loss:0.539, val_acc:0.827]
Epoch [16/120    avg_loss:0.509, val_acc:0.816]
Epoch [17/120    avg_loss:0.466, val_acc:0.804]
Epoch [18/120    avg_loss:0.413, val_acc:0.843]
Epoch [19/120    avg_loss:0.393, val_acc:0.844]
Epoch [20/120    avg_loss:0.356, val_acc:0.857]
Epoch [21/120    avg_loss:0.271, val_acc:0.902]
Epoch [22/120    avg_loss:0.245, val_acc:0.885]
Epoch [23/120    avg_loss:0.195, val_acc:0.910]
Epoch [24/120    avg_loss:0.185, val_acc:0.863]
Epoch [25/120    avg_loss:0.248, val_acc:0.920]
Epoch [26/120    avg_loss:0.251, val_acc:0.865]
Epoch [27/120    avg_loss:0.335, val_acc:0.869]
Epoch [28/120    avg_loss:0.207, val_acc:0.905]
Epoch [29/120    avg_loss:0.212, val_acc:0.904]
Epoch [30/120    avg_loss:0.164, val_acc:0.907]
Epoch [31/120    avg_loss:0.151, val_acc:0.910]
Epoch [32/120    avg_loss:0.127, val_acc:0.916]
Epoch [33/120    avg_loss:0.138, val_acc:0.922]
Epoch [34/120    avg_loss:0.108, val_acc:0.904]
Epoch [35/120    avg_loss:0.108, val_acc:0.916]
Epoch [36/120    avg_loss:0.144, val_acc:0.909]
Epoch [37/120    avg_loss:0.164, val_acc:0.930]
Epoch [38/120    avg_loss:0.094, val_acc:0.934]
Epoch [39/120    avg_loss:0.096, val_acc:0.923]
Epoch [40/120    avg_loss:0.103, val_acc:0.933]
Epoch [41/120    avg_loss:0.087, val_acc:0.948]
Epoch [42/120    avg_loss:0.078, val_acc:0.938]
Epoch [43/120    avg_loss:0.080, val_acc:0.940]
Epoch [44/120    avg_loss:0.090, val_acc:0.935]
Epoch [45/120    avg_loss:0.090, val_acc:0.951]
Epoch [46/120    avg_loss:0.091, val_acc:0.949]
Epoch [47/120    avg_loss:0.128, val_acc:0.934]
Epoch [48/120    avg_loss:0.115, val_acc:0.938]
Epoch [49/120    avg_loss:0.089, val_acc:0.946]
Epoch [50/120    avg_loss:0.053, val_acc:0.939]
Epoch [51/120    avg_loss:0.048, val_acc:0.954]
Epoch [52/120    avg_loss:0.049, val_acc:0.955]
Epoch [53/120    avg_loss:0.046, val_acc:0.945]
Epoch [54/120    avg_loss:0.052, val_acc:0.964]
Epoch [55/120    avg_loss:0.063, val_acc:0.948]
Epoch [56/120    avg_loss:0.056, val_acc:0.946]
Epoch [57/120    avg_loss:0.047, val_acc:0.958]
Epoch [58/120    avg_loss:0.051, val_acc:0.953]
Epoch [59/120    avg_loss:0.056, val_acc:0.951]
Epoch [60/120    avg_loss:0.039, val_acc:0.954]
Epoch [61/120    avg_loss:0.036, val_acc:0.953]
Epoch [62/120    avg_loss:0.041, val_acc:0.961]
Epoch [63/120    avg_loss:0.046, val_acc:0.950]
Epoch [64/120    avg_loss:0.041, val_acc:0.955]
Epoch [65/120    avg_loss:0.036, val_acc:0.963]
Epoch [66/120    avg_loss:0.043, val_acc:0.946]
Epoch [67/120    avg_loss:0.036, val_acc:0.956]
Epoch [68/120    avg_loss:0.028, val_acc:0.967]
Epoch [69/120    avg_loss:0.024, val_acc:0.964]
Epoch [70/120    avg_loss:0.021, val_acc:0.968]
Epoch [71/120    avg_loss:0.019, val_acc:0.964]
Epoch [72/120    avg_loss:0.018, val_acc:0.968]
Epoch [73/120    avg_loss:0.017, val_acc:0.969]
Epoch [74/120    avg_loss:0.018, val_acc:0.970]
Epoch [75/120    avg_loss:0.020, val_acc:0.970]
Epoch [76/120    avg_loss:0.017, val_acc:0.969]
Epoch [77/120    avg_loss:0.019, val_acc:0.968]
Epoch [78/120    avg_loss:0.015, val_acc:0.968]
Epoch [79/120    avg_loss:0.016, val_acc:0.968]
Epoch [80/120    avg_loss:0.018, val_acc:0.970]
Epoch [81/120    avg_loss:0.016, val_acc:0.970]
Epoch [82/120    avg_loss:0.017, val_acc:0.969]
Epoch [83/120    avg_loss:0.015, val_acc:0.969]
Epoch [84/120    avg_loss:0.019, val_acc:0.968]
Epoch [85/120    avg_loss:0.015, val_acc:0.969]
Epoch [86/120    avg_loss:0.014, val_acc:0.972]
Epoch [87/120    avg_loss:0.013, val_acc:0.973]
Epoch [88/120    avg_loss:0.022, val_acc:0.973]
Epoch [89/120    avg_loss:0.019, val_acc:0.969]
Epoch [90/120    avg_loss:0.016, val_acc:0.970]
Epoch [91/120    avg_loss:0.018, val_acc:0.972]
Epoch [92/120    avg_loss:0.015, val_acc:0.974]
Epoch [93/120    avg_loss:0.014, val_acc:0.974]
Epoch [94/120    avg_loss:0.015, val_acc:0.972]
Epoch [95/120    avg_loss:0.014, val_acc:0.975]
Epoch [96/120    avg_loss:0.017, val_acc:0.975]
Epoch [97/120    avg_loss:0.014, val_acc:0.974]
Epoch [98/120    avg_loss:0.014, val_acc:0.977]
Epoch [99/120    avg_loss:0.016, val_acc:0.973]
Epoch [100/120    avg_loss:0.014, val_acc:0.976]
Epoch [101/120    avg_loss:0.015, val_acc:0.977]
Epoch [102/120    avg_loss:0.014, val_acc:0.976]
Epoch [103/120    avg_loss:0.012, val_acc:0.977]
Epoch [104/120    avg_loss:0.014, val_acc:0.974]
Epoch [105/120    avg_loss:0.012, val_acc:0.974]
Epoch [106/120    avg_loss:0.019, val_acc:0.976]
Epoch [107/120    avg_loss:0.012, val_acc:0.974]
Epoch [108/120    avg_loss:0.015, val_acc:0.974]
Epoch [109/120    avg_loss:0.013, val_acc:0.974]
Epoch [110/120    avg_loss:0.013, val_acc:0.974]
Epoch [111/120    avg_loss:0.015, val_acc:0.974]
Epoch [112/120    avg_loss:0.016, val_acc:0.974]
Epoch [113/120    avg_loss:0.015, val_acc:0.975]
Epoch [114/120    avg_loss:0.013, val_acc:0.975]
Epoch [115/120    avg_loss:0.014, val_acc:0.973]
Epoch [116/120    avg_loss:0.013, val_acc:0.973]
Epoch [117/120    avg_loss:0.013, val_acc:0.973]
Epoch [118/120    avg_loss:0.012, val_acc:0.973]
Epoch [119/120    avg_loss:0.011, val_acc:0.973]
Epoch [120/120    avg_loss:0.013, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    2    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1248    1    7    1    7    0    0    0    3   16    2    0
     0    0    0]
 [   0    0    0  718    4    1    0    0    0    4    1    0   10    9
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    8    0    0    2    5    0    0    0  841   19    0    0
     0    0    0]
 [   0    0   23    0    0    0    0    0    0    1    5 2174    6    0
     0    1    0]
 [   0    0    0    1    0    0    0    0    0    0   16    4  509    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1119   17    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    18  329    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.76693766937669

F1 scores:
[       nan 0.975      0.97347894 0.97886844 0.97482838 0.98861048
 0.99018868 1.         1.         0.82926829 0.96611143 0.98282098
 0.95766698 0.9762533  0.9828722  0.9454023  0.97619048]

Kappa:
0.974546281285833
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6d71464860>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.614, val_acc:0.393]
Epoch [2/120    avg_loss:2.143, val_acc:0.558]
Epoch [3/120    avg_loss:1.841, val_acc:0.579]
Epoch [4/120    avg_loss:1.605, val_acc:0.603]
Epoch [5/120    avg_loss:1.409, val_acc:0.661]
Epoch [6/120    avg_loss:1.220, val_acc:0.667]
Epoch [7/120    avg_loss:1.060, val_acc:0.683]
Epoch [8/120    avg_loss:0.927, val_acc:0.742]
Epoch [9/120    avg_loss:0.894, val_acc:0.735]
Epoch [10/120    avg_loss:0.762, val_acc:0.761]
Epoch [11/120    avg_loss:0.659, val_acc:0.771]
Epoch [12/120    avg_loss:0.585, val_acc:0.791]
Epoch [13/120    avg_loss:0.538, val_acc:0.844]
Epoch [14/120    avg_loss:0.445, val_acc:0.855]
Epoch [15/120    avg_loss:0.418, val_acc:0.827]
Epoch [16/120    avg_loss:0.435, val_acc:0.851]
Epoch [17/120    avg_loss:0.378, val_acc:0.846]
Epoch [18/120    avg_loss:0.280, val_acc:0.868]
Epoch [19/120    avg_loss:0.300, val_acc:0.869]
Epoch [20/120    avg_loss:0.304, val_acc:0.867]
Epoch [21/120    avg_loss:0.254, val_acc:0.861]
Epoch [22/120    avg_loss:0.214, val_acc:0.868]
Epoch [23/120    avg_loss:0.204, val_acc:0.900]
Epoch [24/120    avg_loss:0.205, val_acc:0.880]
Epoch [25/120    avg_loss:0.231, val_acc:0.895]
Epoch [26/120    avg_loss:0.197, val_acc:0.891]
Epoch [27/120    avg_loss:0.161, val_acc:0.919]
Epoch [28/120    avg_loss:0.123, val_acc:0.910]
Epoch [29/120    avg_loss:0.122, val_acc:0.915]
Epoch [30/120    avg_loss:0.113, val_acc:0.939]
Epoch [31/120    avg_loss:0.119, val_acc:0.929]
Epoch [32/120    avg_loss:0.102, val_acc:0.940]
Epoch [33/120    avg_loss:0.145, val_acc:0.901]
Epoch [34/120    avg_loss:0.139, val_acc:0.933]
Epoch [35/120    avg_loss:0.090, val_acc:0.951]
Epoch [36/120    avg_loss:0.086, val_acc:0.934]
Epoch [37/120    avg_loss:0.074, val_acc:0.948]
Epoch [38/120    avg_loss:0.075, val_acc:0.950]
Epoch [39/120    avg_loss:0.085, val_acc:0.949]
Epoch [40/120    avg_loss:0.091, val_acc:0.944]
Epoch [41/120    avg_loss:0.107, val_acc:0.933]
Epoch [42/120    avg_loss:0.123, val_acc:0.925]
Epoch [43/120    avg_loss:0.124, val_acc:0.946]
Epoch [44/120    avg_loss:0.083, val_acc:0.949]
Epoch [45/120    avg_loss:0.083, val_acc:0.948]
Epoch [46/120    avg_loss:0.057, val_acc:0.965]
Epoch [47/120    avg_loss:0.045, val_acc:0.955]
Epoch [48/120    avg_loss:0.039, val_acc:0.960]
Epoch [49/120    avg_loss:0.046, val_acc:0.953]
Epoch [50/120    avg_loss:0.044, val_acc:0.965]
Epoch [51/120    avg_loss:0.044, val_acc:0.954]
Epoch [52/120    avg_loss:0.043, val_acc:0.961]
Epoch [53/120    avg_loss:0.042, val_acc:0.956]
Epoch [54/120    avg_loss:0.077, val_acc:0.951]
Epoch [55/120    avg_loss:0.062, val_acc:0.957]
Epoch [56/120    avg_loss:0.042, val_acc:0.967]
Epoch [57/120    avg_loss:0.033, val_acc:0.959]
Epoch [58/120    avg_loss:0.033, val_acc:0.960]
Epoch [59/120    avg_loss:0.031, val_acc:0.964]
Epoch [60/120    avg_loss:0.025, val_acc:0.971]
Epoch [61/120    avg_loss:0.028, val_acc:0.967]
Epoch [62/120    avg_loss:0.020, val_acc:0.977]
Epoch [63/120    avg_loss:0.020, val_acc:0.969]
Epoch [64/120    avg_loss:0.027, val_acc:0.971]
Epoch [65/120    avg_loss:0.026, val_acc:0.971]
Epoch [66/120    avg_loss:0.018, val_acc:0.973]
Epoch [67/120    avg_loss:0.027, val_acc:0.969]
Epoch [68/120    avg_loss:0.018, val_acc:0.972]
Epoch [69/120    avg_loss:0.017, val_acc:0.970]
Epoch [70/120    avg_loss:0.027, val_acc:0.968]
Epoch [71/120    avg_loss:0.014, val_acc:0.970]
Epoch [72/120    avg_loss:0.014, val_acc:0.974]
Epoch [73/120    avg_loss:0.015, val_acc:0.967]
Epoch [74/120    avg_loss:0.022, val_acc:0.976]
Epoch [75/120    avg_loss:0.019, val_acc:0.959]
Epoch [76/120    avg_loss:0.023, val_acc:0.976]
Epoch [77/120    avg_loss:0.017, val_acc:0.981]
Epoch [78/120    avg_loss:0.014, val_acc:0.981]
Epoch [79/120    avg_loss:0.011, val_acc:0.979]
Epoch [80/120    avg_loss:0.009, val_acc:0.978]
Epoch [81/120    avg_loss:0.010, val_acc:0.978]
Epoch [82/120    avg_loss:0.012, val_acc:0.979]
Epoch [83/120    avg_loss:0.009, val_acc:0.982]
Epoch [84/120    avg_loss:0.011, val_acc:0.981]
Epoch [85/120    avg_loss:0.008, val_acc:0.980]
Epoch [86/120    avg_loss:0.010, val_acc:0.981]
Epoch [87/120    avg_loss:0.009, val_acc:0.981]
Epoch [88/120    avg_loss:0.009, val_acc:0.982]
Epoch [89/120    avg_loss:0.009, val_acc:0.982]
Epoch [90/120    avg_loss:0.009, val_acc:0.982]
Epoch [91/120    avg_loss:0.007, val_acc:0.980]
Epoch [92/120    avg_loss:0.008, val_acc:0.981]
Epoch [93/120    avg_loss:0.009, val_acc:0.982]
Epoch [94/120    avg_loss:0.007, val_acc:0.981]
Epoch [95/120    avg_loss:0.010, val_acc:0.981]
Epoch [96/120    avg_loss:0.009, val_acc:0.980]
Epoch [97/120    avg_loss:0.007, val_acc:0.980]
Epoch [98/120    avg_loss:0.008, val_acc:0.980]
Epoch [99/120    avg_loss:0.008, val_acc:0.980]
Epoch [100/120    avg_loss:0.008, val_acc:0.980]
Epoch [101/120    avg_loss:0.007, val_acc:0.980]
Epoch [102/120    avg_loss:0.008, val_acc:0.981]
Epoch [103/120    avg_loss:0.007, val_acc:0.980]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.008, val_acc:0.980]
Epoch [106/120    avg_loss:0.009, val_acc:0.981]
Epoch [107/120    avg_loss:0.010, val_acc:0.978]
Epoch [108/120    avg_loss:0.007, val_acc:0.977]
Epoch [109/120    avg_loss:0.008, val_acc:0.979]
Epoch [110/120    avg_loss:0.007, val_acc:0.977]
Epoch [111/120    avg_loss:0.009, val_acc:0.978]
Epoch [112/120    avg_loss:0.006, val_acc:0.980]
Epoch [113/120    avg_loss:0.007, val_acc:0.978]
Epoch [114/120    avg_loss:0.007, val_acc:0.978]
Epoch [115/120    avg_loss:0.007, val_acc:0.980]
Epoch [116/120    avg_loss:0.009, val_acc:0.979]
Epoch [117/120    avg_loss:0.009, val_acc:0.980]
Epoch [118/120    avg_loss:0.008, val_acc:0.980]
Epoch [119/120    avg_loss:0.008, val_acc:0.980]
Epoch [120/120    avg_loss:0.008, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    7    1    0    0    0    0    0    8    9    0    0
     0    0    0]
 [   0    0    0  734    1    0    0    0    0    2    1    3    5    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    1    2    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    1    2    0    0    0  848   20    0    0
     0    1    0]
 [   0    0    2    0    0    1    0    0    0    0    2 2190   15    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0   10    3  519    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1128   10    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
    27  303    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.23306233062331

F1 scores:
[       nan 1.         0.98823529 0.98655914 0.9953271  0.99310345
 0.98348348 0.96153846 0.997669   0.94736842 0.97247706 0.98737601
 0.96468401 0.99730458 0.98300654 0.91540785 0.98809524]

Kappa:
0.979848794158704
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f88bfa85898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.603, val_acc:0.342]
Epoch [2/120    avg_loss:2.195, val_acc:0.448]
Epoch [3/120    avg_loss:1.956, val_acc:0.526]
Epoch [4/120    avg_loss:1.752, val_acc:0.590]
Epoch [5/120    avg_loss:1.593, val_acc:0.615]
Epoch [6/120    avg_loss:1.359, val_acc:0.632]
Epoch [7/120    avg_loss:1.238, val_acc:0.682]
Epoch [8/120    avg_loss:1.066, val_acc:0.694]
Epoch [9/120    avg_loss:0.908, val_acc:0.697]
Epoch [10/120    avg_loss:0.804, val_acc:0.787]
Epoch [11/120    avg_loss:0.735, val_acc:0.788]
Epoch [12/120    avg_loss:0.621, val_acc:0.854]
Epoch [13/120    avg_loss:0.543, val_acc:0.833]
Epoch [14/120    avg_loss:0.502, val_acc:0.868]
Epoch [15/120    avg_loss:0.423, val_acc:0.858]
Epoch [16/120    avg_loss:0.396, val_acc:0.849]
Epoch [17/120    avg_loss:0.386, val_acc:0.845]
Epoch [18/120    avg_loss:0.336, val_acc:0.867]
Epoch [19/120    avg_loss:0.429, val_acc:0.874]
Epoch [20/120    avg_loss:0.326, val_acc:0.910]
Epoch [21/120    avg_loss:0.328, val_acc:0.915]
Epoch [22/120    avg_loss:0.230, val_acc:0.908]
Epoch [23/120    avg_loss:0.258, val_acc:0.905]
Epoch [24/120    avg_loss:0.195, val_acc:0.894]
Epoch [25/120    avg_loss:0.190, val_acc:0.909]
Epoch [26/120    avg_loss:0.185, val_acc:0.915]
Epoch [27/120    avg_loss:0.173, val_acc:0.925]
Epoch [28/120    avg_loss:0.185, val_acc:0.918]
Epoch [29/120    avg_loss:0.196, val_acc:0.925]
Epoch [30/120    avg_loss:0.161, val_acc:0.941]
Epoch [31/120    avg_loss:0.137, val_acc:0.916]
Epoch [32/120    avg_loss:0.152, val_acc:0.943]
Epoch [33/120    avg_loss:0.100, val_acc:0.933]
Epoch [34/120    avg_loss:0.094, val_acc:0.956]
Epoch [35/120    avg_loss:0.080, val_acc:0.933]
Epoch [36/120    avg_loss:0.082, val_acc:0.941]
Epoch [37/120    avg_loss:0.087, val_acc:0.948]
Epoch [38/120    avg_loss:0.078, val_acc:0.950]
Epoch [39/120    avg_loss:0.083, val_acc:0.936]
Epoch [40/120    avg_loss:0.087, val_acc:0.945]
Epoch [41/120    avg_loss:0.109, val_acc:0.947]
Epoch [42/120    avg_loss:0.093, val_acc:0.942]
Epoch [43/120    avg_loss:0.106, val_acc:0.946]
Epoch [44/120    avg_loss:0.085, val_acc:0.947]
Epoch [45/120    avg_loss:0.065, val_acc:0.962]
Epoch [46/120    avg_loss:0.053, val_acc:0.953]
Epoch [47/120    avg_loss:0.086, val_acc:0.949]
Epoch [48/120    avg_loss:0.100, val_acc:0.949]
Epoch [49/120    avg_loss:0.093, val_acc:0.879]
Epoch [50/120    avg_loss:0.158, val_acc:0.938]
Epoch [51/120    avg_loss:0.088, val_acc:0.946]
Epoch [52/120    avg_loss:0.097, val_acc:0.949]
Epoch [53/120    avg_loss:0.080, val_acc:0.957]
Epoch [54/120    avg_loss:0.077, val_acc:0.952]
Epoch [55/120    avg_loss:0.057, val_acc:0.957]
Epoch [56/120    avg_loss:0.058, val_acc:0.962]
Epoch [57/120    avg_loss:0.045, val_acc:0.967]
Epoch [58/120    avg_loss:0.040, val_acc:0.964]
Epoch [59/120    avg_loss:0.044, val_acc:0.948]
Epoch [60/120    avg_loss:0.043, val_acc:0.965]
Epoch [61/120    avg_loss:0.043, val_acc:0.970]
Epoch [62/120    avg_loss:0.044, val_acc:0.950]
Epoch [63/120    avg_loss:0.052, val_acc:0.953]
Epoch [64/120    avg_loss:0.060, val_acc:0.957]
Epoch [65/120    avg_loss:0.056, val_acc:0.958]
Epoch [66/120    avg_loss:0.034, val_acc:0.972]
Epoch [67/120    avg_loss:0.036, val_acc:0.958]
Epoch [68/120    avg_loss:0.042, val_acc:0.971]
Epoch [69/120    avg_loss:0.046, val_acc:0.961]
Epoch [70/120    avg_loss:0.034, val_acc:0.968]
Epoch [71/120    avg_loss:0.021, val_acc:0.972]
Epoch [72/120    avg_loss:0.023, val_acc:0.972]
Epoch [73/120    avg_loss:0.026, val_acc:0.975]
Epoch [74/120    avg_loss:0.025, val_acc:0.974]
Epoch [75/120    avg_loss:0.030, val_acc:0.978]
Epoch [76/120    avg_loss:0.019, val_acc:0.976]
Epoch [77/120    avg_loss:0.035, val_acc:0.962]
Epoch [78/120    avg_loss:0.064, val_acc:0.962]
Epoch [79/120    avg_loss:0.044, val_acc:0.959]
Epoch [80/120    avg_loss:0.021, val_acc:0.976]
Epoch [81/120    avg_loss:0.041, val_acc:0.951]
Epoch [82/120    avg_loss:0.077, val_acc:0.967]
Epoch [83/120    avg_loss:0.033, val_acc:0.970]
Epoch [84/120    avg_loss:0.035, val_acc:0.976]
Epoch [85/120    avg_loss:0.020, val_acc:0.974]
Epoch [86/120    avg_loss:0.019, val_acc:0.966]
Epoch [87/120    avg_loss:0.026, val_acc:0.971]
Epoch [88/120    avg_loss:0.012, val_acc:0.975]
Epoch [89/120    avg_loss:0.010, val_acc:0.976]
Epoch [90/120    avg_loss:0.013, val_acc:0.978]
Epoch [91/120    avg_loss:0.012, val_acc:0.977]
Epoch [92/120    avg_loss:0.011, val_acc:0.977]
Epoch [93/120    avg_loss:0.011, val_acc:0.976]
Epoch [94/120    avg_loss:0.012, val_acc:0.976]
Epoch [95/120    avg_loss:0.020, val_acc:0.976]
Epoch [96/120    avg_loss:0.011, val_acc:0.974]
Epoch [97/120    avg_loss:0.012, val_acc:0.977]
Epoch [98/120    avg_loss:0.008, val_acc:0.977]
Epoch [99/120    avg_loss:0.011, val_acc:0.979]
Epoch [100/120    avg_loss:0.018, val_acc:0.979]
Epoch [101/120    avg_loss:0.009, val_acc:0.979]
Epoch [102/120    avg_loss:0.011, val_acc:0.981]
Epoch [103/120    avg_loss:0.008, val_acc:0.981]
Epoch [104/120    avg_loss:0.010, val_acc:0.978]
Epoch [105/120    avg_loss:0.011, val_acc:0.979]
Epoch [106/120    avg_loss:0.009, val_acc:0.979]
Epoch [107/120    avg_loss:0.008, val_acc:0.978]
Epoch [108/120    avg_loss:0.007, val_acc:0.979]
Epoch [109/120    avg_loss:0.009, val_acc:0.980]
Epoch [110/120    avg_loss:0.009, val_acc:0.979]
Epoch [111/120    avg_loss:0.010, val_acc:0.979]
Epoch [112/120    avg_loss:0.011, val_acc:0.980]
Epoch [113/120    avg_loss:0.011, val_acc:0.979]
Epoch [114/120    avg_loss:0.016, val_acc:0.979]
Epoch [115/120    avg_loss:0.007, val_acc:0.978]
Epoch [116/120    avg_loss:0.009, val_acc:0.980]
Epoch [117/120    avg_loss:0.009, val_acc:0.980]
Epoch [118/120    avg_loss:0.008, val_acc:0.980]
Epoch [119/120    avg_loss:0.009, val_acc:0.980]
Epoch [120/120    avg_loss:0.009, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1244    5    0    1    1    0    0    0   10   21    2    0
     0    1    0]
 [   0    0    0  716    2    0    0    0    0    2    1    2   21    2
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    1    2    0    0    0  855   14    0    0
     0    0    0]
 [   0    0    1    0    0    0    1    4    0    1    9 2185    8    1
     0    0    0]
 [   0    0    0    6    0    4    0    0    0    0    1   10  512    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    21  315    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.8970189701897

F1 scores:
[       nan 0.975      0.9822345  0.97150611 0.9953271  0.992
 0.98718915 0.90909091 0.9953271  0.92307692 0.97547062 0.98334833
 0.9437788  0.9919571  0.9860262  0.93610698 0.96969697]

Kappa:
0.9760181756138487
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc6b4183898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.628, val_acc:0.529]
Epoch [2/120    avg_loss:2.172, val_acc:0.541]
Epoch [3/120    avg_loss:1.881, val_acc:0.602]
Epoch [4/120    avg_loss:1.641, val_acc:0.604]
Epoch [5/120    avg_loss:1.432, val_acc:0.666]
Epoch [6/120    avg_loss:1.213, val_acc:0.642]
Epoch [7/120    avg_loss:1.121, val_acc:0.718]
Epoch [8/120    avg_loss:0.939, val_acc:0.714]
Epoch [9/120    avg_loss:0.867, val_acc:0.732]
Epoch [10/120    avg_loss:0.768, val_acc:0.730]
Epoch [11/120    avg_loss:0.697, val_acc:0.781]
Epoch [12/120    avg_loss:0.662, val_acc:0.779]
Epoch [13/120    avg_loss:0.591, val_acc:0.817]
Epoch [14/120    avg_loss:0.580, val_acc:0.832]
Epoch [15/120    avg_loss:0.564, val_acc:0.832]
Epoch [16/120    avg_loss:0.433, val_acc:0.855]
Epoch [17/120    avg_loss:0.349, val_acc:0.848]
Epoch [18/120    avg_loss:0.353, val_acc:0.866]
Epoch [19/120    avg_loss:0.297, val_acc:0.819]
Epoch [20/120    avg_loss:0.275, val_acc:0.895]
Epoch [21/120    avg_loss:0.234, val_acc:0.894]
Epoch [22/120    avg_loss:0.239, val_acc:0.911]
Epoch [23/120    avg_loss:0.258, val_acc:0.894]
Epoch [24/120    avg_loss:0.209, val_acc:0.870]
Epoch [25/120    avg_loss:0.217, val_acc:0.893]
Epoch [26/120    avg_loss:0.239, val_acc:0.892]
Epoch [27/120    avg_loss:0.193, val_acc:0.890]
Epoch [28/120    avg_loss:0.158, val_acc:0.929]
Epoch [29/120    avg_loss:0.153, val_acc:0.928]
Epoch [30/120    avg_loss:0.137, val_acc:0.902]
Epoch [31/120    avg_loss:0.274, val_acc:0.844]
Epoch [32/120    avg_loss:0.256, val_acc:0.880]
Epoch [33/120    avg_loss:0.161, val_acc:0.908]
Epoch [34/120    avg_loss:0.130, val_acc:0.931]
Epoch [35/120    avg_loss:0.123, val_acc:0.919]
Epoch [36/120    avg_loss:0.118, val_acc:0.927]
Epoch [37/120    avg_loss:0.111, val_acc:0.895]
Epoch [38/120    avg_loss:0.105, val_acc:0.944]
Epoch [39/120    avg_loss:0.092, val_acc:0.928]
Epoch [40/120    avg_loss:0.123, val_acc:0.944]
Epoch [41/120    avg_loss:0.077, val_acc:0.951]
Epoch [42/120    avg_loss:0.080, val_acc:0.902]
Epoch [43/120    avg_loss:0.126, val_acc:0.954]
Epoch [44/120    avg_loss:0.092, val_acc:0.949]
Epoch [45/120    avg_loss:0.064, val_acc:0.939]
Epoch [46/120    avg_loss:0.067, val_acc:0.948]
Epoch [47/120    avg_loss:0.087, val_acc:0.946]
Epoch [48/120    avg_loss:0.072, val_acc:0.959]
Epoch [49/120    avg_loss:0.081, val_acc:0.924]
Epoch [50/120    avg_loss:0.084, val_acc:0.939]
Epoch [51/120    avg_loss:0.061, val_acc:0.950]
Epoch [52/120    avg_loss:0.055, val_acc:0.953]
Epoch [53/120    avg_loss:0.053, val_acc:0.949]
Epoch [54/120    avg_loss:0.059, val_acc:0.960]
Epoch [55/120    avg_loss:0.056, val_acc:0.948]
Epoch [56/120    avg_loss:0.044, val_acc:0.940]
Epoch [57/120    avg_loss:0.037, val_acc:0.969]
Epoch [58/120    avg_loss:0.045, val_acc:0.961]
Epoch [59/120    avg_loss:0.041, val_acc:0.954]
Epoch [60/120    avg_loss:0.042, val_acc:0.947]
Epoch [61/120    avg_loss:0.035, val_acc:0.964]
Epoch [62/120    avg_loss:0.038, val_acc:0.965]
Epoch [63/120    avg_loss:0.027, val_acc:0.961]
Epoch [64/120    avg_loss:0.032, val_acc:0.948]
Epoch [65/120    avg_loss:0.046, val_acc:0.950]
Epoch [66/120    avg_loss:0.030, val_acc:0.960]
Epoch [67/120    avg_loss:0.042, val_acc:0.967]
Epoch [68/120    avg_loss:0.032, val_acc:0.958]
Epoch [69/120    avg_loss:0.047, val_acc:0.941]
Epoch [70/120    avg_loss:0.039, val_acc:0.951]
Epoch [71/120    avg_loss:0.032, val_acc:0.958]
Epoch [72/120    avg_loss:0.023, val_acc:0.968]
Epoch [73/120    avg_loss:0.023, val_acc:0.970]
Epoch [74/120    avg_loss:0.018, val_acc:0.970]
Epoch [75/120    avg_loss:0.022, val_acc:0.970]
Epoch [76/120    avg_loss:0.015, val_acc:0.969]
Epoch [77/120    avg_loss:0.017, val_acc:0.969]
Epoch [78/120    avg_loss:0.026, val_acc:0.970]
Epoch [79/120    avg_loss:0.016, val_acc:0.970]
Epoch [80/120    avg_loss:0.023, val_acc:0.970]
Epoch [81/120    avg_loss:0.018, val_acc:0.971]
Epoch [82/120    avg_loss:0.017, val_acc:0.968]
Epoch [83/120    avg_loss:0.018, val_acc:0.969]
Epoch [84/120    avg_loss:0.020, val_acc:0.968]
Epoch [85/120    avg_loss:0.017, val_acc:0.967]
Epoch [86/120    avg_loss:0.017, val_acc:0.967]
Epoch [87/120    avg_loss:0.017, val_acc:0.970]
Epoch [88/120    avg_loss:0.018, val_acc:0.972]
Epoch [89/120    avg_loss:0.017, val_acc:0.972]
Epoch [90/120    avg_loss:0.016, val_acc:0.970]
Epoch [91/120    avg_loss:0.016, val_acc:0.969]
Epoch [92/120    avg_loss:0.014, val_acc:0.970]
Epoch [93/120    avg_loss:0.016, val_acc:0.971]
Epoch [94/120    avg_loss:0.013, val_acc:0.971]
Epoch [95/120    avg_loss:0.016, val_acc:0.971]
Epoch [96/120    avg_loss:0.021, val_acc:0.974]
Epoch [97/120    avg_loss:0.015, val_acc:0.970]
Epoch [98/120    avg_loss:0.017, val_acc:0.972]
Epoch [99/120    avg_loss:0.015, val_acc:0.969]
Epoch [100/120    avg_loss:0.014, val_acc:0.971]
Epoch [101/120    avg_loss:0.014, val_acc:0.969]
Epoch [102/120    avg_loss:0.015, val_acc:0.966]
Epoch [103/120    avg_loss:0.015, val_acc:0.967]
Epoch [104/120    avg_loss:0.012, val_acc:0.967]
Epoch [105/120    avg_loss:0.013, val_acc:0.966]
Epoch [106/120    avg_loss:0.011, val_acc:0.970]
Epoch [107/120    avg_loss:0.015, val_acc:0.971]
Epoch [108/120    avg_loss:0.014, val_acc:0.970]
Epoch [109/120    avg_loss:0.016, val_acc:0.971]
Epoch [110/120    avg_loss:0.012, val_acc:0.971]
Epoch [111/120    avg_loss:0.020, val_acc:0.971]
Epoch [112/120    avg_loss:0.017, val_acc:0.971]
Epoch [113/120    avg_loss:0.011, val_acc:0.973]
Epoch [114/120    avg_loss:0.012, val_acc:0.973]
Epoch [115/120    avg_loss:0.014, val_acc:0.973]
Epoch [116/120    avg_loss:0.015, val_acc:0.973]
Epoch [117/120    avg_loss:0.014, val_acc:0.973]
Epoch [118/120    avg_loss:0.014, val_acc:0.973]
Epoch [119/120    avg_loss:0.014, val_acc:0.974]
Epoch [120/120    avg_loss:0.015, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1258    2    4    5    0    0    0    0    6   10    0    0
     0    0    0]
 [   0    0    0  721    9    0    1    0    0    7    1    1    5    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    2    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    1    5    0    0    0  854    9    0    0
     0    0    0]
 [   0    0    7    0    0    0    4    0    0    3   14 2159   22    1
     0    0    0]
 [   0    0    0    5    2    1    0    0    0    0    0    2  521    0
     2    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    2    0    0    0    0
  1123   14    0]
 [   0    0    0    0    0    0   21    0    0    0    0    0    0    0
    20  306    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.8319783197832

F1 scores:
[       nan 0.98765432 0.98435055 0.97762712 0.96598639 0.98855835
 0.97619048 0.96153846 1.         0.73469388 0.9754426  0.98337509
 0.96303142 0.9919571  0.98293217 0.91754123 0.99408284]

Kappa:
0.9753036899887866
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9862ada8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.541, val_acc:0.499]
Epoch [2/120    avg_loss:2.117, val_acc:0.567]
Epoch [3/120    avg_loss:1.909, val_acc:0.565]
Epoch [4/120    avg_loss:1.700, val_acc:0.584]
Epoch [5/120    avg_loss:1.466, val_acc:0.614]
Epoch [6/120    avg_loss:1.340, val_acc:0.636]
Epoch [7/120    avg_loss:1.146, val_acc:0.686]
Epoch [8/120    avg_loss:1.020, val_acc:0.727]
Epoch [9/120    avg_loss:1.010, val_acc:0.715]
Epoch [10/120    avg_loss:0.920, val_acc:0.742]
Epoch [11/120    avg_loss:0.825, val_acc:0.729]
Epoch [12/120    avg_loss:0.691, val_acc:0.771]
Epoch [13/120    avg_loss:0.615, val_acc:0.756]
Epoch [14/120    avg_loss:0.571, val_acc:0.834]
Epoch [15/120    avg_loss:0.574, val_acc:0.796]
Epoch [16/120    avg_loss:0.458, val_acc:0.859]
Epoch [17/120    avg_loss:0.476, val_acc:0.820]
Epoch [18/120    avg_loss:0.426, val_acc:0.804]
Epoch [19/120    avg_loss:0.361, val_acc:0.885]
Epoch [20/120    avg_loss:0.325, val_acc:0.860]
Epoch [21/120    avg_loss:0.253, val_acc:0.855]
Epoch [22/120    avg_loss:0.391, val_acc:0.767]
Epoch [23/120    avg_loss:0.385, val_acc:0.878]
Epoch [24/120    avg_loss:0.237, val_acc:0.889]
Epoch [25/120    avg_loss:0.238, val_acc:0.878]
Epoch [26/120    avg_loss:0.218, val_acc:0.901]
Epoch [27/120    avg_loss:0.160, val_acc:0.917]
Epoch [28/120    avg_loss:0.156, val_acc:0.910]
Epoch [29/120    avg_loss:0.152, val_acc:0.894]
Epoch [30/120    avg_loss:0.152, val_acc:0.853]
Epoch [31/120    avg_loss:0.164, val_acc:0.897]
Epoch [32/120    avg_loss:0.232, val_acc:0.885]
Epoch [33/120    avg_loss:0.130, val_acc:0.934]
Epoch [34/120    avg_loss:0.142, val_acc:0.919]
Epoch [35/120    avg_loss:0.120, val_acc:0.931]
Epoch [36/120    avg_loss:0.097, val_acc:0.931]
Epoch [37/120    avg_loss:0.169, val_acc:0.846]
Epoch [38/120    avg_loss:0.160, val_acc:0.931]
Epoch [39/120    avg_loss:0.114, val_acc:0.935]
Epoch [40/120    avg_loss:0.093, val_acc:0.950]
Epoch [41/120    avg_loss:0.089, val_acc:0.941]
Epoch [42/120    avg_loss:0.081, val_acc:0.963]
Epoch [43/120    avg_loss:0.066, val_acc:0.942]
Epoch [44/120    avg_loss:0.056, val_acc:0.961]
Epoch [45/120    avg_loss:0.063, val_acc:0.953]
Epoch [46/120    avg_loss:0.084, val_acc:0.936]
Epoch [47/120    avg_loss:0.105, val_acc:0.946]
Epoch [48/120    avg_loss:0.098, val_acc:0.954]
Epoch [49/120    avg_loss:0.065, val_acc:0.946]
Epoch [50/120    avg_loss:0.047, val_acc:0.958]
Epoch [51/120    avg_loss:0.051, val_acc:0.946]
Epoch [52/120    avg_loss:0.068, val_acc:0.956]
Epoch [53/120    avg_loss:0.050, val_acc:0.963]
Epoch [54/120    avg_loss:0.044, val_acc:0.966]
Epoch [55/120    avg_loss:0.042, val_acc:0.965]
Epoch [56/120    avg_loss:0.060, val_acc:0.956]
Epoch [57/120    avg_loss:0.040, val_acc:0.963]
Epoch [58/120    avg_loss:0.045, val_acc:0.953]
Epoch [59/120    avg_loss:0.033, val_acc:0.964]
Epoch [60/120    avg_loss:0.069, val_acc:0.954]
Epoch [61/120    avg_loss:0.056, val_acc:0.951]
Epoch [62/120    avg_loss:0.088, val_acc:0.960]
Epoch [63/120    avg_loss:0.036, val_acc:0.952]
Epoch [64/120    avg_loss:0.043, val_acc:0.972]
Epoch [65/120    avg_loss:0.049, val_acc:0.952]
Epoch [66/120    avg_loss:0.047, val_acc:0.969]
Epoch [67/120    avg_loss:0.033, val_acc:0.967]
Epoch [68/120    avg_loss:0.041, val_acc:0.952]
Epoch [69/120    avg_loss:0.071, val_acc:0.972]
Epoch [70/120    avg_loss:0.031, val_acc:0.972]
Epoch [71/120    avg_loss:0.027, val_acc:0.976]
Epoch [72/120    avg_loss:0.033, val_acc:0.959]
Epoch [73/120    avg_loss:0.037, val_acc:0.954]
Epoch [74/120    avg_loss:0.031, val_acc:0.974]
Epoch [75/120    avg_loss:0.027, val_acc:0.953]
Epoch [76/120    avg_loss:0.028, val_acc:0.970]
Epoch [77/120    avg_loss:0.027, val_acc:0.939]
Epoch [78/120    avg_loss:0.033, val_acc:0.972]
Epoch [79/120    avg_loss:0.024, val_acc:0.972]
Epoch [80/120    avg_loss:0.020, val_acc:0.972]
Epoch [81/120    avg_loss:0.025, val_acc:0.968]
Epoch [82/120    avg_loss:0.022, val_acc:0.960]
Epoch [83/120    avg_loss:0.023, val_acc:0.968]
Epoch [84/120    avg_loss:0.018, val_acc:0.976]
Epoch [85/120    avg_loss:0.019, val_acc:0.962]
Epoch [86/120    avg_loss:0.016, val_acc:0.967]
Epoch [87/120    avg_loss:0.012, val_acc:0.976]
Epoch [88/120    avg_loss:0.023, val_acc:0.962]
Epoch [89/120    avg_loss:0.017, val_acc:0.974]
Epoch [90/120    avg_loss:0.018, val_acc:0.977]
Epoch [91/120    avg_loss:0.019, val_acc:0.975]
Epoch [92/120    avg_loss:0.019, val_acc:0.970]
Epoch [93/120    avg_loss:0.027, val_acc:0.966]
Epoch [94/120    avg_loss:0.035, val_acc:0.952]
Epoch [95/120    avg_loss:0.026, val_acc:0.976]
Epoch [96/120    avg_loss:0.024, val_acc:0.976]
Epoch [97/120    avg_loss:0.033, val_acc:0.971]
Epoch [98/120    avg_loss:0.020, val_acc:0.967]
Epoch [99/120    avg_loss:0.016, val_acc:0.976]
Epoch [100/120    avg_loss:0.013, val_acc:0.980]
Epoch [101/120    avg_loss:0.020, val_acc:0.970]
Epoch [102/120    avg_loss:0.021, val_acc:0.976]
Epoch [103/120    avg_loss:0.022, val_acc:0.979]
Epoch [104/120    avg_loss:0.020, val_acc:0.976]
Epoch [105/120    avg_loss:0.011, val_acc:0.976]
Epoch [106/120    avg_loss:0.014, val_acc:0.977]
Epoch [107/120    avg_loss:0.015, val_acc:0.969]
Epoch [108/120    avg_loss:0.011, val_acc:0.977]
Epoch [109/120    avg_loss:0.012, val_acc:0.970]
Epoch [110/120    avg_loss:0.013, val_acc:0.968]
Epoch [111/120    avg_loss:0.011, val_acc:0.975]
Epoch [112/120    avg_loss:0.013, val_acc:0.965]
Epoch [113/120    avg_loss:0.010, val_acc:0.978]
Epoch [114/120    avg_loss:0.006, val_acc:0.980]
Epoch [115/120    avg_loss:0.008, val_acc:0.981]
Epoch [116/120    avg_loss:0.007, val_acc:0.980]
Epoch [117/120    avg_loss:0.007, val_acc:0.981]
Epoch [118/120    avg_loss:0.006, val_acc:0.979]
Epoch [119/120    avg_loss:0.007, val_acc:0.981]
Epoch [120/120    avg_loss:0.007, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1254    7    1    1    0    0    0    1    7   12    0    0
     1    1    0]
 [   0    0    1  717    8    0    1    0    0   14    0    4    2    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  421    2    7    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    2    2    0    0    1  847   18    1    0
     2    1    0]
 [   0    0   11    1    0    0    5    0    0    3   10 2175    3    1
     0    1    0]
 [   0    0    0    0    0    3    0    0    0    0    2    1  522    0
     4    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1119   20    0]
 [   0    0    1    0    0    0   17    0    0    0    0    0    0    0
    24  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.68021680216802

F1 scores:
[       nan 1.         0.98237368 0.97418478 0.97695853 0.97679814
 0.97986577 0.87719298 1.         0.65454545 0.97300402 0.9841629
 0.98120301 0.99730458 0.97558849 0.90236686 0.98809524]

Kappa:
0.9735593247307485
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f65e7818908>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.617, val_acc:0.478]
Epoch [2/120    avg_loss:2.189, val_acc:0.575]
Epoch [3/120    avg_loss:1.927, val_acc:0.596]
Epoch [4/120    avg_loss:1.730, val_acc:0.616]
Epoch [5/120    avg_loss:1.585, val_acc:0.650]
Epoch [6/120    avg_loss:1.348, val_acc:0.651]
Epoch [7/120    avg_loss:1.169, val_acc:0.690]
Epoch [8/120    avg_loss:1.046, val_acc:0.698]
Epoch [9/120    avg_loss:1.005, val_acc:0.707]
Epoch [10/120    avg_loss:0.852, val_acc:0.752]
Epoch [11/120    avg_loss:0.761, val_acc:0.768]
Epoch [12/120    avg_loss:0.701, val_acc:0.797]
Epoch [13/120    avg_loss:0.681, val_acc:0.758]
Epoch [14/120    avg_loss:0.826, val_acc:0.749]
Epoch [15/120    avg_loss:0.661, val_acc:0.793]
Epoch [16/120    avg_loss:0.583, val_acc:0.803]
Epoch [17/120    avg_loss:0.480, val_acc:0.822]
Epoch [18/120    avg_loss:0.412, val_acc:0.850]
Epoch [19/120    avg_loss:0.371, val_acc:0.848]
Epoch [20/120    avg_loss:0.368, val_acc:0.845]
Epoch [21/120    avg_loss:0.309, val_acc:0.866]
Epoch [22/120    avg_loss:0.307, val_acc:0.796]
Epoch [23/120    avg_loss:0.323, val_acc:0.881]
Epoch [24/120    avg_loss:0.302, val_acc:0.866]
Epoch [25/120    avg_loss:0.252, val_acc:0.884]
Epoch [26/120    avg_loss:0.199, val_acc:0.904]
Epoch [27/120    avg_loss:0.193, val_acc:0.911]
Epoch [28/120    avg_loss:0.199, val_acc:0.890]
Epoch [29/120    avg_loss:0.171, val_acc:0.906]
Epoch [30/120    avg_loss:0.157, val_acc:0.917]
Epoch [31/120    avg_loss:0.159, val_acc:0.913]
Epoch [32/120    avg_loss:0.156, val_acc:0.921]
Epoch [33/120    avg_loss:0.121, val_acc:0.928]
Epoch [34/120    avg_loss:0.157, val_acc:0.894]
Epoch [35/120    avg_loss:0.189, val_acc:0.915]
Epoch [36/120    avg_loss:0.153, val_acc:0.909]
Epoch [37/120    avg_loss:0.143, val_acc:0.928]
Epoch [38/120    avg_loss:0.142, val_acc:0.931]
Epoch [39/120    avg_loss:0.147, val_acc:0.932]
Epoch [40/120    avg_loss:0.120, val_acc:0.913]
Epoch [41/120    avg_loss:0.131, val_acc:0.935]
Epoch [42/120    avg_loss:0.143, val_acc:0.932]
Epoch [43/120    avg_loss:0.150, val_acc:0.926]
Epoch [44/120    avg_loss:0.105, val_acc:0.927]
Epoch [45/120    avg_loss:0.102, val_acc:0.938]
Epoch [46/120    avg_loss:0.093, val_acc:0.942]
Epoch [47/120    avg_loss:0.121, val_acc:0.932]
Epoch [48/120    avg_loss:0.096, val_acc:0.941]
Epoch [49/120    avg_loss:0.098, val_acc:0.948]
Epoch [50/120    avg_loss:0.060, val_acc:0.946]
Epoch [51/120    avg_loss:0.065, val_acc:0.948]
Epoch [52/120    avg_loss:0.056, val_acc:0.957]
Epoch [53/120    avg_loss:0.062, val_acc:0.952]
Epoch [54/120    avg_loss:0.053, val_acc:0.958]
Epoch [55/120    avg_loss:0.069, val_acc:0.963]
Epoch [56/120    avg_loss:0.096, val_acc:0.835]
Epoch [57/120    avg_loss:0.333, val_acc:0.883]
Epoch [58/120    avg_loss:0.142, val_acc:0.931]
Epoch [59/120    avg_loss:0.159, val_acc:0.933]
Epoch [60/120    avg_loss:0.096, val_acc:0.947]
Epoch [61/120    avg_loss:0.086, val_acc:0.944]
Epoch [62/120    avg_loss:0.092, val_acc:0.961]
Epoch [63/120    avg_loss:0.061, val_acc:0.949]
Epoch [64/120    avg_loss:0.044, val_acc:0.958]
Epoch [65/120    avg_loss:0.039, val_acc:0.956]
Epoch [66/120    avg_loss:0.052, val_acc:0.964]
Epoch [67/120    avg_loss:0.044, val_acc:0.954]
Epoch [68/120    avg_loss:0.044, val_acc:0.967]
Epoch [69/120    avg_loss:0.051, val_acc:0.962]
Epoch [70/120    avg_loss:0.044, val_acc:0.962]
Epoch [71/120    avg_loss:0.055, val_acc:0.961]
Epoch [72/120    avg_loss:0.051, val_acc:0.964]
Epoch [73/120    avg_loss:0.036, val_acc:0.975]
Epoch [74/120    avg_loss:0.035, val_acc:0.961]
Epoch [75/120    avg_loss:0.040, val_acc:0.964]
Epoch [76/120    avg_loss:0.040, val_acc:0.974]
Epoch [77/120    avg_loss:0.041, val_acc:0.962]
Epoch [78/120    avg_loss:0.038, val_acc:0.958]
Epoch [79/120    avg_loss:0.038, val_acc:0.960]
Epoch [80/120    avg_loss:0.027, val_acc:0.966]
Epoch [81/120    avg_loss:0.021, val_acc:0.966]
Epoch [82/120    avg_loss:0.031, val_acc:0.969]
Epoch [83/120    avg_loss:0.028, val_acc:0.974]
Epoch [84/120    avg_loss:0.027, val_acc:0.971]
Epoch [85/120    avg_loss:0.030, val_acc:0.953]
Epoch [86/120    avg_loss:0.075, val_acc:0.943]
Epoch [87/120    avg_loss:0.079, val_acc:0.959]
Epoch [88/120    avg_loss:0.042, val_acc:0.963]
Epoch [89/120    avg_loss:0.038, val_acc:0.969]
Epoch [90/120    avg_loss:0.036, val_acc:0.968]
Epoch [91/120    avg_loss:0.033, val_acc:0.970]
Epoch [92/120    avg_loss:0.035, val_acc:0.971]
Epoch [93/120    avg_loss:0.026, val_acc:0.972]
Epoch [94/120    avg_loss:0.028, val_acc:0.974]
Epoch [95/120    avg_loss:0.029, val_acc:0.972]
Epoch [96/120    avg_loss:0.027, val_acc:0.973]
Epoch [97/120    avg_loss:0.027, val_acc:0.973]
Epoch [98/120    avg_loss:0.024, val_acc:0.973]
Epoch [99/120    avg_loss:0.027, val_acc:0.975]
Epoch [100/120    avg_loss:0.019, val_acc:0.973]
Epoch [101/120    avg_loss:0.022, val_acc:0.976]
Epoch [102/120    avg_loss:0.028, val_acc:0.975]
Epoch [103/120    avg_loss:0.019, val_acc:0.975]
Epoch [104/120    avg_loss:0.020, val_acc:0.974]
Epoch [105/120    avg_loss:0.023, val_acc:0.975]
Epoch [106/120    avg_loss:0.023, val_acc:0.974]
Epoch [107/120    avg_loss:0.025, val_acc:0.978]
Epoch [108/120    avg_loss:0.023, val_acc:0.977]
Epoch [109/120    avg_loss:0.022, val_acc:0.976]
Epoch [110/120    avg_loss:0.021, val_acc:0.980]
Epoch [111/120    avg_loss:0.024, val_acc:0.970]
Epoch [112/120    avg_loss:0.021, val_acc:0.975]
Epoch [113/120    avg_loss:0.018, val_acc:0.976]
Epoch [114/120    avg_loss:0.017, val_acc:0.976]
Epoch [115/120    avg_loss:0.014, val_acc:0.975]
Epoch [116/120    avg_loss:0.023, val_acc:0.975]
Epoch [117/120    avg_loss:0.015, val_acc:0.975]
Epoch [118/120    avg_loss:0.017, val_acc:0.977]
Epoch [119/120    avg_loss:0.024, val_acc:0.972]
Epoch [120/120    avg_loss:0.019, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1256    2    6    0    0    0    0    0    6   15    0    0
     0    0    0]
 [   0    0    0  712    3    1    0    0    0    7    0    5   18    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    2    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0    8    1    0    9    2    0    0    0  844    8    0    0
     2    1    0]
 [   0    0    4    0    0    1    1    0    0    0   13 2188    1    1
     1    0    0]
 [   0    0    1    0    0    0    0    0    0    0    0    3  526    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1134    5    0]
 [   0    0    0    0    0    0   25    0    0    4    0    0    0    0
     8  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.07046070460704

F1 scores:
[       nan 0.98765432 0.98355521 0.97400821 0.97931034 0.98289624
 0.97837435 0.96153846 1.         0.71111111 0.9706728  0.98781038
 0.9704797  0.99462366 0.99212598 0.93233083 0.97005988]

Kappa:
0.9780018649349205
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9983152898>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.607, val_acc:0.476]
Epoch [2/120    avg_loss:2.229, val_acc:0.556]
Epoch [3/120    avg_loss:1.989, val_acc:0.530]
Epoch [4/120    avg_loss:1.815, val_acc:0.577]
Epoch [5/120    avg_loss:1.695, val_acc:0.641]
Epoch [6/120    avg_loss:1.503, val_acc:0.635]
Epoch [7/120    avg_loss:1.360, val_acc:0.680]
Epoch [8/120    avg_loss:1.283, val_acc:0.713]
Epoch [9/120    avg_loss:1.099, val_acc:0.760]
Epoch [10/120    avg_loss:1.012, val_acc:0.755]
Epoch [11/120    avg_loss:0.800, val_acc:0.745]
Epoch [12/120    avg_loss:0.737, val_acc:0.828]
Epoch [13/120    avg_loss:0.703, val_acc:0.816]
Epoch [14/120    avg_loss:0.596, val_acc:0.806]
Epoch [15/120    avg_loss:0.547, val_acc:0.812]
Epoch [16/120    avg_loss:0.512, val_acc:0.824]
Epoch [17/120    avg_loss:0.504, val_acc:0.842]
Epoch [18/120    avg_loss:0.443, val_acc:0.839]
Epoch [19/120    avg_loss:0.434, val_acc:0.870]
Epoch [20/120    avg_loss:0.325, val_acc:0.887]
Epoch [21/120    avg_loss:0.278, val_acc:0.895]
Epoch [22/120    avg_loss:0.305, val_acc:0.883]
Epoch [23/120    avg_loss:0.259, val_acc:0.887]
Epoch [24/120    avg_loss:0.258, val_acc:0.869]
Epoch [25/120    avg_loss:0.268, val_acc:0.907]
Epoch [26/120    avg_loss:0.242, val_acc:0.887]
Epoch [27/120    avg_loss:0.232, val_acc:0.890]
Epoch [28/120    avg_loss:0.370, val_acc:0.814]
Epoch [29/120    avg_loss:0.389, val_acc:0.822]
Epoch [30/120    avg_loss:0.352, val_acc:0.888]
Epoch [31/120    avg_loss:0.253, val_acc:0.904]
Epoch [32/120    avg_loss:0.199, val_acc:0.905]
Epoch [33/120    avg_loss:0.206, val_acc:0.895]
Epoch [34/120    avg_loss:0.177, val_acc:0.911]
Epoch [35/120    avg_loss:0.131, val_acc:0.938]
Epoch [36/120    avg_loss:0.122, val_acc:0.941]
Epoch [37/120    avg_loss:0.127, val_acc:0.922]
Epoch [38/120    avg_loss:0.122, val_acc:0.933]
Epoch [39/120    avg_loss:0.111, val_acc:0.947]
Epoch [40/120    avg_loss:0.140, val_acc:0.941]
Epoch [41/120    avg_loss:0.124, val_acc:0.929]
Epoch [42/120    avg_loss:0.120, val_acc:0.944]
Epoch [43/120    avg_loss:0.071, val_acc:0.941]
Epoch [44/120    avg_loss:0.109, val_acc:0.955]
Epoch [45/120    avg_loss:0.092, val_acc:0.936]
Epoch [46/120    avg_loss:0.076, val_acc:0.940]
Epoch [47/120    avg_loss:0.071, val_acc:0.936]
Epoch [48/120    avg_loss:0.061, val_acc:0.948]
Epoch [49/120    avg_loss:0.065, val_acc:0.962]
Epoch [50/120    avg_loss:0.061, val_acc:0.967]
Epoch [51/120    avg_loss:0.062, val_acc:0.956]
Epoch [52/120    avg_loss:0.049, val_acc:0.946]
Epoch [53/120    avg_loss:0.059, val_acc:0.966]
Epoch [54/120    avg_loss:0.096, val_acc:0.939]
Epoch [55/120    avg_loss:0.105, val_acc:0.954]
Epoch [56/120    avg_loss:0.081, val_acc:0.948]
Epoch [57/120    avg_loss:0.081, val_acc:0.967]
Epoch [58/120    avg_loss:0.057, val_acc:0.939]
Epoch [59/120    avg_loss:0.092, val_acc:0.956]
Epoch [60/120    avg_loss:0.076, val_acc:0.959]
Epoch [61/120    avg_loss:0.064, val_acc:0.970]
Epoch [62/120    avg_loss:0.043, val_acc:0.964]
Epoch [63/120    avg_loss:0.052, val_acc:0.940]
Epoch [64/120    avg_loss:0.046, val_acc:0.957]
Epoch [65/120    avg_loss:0.051, val_acc:0.967]
Epoch [66/120    avg_loss:0.039, val_acc:0.969]
Epoch [67/120    avg_loss:0.040, val_acc:0.968]
Epoch [68/120    avg_loss:0.041, val_acc:0.966]
Epoch [69/120    avg_loss:0.032, val_acc:0.968]
Epoch [70/120    avg_loss:0.051, val_acc:0.963]
Epoch [71/120    avg_loss:0.030, val_acc:0.976]
Epoch [72/120    avg_loss:0.042, val_acc:0.964]
Epoch [73/120    avg_loss:0.042, val_acc:0.955]
Epoch [74/120    avg_loss:0.046, val_acc:0.976]
Epoch [75/120    avg_loss:0.041, val_acc:0.967]
Epoch [76/120    avg_loss:0.043, val_acc:0.967]
Epoch [77/120    avg_loss:0.026, val_acc:0.973]
Epoch [78/120    avg_loss:0.028, val_acc:0.961]
Epoch [79/120    avg_loss:0.040, val_acc:0.970]
Epoch [80/120    avg_loss:0.076, val_acc:0.961]
Epoch [81/120    avg_loss:0.047, val_acc:0.957]
Epoch [82/120    avg_loss:0.030, val_acc:0.972]
Epoch [83/120    avg_loss:0.029, val_acc:0.973]
Epoch [84/120    avg_loss:0.018, val_acc:0.975]
Epoch [85/120    avg_loss:0.107, val_acc:0.946]
Epoch [86/120    avg_loss:0.067, val_acc:0.958]
Epoch [87/120    avg_loss:0.044, val_acc:0.961]
Epoch [88/120    avg_loss:0.037, val_acc:0.976]
Epoch [89/120    avg_loss:0.024, val_acc:0.980]
Epoch [90/120    avg_loss:0.018, val_acc:0.978]
Epoch [91/120    avg_loss:0.019, val_acc:0.977]
Epoch [92/120    avg_loss:0.021, val_acc:0.978]
Epoch [93/120    avg_loss:0.015, val_acc:0.980]
Epoch [94/120    avg_loss:0.016, val_acc:0.980]
Epoch [95/120    avg_loss:0.015, val_acc:0.977]
Epoch [96/120    avg_loss:0.012, val_acc:0.977]
Epoch [97/120    avg_loss:0.016, val_acc:0.977]
Epoch [98/120    avg_loss:0.014, val_acc:0.980]
Epoch [99/120    avg_loss:0.016, val_acc:0.975]
Epoch [100/120    avg_loss:0.013, val_acc:0.976]
Epoch [101/120    avg_loss:0.017, val_acc:0.978]
Epoch [102/120    avg_loss:0.013, val_acc:0.976]
Epoch [103/120    avg_loss:0.023, val_acc:0.978]
Epoch [104/120    avg_loss:0.015, val_acc:0.978]
Epoch [105/120    avg_loss:0.017, val_acc:0.976]
Epoch [106/120    avg_loss:0.013, val_acc:0.977]
Epoch [107/120    avg_loss:0.012, val_acc:0.978]
Epoch [108/120    avg_loss:0.011, val_acc:0.978]
Epoch [109/120    avg_loss:0.013, val_acc:0.978]
Epoch [110/120    avg_loss:0.015, val_acc:0.980]
Epoch [111/120    avg_loss:0.017, val_acc:0.978]
Epoch [112/120    avg_loss:0.010, val_acc:0.977]
Epoch [113/120    avg_loss:0.016, val_acc:0.978]
Epoch [114/120    avg_loss:0.013, val_acc:0.978]
Epoch [115/120    avg_loss:0.014, val_acc:0.976]
Epoch [116/120    avg_loss:0.014, val_acc:0.976]
Epoch [117/120    avg_loss:0.011, val_acc:0.977]
Epoch [118/120    avg_loss:0.016, val_acc:0.978]
Epoch [119/120    avg_loss:0.013, val_acc:0.982]
Epoch [120/120    avg_loss:0.013, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1253    0   10    0    0    0    0    0   10   12    0    0
     0    0    0]
 [   0    0    0  667   25   15    0    0    0    4    4    0   28    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    5    1    0    0    0  845    9    3    0
     0    4    0]
 [   0    0    9    0    0    0    1    0    0    0    6 2181   12    1
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    0    1  524    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    1    1    0
  1134    0    0]
 [   0    0    0    0    0    0   28    0    0    0    0    0    0    0
    27  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.33333333333333

F1 scores:
[       nan 0.94871795 0.98082192 0.94342291 0.92407809 0.96875
 0.97767857 1.         1.         0.87804878 0.96737264 0.9882193
 0.95099819 0.98666667 0.98608696 0.90824261 0.98823529]

Kappa:
0.9696097558395276
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5fffc7a908>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.630, val_acc:0.411]
Epoch [2/120    avg_loss:2.191, val_acc:0.468]
Epoch [3/120    avg_loss:1.950, val_acc:0.583]
Epoch [4/120    avg_loss:1.758, val_acc:0.607]
Epoch [5/120    avg_loss:1.615, val_acc:0.625]
Epoch [6/120    avg_loss:1.423, val_acc:0.688]
Epoch [7/120    avg_loss:1.260, val_acc:0.693]
Epoch [8/120    avg_loss:1.066, val_acc:0.737]
Epoch [9/120    avg_loss:0.933, val_acc:0.753]
Epoch [10/120    avg_loss:0.813, val_acc:0.765]
Epoch [11/120    avg_loss:0.796, val_acc:0.759]
Epoch [12/120    avg_loss:0.686, val_acc:0.824]
Epoch [13/120    avg_loss:0.571, val_acc:0.858]
Epoch [14/120    avg_loss:0.571, val_acc:0.817]
Epoch [15/120    avg_loss:0.526, val_acc:0.864]
Epoch [16/120    avg_loss:0.475, val_acc:0.863]
Epoch [17/120    avg_loss:0.346, val_acc:0.860]
Epoch [18/120    avg_loss:0.362, val_acc:0.838]
Epoch [19/120    avg_loss:0.332, val_acc:0.894]
Epoch [20/120    avg_loss:0.359, val_acc:0.885]
Epoch [21/120    avg_loss:0.350, val_acc:0.847]
Epoch [22/120    avg_loss:0.302, val_acc:0.906]
Epoch [23/120    avg_loss:0.299, val_acc:0.906]
Epoch [24/120    avg_loss:0.249, val_acc:0.918]
Epoch [25/120    avg_loss:0.220, val_acc:0.927]
Epoch [26/120    avg_loss:0.210, val_acc:0.929]
Epoch [27/120    avg_loss:0.257, val_acc:0.885]
Epoch [28/120    avg_loss:0.282, val_acc:0.930]
Epoch [29/120    avg_loss:0.207, val_acc:0.932]
Epoch [30/120    avg_loss:0.196, val_acc:0.909]
Epoch [31/120    avg_loss:0.237, val_acc:0.923]
Epoch [32/120    avg_loss:0.209, val_acc:0.935]
Epoch [33/120    avg_loss:0.153, val_acc:0.938]
Epoch [34/120    avg_loss:0.125, val_acc:0.949]
Epoch [35/120    avg_loss:0.161, val_acc:0.906]
Epoch [36/120    avg_loss:0.180, val_acc:0.943]
Epoch [37/120    avg_loss:0.172, val_acc:0.921]
Epoch [38/120    avg_loss:0.152, val_acc:0.948]
Epoch [39/120    avg_loss:0.116, val_acc:0.943]
Epoch [40/120    avg_loss:0.135, val_acc:0.942]
Epoch [41/120    avg_loss:0.140, val_acc:0.934]
Epoch [42/120    avg_loss:0.112, val_acc:0.964]
Epoch [43/120    avg_loss:0.112, val_acc:0.961]
Epoch [44/120    avg_loss:0.121, val_acc:0.940]
Epoch [45/120    avg_loss:0.160, val_acc:0.952]
Epoch [46/120    avg_loss:0.092, val_acc:0.947]
Epoch [47/120    avg_loss:0.087, val_acc:0.957]
Epoch [48/120    avg_loss:0.066, val_acc:0.964]
Epoch [49/120    avg_loss:0.071, val_acc:0.947]
Epoch [50/120    avg_loss:0.096, val_acc:0.938]
Epoch [51/120    avg_loss:0.134, val_acc:0.931]
Epoch [52/120    avg_loss:0.112, val_acc:0.957]
Epoch [53/120    avg_loss:0.078, val_acc:0.960]
Epoch [54/120    avg_loss:0.064, val_acc:0.968]
Epoch [55/120    avg_loss:0.082, val_acc:0.964]
Epoch [56/120    avg_loss:0.081, val_acc:0.956]
Epoch [57/120    avg_loss:0.081, val_acc:0.973]
Epoch [58/120    avg_loss:0.053, val_acc:0.967]
Epoch [59/120    avg_loss:0.046, val_acc:0.968]
Epoch [60/120    avg_loss:0.058, val_acc:0.963]
Epoch [61/120    avg_loss:0.047, val_acc:0.972]
Epoch [62/120    avg_loss:0.035, val_acc:0.978]
Epoch [63/120    avg_loss:0.038, val_acc:0.971]
Epoch [64/120    avg_loss:0.126, val_acc:0.909]
Epoch [65/120    avg_loss:0.140, val_acc:0.947]
Epoch [66/120    avg_loss:0.096, val_acc:0.960]
Epoch [67/120    avg_loss:0.065, val_acc:0.973]
Epoch [68/120    avg_loss:0.061, val_acc:0.966]
Epoch [69/120    avg_loss:0.047, val_acc:0.962]
Epoch [70/120    avg_loss:0.063, val_acc:0.959]
Epoch [71/120    avg_loss:0.046, val_acc:0.963]
Epoch [72/120    avg_loss:0.046, val_acc:0.971]
Epoch [73/120    avg_loss:0.043, val_acc:0.962]
Epoch [74/120    avg_loss:0.032, val_acc:0.969]
Epoch [75/120    avg_loss:0.036, val_acc:0.977]
Epoch [76/120    avg_loss:0.026, val_acc:0.981]
Epoch [77/120    avg_loss:0.029, val_acc:0.983]
Epoch [78/120    avg_loss:0.021, val_acc:0.984]
Epoch [79/120    avg_loss:0.019, val_acc:0.984]
Epoch [80/120    avg_loss:0.022, val_acc:0.986]
Epoch [81/120    avg_loss:0.024, val_acc:0.987]
Epoch [82/120    avg_loss:0.025, val_acc:0.986]
Epoch [83/120    avg_loss:0.018, val_acc:0.985]
Epoch [84/120    avg_loss:0.022, val_acc:0.985]
Epoch [85/120    avg_loss:0.016, val_acc:0.983]
Epoch [86/120    avg_loss:0.020, val_acc:0.982]
Epoch [87/120    avg_loss:0.021, val_acc:0.985]
Epoch [88/120    avg_loss:0.020, val_acc:0.983]
Epoch [89/120    avg_loss:0.018, val_acc:0.987]
Epoch [90/120    avg_loss:0.021, val_acc:0.984]
Epoch [91/120    avg_loss:0.015, val_acc:0.985]
Epoch [92/120    avg_loss:0.016, val_acc:0.984]
Epoch [93/120    avg_loss:0.019, val_acc:0.984]
Epoch [94/120    avg_loss:0.021, val_acc:0.982]
Epoch [95/120    avg_loss:0.022, val_acc:0.982]
Epoch [96/120    avg_loss:0.019, val_acc:0.986]
Epoch [97/120    avg_loss:0.021, val_acc:0.988]
Epoch [98/120    avg_loss:0.018, val_acc:0.987]
Epoch [99/120    avg_loss:0.019, val_acc:0.983]
Epoch [100/120    avg_loss:0.018, val_acc:0.984]
Epoch [101/120    avg_loss:0.017, val_acc:0.986]
Epoch [102/120    avg_loss:0.016, val_acc:0.985]
Epoch [103/120    avg_loss:0.017, val_acc:0.984]
Epoch [104/120    avg_loss:0.022, val_acc:0.985]
Epoch [105/120    avg_loss:0.024, val_acc:0.982]
Epoch [106/120    avg_loss:0.015, val_acc:0.984]
Epoch [107/120    avg_loss:0.020, val_acc:0.984]
Epoch [108/120    avg_loss:0.012, val_acc:0.986]
Epoch [109/120    avg_loss:0.016, val_acc:0.986]
Epoch [110/120    avg_loss:0.013, val_acc:0.987]
Epoch [111/120    avg_loss:0.017, val_acc:0.987]
Epoch [112/120    avg_loss:0.015, val_acc:0.985]
Epoch [113/120    avg_loss:0.014, val_acc:0.987]
Epoch [114/120    avg_loss:0.013, val_acc:0.987]
Epoch [115/120    avg_loss:0.017, val_acc:0.985]
Epoch [116/120    avg_loss:0.016, val_acc:0.986]
Epoch [117/120    avg_loss:0.015, val_acc:0.986]
Epoch [118/120    avg_loss:0.013, val_acc:0.986]
Epoch [119/120    avg_loss:0.013, val_acc:0.986]
Epoch [120/120    avg_loss:0.019, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1256    0    0    3    0    0    0    0   11   13    2    0
     0    0    0]
 [   0    0    0  725    0    0    0    0    0    5    3    0    8    6
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   11    0    0    9    0    0    0    0  845    9    0    0
     0    1    0]
 [   0    0    7    0    0    0    2    0    0    0   16 2184    0    1
     0    0    0]
 [   0    0    0    0    5    2    0    0    0    0    0    0  523    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    3    0    0    0
  1132    2    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    12  331    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.36314363143632

F1 scores:
[       nan 0.96202532 0.98163345 0.98438561 0.98604651 0.97848245
 0.9939302  0.98039216 1.         0.80952381 0.96241458 0.98913043
 0.97940075 0.98143236 0.99080963 0.97067449 0.98245614]

Kappa:
0.9813430779306657
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8ea3069898>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.636, val_acc:0.480]
Epoch [2/120    avg_loss:2.177, val_acc:0.522]
Epoch [3/120    avg_loss:1.962, val_acc:0.520]
Epoch [4/120    avg_loss:1.829, val_acc:0.595]
Epoch [5/120    avg_loss:1.644, val_acc:0.607]
Epoch [6/120    avg_loss:1.511, val_acc:0.629]
Epoch [7/120    avg_loss:1.327, val_acc:0.628]
Epoch [8/120    avg_loss:1.197, val_acc:0.670]
Epoch [9/120    avg_loss:1.059, val_acc:0.670]
Epoch [10/120    avg_loss:0.940, val_acc:0.695]
Epoch [11/120    avg_loss:0.836, val_acc:0.731]
Epoch [12/120    avg_loss:0.793, val_acc:0.725]
Epoch [13/120    avg_loss:0.645, val_acc:0.773]
Epoch [14/120    avg_loss:0.608, val_acc:0.733]
Epoch [15/120    avg_loss:0.563, val_acc:0.848]
Epoch [16/120    avg_loss:0.491, val_acc:0.833]
Epoch [17/120    avg_loss:0.411, val_acc:0.845]
Epoch [18/120    avg_loss:0.381, val_acc:0.851]
Epoch [19/120    avg_loss:0.390, val_acc:0.837]
Epoch [20/120    avg_loss:0.370, val_acc:0.852]
Epoch [21/120    avg_loss:0.389, val_acc:0.848]
Epoch [22/120    avg_loss:0.319, val_acc:0.886]
Epoch [23/120    avg_loss:0.292, val_acc:0.880]
Epoch [24/120    avg_loss:0.212, val_acc:0.873]
Epoch [25/120    avg_loss:0.244, val_acc:0.894]
Epoch [26/120    avg_loss:0.206, val_acc:0.913]
Epoch [27/120    avg_loss:0.140, val_acc:0.881]
Epoch [28/120    avg_loss:0.423, val_acc:0.849]
Epoch [29/120    avg_loss:0.391, val_acc:0.879]
Epoch [30/120    avg_loss:0.219, val_acc:0.883]
Epoch [31/120    avg_loss:0.245, val_acc:0.911]
Epoch [32/120    avg_loss:0.162, val_acc:0.916]
Epoch [33/120    avg_loss:0.175, val_acc:0.930]
Epoch [34/120    avg_loss:0.132, val_acc:0.923]
Epoch [35/120    avg_loss:0.129, val_acc:0.905]
Epoch [36/120    avg_loss:0.099, val_acc:0.929]
Epoch [37/120    avg_loss:0.123, val_acc:0.933]
Epoch [38/120    avg_loss:0.088, val_acc:0.930]
Epoch [39/120    avg_loss:0.091, val_acc:0.907]
Epoch [40/120    avg_loss:0.093, val_acc:0.938]
Epoch [41/120    avg_loss:0.114, val_acc:0.932]
Epoch [42/120    avg_loss:0.114, val_acc:0.931]
Epoch [43/120    avg_loss:0.086, val_acc:0.925]
Epoch [44/120    avg_loss:0.126, val_acc:0.892]
Epoch [45/120    avg_loss:0.090, val_acc:0.935]
Epoch [46/120    avg_loss:0.126, val_acc:0.933]
Epoch [47/120    avg_loss:0.116, val_acc:0.927]
Epoch [48/120    avg_loss:0.101, val_acc:0.909]
Epoch [49/120    avg_loss:0.105, val_acc:0.926]
Epoch [50/120    avg_loss:0.075, val_acc:0.948]
Epoch [51/120    avg_loss:0.073, val_acc:0.945]
Epoch [52/120    avg_loss:0.077, val_acc:0.950]
Epoch [53/120    avg_loss:0.089, val_acc:0.945]
Epoch [54/120    avg_loss:0.073, val_acc:0.943]
Epoch [55/120    avg_loss:0.079, val_acc:0.932]
Epoch [56/120    avg_loss:0.059, val_acc:0.952]
Epoch [57/120    avg_loss:0.054, val_acc:0.956]
Epoch [58/120    avg_loss:0.054, val_acc:0.920]
Epoch [59/120    avg_loss:0.080, val_acc:0.947]
Epoch [60/120    avg_loss:0.089, val_acc:0.912]
Epoch [61/120    avg_loss:0.107, val_acc:0.939]
Epoch [62/120    avg_loss:0.057, val_acc:0.947]
Epoch [63/120    avg_loss:0.042, val_acc:0.954]
Epoch [64/120    avg_loss:0.052, val_acc:0.952]
Epoch [65/120    avg_loss:0.042, val_acc:0.949]
Epoch [66/120    avg_loss:0.057, val_acc:0.944]
Epoch [67/120    avg_loss:0.049, val_acc:0.957]
Epoch [68/120    avg_loss:0.040, val_acc:0.959]
Epoch [69/120    avg_loss:0.043, val_acc:0.956]
Epoch [70/120    avg_loss:0.032, val_acc:0.966]
Epoch [71/120    avg_loss:0.046, val_acc:0.946]
Epoch [72/120    avg_loss:0.045, val_acc:0.953]
Epoch [73/120    avg_loss:0.036, val_acc:0.970]
Epoch [74/120    avg_loss:0.041, val_acc:0.956]
Epoch [75/120    avg_loss:0.028, val_acc:0.964]
Epoch [76/120    avg_loss:0.035, val_acc:0.960]
Epoch [77/120    avg_loss:0.036, val_acc:0.955]
Epoch [78/120    avg_loss:0.028, val_acc:0.974]
Epoch [79/120    avg_loss:0.021, val_acc:0.959]
Epoch [80/120    avg_loss:0.017, val_acc:0.959]
Epoch [81/120    avg_loss:0.018, val_acc:0.966]
Epoch [82/120    avg_loss:0.017, val_acc:0.968]
Epoch [83/120    avg_loss:0.021, val_acc:0.955]
Epoch [84/120    avg_loss:0.021, val_acc:0.962]
Epoch [85/120    avg_loss:0.039, val_acc:0.964]
Epoch [86/120    avg_loss:0.038, val_acc:0.956]
Epoch [87/120    avg_loss:0.021, val_acc:0.964]
Epoch [88/120    avg_loss:0.031, val_acc:0.949]
Epoch [89/120    avg_loss:0.019, val_acc:0.970]
Epoch [90/120    avg_loss:0.027, val_acc:0.964]
Epoch [91/120    avg_loss:0.027, val_acc:0.969]
Epoch [92/120    avg_loss:0.020, val_acc:0.973]
Epoch [93/120    avg_loss:0.024, val_acc:0.973]
Epoch [94/120    avg_loss:0.015, val_acc:0.975]
Epoch [95/120    avg_loss:0.013, val_acc:0.974]
Epoch [96/120    avg_loss:0.016, val_acc:0.973]
Epoch [97/120    avg_loss:0.020, val_acc:0.972]
Epoch [98/120    avg_loss:0.015, val_acc:0.973]
Epoch [99/120    avg_loss:0.014, val_acc:0.973]
Epoch [100/120    avg_loss:0.011, val_acc:0.974]
Epoch [101/120    avg_loss:0.015, val_acc:0.974]
Epoch [102/120    avg_loss:0.011, val_acc:0.974]
Epoch [103/120    avg_loss:0.012, val_acc:0.974]
Epoch [104/120    avg_loss:0.012, val_acc:0.975]
Epoch [105/120    avg_loss:0.014, val_acc:0.974]
Epoch [106/120    avg_loss:0.011, val_acc:0.975]
Epoch [107/120    avg_loss:0.011, val_acc:0.974]
Epoch [108/120    avg_loss:0.010, val_acc:0.974]
Epoch [109/120    avg_loss:0.012, val_acc:0.974]
Epoch [110/120    avg_loss:0.012, val_acc:0.975]
Epoch [111/120    avg_loss:0.012, val_acc:0.976]
Epoch [112/120    avg_loss:0.016, val_acc:0.976]
Epoch [113/120    avg_loss:0.016, val_acc:0.975]
Epoch [114/120    avg_loss:0.012, val_acc:0.973]
Epoch [115/120    avg_loss:0.014, val_acc:0.975]
Epoch [116/120    avg_loss:0.012, val_acc:0.973]
Epoch [117/120    avg_loss:0.012, val_acc:0.973]
Epoch [118/120    avg_loss:0.010, val_acc:0.973]
Epoch [119/120    avg_loss:0.012, val_acc:0.974]
Epoch [120/120    avg_loss:0.012, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1248    0    2    0    0    0    0    0    9   26    0    0
     0    0    0]
 [   0    0    0  690    3   16    0    0    0    2    0    1   35    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    3    0    0    0    0  853   12    0    0
     0    2    0]
 [   0    0    2    0    0    0    0    0    0    0    7 2197    1    3
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    9    4  516    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1129    9    0]
 [   0    0    0    0    0    0   48    0    0    0    0    0    0    0
    12  287    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.59349593495935

F1 scores:
[       nan 0.98765432 0.98267717 0.96033403 0.98839907 0.97418631
 0.96475771 1.         1.         0.92307692 0.97207977 0.98741573
 0.9476584  0.9919571  0.99035088 0.88854489 0.97590361]

Kappa:
0.9725531824557805
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efe45c72860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.612, val_acc:0.508]
Epoch [2/120    avg_loss:2.220, val_acc:0.453]
Epoch [3/120    avg_loss:1.927, val_acc:0.574]
Epoch [4/120    avg_loss:1.771, val_acc:0.606]
Epoch [5/120    avg_loss:1.576, val_acc:0.639]
Epoch [6/120    avg_loss:1.417, val_acc:0.657]
Epoch [7/120    avg_loss:1.224, val_acc:0.719]
Epoch [8/120    avg_loss:1.052, val_acc:0.718]
Epoch [9/120    avg_loss:0.903, val_acc:0.769]
Epoch [10/120    avg_loss:0.805, val_acc:0.775]
Epoch [11/120    avg_loss:0.731, val_acc:0.769]
Epoch [12/120    avg_loss:0.720, val_acc:0.750]
Epoch [13/120    avg_loss:0.666, val_acc:0.769]
Epoch [14/120    avg_loss:0.557, val_acc:0.805]
Epoch [15/120    avg_loss:0.543, val_acc:0.762]
Epoch [16/120    avg_loss:0.483, val_acc:0.823]
Epoch [17/120    avg_loss:0.440, val_acc:0.841]
Epoch [18/120    avg_loss:0.373, val_acc:0.856]
Epoch [19/120    avg_loss:0.374, val_acc:0.867]
Epoch [20/120    avg_loss:0.278, val_acc:0.901]
Epoch [21/120    avg_loss:0.329, val_acc:0.861]
Epoch [22/120    avg_loss:0.301, val_acc:0.886]
Epoch [23/120    avg_loss:0.237, val_acc:0.906]
Epoch [24/120    avg_loss:0.241, val_acc:0.883]
Epoch [25/120    avg_loss:0.218, val_acc:0.914]
Epoch [26/120    avg_loss:0.207, val_acc:0.895]
Epoch [27/120    avg_loss:0.220, val_acc:0.894]
Epoch [28/120    avg_loss:0.172, val_acc:0.916]
Epoch [29/120    avg_loss:0.158, val_acc:0.929]
Epoch [30/120    avg_loss:0.144, val_acc:0.919]
Epoch [31/120    avg_loss:0.176, val_acc:0.923]
Epoch [32/120    avg_loss:0.128, val_acc:0.930]
Epoch [33/120    avg_loss:0.151, val_acc:0.935]
Epoch [34/120    avg_loss:0.133, val_acc:0.936]
Epoch [35/120    avg_loss:0.129, val_acc:0.930]
Epoch [36/120    avg_loss:0.154, val_acc:0.935]
Epoch [37/120    avg_loss:0.115, val_acc:0.928]
Epoch [38/120    avg_loss:0.107, val_acc:0.939]
Epoch [39/120    avg_loss:0.101, val_acc:0.929]
Epoch [40/120    avg_loss:0.103, val_acc:0.952]
Epoch [41/120    avg_loss:0.091, val_acc:0.929]
Epoch [42/120    avg_loss:0.077, val_acc:0.945]
Epoch [43/120    avg_loss:0.083, val_acc:0.942]
Epoch [44/120    avg_loss:0.084, val_acc:0.952]
Epoch [45/120    avg_loss:0.087, val_acc:0.950]
Epoch [46/120    avg_loss:0.107, val_acc:0.950]
Epoch [47/120    avg_loss:0.076, val_acc:0.951]
Epoch [48/120    avg_loss:0.071, val_acc:0.949]
Epoch [49/120    avg_loss:0.084, val_acc:0.953]
Epoch [50/120    avg_loss:0.071, val_acc:0.954]
Epoch [51/120    avg_loss:0.051, val_acc:0.947]
Epoch [52/120    avg_loss:0.081, val_acc:0.951]
Epoch [53/120    avg_loss:0.074, val_acc:0.943]
Epoch [54/120    avg_loss:0.074, val_acc:0.954]
Epoch [55/120    avg_loss:0.061, val_acc:0.963]
Epoch [56/120    avg_loss:0.079, val_acc:0.958]
Epoch [57/120    avg_loss:0.050, val_acc:0.960]
Epoch [58/120    avg_loss:0.040, val_acc:0.961]
Epoch [59/120    avg_loss:0.036, val_acc:0.967]
Epoch [60/120    avg_loss:0.039, val_acc:0.966]
Epoch [61/120    avg_loss:0.028, val_acc:0.964]
Epoch [62/120    avg_loss:0.031, val_acc:0.960]
Epoch [63/120    avg_loss:0.045, val_acc:0.936]
Epoch [64/120    avg_loss:0.036, val_acc:0.967]
Epoch [65/120    avg_loss:0.030, val_acc:0.960]
Epoch [66/120    avg_loss:0.025, val_acc:0.967]
Epoch [67/120    avg_loss:0.032, val_acc:0.963]
Epoch [68/120    avg_loss:0.038, val_acc:0.968]
Epoch [69/120    avg_loss:0.041, val_acc:0.964]
Epoch [70/120    avg_loss:0.048, val_acc:0.930]
Epoch [71/120    avg_loss:0.044, val_acc:0.948]
Epoch [72/120    avg_loss:0.042, val_acc:0.814]
Epoch [73/120    avg_loss:0.274, val_acc:0.914]
Epoch [74/120    avg_loss:0.179, val_acc:0.908]
Epoch [75/120    avg_loss:0.254, val_acc:0.860]
Epoch [76/120    avg_loss:0.213, val_acc:0.936]
Epoch [77/120    avg_loss:0.110, val_acc:0.927]
Epoch [78/120    avg_loss:0.099, val_acc:0.957]
Epoch [79/120    avg_loss:0.058, val_acc:0.950]
Epoch [80/120    avg_loss:0.051, val_acc:0.958]
Epoch [81/120    avg_loss:0.044, val_acc:0.958]
Epoch [82/120    avg_loss:0.044, val_acc:0.963]
Epoch [83/120    avg_loss:0.037, val_acc:0.963]
Epoch [84/120    avg_loss:0.029, val_acc:0.967]
Epoch [85/120    avg_loss:0.035, val_acc:0.967]
Epoch [86/120    avg_loss:0.031, val_acc:0.968]
Epoch [87/120    avg_loss:0.026, val_acc:0.970]
Epoch [88/120    avg_loss:0.030, val_acc:0.971]
Epoch [89/120    avg_loss:0.022, val_acc:0.969]
Epoch [90/120    avg_loss:0.029, val_acc:0.969]
Epoch [91/120    avg_loss:0.028, val_acc:0.969]
Epoch [92/120    avg_loss:0.019, val_acc:0.970]
Epoch [93/120    avg_loss:0.028, val_acc:0.969]
Epoch [94/120    avg_loss:0.028, val_acc:0.969]
Epoch [95/120    avg_loss:0.026, val_acc:0.969]
Epoch [96/120    avg_loss:0.026, val_acc:0.972]
Epoch [97/120    avg_loss:0.027, val_acc:0.972]
Epoch [98/120    avg_loss:0.021, val_acc:0.971]
Epoch [99/120    avg_loss:0.019, val_acc:0.970]
Epoch [100/120    avg_loss:0.024, val_acc:0.971]
Epoch [101/120    avg_loss:0.026, val_acc:0.974]
Epoch [102/120    avg_loss:0.026, val_acc:0.974]
Epoch [103/120    avg_loss:0.025, val_acc:0.971]
Epoch [104/120    avg_loss:0.024, val_acc:0.971]
Epoch [105/120    avg_loss:0.022, val_acc:0.971]
Epoch [106/120    avg_loss:0.018, val_acc:0.971]
Epoch [107/120    avg_loss:0.022, val_acc:0.971]
Epoch [108/120    avg_loss:0.022, val_acc:0.971]
Epoch [109/120    avg_loss:0.034, val_acc:0.974]
Epoch [110/120    avg_loss:0.029, val_acc:0.971]
Epoch [111/120    avg_loss:0.022, val_acc:0.974]
Epoch [112/120    avg_loss:0.026, val_acc:0.972]
Epoch [113/120    avg_loss:0.023, val_acc:0.971]
Epoch [114/120    avg_loss:0.022, val_acc:0.970]
Epoch [115/120    avg_loss:0.019, val_acc:0.971]
Epoch [116/120    avg_loss:0.017, val_acc:0.972]
Epoch [117/120    avg_loss:0.020, val_acc:0.971]
Epoch [118/120    avg_loss:0.019, val_acc:0.971]
Epoch [119/120    avg_loss:0.017, val_acc:0.971]
Epoch [120/120    avg_loss:0.019, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1254    0    0    0    0    0    0    1   10   19    1    0
     0    0    0]
 [   0    0    0  720    3    0    0    0    0    2    2    0   18    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    6    2    0    0    0  834   26    0    0
     0    3    0]
 [   0    0    6    0    0    0    2    0    0    0   16 2183    3    0
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    5    0  516    0
     1    1    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1129    8    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    33  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.8319783197832

F1 scores:
[       nan 0.98765432 0.98391526 0.98025868 0.99300699 0.97833523
 0.99467681 0.98039216 0.99767442 0.82051282 0.95642202 0.98355485
 0.96178938 0.99462366 0.97876029 0.93313522 0.97674419]

Kappa:
0.975274442630656
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f68afeef7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.551, val_acc:0.388]
Epoch [2/120    avg_loss:2.170, val_acc:0.495]
Epoch [3/120    avg_loss:1.937, val_acc:0.559]
Epoch [4/120    avg_loss:1.804, val_acc:0.617]
Epoch [5/120    avg_loss:1.605, val_acc:0.615]
Epoch [6/120    avg_loss:1.499, val_acc:0.676]
Epoch [7/120    avg_loss:1.313, val_acc:0.700]
Epoch [8/120    avg_loss:1.177, val_acc:0.706]
Epoch [9/120    avg_loss:1.032, val_acc:0.730]
Epoch [10/120    avg_loss:0.960, val_acc:0.744]
Epoch [11/120    avg_loss:0.826, val_acc:0.800]
Epoch [12/120    avg_loss:0.729, val_acc:0.793]
Epoch [13/120    avg_loss:0.712, val_acc:0.794]
Epoch [14/120    avg_loss:0.574, val_acc:0.825]
Epoch [15/120    avg_loss:0.515, val_acc:0.867]
Epoch [16/120    avg_loss:0.503, val_acc:0.842]
Epoch [17/120    avg_loss:0.444, val_acc:0.855]
Epoch [18/120    avg_loss:0.469, val_acc:0.859]
Epoch [19/120    avg_loss:0.359, val_acc:0.871]
Epoch [20/120    avg_loss:0.391, val_acc:0.898]
Epoch [21/120    avg_loss:0.318, val_acc:0.871]
Epoch [22/120    avg_loss:0.300, val_acc:0.918]
Epoch [23/120    avg_loss:0.262, val_acc:0.916]
Epoch [24/120    avg_loss:0.207, val_acc:0.927]
Epoch [25/120    avg_loss:0.181, val_acc:0.923]
Epoch [26/120    avg_loss:0.370, val_acc:0.804]
Epoch [27/120    avg_loss:0.325, val_acc:0.894]
Epoch [28/120    avg_loss:0.275, val_acc:0.913]
Epoch [29/120    avg_loss:0.186, val_acc:0.916]
Epoch [30/120    avg_loss:0.191, val_acc:0.921]
Epoch [31/120    avg_loss:0.192, val_acc:0.931]
Epoch [32/120    avg_loss:0.213, val_acc:0.933]
Epoch [33/120    avg_loss:0.143, val_acc:0.932]
Epoch [34/120    avg_loss:0.116, val_acc:0.934]
Epoch [35/120    avg_loss:0.113, val_acc:0.938]
Epoch [36/120    avg_loss:0.125, val_acc:0.953]
Epoch [37/120    avg_loss:0.131, val_acc:0.928]
Epoch [38/120    avg_loss:0.111, val_acc:0.961]
Epoch [39/120    avg_loss:0.096, val_acc:0.939]
Epoch [40/120    avg_loss:0.104, val_acc:0.939]
Epoch [41/120    avg_loss:0.101, val_acc:0.956]
Epoch [42/120    avg_loss:0.180, val_acc:0.946]
Epoch [43/120    avg_loss:0.137, val_acc:0.921]
Epoch [44/120    avg_loss:0.150, val_acc:0.935]
Epoch [45/120    avg_loss:0.156, val_acc:0.941]
Epoch [46/120    avg_loss:0.113, val_acc:0.947]
Epoch [47/120    avg_loss:0.107, val_acc:0.934]
Epoch [48/120    avg_loss:0.122, val_acc:0.946]
Epoch [49/120    avg_loss:0.076, val_acc:0.963]
Epoch [50/120    avg_loss:0.066, val_acc:0.959]
Epoch [51/120    avg_loss:0.071, val_acc:0.933]
Epoch [52/120    avg_loss:0.073, val_acc:0.955]
Epoch [53/120    avg_loss:0.057, val_acc:0.970]
Epoch [54/120    avg_loss:0.049, val_acc:0.972]
Epoch [55/120    avg_loss:0.055, val_acc:0.959]
Epoch [56/120    avg_loss:0.045, val_acc:0.960]
Epoch [57/120    avg_loss:0.040, val_acc:0.968]
Epoch [58/120    avg_loss:0.034, val_acc:0.972]
Epoch [59/120    avg_loss:0.047, val_acc:0.936]
Epoch [60/120    avg_loss:0.058, val_acc:0.959]
Epoch [61/120    avg_loss:0.044, val_acc:0.975]
Epoch [62/120    avg_loss:0.039, val_acc:0.973]
Epoch [63/120    avg_loss:0.055, val_acc:0.914]
Epoch [64/120    avg_loss:0.085, val_acc:0.962]
Epoch [65/120    avg_loss:0.062, val_acc:0.968]
Epoch [66/120    avg_loss:0.060, val_acc:0.964]
Epoch [67/120    avg_loss:0.053, val_acc:0.953]
Epoch [68/120    avg_loss:0.049, val_acc:0.972]
Epoch [69/120    avg_loss:0.054, val_acc:0.964]
Epoch [70/120    avg_loss:0.051, val_acc:0.945]
Epoch [71/120    avg_loss:0.066, val_acc:0.956]
Epoch [72/120    avg_loss:0.055, val_acc:0.949]
Epoch [73/120    avg_loss:0.039, val_acc:0.970]
Epoch [74/120    avg_loss:0.027, val_acc:0.975]
Epoch [75/120    avg_loss:0.028, val_acc:0.977]
Epoch [76/120    avg_loss:0.040, val_acc:0.976]
Epoch [77/120    avg_loss:0.034, val_acc:0.973]
Epoch [78/120    avg_loss:0.024, val_acc:0.977]
Epoch [79/120    avg_loss:0.029, val_acc:0.980]
Epoch [80/120    avg_loss:0.025, val_acc:0.976]
Epoch [81/120    avg_loss:0.021, val_acc:0.978]
Epoch [82/120    avg_loss:0.020, val_acc:0.978]
Epoch [83/120    avg_loss:0.015, val_acc:0.974]
Epoch [84/120    avg_loss:0.020, val_acc:0.976]
Epoch [85/120    avg_loss:0.027, val_acc:0.982]
Epoch [86/120    avg_loss:0.014, val_acc:0.980]
Epoch [87/120    avg_loss:0.015, val_acc:0.973]
Epoch [88/120    avg_loss:0.020, val_acc:0.968]
Epoch [89/120    avg_loss:0.022, val_acc:0.969]
Epoch [90/120    avg_loss:0.019, val_acc:0.978]
Epoch [91/120    avg_loss:0.015, val_acc:0.975]
Epoch [92/120    avg_loss:0.016, val_acc:0.971]
Epoch [93/120    avg_loss:0.026, val_acc:0.976]
Epoch [94/120    avg_loss:0.017, val_acc:0.975]
Epoch [95/120    avg_loss:0.018, val_acc:0.976]
Epoch [96/120    avg_loss:0.018, val_acc:0.977]
Epoch [97/120    avg_loss:0.021, val_acc:0.976]
Epoch [98/120    avg_loss:0.017, val_acc:0.970]
Epoch [99/120    avg_loss:0.014, val_acc:0.973]
Epoch [100/120    avg_loss:0.011, val_acc:0.976]
Epoch [101/120    avg_loss:0.009, val_acc:0.977]
Epoch [102/120    avg_loss:0.015, val_acc:0.977]
Epoch [103/120    avg_loss:0.012, val_acc:0.981]
Epoch [104/120    avg_loss:0.008, val_acc:0.982]
Epoch [105/120    avg_loss:0.008, val_acc:0.982]
Epoch [106/120    avg_loss:0.009, val_acc:0.981]
Epoch [107/120    avg_loss:0.011, val_acc:0.980]
Epoch [108/120    avg_loss:0.011, val_acc:0.980]
Epoch [109/120    avg_loss:0.010, val_acc:0.982]
Epoch [110/120    avg_loss:0.009, val_acc:0.982]
Epoch [111/120    avg_loss:0.007, val_acc:0.982]
Epoch [112/120    avg_loss:0.008, val_acc:0.980]
Epoch [113/120    avg_loss:0.010, val_acc:0.980]
Epoch [114/120    avg_loss:0.009, val_acc:0.981]
Epoch [115/120    avg_loss:0.011, val_acc:0.978]
Epoch [116/120    avg_loss:0.010, val_acc:0.978]
Epoch [117/120    avg_loss:0.008, val_acc:0.980]
Epoch [118/120    avg_loss:0.007, val_acc:0.981]
Epoch [119/120    avg_loss:0.009, val_acc:0.980]
Epoch [120/120    avg_loss:0.008, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1245    0    1    0    2    0    0    0   15   20    2    0
     0    0    0]
 [   0    0    0  714    1    0    0    0    0   13    1    0   18    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    7    0    0    0    0  851   11    3    0
     1    1    0]
 [   0    0    8    0    0    1    3    0    0    0   27 2168    1    2
     0    0    0]
 [   0    0    0    0    1   14    0    0    0    0    0    4  508    0
     0    2    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1137    1    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
   109  235    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.86720867208672

F1 scores:
[       nan 1.         0.98070106 0.97607656 0.99300699 0.96956032
 0.99242424 0.98039216 0.99883586 0.68085106 0.96158192 0.98232895
 0.95220244 0.99462366 0.9510665  0.80204778 0.97109827]

Kappa:
0.9642620267541129
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6c39adb8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.647, val_acc:0.406]
Epoch [2/120    avg_loss:2.262, val_acc:0.486]
Epoch [3/120    avg_loss:2.031, val_acc:0.522]
Epoch [4/120    avg_loss:1.787, val_acc:0.561]
Epoch [5/120    avg_loss:1.602, val_acc:0.631]
Epoch [6/120    avg_loss:1.430, val_acc:0.671]
Epoch [7/120    avg_loss:1.291, val_acc:0.633]
Epoch [8/120    avg_loss:1.113, val_acc:0.695]
Epoch [9/120    avg_loss:0.962, val_acc:0.697]
Epoch [10/120    avg_loss:0.825, val_acc:0.765]
Epoch [11/120    avg_loss:0.730, val_acc:0.790]
Epoch [12/120    avg_loss:0.666, val_acc:0.760]
Epoch [13/120    avg_loss:0.654, val_acc:0.780]
Epoch [14/120    avg_loss:0.551, val_acc:0.794]
Epoch [15/120    avg_loss:0.532, val_acc:0.831]
Epoch [16/120    avg_loss:0.454, val_acc:0.814]
Epoch [17/120    avg_loss:0.425, val_acc:0.833]
Epoch [18/120    avg_loss:0.399, val_acc:0.858]
Epoch [19/120    avg_loss:0.310, val_acc:0.850]
Epoch [20/120    avg_loss:0.432, val_acc:0.875]
Epoch [21/120    avg_loss:0.258, val_acc:0.839]
Epoch [22/120    avg_loss:0.253, val_acc:0.874]
Epoch [23/120    avg_loss:0.191, val_acc:0.886]
Epoch [24/120    avg_loss:0.192, val_acc:0.898]
Epoch [25/120    avg_loss:0.194, val_acc:0.898]
Epoch [26/120    avg_loss:0.258, val_acc:0.848]
Epoch [27/120    avg_loss:0.238, val_acc:0.903]
Epoch [28/120    avg_loss:0.189, val_acc:0.906]
Epoch [29/120    avg_loss:0.193, val_acc:0.918]
Epoch [30/120    avg_loss:0.187, val_acc:0.904]
Epoch [31/120    avg_loss:0.141, val_acc:0.892]
Epoch [32/120    avg_loss:0.150, val_acc:0.916]
Epoch [33/120    avg_loss:0.111, val_acc:0.936]
Epoch [34/120    avg_loss:0.112, val_acc:0.921]
Epoch [35/120    avg_loss:0.199, val_acc:0.913]
Epoch [36/120    avg_loss:0.179, val_acc:0.917]
Epoch [37/120    avg_loss:0.163, val_acc:0.920]
Epoch [38/120    avg_loss:0.115, val_acc:0.928]
Epoch [39/120    avg_loss:0.117, val_acc:0.930]
Epoch [40/120    avg_loss:0.105, val_acc:0.919]
Epoch [41/120    avg_loss:0.089, val_acc:0.929]
Epoch [42/120    avg_loss:0.083, val_acc:0.934]
Epoch [43/120    avg_loss:0.104, val_acc:0.947]
Epoch [44/120    avg_loss:0.078, val_acc:0.950]
Epoch [45/120    avg_loss:0.088, val_acc:0.959]
Epoch [46/120    avg_loss:0.064, val_acc:0.940]
Epoch [47/120    avg_loss:0.074, val_acc:0.939]
Epoch [48/120    avg_loss:0.066, val_acc:0.954]
Epoch [49/120    avg_loss:0.060, val_acc:0.950]
Epoch [50/120    avg_loss:0.073, val_acc:0.927]
Epoch [51/120    avg_loss:0.103, val_acc:0.962]
Epoch [52/120    avg_loss:0.079, val_acc:0.933]
Epoch [53/120    avg_loss:0.074, val_acc:0.941]
Epoch [54/120    avg_loss:0.071, val_acc:0.949]
Epoch [55/120    avg_loss:0.056, val_acc:0.946]
Epoch [56/120    avg_loss:0.049, val_acc:0.954]
Epoch [57/120    avg_loss:0.051, val_acc:0.956]
Epoch [58/120    avg_loss:0.047, val_acc:0.960]
Epoch [59/120    avg_loss:0.049, val_acc:0.948]
Epoch [60/120    avg_loss:0.065, val_acc:0.936]
Epoch [61/120    avg_loss:0.078, val_acc:0.935]
Epoch [62/120    avg_loss:0.051, val_acc:0.953]
Epoch [63/120    avg_loss:0.055, val_acc:0.956]
Epoch [64/120    avg_loss:0.036, val_acc:0.944]
Epoch [65/120    avg_loss:0.035, val_acc:0.968]
Epoch [66/120    avg_loss:0.030, val_acc:0.971]
Epoch [67/120    avg_loss:0.022, val_acc:0.970]
Epoch [68/120    avg_loss:0.024, val_acc:0.972]
Epoch [69/120    avg_loss:0.029, val_acc:0.970]
Epoch [70/120    avg_loss:0.030, val_acc:0.972]
Epoch [71/120    avg_loss:0.027, val_acc:0.974]
Epoch [72/120    avg_loss:0.019, val_acc:0.973]
Epoch [73/120    avg_loss:0.023, val_acc:0.974]
Epoch [74/120    avg_loss:0.019, val_acc:0.974]
Epoch [75/120    avg_loss:0.020, val_acc:0.974]
Epoch [76/120    avg_loss:0.023, val_acc:0.975]
Epoch [77/120    avg_loss:0.022, val_acc:0.975]
Epoch [78/120    avg_loss:0.021, val_acc:0.978]
Epoch [79/120    avg_loss:0.025, val_acc:0.975]
Epoch [80/120    avg_loss:0.019, val_acc:0.975]
Epoch [81/120    avg_loss:0.023, val_acc:0.975]
Epoch [82/120    avg_loss:0.023, val_acc:0.975]
Epoch [83/120    avg_loss:0.021, val_acc:0.973]
Epoch [84/120    avg_loss:0.021, val_acc:0.976]
Epoch [85/120    avg_loss:0.023, val_acc:0.976]
Epoch [86/120    avg_loss:0.023, val_acc:0.975]
Epoch [87/120    avg_loss:0.022, val_acc:0.973]
Epoch [88/120    avg_loss:0.020, val_acc:0.975]
Epoch [89/120    avg_loss:0.021, val_acc:0.976]
Epoch [90/120    avg_loss:0.027, val_acc:0.975]
Epoch [91/120    avg_loss:0.018, val_acc:0.974]
Epoch [92/120    avg_loss:0.019, val_acc:0.974]
Epoch [93/120    avg_loss:0.019, val_acc:0.974]
Epoch [94/120    avg_loss:0.020, val_acc:0.974]
Epoch [95/120    avg_loss:0.019, val_acc:0.974]
Epoch [96/120    avg_loss:0.019, val_acc:0.974]
Epoch [97/120    avg_loss:0.019, val_acc:0.974]
Epoch [98/120    avg_loss:0.019, val_acc:0.974]
Epoch [99/120    avg_loss:0.020, val_acc:0.974]
Epoch [100/120    avg_loss:0.021, val_acc:0.974]
Epoch [101/120    avg_loss:0.019, val_acc:0.974]
Epoch [102/120    avg_loss:0.018, val_acc:0.974]
Epoch [103/120    avg_loss:0.025, val_acc:0.974]
Epoch [104/120    avg_loss:0.020, val_acc:0.974]
Epoch [105/120    avg_loss:0.016, val_acc:0.974]
Epoch [106/120    avg_loss:0.018, val_acc:0.974]
Epoch [107/120    avg_loss:0.016, val_acc:0.974]
Epoch [108/120    avg_loss:0.018, val_acc:0.974]
Epoch [109/120    avg_loss:0.019, val_acc:0.974]
Epoch [110/120    avg_loss:0.017, val_acc:0.974]
Epoch [111/120    avg_loss:0.019, val_acc:0.974]
Epoch [112/120    avg_loss:0.022, val_acc:0.974]
Epoch [113/120    avg_loss:0.018, val_acc:0.974]
Epoch [114/120    avg_loss:0.019, val_acc:0.974]
Epoch [115/120    avg_loss:0.022, val_acc:0.974]
Epoch [116/120    avg_loss:0.015, val_acc:0.974]
Epoch [117/120    avg_loss:0.016, val_acc:0.974]
Epoch [118/120    avg_loss:0.018, val_acc:0.974]
Epoch [119/120    avg_loss:0.020, val_acc:0.974]
Epoch [120/120    avg_loss:0.020, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1242    4    0    3    0    0    0    0    5   26    5    0
     0    0    0]
 [   0    0    1  733    0    4    0    0    0    1    0    0    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   19    0    0    2    0    0    0    0  845    4    0    0
     0    5    0]
 [   0    0    3    0    0    0    3    0    0    0    7 2194    0    1
     0    2    0]
 [   0    0    0    2    0    6    0    0    0    0    4    0  518    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    0    0    0
  1122   14    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    43  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.95121951219512

F1 scores:
[       nan 0.975      0.97411765 0.98654105 1.         0.97959184
 0.99392097 1.         1.         0.97297297 0.97070649 0.98962562
 0.97276995 0.99730458 0.97142857 0.90149254 0.97674419]

Kappa:
0.9766342483963967
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa79aef6908>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.655, val_acc:0.421]
Epoch [2/120    avg_loss:2.266, val_acc:0.498]
Epoch [3/120    avg_loss:1.998, val_acc:0.564]
Epoch [4/120    avg_loss:1.772, val_acc:0.588]
Epoch [5/120    avg_loss:1.598, val_acc:0.603]
Epoch [6/120    avg_loss:1.401, val_acc:0.657]
Epoch [7/120    avg_loss:1.179, val_acc:0.716]
Epoch [8/120    avg_loss:1.017, val_acc:0.690]
Epoch [9/120    avg_loss:0.965, val_acc:0.737]
Epoch [10/120    avg_loss:0.833, val_acc:0.750]
Epoch [11/120    avg_loss:0.702, val_acc:0.794]
Epoch [12/120    avg_loss:0.684, val_acc:0.823]
Epoch [13/120    avg_loss:0.627, val_acc:0.786]
Epoch [14/120    avg_loss:0.583, val_acc:0.822]
Epoch [15/120    avg_loss:0.456, val_acc:0.836]
Epoch [16/120    avg_loss:0.410, val_acc:0.854]
Epoch [17/120    avg_loss:0.432, val_acc:0.806]
Epoch [18/120    avg_loss:0.370, val_acc:0.840]
Epoch [19/120    avg_loss:0.422, val_acc:0.879]
Epoch [20/120    avg_loss:0.303, val_acc:0.840]
Epoch [21/120    avg_loss:0.348, val_acc:0.871]
Epoch [22/120    avg_loss:0.255, val_acc:0.904]
Epoch [23/120    avg_loss:0.219, val_acc:0.899]
Epoch [24/120    avg_loss:0.283, val_acc:0.868]
Epoch [25/120    avg_loss:0.224, val_acc:0.912]
Epoch [26/120    avg_loss:0.216, val_acc:0.903]
Epoch [27/120    avg_loss:0.190, val_acc:0.891]
Epoch [28/120    avg_loss:0.255, val_acc:0.911]
Epoch [29/120    avg_loss:0.422, val_acc:0.791]
Epoch [30/120    avg_loss:0.443, val_acc:0.864]
Epoch [31/120    avg_loss:0.240, val_acc:0.879]
Epoch [32/120    avg_loss:0.213, val_acc:0.909]
Epoch [33/120    avg_loss:0.172, val_acc:0.908]
Epoch [34/120    avg_loss:0.182, val_acc:0.927]
Epoch [35/120    avg_loss:0.207, val_acc:0.882]
Epoch [36/120    avg_loss:0.180, val_acc:0.916]
Epoch [37/120    avg_loss:0.131, val_acc:0.917]
Epoch [38/120    avg_loss:0.109, val_acc:0.932]
Epoch [39/120    avg_loss:0.106, val_acc:0.940]
Epoch [40/120    avg_loss:0.099, val_acc:0.922]
Epoch [41/120    avg_loss:0.101, val_acc:0.929]
Epoch [42/120    avg_loss:0.106, val_acc:0.947]
Epoch [43/120    avg_loss:0.095, val_acc:0.942]
Epoch [44/120    avg_loss:0.098, val_acc:0.944]
Epoch [45/120    avg_loss:0.073, val_acc:0.949]
Epoch [46/120    avg_loss:0.068, val_acc:0.950]
Epoch [47/120    avg_loss:0.073, val_acc:0.941]
Epoch [48/120    avg_loss:0.084, val_acc:0.939]
Epoch [49/120    avg_loss:0.068, val_acc:0.947]
Epoch [50/120    avg_loss:0.066, val_acc:0.961]
Epoch [51/120    avg_loss:0.101, val_acc:0.953]
Epoch [52/120    avg_loss:0.053, val_acc:0.965]
Epoch [53/120    avg_loss:0.061, val_acc:0.953]
Epoch [54/120    avg_loss:0.048, val_acc:0.959]
Epoch [55/120    avg_loss:0.041, val_acc:0.956]
Epoch [56/120    avg_loss:0.058, val_acc:0.959]
Epoch [57/120    avg_loss:0.063, val_acc:0.950]
Epoch [58/120    avg_loss:0.044, val_acc:0.961]
Epoch [59/120    avg_loss:0.073, val_acc:0.941]
Epoch [60/120    avg_loss:0.079, val_acc:0.956]
Epoch [61/120    avg_loss:0.041, val_acc:0.952]
Epoch [62/120    avg_loss:0.044, val_acc:0.960]
Epoch [63/120    avg_loss:0.052, val_acc:0.931]
Epoch [64/120    avg_loss:0.053, val_acc:0.953]
Epoch [65/120    avg_loss:0.051, val_acc:0.926]
Epoch [66/120    avg_loss:0.045, val_acc:0.957]
Epoch [67/120    avg_loss:0.033, val_acc:0.956]
Epoch [68/120    avg_loss:0.039, val_acc:0.958]
Epoch [69/120    avg_loss:0.028, val_acc:0.959]
Epoch [70/120    avg_loss:0.035, val_acc:0.960]
Epoch [71/120    avg_loss:0.024, val_acc:0.961]
Epoch [72/120    avg_loss:0.030, val_acc:0.963]
Epoch [73/120    avg_loss:0.026, val_acc:0.963]
Epoch [74/120    avg_loss:0.023, val_acc:0.964]
Epoch [75/120    avg_loss:0.022, val_acc:0.965]
Epoch [76/120    avg_loss:0.025, val_acc:0.968]
Epoch [77/120    avg_loss:0.019, val_acc:0.965]
Epoch [78/120    avg_loss:0.023, val_acc:0.967]
Epoch [79/120    avg_loss:0.024, val_acc:0.969]
Epoch [80/120    avg_loss:0.023, val_acc:0.966]
Epoch [81/120    avg_loss:0.025, val_acc:0.968]
Epoch [82/120    avg_loss:0.018, val_acc:0.968]
Epoch [83/120    avg_loss:0.023, val_acc:0.968]
Epoch [84/120    avg_loss:0.023, val_acc:0.969]
Epoch [85/120    avg_loss:0.024, val_acc:0.967]
Epoch [86/120    avg_loss:0.020, val_acc:0.968]
Epoch [87/120    avg_loss:0.020, val_acc:0.968]
Epoch [88/120    avg_loss:0.021, val_acc:0.968]
Epoch [89/120    avg_loss:0.022, val_acc:0.967]
Epoch [90/120    avg_loss:0.016, val_acc:0.967]
Epoch [91/120    avg_loss:0.019, val_acc:0.967]
Epoch [92/120    avg_loss:0.027, val_acc:0.967]
Epoch [93/120    avg_loss:0.019, val_acc:0.967]
Epoch [94/120    avg_loss:0.022, val_acc:0.968]
Epoch [95/120    avg_loss:0.020, val_acc:0.972]
Epoch [96/120    avg_loss:0.021, val_acc:0.971]
Epoch [97/120    avg_loss:0.020, val_acc:0.968]
Epoch [98/120    avg_loss:0.021, val_acc:0.969]
Epoch [99/120    avg_loss:0.021, val_acc:0.969]
Epoch [100/120    avg_loss:0.015, val_acc:0.970]
Epoch [101/120    avg_loss:0.019, val_acc:0.974]
Epoch [102/120    avg_loss:0.017, val_acc:0.974]
Epoch [103/120    avg_loss:0.018, val_acc:0.970]
Epoch [104/120    avg_loss:0.018, val_acc:0.971]
Epoch [105/120    avg_loss:0.020, val_acc:0.968]
Epoch [106/120    avg_loss:0.018, val_acc:0.969]
Epoch [107/120    avg_loss:0.024, val_acc:0.971]
Epoch [108/120    avg_loss:0.018, val_acc:0.974]
Epoch [109/120    avg_loss:0.017, val_acc:0.971]
Epoch [110/120    avg_loss:0.017, val_acc:0.974]
Epoch [111/120    avg_loss:0.019, val_acc:0.972]
Epoch [112/120    avg_loss:0.033, val_acc:0.972]
Epoch [113/120    avg_loss:0.018, val_acc:0.968]
Epoch [114/120    avg_loss:0.019, val_acc:0.970]
Epoch [115/120    avg_loss:0.018, val_acc:0.967]
Epoch [116/120    avg_loss:0.018, val_acc:0.968]
Epoch [117/120    avg_loss:0.023, val_acc:0.970]
Epoch [118/120    avg_loss:0.017, val_acc:0.971]
Epoch [119/120    avg_loss:0.020, val_acc:0.968]
Epoch [120/120    avg_loss:0.020, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1252    4    5    0    0    0    0    2   11   11    0    0
     0    0    0]
 [   0    0    0  727    1    5    0    0    0    3    0    0    8    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    8    5    0    0    1  834   19    0    0
     0    0    0]
 [   0    0   13    0    0    0    2    0    0    0   11 2159   24    1
     0    0    0]
 [   0    0    0    1    0    8    0    0    0    0    0    1  522    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    3    0    0    0
  1132    2    0]
 [   0    0    0    0    0    0   33    0    0    0    0    0    0    0
     5  309    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.76693766937669

F1 scores:
[       nan 1.         0.97888976 0.98309669 0.98611111 0.97533632
 0.96740741 1.         0.99883856 0.85714286 0.96193772 0.9804723
 0.95955882 0.98930481 0.99472759 0.93920973 0.98823529]

Kappa:
0.9745561042019483
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f047c439860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.612, val_acc:0.429]
Epoch [2/120    avg_loss:2.226, val_acc:0.539]
Epoch [3/120    avg_loss:2.005, val_acc:0.556]
Epoch [4/120    avg_loss:1.795, val_acc:0.576]
Epoch [5/120    avg_loss:1.661, val_acc:0.604]
Epoch [6/120    avg_loss:1.479, val_acc:0.646]
Epoch [7/120    avg_loss:1.270, val_acc:0.685]
Epoch [8/120    avg_loss:1.163, val_acc:0.734]
Epoch [9/120    avg_loss:0.999, val_acc:0.746]
Epoch [10/120    avg_loss:0.959, val_acc:0.738]
Epoch [11/120    avg_loss:0.834, val_acc:0.744]
Epoch [12/120    avg_loss:0.746, val_acc:0.779]
Epoch [13/120    avg_loss:0.772, val_acc:0.766]
Epoch [14/120    avg_loss:0.619, val_acc:0.818]
Epoch [15/120    avg_loss:0.615, val_acc:0.739]
Epoch [16/120    avg_loss:0.555, val_acc:0.800]
Epoch [17/120    avg_loss:0.534, val_acc:0.845]
Epoch [18/120    avg_loss:0.432, val_acc:0.833]
Epoch [19/120    avg_loss:0.373, val_acc:0.858]
Epoch [20/120    avg_loss:0.335, val_acc:0.890]
Epoch [21/120    avg_loss:0.325, val_acc:0.868]
Epoch [22/120    avg_loss:0.363, val_acc:0.889]
Epoch [23/120    avg_loss:0.352, val_acc:0.834]
Epoch [24/120    avg_loss:0.335, val_acc:0.897]
Epoch [25/120    avg_loss:0.275, val_acc:0.895]
Epoch [26/120    avg_loss:0.210, val_acc:0.904]
Epoch [27/120    avg_loss:0.221, val_acc:0.918]
Epoch [28/120    avg_loss:0.251, val_acc:0.919]
Epoch [29/120    avg_loss:0.186, val_acc:0.909]
Epoch [30/120    avg_loss:0.168, val_acc:0.932]
Epoch [31/120    avg_loss:0.161, val_acc:0.898]
Epoch [32/120    avg_loss:0.155, val_acc:0.921]
Epoch [33/120    avg_loss:0.132, val_acc:0.952]
Epoch [34/120    avg_loss:0.111, val_acc:0.940]
Epoch [35/120    avg_loss:0.114, val_acc:0.918]
Epoch [36/120    avg_loss:0.125, val_acc:0.925]
Epoch [37/120    avg_loss:0.128, val_acc:0.938]
Epoch [38/120    avg_loss:0.119, val_acc:0.932]
Epoch [39/120    avg_loss:0.120, val_acc:0.939]
Epoch [40/120    avg_loss:0.135, val_acc:0.945]
Epoch [41/120    avg_loss:0.114, val_acc:0.928]
Epoch [42/120    avg_loss:0.125, val_acc:0.941]
Epoch [43/120    avg_loss:0.096, val_acc:0.946]
Epoch [44/120    avg_loss:0.088, val_acc:0.943]
Epoch [45/120    avg_loss:0.081, val_acc:0.932]
Epoch [46/120    avg_loss:0.104, val_acc:0.950]
Epoch [47/120    avg_loss:0.065, val_acc:0.954]
Epoch [48/120    avg_loss:0.056, val_acc:0.960]
Epoch [49/120    avg_loss:0.054, val_acc:0.963]
Epoch [50/120    avg_loss:0.049, val_acc:0.966]
Epoch [51/120    avg_loss:0.045, val_acc:0.966]
Epoch [52/120    avg_loss:0.047, val_acc:0.969]
Epoch [53/120    avg_loss:0.042, val_acc:0.964]
Epoch [54/120    avg_loss:0.046, val_acc:0.963]
Epoch [55/120    avg_loss:0.043, val_acc:0.968]
Epoch [56/120    avg_loss:0.051, val_acc:0.963]
Epoch [57/120    avg_loss:0.040, val_acc:0.965]
Epoch [58/120    avg_loss:0.035, val_acc:0.967]
Epoch [59/120    avg_loss:0.042, val_acc:0.962]
Epoch [60/120    avg_loss:0.040, val_acc:0.962]
Epoch [61/120    avg_loss:0.036, val_acc:0.964]
Epoch [62/120    avg_loss:0.041, val_acc:0.964]
Epoch [63/120    avg_loss:0.036, val_acc:0.966]
Epoch [64/120    avg_loss:0.048, val_acc:0.966]
Epoch [65/120    avg_loss:0.037, val_acc:0.959]
Epoch [66/120    avg_loss:0.039, val_acc:0.962]
Epoch [67/120    avg_loss:0.036, val_acc:0.961]
Epoch [68/120    avg_loss:0.039, val_acc:0.962]
Epoch [69/120    avg_loss:0.035, val_acc:0.964]
Epoch [70/120    avg_loss:0.037, val_acc:0.964]
Epoch [71/120    avg_loss:0.036, val_acc:0.965]
Epoch [72/120    avg_loss:0.045, val_acc:0.965]
Epoch [73/120    avg_loss:0.048, val_acc:0.965]
Epoch [74/120    avg_loss:0.038, val_acc:0.966]
Epoch [75/120    avg_loss:0.037, val_acc:0.967]
Epoch [76/120    avg_loss:0.031, val_acc:0.967]
Epoch [77/120    avg_loss:0.035, val_acc:0.966]
Epoch [78/120    avg_loss:0.030, val_acc:0.966]
Epoch [79/120    avg_loss:0.035, val_acc:0.966]
Epoch [80/120    avg_loss:0.033, val_acc:0.966]
Epoch [81/120    avg_loss:0.037, val_acc:0.966]
Epoch [82/120    avg_loss:0.033, val_acc:0.967]
Epoch [83/120    avg_loss:0.036, val_acc:0.967]
Epoch [84/120    avg_loss:0.042, val_acc:0.967]
Epoch [85/120    avg_loss:0.031, val_acc:0.966]
Epoch [86/120    avg_loss:0.036, val_acc:0.967]
Epoch [87/120    avg_loss:0.041, val_acc:0.966]
Epoch [88/120    avg_loss:0.035, val_acc:0.966]
Epoch [89/120    avg_loss:0.032, val_acc:0.967]
Epoch [90/120    avg_loss:0.032, val_acc:0.967]
Epoch [91/120    avg_loss:0.037, val_acc:0.967]
Epoch [92/120    avg_loss:0.042, val_acc:0.967]
Epoch [93/120    avg_loss:0.037, val_acc:0.967]
Epoch [94/120    avg_loss:0.034, val_acc:0.967]
Epoch [95/120    avg_loss:0.035, val_acc:0.967]
Epoch [96/120    avg_loss:0.036, val_acc:0.967]
Epoch [97/120    avg_loss:0.030, val_acc:0.967]
Epoch [98/120    avg_loss:0.045, val_acc:0.967]
Epoch [99/120    avg_loss:0.031, val_acc:0.967]
Epoch [100/120    avg_loss:0.035, val_acc:0.967]
Epoch [101/120    avg_loss:0.032, val_acc:0.967]
Epoch [102/120    avg_loss:0.034, val_acc:0.967]
Epoch [103/120    avg_loss:0.047, val_acc:0.967]
Epoch [104/120    avg_loss:0.037, val_acc:0.967]
Epoch [105/120    avg_loss:0.033, val_acc:0.967]
Epoch [106/120    avg_loss:0.034, val_acc:0.967]
Epoch [107/120    avg_loss:0.035, val_acc:0.967]
Epoch [108/120    avg_loss:0.033, val_acc:0.967]
Epoch [109/120    avg_loss:0.036, val_acc:0.967]
Epoch [110/120    avg_loss:0.036, val_acc:0.967]
Epoch [111/120    avg_loss:0.030, val_acc:0.967]
Epoch [112/120    avg_loss:0.038, val_acc:0.967]
Epoch [113/120    avg_loss:0.037, val_acc:0.967]
Epoch [114/120    avg_loss:0.032, val_acc:0.967]
Epoch [115/120    avg_loss:0.034, val_acc:0.967]
Epoch [116/120    avg_loss:0.032, val_acc:0.967]
Epoch [117/120    avg_loss:0.034, val_acc:0.967]
Epoch [118/120    avg_loss:0.036, val_acc:0.967]
Epoch [119/120    avg_loss:0.037, val_acc:0.967]
Epoch [120/120    avg_loss:0.033, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1239    2    1    1    0    0    0    0   10   29    3    0
     0    0    0]
 [   0    0    7  692    6    6    0    0    0   10    2    0   21    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   43    0    0    6    0    0    0    0  800   24    0    0
     0    2    0]
 [   0    0    6    0    0    0    4    0    0    0    5 2180   11    4
     0    0    0]
 [   0    0    0    0    6    2    0    0    0    1    3    0  518    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1132    6    0]
 [   0    0    0    0    0    0   17    0    0    8    0    0    0    0
    35  287    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.73712737127371

F1 scores:
[       nan 0.95121951 0.96046512 0.96044414 0.97038724 0.97959184
 0.98045113 1.         0.997669   0.64285714 0.94228504 0.98109811
 0.95308188 0.98143236 0.97881539 0.894081   0.98245614]

Kappa:
0.962784178377431
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1bf44678d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.622, val_acc:0.436]
Epoch [2/120    avg_loss:2.222, val_acc:0.509]
Epoch [3/120    avg_loss:1.986, val_acc:0.505]
Epoch [4/120    avg_loss:1.835, val_acc:0.579]
Epoch [5/120    avg_loss:1.673, val_acc:0.597]
Epoch [6/120    avg_loss:1.500, val_acc:0.638]
Epoch [7/120    avg_loss:1.382, val_acc:0.666]
Epoch [8/120    avg_loss:1.218, val_acc:0.652]
Epoch [9/120    avg_loss:1.108, val_acc:0.670]
Epoch [10/120    avg_loss:0.994, val_acc:0.695]
Epoch [11/120    avg_loss:0.892, val_acc:0.722]
Epoch [12/120    avg_loss:0.828, val_acc:0.708]
Epoch [13/120    avg_loss:0.761, val_acc:0.790]
Epoch [14/120    avg_loss:0.726, val_acc:0.804]
Epoch [15/120    avg_loss:0.618, val_acc:0.794]
Epoch [16/120    avg_loss:0.758, val_acc:0.781]
Epoch [17/120    avg_loss:0.683, val_acc:0.759]
Epoch [18/120    avg_loss:0.578, val_acc:0.863]
Epoch [19/120    avg_loss:0.502, val_acc:0.847]
Epoch [20/120    avg_loss:0.409, val_acc:0.871]
Epoch [21/120    avg_loss:0.421, val_acc:0.881]
Epoch [22/120    avg_loss:0.398, val_acc:0.853]
Epoch [23/120    avg_loss:0.321, val_acc:0.874]
Epoch [24/120    avg_loss:0.377, val_acc:0.895]
Epoch [25/120    avg_loss:0.300, val_acc:0.884]
Epoch [26/120    avg_loss:0.326, val_acc:0.903]
Epoch [27/120    avg_loss:0.285, val_acc:0.898]
Epoch [28/120    avg_loss:0.286, val_acc:0.872]
Epoch [29/120    avg_loss:0.228, val_acc:0.920]
Epoch [30/120    avg_loss:0.229, val_acc:0.897]
Epoch [31/120    avg_loss:0.297, val_acc:0.913]
Epoch [32/120    avg_loss:0.230, val_acc:0.927]
Epoch [33/120    avg_loss:0.203, val_acc:0.916]
Epoch [34/120    avg_loss:0.254, val_acc:0.900]
Epoch [35/120    avg_loss:0.199, val_acc:0.938]
Epoch [36/120    avg_loss:0.135, val_acc:0.913]
Epoch [37/120    avg_loss:0.147, val_acc:0.930]
Epoch [38/120    avg_loss:0.183, val_acc:0.919]
Epoch [39/120    avg_loss:0.151, val_acc:0.914]
Epoch [40/120    avg_loss:0.164, val_acc:0.915]
Epoch [41/120    avg_loss:0.172, val_acc:0.927]
Epoch [42/120    avg_loss:0.169, val_acc:0.916]
Epoch [43/120    avg_loss:0.122, val_acc:0.932]
Epoch [44/120    avg_loss:0.103, val_acc:0.943]
Epoch [45/120    avg_loss:0.108, val_acc:0.926]
Epoch [46/120    avg_loss:0.107, val_acc:0.936]
Epoch [47/120    avg_loss:0.092, val_acc:0.944]
Epoch [48/120    avg_loss:0.077, val_acc:0.949]
Epoch [49/120    avg_loss:0.063, val_acc:0.950]
Epoch [50/120    avg_loss:0.106, val_acc:0.928]
Epoch [51/120    avg_loss:0.131, val_acc:0.945]
Epoch [52/120    avg_loss:0.099, val_acc:0.958]
Epoch [53/120    avg_loss:0.085, val_acc:0.946]
Epoch [54/120    avg_loss:0.096, val_acc:0.942]
Epoch [55/120    avg_loss:0.077, val_acc:0.936]
Epoch [56/120    avg_loss:0.088, val_acc:0.943]
Epoch [57/120    avg_loss:0.072, val_acc:0.942]
Epoch [58/120    avg_loss:0.091, val_acc:0.952]
Epoch [59/120    avg_loss:0.065, val_acc:0.957]
Epoch [60/120    avg_loss:0.123, val_acc:0.930]
Epoch [61/120    avg_loss:0.118, val_acc:0.952]
Epoch [62/120    avg_loss:0.099, val_acc:0.948]
Epoch [63/120    avg_loss:0.137, val_acc:0.936]
Epoch [64/120    avg_loss:0.146, val_acc:0.914]
Epoch [65/120    avg_loss:0.121, val_acc:0.941]
Epoch [66/120    avg_loss:0.067, val_acc:0.949]
Epoch [67/120    avg_loss:0.058, val_acc:0.959]
Epoch [68/120    avg_loss:0.053, val_acc:0.958]
Epoch [69/120    avg_loss:0.059, val_acc:0.962]
Epoch [70/120    avg_loss:0.050, val_acc:0.962]
Epoch [71/120    avg_loss:0.049, val_acc:0.960]
Epoch [72/120    avg_loss:0.062, val_acc:0.964]
Epoch [73/120    avg_loss:0.042, val_acc:0.966]
Epoch [74/120    avg_loss:0.046, val_acc:0.963]
Epoch [75/120    avg_loss:0.051, val_acc:0.964]
Epoch [76/120    avg_loss:0.049, val_acc:0.968]
Epoch [77/120    avg_loss:0.044, val_acc:0.968]
Epoch [78/120    avg_loss:0.041, val_acc:0.969]
Epoch [79/120    avg_loss:0.047, val_acc:0.967]
Epoch [80/120    avg_loss:0.041, val_acc:0.967]
Epoch [81/120    avg_loss:0.040, val_acc:0.963]
Epoch [82/120    avg_loss:0.042, val_acc:0.966]
Epoch [83/120    avg_loss:0.039, val_acc:0.966]
Epoch [84/120    avg_loss:0.043, val_acc:0.966]
Epoch [85/120    avg_loss:0.038, val_acc:0.967]
Epoch [86/120    avg_loss:0.037, val_acc:0.969]
Epoch [87/120    avg_loss:0.047, val_acc:0.966]
Epoch [88/120    avg_loss:0.040, val_acc:0.970]
Epoch [89/120    avg_loss:0.036, val_acc:0.972]
Epoch [90/120    avg_loss:0.031, val_acc:0.972]
Epoch [91/120    avg_loss:0.031, val_acc:0.970]
Epoch [92/120    avg_loss:0.044, val_acc:0.967]
Epoch [93/120    avg_loss:0.040, val_acc:0.966]
Epoch [94/120    avg_loss:0.036, val_acc:0.967]
Epoch [95/120    avg_loss:0.034, val_acc:0.967]
Epoch [96/120    avg_loss:0.029, val_acc:0.970]
Epoch [97/120    avg_loss:0.034, val_acc:0.969]
Epoch [98/120    avg_loss:0.033, val_acc:0.967]
Epoch [99/120    avg_loss:0.033, val_acc:0.969]
Epoch [100/120    avg_loss:0.031, val_acc:0.971]
Epoch [101/120    avg_loss:0.033, val_acc:0.972]
Epoch [102/120    avg_loss:0.031, val_acc:0.969]
Epoch [103/120    avg_loss:0.043, val_acc:0.969]
Epoch [104/120    avg_loss:0.034, val_acc:0.969]
Epoch [105/120    avg_loss:0.032, val_acc:0.968]
Epoch [106/120    avg_loss:0.031, val_acc:0.969]
Epoch [107/120    avg_loss:0.032, val_acc:0.968]
Epoch [108/120    avg_loss:0.033, val_acc:0.967]
Epoch [109/120    avg_loss:0.035, val_acc:0.967]
Epoch [110/120    avg_loss:0.034, val_acc:0.968]
Epoch [111/120    avg_loss:0.031, val_acc:0.967]
Epoch [112/120    avg_loss:0.031, val_acc:0.966]
Epoch [113/120    avg_loss:0.038, val_acc:0.966]
Epoch [114/120    avg_loss:0.024, val_acc:0.970]
Epoch [115/120    avg_loss:0.026, val_acc:0.971]
Epoch [116/120    avg_loss:0.028, val_acc:0.972]
Epoch [117/120    avg_loss:0.030, val_acc:0.970]
Epoch [118/120    avg_loss:0.030, val_acc:0.971]
Epoch [119/120    avg_loss:0.034, val_acc:0.970]
Epoch [120/120    avg_loss:0.029, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1235    9    0    0    0    0    0    0    3   37    1    0
     0    0    0]
 [   0    0    1  708    1    3    0    0    0   13    0    2   16    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    6    0    0    0    0    0    0  424    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   20    0    0    9    0    0    0    0  837    6    0    0
     0    3    0]
 [   0    0   11    0    0    0    3    0    0    1   30 2159    4    1
     1    0    0]
 [   0    0    0    0    1    5    0    0    0    5    9    1  508    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    0    0    0
  1118   18    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    28  319    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.11653116531166

F1 scores:
[       nan 0.90697674 0.96786834 0.96721311 0.9953271  0.98083427
 0.9946687  1.         0.99297424 0.62962963 0.95167709 0.9775866
 0.95488722 0.98930481 0.97727273 0.9286754  0.97109827]

Kappa:
0.9671395891131598
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5a468b2860>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.640, val_acc:0.461]
Epoch [2/120    avg_loss:2.232, val_acc:0.516]
Epoch [3/120    avg_loss:2.036, val_acc:0.536]
Epoch [4/120    avg_loss:1.867, val_acc:0.579]
Epoch [5/120    avg_loss:1.712, val_acc:0.594]
Epoch [6/120    avg_loss:1.539, val_acc:0.637]
Epoch [7/120    avg_loss:1.419, val_acc:0.623]
Epoch [8/120    avg_loss:1.282, val_acc:0.668]
Epoch [9/120    avg_loss:1.202, val_acc:0.642]
Epoch [10/120    avg_loss:1.047, val_acc:0.689]
Epoch [11/120    avg_loss:0.924, val_acc:0.732]
Epoch [12/120    avg_loss:0.844, val_acc:0.768]
Epoch [13/120    avg_loss:0.779, val_acc:0.730]
Epoch [14/120    avg_loss:0.850, val_acc:0.777]
Epoch [15/120    avg_loss:0.658, val_acc:0.809]
Epoch [16/120    avg_loss:0.645, val_acc:0.806]
Epoch [17/120    avg_loss:0.533, val_acc:0.808]
Epoch [18/120    avg_loss:0.426, val_acc:0.851]
Epoch [19/120    avg_loss:0.450, val_acc:0.823]
Epoch [20/120    avg_loss:0.459, val_acc:0.833]
Epoch [21/120    avg_loss:0.556, val_acc:0.698]
Epoch [22/120    avg_loss:0.613, val_acc:0.841]
Epoch [23/120    avg_loss:0.432, val_acc:0.831]
Epoch [24/120    avg_loss:0.362, val_acc:0.786]
Epoch [25/120    avg_loss:0.330, val_acc:0.862]
Epoch [26/120    avg_loss:0.345, val_acc:0.864]
Epoch [27/120    avg_loss:0.320, val_acc:0.866]
Epoch [28/120    avg_loss:0.297, val_acc:0.871]
Epoch [29/120    avg_loss:0.251, val_acc:0.872]
Epoch [30/120    avg_loss:0.269, val_acc:0.895]
Epoch [31/120    avg_loss:0.448, val_acc:0.865]
Epoch [32/120    avg_loss:0.298, val_acc:0.880]
Epoch [33/120    avg_loss:0.278, val_acc:0.891]
Epoch [34/120    avg_loss:0.203, val_acc:0.880]
Epoch [35/120    avg_loss:0.171, val_acc:0.903]
Epoch [36/120    avg_loss:0.288, val_acc:0.878]
Epoch [37/120    avg_loss:0.213, val_acc:0.880]
Epoch [38/120    avg_loss:0.224, val_acc:0.892]
Epoch [39/120    avg_loss:0.200, val_acc:0.897]
Epoch [40/120    avg_loss:0.164, val_acc:0.918]
Epoch [41/120    avg_loss:0.197, val_acc:0.917]
Epoch [42/120    avg_loss:0.194, val_acc:0.908]
Epoch [43/120    avg_loss:0.213, val_acc:0.909]
Epoch [44/120    avg_loss:0.135, val_acc:0.916]
Epoch [45/120    avg_loss:0.125, val_acc:0.926]
Epoch [46/120    avg_loss:0.101, val_acc:0.905]
Epoch [47/120    avg_loss:0.108, val_acc:0.934]
Epoch [48/120    avg_loss:0.115, val_acc:0.925]
Epoch [49/120    avg_loss:0.091, val_acc:0.922]
Epoch [50/120    avg_loss:0.107, val_acc:0.941]
Epoch [51/120    avg_loss:0.093, val_acc:0.936]
Epoch [52/120    avg_loss:0.105, val_acc:0.934]
Epoch [53/120    avg_loss:0.105, val_acc:0.925]
Epoch [54/120    avg_loss:0.092, val_acc:0.919]
Epoch [55/120    avg_loss:0.096, val_acc:0.925]
Epoch [56/120    avg_loss:0.132, val_acc:0.922]
Epoch [57/120    avg_loss:0.080, val_acc:0.931]
Epoch [58/120    avg_loss:0.101, val_acc:0.933]
Epoch [59/120    avg_loss:0.076, val_acc:0.957]
Epoch [60/120    avg_loss:0.066, val_acc:0.942]
Epoch [61/120    avg_loss:0.082, val_acc:0.930]
Epoch [62/120    avg_loss:0.084, val_acc:0.942]
Epoch [63/120    avg_loss:0.163, val_acc:0.939]
Epoch [64/120    avg_loss:0.127, val_acc:0.887]
Epoch [65/120    avg_loss:0.095, val_acc:0.922]
Epoch [66/120    avg_loss:0.111, val_acc:0.909]
Epoch [67/120    avg_loss:0.070, val_acc:0.938]
Epoch [68/120    avg_loss:0.063, val_acc:0.940]
Epoch [69/120    avg_loss:0.072, val_acc:0.940]
Epoch [70/120    avg_loss:0.059, val_acc:0.945]
Epoch [71/120    avg_loss:0.054, val_acc:0.931]
Epoch [72/120    avg_loss:0.059, val_acc:0.954]
Epoch [73/120    avg_loss:0.048, val_acc:0.954]
Epoch [74/120    avg_loss:0.037, val_acc:0.959]
Epoch [75/120    avg_loss:0.042, val_acc:0.957]
Epoch [76/120    avg_loss:0.038, val_acc:0.957]
Epoch [77/120    avg_loss:0.025, val_acc:0.958]
Epoch [78/120    avg_loss:0.032, val_acc:0.958]
Epoch [79/120    avg_loss:0.036, val_acc:0.961]
Epoch [80/120    avg_loss:0.029, val_acc:0.964]
Epoch [81/120    avg_loss:0.040, val_acc:0.960]
Epoch [82/120    avg_loss:0.031, val_acc:0.959]
Epoch [83/120    avg_loss:0.031, val_acc:0.959]
Epoch [84/120    avg_loss:0.029, val_acc:0.961]
Epoch [85/120    avg_loss:0.030, val_acc:0.960]
Epoch [86/120    avg_loss:0.028, val_acc:0.962]
Epoch [87/120    avg_loss:0.029, val_acc:0.960]
Epoch [88/120    avg_loss:0.037, val_acc:0.961]
Epoch [89/120    avg_loss:0.034, val_acc:0.960]
Epoch [90/120    avg_loss:0.030, val_acc:0.963]
Epoch [91/120    avg_loss:0.035, val_acc:0.964]
Epoch [92/120    avg_loss:0.024, val_acc:0.962]
Epoch [93/120    avg_loss:0.026, val_acc:0.959]
Epoch [94/120    avg_loss:0.027, val_acc:0.962]
Epoch [95/120    avg_loss:0.030, val_acc:0.961]
Epoch [96/120    avg_loss:0.029, val_acc:0.962]
Epoch [97/120    avg_loss:0.028, val_acc:0.963]
Epoch [98/120    avg_loss:0.026, val_acc:0.964]
Epoch [99/120    avg_loss:0.030, val_acc:0.962]
Epoch [100/120    avg_loss:0.029, val_acc:0.963]
Epoch [101/120    avg_loss:0.025, val_acc:0.962]
Epoch [102/120    avg_loss:0.033, val_acc:0.966]
Epoch [103/120    avg_loss:0.022, val_acc:0.963]
Epoch [104/120    avg_loss:0.030, val_acc:0.967]
Epoch [105/120    avg_loss:0.028, val_acc:0.964]
Epoch [106/120    avg_loss:0.025, val_acc:0.966]
Epoch [107/120    avg_loss:0.025, val_acc:0.968]
Epoch [108/120    avg_loss:0.027, val_acc:0.967]
Epoch [109/120    avg_loss:0.027, val_acc:0.962]
Epoch [110/120    avg_loss:0.027, val_acc:0.964]
Epoch [111/120    avg_loss:0.022, val_acc:0.963]
Epoch [112/120    avg_loss:0.022, val_acc:0.960]
Epoch [113/120    avg_loss:0.024, val_acc:0.967]
Epoch [114/120    avg_loss:0.024, val_acc:0.967]
Epoch [115/120    avg_loss:0.021, val_acc:0.969]
Epoch [116/120    avg_loss:0.025, val_acc:0.967]
Epoch [117/120    avg_loss:0.028, val_acc:0.966]
Epoch [118/120    avg_loss:0.022, val_acc:0.967]
Epoch [119/120    avg_loss:0.025, val_acc:0.968]
Epoch [120/120    avg_loss:0.022, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1212    3    0    0    3    0    0    0   17   46    4    0
     0    0    0]
 [   0    0    0  724    0   10    0    0    0    3    1    0    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    3    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    4   44    0    7    1    0    0    0  801   15    0    0
     0    3    0]
 [   0    0   10    0    0    0   13    0    0    0   19 2161    5    2
     0    0    0]
 [   0    0    0   29    1   15    0    0    0    0    2    1  483    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    3    0    1    1    1    0
  1126    0    0]
 [   0    0    0    0    0    0    0    0    0   13    0    0    0    0
    96  238    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.63143631436314

F1 scores:
[       nan 0.975      0.96535245 0.93600517 0.99765808 0.95016611
 0.98722765 0.98039216 0.99652375 0.62962963 0.93247963 0.97474064
 0.92974013 0.99462366 0.95302581 0.80952381 0.9704142 ]

Kappa:
0.950168364023641
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe7cf38c860>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.620, val_acc:0.455]
Epoch [2/120    avg_loss:2.228, val_acc:0.521]
Epoch [3/120    avg_loss:2.033, val_acc:0.587]
Epoch [4/120    avg_loss:1.827, val_acc:0.616]
Epoch [5/120    avg_loss:1.611, val_acc:0.646]
Epoch [6/120    avg_loss:1.391, val_acc:0.677]
Epoch [7/120    avg_loss:1.229, val_acc:0.690]
Epoch [8/120    avg_loss:1.132, val_acc:0.747]
Epoch [9/120    avg_loss:1.006, val_acc:0.755]
Epoch [10/120    avg_loss:0.889, val_acc:0.797]
Epoch [11/120    avg_loss:0.797, val_acc:0.745]
Epoch [12/120    avg_loss:0.773, val_acc:0.752]
Epoch [13/120    avg_loss:0.700, val_acc:0.800]
Epoch [14/120    avg_loss:0.689, val_acc:0.791]
Epoch [15/120    avg_loss:0.603, val_acc:0.815]
Epoch [16/120    avg_loss:0.526, val_acc:0.848]
Epoch [17/120    avg_loss:0.613, val_acc:0.807]
Epoch [18/120    avg_loss:0.535, val_acc:0.819]
Epoch [19/120    avg_loss:0.504, val_acc:0.828]
Epoch [20/120    avg_loss:0.445, val_acc:0.841]
Epoch [21/120    avg_loss:0.382, val_acc:0.880]
Epoch [22/120    avg_loss:0.396, val_acc:0.882]
Epoch [23/120    avg_loss:0.340, val_acc:0.884]
Epoch [24/120    avg_loss:0.442, val_acc:0.852]
Epoch [25/120    avg_loss:0.331, val_acc:0.894]
Epoch [26/120    avg_loss:0.330, val_acc:0.842]
Epoch [27/120    avg_loss:0.321, val_acc:0.900]
Epoch [28/120    avg_loss:0.365, val_acc:0.834]
Epoch [29/120    avg_loss:0.322, val_acc:0.906]
Epoch [30/120    avg_loss:0.228, val_acc:0.907]
Epoch [31/120    avg_loss:0.228, val_acc:0.909]
Epoch [32/120    avg_loss:0.266, val_acc:0.920]
Epoch [33/120    avg_loss:0.205, val_acc:0.917]
Epoch [34/120    avg_loss:0.162, val_acc:0.923]
Epoch [35/120    avg_loss:0.160, val_acc:0.905]
Epoch [36/120    avg_loss:0.160, val_acc:0.939]
Epoch [37/120    avg_loss:0.163, val_acc:0.919]
Epoch [38/120    avg_loss:0.153, val_acc:0.936]
Epoch [39/120    avg_loss:0.138, val_acc:0.935]
Epoch [40/120    avg_loss:0.132, val_acc:0.934]
Epoch [41/120    avg_loss:0.100, val_acc:0.953]
Epoch [42/120    avg_loss:0.084, val_acc:0.957]
Epoch [43/120    avg_loss:0.084, val_acc:0.950]
Epoch [44/120    avg_loss:0.099, val_acc:0.945]
Epoch [45/120    avg_loss:0.114, val_acc:0.939]
Epoch [46/120    avg_loss:0.129, val_acc:0.955]
Epoch [47/120    avg_loss:0.108, val_acc:0.952]
Epoch [48/120    avg_loss:0.087, val_acc:0.948]
Epoch [49/120    avg_loss:0.087, val_acc:0.955]
Epoch [50/120    avg_loss:0.075, val_acc:0.957]
Epoch [51/120    avg_loss:0.077, val_acc:0.957]
Epoch [52/120    avg_loss:0.078, val_acc:0.945]
Epoch [53/120    avg_loss:0.152, val_acc:0.913]
Epoch [54/120    avg_loss:0.160, val_acc:0.927]
Epoch [55/120    avg_loss:0.108, val_acc:0.952]
Epoch [56/120    avg_loss:0.076, val_acc:0.956]
Epoch [57/120    avg_loss:0.065, val_acc:0.957]
Epoch [58/120    avg_loss:0.114, val_acc:0.939]
Epoch [59/120    avg_loss:0.081, val_acc:0.958]
Epoch [60/120    avg_loss:0.054, val_acc:0.961]
Epoch [61/120    avg_loss:0.047, val_acc:0.968]
Epoch [62/120    avg_loss:0.051, val_acc:0.956]
Epoch [63/120    avg_loss:0.059, val_acc:0.971]
Epoch [64/120    avg_loss:0.066, val_acc:0.963]
Epoch [65/120    avg_loss:0.059, val_acc:0.965]
Epoch [66/120    avg_loss:0.045, val_acc:0.974]
Epoch [67/120    avg_loss:0.046, val_acc:0.966]
Epoch [68/120    avg_loss:0.060, val_acc:0.958]
Epoch [69/120    avg_loss:0.044, val_acc:0.967]
Epoch [70/120    avg_loss:0.045, val_acc:0.952]
Epoch [71/120    avg_loss:0.054, val_acc:0.963]
Epoch [72/120    avg_loss:0.035, val_acc:0.958]
Epoch [73/120    avg_loss:0.043, val_acc:0.965]
Epoch [74/120    avg_loss:0.054, val_acc:0.939]
Epoch [75/120    avg_loss:0.056, val_acc:0.967]
Epoch [76/120    avg_loss:0.035, val_acc:0.973]
Epoch [77/120    avg_loss:0.034, val_acc:0.973]
Epoch [78/120    avg_loss:0.037, val_acc:0.965]
Epoch [79/120    avg_loss:0.037, val_acc:0.967]
Epoch [80/120    avg_loss:0.023, val_acc:0.973]
Epoch [81/120    avg_loss:0.024, val_acc:0.973]
Epoch [82/120    avg_loss:0.027, val_acc:0.975]
Epoch [83/120    avg_loss:0.025, val_acc:0.978]
Epoch [84/120    avg_loss:0.021, val_acc:0.979]
Epoch [85/120    avg_loss:0.019, val_acc:0.976]
Epoch [86/120    avg_loss:0.021, val_acc:0.979]
Epoch [87/120    avg_loss:0.024, val_acc:0.979]
Epoch [88/120    avg_loss:0.025, val_acc:0.975]
Epoch [89/120    avg_loss:0.032, val_acc:0.978]
Epoch [90/120    avg_loss:0.021, val_acc:0.979]
Epoch [91/120    avg_loss:0.021, val_acc:0.980]
Epoch [92/120    avg_loss:0.021, val_acc:0.979]
Epoch [93/120    avg_loss:0.020, val_acc:0.980]
Epoch [94/120    avg_loss:0.024, val_acc:0.981]
Epoch [95/120    avg_loss:0.022, val_acc:0.981]
Epoch [96/120    avg_loss:0.018, val_acc:0.980]
Epoch [97/120    avg_loss:0.022, val_acc:0.980]
Epoch [98/120    avg_loss:0.018, val_acc:0.979]
Epoch [99/120    avg_loss:0.030, val_acc:0.981]
Epoch [100/120    avg_loss:0.019, val_acc:0.981]
Epoch [101/120    avg_loss:0.022, val_acc:0.979]
Epoch [102/120    avg_loss:0.017, val_acc:0.979]
Epoch [103/120    avg_loss:0.020, val_acc:0.978]
Epoch [104/120    avg_loss:0.017, val_acc:0.981]
Epoch [105/120    avg_loss:0.016, val_acc:0.980]
Epoch [106/120    avg_loss:0.027, val_acc:0.980]
Epoch [107/120    avg_loss:0.019, val_acc:0.982]
Epoch [108/120    avg_loss:0.023, val_acc:0.982]
Epoch [109/120    avg_loss:0.023, val_acc:0.982]
Epoch [110/120    avg_loss:0.020, val_acc:0.981]
Epoch [111/120    avg_loss:0.023, val_acc:0.980]
Epoch [112/120    avg_loss:0.020, val_acc:0.979]
Epoch [113/120    avg_loss:0.017, val_acc:0.976]
Epoch [114/120    avg_loss:0.022, val_acc:0.978]
Epoch [115/120    avg_loss:0.018, val_acc:0.979]
Epoch [116/120    avg_loss:0.019, val_acc:0.976]
Epoch [117/120    avg_loss:0.019, val_acc:0.978]
Epoch [118/120    avg_loss:0.016, val_acc:0.981]
Epoch [119/120    avg_loss:0.019, val_acc:0.984]
Epoch [120/120    avg_loss:0.019, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1248    6    2    0    0    0    0    1    3   21    4    0
     0    0    0]
 [   0    0    0  722    4    6    0    0    0    3    0    1   11    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   13   16    0    2    0    0    0    0  826   16    1    0
     0    1    0]
 [   0    0   14    0    0    1    9    0    2    0   22 2156    5    1
     0    0    0]
 [   0    0    0    6    7    6    0    0    0    0    8    5  498    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    2    1    0    0    4    0    0    1    0    1    0    0    0
  1129    1    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    34  297    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.00813008130082

F1 scores:
[       nan 0.925      0.97461929 0.96459586 0.97038724 0.9740113
 0.98056801 0.98039216 0.99185099 0.85714286 0.94997125 0.97777778
 0.94050992 0.99730458 0.98046027 0.91950464 0.96470588]

Kappa:
0.9658940786703489
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe468a9b860>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.630, val_acc:0.510]
Epoch [2/120    avg_loss:2.207, val_acc:0.548]
Epoch [3/120    avg_loss:1.971, val_acc:0.565]
Epoch [4/120    avg_loss:1.805, val_acc:0.599]
Epoch [5/120    avg_loss:1.685, val_acc:0.657]
Epoch [6/120    avg_loss:1.525, val_acc:0.671]
Epoch [7/120    avg_loss:1.386, val_acc:0.705]
Epoch [8/120    avg_loss:1.217, val_acc:0.713]
Epoch [9/120    avg_loss:1.067, val_acc:0.720]
Epoch [10/120    avg_loss:0.985, val_acc:0.756]
Epoch [11/120    avg_loss:0.857, val_acc:0.768]
Epoch [12/120    avg_loss:0.709, val_acc:0.800]
Epoch [13/120    avg_loss:0.739, val_acc:0.766]
Epoch [14/120    avg_loss:0.676, val_acc:0.792]
Epoch [15/120    avg_loss:0.599, val_acc:0.831]
Epoch [16/120    avg_loss:0.482, val_acc:0.799]
Epoch [17/120    avg_loss:0.495, val_acc:0.815]
Epoch [18/120    avg_loss:0.478, val_acc:0.844]
Epoch [19/120    avg_loss:0.431, val_acc:0.833]
Epoch [20/120    avg_loss:0.454, val_acc:0.836]
Epoch [21/120    avg_loss:0.377, val_acc:0.840]
Epoch [22/120    avg_loss:0.378, val_acc:0.855]
Epoch [23/120    avg_loss:0.374, val_acc:0.878]
Epoch [24/120    avg_loss:0.276, val_acc:0.872]
Epoch [25/120    avg_loss:0.286, val_acc:0.888]
Epoch [26/120    avg_loss:0.258, val_acc:0.866]
Epoch [27/120    avg_loss:0.220, val_acc:0.885]
Epoch [28/120    avg_loss:0.243, val_acc:0.881]
Epoch [29/120    avg_loss:0.236, val_acc:0.867]
Epoch [30/120    avg_loss:0.241, val_acc:0.913]
Epoch [31/120    avg_loss:0.217, val_acc:0.891]
Epoch [32/120    avg_loss:0.158, val_acc:0.915]
Epoch [33/120    avg_loss:0.224, val_acc:0.893]
Epoch [34/120    avg_loss:0.225, val_acc:0.902]
Epoch [35/120    avg_loss:0.177, val_acc:0.916]
Epoch [36/120    avg_loss:0.147, val_acc:0.920]
Epoch [37/120    avg_loss:0.122, val_acc:0.922]
Epoch [38/120    avg_loss:0.123, val_acc:0.931]
Epoch [39/120    avg_loss:0.128, val_acc:0.935]
Epoch [40/120    avg_loss:0.136, val_acc:0.895]
Epoch [41/120    avg_loss:0.115, val_acc:0.929]
Epoch [42/120    avg_loss:0.138, val_acc:0.925]
Epoch [43/120    avg_loss:0.148, val_acc:0.915]
Epoch [44/120    avg_loss:0.143, val_acc:0.931]
Epoch [45/120    avg_loss:0.105, val_acc:0.925]
Epoch [46/120    avg_loss:0.183, val_acc:0.908]
Epoch [47/120    avg_loss:0.119, val_acc:0.930]
Epoch [48/120    avg_loss:0.106, val_acc:0.934]
Epoch [49/120    avg_loss:0.110, val_acc:0.941]
Epoch [50/120    avg_loss:0.096, val_acc:0.933]
Epoch [51/120    avg_loss:0.104, val_acc:0.939]
Epoch [52/120    avg_loss:0.082, val_acc:0.941]
Epoch [53/120    avg_loss:0.089, val_acc:0.944]
Epoch [54/120    avg_loss:0.076, val_acc:0.949]
Epoch [55/120    avg_loss:0.072, val_acc:0.936]
Epoch [56/120    avg_loss:0.075, val_acc:0.945]
Epoch [57/120    avg_loss:0.101, val_acc:0.916]
Epoch [58/120    avg_loss:0.078, val_acc:0.950]
Epoch [59/120    avg_loss:0.102, val_acc:0.942]
Epoch [60/120    avg_loss:0.079, val_acc:0.949]
Epoch [61/120    avg_loss:0.064, val_acc:0.950]
Epoch [62/120    avg_loss:0.066, val_acc:0.949]
Epoch [63/120    avg_loss:0.064, val_acc:0.927]
Epoch [64/120    avg_loss:0.068, val_acc:0.940]
Epoch [65/120    avg_loss:0.044, val_acc:0.952]
Epoch [66/120    avg_loss:0.061, val_acc:0.945]
Epoch [67/120    avg_loss:0.054, val_acc:0.945]
Epoch [68/120    avg_loss:0.093, val_acc:0.908]
Epoch [69/120    avg_loss:0.079, val_acc:0.933]
Epoch [70/120    avg_loss:0.090, val_acc:0.956]
Epoch [71/120    avg_loss:0.060, val_acc:0.946]
Epoch [72/120    avg_loss:0.060, val_acc:0.952]
Epoch [73/120    avg_loss:0.039, val_acc:0.954]
Epoch [74/120    avg_loss:0.052, val_acc:0.950]
Epoch [75/120    avg_loss:0.038, val_acc:0.955]
Epoch [76/120    avg_loss:0.036, val_acc:0.956]
Epoch [77/120    avg_loss:0.061, val_acc:0.952]
Epoch [78/120    avg_loss:0.045, val_acc:0.961]
Epoch [79/120    avg_loss:0.053, val_acc:0.954]
Epoch [80/120    avg_loss:0.046, val_acc:0.949]
Epoch [81/120    avg_loss:0.049, val_acc:0.960]
Epoch [82/120    avg_loss:0.045, val_acc:0.955]
Epoch [83/120    avg_loss:0.042, val_acc:0.965]
Epoch [84/120    avg_loss:0.031, val_acc:0.959]
Epoch [85/120    avg_loss:0.030, val_acc:0.963]
Epoch [86/120    avg_loss:0.033, val_acc:0.960]
Epoch [87/120    avg_loss:0.037, val_acc:0.965]
Epoch [88/120    avg_loss:0.035, val_acc:0.954]
Epoch [89/120    avg_loss:0.030, val_acc:0.967]
Epoch [90/120    avg_loss:0.026, val_acc:0.964]
Epoch [91/120    avg_loss:0.028, val_acc:0.948]
Epoch [92/120    avg_loss:0.031, val_acc:0.964]
Epoch [93/120    avg_loss:0.028, val_acc:0.969]
Epoch [94/120    avg_loss:0.025, val_acc:0.972]
Epoch [95/120    avg_loss:0.031, val_acc:0.965]
Epoch [96/120    avg_loss:0.024, val_acc:0.961]
Epoch [97/120    avg_loss:0.033, val_acc:0.960]
Epoch [98/120    avg_loss:0.065, val_acc:0.965]
Epoch [99/120    avg_loss:0.048, val_acc:0.967]
Epoch [100/120    avg_loss:0.040, val_acc:0.962]
Epoch [101/120    avg_loss:0.022, val_acc:0.968]
Epoch [102/120    avg_loss:0.026, val_acc:0.967]
Epoch [103/120    avg_loss:0.035, val_acc:0.972]
Epoch [104/120    avg_loss:0.036, val_acc:0.948]
Epoch [105/120    avg_loss:0.038, val_acc:0.971]
Epoch [106/120    avg_loss:0.039, val_acc:0.958]
Epoch [107/120    avg_loss:0.035, val_acc:0.941]
Epoch [108/120    avg_loss:0.036, val_acc:0.964]
Epoch [109/120    avg_loss:0.017, val_acc:0.973]
Epoch [110/120    avg_loss:0.018, val_acc:0.965]
Epoch [111/120    avg_loss:0.024, val_acc:0.969]
Epoch [112/120    avg_loss:0.017, val_acc:0.970]
Epoch [113/120    avg_loss:0.016, val_acc:0.973]
Epoch [114/120    avg_loss:0.017, val_acc:0.969]
Epoch [115/120    avg_loss:0.016, val_acc:0.978]
Epoch [116/120    avg_loss:0.012, val_acc:0.972]
Epoch [117/120    avg_loss:0.014, val_acc:0.972]
Epoch [118/120    avg_loss:0.012, val_acc:0.971]
Epoch [119/120    avg_loss:0.016, val_acc:0.972]
Epoch [120/120    avg_loss:0.013, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1232    1    0    0    3    0    0    0   13   31    5    0
     0    0    0]
 [   0    0    1  706    0   13    0    0    0   16    9    0    1    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4   23    0    9    3    0    0    0  831    2    0    0
     2    1    0]
 [   0    0    8    0    0    0    2    0    7    0   30 2161    0    1
     1    0    0]
 [   0    0    0    4    4   13    0    0    0    1    4   20  482    0
     0    1    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    3    0    2    0    0    0
  1129    4    0]
 [   0    0    0    0    0    0   16    0    0    2    0    0    0    0
    75  254    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.21680216802169

F1 scores:
[       nan 0.975      0.97391304 0.95340986 0.99069767 0.9556541
 0.98130142 0.98039216 0.98850575 0.62068966 0.94110985 0.97672316
 0.94324853 0.99462366 0.96248934 0.8369028  0.97109827]

Kappa:
0.9568554224707811
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f02d13208d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.631, val_acc:0.500]
Epoch [2/120    avg_loss:2.255, val_acc:0.585]
Epoch [3/120    avg_loss:2.011, val_acc:0.610]
Epoch [4/120    avg_loss:1.781, val_acc:0.626]
Epoch [5/120    avg_loss:1.569, val_acc:0.665]
Epoch [6/120    avg_loss:1.477, val_acc:0.680]
Epoch [7/120    avg_loss:1.242, val_acc:0.739]
Epoch [8/120    avg_loss:1.195, val_acc:0.690]
Epoch [9/120    avg_loss:0.999, val_acc:0.758]
Epoch [10/120    avg_loss:0.852, val_acc:0.771]
Epoch [11/120    avg_loss:0.766, val_acc:0.818]
Epoch [12/120    avg_loss:0.747, val_acc:0.794]
Epoch [13/120    avg_loss:0.647, val_acc:0.829]
Epoch [14/120    avg_loss:0.559, val_acc:0.814]
Epoch [15/120    avg_loss:0.537, val_acc:0.862]
Epoch [16/120    avg_loss:0.478, val_acc:0.890]
Epoch [17/120    avg_loss:0.452, val_acc:0.853]
Epoch [18/120    avg_loss:0.411, val_acc:0.872]
Epoch [19/120    avg_loss:0.370, val_acc:0.848]
Epoch [20/120    avg_loss:0.341, val_acc:0.888]
Epoch [21/120    avg_loss:0.314, val_acc:0.904]
Epoch [22/120    avg_loss:0.246, val_acc:0.894]
Epoch [23/120    avg_loss:0.263, val_acc:0.902]
Epoch [24/120    avg_loss:0.263, val_acc:0.911]
Epoch [25/120    avg_loss:0.275, val_acc:0.907]
Epoch [26/120    avg_loss:0.307, val_acc:0.912]
Epoch [27/120    avg_loss:0.271, val_acc:0.875]
Epoch [28/120    avg_loss:0.300, val_acc:0.892]
Epoch [29/120    avg_loss:0.235, val_acc:0.913]
Epoch [30/120    avg_loss:0.190, val_acc:0.924]
Epoch [31/120    avg_loss:0.184, val_acc:0.933]
Epoch [32/120    avg_loss:0.198, val_acc:0.925]
Epoch [33/120    avg_loss:0.173, val_acc:0.923]
Epoch [34/120    avg_loss:0.218, val_acc:0.916]
Epoch [35/120    avg_loss:0.188, val_acc:0.938]
Epoch [36/120    avg_loss:0.131, val_acc:0.933]
Epoch [37/120    avg_loss:0.144, val_acc:0.938]
Epoch [38/120    avg_loss:0.142, val_acc:0.908]
Epoch [39/120    avg_loss:0.208, val_acc:0.890]
Epoch [40/120    avg_loss:0.164, val_acc:0.940]
Epoch [41/120    avg_loss:0.118, val_acc:0.948]
Epoch [42/120    avg_loss:0.102, val_acc:0.951]
Epoch [43/120    avg_loss:0.099, val_acc:0.951]
Epoch [44/120    avg_loss:0.211, val_acc:0.916]
Epoch [45/120    avg_loss:0.145, val_acc:0.948]
Epoch [46/120    avg_loss:0.105, val_acc:0.953]
Epoch [47/120    avg_loss:0.101, val_acc:0.950]
Epoch [48/120    avg_loss:0.108, val_acc:0.942]
Epoch [49/120    avg_loss:0.110, val_acc:0.951]
Epoch [50/120    avg_loss:0.081, val_acc:0.960]
Epoch [51/120    avg_loss:0.087, val_acc:0.948]
Epoch [52/120    avg_loss:0.080, val_acc:0.958]
Epoch [53/120    avg_loss:0.066, val_acc:0.956]
Epoch [54/120    avg_loss:0.052, val_acc:0.963]
Epoch [55/120    avg_loss:0.070, val_acc:0.954]
Epoch [56/120    avg_loss:0.084, val_acc:0.935]
Epoch [57/120    avg_loss:0.076, val_acc:0.960]
Epoch [58/120    avg_loss:0.050, val_acc:0.967]
Epoch [59/120    avg_loss:0.057, val_acc:0.964]
Epoch [60/120    avg_loss:0.068, val_acc:0.960]
Epoch [61/120    avg_loss:0.341, val_acc:0.640]
Epoch [62/120    avg_loss:1.953, val_acc:0.492]
Epoch [63/120    avg_loss:1.771, val_acc:0.545]
Epoch [64/120    avg_loss:1.629, val_acc:0.588]
Epoch [65/120    avg_loss:1.472, val_acc:0.612]
Epoch [66/120    avg_loss:1.381, val_acc:0.638]
Epoch [67/120    avg_loss:1.242, val_acc:0.642]
Epoch [68/120    avg_loss:1.149, val_acc:0.672]
Epoch [69/120    avg_loss:1.096, val_acc:0.662]
Epoch [70/120    avg_loss:1.040, val_acc:0.694]
Epoch [71/120    avg_loss:1.026, val_acc:0.715]
Epoch [72/120    avg_loss:0.899, val_acc:0.720]
Epoch [73/120    avg_loss:0.893, val_acc:0.728]
Epoch [74/120    avg_loss:0.839, val_acc:0.732]
Epoch [75/120    avg_loss:0.831, val_acc:0.735]
Epoch [76/120    avg_loss:0.864, val_acc:0.741]
Epoch [77/120    avg_loss:0.829, val_acc:0.735]
Epoch [78/120    avg_loss:0.843, val_acc:0.735]
Epoch [79/120    avg_loss:0.845, val_acc:0.739]
Epoch [80/120    avg_loss:0.819, val_acc:0.744]
Epoch [81/120    avg_loss:0.758, val_acc:0.749]
Epoch [82/120    avg_loss:0.801, val_acc:0.749]
Epoch [83/120    avg_loss:0.776, val_acc:0.751]
Epoch [84/120    avg_loss:0.802, val_acc:0.742]
Epoch [85/120    avg_loss:0.764, val_acc:0.746]
Epoch [86/120    avg_loss:0.794, val_acc:0.744]
Epoch [87/120    avg_loss:0.754, val_acc:0.748]
Epoch [88/120    avg_loss:0.747, val_acc:0.747]
Epoch [89/120    avg_loss:0.797, val_acc:0.750]
Epoch [90/120    avg_loss:0.783, val_acc:0.749]
Epoch [91/120    avg_loss:0.770, val_acc:0.749]
Epoch [92/120    avg_loss:0.787, val_acc:0.749]
Epoch [93/120    avg_loss:0.767, val_acc:0.751]
Epoch [94/120    avg_loss:0.747, val_acc:0.751]
Epoch [95/120    avg_loss:0.780, val_acc:0.751]
Epoch [96/120    avg_loss:0.782, val_acc:0.751]
Epoch [97/120    avg_loss:0.744, val_acc:0.752]
Epoch [98/120    avg_loss:0.733, val_acc:0.752]
Epoch [99/120    avg_loss:0.787, val_acc:0.752]
Epoch [100/120    avg_loss:0.732, val_acc:0.752]
Epoch [101/120    avg_loss:0.755, val_acc:0.752]
Epoch [102/120    avg_loss:0.755, val_acc:0.752]
Epoch [103/120    avg_loss:0.760, val_acc:0.752]
Epoch [104/120    avg_loss:0.756, val_acc:0.752]
Epoch [105/120    avg_loss:0.754, val_acc:0.753]
Epoch [106/120    avg_loss:0.763, val_acc:0.752]
Epoch [107/120    avg_loss:0.748, val_acc:0.753]
Epoch [108/120    avg_loss:0.810, val_acc:0.753]
Epoch [109/120    avg_loss:0.770, val_acc:0.753]
Epoch [110/120    avg_loss:0.745, val_acc:0.753]
Epoch [111/120    avg_loss:0.769, val_acc:0.753]
Epoch [112/120    avg_loss:0.722, val_acc:0.753]
Epoch [113/120    avg_loss:0.746, val_acc:0.753]
Epoch [114/120    avg_loss:0.782, val_acc:0.753]
Epoch [115/120    avg_loss:0.729, val_acc:0.753]
Epoch [116/120    avg_loss:0.808, val_acc:0.753]
Epoch [117/120    avg_loss:0.741, val_acc:0.753]
Epoch [118/120    avg_loss:0.744, val_acc:0.753]
Epoch [119/120    avg_loss:0.747, val_acc:0.753]
Epoch [120/120    avg_loss:0.783, val_acc:0.753]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   32    0    0    0    0    0    5    0    0    4    0    0    0
     0    0    0]
 [   0    4  978   88   17    0   14    0    0    0   51  128    0    0
     3    2    0]
 [   0    0   25  298   53   82   32    0    0   16    1   70  109   51
     5    5    0]
 [   0    0    0   12  197    0    1    0    0    0    0    0    0    0
     0    3    0]
 [   0    0    0    2    0  342   18   52    0    0    0    0    0    0
    21    0    0]
 [   0    0    0   27    7   11  536    0    0    0    0   13    0   55
     8    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    5    0    0    0    0  409    7    0    3    0    0
     0    3    0]
 [   0    0    0    0    6    0   10    0    0    2    0    0    0    0
     0    0    0]
 [   0    3   75   78    9   62    2    0    0    0  578   31    7    0
     3   27    0]
 [   0    7  153  137    3   57   44    0    0    2   25 1577  144   16
     6   39    0]
 [   0    0    9   15   73   17    6    0    0    0    1    8  349    0
     0   23   33]
 [   0    0    0    0    0    0   18    0    0    0    0    0    0  167
     0    0    0]
 [   0    6    1    0    4   24    1    0    0    0    0    5    0    0
  1057   41    0]
 [   0   21    0   22    0    3   46    4    0    0    0   18    8    0
   100  125    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
73.19241192411924

F1 scores:
[       nan 0.54700855 0.77434679 0.41649196 0.67697595 0.6615087
 0.77400722 0.43636364 0.9749702  0.08888889 0.75309446 0.77627369
 0.60485269 0.70464135 0.90264731 0.40650407 0.81818182]

Kappa:
0.6976991868904944
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc61cf86898>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.642, val_acc:0.391]
Epoch [2/120    avg_loss:2.224, val_acc:0.536]
Epoch [3/120    avg_loss:1.994, val_acc:0.565]
Epoch [4/120    avg_loss:1.792, val_acc:0.577]
Epoch [5/120    avg_loss:1.633, val_acc:0.635]
Epoch [6/120    avg_loss:1.460, val_acc:0.630]
Epoch [7/120    avg_loss:1.235, val_acc:0.668]
Epoch [8/120    avg_loss:1.084, val_acc:0.678]
Epoch [9/120    avg_loss:0.962, val_acc:0.678]
Epoch [10/120    avg_loss:0.874, val_acc:0.694]
Epoch [11/120    avg_loss:0.833, val_acc:0.721]
Epoch [12/120    avg_loss:0.812, val_acc:0.775]
Epoch [13/120    avg_loss:0.659, val_acc:0.743]
Epoch [14/120    avg_loss:0.707, val_acc:0.771]
Epoch [15/120    avg_loss:0.624, val_acc:0.799]
Epoch [16/120    avg_loss:0.523, val_acc:0.802]
Epoch [17/120    avg_loss:0.499, val_acc:0.812]
Epoch [18/120    avg_loss:0.463, val_acc:0.812]
Epoch [19/120    avg_loss:0.437, val_acc:0.808]
Epoch [20/120    avg_loss:0.483, val_acc:0.860]
Epoch [21/120    avg_loss:0.369, val_acc:0.854]
Epoch [22/120    avg_loss:0.336, val_acc:0.848]
Epoch [23/120    avg_loss:0.280, val_acc:0.836]
Epoch [24/120    avg_loss:0.314, val_acc:0.877]
Epoch [25/120    avg_loss:0.321, val_acc:0.879]
Epoch [26/120    avg_loss:0.252, val_acc:0.882]
Epoch [27/120    avg_loss:0.240, val_acc:0.827]
Epoch [28/120    avg_loss:0.347, val_acc:0.863]
Epoch [29/120    avg_loss:0.252, val_acc:0.887]
Epoch [30/120    avg_loss:0.206, val_acc:0.883]
Epoch [31/120    avg_loss:0.218, val_acc:0.887]
Epoch [32/120    avg_loss:0.172, val_acc:0.905]
Epoch [33/120    avg_loss:0.177, val_acc:0.902]
Epoch [34/120    avg_loss:0.175, val_acc:0.905]
Epoch [35/120    avg_loss:0.171, val_acc:0.912]
Epoch [36/120    avg_loss:0.209, val_acc:0.904]
Epoch [37/120    avg_loss:0.194, val_acc:0.912]
Epoch [38/120    avg_loss:0.183, val_acc:0.904]
Epoch [39/120    avg_loss:0.203, val_acc:0.907]
Epoch [40/120    avg_loss:0.131, val_acc:0.936]
Epoch [41/120    avg_loss:0.126, val_acc:0.934]
Epoch [42/120    avg_loss:0.111, val_acc:0.932]
Epoch [43/120    avg_loss:0.101, val_acc:0.941]
Epoch [44/120    avg_loss:0.121, val_acc:0.930]
Epoch [45/120    avg_loss:0.096, val_acc:0.922]
Epoch [46/120    avg_loss:0.098, val_acc:0.923]
Epoch [47/120    avg_loss:0.123, val_acc:0.930]
Epoch [48/120    avg_loss:0.147, val_acc:0.919]
Epoch [49/120    avg_loss:0.106, val_acc:0.934]
Epoch [50/120    avg_loss:0.085, val_acc:0.927]
Epoch [51/120    avg_loss:0.095, val_acc:0.925]
Epoch [52/120    avg_loss:0.089, val_acc:0.933]
Epoch [53/120    avg_loss:0.105, val_acc:0.928]
Epoch [54/120    avg_loss:0.221, val_acc:0.872]
Epoch [55/120    avg_loss:0.181, val_acc:0.918]
Epoch [56/120    avg_loss:0.112, val_acc:0.932]
Epoch [57/120    avg_loss:0.073, val_acc:0.940]
Epoch [58/120    avg_loss:0.063, val_acc:0.944]
Epoch [59/120    avg_loss:0.064, val_acc:0.943]
Epoch [60/120    avg_loss:0.059, val_acc:0.944]
Epoch [61/120    avg_loss:0.057, val_acc:0.945]
Epoch [62/120    avg_loss:0.056, val_acc:0.946]
Epoch [63/120    avg_loss:0.047, val_acc:0.944]
Epoch [64/120    avg_loss:0.049, val_acc:0.947]
Epoch [65/120    avg_loss:0.051, val_acc:0.948]
Epoch [66/120    avg_loss:0.047, val_acc:0.952]
Epoch [67/120    avg_loss:0.049, val_acc:0.951]
Epoch [68/120    avg_loss:0.054, val_acc:0.951]
Epoch [69/120    avg_loss:0.041, val_acc:0.951]
Epoch [70/120    avg_loss:0.045, val_acc:0.950]
Epoch [71/120    avg_loss:0.056, val_acc:0.950]
Epoch [72/120    avg_loss:0.052, val_acc:0.947]
Epoch [73/120    avg_loss:0.051, val_acc:0.947]
Epoch [74/120    avg_loss:0.038, val_acc:0.944]
Epoch [75/120    avg_loss:0.042, val_acc:0.950]
Epoch [76/120    avg_loss:0.046, val_acc:0.945]
Epoch [77/120    avg_loss:0.044, val_acc:0.948]
Epoch [78/120    avg_loss:0.044, val_acc:0.948]
Epoch [79/120    avg_loss:0.046, val_acc:0.950]
Epoch [80/120    avg_loss:0.036, val_acc:0.952]
Epoch [81/120    avg_loss:0.037, val_acc:0.951]
Epoch [82/120    avg_loss:0.038, val_acc:0.951]
Epoch [83/120    avg_loss:0.039, val_acc:0.950]
Epoch [84/120    avg_loss:0.037, val_acc:0.951]
Epoch [85/120    avg_loss:0.048, val_acc:0.951]
Epoch [86/120    avg_loss:0.039, val_acc:0.951]
Epoch [87/120    avg_loss:0.041, val_acc:0.952]
Epoch [88/120    avg_loss:0.042, val_acc:0.950]
Epoch [89/120    avg_loss:0.039, val_acc:0.950]
Epoch [90/120    avg_loss:0.040, val_acc:0.948]
Epoch [91/120    avg_loss:0.046, val_acc:0.948]
Epoch [92/120    avg_loss:0.045, val_acc:0.948]
Epoch [93/120    avg_loss:0.041, val_acc:0.948]
Epoch [94/120    avg_loss:0.039, val_acc:0.948]
Epoch [95/120    avg_loss:0.041, val_acc:0.947]
Epoch [96/120    avg_loss:0.038, val_acc:0.948]
Epoch [97/120    avg_loss:0.041, val_acc:0.950]
Epoch [98/120    avg_loss:0.044, val_acc:0.950]
Epoch [99/120    avg_loss:0.047, val_acc:0.948]
Epoch [100/120    avg_loss:0.038, val_acc:0.948]
Epoch [101/120    avg_loss:0.041, val_acc:0.948]
Epoch [102/120    avg_loss:0.041, val_acc:0.948]
Epoch [103/120    avg_loss:0.042, val_acc:0.948]
Epoch [104/120    avg_loss:0.037, val_acc:0.948]
Epoch [105/120    avg_loss:0.045, val_acc:0.948]
Epoch [106/120    avg_loss:0.040, val_acc:0.948]
Epoch [107/120    avg_loss:0.038, val_acc:0.948]
Epoch [108/120    avg_loss:0.036, val_acc:0.948]
Epoch [109/120    avg_loss:0.042, val_acc:0.948]
Epoch [110/120    avg_loss:0.037, val_acc:0.948]
Epoch [111/120    avg_loss:0.043, val_acc:0.948]
Epoch [112/120    avg_loss:0.039, val_acc:0.948]
Epoch [113/120    avg_loss:0.040, val_acc:0.948]
Epoch [114/120    avg_loss:0.043, val_acc:0.948]
Epoch [115/120    avg_loss:0.036, val_acc:0.948]
Epoch [116/120    avg_loss:0.039, val_acc:0.948]
Epoch [117/120    avg_loss:0.042, val_acc:0.948]
Epoch [118/120    avg_loss:0.037, val_acc:0.948]
Epoch [119/120    avg_loss:0.045, val_acc:0.948]
Epoch [120/120    avg_loss:0.037, val_acc:0.948]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1202    2    0    0    1    0    0    0   25   55    0    0
     0    0    0]
 [   0    0    0  658    8   12    0    0    0    5    0    0   62    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    4    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   49   31    0    9    2    0    0    0  763   16    0    0
     0    5    0]
 [   0    0   12    0    0    3   11    0    1    0   15 2158    6    4
     0    0    0]
 [   0    3    0    0    1   15    0    0    0    0    0   15  493    0
     1    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   13    0    0    1    0    4    1    0    0
  1120    0    0]
 [   0    0    0    0    0    0    4    0    0    2    0    0    0    0
    44  297    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.99186991869918

F1 scores:
[       nan 0.91358025 0.94348509 0.91515994 0.97931034 0.94130435
 0.98113208 0.98039216 0.99767981 0.76190476 0.90510083 0.96793003
 0.89799636 0.98404255 0.97095795 0.91525424 0.95953757]

Kappa:
0.9428821858490738
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3ec46fe828>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.607, val_acc:0.368]
Epoch [2/120    avg_loss:2.204, val_acc:0.533]
Epoch [3/120    avg_loss:2.006, val_acc:0.567]
Epoch [4/120    avg_loss:1.851, val_acc:0.584]
Epoch [5/120    avg_loss:1.703, val_acc:0.592]
Epoch [6/120    avg_loss:1.587, val_acc:0.640]
Epoch [7/120    avg_loss:1.437, val_acc:0.637]
Epoch [8/120    avg_loss:1.328, val_acc:0.692]
Epoch [9/120    avg_loss:1.191, val_acc:0.696]
Epoch [10/120    avg_loss:1.113, val_acc:0.738]
Epoch [11/120    avg_loss:1.009, val_acc:0.743]
Epoch [12/120    avg_loss:0.889, val_acc:0.762]
Epoch [13/120    avg_loss:0.808, val_acc:0.792]
Epoch [14/120    avg_loss:0.687, val_acc:0.839]
Epoch [15/120    avg_loss:0.656, val_acc:0.844]
Epoch [16/120    avg_loss:0.547, val_acc:0.858]
Epoch [17/120    avg_loss:0.521, val_acc:0.876]
Epoch [18/120    avg_loss:0.456, val_acc:0.853]
Epoch [19/120    avg_loss:0.397, val_acc:0.893]
Epoch [20/120    avg_loss:0.362, val_acc:0.914]
Epoch [21/120    avg_loss:0.347, val_acc:0.901]
Epoch [22/120    avg_loss:0.295, val_acc:0.906]
Epoch [23/120    avg_loss:0.250, val_acc:0.919]
Epoch [24/120    avg_loss:0.251, val_acc:0.894]
Epoch [25/120    avg_loss:0.244, val_acc:0.879]
Epoch [26/120    avg_loss:0.214, val_acc:0.925]
Epoch [27/120    avg_loss:0.256, val_acc:0.911]
Epoch [28/120    avg_loss:0.237, val_acc:0.911]
Epoch [29/120    avg_loss:0.185, val_acc:0.922]
Epoch [30/120    avg_loss:0.185, val_acc:0.902]
Epoch [31/120    avg_loss:0.198, val_acc:0.933]
Epoch [32/120    avg_loss:0.195, val_acc:0.939]
Epoch [33/120    avg_loss:0.144, val_acc:0.921]
Epoch [34/120    avg_loss:0.149, val_acc:0.942]
Epoch [35/120    avg_loss:0.144, val_acc:0.949]
Epoch [36/120    avg_loss:0.119, val_acc:0.955]
Epoch [37/120    avg_loss:0.121, val_acc:0.945]
Epoch [38/120    avg_loss:0.128, val_acc:0.951]
Epoch [39/120    avg_loss:0.123, val_acc:0.938]
Epoch [40/120    avg_loss:0.130, val_acc:0.950]
Epoch [41/120    avg_loss:0.114, val_acc:0.951]
Epoch [42/120    avg_loss:0.101, val_acc:0.943]
Epoch [43/120    avg_loss:0.084, val_acc:0.943]
Epoch [44/120    avg_loss:0.084, val_acc:0.951]
Epoch [45/120    avg_loss:0.087, val_acc:0.959]
Epoch [46/120    avg_loss:0.087, val_acc:0.964]
Epoch [47/120    avg_loss:0.067, val_acc:0.960]
Epoch [48/120    avg_loss:0.063, val_acc:0.965]
Epoch [49/120    avg_loss:0.060, val_acc:0.967]
Epoch [50/120    avg_loss:0.056, val_acc:0.969]
Epoch [51/120    avg_loss:0.048, val_acc:0.958]
Epoch [52/120    avg_loss:0.076, val_acc:0.959]
Epoch [53/120    avg_loss:0.070, val_acc:0.956]
Epoch [54/120    avg_loss:0.088, val_acc:0.958]
Epoch [55/120    avg_loss:0.053, val_acc:0.964]
Epoch [56/120    avg_loss:0.060, val_acc:0.956]
Epoch [57/120    avg_loss:0.066, val_acc:0.926]
Epoch [58/120    avg_loss:0.129, val_acc:0.950]
Epoch [59/120    avg_loss:0.085, val_acc:0.943]
Epoch [60/120    avg_loss:0.084, val_acc:0.963]
Epoch [61/120    avg_loss:0.091, val_acc:0.968]
Epoch [62/120    avg_loss:0.072, val_acc:0.938]
Epoch [63/120    avg_loss:0.063, val_acc:0.967]
Epoch [64/120    avg_loss:0.049, val_acc:0.965]
Epoch [65/120    avg_loss:0.036, val_acc:0.969]
Epoch [66/120    avg_loss:0.046, val_acc:0.969]
Epoch [67/120    avg_loss:0.031, val_acc:0.971]
Epoch [68/120    avg_loss:0.032, val_acc:0.971]
Epoch [69/120    avg_loss:0.031, val_acc:0.972]
Epoch [70/120    avg_loss:0.029, val_acc:0.971]
Epoch [71/120    avg_loss:0.029, val_acc:0.971]
Epoch [72/120    avg_loss:0.031, val_acc:0.971]
Epoch [73/120    avg_loss:0.027, val_acc:0.971]
Epoch [74/120    avg_loss:0.025, val_acc:0.971]
Epoch [75/120    avg_loss:0.023, val_acc:0.971]
Epoch [76/120    avg_loss:0.032, val_acc:0.971]
Epoch [77/120    avg_loss:0.027, val_acc:0.970]
Epoch [78/120    avg_loss:0.027, val_acc:0.973]
Epoch [79/120    avg_loss:0.028, val_acc:0.972]
Epoch [80/120    avg_loss:0.028, val_acc:0.972]
Epoch [81/120    avg_loss:0.026, val_acc:0.972]
Epoch [82/120    avg_loss:0.026, val_acc:0.972]
Epoch [83/120    avg_loss:0.026, val_acc:0.973]
Epoch [84/120    avg_loss:0.026, val_acc:0.973]
Epoch [85/120    avg_loss:0.028, val_acc:0.973]
Epoch [86/120    avg_loss:0.028, val_acc:0.971]
Epoch [87/120    avg_loss:0.024, val_acc:0.972]
Epoch [88/120    avg_loss:0.020, val_acc:0.973]
Epoch [89/120    avg_loss:0.020, val_acc:0.974]
Epoch [90/120    avg_loss:0.026, val_acc:0.972]
Epoch [91/120    avg_loss:0.019, val_acc:0.973]
Epoch [92/120    avg_loss:0.026, val_acc:0.971]
Epoch [93/120    avg_loss:0.025, val_acc:0.974]
Epoch [94/120    avg_loss:0.021, val_acc:0.972]
Epoch [95/120    avg_loss:0.021, val_acc:0.974]
Epoch [96/120    avg_loss:0.023, val_acc:0.972]
Epoch [97/120    avg_loss:0.022, val_acc:0.974]
Epoch [98/120    avg_loss:0.022, val_acc:0.973]
Epoch [99/120    avg_loss:0.025, val_acc:0.973]
Epoch [100/120    avg_loss:0.021, val_acc:0.972]
Epoch [101/120    avg_loss:0.022, val_acc:0.974]
Epoch [102/120    avg_loss:0.018, val_acc:0.972]
Epoch [103/120    avg_loss:0.025, val_acc:0.970]
Epoch [104/120    avg_loss:0.025, val_acc:0.974]
Epoch [105/120    avg_loss:0.025, val_acc:0.975]
Epoch [106/120    avg_loss:0.020, val_acc:0.973]
Epoch [107/120    avg_loss:0.021, val_acc:0.977]
Epoch [108/120    avg_loss:0.020, val_acc:0.977]
Epoch [109/120    avg_loss:0.023, val_acc:0.973]
Epoch [110/120    avg_loss:0.021, val_acc:0.974]
Epoch [111/120    avg_loss:0.026, val_acc:0.979]
Epoch [112/120    avg_loss:0.020, val_acc:0.978]
Epoch [113/120    avg_loss:0.022, val_acc:0.978]
Epoch [114/120    avg_loss:0.019, val_acc:0.977]
Epoch [115/120    avg_loss:0.022, val_acc:0.977]
Epoch [116/120    avg_loss:0.020, val_acc:0.974]
Epoch [117/120    avg_loss:0.023, val_acc:0.975]
Epoch [118/120    avg_loss:0.019, val_acc:0.977]
Epoch [119/120    avg_loss:0.017, val_acc:0.977]
Epoch [120/120    avg_loss:0.024, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1237    1    7    0    7    0    0    1    4   28    0    0
     0    0    0]
 [   0    0    1  715    0   16    0    0    0    4    0    0    6    4
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2   19    0    9    0    0    0    0  835    9    0    0
     0    1    0]
 [   0    0    4    0    0    1    4    0    1    0   22 2176    0    2
     0    0    0]
 [   0    0    0   11   12   10    0    0    0    0    0    0  499    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    3    0    1    0    0    0
  1129    2    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    40  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.22493224932249

F1 scores:
[       nan 0.94871795 0.97825227 0.95780308 0.95730337 0.95143488
 0.98644578 0.98039216 0.99537037 0.85714286 0.95921884 0.98372514
 0.95869356 0.98404255 0.97706621 0.92496172 0.97619048]

Kappa:
0.9683667176130931
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3d12048940>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.578, val_acc:0.345]
Epoch [2/120    avg_loss:2.199, val_acc:0.519]
Epoch [3/120    avg_loss:1.961, val_acc:0.577]
Epoch [4/120    avg_loss:1.768, val_acc:0.602]
Epoch [5/120    avg_loss:1.557, val_acc:0.582]
Epoch [6/120    avg_loss:1.435, val_acc:0.652]
Epoch [7/120    avg_loss:1.309, val_acc:0.628]
Epoch [8/120    avg_loss:1.122, val_acc:0.700]
Epoch [9/120    avg_loss:0.966, val_acc:0.716]
Epoch [10/120    avg_loss:0.999, val_acc:0.723]
Epoch [11/120    avg_loss:0.822, val_acc:0.777]
Epoch [12/120    avg_loss:0.717, val_acc:0.779]
Epoch [13/120    avg_loss:0.656, val_acc:0.776]
Epoch [14/120    avg_loss:0.590, val_acc:0.782]
Epoch [15/120    avg_loss:0.666, val_acc:0.783]
Epoch [16/120    avg_loss:0.603, val_acc:0.802]
Epoch [17/120    avg_loss:0.550, val_acc:0.797]
Epoch [18/120    avg_loss:0.513, val_acc:0.855]
Epoch [19/120    avg_loss:0.448, val_acc:0.851]
Epoch [20/120    avg_loss:0.411, val_acc:0.843]
Epoch [21/120    avg_loss:0.381, val_acc:0.857]
Epoch [22/120    avg_loss:0.443, val_acc:0.851]
Epoch [23/120    avg_loss:0.391, val_acc:0.838]
Epoch [24/120    avg_loss:0.372, val_acc:0.876]
Epoch [25/120    avg_loss:0.323, val_acc:0.868]
Epoch [26/120    avg_loss:0.314, val_acc:0.853]
Epoch [27/120    avg_loss:0.362, val_acc:0.812]
Epoch [28/120    avg_loss:0.391, val_acc:0.861]
Epoch [29/120    avg_loss:0.294, val_acc:0.877]
Epoch [30/120    avg_loss:0.233, val_acc:0.904]
Epoch [31/120    avg_loss:0.228, val_acc:0.867]
Epoch [32/120    avg_loss:0.243, val_acc:0.905]
Epoch [33/120    avg_loss:0.193, val_acc:0.929]
Epoch [34/120    avg_loss:0.159, val_acc:0.921]
Epoch [35/120    avg_loss:0.152, val_acc:0.936]
Epoch [36/120    avg_loss:0.129, val_acc:0.943]
Epoch [37/120    avg_loss:0.124, val_acc:0.936]
Epoch [38/120    avg_loss:0.152, val_acc:0.895]
Epoch [39/120    avg_loss:0.142, val_acc:0.918]
Epoch [40/120    avg_loss:0.115, val_acc:0.943]
Epoch [41/120    avg_loss:0.123, val_acc:0.929]
Epoch [42/120    avg_loss:0.130, val_acc:0.942]
Epoch [43/120    avg_loss:0.105, val_acc:0.938]
Epoch [44/120    avg_loss:0.108, val_acc:0.946]
Epoch [45/120    avg_loss:0.100, val_acc:0.930]
Epoch [46/120    avg_loss:0.097, val_acc:0.954]
Epoch [47/120    avg_loss:0.118, val_acc:0.912]
Epoch [48/120    avg_loss:0.144, val_acc:0.931]
Epoch [49/120    avg_loss:0.120, val_acc:0.943]
Epoch [50/120    avg_loss:0.094, val_acc:0.939]
Epoch [51/120    avg_loss:0.099, val_acc:0.942]
Epoch [52/120    avg_loss:0.103, val_acc:0.941]
Epoch [53/120    avg_loss:0.104, val_acc:0.921]
Epoch [54/120    avg_loss:0.095, val_acc:0.936]
Epoch [55/120    avg_loss:0.098, val_acc:0.957]
Epoch [56/120    avg_loss:0.088, val_acc:0.953]
Epoch [57/120    avg_loss:0.054, val_acc:0.956]
Epoch [58/120    avg_loss:0.058, val_acc:0.957]
Epoch [59/120    avg_loss:0.065, val_acc:0.957]
Epoch [60/120    avg_loss:0.058, val_acc:0.961]
Epoch [61/120    avg_loss:0.055, val_acc:0.958]
Epoch [62/120    avg_loss:0.112, val_acc:0.933]
Epoch [63/120    avg_loss:0.136, val_acc:0.950]
Epoch [64/120    avg_loss:0.059, val_acc:0.952]
Epoch [65/120    avg_loss:0.074, val_acc:0.950]
Epoch [66/120    avg_loss:0.120, val_acc:0.955]
Epoch [67/120    avg_loss:0.066, val_acc:0.965]
Epoch [68/120    avg_loss:0.069, val_acc:0.950]
Epoch [69/120    avg_loss:0.064, val_acc:0.953]
Epoch [70/120    avg_loss:0.066, val_acc:0.952]
Epoch [71/120    avg_loss:0.089, val_acc:0.939]
Epoch [72/120    avg_loss:0.086, val_acc:0.962]
Epoch [73/120    avg_loss:0.035, val_acc:0.969]
Epoch [74/120    avg_loss:0.039, val_acc:0.958]
Epoch [75/120    avg_loss:0.037, val_acc:0.970]
Epoch [76/120    avg_loss:0.029, val_acc:0.973]
Epoch [77/120    avg_loss:0.030, val_acc:0.973]
Epoch [78/120    avg_loss:0.036, val_acc:0.969]
Epoch [79/120    avg_loss:0.046, val_acc:0.967]
Epoch [80/120    avg_loss:0.046, val_acc:0.952]
Epoch [81/120    avg_loss:0.082, val_acc:0.947]
Epoch [82/120    avg_loss:0.068, val_acc:0.943]
Epoch [83/120    avg_loss:0.065, val_acc:0.961]
Epoch [84/120    avg_loss:0.049, val_acc:0.959]
Epoch [85/120    avg_loss:0.035, val_acc:0.971]
Epoch [86/120    avg_loss:0.026, val_acc:0.974]
Epoch [87/120    avg_loss:0.029, val_acc:0.966]
Epoch [88/120    avg_loss:0.027, val_acc:0.976]
Epoch [89/120    avg_loss:0.022, val_acc:0.971]
Epoch [90/120    avg_loss:0.042, val_acc:0.923]
Epoch [91/120    avg_loss:0.115, val_acc:0.950]
Epoch [92/120    avg_loss:0.054, val_acc:0.964]
Epoch [93/120    avg_loss:0.049, val_acc:0.961]
Epoch [94/120    avg_loss:0.063, val_acc:0.964]
Epoch [95/120    avg_loss:0.043, val_acc:0.957]
Epoch [96/120    avg_loss:0.042, val_acc:0.971]
Epoch [97/120    avg_loss:0.036, val_acc:0.971]
Epoch [98/120    avg_loss:0.029, val_acc:0.965]
Epoch [99/120    avg_loss:0.024, val_acc:0.958]
Epoch [100/120    avg_loss:0.034, val_acc:0.970]
Epoch [101/120    avg_loss:0.029, val_acc:0.970]
Epoch [102/120    avg_loss:0.019, val_acc:0.973]
Epoch [103/120    avg_loss:0.014, val_acc:0.973]
Epoch [104/120    avg_loss:0.015, val_acc:0.974]
Epoch [105/120    avg_loss:0.019, val_acc:0.975]
Epoch [106/120    avg_loss:0.014, val_acc:0.976]
Epoch [107/120    avg_loss:0.013, val_acc:0.976]
Epoch [108/120    avg_loss:0.014, val_acc:0.979]
Epoch [109/120    avg_loss:0.016, val_acc:0.978]
Epoch [110/120    avg_loss:0.016, val_acc:0.978]
Epoch [111/120    avg_loss:0.014, val_acc:0.980]
Epoch [112/120    avg_loss:0.012, val_acc:0.980]
Epoch [113/120    avg_loss:0.016, val_acc:0.979]
Epoch [114/120    avg_loss:0.018, val_acc:0.980]
Epoch [115/120    avg_loss:0.015, val_acc:0.978]
Epoch [116/120    avg_loss:0.012, val_acc:0.980]
Epoch [117/120    avg_loss:0.014, val_acc:0.979]
Epoch [118/120    avg_loss:0.012, val_acc:0.978]
Epoch [119/120    avg_loss:0.012, val_acc:0.977]
Epoch [120/120    avg_loss:0.012, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1236   12    0    0    0    0    0    0    2   33    1    0
     0    1    0]
 [   0    0    2  714    3   16    0    0    0    6    0    0    5    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0   21   41    0    3    0    0    0    0  796    9    0    0
     0    5    0]
 [   0    0    3    0    0    0    1    0    0    0    4 2202    0    0
     0    0    0]
 [   0    0    8    5    3    2    0    0    0    0    5   15  488    0
     0    3    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    1    0    0    0    0    0    2    0    2    1    0    0
  1133    0    0]
 [   0    0    0    0    0    0   39    0    0    0    0    0    0    0
    76  232    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.24932249322494

F1 scores:
[       nan 0.975      0.96713615 0.94009217 0.98611111 0.97297297
 0.96969697 0.98039216 0.99767981 0.76923077 0.94424674 0.98479428
 0.94573643 1.         0.96425532 0.78911565 0.96511628]

Kappa:
0.9571691359869365
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f081f9b28d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.686, val_acc:0.309]
Epoch [2/120    avg_loss:2.283, val_acc:0.375]
Epoch [3/120    avg_loss:2.054, val_acc:0.516]
Epoch [4/120    avg_loss:1.881, val_acc:0.583]
Epoch [5/120    avg_loss:1.680, val_acc:0.631]
Epoch [6/120    avg_loss:1.450, val_acc:0.648]
Epoch [7/120    avg_loss:1.275, val_acc:0.669]
Epoch [8/120    avg_loss:1.185, val_acc:0.706]
Epoch [9/120    avg_loss:1.023, val_acc:0.701]
Epoch [10/120    avg_loss:0.918, val_acc:0.754]
Epoch [11/120    avg_loss:0.816, val_acc:0.710]
Epoch [12/120    avg_loss:0.788, val_acc:0.757]
Epoch [13/120    avg_loss:0.682, val_acc:0.788]
Epoch [14/120    avg_loss:0.662, val_acc:0.786]
Epoch [15/120    avg_loss:0.574, val_acc:0.812]
Epoch [16/120    avg_loss:0.583, val_acc:0.830]
Epoch [17/120    avg_loss:0.459, val_acc:0.820]
Epoch [18/120    avg_loss:0.527, val_acc:0.819]
Epoch [19/120    avg_loss:0.461, val_acc:0.830]
Epoch [20/120    avg_loss:0.390, val_acc:0.814]
Epoch [21/120    avg_loss:0.400, val_acc:0.790]
Epoch [22/120    avg_loss:0.371, val_acc:0.854]
Epoch [23/120    avg_loss:0.312, val_acc:0.825]
Epoch [24/120    avg_loss:0.293, val_acc:0.874]
Epoch [25/120    avg_loss:0.302, val_acc:0.827]
Epoch [26/120    avg_loss:0.298, val_acc:0.852]
Epoch [27/120    avg_loss:0.252, val_acc:0.893]
Epoch [28/120    avg_loss:0.264, val_acc:0.895]
Epoch [29/120    avg_loss:0.263, val_acc:0.877]
Epoch [30/120    avg_loss:0.214, val_acc:0.886]
Epoch [31/120    avg_loss:0.242, val_acc:0.905]
Epoch [32/120    avg_loss:0.189, val_acc:0.895]
Epoch [33/120    avg_loss:0.198, val_acc:0.900]
Epoch [34/120    avg_loss:0.199, val_acc:0.915]
Epoch [35/120    avg_loss:0.140, val_acc:0.923]
Epoch [36/120    avg_loss:0.176, val_acc:0.888]
Epoch [37/120    avg_loss:0.203, val_acc:0.898]
Epoch [38/120    avg_loss:0.192, val_acc:0.886]
Epoch [39/120    avg_loss:0.156, val_acc:0.872]
Epoch [40/120    avg_loss:0.197, val_acc:0.862]
Epoch [41/120    avg_loss:0.136, val_acc:0.933]
Epoch [42/120    avg_loss:0.132, val_acc:0.933]
Epoch [43/120    avg_loss:0.165, val_acc:0.926]
Epoch [44/120    avg_loss:0.132, val_acc:0.914]
Epoch [45/120    avg_loss:0.146, val_acc:0.916]
Epoch [46/120    avg_loss:0.098, val_acc:0.924]
Epoch [47/120    avg_loss:0.107, val_acc:0.919]
Epoch [48/120    avg_loss:0.135, val_acc:0.921]
Epoch [49/120    avg_loss:0.118, val_acc:0.921]
Epoch [50/120    avg_loss:0.112, val_acc:0.921]
Epoch [51/120    avg_loss:0.095, val_acc:0.942]
Epoch [52/120    avg_loss:0.097, val_acc:0.924]
Epoch [53/120    avg_loss:0.113, val_acc:0.926]
Epoch [54/120    avg_loss:0.074, val_acc:0.948]
Epoch [55/120    avg_loss:0.070, val_acc:0.941]
Epoch [56/120    avg_loss:0.098, val_acc:0.913]
Epoch [57/120    avg_loss:0.091, val_acc:0.939]
Epoch [58/120    avg_loss:0.065, val_acc:0.945]
Epoch [59/120    avg_loss:0.072, val_acc:0.931]
Epoch [60/120    avg_loss:0.058, val_acc:0.949]
Epoch [61/120    avg_loss:0.044, val_acc:0.904]
Epoch [62/120    avg_loss:0.050, val_acc:0.954]
Epoch [63/120    avg_loss:0.087, val_acc:0.932]
Epoch [64/120    avg_loss:0.060, val_acc:0.950]
Epoch [65/120    avg_loss:0.063, val_acc:0.944]
Epoch [66/120    avg_loss:0.037, val_acc:0.961]
Epoch [67/120    avg_loss:0.060, val_acc:0.958]
Epoch [68/120    avg_loss:0.084, val_acc:0.930]
Epoch [69/120    avg_loss:0.082, val_acc:0.954]
Epoch [70/120    avg_loss:0.089, val_acc:0.951]
Epoch [71/120    avg_loss:0.064, val_acc:0.953]
Epoch [72/120    avg_loss:0.079, val_acc:0.951]
Epoch [73/120    avg_loss:0.039, val_acc:0.954]
Epoch [74/120    avg_loss:0.044, val_acc:0.959]
Epoch [75/120    avg_loss:0.040, val_acc:0.953]
Epoch [76/120    avg_loss:0.040, val_acc:0.950]
Epoch [77/120    avg_loss:0.032, val_acc:0.961]
Epoch [78/120    avg_loss:0.039, val_acc:0.964]
Epoch [79/120    avg_loss:0.035, val_acc:0.970]
Epoch [80/120    avg_loss:0.037, val_acc:0.956]
Epoch [81/120    avg_loss:0.040, val_acc:0.958]
Epoch [82/120    avg_loss:0.037, val_acc:0.958]
Epoch [83/120    avg_loss:0.049, val_acc:0.956]
Epoch [84/120    avg_loss:0.027, val_acc:0.964]
Epoch [85/120    avg_loss:0.025, val_acc:0.959]
Epoch [86/120    avg_loss:0.028, val_acc:0.967]
Epoch [87/120    avg_loss:0.022, val_acc:0.970]
Epoch [88/120    avg_loss:0.031, val_acc:0.946]
Epoch [89/120    avg_loss:0.042, val_acc:0.964]
Epoch [90/120    avg_loss:0.028, val_acc:0.963]
Epoch [91/120    avg_loss:0.029, val_acc:0.958]
Epoch [92/120    avg_loss:0.044, val_acc:0.955]
Epoch [93/120    avg_loss:0.034, val_acc:0.950]
Epoch [94/120    avg_loss:0.085, val_acc:0.915]
Epoch [95/120    avg_loss:0.128, val_acc:0.923]
Epoch [96/120    avg_loss:0.093, val_acc:0.952]
Epoch [97/120    avg_loss:0.052, val_acc:0.953]
Epoch [98/120    avg_loss:0.117, val_acc:0.929]
Epoch [99/120    avg_loss:0.125, val_acc:0.934]
Epoch [100/120    avg_loss:0.059, val_acc:0.955]
Epoch [101/120    avg_loss:0.033, val_acc:0.965]
Epoch [102/120    avg_loss:0.023, val_acc:0.970]
Epoch [103/120    avg_loss:0.025, val_acc:0.974]
Epoch [104/120    avg_loss:0.026, val_acc:0.971]
Epoch [105/120    avg_loss:0.019, val_acc:0.973]
Epoch [106/120    avg_loss:0.030, val_acc:0.970]
Epoch [107/120    avg_loss:0.020, val_acc:0.973]
Epoch [108/120    avg_loss:0.020, val_acc:0.974]
Epoch [109/120    avg_loss:0.025, val_acc:0.973]
Epoch [110/120    avg_loss:0.022, val_acc:0.972]
Epoch [111/120    avg_loss:0.018, val_acc:0.972]
Epoch [112/120    avg_loss:0.018, val_acc:0.971]
Epoch [113/120    avg_loss:0.019, val_acc:0.973]
Epoch [114/120    avg_loss:0.017, val_acc:0.970]
Epoch [115/120    avg_loss:0.016, val_acc:0.972]
Epoch [116/120    avg_loss:0.021, val_acc:0.974]
Epoch [117/120    avg_loss:0.016, val_acc:0.975]
Epoch [118/120    avg_loss:0.019, val_acc:0.973]
Epoch [119/120    avg_loss:0.015, val_acc:0.972]
Epoch [120/120    avg_loss:0.016, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1237    2    7    0    4    0    0    5   15   12    3    0
     0    0    0]
 [   0    0    0  703    2    0    0    0    0   14    0    0   26    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    1    2    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   15    0    0    2    0
     0    0    0]
 [   0    0    4   73    0    7    0    0    0    0  776   12    1    0
     0    2    0]
 [   0    0   17    0    0    0   11    0    0    0   10 2170    0    2
     0    0    0]
 [   0    0    0   14    5    3    0    0    0    2    0    2  502    0
     0    3    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    1    0    2    1    0    0
  1119   14    0]
 [   0    0    0    0    0    0    5    0    0    4    0    0    0    0
    89  249    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.70731707317073

F1 scores:
[       nan 0.94871795 0.97286669 0.91298701 0.96818182 0.97828571
 0.98350825 0.96153846 0.99883856 0.48387097 0.92271106 0.9845735
 0.94007491 0.98930481 0.95355773 0.8097561  0.98245614]

Kappa:
0.951069416733946
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcc178368d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.541, val_acc:0.476]
Epoch [2/120    avg_loss:2.147, val_acc:0.534]
Epoch [3/120    avg_loss:2.045, val_acc:0.584]
Epoch [4/120    avg_loss:1.821, val_acc:0.616]
Epoch [5/120    avg_loss:1.674, val_acc:0.658]
Epoch [6/120    avg_loss:1.478, val_acc:0.676]
Epoch [7/120    avg_loss:1.300, val_acc:0.682]
Epoch [8/120    avg_loss:1.138, val_acc:0.688]
Epoch [9/120    avg_loss:0.996, val_acc:0.733]
Epoch [10/120    avg_loss:0.871, val_acc:0.740]
Epoch [11/120    avg_loss:0.837, val_acc:0.757]
Epoch [12/120    avg_loss:0.715, val_acc:0.792]
Epoch [13/120    avg_loss:0.688, val_acc:0.785]
Epoch [14/120    avg_loss:0.607, val_acc:0.834]
Epoch [15/120    avg_loss:0.550, val_acc:0.833]
Epoch [16/120    avg_loss:0.525, val_acc:0.828]
Epoch [17/120    avg_loss:0.502, val_acc:0.848]
Epoch [18/120    avg_loss:0.434, val_acc:0.855]
Epoch [19/120    avg_loss:0.380, val_acc:0.847]
Epoch [20/120    avg_loss:0.380, val_acc:0.860]
Epoch [21/120    avg_loss:0.318, val_acc:0.895]
Epoch [22/120    avg_loss:0.302, val_acc:0.882]
Epoch [23/120    avg_loss:0.352, val_acc:0.875]
Epoch [24/120    avg_loss:0.338, val_acc:0.881]
Epoch [25/120    avg_loss:0.279, val_acc:0.893]
Epoch [26/120    avg_loss:0.248, val_acc:0.903]
Epoch [27/120    avg_loss:0.248, val_acc:0.905]
Epoch [28/120    avg_loss:0.231, val_acc:0.884]
Epoch [29/120    avg_loss:0.186, val_acc:0.919]
Epoch [30/120    avg_loss:0.168, val_acc:0.902]
Epoch [31/120    avg_loss:0.193, val_acc:0.903]
Epoch [32/120    avg_loss:0.176, val_acc:0.917]
Epoch [33/120    avg_loss:0.257, val_acc:0.926]
Epoch [34/120    avg_loss:0.242, val_acc:0.860]
Epoch [35/120    avg_loss:0.248, val_acc:0.917]
Epoch [36/120    avg_loss:0.244, val_acc:0.920]
Epoch [37/120    avg_loss:0.154, val_acc:0.933]
Epoch [38/120    avg_loss:0.145, val_acc:0.915]
Epoch [39/120    avg_loss:0.138, val_acc:0.944]
Epoch [40/120    avg_loss:0.122, val_acc:0.941]
Epoch [41/120    avg_loss:0.111, val_acc:0.925]
Epoch [42/120    avg_loss:0.093, val_acc:0.943]
Epoch [43/120    avg_loss:0.102, val_acc:0.938]
Epoch [44/120    avg_loss:0.101, val_acc:0.934]
Epoch [45/120    avg_loss:0.101, val_acc:0.945]
Epoch [46/120    avg_loss:0.078, val_acc:0.935]
Epoch [47/120    avg_loss:0.080, val_acc:0.936]
Epoch [48/120    avg_loss:0.088, val_acc:0.951]
Epoch [49/120    avg_loss:0.065, val_acc:0.941]
Epoch [50/120    avg_loss:0.078, val_acc:0.914]
Epoch [51/120    avg_loss:0.079, val_acc:0.965]
Epoch [52/120    avg_loss:0.064, val_acc:0.951]
Epoch [53/120    avg_loss:0.062, val_acc:0.957]
Epoch [54/120    avg_loss:0.061, val_acc:0.963]
Epoch [55/120    avg_loss:0.063, val_acc:0.958]
Epoch [56/120    avg_loss:0.070, val_acc:0.935]
Epoch [57/120    avg_loss:0.065, val_acc:0.963]
Epoch [58/120    avg_loss:0.070, val_acc:0.953]
Epoch [59/120    avg_loss:0.070, val_acc:0.964]
Epoch [60/120    avg_loss:0.071, val_acc:0.942]
Epoch [61/120    avg_loss:0.075, val_acc:0.905]
Epoch [62/120    avg_loss:0.155, val_acc:0.944]
Epoch [63/120    avg_loss:0.065, val_acc:0.951]
Epoch [64/120    avg_loss:0.074, val_acc:0.961]
Epoch [65/120    avg_loss:0.046, val_acc:0.970]
Epoch [66/120    avg_loss:0.035, val_acc:0.968]
Epoch [67/120    avg_loss:0.037, val_acc:0.967]
Epoch [68/120    avg_loss:0.033, val_acc:0.968]
Epoch [69/120    avg_loss:0.032, val_acc:0.967]
Epoch [70/120    avg_loss:0.043, val_acc:0.965]
Epoch [71/120    avg_loss:0.034, val_acc:0.966]
Epoch [72/120    avg_loss:0.027, val_acc:0.967]
Epoch [73/120    avg_loss:0.032, val_acc:0.967]
Epoch [74/120    avg_loss:0.031, val_acc:0.968]
Epoch [75/120    avg_loss:0.025, val_acc:0.967]
Epoch [76/120    avg_loss:0.030, val_acc:0.969]
Epoch [77/120    avg_loss:0.023, val_acc:0.969]
Epoch [78/120    avg_loss:0.028, val_acc:0.970]
Epoch [79/120    avg_loss:0.022, val_acc:0.969]
Epoch [80/120    avg_loss:0.029, val_acc:0.968]
Epoch [81/120    avg_loss:0.030, val_acc:0.973]
Epoch [82/120    avg_loss:0.025, val_acc:0.973]
Epoch [83/120    avg_loss:0.026, val_acc:0.973]
Epoch [84/120    avg_loss:0.029, val_acc:0.974]
Epoch [85/120    avg_loss:0.027, val_acc:0.969]
Epoch [86/120    avg_loss:0.022, val_acc:0.969]
Epoch [87/120    avg_loss:0.025, val_acc:0.972]
Epoch [88/120    avg_loss:0.024, val_acc:0.969]
Epoch [89/120    avg_loss:0.029, val_acc:0.972]
Epoch [90/120    avg_loss:0.024, val_acc:0.972]
Epoch [91/120    avg_loss:0.025, val_acc:0.972]
Epoch [92/120    avg_loss:0.022, val_acc:0.976]
Epoch [93/120    avg_loss:0.025, val_acc:0.972]
Epoch [94/120    avg_loss:0.023, val_acc:0.970]
Epoch [95/120    avg_loss:0.022, val_acc:0.970]
Epoch [96/120    avg_loss:0.023, val_acc:0.972]
Epoch [97/120    avg_loss:0.024, val_acc:0.969]
Epoch [98/120    avg_loss:0.022, val_acc:0.972]
Epoch [99/120    avg_loss:0.026, val_acc:0.972]
Epoch [100/120    avg_loss:0.018, val_acc:0.973]
Epoch [101/120    avg_loss:0.021, val_acc:0.973]
Epoch [102/120    avg_loss:0.022, val_acc:0.970]
Epoch [103/120    avg_loss:0.024, val_acc:0.974]
Epoch [104/120    avg_loss:0.022, val_acc:0.974]
Epoch [105/120    avg_loss:0.019, val_acc:0.970]
Epoch [106/120    avg_loss:0.021, val_acc:0.970]
Epoch [107/120    avg_loss:0.024, val_acc:0.970]
Epoch [108/120    avg_loss:0.019, val_acc:0.970]
Epoch [109/120    avg_loss:0.024, val_acc:0.970]
Epoch [110/120    avg_loss:0.024, val_acc:0.972]
Epoch [111/120    avg_loss:0.021, val_acc:0.970]
Epoch [112/120    avg_loss:0.020, val_acc:0.970]
Epoch [113/120    avg_loss:0.019, val_acc:0.970]
Epoch [114/120    avg_loss:0.025, val_acc:0.970]
Epoch [115/120    avg_loss:0.021, val_acc:0.970]
Epoch [116/120    avg_loss:0.017, val_acc:0.970]
Epoch [117/120    avg_loss:0.021, val_acc:0.970]
Epoch [118/120    avg_loss:0.024, val_acc:0.970]
Epoch [119/120    avg_loss:0.025, val_acc:0.970]
Epoch [120/120    avg_loss:0.020, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1249    4    0    0    1    0    0    0    5   25    1    0
     0    0    0]
 [   0    0    3  708    1   15    0    0    0    8    0    0    4    5
     3    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   14   37    0    7    0    0    0    0  800   16    0    0
     1    0    0]
 [   0    0   18    0    0    0    6    0    0    0   21 2165    0    0
     0    0    0]
 [   0    0    3   15    2   12    0    0    0    0   10   13  475    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    3    0    0    0
  1132    0    0]
 [   0    0    0    0    0    0    0    0    0    4    0    0    0    0
    64  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
96.2710027100271

F1 scores:
[       nan 1.         0.97122862 0.93712773 0.99300699 0.95238095
 0.99241275 1.         1.         0.66666667 0.93348891 0.97698556
 0.93046033 0.98666667 0.96669513 0.8913738  0.94610778]

Kappa:
0.9574557401073669
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4f7f6e78d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.611, val_acc:0.355]
Epoch [2/120    avg_loss:2.243, val_acc:0.546]
Epoch [3/120    avg_loss:2.034, val_acc:0.576]
Epoch [4/120    avg_loss:1.873, val_acc:0.577]
Epoch [5/120    avg_loss:1.718, val_acc:0.632]
Epoch [6/120    avg_loss:1.593, val_acc:0.678]
Epoch [7/120    avg_loss:1.455, val_acc:0.730]
Epoch [8/120    avg_loss:1.245, val_acc:0.758]
Epoch [9/120    avg_loss:1.175, val_acc:0.770]
Epoch [10/120    avg_loss:1.058, val_acc:0.732]
Epoch [11/120    avg_loss:0.994, val_acc:0.780]
Epoch [12/120    avg_loss:0.819, val_acc:0.780]
Epoch [13/120    avg_loss:0.688, val_acc:0.795]
Epoch [14/120    avg_loss:0.633, val_acc:0.849]
Epoch [15/120    avg_loss:0.586, val_acc:0.839]
Epoch [16/120    avg_loss:0.530, val_acc:0.861]
Epoch [17/120    avg_loss:0.528, val_acc:0.821]
Epoch [18/120    avg_loss:0.436, val_acc:0.879]
Epoch [19/120    avg_loss:0.428, val_acc:0.870]
Epoch [20/120    avg_loss:0.368, val_acc:0.885]
Epoch [21/120    avg_loss:0.381, val_acc:0.901]
Epoch [22/120    avg_loss:0.401, val_acc:0.865]
Epoch [23/120    avg_loss:0.354, val_acc:0.851]
Epoch [24/120    avg_loss:0.310, val_acc:0.882]
Epoch [25/120    avg_loss:0.258, val_acc:0.894]
Epoch [26/120    avg_loss:0.254, val_acc:0.920]
Epoch [27/120    avg_loss:0.348, val_acc:0.865]
Epoch [28/120    avg_loss:0.262, val_acc:0.918]
Epoch [29/120    avg_loss:0.221, val_acc:0.906]
Epoch [30/120    avg_loss:0.194, val_acc:0.920]
Epoch [31/120    avg_loss:0.238, val_acc:0.910]
Epoch [32/120    avg_loss:0.183, val_acc:0.921]
Epoch [33/120    avg_loss:0.164, val_acc:0.916]
Epoch [34/120    avg_loss:0.182, val_acc:0.910]
Epoch [35/120    avg_loss:0.236, val_acc:0.911]
Epoch [36/120    avg_loss:0.153, val_acc:0.924]
Epoch [37/120    avg_loss:0.171, val_acc:0.921]
Epoch [38/120    avg_loss:0.175, val_acc:0.920]
Epoch [39/120    avg_loss:0.140, val_acc:0.925]
Epoch [40/120    avg_loss:0.169, val_acc:0.946]
Epoch [41/120    avg_loss:0.147, val_acc:0.940]
Epoch [42/120    avg_loss:0.127, val_acc:0.943]
Epoch [43/120    avg_loss:0.094, val_acc:0.955]
Epoch [44/120    avg_loss:0.107, val_acc:0.951]
Epoch [45/120    avg_loss:0.108, val_acc:0.947]
Epoch [46/120    avg_loss:0.125, val_acc:0.922]
Epoch [47/120    avg_loss:0.135, val_acc:0.945]
Epoch [48/120    avg_loss:0.078, val_acc:0.963]
Epoch [49/120    avg_loss:0.098, val_acc:0.953]
Epoch [50/120    avg_loss:0.102, val_acc:0.954]
Epoch [51/120    avg_loss:0.083, val_acc:0.941]
Epoch [52/120    avg_loss:0.092, val_acc:0.945]
Epoch [53/120    avg_loss:0.095, val_acc:0.950]
Epoch [54/120    avg_loss:0.076, val_acc:0.959]
Epoch [55/120    avg_loss:0.114, val_acc:0.953]
Epoch [56/120    avg_loss:0.086, val_acc:0.950]
Epoch [57/120    avg_loss:0.052, val_acc:0.970]
Epoch [58/120    avg_loss:0.060, val_acc:0.951]
Epoch [59/120    avg_loss:0.065, val_acc:0.956]
Epoch [60/120    avg_loss:0.054, val_acc:0.952]
Epoch [61/120    avg_loss:0.066, val_acc:0.957]
Epoch [62/120    avg_loss:0.045, val_acc:0.969]
Epoch [63/120    avg_loss:0.040, val_acc:0.952]
Epoch [64/120    avg_loss:0.036, val_acc:0.969]
Epoch [65/120    avg_loss:0.038, val_acc:0.961]
Epoch [66/120    avg_loss:0.034, val_acc:0.974]
Epoch [67/120    avg_loss:0.028, val_acc:0.968]
Epoch [68/120    avg_loss:0.058, val_acc:0.942]
Epoch [69/120    avg_loss:0.044, val_acc:0.971]
Epoch [70/120    avg_loss:0.036, val_acc:0.942]
Epoch [71/120    avg_loss:0.034, val_acc:0.969]
Epoch [72/120    avg_loss:0.024, val_acc:0.974]
Epoch [73/120    avg_loss:0.033, val_acc:0.971]
Epoch [74/120    avg_loss:0.038, val_acc:0.981]
Epoch [75/120    avg_loss:0.027, val_acc:0.979]
Epoch [76/120    avg_loss:0.020, val_acc:0.979]
Epoch [77/120    avg_loss:0.032, val_acc:0.971]
Epoch [78/120    avg_loss:0.048, val_acc:0.971]
Epoch [79/120    avg_loss:0.043, val_acc:0.959]
Epoch [80/120    avg_loss:0.049, val_acc:0.967]
Epoch [81/120    avg_loss:0.031, val_acc:0.977]
Epoch [82/120    avg_loss:0.024, val_acc:0.979]
Epoch [83/120    avg_loss:0.021, val_acc:0.974]
Epoch [84/120    avg_loss:0.023, val_acc:0.980]
Epoch [85/120    avg_loss:0.029, val_acc:0.973]
Epoch [86/120    avg_loss:0.028, val_acc:0.975]
Epoch [87/120    avg_loss:0.030, val_acc:0.966]
Epoch [88/120    avg_loss:0.023, val_acc:0.975]
Epoch [89/120    avg_loss:0.016, val_acc:0.979]
Epoch [90/120    avg_loss:0.017, val_acc:0.984]
Epoch [91/120    avg_loss:0.014, val_acc:0.982]
Epoch [92/120    avg_loss:0.016, val_acc:0.981]
Epoch [93/120    avg_loss:0.015, val_acc:0.985]
Epoch [94/120    avg_loss:0.016, val_acc:0.980]
Epoch [95/120    avg_loss:0.018, val_acc:0.982]
Epoch [96/120    avg_loss:0.013, val_acc:0.984]
Epoch [97/120    avg_loss:0.013, val_acc:0.984]
Epoch [98/120    avg_loss:0.013, val_acc:0.980]
Epoch [99/120    avg_loss:0.015, val_acc:0.984]
Epoch [100/120    avg_loss:0.014, val_acc:0.985]
Epoch [101/120    avg_loss:0.014, val_acc:0.984]
Epoch [102/120    avg_loss:0.013, val_acc:0.982]
Epoch [103/120    avg_loss:0.012, val_acc:0.982]
Epoch [104/120    avg_loss:0.013, val_acc:0.985]
Epoch [105/120    avg_loss:0.010, val_acc:0.982]
Epoch [106/120    avg_loss:0.010, val_acc:0.984]
Epoch [107/120    avg_loss:0.011, val_acc:0.982]
Epoch [108/120    avg_loss:0.015, val_acc:0.985]
Epoch [109/120    avg_loss:0.013, val_acc:0.985]
Epoch [110/120    avg_loss:0.010, val_acc:0.982]
Epoch [111/120    avg_loss:0.014, val_acc:0.986]
Epoch [112/120    avg_loss:0.014, val_acc:0.986]
Epoch [113/120    avg_loss:0.011, val_acc:0.985]
Epoch [114/120    avg_loss:0.013, val_acc:0.986]
Epoch [115/120    avg_loss:0.011, val_acc:0.985]
Epoch [116/120    avg_loss:0.014, val_acc:0.987]
Epoch [117/120    avg_loss:0.014, val_acc:0.982]
Epoch [118/120    avg_loss:0.012, val_acc:0.982]
Epoch [119/120    avg_loss:0.012, val_acc:0.984]
Epoch [120/120    avg_loss:0.013, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1259    2    0    3    1    0    0    0    4   13    3    0
     0    0    0]
 [   0    0    1  714    4   13    0    0    0    7    1    0    5    2
     0    0    0]
 [   0    0    0   17  196    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    1    0    3    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9   36    0    3    1    0    0    0  822    1    0    0
     0    3    0]
 [   0    0    2    0    0    0    2    0    0    0   11 2194    1    0
     0    0    0]
 [   0    0    0    8    7   10    0    0    0    0    6   15  485    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    4    0    1    0    1    0    0    0
  1131    0    0]
 [   0    0    0    0    0    0   45    0    0    0    0    0    0    0
    62  240    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.52032520325203

F1 scores:
[       nan 0.98765432 0.98513302 0.93700787 0.93333333 0.95632699
 0.96046852 0.98039216 0.99883856 0.7826087  0.95525857 0.98962562
 0.94174757 0.99462366 0.96832192 0.81218274 0.97619048]

Kappa:
0.9602945739088773
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5c56ef4940>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.596, val_acc:0.515]
Epoch [2/120    avg_loss:2.192, val_acc:0.555]
Epoch [3/120    avg_loss:2.001, val_acc:0.570]
Epoch [4/120    avg_loss:1.800, val_acc:0.579]
Epoch [5/120    avg_loss:1.641, val_acc:0.626]
Epoch [6/120    avg_loss:1.500, val_acc:0.658]
Epoch [7/120    avg_loss:1.343, val_acc:0.703]
Epoch [8/120    avg_loss:1.170, val_acc:0.698]
Epoch [9/120    avg_loss:1.008, val_acc:0.712]
Epoch [10/120    avg_loss:0.890, val_acc:0.738]
Epoch [11/120    avg_loss:0.824, val_acc:0.770]
Epoch [12/120    avg_loss:0.803, val_acc:0.741]
Epoch [13/120    avg_loss:0.656, val_acc:0.714]
Epoch [14/120    avg_loss:0.609, val_acc:0.817]
Epoch [15/120    avg_loss:0.644, val_acc:0.783]
Epoch [16/120    avg_loss:0.533, val_acc:0.794]
Epoch [17/120    avg_loss:0.488, val_acc:0.838]
Epoch [18/120    avg_loss:0.425, val_acc:0.845]
Epoch [19/120    avg_loss:0.392, val_acc:0.854]
Epoch [20/120    avg_loss:0.372, val_acc:0.869]
Epoch [21/120    avg_loss:0.363, val_acc:0.858]
Epoch [22/120    avg_loss:0.345, val_acc:0.867]
Epoch [23/120    avg_loss:0.362, val_acc:0.878]
Epoch [24/120    avg_loss:0.351, val_acc:0.881]
Epoch [25/120    avg_loss:0.298, val_acc:0.916]
Epoch [26/120    avg_loss:0.394, val_acc:0.881]
Epoch [27/120    avg_loss:0.275, val_acc:0.906]
Epoch [28/120    avg_loss:0.243, val_acc:0.888]
Epoch [29/120    avg_loss:0.249, val_acc:0.906]
Epoch [30/120    avg_loss:0.242, val_acc:0.904]
Epoch [31/120    avg_loss:0.228, val_acc:0.927]
Epoch [32/120    avg_loss:0.225, val_acc:0.912]
Epoch [33/120    avg_loss:0.195, val_acc:0.911]
Epoch [34/120    avg_loss:0.189, val_acc:0.913]
Epoch [35/120    avg_loss:0.155, val_acc:0.925]
Epoch [36/120    avg_loss:0.177, val_acc:0.903]
Epoch [37/120    avg_loss:0.174, val_acc:0.906]
Epoch [38/120    avg_loss:0.156, val_acc:0.933]
Epoch [39/120    avg_loss:0.151, val_acc:0.933]
Epoch [40/120    avg_loss:0.121, val_acc:0.938]
Epoch [41/120    avg_loss:0.165, val_acc:0.917]
Epoch [42/120    avg_loss:0.126, val_acc:0.944]
Epoch [43/120    avg_loss:0.100, val_acc:0.942]
Epoch [44/120    avg_loss:0.102, val_acc:0.946]
Epoch [45/120    avg_loss:0.087, val_acc:0.946]
Epoch [46/120    avg_loss:0.109, val_acc:0.934]
Epoch [47/120    avg_loss:0.121, val_acc:0.932]
Epoch [48/120    avg_loss:0.110, val_acc:0.925]
Epoch [49/120    avg_loss:0.171, val_acc:0.907]
Epoch [50/120    avg_loss:0.170, val_acc:0.932]
Epoch [51/120    avg_loss:0.123, val_acc:0.944]
Epoch [52/120    avg_loss:0.087, val_acc:0.936]
Epoch [53/120    avg_loss:0.092, val_acc:0.945]
Epoch [54/120    avg_loss:0.075, val_acc:0.942]
Epoch [55/120    avg_loss:0.072, val_acc:0.945]
Epoch [56/120    avg_loss:0.064, val_acc:0.946]
Epoch [57/120    avg_loss:0.057, val_acc:0.944]
Epoch [58/120    avg_loss:0.131, val_acc:0.938]
Epoch [59/120    avg_loss:0.100, val_acc:0.951]
Epoch [60/120    avg_loss:0.090, val_acc:0.932]
Epoch [61/120    avg_loss:0.131, val_acc:0.941]
Epoch [62/120    avg_loss:0.159, val_acc:0.934]
Epoch [63/120    avg_loss:0.123, val_acc:0.915]
Epoch [64/120    avg_loss:0.092, val_acc:0.944]
Epoch [65/120    avg_loss:0.079, val_acc:0.929]
Epoch [66/120    avg_loss:0.066, val_acc:0.951]
Epoch [67/120    avg_loss:0.079, val_acc:0.946]
Epoch [68/120    avg_loss:0.059, val_acc:0.950]
Epoch [69/120    avg_loss:0.100, val_acc:0.940]
Epoch [70/120    avg_loss:0.076, val_acc:0.941]
Epoch [71/120    avg_loss:0.069, val_acc:0.948]
Epoch [72/120    avg_loss:0.101, val_acc:0.951]
Epoch [73/120    avg_loss:0.095, val_acc:0.955]
Epoch [74/120    avg_loss:0.063, val_acc:0.956]
Epoch [75/120    avg_loss:0.055, val_acc:0.943]
Epoch [76/120    avg_loss:0.067, val_acc:0.954]
Epoch [77/120    avg_loss:0.038, val_acc:0.955]
Epoch [78/120    avg_loss:0.063, val_acc:0.949]
Epoch [79/120    avg_loss:0.057, val_acc:0.952]
Epoch [80/120    avg_loss:0.038, val_acc:0.968]
Epoch [81/120    avg_loss:0.040, val_acc:0.960]
Epoch [82/120    avg_loss:0.034, val_acc:0.961]
Epoch [83/120    avg_loss:0.032, val_acc:0.949]
Epoch [84/120    avg_loss:0.029, val_acc:0.965]
Epoch [85/120    avg_loss:0.033, val_acc:0.963]
Epoch [86/120    avg_loss:0.038, val_acc:0.963]
Epoch [87/120    avg_loss:0.032, val_acc:0.962]
Epoch [88/120    avg_loss:0.043, val_acc:0.959]
Epoch [89/120    avg_loss:0.029, val_acc:0.969]
Epoch [90/120    avg_loss:0.027, val_acc:0.967]
Epoch [91/120    avg_loss:0.028, val_acc:0.961]
Epoch [92/120    avg_loss:0.070, val_acc:0.962]
Epoch [93/120    avg_loss:0.053, val_acc:0.952]
Epoch [94/120    avg_loss:0.038, val_acc:0.962]
Epoch [95/120    avg_loss:0.038, val_acc:0.968]
Epoch [96/120    avg_loss:0.032, val_acc:0.967]
Epoch [97/120    avg_loss:0.036, val_acc:0.969]
Epoch [98/120    avg_loss:0.027, val_acc:0.970]
Epoch [99/120    avg_loss:0.028, val_acc:0.958]
Epoch [100/120    avg_loss:0.031, val_acc:0.969]
Epoch [101/120    avg_loss:0.027, val_acc:0.973]
Epoch [102/120    avg_loss:0.023, val_acc:0.970]
Epoch [103/120    avg_loss:0.036, val_acc:0.965]
Epoch [104/120    avg_loss:0.026, val_acc:0.965]
Epoch [105/120    avg_loss:0.019, val_acc:0.970]
Epoch [106/120    avg_loss:0.027, val_acc:0.972]
Epoch [107/120    avg_loss:0.020, val_acc:0.964]
Epoch [108/120    avg_loss:0.026, val_acc:0.962]
Epoch [109/120    avg_loss:0.032, val_acc:0.969]
Epoch [110/120    avg_loss:0.018, val_acc:0.977]
Epoch [111/120    avg_loss:0.015, val_acc:0.975]
Epoch [112/120    avg_loss:0.023, val_acc:0.968]
Epoch [113/120    avg_loss:0.016, val_acc:0.978]
Epoch [114/120    avg_loss:0.042, val_acc:0.965]
Epoch [115/120    avg_loss:0.030, val_acc:0.958]
Epoch [116/120    avg_loss:0.054, val_acc:0.955]
Epoch [117/120    avg_loss:0.040, val_acc:0.964]
Epoch [118/120    avg_loss:0.020, val_acc:0.968]
Epoch [119/120    avg_loss:0.034, val_acc:0.952]
Epoch [120/120    avg_loss:0.061, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    2 1163    4    6    0    4    0    0    0   44   58    0    0
     0    4    0]
 [   0    0    3  684    1   13    0    0    0    4    0   17   25    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  413    0   19    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1   90    0    9    0    0    0    0  770    0    0    0
     0    5    0]
 [   0    0    9    0    0    7   22    0    0    0   20 2146    0    5
     1    0    0]
 [   0    0    1   15    0    4    0    0    0    0   15   18  473    0
     0    0    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   11    0    0    0    0    2    1    0    4
  1118    3    0]
 [   0    0    0    0    0    0   40    0    0    0    0    0    0   22
    60  225    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
93.68021680216802

F1 scores:
[       nan 0.96385542 0.94476036 0.88715953 0.97911833 0.92600897
 0.95217391 0.72463768 1.         0.8372093  0.89171975 0.96449438
 0.91666667 0.92269327 0.96462468 0.77054795 0.95454545]

Kappa:
0.9279560079244742
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f35f5429908>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.634, val_acc:0.397]
Epoch [2/120    avg_loss:2.292, val_acc:0.565]
Epoch [3/120    avg_loss:2.079, val_acc:0.597]
Epoch [4/120    avg_loss:1.886, val_acc:0.586]
Epoch [5/120    avg_loss:1.722, val_acc:0.646]
Epoch [6/120    avg_loss:1.534, val_acc:0.650]
Epoch [7/120    avg_loss:1.327, val_acc:0.705]
Epoch [8/120    avg_loss:1.172, val_acc:0.708]
Epoch [9/120    avg_loss:1.017, val_acc:0.754]
Epoch [10/120    avg_loss:0.852, val_acc:0.762]
Epoch [11/120    avg_loss:0.777, val_acc:0.798]
Epoch [12/120    avg_loss:0.774, val_acc:0.741]
Epoch [13/120    avg_loss:0.661, val_acc:0.809]
Epoch [14/120    avg_loss:0.645, val_acc:0.805]
Epoch [15/120    avg_loss:0.522, val_acc:0.830]
Epoch [16/120    avg_loss:0.575, val_acc:0.800]
Epoch [17/120    avg_loss:0.499, val_acc:0.818]
Epoch [18/120    avg_loss:0.483, val_acc:0.836]
Epoch [19/120    avg_loss:0.381, val_acc:0.862]
Epoch [20/120    avg_loss:0.420, val_acc:0.844]
Epoch [21/120    avg_loss:0.387, val_acc:0.847]
Epoch [22/120    avg_loss:0.331, val_acc:0.869]
Epoch [23/120    avg_loss:0.378, val_acc:0.819]
Epoch [24/120    avg_loss:0.373, val_acc:0.865]
Epoch [25/120    avg_loss:0.338, val_acc:0.859]
Epoch [26/120    avg_loss:0.301, val_acc:0.857]
Epoch [27/120    avg_loss:0.294, val_acc:0.895]
Epoch [28/120    avg_loss:0.223, val_acc:0.921]
Epoch [29/120    avg_loss:0.249, val_acc:0.914]
Epoch [30/120    avg_loss:0.239, val_acc:0.921]
Epoch [31/120    avg_loss:0.204, val_acc:0.919]
Epoch [32/120    avg_loss:0.231, val_acc:0.904]
Epoch [33/120    avg_loss:0.243, val_acc:0.920]
Epoch [34/120    avg_loss:0.191, val_acc:0.939]
Epoch [35/120    avg_loss:0.178, val_acc:0.897]
Epoch [36/120    avg_loss:0.212, val_acc:0.932]
Epoch [37/120    avg_loss:0.210, val_acc:0.925]
Epoch [38/120    avg_loss:0.153, val_acc:0.945]
Epoch [39/120    avg_loss:0.163, val_acc:0.931]
Epoch [40/120    avg_loss:0.187, val_acc:0.858]
Epoch [41/120    avg_loss:0.281, val_acc:0.905]
Epoch [42/120    avg_loss:0.178, val_acc:0.934]
Epoch [43/120    avg_loss:0.136, val_acc:0.946]
Epoch [44/120    avg_loss:0.118, val_acc:0.929]
Epoch [45/120    avg_loss:0.122, val_acc:0.951]
Epoch [46/120    avg_loss:0.143, val_acc:0.917]
Epoch [47/120    avg_loss:0.142, val_acc:0.943]
Epoch [48/120    avg_loss:0.117, val_acc:0.942]
Epoch [49/120    avg_loss:0.117, val_acc:0.944]
Epoch [50/120    avg_loss:0.112, val_acc:0.951]
Epoch [51/120    avg_loss:0.083, val_acc:0.953]
Epoch [52/120    avg_loss:0.109, val_acc:0.934]
Epoch [53/120    avg_loss:0.108, val_acc:0.951]
Epoch [54/120    avg_loss:0.124, val_acc:0.944]
Epoch [55/120    avg_loss:0.081, val_acc:0.953]
Epoch [56/120    avg_loss:0.091, val_acc:0.949]
Epoch [57/120    avg_loss:0.083, val_acc:0.931]
Epoch [58/120    avg_loss:0.089, val_acc:0.946]
Epoch [59/120    avg_loss:0.089, val_acc:0.952]
Epoch [60/120    avg_loss:0.098, val_acc:0.934]
Epoch [61/120    avg_loss:0.070, val_acc:0.958]
Epoch [62/120    avg_loss:0.096, val_acc:0.948]
Epoch [63/120    avg_loss:0.060, val_acc:0.963]
Epoch [64/120    avg_loss:0.089, val_acc:0.943]
Epoch [65/120    avg_loss:0.079, val_acc:0.953]
Epoch [66/120    avg_loss:0.063, val_acc:0.956]
Epoch [67/120    avg_loss:0.055, val_acc:0.954]
Epoch [68/120    avg_loss:0.054, val_acc:0.958]
Epoch [69/120    avg_loss:0.055, val_acc:0.960]
Epoch [70/120    avg_loss:0.057, val_acc:0.954]
Epoch [71/120    avg_loss:0.050, val_acc:0.946]
Epoch [72/120    avg_loss:0.072, val_acc:0.959]
Epoch [73/120    avg_loss:0.062, val_acc:0.958]
Epoch [74/120    avg_loss:0.055, val_acc:0.949]
Epoch [75/120    avg_loss:0.065, val_acc:0.941]
Epoch [76/120    avg_loss:0.072, val_acc:0.962]
Epoch [77/120    avg_loss:0.048, val_acc:0.967]
Epoch [78/120    avg_loss:0.041, val_acc:0.968]
Epoch [79/120    avg_loss:0.036, val_acc:0.968]
Epoch [80/120    avg_loss:0.040, val_acc:0.969]
Epoch [81/120    avg_loss:0.033, val_acc:0.969]
Epoch [82/120    avg_loss:0.032, val_acc:0.970]
Epoch [83/120    avg_loss:0.034, val_acc:0.971]
Epoch [84/120    avg_loss:0.033, val_acc:0.971]
Epoch [85/120    avg_loss:0.033, val_acc:0.968]
Epoch [86/120    avg_loss:0.031, val_acc:0.969]
Epoch [87/120    avg_loss:0.029, val_acc:0.970]
Epoch [88/120    avg_loss:0.027, val_acc:0.970]
Epoch [89/120    avg_loss:0.032, val_acc:0.972]
Epoch [90/120    avg_loss:0.029, val_acc:0.972]
Epoch [91/120    avg_loss:0.029, val_acc:0.971]
Epoch [92/120    avg_loss:0.032, val_acc:0.969]
Epoch [93/120    avg_loss:0.034, val_acc:0.971]
Epoch [94/120    avg_loss:0.036, val_acc:0.971]
Epoch [95/120    avg_loss:0.025, val_acc:0.971]
Epoch [96/120    avg_loss:0.031, val_acc:0.971]
Epoch [97/120    avg_loss:0.034, val_acc:0.971]
Epoch [98/120    avg_loss:0.027, val_acc:0.968]
Epoch [99/120    avg_loss:0.026, val_acc:0.969]
Epoch [100/120    avg_loss:0.027, val_acc:0.972]
Epoch [101/120    avg_loss:0.019, val_acc:0.971]
Epoch [102/120    avg_loss:0.024, val_acc:0.969]
Epoch [103/120    avg_loss:0.022, val_acc:0.971]
Epoch [104/120    avg_loss:0.024, val_acc:0.970]
Epoch [105/120    avg_loss:0.027, val_acc:0.972]
Epoch [106/120    avg_loss:0.022, val_acc:0.971]
Epoch [107/120    avg_loss:0.025, val_acc:0.973]
Epoch [108/120    avg_loss:0.026, val_acc:0.971]
Epoch [109/120    avg_loss:0.027, val_acc:0.972]
Epoch [110/120    avg_loss:0.021, val_acc:0.971]
Epoch [111/120    avg_loss:0.022, val_acc:0.972]
Epoch [112/120    avg_loss:0.025, val_acc:0.971]
Epoch [113/120    avg_loss:0.026, val_acc:0.973]
Epoch [114/120    avg_loss:0.026, val_acc:0.974]
Epoch [115/120    avg_loss:0.026, val_acc:0.973]
Epoch [116/120    avg_loss:0.021, val_acc:0.975]
Epoch [117/120    avg_loss:0.023, val_acc:0.974]
Epoch [118/120    avg_loss:0.024, val_acc:0.974]
Epoch [119/120    avg_loss:0.024, val_acc:0.972]
Epoch [120/120    avg_loss:0.023, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    2 1229    4    2    0    5    0    0    1    3   34    5    0
     0    0    0]
 [   0    0    0  722    0    9    0    0    0    7    0    0    1    8
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    4    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    5    0    0    0    0    0    0  425    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5   90    0    8    0    0    0    0  759    3    0    0
     2    8    0]
 [   0    1   13    0    0    2   13    0    0    0   13 2159    7    2
     0    0    0]
 [   0    0    0   31   12   12    0    0    0    0    4    0  469    0
     1    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    3    0    2    0    0    0
  1130    0    0]
 [   0    0    0    0    0    0    0    0    0    2    0    0    0    3
   111  231    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.07859078590786

F1 scores:
[       nan 0.8988764  0.97077409 0.90589711 0.96818182 0.95206243
 0.98496241 1.         0.99067599 0.72       0.91611346 0.98002724
 0.92322835 0.96605744 0.94600251 0.7883959  0.97109827]

Kappa:
0.9438896000164413
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f82c3e488d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.648, val_acc:0.492]
Epoch [2/120    avg_loss:2.276, val_acc:0.583]
Epoch [3/120    avg_loss:2.045, val_acc:0.597]
Epoch [4/120    avg_loss:1.828, val_acc:0.608]
Epoch [5/120    avg_loss:1.660, val_acc:0.632]
Epoch [6/120    avg_loss:1.516, val_acc:0.670]
Epoch [7/120    avg_loss:1.361, val_acc:0.693]
Epoch [8/120    avg_loss:1.231, val_acc:0.714]
Epoch [9/120    avg_loss:1.080, val_acc:0.714]
Epoch [10/120    avg_loss:0.982, val_acc:0.791]
Epoch [11/120    avg_loss:0.890, val_acc:0.778]
Epoch [12/120    avg_loss:0.786, val_acc:0.839]
Epoch [13/120    avg_loss:0.695, val_acc:0.800]
Epoch [14/120    avg_loss:0.635, val_acc:0.814]
Epoch [15/120    avg_loss:0.584, val_acc:0.839]
Epoch [16/120    avg_loss:0.574, val_acc:0.827]
Epoch [17/120    avg_loss:0.475, val_acc:0.857]
Epoch [18/120    avg_loss:0.464, val_acc:0.845]
Epoch [19/120    avg_loss:0.439, val_acc:0.839]
Epoch [20/120    avg_loss:0.490, val_acc:0.837]
Epoch [21/120    avg_loss:0.405, val_acc:0.866]
Epoch [22/120    avg_loss:0.362, val_acc:0.887]
Epoch [23/120    avg_loss:0.323, val_acc:0.901]
Epoch [24/120    avg_loss:0.293, val_acc:0.908]
Epoch [25/120    avg_loss:0.297, val_acc:0.914]
Epoch [26/120    avg_loss:0.270, val_acc:0.903]
Epoch [27/120    avg_loss:0.286, val_acc:0.869]
Epoch [28/120    avg_loss:0.296, val_acc:0.893]
Epoch [29/120    avg_loss:0.230, val_acc:0.903]
Epoch [30/120    avg_loss:0.207, val_acc:0.910]
Epoch [31/120    avg_loss:0.203, val_acc:0.920]
Epoch [32/120    avg_loss:0.199, val_acc:0.902]
Epoch [33/120    avg_loss:0.208, val_acc:0.924]
Epoch [34/120    avg_loss:0.150, val_acc:0.923]
Epoch [35/120    avg_loss:0.155, val_acc:0.943]
Epoch [36/120    avg_loss:0.138, val_acc:0.934]
Epoch [37/120    avg_loss:0.151, val_acc:0.912]
Epoch [38/120    avg_loss:0.148, val_acc:0.930]
Epoch [39/120    avg_loss:0.167, val_acc:0.927]
Epoch [40/120    avg_loss:0.145, val_acc:0.930]
Epoch [41/120    avg_loss:0.188, val_acc:0.930]
Epoch [42/120    avg_loss:0.190, val_acc:0.926]
Epoch [43/120    avg_loss:0.163, val_acc:0.935]
Epoch [44/120    avg_loss:0.119, val_acc:0.940]
Epoch [45/120    avg_loss:0.191, val_acc:0.945]
Epoch [46/120    avg_loss:0.143, val_acc:0.945]
Epoch [47/120    avg_loss:0.103, val_acc:0.940]
Epoch [48/120    avg_loss:0.107, val_acc:0.944]
Epoch [49/120    avg_loss:0.099, val_acc:0.955]
Epoch [50/120    avg_loss:0.080, val_acc:0.935]
Epoch [51/120    avg_loss:0.119, val_acc:0.935]
Epoch [52/120    avg_loss:0.207, val_acc:0.835]
Epoch [53/120    avg_loss:0.176, val_acc:0.921]
Epoch [54/120    avg_loss:0.201, val_acc:0.929]
Epoch [55/120    avg_loss:0.202, val_acc:0.912]
Epoch [56/120    avg_loss:0.211, val_acc:0.916]
Epoch [57/120    avg_loss:0.145, val_acc:0.927]
Epoch [58/120    avg_loss:0.120, val_acc:0.945]
Epoch [59/120    avg_loss:0.103, val_acc:0.939]
Epoch [60/120    avg_loss:0.092, val_acc:0.950]
Epoch [61/120    avg_loss:0.158, val_acc:0.927]
Epoch [62/120    avg_loss:0.154, val_acc:0.948]
Epoch [63/120    avg_loss:0.083, val_acc:0.955]
Epoch [64/120    avg_loss:0.084, val_acc:0.958]
Epoch [65/120    avg_loss:0.069, val_acc:0.958]
Epoch [66/120    avg_loss:0.065, val_acc:0.959]
Epoch [67/120    avg_loss:0.081, val_acc:0.954]
Epoch [68/120    avg_loss:0.061, val_acc:0.958]
Epoch [69/120    avg_loss:0.059, val_acc:0.960]
Epoch [70/120    avg_loss:0.056, val_acc:0.961]
Epoch [71/120    avg_loss:0.058, val_acc:0.963]
Epoch [72/120    avg_loss:0.063, val_acc:0.959]
Epoch [73/120    avg_loss:0.054, val_acc:0.961]
Epoch [74/120    avg_loss:0.049, val_acc:0.961]
Epoch [75/120    avg_loss:0.048, val_acc:0.960]
Epoch [76/120    avg_loss:0.053, val_acc:0.960]
Epoch [77/120    avg_loss:0.052, val_acc:0.962]
Epoch [78/120    avg_loss:0.054, val_acc:0.959]
Epoch [79/120    avg_loss:0.048, val_acc:0.960]
Epoch [80/120    avg_loss:0.056, val_acc:0.963]
Epoch [81/120    avg_loss:0.045, val_acc:0.962]
Epoch [82/120    avg_loss:0.053, val_acc:0.967]
Epoch [83/120    avg_loss:0.038, val_acc:0.965]
Epoch [84/120    avg_loss:0.048, val_acc:0.963]
Epoch [85/120    avg_loss:0.046, val_acc:0.968]
Epoch [86/120    avg_loss:0.038, val_acc:0.968]
Epoch [87/120    avg_loss:0.045, val_acc:0.968]
Epoch [88/120    avg_loss:0.049, val_acc:0.969]
Epoch [89/120    avg_loss:0.044, val_acc:0.960]
Epoch [90/120    avg_loss:0.049, val_acc:0.965]
Epoch [91/120    avg_loss:0.042, val_acc:0.965]
Epoch [92/120    avg_loss:0.043, val_acc:0.967]
Epoch [93/120    avg_loss:0.048, val_acc:0.964]
Epoch [94/120    avg_loss:0.047, val_acc:0.967]
Epoch [95/120    avg_loss:0.040, val_acc:0.968]
Epoch [96/120    avg_loss:0.039, val_acc:0.964]
Epoch [97/120    avg_loss:0.039, val_acc:0.965]
Epoch [98/120    avg_loss:0.041, val_acc:0.964]
Epoch [99/120    avg_loss:0.039, val_acc:0.967]
Epoch [100/120    avg_loss:0.045, val_acc:0.969]
Epoch [101/120    avg_loss:0.039, val_acc:0.969]
Epoch [102/120    avg_loss:0.038, val_acc:0.968]
Epoch [103/120    avg_loss:0.046, val_acc:0.969]
Epoch [104/120    avg_loss:0.040, val_acc:0.968]
Epoch [105/120    avg_loss:0.040, val_acc:0.967]
Epoch [106/120    avg_loss:0.045, val_acc:0.967]
Epoch [107/120    avg_loss:0.040, val_acc:0.969]
Epoch [108/120    avg_loss:0.039, val_acc:0.969]
Epoch [109/120    avg_loss:0.033, val_acc:0.969]
Epoch [110/120    avg_loss:0.034, val_acc:0.965]
Epoch [111/120    avg_loss:0.033, val_acc:0.969]
Epoch [112/120    avg_loss:0.039, val_acc:0.968]
Epoch [113/120    avg_loss:0.038, val_acc:0.967]
Epoch [114/120    avg_loss:0.041, val_acc:0.970]
Epoch [115/120    avg_loss:0.036, val_acc:0.970]
Epoch [116/120    avg_loss:0.033, val_acc:0.970]
Epoch [117/120    avg_loss:0.036, val_acc:0.969]
Epoch [118/120    avg_loss:0.034, val_acc:0.970]
Epoch [119/120    avg_loss:0.033, val_acc:0.970]
Epoch [120/120    avg_loss:0.035, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1202    0    0    0    6    0    0    1    9   63    0    0
     0    4    0]
 [   0    0    3  705    1    6    0    0    0   20    0    0   11    1
     0    0    0]
 [   0    0    0    3  210    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  414    0    5    0    6    0    0    0    0
    10    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    4    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    5    0    0   11    0    0    2    0
     0    0    0]
 [   0    0   33   90    0    5    0    0    0    0  737    2    0    0
     1    7    0]
 [   0    0   24    0    0    0   11    0    1    0    2 2161    5    3
     3    0    0]
 [   0    0    0   39    1   15    0    0    0    0    9    0  463    0
     0    1    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    1    0    1    2    0    0
  1131    0    0]
 [   0    0    0    0    0    2    7    0    0    8    0    0    0    0
   122  208    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
93.76693766937669

F1 scores:
[       nan 0.94871795 0.94385552 0.89015152 0.98823529 0.93984109
 0.97305389 0.90909091 0.99767981 0.34375    0.90042761 0.97298514
 0.90962672 0.98930481 0.93897883 0.73368607 0.94736842]

Kappa:
0.92885431365945
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa594950828>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.696, val_acc:0.408]
Epoch [2/120    avg_loss:2.279, val_acc:0.521]
Epoch [3/120    avg_loss:2.061, val_acc:0.554]
Epoch [4/120    avg_loss:1.879, val_acc:0.565]
Epoch [5/120    avg_loss:1.740, val_acc:0.617]
Epoch [6/120    avg_loss:1.596, val_acc:0.606]
Epoch [7/120    avg_loss:1.424, val_acc:0.660]
Epoch [8/120    avg_loss:1.301, val_acc:0.689]
Epoch [9/120    avg_loss:1.101, val_acc:0.681]
Epoch [10/120    avg_loss:0.994, val_acc:0.711]
Epoch [11/120    avg_loss:0.925, val_acc:0.744]
Epoch [12/120    avg_loss:0.816, val_acc:0.781]
Epoch [13/120    avg_loss:0.682, val_acc:0.828]
Epoch [14/120    avg_loss:0.676, val_acc:0.821]
Epoch [15/120    avg_loss:0.654, val_acc:0.819]
Epoch [16/120    avg_loss:0.519, val_acc:0.843]
Epoch [17/120    avg_loss:0.495, val_acc:0.840]
Epoch [18/120    avg_loss:0.500, val_acc:0.847]
Epoch [19/120    avg_loss:0.403, val_acc:0.871]
Epoch [20/120    avg_loss:0.423, val_acc:0.867]
Epoch [21/120    avg_loss:0.320, val_acc:0.891]
Epoch [22/120    avg_loss:0.367, val_acc:0.855]
Epoch [23/120    avg_loss:0.316, val_acc:0.905]
Epoch [24/120    avg_loss:0.279, val_acc:0.881]
Epoch [25/120    avg_loss:0.302, val_acc:0.891]
Epoch [26/120    avg_loss:0.238, val_acc:0.895]
Epoch [27/120    avg_loss:0.251, val_acc:0.896]
Epoch [28/120    avg_loss:0.224, val_acc:0.920]
Epoch [29/120    avg_loss:0.338, val_acc:0.895]
Epoch [30/120    avg_loss:1.261, val_acc:0.297]
Epoch [31/120    avg_loss:1.553, val_acc:0.570]
Epoch [32/120    avg_loss:1.005, val_acc:0.701]
Epoch [33/120    avg_loss:0.843, val_acc:0.741]
Epoch [34/120    avg_loss:0.739, val_acc:0.731]
Epoch [35/120    avg_loss:0.642, val_acc:0.773]
Epoch [36/120    avg_loss:0.549, val_acc:0.810]
Epoch [37/120    avg_loss:0.551, val_acc:0.809]
Epoch [38/120    avg_loss:0.496, val_acc:0.831]
Epoch [39/120    avg_loss:0.391, val_acc:0.827]
Epoch [40/120    avg_loss:0.399, val_acc:0.848]
Epoch [41/120    avg_loss:0.355, val_acc:0.852]
Epoch [42/120    avg_loss:0.309, val_acc:0.885]
Epoch [43/120    avg_loss:0.283, val_acc:0.892]
Epoch [44/120    avg_loss:0.255, val_acc:0.891]
Epoch [45/120    avg_loss:0.261, val_acc:0.898]
Epoch [46/120    avg_loss:0.272, val_acc:0.894]
Epoch [47/120    avg_loss:0.262, val_acc:0.900]
Epoch [48/120    avg_loss:0.252, val_acc:0.902]
Epoch [49/120    avg_loss:0.249, val_acc:0.903]
Epoch [50/120    avg_loss:0.245, val_acc:0.906]
Epoch [51/120    avg_loss:0.236, val_acc:0.910]
Epoch [52/120    avg_loss:0.238, val_acc:0.913]
Epoch [53/120    avg_loss:0.214, val_acc:0.914]
Epoch [54/120    avg_loss:0.219, val_acc:0.916]
Epoch [55/120    avg_loss:0.197, val_acc:0.914]
Epoch [56/120    avg_loss:0.240, val_acc:0.914]
Epoch [57/120    avg_loss:0.221, val_acc:0.916]
Epoch [58/120    avg_loss:0.215, val_acc:0.915]
Epoch [59/120    avg_loss:0.226, val_acc:0.915]
Epoch [60/120    avg_loss:0.208, val_acc:0.915]
Epoch [61/120    avg_loss:0.217, val_acc:0.916]
Epoch [62/120    avg_loss:0.218, val_acc:0.914]
Epoch [63/120    avg_loss:0.206, val_acc:0.915]
Epoch [64/120    avg_loss:0.216, val_acc:0.915]
Epoch [65/120    avg_loss:0.223, val_acc:0.915]
Epoch [66/120    avg_loss:0.215, val_acc:0.915]
Epoch [67/120    avg_loss:0.218, val_acc:0.914]
Epoch [68/120    avg_loss:0.213, val_acc:0.914]
Epoch [69/120    avg_loss:0.204, val_acc:0.914]
Epoch [70/120    avg_loss:0.202, val_acc:0.914]
Epoch [71/120    avg_loss:0.215, val_acc:0.914]
Epoch [72/120    avg_loss:0.205, val_acc:0.914]
Epoch [73/120    avg_loss:0.218, val_acc:0.914]
Epoch [74/120    avg_loss:0.213, val_acc:0.914]
Epoch [75/120    avg_loss:0.228, val_acc:0.914]
Epoch [76/120    avg_loss:0.216, val_acc:0.914]
Epoch [77/120    avg_loss:0.209, val_acc:0.914]
Epoch [78/120    avg_loss:0.211, val_acc:0.914]
Epoch [79/120    avg_loss:0.227, val_acc:0.914]
Epoch [80/120    avg_loss:0.219, val_acc:0.914]
Epoch [81/120    avg_loss:0.218, val_acc:0.914]
Epoch [82/120    avg_loss:0.210, val_acc:0.914]
Epoch [83/120    avg_loss:0.202, val_acc:0.914]
Epoch [84/120    avg_loss:0.208, val_acc:0.914]
Epoch [85/120    avg_loss:0.214, val_acc:0.914]
Epoch [86/120    avg_loss:0.221, val_acc:0.914]
Epoch [87/120    avg_loss:0.220, val_acc:0.914]
Epoch [88/120    avg_loss:0.203, val_acc:0.914]
Epoch [89/120    avg_loss:0.200, val_acc:0.914]
Epoch [90/120    avg_loss:0.221, val_acc:0.914]
Epoch [91/120    avg_loss:0.211, val_acc:0.914]
Epoch [92/120    avg_loss:0.221, val_acc:0.914]
Epoch [93/120    avg_loss:0.200, val_acc:0.914]
Epoch [94/120    avg_loss:0.203, val_acc:0.914]
Epoch [95/120    avg_loss:0.203, val_acc:0.914]
Epoch [96/120    avg_loss:0.226, val_acc:0.914]
Epoch [97/120    avg_loss:0.217, val_acc:0.914]
Epoch [98/120    avg_loss:0.197, val_acc:0.914]
Epoch [99/120    avg_loss:0.217, val_acc:0.914]
Epoch [100/120    avg_loss:0.199, val_acc:0.914]
Epoch [101/120    avg_loss:0.197, val_acc:0.914]
Epoch [102/120    avg_loss:0.206, val_acc:0.914]
Epoch [103/120    avg_loss:0.212, val_acc:0.914]
Epoch [104/120    avg_loss:0.223, val_acc:0.914]
Epoch [105/120    avg_loss:0.218, val_acc:0.914]
Epoch [106/120    avg_loss:0.209, val_acc:0.914]
Epoch [107/120    avg_loss:0.212, val_acc:0.914]
Epoch [108/120    avg_loss:0.210, val_acc:0.914]
Epoch [109/120    avg_loss:0.229, val_acc:0.914]
Epoch [110/120    avg_loss:0.217, val_acc:0.914]
Epoch [111/120    avg_loss:0.217, val_acc:0.914]
Epoch [112/120    avg_loss:0.205, val_acc:0.914]
Epoch [113/120    avg_loss:0.204, val_acc:0.914]
Epoch [114/120    avg_loss:0.216, val_acc:0.914]
Epoch [115/120    avg_loss:0.207, val_acc:0.914]
Epoch [116/120    avg_loss:0.220, val_acc:0.914]
Epoch [117/120    avg_loss:0.237, val_acc:0.914]
Epoch [118/120    avg_loss:0.206, val_acc:0.914]
Epoch [119/120    avg_loss:0.202, val_acc:0.914]
Epoch [120/120    avg_loss:0.205, val_acc:0.914]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1090   25    0    0    7    0    0    1   43  117    0    0
     0    0    2]
 [   0    0   12  654   25    7    0    0    0   27    3    4    5    5
     5    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  403    0   23    0    2    0    0    0    0
     7    0    0]
 [   0    0    0    0    1    0  648    0    0    0    0    2    0    0
     6    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   41   90    0   30    1    0    0    0  664   29    4    0
     5   11    0]
 [   0    2   62   53    0   16   32    0   11    0   39 1923   60    3
     9    0    0]
 [   0    0    0   41    3   15    0    0    0    1    3    0  442    0
     0    0   29]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    4    0    4    3    2    0
  1116    0    0]
 [   0    0    0    1    0    0    0    0    0    6    0    0    0    0
   216  124    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
87.27371273712737

F1 scores:
[       nan 0.89156627 0.87550201 0.81091135 0.93156733 0.87991266
 0.96356877 0.68493151 0.9793578  0.49315068 0.81223242 0.89692164
 0.8443171  0.97883598 0.89172992 0.51452282 0.84422111]

Kappa:
0.8552541359354938
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fad963ff8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.609, val_acc:0.492]
Epoch [2/120    avg_loss:2.198, val_acc:0.535]
Epoch [3/120    avg_loss:2.000, val_acc:0.566]
Epoch [4/120    avg_loss:1.830, val_acc:0.597]
Epoch [5/120    avg_loss:1.642, val_acc:0.617]
Epoch [6/120    avg_loss:1.490, val_acc:0.682]
Epoch [7/120    avg_loss:1.364, val_acc:0.677]
Epoch [8/120    avg_loss:1.117, val_acc:0.713]
Epoch [9/120    avg_loss:1.036, val_acc:0.668]
Epoch [10/120    avg_loss:0.958, val_acc:0.743]
Epoch [11/120    avg_loss:0.815, val_acc:0.745]
Epoch [12/120    avg_loss:0.731, val_acc:0.792]
Epoch [13/120    avg_loss:0.710, val_acc:0.744]
Epoch [14/120    avg_loss:0.657, val_acc:0.785]
Epoch [15/120    avg_loss:0.666, val_acc:0.784]
Epoch [16/120    avg_loss:0.590, val_acc:0.818]
Epoch [17/120    avg_loss:0.549, val_acc:0.806]
Epoch [18/120    avg_loss:0.586, val_acc:0.823]
Epoch [19/120    avg_loss:0.536, val_acc:0.780]
Epoch [20/120    avg_loss:0.418, val_acc:0.830]
Epoch [21/120    avg_loss:0.441, val_acc:0.820]
Epoch [22/120    avg_loss:0.368, val_acc:0.866]
Epoch [23/120    avg_loss:0.352, val_acc:0.874]
Epoch [24/120    avg_loss:0.291, val_acc:0.887]
Epoch [25/120    avg_loss:0.277, val_acc:0.889]
Epoch [26/120    avg_loss:0.300, val_acc:0.865]
Epoch [27/120    avg_loss:0.365, val_acc:0.860]
Epoch [28/120    avg_loss:0.283, val_acc:0.857]
Epoch [29/120    avg_loss:0.285, val_acc:0.882]
Epoch [30/120    avg_loss:0.308, val_acc:0.890]
Epoch [31/120    avg_loss:0.273, val_acc:0.877]
Epoch [32/120    avg_loss:0.296, val_acc:0.890]
Epoch [33/120    avg_loss:0.269, val_acc:0.894]
Epoch [34/120    avg_loss:0.253, val_acc:0.898]
Epoch [35/120    avg_loss:0.244, val_acc:0.906]
Epoch [36/120    avg_loss:0.225, val_acc:0.903]
Epoch [37/120    avg_loss:0.203, val_acc:0.890]
Epoch [38/120    avg_loss:0.189, val_acc:0.909]
Epoch [39/120    avg_loss:0.176, val_acc:0.918]
Epoch [40/120    avg_loss:0.172, val_acc:0.923]
Epoch [41/120    avg_loss:0.184, val_acc:0.936]
Epoch [42/120    avg_loss:0.169, val_acc:0.886]
Epoch [43/120    avg_loss:0.190, val_acc:0.912]
Epoch [44/120    avg_loss:0.180, val_acc:0.939]
Epoch [45/120    avg_loss:0.147, val_acc:0.935]
Epoch [46/120    avg_loss:0.143, val_acc:0.941]
Epoch [47/120    avg_loss:0.127, val_acc:0.935]
Epoch [48/120    avg_loss:0.148, val_acc:0.917]
Epoch [49/120    avg_loss:0.146, val_acc:0.909]
Epoch [50/120    avg_loss:0.176, val_acc:0.940]
Epoch [51/120    avg_loss:0.129, val_acc:0.945]
Epoch [52/120    avg_loss:0.123, val_acc:0.936]
Epoch [53/120    avg_loss:0.148, val_acc:0.934]
Epoch [54/120    avg_loss:0.106, val_acc:0.943]
Epoch [55/120    avg_loss:0.117, val_acc:0.951]
Epoch [56/120    avg_loss:0.198, val_acc:0.823]
Epoch [57/120    avg_loss:0.285, val_acc:0.891]
Epoch [58/120    avg_loss:0.135, val_acc:0.952]
Epoch [59/120    avg_loss:0.120, val_acc:0.939]
Epoch [60/120    avg_loss:0.118, val_acc:0.953]
Epoch [61/120    avg_loss:0.111, val_acc:0.945]
Epoch [62/120    avg_loss:0.077, val_acc:0.952]
Epoch [63/120    avg_loss:0.071, val_acc:0.950]
Epoch [64/120    avg_loss:0.079, val_acc:0.950]
Epoch [65/120    avg_loss:0.079, val_acc:0.952]
Epoch [66/120    avg_loss:0.085, val_acc:0.942]
Epoch [67/120    avg_loss:0.071, val_acc:0.948]
Epoch [68/120    avg_loss:0.066, val_acc:0.964]
Epoch [69/120    avg_loss:0.078, val_acc:0.955]
Epoch [70/120    avg_loss:0.090, val_acc:0.956]
Epoch [71/120    avg_loss:0.068, val_acc:0.965]
Epoch [72/120    avg_loss:0.072, val_acc:0.952]
Epoch [73/120    avg_loss:0.132, val_acc:0.887]
Epoch [74/120    avg_loss:0.147, val_acc:0.942]
Epoch [75/120    avg_loss:0.107, val_acc:0.948]
Epoch [76/120    avg_loss:0.077, val_acc:0.957]
Epoch [77/120    avg_loss:0.058, val_acc:0.958]
Epoch [78/120    avg_loss:0.067, val_acc:0.960]
Epoch [79/120    avg_loss:0.051, val_acc:0.958]
Epoch [80/120    avg_loss:0.054, val_acc:0.957]
Epoch [81/120    avg_loss:0.051, val_acc:0.975]
Epoch [82/120    avg_loss:0.047, val_acc:0.970]
Epoch [83/120    avg_loss:0.056, val_acc:0.964]
Epoch [84/120    avg_loss:0.087, val_acc:0.949]
Epoch [85/120    avg_loss:0.084, val_acc:0.956]
Epoch [86/120    avg_loss:0.068, val_acc:0.970]
Epoch [87/120    avg_loss:0.054, val_acc:0.963]
Epoch [88/120    avg_loss:0.067, val_acc:0.956]
Epoch [89/120    avg_loss:0.066, val_acc:0.947]
Epoch [90/120    avg_loss:0.054, val_acc:0.967]
Epoch [91/120    avg_loss:0.045, val_acc:0.970]
Epoch [92/120    avg_loss:0.039, val_acc:0.965]
Epoch [93/120    avg_loss:0.045, val_acc:0.969]
Epoch [94/120    avg_loss:0.038, val_acc:0.960]
Epoch [95/120    avg_loss:0.034, val_acc:0.973]
Epoch [96/120    avg_loss:0.032, val_acc:0.975]
Epoch [97/120    avg_loss:0.027, val_acc:0.978]
Epoch [98/120    avg_loss:0.028, val_acc:0.975]
Epoch [99/120    avg_loss:0.026, val_acc:0.977]
Epoch [100/120    avg_loss:0.024, val_acc:0.978]
Epoch [101/120    avg_loss:0.028, val_acc:0.980]
Epoch [102/120    avg_loss:0.031, val_acc:0.978]
Epoch [103/120    avg_loss:0.028, val_acc:0.978]
Epoch [104/120    avg_loss:0.026, val_acc:0.977]
Epoch [105/120    avg_loss:0.023, val_acc:0.981]
Epoch [106/120    avg_loss:0.024, val_acc:0.982]
Epoch [107/120    avg_loss:0.020, val_acc:0.982]
Epoch [108/120    avg_loss:0.024, val_acc:0.977]
Epoch [109/120    avg_loss:0.023, val_acc:0.980]
Epoch [110/120    avg_loss:0.024, val_acc:0.982]
Epoch [111/120    avg_loss:0.022, val_acc:0.978]
Epoch [112/120    avg_loss:0.023, val_acc:0.980]
Epoch [113/120    avg_loss:0.023, val_acc:0.980]
Epoch [114/120    avg_loss:0.022, val_acc:0.980]
Epoch [115/120    avg_loss:0.023, val_acc:0.981]
Epoch [116/120    avg_loss:0.021, val_acc:0.980]
Epoch [117/120    avg_loss:0.024, val_acc:0.982]
Epoch [118/120    avg_loss:0.020, val_acc:0.981]
Epoch [119/120    avg_loss:0.021, val_acc:0.983]
Epoch [120/120    avg_loss:0.025, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    1 1247    6    0    0    2    0    0    2    7   18    2    0
     0    0    0]
 [   0    0    4  731    0    1    0    0    0    7    0    0    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  417    0    3    0    3    0    0    0    0
    12    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   12    0    0    4    0
     0    0    0]
 [   0    0   12   90    0   10    0    0    0    0  757    5    0    0
     0    1    0]
 [   0    0   12    2    0    2    6    0    0    0   15 2120   47    3
     3    0    0]
 [   0    0    3   35    7    6    0    0    0    0   13    0  467    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    1    0    0    0
  1136    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
    99  246    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.94850948509485

F1 scores:
[       nan 0.93670886 0.97307842 0.90638562 0.98383372 0.95423341
 0.9908953  0.94339623 0.99883856 0.57142857 0.90550239 0.97314666
 0.88196412 0.9919571  0.95102553 0.82828283 0.97647059]

Kappa:
0.9424343240630525
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2242d1a7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.626, val_acc:0.467]
Epoch [2/120    avg_loss:2.269, val_acc:0.569]
Epoch [3/120    avg_loss:2.020, val_acc:0.570]
Epoch [4/120    avg_loss:1.844, val_acc:0.573]
Epoch [5/120    avg_loss:1.755, val_acc:0.569]
Epoch [6/120    avg_loss:1.562, val_acc:0.608]
Epoch [7/120    avg_loss:1.447, val_acc:0.598]
Epoch [8/120    avg_loss:1.314, val_acc:0.634]
Epoch [9/120    avg_loss:1.167, val_acc:0.676]
Epoch [10/120    avg_loss:1.027, val_acc:0.727]
Epoch [11/120    avg_loss:0.971, val_acc:0.695]
Epoch [12/120    avg_loss:0.919, val_acc:0.740]
Epoch [13/120    avg_loss:0.807, val_acc:0.723]
Epoch [14/120    avg_loss:0.692, val_acc:0.740]
Epoch [15/120    avg_loss:0.689, val_acc:0.746]
Epoch [16/120    avg_loss:0.709, val_acc:0.729]
Epoch [17/120    avg_loss:0.629, val_acc:0.763]
Epoch [18/120    avg_loss:0.518, val_acc:0.795]
Epoch [19/120    avg_loss:0.633, val_acc:0.797]
Epoch [20/120    avg_loss:0.539, val_acc:0.804]
Epoch [21/120    avg_loss:0.507, val_acc:0.796]
Epoch [22/120    avg_loss:0.522, val_acc:0.796]
Epoch [23/120    avg_loss:0.471, val_acc:0.776]
Epoch [24/120    avg_loss:0.390, val_acc:0.811]
Epoch [25/120    avg_loss:0.363, val_acc:0.846]
Epoch [26/120    avg_loss:0.377, val_acc:0.833]
Epoch [27/120    avg_loss:0.348, val_acc:0.855]
Epoch [28/120    avg_loss:0.338, val_acc:0.854]
Epoch [29/120    avg_loss:0.327, val_acc:0.828]
Epoch [30/120    avg_loss:0.347, val_acc:0.826]
Epoch [31/120    avg_loss:0.314, val_acc:0.883]
Epoch [32/120    avg_loss:0.329, val_acc:0.853]
Epoch [33/120    avg_loss:0.262, val_acc:0.887]
Epoch [34/120    avg_loss:0.294, val_acc:0.857]
Epoch [35/120    avg_loss:0.255, val_acc:0.905]
Epoch [36/120    avg_loss:0.264, val_acc:0.859]
Epoch [37/120    avg_loss:0.454, val_acc:0.782]
Epoch [38/120    avg_loss:0.374, val_acc:0.830]
Epoch [39/120    avg_loss:0.297, val_acc:0.871]
Epoch [40/120    avg_loss:0.206, val_acc:0.903]
Epoch [41/120    avg_loss:0.192, val_acc:0.898]
Epoch [42/120    avg_loss:0.201, val_acc:0.896]
Epoch [43/120    avg_loss:0.172, val_acc:0.892]
Epoch [44/120    avg_loss:0.187, val_acc:0.885]
Epoch [45/120    avg_loss:0.219, val_acc:0.901]
Epoch [46/120    avg_loss:0.162, val_acc:0.913]
Epoch [47/120    avg_loss:0.219, val_acc:0.898]
Epoch [48/120    avg_loss:0.189, val_acc:0.902]
Epoch [49/120    avg_loss:0.163, val_acc:0.926]
Epoch [50/120    avg_loss:0.150, val_acc:0.916]
Epoch [51/120    avg_loss:0.128, val_acc:0.915]
Epoch [52/120    avg_loss:0.154, val_acc:0.916]
Epoch [53/120    avg_loss:0.107, val_acc:0.922]
Epoch [54/120    avg_loss:0.106, val_acc:0.939]
Epoch [55/120    avg_loss:0.137, val_acc:0.911]
Epoch [56/120    avg_loss:0.127, val_acc:0.931]
Epoch [57/120    avg_loss:0.101, val_acc:0.936]
Epoch [58/120    avg_loss:0.133, val_acc:0.891]
Epoch [59/120    avg_loss:0.160, val_acc:0.922]
Epoch [60/120    avg_loss:0.149, val_acc:0.913]
Epoch [61/120    avg_loss:0.105, val_acc:0.933]
Epoch [62/120    avg_loss:0.099, val_acc:0.926]
Epoch [63/120    avg_loss:0.107, val_acc:0.940]
Epoch [64/120    avg_loss:0.078, val_acc:0.939]
Epoch [65/120    avg_loss:0.102, val_acc:0.944]
Epoch [66/120    avg_loss:0.101, val_acc:0.919]
Epoch [67/120    avg_loss:0.206, val_acc:0.930]
Epoch [68/120    avg_loss:0.126, val_acc:0.942]
Epoch [69/120    avg_loss:0.105, val_acc:0.917]
Epoch [70/120    avg_loss:0.114, val_acc:0.926]
Epoch [71/120    avg_loss:0.098, val_acc:0.946]
Epoch [72/120    avg_loss:0.078, val_acc:0.926]
Epoch [73/120    avg_loss:0.075, val_acc:0.941]
Epoch [74/120    avg_loss:0.073, val_acc:0.944]
Epoch [75/120    avg_loss:0.074, val_acc:0.936]
Epoch [76/120    avg_loss:0.054, val_acc:0.942]
Epoch [77/120    avg_loss:0.075, val_acc:0.945]
Epoch [78/120    avg_loss:0.065, val_acc:0.946]
Epoch [79/120    avg_loss:0.075, val_acc:0.953]
Epoch [80/120    avg_loss:0.065, val_acc:0.934]
Epoch [81/120    avg_loss:0.070, val_acc:0.946]
Epoch [82/120    avg_loss:0.051, val_acc:0.948]
Epoch [83/120    avg_loss:0.063, val_acc:0.948]
Epoch [84/120    avg_loss:0.051, val_acc:0.959]
Epoch [85/120    avg_loss:0.057, val_acc:0.953]
Epoch [86/120    avg_loss:0.062, val_acc:0.958]
Epoch [87/120    avg_loss:0.050, val_acc:0.952]
Epoch [88/120    avg_loss:0.055, val_acc:0.950]
Epoch [89/120    avg_loss:0.101, val_acc:0.934]
Epoch [90/120    avg_loss:0.119, val_acc:0.934]
Epoch [91/120    avg_loss:0.048, val_acc:0.927]
Epoch [92/120    avg_loss:0.066, val_acc:0.948]
Epoch [93/120    avg_loss:0.104, val_acc:0.952]
Epoch [94/120    avg_loss:0.136, val_acc:0.849]
Epoch [95/120    avg_loss:0.140, val_acc:0.944]
Epoch [96/120    avg_loss:0.071, val_acc:0.951]
Epoch [97/120    avg_loss:0.062, val_acc:0.952]
Epoch [98/120    avg_loss:0.056, val_acc:0.960]
Epoch [99/120    avg_loss:0.044, val_acc:0.961]
Epoch [100/120    avg_loss:0.041, val_acc:0.963]
Epoch [101/120    avg_loss:0.038, val_acc:0.965]
Epoch [102/120    avg_loss:0.044, val_acc:0.965]
Epoch [103/120    avg_loss:0.043, val_acc:0.964]
Epoch [104/120    avg_loss:0.034, val_acc:0.969]
Epoch [105/120    avg_loss:0.032, val_acc:0.967]
Epoch [106/120    avg_loss:0.034, val_acc:0.967]
Epoch [107/120    avg_loss:0.030, val_acc:0.963]
Epoch [108/120    avg_loss:0.029, val_acc:0.967]
Epoch [109/120    avg_loss:0.032, val_acc:0.970]
Epoch [110/120    avg_loss:0.031, val_acc:0.970]
Epoch [111/120    avg_loss:0.033, val_acc:0.965]
Epoch [112/120    avg_loss:0.032, val_acc:0.964]
Epoch [113/120    avg_loss:0.032, val_acc:0.967]
Epoch [114/120    avg_loss:0.025, val_acc:0.970]
Epoch [115/120    avg_loss:0.026, val_acc:0.969]
Epoch [116/120    avg_loss:0.028, val_acc:0.968]
Epoch [117/120    avg_loss:0.024, val_acc:0.969]
Epoch [118/120    avg_loss:0.029, val_acc:0.968]
Epoch [119/120    avg_loss:0.028, val_acc:0.970]
Epoch [120/120    avg_loss:0.031, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1201    2    0    0    4    0    0    0   12   59    6    0
     0    1    0]
 [   0    0    1  730    0    5    0    0    0    6    0    0    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    0    0    2    0    0    0    0
     9    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   16   74    0    4    0    0    0    0  771    3    0    0
     0    7    0]
 [   0    0    8    0    0    0    5    0    0    0    7 2182    4    3
     1    0    0]
 [   0    0    0   29    2    5    0    0    0    1    3    0  486    0
     0    0    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    4    0    0    0
  1127    0    0]
 [   0    0    0    0    0    0   29    0    0    1    0    0    0    0
   101  216    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.1869918699187

F1 scores:
[       nan 0.925      0.956591   0.92288243 0.9953271  0.96363636
 0.97113249 1.         0.99650757 0.7826087  0.92004773 0.97957351
 0.93731919 0.9919571  0.9482541  0.75656743 0.94252874]

Kappa:
0.9450641472884411
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6fd1dc18d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.658, val_acc:0.344]
Epoch [2/120    avg_loss:2.259, val_acc:0.497]
Epoch [3/120    avg_loss:2.022, val_acc:0.569]
Epoch [4/120    avg_loss:1.782, val_acc:0.609]
Epoch [5/120    avg_loss:1.623, val_acc:0.632]
Epoch [6/120    avg_loss:1.369, val_acc:0.662]
Epoch [7/120    avg_loss:1.250, val_acc:0.636]
Epoch [8/120    avg_loss:1.148, val_acc:0.691]
Epoch [9/120    avg_loss:0.934, val_acc:0.724]
Epoch [10/120    avg_loss:0.830, val_acc:0.750]
Epoch [11/120    avg_loss:0.862, val_acc:0.751]
Epoch [12/120    avg_loss:0.706, val_acc:0.787]
Epoch [13/120    avg_loss:0.681, val_acc:0.791]
Epoch [14/120    avg_loss:0.637, val_acc:0.785]
Epoch [15/120    avg_loss:0.598, val_acc:0.744]
Epoch [16/120    avg_loss:0.577, val_acc:0.818]
Epoch [17/120    avg_loss:0.554, val_acc:0.817]
Epoch [18/120    avg_loss:0.495, val_acc:0.815]
Epoch [19/120    avg_loss:0.406, val_acc:0.828]
Epoch [20/120    avg_loss:0.399, val_acc:0.866]
Epoch [21/120    avg_loss:0.311, val_acc:0.886]
Epoch [22/120    avg_loss:0.313, val_acc:0.823]
Epoch [23/120    avg_loss:0.345, val_acc:0.858]
Epoch [24/120    avg_loss:0.328, val_acc:0.863]
Epoch [25/120    avg_loss:0.308, val_acc:0.906]
Epoch [26/120    avg_loss:0.277, val_acc:0.900]
Epoch [27/120    avg_loss:0.256, val_acc:0.894]
Epoch [28/120    avg_loss:0.228, val_acc:0.916]
Epoch [29/120    avg_loss:0.246, val_acc:0.865]
Epoch [30/120    avg_loss:0.238, val_acc:0.886]
Epoch [31/120    avg_loss:0.235, val_acc:0.887]
Epoch [32/120    avg_loss:0.193, val_acc:0.903]
Epoch [33/120    avg_loss:0.198, val_acc:0.912]
Epoch [34/120    avg_loss:0.191, val_acc:0.905]
Epoch [35/120    avg_loss:0.160, val_acc:0.911]
Epoch [36/120    avg_loss:0.193, val_acc:0.925]
Epoch [37/120    avg_loss:0.164, val_acc:0.910]
Epoch [38/120    avg_loss:0.201, val_acc:0.931]
Epoch [39/120    avg_loss:0.153, val_acc:0.925]
Epoch [40/120    avg_loss:0.131, val_acc:0.931]
Epoch [41/120    avg_loss:0.120, val_acc:0.942]
Epoch [42/120    avg_loss:0.159, val_acc:0.909]
Epoch [43/120    avg_loss:0.192, val_acc:0.934]
Epoch [44/120    avg_loss:0.198, val_acc:0.883]
Epoch [45/120    avg_loss:0.165, val_acc:0.901]
Epoch [46/120    avg_loss:0.135, val_acc:0.934]
Epoch [47/120    avg_loss:0.169, val_acc:0.924]
Epoch [48/120    avg_loss:0.126, val_acc:0.934]
Epoch [49/120    avg_loss:0.097, val_acc:0.940]
Epoch [50/120    avg_loss:0.129, val_acc:0.923]
Epoch [51/120    avg_loss:0.109, val_acc:0.947]
Epoch [52/120    avg_loss:0.103, val_acc:0.948]
Epoch [53/120    avg_loss:0.102, val_acc:0.920]
Epoch [54/120    avg_loss:0.112, val_acc:0.957]
Epoch [55/120    avg_loss:0.100, val_acc:0.945]
Epoch [56/120    avg_loss:0.075, val_acc:0.947]
Epoch [57/120    avg_loss:0.066, val_acc:0.959]
Epoch [58/120    avg_loss:0.073, val_acc:0.948]
Epoch [59/120    avg_loss:0.068, val_acc:0.960]
Epoch [60/120    avg_loss:0.087, val_acc:0.951]
Epoch [61/120    avg_loss:0.089, val_acc:0.968]
Epoch [62/120    avg_loss:0.068, val_acc:0.951]
Epoch [63/120    avg_loss:0.060, val_acc:0.967]
Epoch [64/120    avg_loss:0.057, val_acc:0.956]
Epoch [65/120    avg_loss:0.053, val_acc:0.963]
Epoch [66/120    avg_loss:0.065, val_acc:0.969]
Epoch [67/120    avg_loss:0.052, val_acc:0.959]
Epoch [68/120    avg_loss:0.095, val_acc:0.948]
Epoch [69/120    avg_loss:0.069, val_acc:0.961]
Epoch [70/120    avg_loss:0.056, val_acc:0.960]
Epoch [71/120    avg_loss:0.056, val_acc:0.967]
Epoch [72/120    avg_loss:0.054, val_acc:0.959]
Epoch [73/120    avg_loss:0.058, val_acc:0.974]
Epoch [74/120    avg_loss:0.058, val_acc:0.963]
Epoch [75/120    avg_loss:0.049, val_acc:0.980]
Epoch [76/120    avg_loss:0.035, val_acc:0.970]
Epoch [77/120    avg_loss:0.054, val_acc:0.977]
Epoch [78/120    avg_loss:0.043, val_acc:0.968]
Epoch [79/120    avg_loss:0.042, val_acc:0.959]
Epoch [80/120    avg_loss:0.049, val_acc:0.964]
Epoch [81/120    avg_loss:0.037, val_acc:0.966]
Epoch [82/120    avg_loss:0.050, val_acc:0.976]
Epoch [83/120    avg_loss:0.032, val_acc:0.980]
Epoch [84/120    avg_loss:0.024, val_acc:0.982]
Epoch [85/120    avg_loss:0.034, val_acc:0.977]
Epoch [86/120    avg_loss:0.047, val_acc:0.964]
Epoch [87/120    avg_loss:0.034, val_acc:0.965]
Epoch [88/120    avg_loss:0.035, val_acc:0.978]
Epoch [89/120    avg_loss:0.027, val_acc:0.965]
Epoch [90/120    avg_loss:0.039, val_acc:0.977]
Epoch [91/120    avg_loss:0.030, val_acc:0.976]
Epoch [92/120    avg_loss:0.036, val_acc:0.973]
Epoch [93/120    avg_loss:0.029, val_acc:0.963]
Epoch [94/120    avg_loss:0.032, val_acc:0.976]
Epoch [95/120    avg_loss:0.019, val_acc:0.977]
Epoch [96/120    avg_loss:0.045, val_acc:0.966]
Epoch [97/120    avg_loss:0.035, val_acc:0.977]
Epoch [98/120    avg_loss:0.029, val_acc:0.982]
Epoch [99/120    avg_loss:0.019, val_acc:0.982]
Epoch [100/120    avg_loss:0.017, val_acc:0.983]
Epoch [101/120    avg_loss:0.017, val_acc:0.983]
Epoch [102/120    avg_loss:0.017, val_acc:0.984]
Epoch [103/120    avg_loss:0.017, val_acc:0.983]
Epoch [104/120    avg_loss:0.017, val_acc:0.983]
Epoch [105/120    avg_loss:0.017, val_acc:0.986]
Epoch [106/120    avg_loss:0.019, val_acc:0.988]
Epoch [107/120    avg_loss:0.019, val_acc:0.988]
Epoch [108/120    avg_loss:0.017, val_acc:0.988]
Epoch [109/120    avg_loss:0.018, val_acc:0.988]
Epoch [110/120    avg_loss:0.016, val_acc:0.989]
Epoch [111/120    avg_loss:0.017, val_acc:0.989]
Epoch [112/120    avg_loss:0.016, val_acc:0.985]
Epoch [113/120    avg_loss:0.013, val_acc:0.986]
Epoch [114/120    avg_loss:0.012, val_acc:0.986]
Epoch [115/120    avg_loss:0.013, val_acc:0.988]
Epoch [116/120    avg_loss:0.013, val_acc:0.988]
Epoch [117/120    avg_loss:0.013, val_acc:0.988]
Epoch [118/120    avg_loss:0.011, val_acc:0.988]
Epoch [119/120    avg_loss:0.014, val_acc:0.986]
Epoch [120/120    avg_loss:0.012, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1239    4    0    0    0    0    0    0   21   18    3    0
     0    0    0]
 [   0    0    4  731    1    4    0    0    0    3    0    0    2    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   13    0    0    4    0
     0    0    0]
 [   0    0    7   84    0    3    0    0    0    0  775    6    0    0
     0    0    0]
 [   0    0   11    0    0    2    6    0    0    0    9 2179    1    2
     0    0    0]
 [   0    0    0   21   17    0    0    0    0    0    4    0  486    0
     1    1    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   12    0    0    0    0    3    1    0    0
  1123    0    0]
 [   0    0    0    0    0    0    5    0    0    1    0    0    0    0
   120  221    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
95.67479674796748

F1 scores:
[       nan 0.95238095 0.97329144 0.92065491 0.95945946 0.97181511
 0.99169811 1.         0.99649942 0.72222222 0.91824645 0.98731309
 0.94094869 0.98930481 0.94132439 0.77680141 0.95857988]

Kappa:
0.9506551158349128
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff48f156908>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.639, val_acc:0.435]
Epoch [2/120    avg_loss:2.266, val_acc:0.428]
Epoch [3/120    avg_loss:2.054, val_acc:0.510]
Epoch [4/120    avg_loss:1.906, val_acc:0.584]
Epoch [5/120    avg_loss:1.732, val_acc:0.606]
Epoch [6/120    avg_loss:1.584, val_acc:0.621]
Epoch [7/120    avg_loss:1.481, val_acc:0.670]
Epoch [8/120    avg_loss:1.302, val_acc:0.685]
Epoch [9/120    avg_loss:1.160, val_acc:0.698]
Epoch [10/120    avg_loss:1.039, val_acc:0.704]
Epoch [11/120    avg_loss:0.912, val_acc:0.738]
Epoch [12/120    avg_loss:0.806, val_acc:0.795]
Epoch [13/120    avg_loss:0.806, val_acc:0.795]
Epoch [14/120    avg_loss:0.718, val_acc:0.774]
Epoch [15/120    avg_loss:0.599, val_acc:0.674]
Epoch [16/120    avg_loss:0.619, val_acc:0.810]
Epoch [17/120    avg_loss:0.560, val_acc:0.805]
Epoch [18/120    avg_loss:0.490, val_acc:0.828]
Epoch [19/120    avg_loss:0.465, val_acc:0.794]
Epoch [20/120    avg_loss:0.469, val_acc:0.851]
Epoch [21/120    avg_loss:0.404, val_acc:0.849]
Epoch [22/120    avg_loss:0.368, val_acc:0.837]
Epoch [23/120    avg_loss:0.375, val_acc:0.848]
Epoch [24/120    avg_loss:0.390, val_acc:0.825]
Epoch [25/120    avg_loss:0.313, val_acc:0.843]
Epoch [26/120    avg_loss:0.307, val_acc:0.882]
Epoch [27/120    avg_loss:0.246, val_acc:0.893]
Epoch [28/120    avg_loss:0.262, val_acc:0.865]
Epoch [29/120    avg_loss:0.323, val_acc:0.879]
Epoch [30/120    avg_loss:0.330, val_acc:0.873]
Epoch [31/120    avg_loss:0.320, val_acc:0.846]
Epoch [32/120    avg_loss:0.280, val_acc:0.859]
Epoch [33/120    avg_loss:0.245, val_acc:0.895]
Epoch [34/120    avg_loss:0.247, val_acc:0.917]
Epoch [35/120    avg_loss:0.188, val_acc:0.916]
Epoch [36/120    avg_loss:0.178, val_acc:0.911]
Epoch [37/120    avg_loss:0.141, val_acc:0.916]
Epoch [38/120    avg_loss:0.210, val_acc:0.899]
Epoch [39/120    avg_loss:0.205, val_acc:0.917]
Epoch [40/120    avg_loss:0.143, val_acc:0.932]
Epoch [41/120    avg_loss:0.122, val_acc:0.927]
Epoch [42/120    avg_loss:0.138, val_acc:0.929]
Epoch [43/120    avg_loss:0.137, val_acc:0.930]
Epoch [44/120    avg_loss:0.136, val_acc:0.901]
Epoch [45/120    avg_loss:0.177, val_acc:0.917]
Epoch [46/120    avg_loss:0.194, val_acc:0.898]
Epoch [47/120    avg_loss:0.141, val_acc:0.932]
Epoch [48/120    avg_loss:0.115, val_acc:0.928]
Epoch [49/120    avg_loss:0.146, val_acc:0.939]
Epoch [50/120    avg_loss:0.125, val_acc:0.938]
Epoch [51/120    avg_loss:0.096, val_acc:0.931]
Epoch [52/120    avg_loss:0.104, val_acc:0.931]
Epoch [53/120    avg_loss:0.107, val_acc:0.929]
Epoch [54/120    avg_loss:0.101, val_acc:0.942]
Epoch [55/120    avg_loss:0.084, val_acc:0.945]
Epoch [56/120    avg_loss:0.075, val_acc:0.950]
Epoch [57/120    avg_loss:0.085, val_acc:0.948]
Epoch [58/120    avg_loss:0.078, val_acc:0.936]
Epoch [59/120    avg_loss:0.182, val_acc:0.924]
Epoch [60/120    avg_loss:0.089, val_acc:0.933]
Epoch [61/120    avg_loss:0.118, val_acc:0.936]
Epoch [62/120    avg_loss:0.081, val_acc:0.938]
Epoch [63/120    avg_loss:0.059, val_acc:0.945]
Epoch [64/120    avg_loss:0.055, val_acc:0.953]
Epoch [65/120    avg_loss:0.062, val_acc:0.942]
Epoch [66/120    avg_loss:0.065, val_acc:0.947]
Epoch [67/120    avg_loss:0.052, val_acc:0.954]
Epoch [68/120    avg_loss:0.050, val_acc:0.950]
Epoch [69/120    avg_loss:0.060, val_acc:0.955]
Epoch [70/120    avg_loss:0.065, val_acc:0.944]
Epoch [71/120    avg_loss:0.067, val_acc:0.953]
Epoch [72/120    avg_loss:0.053, val_acc:0.940]
Epoch [73/120    avg_loss:0.057, val_acc:0.952]
Epoch [74/120    avg_loss:0.076, val_acc:0.962]
Epoch [75/120    avg_loss:0.047, val_acc:0.948]
Epoch [76/120    avg_loss:0.144, val_acc:0.939]
Epoch [77/120    avg_loss:0.066, val_acc:0.957]
Epoch [78/120    avg_loss:0.068, val_acc:0.954]
Epoch [79/120    avg_loss:0.045, val_acc:0.962]
Epoch [80/120    avg_loss:0.052, val_acc:0.956]
Epoch [81/120    avg_loss:0.043, val_acc:0.962]
Epoch [82/120    avg_loss:0.057, val_acc:0.957]
Epoch [83/120    avg_loss:0.038, val_acc:0.964]
Epoch [84/120    avg_loss:0.030, val_acc:0.964]
Epoch [85/120    avg_loss:0.025, val_acc:0.968]
Epoch [86/120    avg_loss:0.038, val_acc:0.965]
Epoch [87/120    avg_loss:0.028, val_acc:0.964]
Epoch [88/120    avg_loss:0.029, val_acc:0.969]
Epoch [89/120    avg_loss:0.042, val_acc:0.965]
Epoch [90/120    avg_loss:0.053, val_acc:0.945]
Epoch [91/120    avg_loss:0.057, val_acc:0.948]
Epoch [92/120    avg_loss:0.041, val_acc:0.964]
Epoch [93/120    avg_loss:0.035, val_acc:0.970]
Epoch [94/120    avg_loss:0.052, val_acc:0.947]
Epoch [95/120    avg_loss:0.086, val_acc:0.965]
Epoch [96/120    avg_loss:0.055, val_acc:0.951]
Epoch [97/120    avg_loss:0.056, val_acc:0.965]
Epoch [98/120    avg_loss:0.028, val_acc:0.964]
Epoch [99/120    avg_loss:0.029, val_acc:0.973]
Epoch [100/120    avg_loss:0.131, val_acc:0.943]
Epoch [101/120    avg_loss:0.073, val_acc:0.922]
Epoch [102/120    avg_loss:0.075, val_acc:0.946]
Epoch [103/120    avg_loss:0.045, val_acc:0.945]
Epoch [104/120    avg_loss:0.040, val_acc:0.959]
Epoch [105/120    avg_loss:0.039, val_acc:0.959]
Epoch [106/120    avg_loss:0.032, val_acc:0.956]
Epoch [107/120    avg_loss:0.043, val_acc:0.965]
Epoch [108/120    avg_loss:0.047, val_acc:0.966]
Epoch [109/120    avg_loss:0.054, val_acc:0.959]
Epoch [110/120    avg_loss:0.043, val_acc:0.965]
Epoch [111/120    avg_loss:0.030, val_acc:0.963]
Epoch [112/120    avg_loss:0.049, val_acc:0.963]
Epoch [113/120    avg_loss:0.045, val_acc:0.970]
Epoch [114/120    avg_loss:0.031, val_acc:0.975]
Epoch [115/120    avg_loss:0.018, val_acc:0.975]
Epoch [116/120    avg_loss:0.027, val_acc:0.976]
Epoch [117/120    avg_loss:0.016, val_acc:0.977]
Epoch [118/120    avg_loss:0.025, val_acc:0.975]
Epoch [119/120    avg_loss:0.017, val_acc:0.977]
Epoch [120/120    avg_loss:0.018, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    9 1230    4    7    0    3    0    0    0    5   24    2    0
     0    1    0]
 [   0    0    0  724    0   16    0    0    0    5    0    0    1    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    6    0    4    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   15    0    0    2    0
     0    0    0]
 [   0    0   18   90    0    7    0    0    0    0  743    0    3    0
     2   12    0]
 [   0    0    6    0    0    3    8    0    0    0   11 2171    7    4
     0    0    0]
 [   0    0    0    8    0    6    0    0    0    0   10   22  483    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    1    0    0
  1132    2    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   104  243    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.28455284552845

F1 scores:
[       nan 0.83516484 0.96888539 0.91994917 0.98383372 0.94938133
 0.99093656 0.89285714 0.995338   0.71428571 0.90060606 0.98035674
 0.93514037 0.98666667 0.9512605  0.80330579 0.96511628]

Kappa:
0.9462134828032996
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbea25508d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.608, val_acc:0.456]
Epoch [2/120    avg_loss:2.217, val_acc:0.498]
Epoch [3/120    avg_loss:2.010, val_acc:0.537]
Epoch [4/120    avg_loss:1.843, val_acc:0.594]
Epoch [5/120    avg_loss:1.680, val_acc:0.615]
Epoch [6/120    avg_loss:1.588, val_acc:0.634]
Epoch [7/120    avg_loss:1.430, val_acc:0.676]
Epoch [8/120    avg_loss:1.271, val_acc:0.734]
Epoch [9/120    avg_loss:1.106, val_acc:0.714]
Epoch [10/120    avg_loss:0.915, val_acc:0.782]
Epoch [11/120    avg_loss:0.887, val_acc:0.776]
Epoch [12/120    avg_loss:0.745, val_acc:0.798]
Epoch [13/120    avg_loss:0.743, val_acc:0.791]
Epoch [14/120    avg_loss:0.617, val_acc:0.796]
Epoch [15/120    avg_loss:0.641, val_acc:0.806]
Epoch [16/120    avg_loss:0.569, val_acc:0.825]
Epoch [17/120    avg_loss:0.593, val_acc:0.819]
Epoch [18/120    avg_loss:0.524, val_acc:0.798]
Epoch [19/120    avg_loss:0.455, val_acc:0.843]
Epoch [20/120    avg_loss:0.435, val_acc:0.845]
Epoch [21/120    avg_loss:0.395, val_acc:0.881]
Epoch [22/120    avg_loss:0.318, val_acc:0.862]
Epoch [23/120    avg_loss:0.352, val_acc:0.865]
Epoch [24/120    avg_loss:0.472, val_acc:0.856]
Epoch [25/120    avg_loss:0.362, val_acc:0.875]
Epoch [26/120    avg_loss:0.336, val_acc:0.877]
Epoch [27/120    avg_loss:0.314, val_acc:0.891]
Epoch [28/120    avg_loss:0.317, val_acc:0.896]
Epoch [29/120    avg_loss:0.257, val_acc:0.886]
Epoch [30/120    avg_loss:0.276, val_acc:0.906]
Epoch [31/120    avg_loss:0.213, val_acc:0.919]
Epoch [32/120    avg_loss:0.235, val_acc:0.935]
Epoch [33/120    avg_loss:0.254, val_acc:0.904]
Epoch [34/120    avg_loss:0.198, val_acc:0.931]
Epoch [35/120    avg_loss:0.214, val_acc:0.938]
Epoch [36/120    avg_loss:0.186, val_acc:0.917]
Epoch [37/120    avg_loss:0.177, val_acc:0.924]
Epoch [38/120    avg_loss:0.179, val_acc:0.929]
Epoch [39/120    avg_loss:0.166, val_acc:0.927]
Epoch [40/120    avg_loss:0.157, val_acc:0.932]
Epoch [41/120    avg_loss:0.132, val_acc:0.946]
Epoch [42/120    avg_loss:0.146, val_acc:0.929]
Epoch [43/120    avg_loss:0.120, val_acc:0.945]
Epoch [44/120    avg_loss:0.130, val_acc:0.946]
Epoch [45/120    avg_loss:0.118, val_acc:0.954]
Epoch [46/120    avg_loss:0.105, val_acc:0.945]
Epoch [47/120    avg_loss:0.136, val_acc:0.954]
Epoch [48/120    avg_loss:0.162, val_acc:0.949]
Epoch [49/120    avg_loss:0.159, val_acc:0.913]
Epoch [50/120    avg_loss:0.153, val_acc:0.948]
Epoch [51/120    avg_loss:0.132, val_acc:0.942]
Epoch [52/120    avg_loss:0.115, val_acc:0.943]
Epoch [53/120    avg_loss:0.096, val_acc:0.961]
Epoch [54/120    avg_loss:0.083, val_acc:0.936]
Epoch [55/120    avg_loss:0.095, val_acc:0.948]
Epoch [56/120    avg_loss:0.074, val_acc:0.970]
Epoch [57/120    avg_loss:0.084, val_acc:0.924]
Epoch [58/120    avg_loss:0.106, val_acc:0.952]
Epoch [59/120    avg_loss:0.150, val_acc:0.917]
Epoch [60/120    avg_loss:0.169, val_acc:0.917]
Epoch [61/120    avg_loss:0.142, val_acc:0.931]
Epoch [62/120    avg_loss:0.091, val_acc:0.956]
Epoch [63/120    avg_loss:0.092, val_acc:0.950]
Epoch [64/120    avg_loss:0.085, val_acc:0.953]
Epoch [65/120    avg_loss:0.087, val_acc:0.940]
Epoch [66/120    avg_loss:0.079, val_acc:0.959]
Epoch [67/120    avg_loss:0.069, val_acc:0.969]
Epoch [68/120    avg_loss:0.069, val_acc:0.969]
Epoch [69/120    avg_loss:0.131, val_acc:0.936]
Epoch [70/120    avg_loss:0.086, val_acc:0.961]
Epoch [71/120    avg_loss:0.069, val_acc:0.960]
Epoch [72/120    avg_loss:0.057, val_acc:0.965]
Epoch [73/120    avg_loss:0.065, val_acc:0.969]
Epoch [74/120    avg_loss:0.059, val_acc:0.971]
Epoch [75/120    avg_loss:0.050, val_acc:0.969]
Epoch [76/120    avg_loss:0.049, val_acc:0.971]
Epoch [77/120    avg_loss:0.040, val_acc:0.971]
Epoch [78/120    avg_loss:0.047, val_acc:0.969]
Epoch [79/120    avg_loss:0.040, val_acc:0.973]
Epoch [80/120    avg_loss:0.040, val_acc:0.977]
Epoch [81/120    avg_loss:0.039, val_acc:0.973]
Epoch [82/120    avg_loss:0.043, val_acc:0.971]
Epoch [83/120    avg_loss:0.041, val_acc:0.972]
Epoch [84/120    avg_loss:0.041, val_acc:0.973]
Epoch [85/120    avg_loss:0.038, val_acc:0.975]
Epoch [86/120    avg_loss:0.038, val_acc:0.975]
Epoch [87/120    avg_loss:0.039, val_acc:0.978]
Epoch [88/120    avg_loss:0.041, val_acc:0.974]
Epoch [89/120    avg_loss:0.037, val_acc:0.973]
Epoch [90/120    avg_loss:0.041, val_acc:0.977]
Epoch [91/120    avg_loss:0.040, val_acc:0.973]
Epoch [92/120    avg_loss:0.041, val_acc:0.975]
Epoch [93/120    avg_loss:0.038, val_acc:0.975]
Epoch [94/120    avg_loss:0.039, val_acc:0.974]
Epoch [95/120    avg_loss:0.034, val_acc:0.977]
Epoch [96/120    avg_loss:0.035, val_acc:0.978]
Epoch [97/120    avg_loss:0.032, val_acc:0.979]
Epoch [98/120    avg_loss:0.034, val_acc:0.978]
Epoch [99/120    avg_loss:0.030, val_acc:0.975]
Epoch [100/120    avg_loss:0.034, val_acc:0.979]
Epoch [101/120    avg_loss:0.038, val_acc:0.977]
Epoch [102/120    avg_loss:0.033, val_acc:0.978]
Epoch [103/120    avg_loss:0.034, val_acc:0.977]
Epoch [104/120    avg_loss:0.038, val_acc:0.981]
Epoch [105/120    avg_loss:0.035, val_acc:0.979]
Epoch [106/120    avg_loss:0.037, val_acc:0.981]
Epoch [107/120    avg_loss:0.030, val_acc:0.977]
Epoch [108/120    avg_loss:0.031, val_acc:0.975]
Epoch [109/120    avg_loss:0.030, val_acc:0.979]
Epoch [110/120    avg_loss:0.038, val_acc:0.978]
Epoch [111/120    avg_loss:0.038, val_acc:0.977]
Epoch [112/120    avg_loss:0.032, val_acc:0.978]
Epoch [113/120    avg_loss:0.033, val_acc:0.982]
Epoch [114/120    avg_loss:0.029, val_acc:0.981]
Epoch [115/120    avg_loss:0.027, val_acc:0.980]
Epoch [116/120    avg_loss:0.028, val_acc:0.981]
Epoch [117/120    avg_loss:0.029, val_acc:0.979]
Epoch [118/120    avg_loss:0.037, val_acc:0.979]
Epoch [119/120    avg_loss:0.034, val_acc:0.981]
Epoch [120/120    avg_loss:0.031, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1228    1    0    0    7    0    0    0    9   36    4    0
     0    0    0]
 [   0    0    2  707    4    3    0    0    0    7    0    0   20    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    1    0    4    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0    9   87    0    6    0    0    0    0  752   13    2    0
     3    3    0]
 [   0    0   11    0    0    1   13    0    2    0    8 2164    8    2
     1    0    0]
 [   0    0    0    1    7    0    0    0    0    0   13    3  501    0
     0    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   11    0    0    3    0    3    0    0    0
  1122    0    0]
 [   0    0    0    0    0    0    0    0    0    2    0    0    1    0
   118  226    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.08943089430895

F1 scores:
[       nan 0.96202532 0.96883629 0.91639663 0.97482838 0.96598639
 0.98348348 0.98039216 0.99421965 0.68085106 0.90438966 0.97763723
 0.93470149 0.98404255 0.93969849 0.78472222 0.94915254]

Kappa:
0.9439843170430352
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f94896b0828>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.635, val_acc:0.356]
Epoch [2/120    avg_loss:2.244, val_acc:0.523]
Epoch [3/120    avg_loss:2.030, val_acc:0.490]
Epoch [4/120    avg_loss:1.798, val_acc:0.605]
Epoch [5/120    avg_loss:1.624, val_acc:0.621]
Epoch [6/120    avg_loss:1.457, val_acc:0.637]
Epoch [7/120    avg_loss:1.234, val_acc:0.691]
Epoch [8/120    avg_loss:1.100, val_acc:0.690]
Epoch [9/120    avg_loss:0.962, val_acc:0.714]
Epoch [10/120    avg_loss:0.838, val_acc:0.752]
Epoch [11/120    avg_loss:0.755, val_acc:0.747]
Epoch [12/120    avg_loss:0.741, val_acc:0.786]
Epoch [13/120    avg_loss:0.671, val_acc:0.772]
Epoch [14/120    avg_loss:0.608, val_acc:0.785]
Epoch [15/120    avg_loss:0.534, val_acc:0.809]
Epoch [16/120    avg_loss:0.524, val_acc:0.828]
Epoch [17/120    avg_loss:0.478, val_acc:0.812]
Epoch [18/120    avg_loss:0.478, val_acc:0.818]
Epoch [19/120    avg_loss:0.400, val_acc:0.864]
Epoch [20/120    avg_loss:0.389, val_acc:0.835]
Epoch [21/120    avg_loss:0.412, val_acc:0.857]
Epoch [22/120    avg_loss:0.383, val_acc:0.852]
Epoch [23/120    avg_loss:0.394, val_acc:0.820]
Epoch [24/120    avg_loss:0.337, val_acc:0.842]
Epoch [25/120    avg_loss:0.285, val_acc:0.869]
Epoch [26/120    avg_loss:0.302, val_acc:0.895]
Epoch [27/120    avg_loss:0.358, val_acc:0.860]
Epoch [28/120    avg_loss:0.260, val_acc:0.887]
Epoch [29/120    avg_loss:0.333, val_acc:0.852]
Epoch [30/120    avg_loss:0.312, val_acc:0.894]
Epoch [31/120    avg_loss:0.249, val_acc:0.902]
Epoch [32/120    avg_loss:0.235, val_acc:0.859]
Epoch [33/120    avg_loss:0.221, val_acc:0.902]
Epoch [34/120    avg_loss:0.188, val_acc:0.913]
Epoch [35/120    avg_loss:0.242, val_acc:0.894]
Epoch [36/120    avg_loss:0.237, val_acc:0.895]
Epoch [37/120    avg_loss:0.210, val_acc:0.902]
Epoch [38/120    avg_loss:0.178, val_acc:0.913]
Epoch [39/120    avg_loss:0.186, val_acc:0.911]
Epoch [40/120    avg_loss:0.176, val_acc:0.908]
Epoch [41/120    avg_loss:0.139, val_acc:0.907]
Epoch [42/120    avg_loss:0.129, val_acc:0.913]
Epoch [43/120    avg_loss:0.200, val_acc:0.858]
Epoch [44/120    avg_loss:0.269, val_acc:0.866]
Epoch [45/120    avg_loss:0.369, val_acc:0.871]
Epoch [46/120    avg_loss:0.188, val_acc:0.886]
Epoch [47/120    avg_loss:0.196, val_acc:0.900]
Epoch [48/120    avg_loss:0.148, val_acc:0.931]
Epoch [49/120    avg_loss:0.109, val_acc:0.951]
Epoch [50/120    avg_loss:0.101, val_acc:0.945]
Epoch [51/120    avg_loss:0.102, val_acc:0.950]
Epoch [52/120    avg_loss:0.125, val_acc:0.940]
Epoch [53/120    avg_loss:0.097, val_acc:0.948]
Epoch [54/120    avg_loss:0.142, val_acc:0.923]
Epoch [55/120    avg_loss:0.126, val_acc:0.956]
Epoch [56/120    avg_loss:0.115, val_acc:0.952]
Epoch [57/120    avg_loss:0.120, val_acc:0.935]
Epoch [58/120    avg_loss:0.099, val_acc:0.944]
Epoch [59/120    avg_loss:0.097, val_acc:0.942]
Epoch [60/120    avg_loss:0.067, val_acc:0.951]
Epoch [61/120    avg_loss:0.086, val_acc:0.946]
Epoch [62/120    avg_loss:0.070, val_acc:0.950]
Epoch [63/120    avg_loss:0.052, val_acc:0.964]
Epoch [64/120    avg_loss:0.071, val_acc:0.932]
Epoch [65/120    avg_loss:0.093, val_acc:0.946]
Epoch [66/120    avg_loss:0.093, val_acc:0.958]
Epoch [67/120    avg_loss:0.051, val_acc:0.958]
Epoch [68/120    avg_loss:0.073, val_acc:0.951]
Epoch [69/120    avg_loss:0.062, val_acc:0.960]
Epoch [70/120    avg_loss:0.056, val_acc:0.954]
Epoch [71/120    avg_loss:0.046, val_acc:0.963]
Epoch [72/120    avg_loss:0.069, val_acc:0.945]
Epoch [73/120    avg_loss:0.048, val_acc:0.956]
Epoch [74/120    avg_loss:0.049, val_acc:0.960]
Epoch [75/120    avg_loss:0.065, val_acc:0.944]
Epoch [76/120    avg_loss:0.060, val_acc:0.961]
Epoch [77/120    avg_loss:0.046, val_acc:0.968]
Epoch [78/120    avg_loss:0.032, val_acc:0.970]
Epoch [79/120    avg_loss:0.036, val_acc:0.968]
Epoch [80/120    avg_loss:0.028, val_acc:0.972]
Epoch [81/120    avg_loss:0.029, val_acc:0.972]
Epoch [82/120    avg_loss:0.032, val_acc:0.972]
Epoch [83/120    avg_loss:0.031, val_acc:0.974]
Epoch [84/120    avg_loss:0.039, val_acc:0.972]
Epoch [85/120    avg_loss:0.027, val_acc:0.975]
Epoch [86/120    avg_loss:0.030, val_acc:0.978]
Epoch [87/120    avg_loss:0.031, val_acc:0.974]
Epoch [88/120    avg_loss:0.036, val_acc:0.974]
Epoch [89/120    avg_loss:0.027, val_acc:0.975]
Epoch [90/120    avg_loss:0.026, val_acc:0.977]
Epoch [91/120    avg_loss:0.023, val_acc:0.975]
Epoch [92/120    avg_loss:0.032, val_acc:0.974]
Epoch [93/120    avg_loss:0.025, val_acc:0.975]
Epoch [94/120    avg_loss:0.024, val_acc:0.975]
Epoch [95/120    avg_loss:0.024, val_acc:0.975]
Epoch [96/120    avg_loss:0.025, val_acc:0.977]
Epoch [97/120    avg_loss:0.024, val_acc:0.975]
Epoch [98/120    avg_loss:0.060, val_acc:0.975]
Epoch [99/120    avg_loss:0.035, val_acc:0.973]
Epoch [100/120    avg_loss:0.030, val_acc:0.974]
Epoch [101/120    avg_loss:0.028, val_acc:0.974]
Epoch [102/120    avg_loss:0.027, val_acc:0.974]
Epoch [103/120    avg_loss:0.025, val_acc:0.974]
Epoch [104/120    avg_loss:0.032, val_acc:0.974]
Epoch [105/120    avg_loss:0.025, val_acc:0.974]
Epoch [106/120    avg_loss:0.021, val_acc:0.974]
Epoch [107/120    avg_loss:0.027, val_acc:0.974]
Epoch [108/120    avg_loss:0.030, val_acc:0.975]
Epoch [109/120    avg_loss:0.022, val_acc:0.975]
Epoch [110/120    avg_loss:0.026, val_acc:0.974]
Epoch [111/120    avg_loss:0.021, val_acc:0.975]
Epoch [112/120    avg_loss:0.028, val_acc:0.975]
Epoch [113/120    avg_loss:0.025, val_acc:0.975]
Epoch [114/120    avg_loss:0.026, val_acc:0.975]
Epoch [115/120    avg_loss:0.024, val_acc:0.975]
Epoch [116/120    avg_loss:0.023, val_acc:0.975]
Epoch [117/120    avg_loss:0.024, val_acc:0.975]
Epoch [118/120    avg_loss:0.025, val_acc:0.975]
Epoch [119/120    avg_loss:0.022, val_acc:0.975]
Epoch [120/120    avg_loss:0.028, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1218    6    0    0    2    0    0    0   17   42    0    0
     0    0    0]
 [   0    0    0  719    1    2    0    0    0    6    0    3   16    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    7    0    0    0    0    0    0  423    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   15    0    0    1    0
     0    0    0]
 [   0    0    5   90    0    7    0    0    0    0  770    0    0    0
     1    2    0]
 [   0    0    7    0    0    5   11    0    3    0   12 2166    4    2
     0    0    0]
 [   0    0    0    1    5   18    0    0    0    0    1   29  465    0
     0    0   15]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    2    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0   12    0    0    8    0    0    0    0
   120  207    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.82926829268293

F1 scores:
[       nan 0.90909091 0.9682035  0.92002559 0.98611111 0.95875139
 0.97681376 1.         0.98601399 0.6122449  0.9183065  0.97326443
 0.91176471 0.99462366 0.9454394  0.74460432 0.91803279]

Kappa:
0.9409835372995055
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f376846f860>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.683, val_acc:0.312]
Epoch [2/120    avg_loss:2.352, val_acc:0.446]
Epoch [3/120    avg_loss:2.161, val_acc:0.518]
Epoch [4/120    avg_loss:1.944, val_acc:0.546]
Epoch [5/120    avg_loss:1.767, val_acc:0.562]
Epoch [6/120    avg_loss:1.623, val_acc:0.624]
Epoch [7/120    avg_loss:1.461, val_acc:0.610]
Epoch [8/120    avg_loss:1.306, val_acc:0.615]
Epoch [9/120    avg_loss:1.201, val_acc:0.659]
Epoch [10/120    avg_loss:1.065, val_acc:0.648]
Epoch [11/120    avg_loss:1.049, val_acc:0.653]
Epoch [12/120    avg_loss:0.917, val_acc:0.716]
Epoch [13/120    avg_loss:0.795, val_acc:0.733]
Epoch [14/120    avg_loss:0.762, val_acc:0.800]
Epoch [15/120    avg_loss:0.653, val_acc:0.799]
Epoch [16/120    avg_loss:0.581, val_acc:0.786]
Epoch [17/120    avg_loss:0.525, val_acc:0.823]
Epoch [18/120    avg_loss:0.556, val_acc:0.797]
Epoch [19/120    avg_loss:0.464, val_acc:0.842]
Epoch [20/120    avg_loss:0.418, val_acc:0.842]
Epoch [21/120    avg_loss:0.406, val_acc:0.887]
Epoch [22/120    avg_loss:0.299, val_acc:0.845]
Epoch [23/120    avg_loss:0.321, val_acc:0.857]
Epoch [24/120    avg_loss:0.245, val_acc:0.887]
Epoch [25/120    avg_loss:0.274, val_acc:0.878]
Epoch [26/120    avg_loss:0.229, val_acc:0.911]
Epoch [27/120    avg_loss:0.204, val_acc:0.893]
Epoch [28/120    avg_loss:0.210, val_acc:0.892]
Epoch [29/120    avg_loss:0.162, val_acc:0.909]
Epoch [30/120    avg_loss:0.174, val_acc:0.892]
Epoch [31/120    avg_loss:0.183, val_acc:0.906]
Epoch [32/120    avg_loss:0.178, val_acc:0.899]
Epoch [33/120    avg_loss:0.129, val_acc:0.936]
Epoch [34/120    avg_loss:0.168, val_acc:0.914]
Epoch [35/120    avg_loss:0.160, val_acc:0.914]
Epoch [36/120    avg_loss:0.153, val_acc:0.926]
Epoch [37/120    avg_loss:0.132, val_acc:0.934]
Epoch [38/120    avg_loss:0.111, val_acc:0.918]
Epoch [39/120    avg_loss:0.139, val_acc:0.908]
Epoch [40/120    avg_loss:0.137, val_acc:0.933]
Epoch [41/120    avg_loss:0.127, val_acc:0.900]
Epoch [42/120    avg_loss:0.095, val_acc:0.919]
Epoch [43/120    avg_loss:0.113, val_acc:0.952]
Epoch [44/120    avg_loss:0.104, val_acc:0.941]
Epoch [45/120    avg_loss:0.064, val_acc:0.960]
Epoch [46/120    avg_loss:0.070, val_acc:0.934]
Epoch [47/120    avg_loss:0.083, val_acc:0.944]
Epoch [48/120    avg_loss:0.077, val_acc:0.959]
Epoch [49/120    avg_loss:0.086, val_acc:0.934]
Epoch [50/120    avg_loss:0.096, val_acc:0.929]
Epoch [51/120    avg_loss:0.078, val_acc:0.933]
Epoch [52/120    avg_loss:0.072, val_acc:0.943]
Epoch [53/120    avg_loss:0.064, val_acc:0.953]
Epoch [54/120    avg_loss:0.071, val_acc:0.945]
Epoch [55/120    avg_loss:0.051, val_acc:0.927]
Epoch [56/120    avg_loss:0.044, val_acc:0.951]
Epoch [57/120    avg_loss:0.041, val_acc:0.955]
Epoch [58/120    avg_loss:0.049, val_acc:0.951]
Epoch [59/120    avg_loss:0.033, val_acc:0.959]
Epoch [60/120    avg_loss:0.036, val_acc:0.956]
Epoch [61/120    avg_loss:0.028, val_acc:0.957]
Epoch [62/120    avg_loss:0.029, val_acc:0.965]
Epoch [63/120    avg_loss:0.031, val_acc:0.963]
Epoch [64/120    avg_loss:0.022, val_acc:0.965]
Epoch [65/120    avg_loss:0.028, val_acc:0.961]
Epoch [66/120    avg_loss:0.028, val_acc:0.965]
Epoch [67/120    avg_loss:0.022, val_acc:0.965]
Epoch [68/120    avg_loss:0.027, val_acc:0.963]
Epoch [69/120    avg_loss:0.025, val_acc:0.961]
Epoch [70/120    avg_loss:0.024, val_acc:0.963]
Epoch [71/120    avg_loss:0.022, val_acc:0.963]
Epoch [72/120    avg_loss:0.029, val_acc:0.964]
Epoch [73/120    avg_loss:0.027, val_acc:0.963]
Epoch [74/120    avg_loss:0.024, val_acc:0.964]
Epoch [75/120    avg_loss:0.023, val_acc:0.960]
Epoch [76/120    avg_loss:0.026, val_acc:0.963]
Epoch [77/120    avg_loss:0.026, val_acc:0.963]
Epoch [78/120    avg_loss:0.019, val_acc:0.968]
Epoch [79/120    avg_loss:0.020, val_acc:0.964]
Epoch [80/120    avg_loss:0.026, val_acc:0.966]
Epoch [81/120    avg_loss:0.021, val_acc:0.967]
Epoch [82/120    avg_loss:0.018, val_acc:0.967]
Epoch [83/120    avg_loss:0.022, val_acc:0.965]
Epoch [84/120    avg_loss:0.019, val_acc:0.964]
Epoch [85/120    avg_loss:0.018, val_acc:0.964]
Epoch [86/120    avg_loss:0.022, val_acc:0.965]
Epoch [87/120    avg_loss:0.019, val_acc:0.965]
Epoch [88/120    avg_loss:0.018, val_acc:0.964]
Epoch [89/120    avg_loss:0.018, val_acc:0.964]
Epoch [90/120    avg_loss:0.024, val_acc:0.967]
Epoch [91/120    avg_loss:0.022, val_acc:0.967]
Epoch [92/120    avg_loss:0.017, val_acc:0.967]
Epoch [93/120    avg_loss:0.020, val_acc:0.966]
Epoch [94/120    avg_loss:0.020, val_acc:0.966]
Epoch [95/120    avg_loss:0.022, val_acc:0.967]
Epoch [96/120    avg_loss:0.019, val_acc:0.967]
Epoch [97/120    avg_loss:0.020, val_acc:0.967]
Epoch [98/120    avg_loss:0.018, val_acc:0.967]
Epoch [99/120    avg_loss:0.020, val_acc:0.967]
Epoch [100/120    avg_loss:0.018, val_acc:0.966]
Epoch [101/120    avg_loss:0.019, val_acc:0.967]
Epoch [102/120    avg_loss:0.022, val_acc:0.968]
Epoch [103/120    avg_loss:0.018, val_acc:0.968]
Epoch [104/120    avg_loss:0.017, val_acc:0.967]
Epoch [105/120    avg_loss:0.020, val_acc:0.966]
Epoch [106/120    avg_loss:0.022, val_acc:0.967]
Epoch [107/120    avg_loss:0.021, val_acc:0.967]
Epoch [108/120    avg_loss:0.019, val_acc:0.966]
Epoch [109/120    avg_loss:0.018, val_acc:0.966]
Epoch [110/120    avg_loss:0.021, val_acc:0.968]
Epoch [111/120    avg_loss:0.016, val_acc:0.967]
Epoch [112/120    avg_loss:0.018, val_acc:0.967]
Epoch [113/120    avg_loss:0.019, val_acc:0.967]
Epoch [114/120    avg_loss:0.015, val_acc:0.967]
Epoch [115/120    avg_loss:0.021, val_acc:0.967]
Epoch [116/120    avg_loss:0.020, val_acc:0.966]
Epoch [117/120    avg_loss:0.023, val_acc:0.965]
Epoch [118/120    avg_loss:0.016, val_acc:0.966]
Epoch [119/120    avg_loss:0.022, val_acc:0.966]
Epoch [120/120    avg_loss:0.018, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    2    0    0    0    0    0
     0    0    0]
 [   0    0 1259    3    0    0    0    0    0    0    3   19    1    0
     0    0    0]
 [   0    0    0  714    2    0    3    0    0    4    1    2   21    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    1    0    0    0    1    0    0
     5    1    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    1    0    0    0    0  851   15    1    0
     0    0    0]
 [   0    0   17    0    0    0    0    0    0    0   20 2158   12    0
     0    3    0]
 [   0    0    0    5    1    0    0    0    0    0    0    1  525    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  184
     0    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    96  251    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.12737127371274

F1 scores:
[       nan 0.975      0.9805296  0.97208986 0.99300699 0.98957126
 0.9946687  0.98039216 0.99767981 0.9        0.97257143 0.97890678
 0.95890411 0.99728997 0.95318431 0.82026144 0.98224852]

Kappa:
0.9672300868747706
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6ce38f6898>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.702, val_acc:0.381]
Epoch [2/120    avg_loss:2.373, val_acc:0.481]
Epoch [3/120    avg_loss:2.135, val_acc:0.526]
Epoch [4/120    avg_loss:2.011, val_acc:0.541]
Epoch [5/120    avg_loss:1.863, val_acc:0.567]
Epoch [6/120    avg_loss:1.701, val_acc:0.570]
Epoch [7/120    avg_loss:1.548, val_acc:0.602]
Epoch [8/120    avg_loss:1.413, val_acc:0.609]
Epoch [9/120    avg_loss:1.239, val_acc:0.640]
Epoch [10/120    avg_loss:1.130, val_acc:0.615]
Epoch [11/120    avg_loss:1.031, val_acc:0.669]
Epoch [12/120    avg_loss:0.871, val_acc:0.707]
Epoch [13/120    avg_loss:0.762, val_acc:0.760]
Epoch [14/120    avg_loss:0.639, val_acc:0.755]
Epoch [15/120    avg_loss:0.602, val_acc:0.805]
Epoch [16/120    avg_loss:0.516, val_acc:0.816]
Epoch [17/120    avg_loss:0.458, val_acc:0.793]
Epoch [18/120    avg_loss:0.474, val_acc:0.792]
Epoch [19/120    avg_loss:0.365, val_acc:0.819]
Epoch [20/120    avg_loss:0.286, val_acc:0.858]
Epoch [21/120    avg_loss:0.283, val_acc:0.838]
Epoch [22/120    avg_loss:0.260, val_acc:0.867]
Epoch [23/120    avg_loss:0.215, val_acc:0.875]
Epoch [24/120    avg_loss:0.255, val_acc:0.852]
Epoch [25/120    avg_loss:0.237, val_acc:0.889]
Epoch [26/120    avg_loss:0.191, val_acc:0.872]
Epoch [27/120    avg_loss:0.176, val_acc:0.911]
Epoch [28/120    avg_loss:0.186, val_acc:0.903]
Epoch [29/120    avg_loss:0.168, val_acc:0.901]
Epoch [30/120    avg_loss:0.176, val_acc:0.914]
Epoch [31/120    avg_loss:0.143, val_acc:0.917]
Epoch [32/120    avg_loss:0.128, val_acc:0.925]
Epoch [33/120    avg_loss:0.109, val_acc:0.930]
Epoch [34/120    avg_loss:0.105, val_acc:0.934]
Epoch [35/120    avg_loss:0.095, val_acc:0.945]
Epoch [36/120    avg_loss:0.097, val_acc:0.943]
Epoch [37/120    avg_loss:0.091, val_acc:0.938]
Epoch [38/120    avg_loss:0.086, val_acc:0.928]
Epoch [39/120    avg_loss:0.072, val_acc:0.931]
Epoch [40/120    avg_loss:0.103, val_acc:0.914]
Epoch [41/120    avg_loss:0.106, val_acc:0.933]
Epoch [42/120    avg_loss:0.089, val_acc:0.939]
Epoch [43/120    avg_loss:0.087, val_acc:0.928]
Epoch [44/120    avg_loss:0.077, val_acc:0.940]
Epoch [45/120    avg_loss:0.061, val_acc:0.940]
Epoch [46/120    avg_loss:0.056, val_acc:0.949]
Epoch [47/120    avg_loss:0.043, val_acc:0.951]
Epoch [48/120    avg_loss:0.043, val_acc:0.949]
Epoch [49/120    avg_loss:0.054, val_acc:0.938]
Epoch [50/120    avg_loss:0.060, val_acc:0.956]
Epoch [51/120    avg_loss:0.054, val_acc:0.947]
Epoch [52/120    avg_loss:0.087, val_acc:0.938]
Epoch [53/120    avg_loss:0.060, val_acc:0.954]
Epoch [54/120    avg_loss:0.044, val_acc:0.952]
Epoch [55/120    avg_loss:0.035, val_acc:0.943]
Epoch [56/120    avg_loss:0.049, val_acc:0.952]
Epoch [57/120    avg_loss:0.053, val_acc:0.951]
Epoch [58/120    avg_loss:0.051, val_acc:0.952]
Epoch [59/120    avg_loss:0.048, val_acc:0.929]
Epoch [60/120    avg_loss:0.042, val_acc:0.958]
Epoch [61/120    avg_loss:0.030, val_acc:0.958]
Epoch [62/120    avg_loss:0.029, val_acc:0.955]
Epoch [63/120    avg_loss:0.043, val_acc:0.955]
Epoch [64/120    avg_loss:0.048, val_acc:0.965]
Epoch [65/120    avg_loss:0.030, val_acc:0.966]
Epoch [66/120    avg_loss:0.021, val_acc:0.974]
Epoch [67/120    avg_loss:0.023, val_acc:0.971]
Epoch [68/120    avg_loss:0.019, val_acc:0.971]
Epoch [69/120    avg_loss:0.031, val_acc:0.954]
Epoch [70/120    avg_loss:0.024, val_acc:0.967]
Epoch [71/120    avg_loss:0.024, val_acc:0.967]
Epoch [72/120    avg_loss:0.014, val_acc:0.967]
Epoch [73/120    avg_loss:0.023, val_acc:0.972]
Epoch [74/120    avg_loss:0.029, val_acc:0.953]
Epoch [75/120    avg_loss:0.032, val_acc:0.965]
Epoch [76/120    avg_loss:0.020, val_acc:0.961]
Epoch [77/120    avg_loss:0.019, val_acc:0.959]
Epoch [78/120    avg_loss:0.019, val_acc:0.970]
Epoch [79/120    avg_loss:0.016, val_acc:0.970]
Epoch [80/120    avg_loss:0.016, val_acc:0.976]
Epoch [81/120    avg_loss:0.012, val_acc:0.974]
Epoch [82/120    avg_loss:0.015, val_acc:0.974]
Epoch [83/120    avg_loss:0.012, val_acc:0.975]
Epoch [84/120    avg_loss:0.009, val_acc:0.975]
Epoch [85/120    avg_loss:0.012, val_acc:0.976]
Epoch [86/120    avg_loss:0.011, val_acc:0.978]
Epoch [87/120    avg_loss:0.012, val_acc:0.977]
Epoch [88/120    avg_loss:0.011, val_acc:0.978]
Epoch [89/120    avg_loss:0.010, val_acc:0.979]
Epoch [90/120    avg_loss:0.007, val_acc:0.979]
Epoch [91/120    avg_loss:0.009, val_acc:0.978]
Epoch [92/120    avg_loss:0.009, val_acc:0.979]
Epoch [93/120    avg_loss:0.010, val_acc:0.980]
Epoch [94/120    avg_loss:0.008, val_acc:0.981]
Epoch [95/120    avg_loss:0.011, val_acc:0.978]
Epoch [96/120    avg_loss:0.009, val_acc:0.979]
Epoch [97/120    avg_loss:0.007, val_acc:0.979]
Epoch [98/120    avg_loss:0.009, val_acc:0.979]
Epoch [99/120    avg_loss:0.013, val_acc:0.977]
Epoch [100/120    avg_loss:0.008, val_acc:0.977]
Epoch [101/120    avg_loss:0.009, val_acc:0.977]
Epoch [102/120    avg_loss:0.008, val_acc:0.978]
Epoch [103/120    avg_loss:0.009, val_acc:0.978]
Epoch [104/120    avg_loss:0.008, val_acc:0.977]
Epoch [105/120    avg_loss:0.008, val_acc:0.978]
Epoch [106/120    avg_loss:0.009, val_acc:0.978]
Epoch [107/120    avg_loss:0.008, val_acc:0.978]
Epoch [108/120    avg_loss:0.008, val_acc:0.978]
Epoch [109/120    avg_loss:0.008, val_acc:0.978]
Epoch [110/120    avg_loss:0.009, val_acc:0.978]
Epoch [111/120    avg_loss:0.009, val_acc:0.978]
Epoch [112/120    avg_loss:0.011, val_acc:0.978]
Epoch [113/120    avg_loss:0.008, val_acc:0.978]
Epoch [114/120    avg_loss:0.008, val_acc:0.978]
Epoch [115/120    avg_loss:0.008, val_acc:0.978]
Epoch [116/120    avg_loss:0.010, val_acc:0.978]
Epoch [117/120    avg_loss:0.009, val_acc:0.978]
Epoch [118/120    avg_loss:0.009, val_acc:0.978]
Epoch [119/120    avg_loss:0.008, val_acc:0.978]
Epoch [120/120    avg_loss:0.008, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    4    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1259    4    2    0    0    0    0    0    1   15    4    0
     0    0    0]
 [   0    0    0  729    3    0    1    0    0    1    3    8    2    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    1    0    0    0    1    0    0    0  854   18    0    0
     0    1    0]
 [   0    0    6    6    0    0    0    0    0    4   35 2154    5    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1  531    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
  1126   10    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    63  269    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.53929539295393

F1 scores:
[       nan 0.94871795 0.98551859 0.97983871 0.98368298 0.99884925
 0.98271976 0.98039216 0.99883586 0.85       0.96606335 0.97709231
 0.98607242 1.         0.9669386  0.85805423 0.98823529]

Kappa:
0.9719388262056194
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f43b721a8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.711, val_acc:0.470]
Epoch [2/120    avg_loss:2.342, val_acc:0.483]
Epoch [3/120    avg_loss:2.103, val_acc:0.526]
Epoch [4/120    avg_loss:1.894, val_acc:0.537]
Epoch [5/120    avg_loss:1.809, val_acc:0.551]
Epoch [6/120    avg_loss:1.624, val_acc:0.570]
Epoch [7/120    avg_loss:1.434, val_acc:0.604]
Epoch [8/120    avg_loss:1.335, val_acc:0.634]
Epoch [9/120    avg_loss:1.208, val_acc:0.655]
Epoch [10/120    avg_loss:1.080, val_acc:0.668]
Epoch [11/120    avg_loss:0.980, val_acc:0.688]
Epoch [12/120    avg_loss:0.873, val_acc:0.697]
Epoch [13/120    avg_loss:0.809, val_acc:0.728]
Epoch [14/120    avg_loss:0.713, val_acc:0.744]
Epoch [15/120    avg_loss:0.663, val_acc:0.754]
Epoch [16/120    avg_loss:0.597, val_acc:0.768]
Epoch [17/120    avg_loss:0.546, val_acc:0.786]
Epoch [18/120    avg_loss:0.436, val_acc:0.821]
Epoch [19/120    avg_loss:0.413, val_acc:0.825]
Epoch [20/120    avg_loss:0.404, val_acc:0.783]
Epoch [21/120    avg_loss:0.360, val_acc:0.811]
Epoch [22/120    avg_loss:0.289, val_acc:0.838]
Epoch [23/120    avg_loss:0.284, val_acc:0.827]
Epoch [24/120    avg_loss:0.269, val_acc:0.836]
Epoch [25/120    avg_loss:0.261, val_acc:0.843]
Epoch [26/120    avg_loss:0.278, val_acc:0.871]
Epoch [27/120    avg_loss:0.241, val_acc:0.810]
Epoch [28/120    avg_loss:0.230, val_acc:0.844]
Epoch [29/120    avg_loss:0.227, val_acc:0.878]
Epoch [30/120    avg_loss:0.190, val_acc:0.887]
Epoch [31/120    avg_loss:0.151, val_acc:0.890]
Epoch [32/120    avg_loss:0.134, val_acc:0.898]
Epoch [33/120    avg_loss:0.134, val_acc:0.876]
Epoch [34/120    avg_loss:0.145, val_acc:0.882]
Epoch [35/120    avg_loss:0.146, val_acc:0.892]
Epoch [36/120    avg_loss:0.145, val_acc:0.916]
Epoch [37/120    avg_loss:0.129, val_acc:0.903]
Epoch [38/120    avg_loss:0.122, val_acc:0.872]
Epoch [39/120    avg_loss:0.135, val_acc:0.917]
Epoch [40/120    avg_loss:0.164, val_acc:0.872]
Epoch [41/120    avg_loss:0.359, val_acc:0.861]
Epoch [42/120    avg_loss:0.264, val_acc:0.883]
Epoch [43/120    avg_loss:0.131, val_acc:0.897]
Epoch [44/120    avg_loss:0.126, val_acc:0.897]
Epoch [45/120    avg_loss:0.188, val_acc:0.890]
Epoch [46/120    avg_loss:0.172, val_acc:0.899]
Epoch [47/120    avg_loss:0.120, val_acc:0.923]
Epoch [48/120    avg_loss:0.093, val_acc:0.924]
Epoch [49/120    avg_loss:0.094, val_acc:0.926]
Epoch [50/120    avg_loss:0.097, val_acc:0.906]
Epoch [51/120    avg_loss:0.090, val_acc:0.922]
Epoch [52/120    avg_loss:0.082, val_acc:0.917]
Epoch [53/120    avg_loss:0.067, val_acc:0.938]
Epoch [54/120    avg_loss:0.047, val_acc:0.925]
Epoch [55/120    avg_loss:0.062, val_acc:0.924]
Epoch [56/120    avg_loss:0.070, val_acc:0.940]
Epoch [57/120    avg_loss:0.062, val_acc:0.905]
Epoch [58/120    avg_loss:0.067, val_acc:0.934]
Epoch [59/120    avg_loss:0.051, val_acc:0.929]
Epoch [60/120    avg_loss:0.044, val_acc:0.939]
Epoch [61/120    avg_loss:0.035, val_acc:0.952]
Epoch [62/120    avg_loss:0.039, val_acc:0.942]
Epoch [63/120    avg_loss:0.059, val_acc:0.925]
Epoch [64/120    avg_loss:0.052, val_acc:0.951]
Epoch [65/120    avg_loss:0.033, val_acc:0.948]
Epoch [66/120    avg_loss:0.034, val_acc:0.946]
Epoch [67/120    avg_loss:0.036, val_acc:0.942]
Epoch [68/120    avg_loss:0.033, val_acc:0.944]
Epoch [69/120    avg_loss:0.031, val_acc:0.956]
Epoch [70/120    avg_loss:0.036, val_acc:0.950]
Epoch [71/120    avg_loss:0.033, val_acc:0.951]
Epoch [72/120    avg_loss:0.043, val_acc:0.946]
Epoch [73/120    avg_loss:0.042, val_acc:0.947]
Epoch [74/120    avg_loss:0.039, val_acc:0.946]
Epoch [75/120    avg_loss:0.033, val_acc:0.957]
Epoch [76/120    avg_loss:0.036, val_acc:0.944]
Epoch [77/120    avg_loss:0.041, val_acc:0.946]
Epoch [78/120    avg_loss:0.030, val_acc:0.954]
Epoch [79/120    avg_loss:0.027, val_acc:0.952]
Epoch [80/120    avg_loss:0.033, val_acc:0.950]
Epoch [81/120    avg_loss:0.023, val_acc:0.955]
Epoch [82/120    avg_loss:0.019, val_acc:0.955]
Epoch [83/120    avg_loss:0.018, val_acc:0.951]
Epoch [84/120    avg_loss:0.024, val_acc:0.955]
Epoch [85/120    avg_loss:0.029, val_acc:0.951]
Epoch [86/120    avg_loss:0.028, val_acc:0.946]
Epoch [87/120    avg_loss:0.027, val_acc:0.957]
Epoch [88/120    avg_loss:0.020, val_acc:0.959]
Epoch [89/120    avg_loss:0.014, val_acc:0.958]
Epoch [90/120    avg_loss:0.014, val_acc:0.961]
Epoch [91/120    avg_loss:0.025, val_acc:0.944]
Epoch [92/120    avg_loss:0.031, val_acc:0.948]
Epoch [93/120    avg_loss:0.020, val_acc:0.968]
Epoch [94/120    avg_loss:0.025, val_acc:0.953]
Epoch [95/120    avg_loss:0.018, val_acc:0.956]
Epoch [96/120    avg_loss:0.016, val_acc:0.967]
Epoch [97/120    avg_loss:0.024, val_acc:0.958]
Epoch [98/120    avg_loss:0.028, val_acc:0.939]
Epoch [99/120    avg_loss:0.021, val_acc:0.963]
Epoch [100/120    avg_loss:0.025, val_acc:0.947]
Epoch [101/120    avg_loss:0.028, val_acc:0.961]
Epoch [102/120    avg_loss:0.022, val_acc:0.968]
Epoch [103/120    avg_loss:0.023, val_acc:0.963]
Epoch [104/120    avg_loss:0.021, val_acc:0.960]
Epoch [105/120    avg_loss:0.017, val_acc:0.961]
Epoch [106/120    avg_loss:0.014, val_acc:0.963]
Epoch [107/120    avg_loss:0.016, val_acc:0.956]
Epoch [108/120    avg_loss:0.013, val_acc:0.964]
Epoch [109/120    avg_loss:0.012, val_acc:0.964]
Epoch [110/120    avg_loss:0.015, val_acc:0.966]
Epoch [111/120    avg_loss:0.009, val_acc:0.970]
Epoch [112/120    avg_loss:0.008, val_acc:0.975]
Epoch [113/120    avg_loss:0.016, val_acc:0.968]
Epoch [114/120    avg_loss:0.014, val_acc:0.960]
Epoch [115/120    avg_loss:0.012, val_acc:0.966]
Epoch [116/120    avg_loss:0.018, val_acc:0.958]
Epoch [117/120    avg_loss:0.012, val_acc:0.968]
Epoch [118/120    avg_loss:0.009, val_acc:0.973]
Epoch [119/120    avg_loss:0.014, val_acc:0.949]
Epoch [120/120    avg_loss:0.015, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    1    0    1    0
     0    0    0]
 [   0    0 1271    3    4    0    0    0    0    0    2    5    0    0
     0    0    0]
 [   0    0    0  724    4    0    0    0    0    1    1    3   13    1
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    1    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   25    0    0    0    0    0    0    0  826   23    0    0
     1    0    0]
 [   0    0   22    0    0    0    1    0    6    0   22 2146   13    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    1    4  523    0
     1    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
  1122   13    0]
 [   0    0    0    0    0    1   11    0    0    0    1    0    0    0
    95  239    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
96.73712737127371

F1 scores:
[       nan 0.975      0.9765655  0.98036561 0.97921478 0.99074074
 0.98570354 1.         0.99307159 0.97297297 0.95546559 0.97678653
 0.96228151 0.99730458 0.94964029 0.7953411  0.97590361]

Kappa:
0.9627804290542682
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1cb20a88d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.708, val_acc:0.326]
Epoch [2/120    avg_loss:2.363, val_acc:0.448]
Epoch [3/120    avg_loss:2.164, val_acc:0.498]
Epoch [4/120    avg_loss:1.973, val_acc:0.537]
Epoch [5/120    avg_loss:1.805, val_acc:0.534]
Epoch [6/120    avg_loss:1.644, val_acc:0.522]
Epoch [7/120    avg_loss:1.521, val_acc:0.564]
Epoch [8/120    avg_loss:1.411, val_acc:0.617]
Epoch [9/120    avg_loss:1.265, val_acc:0.665]
Epoch [10/120    avg_loss:1.089, val_acc:0.709]
Epoch [11/120    avg_loss:0.991, val_acc:0.727]
Epoch [12/120    avg_loss:0.858, val_acc:0.774]
Epoch [13/120    avg_loss:0.741, val_acc:0.757]
Epoch [14/120    avg_loss:0.654, val_acc:0.784]
Epoch [15/120    avg_loss:0.615, val_acc:0.790]
Epoch [16/120    avg_loss:0.522, val_acc:0.793]
Epoch [17/120    avg_loss:0.515, val_acc:0.816]
Epoch [18/120    avg_loss:0.411, val_acc:0.843]
Epoch [19/120    avg_loss:0.338, val_acc:0.843]
Epoch [20/120    avg_loss:0.335, val_acc:0.858]
Epoch [21/120    avg_loss:0.297, val_acc:0.853]
Epoch [22/120    avg_loss:0.299, val_acc:0.866]
Epoch [23/120    avg_loss:0.224, val_acc:0.893]
Epoch [24/120    avg_loss:0.207, val_acc:0.916]
Epoch [25/120    avg_loss:0.199, val_acc:0.885]
Epoch [26/120    avg_loss:0.321, val_acc:0.854]
Epoch [27/120    avg_loss:0.238, val_acc:0.893]
Epoch [28/120    avg_loss:0.193, val_acc:0.886]
Epoch [29/120    avg_loss:0.180, val_acc:0.898]
Epoch [30/120    avg_loss:0.146, val_acc:0.895]
Epoch [31/120    avg_loss:0.143, val_acc:0.932]
Epoch [32/120    avg_loss:0.144, val_acc:0.897]
Epoch [33/120    avg_loss:0.140, val_acc:0.923]
Epoch [34/120    avg_loss:0.158, val_acc:0.924]
Epoch [35/120    avg_loss:0.134, val_acc:0.928]
Epoch [36/120    avg_loss:0.119, val_acc:0.911]
Epoch [37/120    avg_loss:0.224, val_acc:0.893]
Epoch [38/120    avg_loss:0.179, val_acc:0.927]
Epoch [39/120    avg_loss:0.105, val_acc:0.919]
Epoch [40/120    avg_loss:0.089, val_acc:0.931]
Epoch [41/120    avg_loss:0.068, val_acc:0.939]
Epoch [42/120    avg_loss:0.069, val_acc:0.940]
Epoch [43/120    avg_loss:0.064, val_acc:0.950]
Epoch [44/120    avg_loss:0.066, val_acc:0.931]
Epoch [45/120    avg_loss:0.059, val_acc:0.944]
Epoch [46/120    avg_loss:0.092, val_acc:0.945]
Epoch [47/120    avg_loss:0.072, val_acc:0.954]
Epoch [48/120    avg_loss:0.060, val_acc:0.949]
Epoch [49/120    avg_loss:0.050, val_acc:0.944]
Epoch [50/120    avg_loss:0.048, val_acc:0.965]
Epoch [51/120    avg_loss:0.051, val_acc:0.953]
Epoch [52/120    avg_loss:0.052, val_acc:0.956]
Epoch [53/120    avg_loss:0.046, val_acc:0.958]
Epoch [54/120    avg_loss:0.050, val_acc:0.941]
Epoch [55/120    avg_loss:0.064, val_acc:0.957]
Epoch [56/120    avg_loss:0.042, val_acc:0.957]
Epoch [57/120    avg_loss:0.065, val_acc:0.946]
Epoch [58/120    avg_loss:0.034, val_acc:0.953]
Epoch [59/120    avg_loss:0.053, val_acc:0.941]
Epoch [60/120    avg_loss:0.044, val_acc:0.957]
Epoch [61/120    avg_loss:0.042, val_acc:0.946]
Epoch [62/120    avg_loss:0.057, val_acc:0.917]
Epoch [63/120    avg_loss:0.067, val_acc:0.925]
Epoch [64/120    avg_loss:0.052, val_acc:0.957]
Epoch [65/120    avg_loss:0.030, val_acc:0.959]
Epoch [66/120    avg_loss:0.027, val_acc:0.961]
Epoch [67/120    avg_loss:0.032, val_acc:0.961]
Epoch [68/120    avg_loss:0.030, val_acc:0.964]
Epoch [69/120    avg_loss:0.021, val_acc:0.966]
Epoch [70/120    avg_loss:0.023, val_acc:0.965]
Epoch [71/120    avg_loss:0.025, val_acc:0.968]
Epoch [72/120    avg_loss:0.021, val_acc:0.966]
Epoch [73/120    avg_loss:0.022, val_acc:0.968]
Epoch [74/120    avg_loss:0.022, val_acc:0.967]
Epoch [75/120    avg_loss:0.023, val_acc:0.970]
Epoch [76/120    avg_loss:0.027, val_acc:0.967]
Epoch [77/120    avg_loss:0.023, val_acc:0.970]
Epoch [78/120    avg_loss:0.022, val_acc:0.970]
Epoch [79/120    avg_loss:0.024, val_acc:0.968]
Epoch [80/120    avg_loss:0.020, val_acc:0.965]
Epoch [81/120    avg_loss:0.025, val_acc:0.967]
Epoch [82/120    avg_loss:0.017, val_acc:0.967]
Epoch [83/120    avg_loss:0.023, val_acc:0.966]
Epoch [84/120    avg_loss:0.018, val_acc:0.964]
Epoch [85/120    avg_loss:0.022, val_acc:0.969]
Epoch [86/120    avg_loss:0.016, val_acc:0.970]
Epoch [87/120    avg_loss:0.017, val_acc:0.970]
Epoch [88/120    avg_loss:0.017, val_acc:0.969]
Epoch [89/120    avg_loss:0.018, val_acc:0.969]
Epoch [90/120    avg_loss:0.015, val_acc:0.968]
Epoch [91/120    avg_loss:0.016, val_acc:0.968]
Epoch [92/120    avg_loss:0.019, val_acc:0.969]
Epoch [93/120    avg_loss:0.017, val_acc:0.968]
Epoch [94/120    avg_loss:0.015, val_acc:0.969]
Epoch [95/120    avg_loss:0.016, val_acc:0.970]
Epoch [96/120    avg_loss:0.023, val_acc:0.971]
Epoch [97/120    avg_loss:0.018, val_acc:0.971]
Epoch [98/120    avg_loss:0.019, val_acc:0.969]
Epoch [99/120    avg_loss:0.017, val_acc:0.970]
Epoch [100/120    avg_loss:0.016, val_acc:0.970]
Epoch [101/120    avg_loss:0.011, val_acc:0.970]
Epoch [102/120    avg_loss:0.015, val_acc:0.970]
Epoch [103/120    avg_loss:0.015, val_acc:0.971]
Epoch [104/120    avg_loss:0.017, val_acc:0.971]
Epoch [105/120    avg_loss:0.018, val_acc:0.969]
Epoch [106/120    avg_loss:0.016, val_acc:0.969]
Epoch [107/120    avg_loss:0.017, val_acc:0.967]
Epoch [108/120    avg_loss:0.016, val_acc:0.970]
Epoch [109/120    avg_loss:0.014, val_acc:0.969]
Epoch [110/120    avg_loss:0.014, val_acc:0.969]
Epoch [111/120    avg_loss:0.017, val_acc:0.970]
Epoch [112/120    avg_loss:0.021, val_acc:0.969]
Epoch [113/120    avg_loss:0.018, val_acc:0.971]
Epoch [114/120    avg_loss:0.017, val_acc:0.970]
Epoch [115/120    avg_loss:0.017, val_acc:0.970]
Epoch [116/120    avg_loss:0.017, val_acc:0.970]
Epoch [117/120    avg_loss:0.017, val_acc:0.970]
Epoch [118/120    avg_loss:0.013, val_acc:0.971]
Epoch [119/120    avg_loss:0.017, val_acc:0.970]
Epoch [120/120    avg_loss:0.015, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1251    2    6    2    2    0    0    0    3   19    0    0
     0    0    0]
 [   0    0    1  729    1    0    0    0    0    1    2    6    4    0
     0    3    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    7    1    0    0    0    0    0    0  843   20    0    0
     2    2    0]
 [   0    0    9    0    0    2    1    0    0    3   19 2140   33    0
     1    2    0]
 [   0    0    0    3    0    0    0    0    0    0    0    6  519    0
     1    4    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    68  274    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.09485094850949

F1 scores:
[       nan 0.98765432 0.9800235  0.98380567 0.98383372 0.99082569
 0.99318698 0.97959184 1.         0.87179487 0.96729776 0.97250625
 0.94794521 1.         0.96109448 0.84698609 0.96969697]

Kappa:
0.9668807218097714
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd98aa96908>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.743, val_acc:0.348]
Epoch [2/120    avg_loss:2.387, val_acc:0.485]
Epoch [3/120    avg_loss:2.171, val_acc:0.521]
Epoch [4/120    avg_loss:1.980, val_acc:0.574]
Epoch [5/120    avg_loss:1.784, val_acc:0.608]
Epoch [6/120    avg_loss:1.578, val_acc:0.625]
Epoch [7/120    avg_loss:1.443, val_acc:0.643]
Epoch [8/120    avg_loss:1.235, val_acc:0.636]
Epoch [9/120    avg_loss:1.181, val_acc:0.673]
Epoch [10/120    avg_loss:1.002, val_acc:0.749]
Epoch [11/120    avg_loss:0.934, val_acc:0.744]
Epoch [12/120    avg_loss:0.755, val_acc:0.791]
Epoch [13/120    avg_loss:0.743, val_acc:0.730]
Epoch [14/120    avg_loss:0.609, val_acc:0.813]
Epoch [15/120    avg_loss:0.654, val_acc:0.756]
Epoch [16/120    avg_loss:0.534, val_acc:0.798]
Epoch [17/120    avg_loss:0.548, val_acc:0.828]
Epoch [18/120    avg_loss:0.436, val_acc:0.835]
Epoch [19/120    avg_loss:0.480, val_acc:0.819]
Epoch [20/120    avg_loss:0.405, val_acc:0.855]
Epoch [21/120    avg_loss:0.368, val_acc:0.796]
Epoch [22/120    avg_loss:0.348, val_acc:0.891]
Epoch [23/120    avg_loss:0.263, val_acc:0.880]
Epoch [24/120    avg_loss:0.235, val_acc:0.848]
Epoch [25/120    avg_loss:0.283, val_acc:0.876]
Epoch [26/120    avg_loss:0.223, val_acc:0.900]
Epoch [27/120    avg_loss:0.200, val_acc:0.916]
Epoch [28/120    avg_loss:0.172, val_acc:0.917]
Epoch [29/120    avg_loss:0.164, val_acc:0.917]
Epoch [30/120    avg_loss:0.150, val_acc:0.919]
Epoch [31/120    avg_loss:0.136, val_acc:0.938]
Epoch [32/120    avg_loss:0.129, val_acc:0.937]
Epoch [33/120    avg_loss:0.118, val_acc:0.921]
Epoch [34/120    avg_loss:0.116, val_acc:0.935]
Epoch [35/120    avg_loss:0.120, val_acc:0.938]
Epoch [36/120    avg_loss:0.101, val_acc:0.928]
Epoch [37/120    avg_loss:0.185, val_acc:0.897]
Epoch [38/120    avg_loss:0.140, val_acc:0.887]
Epoch [39/120    avg_loss:0.112, val_acc:0.936]
Epoch [40/120    avg_loss:0.095, val_acc:0.937]
Epoch [41/120    avg_loss:0.101, val_acc:0.936]
Epoch [42/120    avg_loss:0.093, val_acc:0.935]
Epoch [43/120    avg_loss:0.094, val_acc:0.935]
Epoch [44/120    avg_loss:0.072, val_acc:0.953]
Epoch [45/120    avg_loss:0.083, val_acc:0.934]
Epoch [46/120    avg_loss:0.096, val_acc:0.931]
Epoch [47/120    avg_loss:0.114, val_acc:0.925]
Epoch [48/120    avg_loss:0.084, val_acc:0.962]
Epoch [49/120    avg_loss:0.061, val_acc:0.954]
Epoch [50/120    avg_loss:0.052, val_acc:0.960]
Epoch [51/120    avg_loss:0.040, val_acc:0.959]
Epoch [52/120    avg_loss:0.046, val_acc:0.958]
Epoch [53/120    avg_loss:0.058, val_acc:0.955]
Epoch [54/120    avg_loss:0.092, val_acc:0.922]
Epoch [55/120    avg_loss:0.086, val_acc:0.952]
Epoch [56/120    avg_loss:0.077, val_acc:0.951]
Epoch [57/120    avg_loss:0.048, val_acc:0.958]
Epoch [58/120    avg_loss:0.043, val_acc:0.946]
Epoch [59/120    avg_loss:0.043, val_acc:0.966]
Epoch [60/120    avg_loss:0.041, val_acc:0.926]
Epoch [61/120    avg_loss:0.046, val_acc:0.940]
Epoch [62/120    avg_loss:0.034, val_acc:0.965]
Epoch [63/120    avg_loss:0.031, val_acc:0.966]
Epoch [64/120    avg_loss:0.022, val_acc:0.971]
Epoch [65/120    avg_loss:0.022, val_acc:0.970]
Epoch [66/120    avg_loss:0.041, val_acc:0.955]
Epoch [67/120    avg_loss:0.028, val_acc:0.937]
Epoch [68/120    avg_loss:0.037, val_acc:0.950]
Epoch [69/120    avg_loss:0.044, val_acc:0.956]
Epoch [70/120    avg_loss:0.027, val_acc:0.967]
Epoch [71/120    avg_loss:0.032, val_acc:0.964]
Epoch [72/120    avg_loss:0.017, val_acc:0.970]
Epoch [73/120    avg_loss:0.017, val_acc:0.964]
Epoch [74/120    avg_loss:0.021, val_acc:0.964]
Epoch [75/120    avg_loss:0.022, val_acc:0.966]
Epoch [76/120    avg_loss:0.020, val_acc:0.967]
Epoch [77/120    avg_loss:0.014, val_acc:0.968]
Epoch [78/120    avg_loss:0.018, val_acc:0.970]
Epoch [79/120    avg_loss:0.010, val_acc:0.974]
Epoch [80/120    avg_loss:0.010, val_acc:0.975]
Epoch [81/120    avg_loss:0.010, val_acc:0.974]
Epoch [82/120    avg_loss:0.009, val_acc:0.973]
Epoch [83/120    avg_loss:0.010, val_acc:0.974]
Epoch [84/120    avg_loss:0.009, val_acc:0.975]
Epoch [85/120    avg_loss:0.010, val_acc:0.974]
Epoch [86/120    avg_loss:0.009, val_acc:0.975]
Epoch [87/120    avg_loss:0.009, val_acc:0.974]
Epoch [88/120    avg_loss:0.010, val_acc:0.974]
Epoch [89/120    avg_loss:0.009, val_acc:0.974]
Epoch [90/120    avg_loss:0.009, val_acc:0.974]
Epoch [91/120    avg_loss:0.009, val_acc:0.975]
Epoch [92/120    avg_loss:0.011, val_acc:0.975]
Epoch [93/120    avg_loss:0.009, val_acc:0.975]
Epoch [94/120    avg_loss:0.009, val_acc:0.973]
Epoch [95/120    avg_loss:0.008, val_acc:0.974]
Epoch [96/120    avg_loss:0.010, val_acc:0.973]
Epoch [97/120    avg_loss:0.008, val_acc:0.972]
Epoch [98/120    avg_loss:0.011, val_acc:0.975]
Epoch [99/120    avg_loss:0.010, val_acc:0.974]
Epoch [100/120    avg_loss:0.009, val_acc:0.974]
Epoch [101/120    avg_loss:0.010, val_acc:0.974]
Epoch [102/120    avg_loss:0.010, val_acc:0.974]
Epoch [103/120    avg_loss:0.009, val_acc:0.975]
Epoch [104/120    avg_loss:0.008, val_acc:0.973]
Epoch [105/120    avg_loss:0.007, val_acc:0.974]
Epoch [106/120    avg_loss:0.007, val_acc:0.974]
Epoch [107/120    avg_loss:0.007, val_acc:0.971]
Epoch [108/120    avg_loss:0.008, val_acc:0.973]
Epoch [109/120    avg_loss:0.009, val_acc:0.971]
Epoch [110/120    avg_loss:0.008, val_acc:0.972]
Epoch [111/120    avg_loss:0.008, val_acc:0.971]
Epoch [112/120    avg_loss:0.007, val_acc:0.971]
Epoch [113/120    avg_loss:0.008, val_acc:0.972]
Epoch [114/120    avg_loss:0.009, val_acc:0.973]
Epoch [115/120    avg_loss:0.007, val_acc:0.972]
Epoch [116/120    avg_loss:0.009, val_acc:0.971]
Epoch [117/120    avg_loss:0.008, val_acc:0.970]
Epoch [118/120    avg_loss:0.007, val_acc:0.971]
Epoch [119/120    avg_loss:0.012, val_acc:0.972]
Epoch [120/120    avg_loss:0.008, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    1    4    0    0    0    0    0    5   13    0    0
     0    2    0]
 [   0    0    2  735    2    1    1    0    0    0    0    2    4    0
     0    0    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  427    0    1    0    0    0    1    0    0
     6    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   13    0    0    0    0    0    0    0  847   15    0    0
     0    0    0]
 [   0    0   15    1    0    0    0    0    0    1   12 2168   13    0
     0    0    0]
 [   0    0    1    5    0    0    0    0    0    0    0    5  519    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1127   12    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    64  274    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.58265582655827

F1 scores:
[       nan 0.98765432 0.97788126 0.98657718 0.98139535 0.98957126
 0.99168556 0.98039216 1.         0.94444444 0.97412306 0.98210646
 0.96648045 0.99728997 0.96448438 0.86028257 0.97619048]

Kappa:
0.9724254981777244
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3d3e792908>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.699, val_acc:0.314]
Epoch [2/120    avg_loss:2.342, val_acc:0.471]
Epoch [3/120    avg_loss:2.140, val_acc:0.518]
Epoch [4/120    avg_loss:1.931, val_acc:0.572]
Epoch [5/120    avg_loss:1.746, val_acc:0.595]
Epoch [6/120    avg_loss:1.549, val_acc:0.619]
Epoch [7/120    avg_loss:1.370, val_acc:0.654]
Epoch [8/120    avg_loss:1.230, val_acc:0.681]
Epoch [9/120    avg_loss:1.114, val_acc:0.657]
Epoch [10/120    avg_loss:0.988, val_acc:0.704]
Epoch [11/120    avg_loss:0.859, val_acc:0.746]
Epoch [12/120    avg_loss:0.848, val_acc:0.726]
Epoch [13/120    avg_loss:0.790, val_acc:0.778]
Epoch [14/120    avg_loss:0.743, val_acc:0.777]
Epoch [15/120    avg_loss:0.582, val_acc:0.806]
Epoch [16/120    avg_loss:0.483, val_acc:0.847]
Epoch [17/120    avg_loss:0.586, val_acc:0.823]
Epoch [18/120    avg_loss:0.490, val_acc:0.838]
Epoch [19/120    avg_loss:0.388, val_acc:0.851]
Epoch [20/120    avg_loss:0.384, val_acc:0.860]
Epoch [21/120    avg_loss:0.319, val_acc:0.879]
Epoch [22/120    avg_loss:0.311, val_acc:0.876]
Epoch [23/120    avg_loss:0.252, val_acc:0.890]
Epoch [24/120    avg_loss:0.214, val_acc:0.893]
Epoch [25/120    avg_loss:0.184, val_acc:0.932]
Epoch [26/120    avg_loss:0.249, val_acc:0.906]
Epoch [27/120    avg_loss:0.231, val_acc:0.923]
Epoch [28/120    avg_loss:0.172, val_acc:0.889]
Epoch [29/120    avg_loss:0.153, val_acc:0.928]
Epoch [30/120    avg_loss:0.144, val_acc:0.934]
Epoch [31/120    avg_loss:0.139, val_acc:0.930]
Epoch [32/120    avg_loss:0.113, val_acc:0.942]
Epoch [33/120    avg_loss:0.106, val_acc:0.941]
Epoch [34/120    avg_loss:0.100, val_acc:0.929]
Epoch [35/120    avg_loss:0.130, val_acc:0.938]
Epoch [36/120    avg_loss:0.147, val_acc:0.917]
Epoch [37/120    avg_loss:0.130, val_acc:0.936]
Epoch [38/120    avg_loss:0.104, val_acc:0.942]
Epoch [39/120    avg_loss:0.077, val_acc:0.968]
Epoch [40/120    avg_loss:0.069, val_acc:0.957]
Epoch [41/120    avg_loss:0.066, val_acc:0.961]
Epoch [42/120    avg_loss:0.056, val_acc:0.974]
Epoch [43/120    avg_loss:0.073, val_acc:0.875]
Epoch [44/120    avg_loss:0.104, val_acc:0.941]
Epoch [45/120    avg_loss:0.145, val_acc:0.884]
Epoch [46/120    avg_loss:0.106, val_acc:0.941]
Epoch [47/120    avg_loss:0.096, val_acc:0.954]
Epoch [48/120    avg_loss:0.116, val_acc:0.935]
Epoch [49/120    avg_loss:0.152, val_acc:0.928]
Epoch [50/120    avg_loss:0.147, val_acc:0.925]
Epoch [51/120    avg_loss:0.109, val_acc:0.947]
Epoch [52/120    avg_loss:0.087, val_acc:0.953]
Epoch [53/120    avg_loss:0.064, val_acc:0.964]
Epoch [54/120    avg_loss:0.066, val_acc:0.968]
Epoch [55/120    avg_loss:0.043, val_acc:0.966]
Epoch [56/120    avg_loss:0.046, val_acc:0.975]
Epoch [57/120    avg_loss:0.043, val_acc:0.972]
Epoch [58/120    avg_loss:0.036, val_acc:0.974]
Epoch [59/120    avg_loss:0.029, val_acc:0.973]
Epoch [60/120    avg_loss:0.033, val_acc:0.975]
Epoch [61/120    avg_loss:0.033, val_acc:0.976]
Epoch [62/120    avg_loss:0.038, val_acc:0.979]
Epoch [63/120    avg_loss:0.037, val_acc:0.978]
Epoch [64/120    avg_loss:0.032, val_acc:0.976]
Epoch [65/120    avg_loss:0.028, val_acc:0.978]
Epoch [66/120    avg_loss:0.030, val_acc:0.978]
Epoch [67/120    avg_loss:0.028, val_acc:0.978]
Epoch [68/120    avg_loss:0.033, val_acc:0.979]
Epoch [69/120    avg_loss:0.032, val_acc:0.979]
Epoch [70/120    avg_loss:0.026, val_acc:0.979]
Epoch [71/120    avg_loss:0.027, val_acc:0.980]
Epoch [72/120    avg_loss:0.035, val_acc:0.982]
Epoch [73/120    avg_loss:0.030, val_acc:0.981]
Epoch [74/120    avg_loss:0.028, val_acc:0.979]
Epoch [75/120    avg_loss:0.030, val_acc:0.978]
Epoch [76/120    avg_loss:0.032, val_acc:0.979]
Epoch [77/120    avg_loss:0.024, val_acc:0.981]
Epoch [78/120    avg_loss:0.026, val_acc:0.979]
Epoch [79/120    avg_loss:0.025, val_acc:0.978]
Epoch [80/120    avg_loss:0.025, val_acc:0.978]
Epoch [81/120    avg_loss:0.027, val_acc:0.980]
Epoch [82/120    avg_loss:0.023, val_acc:0.980]
Epoch [83/120    avg_loss:0.021, val_acc:0.980]
Epoch [84/120    avg_loss:0.028, val_acc:0.980]
Epoch [85/120    avg_loss:0.030, val_acc:0.978]
Epoch [86/120    avg_loss:0.026, val_acc:0.980]
Epoch [87/120    avg_loss:0.028, val_acc:0.980]
Epoch [88/120    avg_loss:0.023, val_acc:0.980]
Epoch [89/120    avg_loss:0.024, val_acc:0.980]
Epoch [90/120    avg_loss:0.020, val_acc:0.981]
Epoch [91/120    avg_loss:0.028, val_acc:0.980]
Epoch [92/120    avg_loss:0.028, val_acc:0.981]
Epoch [93/120    avg_loss:0.020, val_acc:0.981]
Epoch [94/120    avg_loss:0.020, val_acc:0.981]
Epoch [95/120    avg_loss:0.025, val_acc:0.981]
Epoch [96/120    avg_loss:0.021, val_acc:0.981]
Epoch [97/120    avg_loss:0.022, val_acc:0.980]
Epoch [98/120    avg_loss:0.025, val_acc:0.981]
Epoch [99/120    avg_loss:0.028, val_acc:0.981]
Epoch [100/120    avg_loss:0.026, val_acc:0.981]
Epoch [101/120    avg_loss:0.023, val_acc:0.981]
Epoch [102/120    avg_loss:0.028, val_acc:0.981]
Epoch [103/120    avg_loss:0.023, val_acc:0.981]
Epoch [104/120    avg_loss:0.024, val_acc:0.981]
Epoch [105/120    avg_loss:0.025, val_acc:0.981]
Epoch [106/120    avg_loss:0.026, val_acc:0.981]
Epoch [107/120    avg_loss:0.026, val_acc:0.981]
Epoch [108/120    avg_loss:0.023, val_acc:0.981]
Epoch [109/120    avg_loss:0.024, val_acc:0.980]
Epoch [110/120    avg_loss:0.022, val_acc:0.980]
Epoch [111/120    avg_loss:0.025, val_acc:0.980]
Epoch [112/120    avg_loss:0.023, val_acc:0.980]
Epoch [113/120    avg_loss:0.022, val_acc:0.981]
Epoch [114/120    avg_loss:0.027, val_acc:0.981]
Epoch [115/120    avg_loss:0.022, val_acc:0.981]
Epoch [116/120    avg_loss:0.020, val_acc:0.980]
Epoch [117/120    avg_loss:0.020, val_acc:0.980]
Epoch [118/120    avg_loss:0.021, val_acc:0.980]
Epoch [119/120    avg_loss:0.026, val_acc:0.980]
Epoch [120/120    avg_loss:0.023, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0 1238   16    1    1    0    0    0    0    0   29    0    0
     0    0    0]
 [   0    0    5  733    0    0    0    0    0    0    4    1    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    0    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    1    0    0    0    0  825   41    0    0
     0    0    0]
 [   0    0    6    0    0    1    9    0    0    0   13 2157   24    0
     0    0    0]
 [   0    0    4    5    0    0    0    0    0    0    0    0  520    0
     1    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1125   13    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    64  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.04065040650407

F1 scores:
[       nan 0.98765432 0.97250589 0.97668221 0.99765808 0.98614319
 0.98864497 1.         1.         1.         0.96097845 0.97140284
 0.95940959 1.         0.96194955 0.87402799 0.97619048]

Kappa:
0.9662357219003561
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7effb06077f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.662, val_acc:0.445]
Epoch [2/120    avg_loss:2.333, val_acc:0.511]
Epoch [3/120    avg_loss:2.163, val_acc:0.557]
Epoch [4/120    avg_loss:1.975, val_acc:0.563]
Epoch [5/120    avg_loss:1.831, val_acc:0.565]
Epoch [6/120    avg_loss:1.647, val_acc:0.593]
Epoch [7/120    avg_loss:1.509, val_acc:0.622]
Epoch [8/120    avg_loss:1.381, val_acc:0.640]
Epoch [9/120    avg_loss:1.200, val_acc:0.667]
Epoch [10/120    avg_loss:1.043, val_acc:0.718]
Epoch [11/120    avg_loss:0.846, val_acc:0.774]
Epoch [12/120    avg_loss:0.902, val_acc:0.729]
Epoch [13/120    avg_loss:0.846, val_acc:0.753]
Epoch [14/120    avg_loss:0.688, val_acc:0.779]
Epoch [15/120    avg_loss:0.586, val_acc:0.761]
Epoch [16/120    avg_loss:0.560, val_acc:0.810]
Epoch [17/120    avg_loss:0.500, val_acc:0.842]
Epoch [18/120    avg_loss:0.514, val_acc:0.822]
Epoch [19/120    avg_loss:0.409, val_acc:0.848]
Epoch [20/120    avg_loss:0.348, val_acc:0.849]
Epoch [21/120    avg_loss:0.321, val_acc:0.848]
Epoch [22/120    avg_loss:0.345, val_acc:0.870]
Epoch [23/120    avg_loss:0.249, val_acc:0.868]
Epoch [24/120    avg_loss:0.232, val_acc:0.883]
Epoch [25/120    avg_loss:0.363, val_acc:0.830]
Epoch [26/120    avg_loss:0.267, val_acc:0.854]
Epoch [27/120    avg_loss:0.248, val_acc:0.881]
Epoch [28/120    avg_loss:0.194, val_acc:0.902]
Epoch [29/120    avg_loss:0.157, val_acc:0.928]
Epoch [30/120    avg_loss:0.169, val_acc:0.905]
Epoch [31/120    avg_loss:0.143, val_acc:0.914]
Epoch [32/120    avg_loss:0.141, val_acc:0.926]
Epoch [33/120    avg_loss:0.144, val_acc:0.925]
Epoch [34/120    avg_loss:0.118, val_acc:0.920]
Epoch [35/120    avg_loss:0.229, val_acc:0.842]
Epoch [36/120    avg_loss:0.273, val_acc:0.894]
Epoch [37/120    avg_loss:0.181, val_acc:0.905]
Epoch [38/120    avg_loss:0.139, val_acc:0.911]
Epoch [39/120    avg_loss:0.136, val_acc:0.914]
Epoch [40/120    avg_loss:0.116, val_acc:0.905]
Epoch [41/120    avg_loss:0.121, val_acc:0.930]
Epoch [42/120    avg_loss:0.092, val_acc:0.929]
Epoch [43/120    avg_loss:0.092, val_acc:0.924]
Epoch [44/120    avg_loss:0.130, val_acc:0.863]
Epoch [45/120    avg_loss:0.246, val_acc:0.897]
Epoch [46/120    avg_loss:0.147, val_acc:0.916]
Epoch [47/120    avg_loss:0.112, val_acc:0.931]
Epoch [48/120    avg_loss:0.127, val_acc:0.946]
Epoch [49/120    avg_loss:0.085, val_acc:0.941]
Epoch [50/120    avg_loss:0.078, val_acc:0.941]
Epoch [51/120    avg_loss:0.086, val_acc:0.933]
Epoch [52/120    avg_loss:0.073, val_acc:0.948]
Epoch [53/120    avg_loss:0.056, val_acc:0.944]
Epoch [54/120    avg_loss:0.065, val_acc:0.954]
Epoch [55/120    avg_loss:0.065, val_acc:0.956]
Epoch [56/120    avg_loss:0.051, val_acc:0.965]
Epoch [57/120    avg_loss:0.047, val_acc:0.962]
Epoch [58/120    avg_loss:0.034, val_acc:0.974]
Epoch [59/120    avg_loss:0.031, val_acc:0.947]
Epoch [60/120    avg_loss:0.045, val_acc:0.953]
Epoch [61/120    avg_loss:0.042, val_acc:0.958]
Epoch [62/120    avg_loss:0.056, val_acc:0.944]
Epoch [63/120    avg_loss:0.057, val_acc:0.958]
Epoch [64/120    avg_loss:0.041, val_acc:0.973]
Epoch [65/120    avg_loss:0.047, val_acc:0.963]
Epoch [66/120    avg_loss:0.037, val_acc:0.961]
Epoch [67/120    avg_loss:0.042, val_acc:0.956]
Epoch [68/120    avg_loss:0.041, val_acc:0.962]
Epoch [69/120    avg_loss:0.039, val_acc:0.963]
Epoch [70/120    avg_loss:0.033, val_acc:0.966]
Epoch [71/120    avg_loss:0.030, val_acc:0.970]
Epoch [72/120    avg_loss:0.021, val_acc:0.967]
Epoch [73/120    avg_loss:0.015, val_acc:0.971]
Epoch [74/120    avg_loss:0.017, val_acc:0.972]
Epoch [75/120    avg_loss:0.018, val_acc:0.972]
Epoch [76/120    avg_loss:0.018, val_acc:0.971]
Epoch [77/120    avg_loss:0.021, val_acc:0.974]
Epoch [78/120    avg_loss:0.016, val_acc:0.973]
Epoch [79/120    avg_loss:0.016, val_acc:0.974]
Epoch [80/120    avg_loss:0.016, val_acc:0.974]
Epoch [81/120    avg_loss:0.015, val_acc:0.973]
Epoch [82/120    avg_loss:0.015, val_acc:0.976]
Epoch [83/120    avg_loss:0.013, val_acc:0.977]
Epoch [84/120    avg_loss:0.015, val_acc:0.974]
Epoch [85/120    avg_loss:0.012, val_acc:0.975]
Epoch [86/120    avg_loss:0.012, val_acc:0.975]
Epoch [87/120    avg_loss:0.012, val_acc:0.975]
Epoch [88/120    avg_loss:0.015, val_acc:0.976]
Epoch [89/120    avg_loss:0.015, val_acc:0.975]
Epoch [90/120    avg_loss:0.017, val_acc:0.975]
Epoch [91/120    avg_loss:0.011, val_acc:0.976]
Epoch [92/120    avg_loss:0.014, val_acc:0.977]
Epoch [93/120    avg_loss:0.013, val_acc:0.975]
Epoch [94/120    avg_loss:0.014, val_acc:0.979]
Epoch [95/120    avg_loss:0.013, val_acc:0.980]
Epoch [96/120    avg_loss:0.013, val_acc:0.978]
Epoch [97/120    avg_loss:0.013, val_acc:0.979]
Epoch [98/120    avg_loss:0.013, val_acc:0.974]
Epoch [99/120    avg_loss:0.013, val_acc:0.974]
Epoch [100/120    avg_loss:0.011, val_acc:0.976]
Epoch [101/120    avg_loss:0.012, val_acc:0.977]
Epoch [102/120    avg_loss:0.012, val_acc:0.972]
Epoch [103/120    avg_loss:0.013, val_acc:0.975]
Epoch [104/120    avg_loss:0.011, val_acc:0.975]
Epoch [105/120    avg_loss:0.014, val_acc:0.974]
Epoch [106/120    avg_loss:0.016, val_acc:0.975]
Epoch [107/120    avg_loss:0.014, val_acc:0.978]
Epoch [108/120    avg_loss:0.018, val_acc:0.976]
Epoch [109/120    avg_loss:0.011, val_acc:0.976]
Epoch [110/120    avg_loss:0.018, val_acc:0.976]
Epoch [111/120    avg_loss:0.013, val_acc:0.976]
Epoch [112/120    avg_loss:0.013, val_acc:0.976]
Epoch [113/120    avg_loss:0.013, val_acc:0.977]
Epoch [114/120    avg_loss:0.012, val_acc:0.977]
Epoch [115/120    avg_loss:0.012, val_acc:0.977]
Epoch [116/120    avg_loss:0.016, val_acc:0.977]
Epoch [117/120    avg_loss:0.014, val_acc:0.977]
Epoch [118/120    avg_loss:0.015, val_acc:0.977]
Epoch [119/120    avg_loss:0.012, val_acc:0.977]
Epoch [120/120    avg_loss:0.015, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1263    0    2    0    0    0    0    0    0   20    0    0
     0    0    0]
 [   0    0    0  717    5    0    0    0    0    1    1   14    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    1    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    1    0    0    0  841   28    1    0
     0    0    0]
 [   0    0    6    2    0    1    0    0    0    1   11 2170   17    2
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0   12  515    0
     0    3    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1122   16    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    69  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.4308943089431

F1 scores:
[       nan 1.         0.98749023 0.97750511 0.98383372 0.99539171
 0.99695122 1.         1.         0.92307692 0.97281666 0.97396768
 0.95724907 0.99462366 0.96185169 0.86335404 0.98245614]

Kappa:
0.9706803455289078
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f19ce8f2828>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.668, val_acc:0.423]
Epoch [2/120    avg_loss:2.318, val_acc:0.448]
Epoch [3/120    avg_loss:2.113, val_acc:0.487]
Epoch [4/120    avg_loss:1.885, val_acc:0.552]
Epoch [5/120    avg_loss:1.843, val_acc:0.522]
Epoch [6/120    avg_loss:1.679, val_acc:0.581]
Epoch [7/120    avg_loss:1.520, val_acc:0.580]
Epoch [8/120    avg_loss:1.428, val_acc:0.640]
Epoch [9/120    avg_loss:1.322, val_acc:0.660]
Epoch [10/120    avg_loss:1.160, val_acc:0.696]
Epoch [11/120    avg_loss:1.086, val_acc:0.688]
Epoch [12/120    avg_loss:1.021, val_acc:0.685]
Epoch [13/120    avg_loss:0.900, val_acc:0.727]
Epoch [14/120    avg_loss:0.772, val_acc:0.724]
Epoch [15/120    avg_loss:0.735, val_acc:0.789]
Epoch [16/120    avg_loss:0.595, val_acc:0.795]
Epoch [17/120    avg_loss:0.602, val_acc:0.725]
Epoch [18/120    avg_loss:0.611, val_acc:0.821]
Epoch [19/120    avg_loss:0.452, val_acc:0.830]
Epoch [20/120    avg_loss:0.381, val_acc:0.814]
Epoch [21/120    avg_loss:0.346, val_acc:0.833]
Epoch [22/120    avg_loss:0.342, val_acc:0.850]
Epoch [23/120    avg_loss:0.321, val_acc:0.823]
Epoch [24/120    avg_loss:0.463, val_acc:0.810]
Epoch [25/120    avg_loss:0.343, val_acc:0.879]
Epoch [26/120    avg_loss:0.264, val_acc:0.854]
Epoch [27/120    avg_loss:0.247, val_acc:0.869]
Epoch [28/120    avg_loss:0.307, val_acc:0.867]
Epoch [29/120    avg_loss:0.224, val_acc:0.895]
Epoch [30/120    avg_loss:0.192, val_acc:0.897]
Epoch [31/120    avg_loss:0.174, val_acc:0.910]
Epoch [32/120    avg_loss:0.167, val_acc:0.921]
Epoch [33/120    avg_loss:0.146, val_acc:0.891]
Epoch [34/120    avg_loss:0.146, val_acc:0.908]
Epoch [35/120    avg_loss:0.134, val_acc:0.910]
Epoch [36/120    avg_loss:0.151, val_acc:0.920]
Epoch [37/120    avg_loss:0.136, val_acc:0.916]
Epoch [38/120    avg_loss:0.099, val_acc:0.926]
Epoch [39/120    avg_loss:0.101, val_acc:0.932]
Epoch [40/120    avg_loss:0.092, val_acc:0.934]
Epoch [41/120    avg_loss:0.363, val_acc:0.846]
Epoch [42/120    avg_loss:0.315, val_acc:0.850]
Epoch [43/120    avg_loss:0.246, val_acc:0.887]
Epoch [44/120    avg_loss:0.150, val_acc:0.900]
Epoch [45/120    avg_loss:0.104, val_acc:0.879]
Epoch [46/120    avg_loss:0.154, val_acc:0.929]
Epoch [47/120    avg_loss:0.100, val_acc:0.925]
Epoch [48/120    avg_loss:0.092, val_acc:0.932]
Epoch [49/120    avg_loss:0.073, val_acc:0.944]
Epoch [50/120    avg_loss:0.067, val_acc:0.943]
Epoch [51/120    avg_loss:0.070, val_acc:0.927]
Epoch [52/120    avg_loss:0.076, val_acc:0.935]
Epoch [53/120    avg_loss:0.069, val_acc:0.923]
Epoch [54/120    avg_loss:0.070, val_acc:0.929]
Epoch [55/120    avg_loss:0.063, val_acc:0.938]
Epoch [56/120    avg_loss:0.069, val_acc:0.926]
Epoch [57/120    avg_loss:0.082, val_acc:0.936]
Epoch [58/120    avg_loss:0.065, val_acc:0.947]
Epoch [59/120    avg_loss:0.043, val_acc:0.944]
Epoch [60/120    avg_loss:0.053, val_acc:0.939]
Epoch [61/120    avg_loss:0.062, val_acc:0.934]
Epoch [62/120    avg_loss:0.057, val_acc:0.935]
Epoch [63/120    avg_loss:0.055, val_acc:0.943]
Epoch [64/120    avg_loss:0.047, val_acc:0.947]
Epoch [65/120    avg_loss:0.048, val_acc:0.946]
Epoch [66/120    avg_loss:0.035, val_acc:0.952]
Epoch [67/120    avg_loss:0.048, val_acc:0.928]
Epoch [68/120    avg_loss:0.042, val_acc:0.941]
Epoch [69/120    avg_loss:0.030, val_acc:0.949]
Epoch [70/120    avg_loss:0.038, val_acc:0.927]
Epoch [71/120    avg_loss:0.060, val_acc:0.943]
Epoch [72/120    avg_loss:0.075, val_acc:0.945]
Epoch [73/120    avg_loss:0.046, val_acc:0.929]
Epoch [74/120    avg_loss:0.032, val_acc:0.938]
Epoch [75/120    avg_loss:0.033, val_acc:0.942]
Epoch [76/120    avg_loss:0.025, val_acc:0.943]
Epoch [77/120    avg_loss:0.020, val_acc:0.952]
Epoch [78/120    avg_loss:0.022, val_acc:0.959]
Epoch [79/120    avg_loss:0.025, val_acc:0.949]
Epoch [80/120    avg_loss:0.018, val_acc:0.956]
Epoch [81/120    avg_loss:0.021, val_acc:0.955]
Epoch [82/120    avg_loss:0.028, val_acc:0.949]
Epoch [83/120    avg_loss:0.022, val_acc:0.954]
Epoch [84/120    avg_loss:0.024, val_acc:0.954]
Epoch [85/120    avg_loss:0.018, val_acc:0.956]
Epoch [86/120    avg_loss:0.015, val_acc:0.952]
Epoch [87/120    avg_loss:0.018, val_acc:0.956]
Epoch [88/120    avg_loss:0.014, val_acc:0.954]
Epoch [89/120    avg_loss:0.020, val_acc:0.960]
Epoch [90/120    avg_loss:0.013, val_acc:0.950]
Epoch [91/120    avg_loss:0.018, val_acc:0.930]
Epoch [92/120    avg_loss:0.032, val_acc:0.955]
Epoch [93/120    avg_loss:0.019, val_acc:0.942]
Epoch [94/120    avg_loss:0.037, val_acc:0.947]
Epoch [95/120    avg_loss:0.018, val_acc:0.957]
Epoch [96/120    avg_loss:0.019, val_acc:0.958]
Epoch [97/120    avg_loss:0.022, val_acc:0.953]
Epoch [98/120    avg_loss:0.026, val_acc:0.948]
Epoch [99/120    avg_loss:0.023, val_acc:0.958]
Epoch [100/120    avg_loss:0.016, val_acc:0.961]
Epoch [101/120    avg_loss:0.013, val_acc:0.963]
Epoch [102/120    avg_loss:0.024, val_acc:0.941]
Epoch [103/120    avg_loss:0.023, val_acc:0.961]
Epoch [104/120    avg_loss:0.019, val_acc:0.960]
Epoch [105/120    avg_loss:0.029, val_acc:0.949]
Epoch [106/120    avg_loss:0.032, val_acc:0.947]
Epoch [107/120    avg_loss:0.016, val_acc:0.955]
Epoch [108/120    avg_loss:0.012, val_acc:0.969]
Epoch [109/120    avg_loss:0.011, val_acc:0.966]
Epoch [110/120    avg_loss:0.011, val_acc:0.960]
Epoch [111/120    avg_loss:0.010, val_acc:0.966]
Epoch [112/120    avg_loss:0.009, val_acc:0.969]
Epoch [113/120    avg_loss:0.017, val_acc:0.954]
Epoch [114/120    avg_loss:0.009, val_acc:0.968]
Epoch [115/120    avg_loss:0.011, val_acc:0.967]
Epoch [116/120    avg_loss:0.014, val_acc:0.964]
Epoch [117/120    avg_loss:0.008, val_acc:0.968]
Epoch [118/120    avg_loss:0.012, val_acc:0.960]
Epoch [119/120    avg_loss:0.009, val_acc:0.972]
Epoch [120/120    avg_loss:0.008, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1249   12    4    0    1    0    0    0    2   14    0    0
     3    0    0]
 [   0    0    5  721    6    0    0    0    0    1    2    5    6    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    1    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   14    0    0    0    0
     0    4    0]
 [   0    0    7    0    0    0    0    0    0    0  836   30    0    0
     2    0    0]
 [   0    0    4    0    0    0    0    0    0    1   25 2166   13    0
     1    0    0]
 [   0    0    6    2    0    0    0    0    0    0    0    6  516    0
     1    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1130    8    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    98  237    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.8130081300813

F1 scores:
[       nan 0.975      0.97692609 0.97300945 0.97706422 0.99538106
 0.98871332 1.         0.99883586 0.82352941 0.9603676  0.97765741
 0.96358543 1.         0.95077829 0.79264214 0.98224852]

Kappa:
0.9636256357919399
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f134cf79828>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.683, val_acc:0.350]
Epoch [2/120    avg_loss:2.312, val_acc:0.478]
Epoch [3/120    avg_loss:2.111, val_acc:0.530]
Epoch [4/120    avg_loss:1.952, val_acc:0.543]
Epoch [5/120    avg_loss:1.788, val_acc:0.560]
Epoch [6/120    avg_loss:1.585, val_acc:0.600]
Epoch [7/120    avg_loss:1.505, val_acc:0.619]
Epoch [8/120    avg_loss:1.314, val_acc:0.625]
Epoch [9/120    avg_loss:1.199, val_acc:0.709]
Epoch [10/120    avg_loss:1.078, val_acc:0.686]
Epoch [11/120    avg_loss:0.942, val_acc:0.754]
Epoch [12/120    avg_loss:0.916, val_acc:0.739]
Epoch [13/120    avg_loss:0.787, val_acc:0.780]
Epoch [14/120    avg_loss:0.741, val_acc:0.773]
Epoch [15/120    avg_loss:0.679, val_acc:0.798]
Epoch [16/120    avg_loss:0.588, val_acc:0.851]
Epoch [17/120    avg_loss:0.521, val_acc:0.821]
Epoch [18/120    avg_loss:0.437, val_acc:0.830]
Epoch [19/120    avg_loss:0.434, val_acc:0.822]
Epoch [20/120    avg_loss:0.403, val_acc:0.870]
Epoch [21/120    avg_loss:0.315, val_acc:0.865]
Epoch [22/120    avg_loss:0.266, val_acc:0.895]
Epoch [23/120    avg_loss:0.277, val_acc:0.882]
Epoch [24/120    avg_loss:0.316, val_acc:0.882]
Epoch [25/120    avg_loss:0.252, val_acc:0.891]
Epoch [26/120    avg_loss:0.257, val_acc:0.869]
Epoch [27/120    avg_loss:0.393, val_acc:0.867]
Epoch [28/120    avg_loss:0.269, val_acc:0.896]
Epoch [29/120    avg_loss:0.198, val_acc:0.917]
Epoch [30/120    avg_loss:0.197, val_acc:0.874]
Epoch [31/120    avg_loss:0.221, val_acc:0.868]
Epoch [32/120    avg_loss:0.245, val_acc:0.882]
Epoch [33/120    avg_loss:0.239, val_acc:0.916]
Epoch [34/120    avg_loss:0.172, val_acc:0.904]
Epoch [35/120    avg_loss:0.137, val_acc:0.919]
Epoch [36/120    avg_loss:0.128, val_acc:0.940]
Epoch [37/120    avg_loss:0.094, val_acc:0.935]
Epoch [38/120    avg_loss:0.140, val_acc:0.916]
Epoch [39/120    avg_loss:0.234, val_acc:0.902]
Epoch [40/120    avg_loss:0.120, val_acc:0.928]
Epoch [41/120    avg_loss:0.120, val_acc:0.938]
Epoch [42/120    avg_loss:0.092, val_acc:0.946]
Epoch [43/120    avg_loss:0.091, val_acc:0.945]
Epoch [44/120    avg_loss:0.114, val_acc:0.924]
Epoch [45/120    avg_loss:0.109, val_acc:0.935]
Epoch [46/120    avg_loss:0.091, val_acc:0.947]
Epoch [47/120    avg_loss:0.083, val_acc:0.948]
Epoch [48/120    avg_loss:0.065, val_acc:0.945]
Epoch [49/120    avg_loss:0.064, val_acc:0.952]
Epoch [50/120    avg_loss:0.061, val_acc:0.945]
Epoch [51/120    avg_loss:0.064, val_acc:0.949]
Epoch [52/120    avg_loss:0.067, val_acc:0.964]
Epoch [53/120    avg_loss:0.050, val_acc:0.954]
Epoch [54/120    avg_loss:0.057, val_acc:0.963]
Epoch [55/120    avg_loss:0.109, val_acc:0.925]
Epoch [56/120    avg_loss:0.085, val_acc:0.944]
Epoch [57/120    avg_loss:0.054, val_acc:0.957]
Epoch [58/120    avg_loss:0.050, val_acc:0.959]
Epoch [59/120    avg_loss:0.078, val_acc:0.907]
Epoch [60/120    avg_loss:0.139, val_acc:0.902]
Epoch [61/120    avg_loss:0.116, val_acc:0.944]
Epoch [62/120    avg_loss:0.114, val_acc:0.920]
Epoch [63/120    avg_loss:0.128, val_acc:0.945]
Epoch [64/120    avg_loss:0.074, val_acc:0.955]
Epoch [65/120    avg_loss:0.054, val_acc:0.948]
Epoch [66/120    avg_loss:0.048, val_acc:0.958]
Epoch [67/120    avg_loss:0.039, val_acc:0.954]
Epoch [68/120    avg_loss:0.038, val_acc:0.956]
Epoch [69/120    avg_loss:0.042, val_acc:0.961]
Epoch [70/120    avg_loss:0.034, val_acc:0.963]
Epoch [71/120    avg_loss:0.031, val_acc:0.963]
Epoch [72/120    avg_loss:0.028, val_acc:0.965]
Epoch [73/120    avg_loss:0.033, val_acc:0.963]
Epoch [74/120    avg_loss:0.028, val_acc:0.963]
Epoch [75/120    avg_loss:0.033, val_acc:0.961]
Epoch [76/120    avg_loss:0.030, val_acc:0.960]
Epoch [77/120    avg_loss:0.029, val_acc:0.964]
Epoch [78/120    avg_loss:0.028, val_acc:0.964]
Epoch [79/120    avg_loss:0.030, val_acc:0.964]
Epoch [80/120    avg_loss:0.031, val_acc:0.966]
Epoch [81/120    avg_loss:0.026, val_acc:0.965]
Epoch [82/120    avg_loss:0.026, val_acc:0.966]
Epoch [83/120    avg_loss:0.029, val_acc:0.965]
Epoch [84/120    avg_loss:0.029, val_acc:0.965]
Epoch [85/120    avg_loss:0.037, val_acc:0.964]
Epoch [86/120    avg_loss:0.030, val_acc:0.964]
Epoch [87/120    avg_loss:0.033, val_acc:0.965]
Epoch [88/120    avg_loss:0.029, val_acc:0.965]
Epoch [89/120    avg_loss:0.027, val_acc:0.966]
Epoch [90/120    avg_loss:0.026, val_acc:0.966]
Epoch [91/120    avg_loss:0.030, val_acc:0.965]
Epoch [92/120    avg_loss:0.026, val_acc:0.966]
Epoch [93/120    avg_loss:0.023, val_acc:0.965]
Epoch [94/120    avg_loss:0.027, val_acc:0.964]
Epoch [95/120    avg_loss:0.024, val_acc:0.966]
Epoch [96/120    avg_loss:0.020, val_acc:0.966]
Epoch [97/120    avg_loss:0.030, val_acc:0.965]
Epoch [98/120    avg_loss:0.023, val_acc:0.967]
Epoch [99/120    avg_loss:0.027, val_acc:0.966]
Epoch [100/120    avg_loss:0.026, val_acc:0.965]
Epoch [101/120    avg_loss:0.025, val_acc:0.966]
Epoch [102/120    avg_loss:0.026, val_acc:0.964]
Epoch [103/120    avg_loss:0.024, val_acc:0.966]
Epoch [104/120    avg_loss:0.026, val_acc:0.967]
Epoch [105/120    avg_loss:0.019, val_acc:0.966]
Epoch [106/120    avg_loss:0.024, val_acc:0.965]
Epoch [107/120    avg_loss:0.025, val_acc:0.968]
Epoch [108/120    avg_loss:0.021, val_acc:0.968]
Epoch [109/120    avg_loss:0.022, val_acc:0.966]
Epoch [110/120    avg_loss:0.022, val_acc:0.967]
Epoch [111/120    avg_loss:0.023, val_acc:0.966]
Epoch [112/120    avg_loss:0.030, val_acc:0.967]
Epoch [113/120    avg_loss:0.024, val_acc:0.968]
Epoch [114/120    avg_loss:0.031, val_acc:0.967]
Epoch [115/120    avg_loss:0.020, val_acc:0.967]
Epoch [116/120    avg_loss:0.022, val_acc:0.966]
Epoch [117/120    avg_loss:0.024, val_acc:0.969]
Epoch [118/120    avg_loss:0.024, val_acc:0.969]
Epoch [119/120    avg_loss:0.020, val_acc:0.968]
Epoch [120/120    avg_loss:0.026, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    1    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0 1251    0    7    1    1    0    0    0    5   18    2    0
     0    0    0]
 [   0    0    1  718    6    4    0    0    0    2    4    9    3    0
     0    0    0]
 [   0    0    0    0  211    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  425    0    2    0    0    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  649    0    0    0    1    4    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    5    0    0    2    0    0    0    0  850   13    5    0
     0    0    0]
 [   0    0    6    1    0    0    0    0    0    3   40 2145   15    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    1   12  513    0
     0    4    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
    54  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.1490514905149

F1 scores:
[       nan 0.975      0.98194662 0.97820163 0.96567506 0.97926267
 0.993114   0.96153846 1.         0.85       0.95720721 0.97234814
 0.9535316  1.         0.96740995 0.89296636 0.98823529]

Kappa:
0.9674938296323632
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f91f50627b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.728, val_acc:0.282]
Epoch [2/120    avg_loss:2.365, val_acc:0.429]
Epoch [3/120    avg_loss:2.109, val_acc:0.450]
Epoch [4/120    avg_loss:1.900, val_acc:0.547]
Epoch [5/120    avg_loss:1.788, val_acc:0.575]
Epoch [6/120    avg_loss:1.589, val_acc:0.624]
Epoch [7/120    avg_loss:1.511, val_acc:0.631]
Epoch [8/120    avg_loss:1.315, val_acc:0.626]
Epoch [9/120    avg_loss:1.173, val_acc:0.675]
Epoch [10/120    avg_loss:1.023, val_acc:0.708]
Epoch [11/120    avg_loss:0.895, val_acc:0.706]
Epoch [12/120    avg_loss:0.846, val_acc:0.774]
Epoch [13/120    avg_loss:0.716, val_acc:0.760]
Epoch [14/120    avg_loss:0.680, val_acc:0.792]
Epoch [15/120    avg_loss:0.583, val_acc:0.786]
Epoch [16/120    avg_loss:0.508, val_acc:0.808]
Epoch [17/120    avg_loss:0.475, val_acc:0.841]
Epoch [18/120    avg_loss:0.389, val_acc:0.846]
Epoch [19/120    avg_loss:0.368, val_acc:0.848]
Epoch [20/120    avg_loss:0.353, val_acc:0.846]
Epoch [21/120    avg_loss:0.322, val_acc:0.822]
Epoch [22/120    avg_loss:0.277, val_acc:0.851]
Epoch [23/120    avg_loss:0.265, val_acc:0.866]
Epoch [24/120    avg_loss:0.221, val_acc:0.880]
Epoch [25/120    avg_loss:0.253, val_acc:0.886]
Epoch [26/120    avg_loss:0.328, val_acc:0.823]
Epoch [27/120    avg_loss:0.370, val_acc:0.823]
Epoch [28/120    avg_loss:0.259, val_acc:0.858]
Epoch [29/120    avg_loss:0.200, val_acc:0.899]
Epoch [30/120    avg_loss:0.147, val_acc:0.911]
Epoch [31/120    avg_loss:0.154, val_acc:0.895]
Epoch [32/120    avg_loss:0.164, val_acc:0.911]
Epoch [33/120    avg_loss:0.139, val_acc:0.920]
Epoch [34/120    avg_loss:0.139, val_acc:0.900]
Epoch [35/120    avg_loss:0.119, val_acc:0.923]
Epoch [36/120    avg_loss:0.102, val_acc:0.936]
Epoch [37/120    avg_loss:0.104, val_acc:0.917]
Epoch [38/120    avg_loss:0.122, val_acc:0.927]
Epoch [39/120    avg_loss:0.088, val_acc:0.927]
Epoch [40/120    avg_loss:0.090, val_acc:0.925]
Epoch [41/120    avg_loss:0.115, val_acc:0.935]
Epoch [42/120    avg_loss:0.142, val_acc:0.931]
Epoch [43/120    avg_loss:0.157, val_acc:0.919]
Epoch [44/120    avg_loss:0.131, val_acc:0.918]
Epoch [45/120    avg_loss:0.089, val_acc:0.927]
Epoch [46/120    avg_loss:0.078, val_acc:0.932]
Epoch [47/120    avg_loss:0.101, val_acc:0.944]
Epoch [48/120    avg_loss:0.083, val_acc:0.935]
Epoch [49/120    avg_loss:0.107, val_acc:0.928]
Epoch [50/120    avg_loss:0.109, val_acc:0.938]
Epoch [51/120    avg_loss:0.109, val_acc:0.874]
Epoch [52/120    avg_loss:0.125, val_acc:0.942]
Epoch [53/120    avg_loss:0.060, val_acc:0.957]
Epoch [54/120    avg_loss:0.048, val_acc:0.955]
Epoch [55/120    avg_loss:0.046, val_acc:0.965]
Epoch [56/120    avg_loss:0.050, val_acc:0.957]
Epoch [57/120    avg_loss:0.062, val_acc:0.956]
Epoch [58/120    avg_loss:0.073, val_acc:0.949]
Epoch [59/120    avg_loss:0.057, val_acc:0.941]
Epoch [60/120    avg_loss:0.052, val_acc:0.960]
Epoch [61/120    avg_loss:0.042, val_acc:0.944]
Epoch [62/120    avg_loss:0.037, val_acc:0.967]
Epoch [63/120    avg_loss:0.051, val_acc:0.958]
Epoch [64/120    avg_loss:0.041, val_acc:0.963]
Epoch [65/120    avg_loss:0.037, val_acc:0.958]
Epoch [66/120    avg_loss:0.042, val_acc:0.954]
Epoch [67/120    avg_loss:0.037, val_acc:0.961]
Epoch [68/120    avg_loss:0.033, val_acc:0.967]
Epoch [69/120    avg_loss:0.031, val_acc:0.969]
Epoch [70/120    avg_loss:0.025, val_acc:0.969]
Epoch [71/120    avg_loss:0.021, val_acc:0.958]
Epoch [72/120    avg_loss:0.026, val_acc:0.971]
Epoch [73/120    avg_loss:0.026, val_acc:0.976]
Epoch [74/120    avg_loss:0.020, val_acc:0.973]
Epoch [75/120    avg_loss:0.020, val_acc:0.971]
Epoch [76/120    avg_loss:0.022, val_acc:0.968]
Epoch [77/120    avg_loss:0.026, val_acc:0.966]
Epoch [78/120    avg_loss:0.031, val_acc:0.947]
Epoch [79/120    avg_loss:0.024, val_acc:0.969]
Epoch [80/120    avg_loss:0.021, val_acc:0.971]
Epoch [81/120    avg_loss:0.022, val_acc:0.958]
Epoch [82/120    avg_loss:0.046, val_acc:0.952]
Epoch [83/120    avg_loss:0.035, val_acc:0.963]
Epoch [84/120    avg_loss:1.000, val_acc:0.742]
Epoch [85/120    avg_loss:0.638, val_acc:0.832]
Epoch [86/120    avg_loss:0.277, val_acc:0.881]
Epoch [87/120    avg_loss:0.154, val_acc:0.892]
Epoch [88/120    avg_loss:0.136, val_acc:0.901]
Epoch [89/120    avg_loss:0.139, val_acc:0.905]
Epoch [90/120    avg_loss:0.140, val_acc:0.911]
Epoch [91/120    avg_loss:0.130, val_acc:0.917]
Epoch [92/120    avg_loss:0.118, val_acc:0.911]
Epoch [93/120    avg_loss:0.102, val_acc:0.918]
Epoch [94/120    avg_loss:0.108, val_acc:0.918]
Epoch [95/120    avg_loss:0.102, val_acc:0.925]
Epoch [96/120    avg_loss:0.095, val_acc:0.925]
Epoch [97/120    avg_loss:0.102, val_acc:0.929]
Epoch [98/120    avg_loss:0.084, val_acc:0.927]
Epoch [99/120    avg_loss:0.084, val_acc:0.927]
Epoch [100/120    avg_loss:0.087, val_acc:0.929]
Epoch [101/120    avg_loss:0.073, val_acc:0.928]
Epoch [102/120    avg_loss:0.090, val_acc:0.928]
Epoch [103/120    avg_loss:0.078, val_acc:0.928]
Epoch [104/120    avg_loss:0.082, val_acc:0.928]
Epoch [105/120    avg_loss:0.080, val_acc:0.928]
Epoch [106/120    avg_loss:0.092, val_acc:0.928]
Epoch [107/120    avg_loss:0.079, val_acc:0.928]
Epoch [108/120    avg_loss:0.085, val_acc:0.929]
Epoch [109/120    avg_loss:0.073, val_acc:0.929]
Epoch [110/120    avg_loss:0.079, val_acc:0.930]
Epoch [111/120    avg_loss:0.076, val_acc:0.929]
Epoch [112/120    avg_loss:0.068, val_acc:0.930]
Epoch [113/120    avg_loss:0.088, val_acc:0.930]
Epoch [114/120    avg_loss:0.076, val_acc:0.930]
Epoch [115/120    avg_loss:0.073, val_acc:0.930]
Epoch [116/120    avg_loss:0.070, val_acc:0.930]
Epoch [117/120    avg_loss:0.082, val_acc:0.930]
Epoch [118/120    avg_loss:0.084, val_acc:0.930]
Epoch [119/120    avg_loss:0.077, val_acc:0.930]
Epoch [120/120    avg_loss:0.079, val_acc:0.930]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0 1175   32    4    0    1    0    0    0   43   25    5    0
     0    0    0]
 [   0    0    0  702   12    0    5    0    0    5    1    0   18    3
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    1    0    0    0    0    0    0
     8    0    0]
 [   0    0    0    0    1    1  650    0    0    0    0    3    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   13    3    3    2    5    0    0    0  789   50    6    0
     1    3    0]
 [   0    2   13   34    7    2    1    0    0    0   29 2067   44    1
     2    0    8]
 [   0    0    0   10    3    1    0    0    0    0    0    0  515    0
     0    2    3]
 [   0    0    0    0    1    1    0    0    0    0    0    0    0  183
     0    0    0]
 [   0    0    0    0    0   22    0    0    0    4    0    0    0    0
  1097   16    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    47  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.39566395663957

F1 scores:
[       nan 0.96385542 0.94529364 0.91884817 0.9321663  0.95730337
 0.98410295 0.98039216 0.9953271  0.74418605 0.90846287 0.94925373
 0.91393079 0.98387097 0.95557491 0.89686099 0.93854749]

Kappa:
0.9362545592460196
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f61b7fed8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.718, val_acc:0.390]
Epoch [2/120    avg_loss:2.372, val_acc:0.436]
Epoch [3/120    avg_loss:2.152, val_acc:0.499]
Epoch [4/120    avg_loss:1.961, val_acc:0.521]
Epoch [5/120    avg_loss:1.807, val_acc:0.551]
Epoch [6/120    avg_loss:1.661, val_acc:0.564]
Epoch [7/120    avg_loss:1.484, val_acc:0.625]
Epoch [8/120    avg_loss:1.304, val_acc:0.656]
Epoch [9/120    avg_loss:1.169, val_acc:0.681]
Epoch [10/120    avg_loss:1.039, val_acc:0.726]
Epoch [11/120    avg_loss:0.888, val_acc:0.754]
Epoch [12/120    avg_loss:0.842, val_acc:0.783]
Epoch [13/120    avg_loss:0.695, val_acc:0.822]
Epoch [14/120    avg_loss:0.649, val_acc:0.776]
Epoch [15/120    avg_loss:0.641, val_acc:0.811]
Epoch [16/120    avg_loss:0.545, val_acc:0.822]
Epoch [17/120    avg_loss:0.512, val_acc:0.832]
Epoch [18/120    avg_loss:0.411, val_acc:0.864]
Epoch [19/120    avg_loss:0.403, val_acc:0.870]
Epoch [20/120    avg_loss:0.341, val_acc:0.874]
Epoch [21/120    avg_loss:0.417, val_acc:0.799]
Epoch [22/120    avg_loss:0.369, val_acc:0.899]
Epoch [23/120    avg_loss:0.355, val_acc:0.904]
Epoch [24/120    avg_loss:0.253, val_acc:0.884]
Epoch [25/120    avg_loss:0.239, val_acc:0.901]
Epoch [26/120    avg_loss:0.230, val_acc:0.886]
Epoch [27/120    avg_loss:0.235, val_acc:0.911]
Epoch [28/120    avg_loss:0.191, val_acc:0.939]
Epoch [29/120    avg_loss:0.189, val_acc:0.908]
Epoch [30/120    avg_loss:0.183, val_acc:0.908]
Epoch [31/120    avg_loss:0.172, val_acc:0.927]
Epoch [32/120    avg_loss:0.154, val_acc:0.917]
Epoch [33/120    avg_loss:0.194, val_acc:0.943]
Epoch [34/120    avg_loss:0.150, val_acc:0.939]
Epoch [35/120    avg_loss:0.123, val_acc:0.952]
Epoch [36/120    avg_loss:0.128, val_acc:0.941]
Epoch [37/120    avg_loss:0.088, val_acc:0.954]
Epoch [38/120    avg_loss:0.073, val_acc:0.967]
Epoch [39/120    avg_loss:0.086, val_acc:0.958]
Epoch [40/120    avg_loss:0.095, val_acc:0.955]
Epoch [41/120    avg_loss:0.092, val_acc:0.966]
Epoch [42/120    avg_loss:0.082, val_acc:0.933]
Epoch [43/120    avg_loss:0.093, val_acc:0.952]
Epoch [44/120    avg_loss:0.106, val_acc:0.954]
Epoch [45/120    avg_loss:0.085, val_acc:0.953]
Epoch [46/120    avg_loss:0.077, val_acc:0.964]
Epoch [47/120    avg_loss:0.082, val_acc:0.959]
Epoch [48/120    avg_loss:0.076, val_acc:0.969]
Epoch [49/120    avg_loss:0.078, val_acc:0.933]
Epoch [50/120    avg_loss:0.090, val_acc:0.966]
Epoch [51/120    avg_loss:0.094, val_acc:0.967]
Epoch [52/120    avg_loss:0.082, val_acc:0.958]
Epoch [53/120    avg_loss:0.061, val_acc:0.973]
Epoch [54/120    avg_loss:0.055, val_acc:0.978]
Epoch [55/120    avg_loss:0.041, val_acc:0.968]
Epoch [56/120    avg_loss:0.040, val_acc:0.979]
Epoch [57/120    avg_loss:0.039, val_acc:0.975]
Epoch [58/120    avg_loss:0.044, val_acc:0.970]
Epoch [59/120    avg_loss:0.057, val_acc:0.962]
Epoch [60/120    avg_loss:0.045, val_acc:0.971]
Epoch [61/120    avg_loss:0.037, val_acc:0.978]
Epoch [62/120    avg_loss:0.046, val_acc:0.958]
Epoch [63/120    avg_loss:0.063, val_acc:0.944]
Epoch [64/120    avg_loss:0.125, val_acc:0.943]
Epoch [65/120    avg_loss:0.052, val_acc:0.967]
Epoch [66/120    avg_loss:0.050, val_acc:0.972]
Epoch [67/120    avg_loss:0.042, val_acc:0.968]
Epoch [68/120    avg_loss:0.043, val_acc:0.965]
Epoch [69/120    avg_loss:0.046, val_acc:0.971]
Epoch [70/120    avg_loss:0.034, val_acc:0.975]
Epoch [71/120    avg_loss:0.029, val_acc:0.978]
Epoch [72/120    avg_loss:0.027, val_acc:0.982]
Epoch [73/120    avg_loss:0.029, val_acc:0.982]
Epoch [74/120    avg_loss:0.026, val_acc:0.981]
Epoch [75/120    avg_loss:0.021, val_acc:0.980]
Epoch [76/120    avg_loss:0.024, val_acc:0.979]
Epoch [77/120    avg_loss:0.019, val_acc:0.980]
Epoch [78/120    avg_loss:0.022, val_acc:0.982]
Epoch [79/120    avg_loss:0.023, val_acc:0.980]
Epoch [80/120    avg_loss:0.024, val_acc:0.980]
Epoch [81/120    avg_loss:0.020, val_acc:0.981]
Epoch [82/120    avg_loss:0.021, val_acc:0.981]
Epoch [83/120    avg_loss:0.020, val_acc:0.981]
Epoch [84/120    avg_loss:0.022, val_acc:0.981]
Epoch [85/120    avg_loss:0.023, val_acc:0.980]
Epoch [86/120    avg_loss:0.023, val_acc:0.980]
Epoch [87/120    avg_loss:0.021, val_acc:0.982]
Epoch [88/120    avg_loss:0.021, val_acc:0.983]
Epoch [89/120    avg_loss:0.017, val_acc:0.981]
Epoch [90/120    avg_loss:0.019, val_acc:0.981]
Epoch [91/120    avg_loss:0.022, val_acc:0.983]
Epoch [92/120    avg_loss:0.018, val_acc:0.983]
Epoch [93/120    avg_loss:0.022, val_acc:0.981]
Epoch [94/120    avg_loss:0.019, val_acc:0.984]
Epoch [95/120    avg_loss:0.018, val_acc:0.983]
Epoch [96/120    avg_loss:0.017, val_acc:0.983]
Epoch [97/120    avg_loss:0.019, val_acc:0.982]
Epoch [98/120    avg_loss:0.019, val_acc:0.980]
Epoch [99/120    avg_loss:0.016, val_acc:0.982]
Epoch [100/120    avg_loss:0.020, val_acc:0.982]
Epoch [101/120    avg_loss:0.016, val_acc:0.981]
Epoch [102/120    avg_loss:0.017, val_acc:0.984]
Epoch [103/120    avg_loss:0.017, val_acc:0.985]
Epoch [104/120    avg_loss:0.019, val_acc:0.983]
Epoch [105/120    avg_loss:0.018, val_acc:0.982]
Epoch [106/120    avg_loss:0.019, val_acc:0.981]
Epoch [107/120    avg_loss:0.016, val_acc:0.982]
Epoch [108/120    avg_loss:0.013, val_acc:0.982]
Epoch [109/120    avg_loss:0.015, val_acc:0.983]
Epoch [110/120    avg_loss:0.016, val_acc:0.985]
Epoch [111/120    avg_loss:0.017, val_acc:0.987]
Epoch [112/120    avg_loss:0.020, val_acc:0.984]
Epoch [113/120    avg_loss:0.016, val_acc:0.983]
Epoch [114/120    avg_loss:0.018, val_acc:0.983]
Epoch [115/120    avg_loss:0.016, val_acc:0.980]
Epoch [116/120    avg_loss:0.015, val_acc:0.983]
Epoch [117/120    avg_loss:0.016, val_acc:0.983]
Epoch [118/120    avg_loss:0.014, val_acc:0.983]
Epoch [119/120    avg_loss:0.013, val_acc:0.982]
Epoch [120/120    avg_loss:0.018, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1262    0    1    0    1    0    0    0    1   20    0    0
     0    0    0]
 [   0    0    0  699    1    2    0    0    0    4    1    7   31    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    5    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   19    0    0    0    1    0    0    0  830   19    6    0
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    7 2195    0    1
     0    0    0]
 [   0    0    0    4    0    0    0    0    0    0    6    0  519    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1138    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    28  319    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.98373983739837

F1 scores:
[       nan 0.98765432 0.98324893 0.96413793 0.9953271  0.98398169
 0.99619193 0.90909091 0.99883586 0.9        0.96511628 0.98585223
 0.95054945 0.9919571  0.98699046 0.95223881 0.98224852]

Kappa:
0.9769979468604781
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa989492860>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.731, val_acc:0.368]
Epoch [2/120    avg_loss:2.345, val_acc:0.484]
Epoch [3/120    avg_loss:2.117, val_acc:0.563]
Epoch [4/120    avg_loss:1.926, val_acc:0.578]
Epoch [5/120    avg_loss:1.799, val_acc:0.579]
Epoch [6/120    avg_loss:1.640, val_acc:0.587]
Epoch [7/120    avg_loss:1.512, val_acc:0.575]
Epoch [8/120    avg_loss:1.419, val_acc:0.672]
Epoch [9/120    avg_loss:1.282, val_acc:0.649]
Epoch [10/120    avg_loss:1.296, val_acc:0.629]
Epoch [11/120    avg_loss:1.033, val_acc:0.707]
Epoch [12/120    avg_loss:0.980, val_acc:0.740]
Epoch [13/120    avg_loss:0.821, val_acc:0.771]
Epoch [14/120    avg_loss:0.703, val_acc:0.793]
Epoch [15/120    avg_loss:0.740, val_acc:0.804]
Epoch [16/120    avg_loss:0.594, val_acc:0.747]
Epoch [17/120    avg_loss:0.523, val_acc:0.801]
Epoch [18/120    avg_loss:0.432, val_acc:0.830]
Epoch [19/120    avg_loss:0.389, val_acc:0.859]
Epoch [20/120    avg_loss:0.380, val_acc:0.850]
Epoch [21/120    avg_loss:0.466, val_acc:0.839]
Epoch [22/120    avg_loss:0.318, val_acc:0.864]
Epoch [23/120    avg_loss:0.312, val_acc:0.873]
Epoch [24/120    avg_loss:0.239, val_acc:0.897]
Epoch [25/120    avg_loss:0.266, val_acc:0.889]
Epoch [26/120    avg_loss:0.250, val_acc:0.877]
Epoch [27/120    avg_loss:0.238, val_acc:0.876]
Epoch [28/120    avg_loss:0.187, val_acc:0.917]
Epoch [29/120    avg_loss:0.158, val_acc:0.908]
Epoch [30/120    avg_loss:0.136, val_acc:0.907]
Epoch [31/120    avg_loss:0.205, val_acc:0.896]
Epoch [32/120    avg_loss:0.145, val_acc:0.941]
Epoch [33/120    avg_loss:0.129, val_acc:0.941]
Epoch [34/120    avg_loss:0.147, val_acc:0.913]
Epoch [35/120    avg_loss:0.136, val_acc:0.940]
Epoch [36/120    avg_loss:0.151, val_acc:0.913]
Epoch [37/120    avg_loss:0.167, val_acc:0.912]
Epoch [38/120    avg_loss:0.139, val_acc:0.932]
Epoch [39/120    avg_loss:0.107, val_acc:0.941]
Epoch [40/120    avg_loss:0.103, val_acc:0.902]
Epoch [41/120    avg_loss:0.099, val_acc:0.952]
Epoch [42/120    avg_loss:0.084, val_acc:0.958]
Epoch [43/120    avg_loss:0.072, val_acc:0.949]
Epoch [44/120    avg_loss:0.077, val_acc:0.955]
Epoch [45/120    avg_loss:0.093, val_acc:0.940]
Epoch [46/120    avg_loss:0.073, val_acc:0.956]
Epoch [47/120    avg_loss:0.086, val_acc:0.954]
Epoch [48/120    avg_loss:0.111, val_acc:0.919]
Epoch [49/120    avg_loss:0.109, val_acc:0.938]
Epoch [50/120    avg_loss:0.098, val_acc:0.938]
Epoch [51/120    avg_loss:0.153, val_acc:0.916]
Epoch [52/120    avg_loss:0.082, val_acc:0.940]
Epoch [53/120    avg_loss:0.079, val_acc:0.942]
Epoch [54/120    avg_loss:0.089, val_acc:0.963]
Epoch [55/120    avg_loss:0.057, val_acc:0.959]
Epoch [56/120    avg_loss:0.068, val_acc:0.944]
Epoch [57/120    avg_loss:0.071, val_acc:0.955]
Epoch [58/120    avg_loss:0.075, val_acc:0.943]
Epoch [59/120    avg_loss:0.043, val_acc:0.955]
Epoch [60/120    avg_loss:0.054, val_acc:0.964]
Epoch [61/120    avg_loss:0.043, val_acc:0.953]
Epoch [62/120    avg_loss:0.037, val_acc:0.960]
Epoch [63/120    avg_loss:0.125, val_acc:0.911]
Epoch [64/120    avg_loss:0.182, val_acc:0.932]
Epoch [65/120    avg_loss:0.092, val_acc:0.951]
Epoch [66/120    avg_loss:0.064, val_acc:0.946]
Epoch [67/120    avg_loss:0.050, val_acc:0.954]
Epoch [68/120    avg_loss:0.051, val_acc:0.958]
Epoch [69/120    avg_loss:0.037, val_acc:0.963]
Epoch [70/120    avg_loss:0.046, val_acc:0.960]
Epoch [71/120    avg_loss:0.038, val_acc:0.966]
Epoch [72/120    avg_loss:0.023, val_acc:0.969]
Epoch [73/120    avg_loss:0.033, val_acc:0.967]
Epoch [74/120    avg_loss:0.032, val_acc:0.975]
Epoch [75/120    avg_loss:0.042, val_acc:0.963]
Epoch [76/120    avg_loss:0.032, val_acc:0.979]
Epoch [77/120    avg_loss:0.022, val_acc:0.981]
Epoch [78/120    avg_loss:0.028, val_acc:0.976]
Epoch [79/120    avg_loss:0.019, val_acc:0.974]
Epoch [80/120    avg_loss:0.021, val_acc:0.975]
Epoch [81/120    avg_loss:0.032, val_acc:0.967]
Epoch [82/120    avg_loss:0.028, val_acc:0.973]
Epoch [83/120    avg_loss:0.035, val_acc:0.965]
Epoch [84/120    avg_loss:0.069, val_acc:0.971]
Epoch [85/120    avg_loss:0.054, val_acc:0.967]
Epoch [86/120    avg_loss:0.029, val_acc:0.957]
Epoch [87/120    avg_loss:0.028, val_acc:0.975]
Epoch [88/120    avg_loss:0.022, val_acc:0.979]
Epoch [89/120    avg_loss:0.018, val_acc:0.968]
Epoch [90/120    avg_loss:0.022, val_acc:0.980]
Epoch [91/120    avg_loss:0.016, val_acc:0.979]
Epoch [92/120    avg_loss:0.016, val_acc:0.980]
Epoch [93/120    avg_loss:0.015, val_acc:0.981]
Epoch [94/120    avg_loss:0.016, val_acc:0.981]
Epoch [95/120    avg_loss:0.015, val_acc:0.981]
Epoch [96/120    avg_loss:0.012, val_acc:0.985]
Epoch [97/120    avg_loss:0.012, val_acc:0.983]
Epoch [98/120    avg_loss:0.010, val_acc:0.984]
Epoch [99/120    avg_loss:0.013, val_acc:0.986]
Epoch [100/120    avg_loss:0.014, val_acc:0.981]
Epoch [101/120    avg_loss:0.011, val_acc:0.983]
Epoch [102/120    avg_loss:0.011, val_acc:0.980]
Epoch [103/120    avg_loss:0.010, val_acc:0.983]
Epoch [104/120    avg_loss:0.012, val_acc:0.982]
Epoch [105/120    avg_loss:0.013, val_acc:0.985]
Epoch [106/120    avg_loss:0.015, val_acc:0.985]
Epoch [107/120    avg_loss:0.012, val_acc:0.984]
Epoch [108/120    avg_loss:0.012, val_acc:0.987]
Epoch [109/120    avg_loss:0.008, val_acc:0.985]
Epoch [110/120    avg_loss:0.011, val_acc:0.983]
Epoch [111/120    avg_loss:0.010, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.986]
Epoch [113/120    avg_loss:0.012, val_acc:0.985]
Epoch [114/120    avg_loss:0.013, val_acc:0.982]
Epoch [115/120    avg_loss:0.009, val_acc:0.983]
Epoch [116/120    avg_loss:0.011, val_acc:0.982]
Epoch [117/120    avg_loss:0.008, val_acc:0.981]
Epoch [118/120    avg_loss:0.011, val_acc:0.982]
Epoch [119/120    avg_loss:0.014, val_acc:0.982]
Epoch [120/120    avg_loss:0.009, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1256    3    1    0    0    0    0    0    2   23    0    0
     0    0    0]
 [   0    0    0  738    2    1    0    0    0    3    0    0    3    0
     0    0    0]
 [   0    0    0    7  206    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    2    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    2    2    0    0    0  845   15    0    0
     0    1    0]
 [   0    0    4    5    0    0    5    0    0    0    5 2187    3    1
     0    0    0]
 [   0    0    0    8    0    0    0    0    0    0    0    0  524    0
     0    0    2]
 [   0    0    0    1    0    0    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    1    4    0    0    0    0    0    0    0
  1129    5    0]
 [   0    0    0    0    0    1   19    0    0    0    0    0    0    0
    20  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.11382113821138

F1 scores:
[       nan 0.93975904 0.98240125 0.97813121 0.97630332 0.98500577
 0.97615499 0.96153846 0.99649942 0.92307692 0.97857556 0.98580122
 0.98496241 0.99459459 0.98430689 0.93030303 0.98823529]

Kappa:
0.9784867705801953
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9bf73f5860>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.645, val_acc:0.358]
Epoch [2/120    avg_loss:2.306, val_acc:0.444]
Epoch [3/120    avg_loss:2.142, val_acc:0.532]
Epoch [4/120    avg_loss:1.933, val_acc:0.597]
Epoch [5/120    avg_loss:1.799, val_acc:0.599]
Epoch [6/120    avg_loss:1.661, val_acc:0.610]
Epoch [7/120    avg_loss:1.548, val_acc:0.619]
Epoch [8/120    avg_loss:1.460, val_acc:0.649]
Epoch [9/120    avg_loss:1.342, val_acc:0.666]
Epoch [10/120    avg_loss:1.224, val_acc:0.685]
Epoch [11/120    avg_loss:1.042, val_acc:0.769]
Epoch [12/120    avg_loss:0.917, val_acc:0.766]
Epoch [13/120    avg_loss:0.935, val_acc:0.734]
Epoch [14/120    avg_loss:0.779, val_acc:0.788]
Epoch [15/120    avg_loss:0.715, val_acc:0.765]
Epoch [16/120    avg_loss:0.657, val_acc:0.775]
Epoch [17/120    avg_loss:0.588, val_acc:0.833]
Epoch [18/120    avg_loss:0.452, val_acc:0.842]
Epoch [19/120    avg_loss:0.437, val_acc:0.843]
Epoch [20/120    avg_loss:0.425, val_acc:0.824]
Epoch [21/120    avg_loss:0.516, val_acc:0.812]
Epoch [22/120    avg_loss:0.415, val_acc:0.844]
Epoch [23/120    avg_loss:0.302, val_acc:0.864]
Epoch [24/120    avg_loss:0.310, val_acc:0.856]
Epoch [25/120    avg_loss:0.370, val_acc:0.870]
Epoch [26/120    avg_loss:0.362, val_acc:0.863]
Epoch [27/120    avg_loss:0.338, val_acc:0.890]
Epoch [28/120    avg_loss:0.287, val_acc:0.911]
Epoch [29/120    avg_loss:0.211, val_acc:0.918]
Epoch [30/120    avg_loss:0.184, val_acc:0.900]
Epoch [31/120    avg_loss:0.171, val_acc:0.902]
Epoch [32/120    avg_loss:0.168, val_acc:0.921]
Epoch [33/120    avg_loss:0.196, val_acc:0.939]
Epoch [34/120    avg_loss:0.151, val_acc:0.935]
Epoch [35/120    avg_loss:0.150, val_acc:0.924]
Epoch [36/120    avg_loss:0.141, val_acc:0.931]
Epoch [37/120    avg_loss:0.124, val_acc:0.936]
Epoch [38/120    avg_loss:0.125, val_acc:0.937]
Epoch [39/120    avg_loss:0.104, val_acc:0.943]
Epoch [40/120    avg_loss:0.117, val_acc:0.944]
Epoch [41/120    avg_loss:0.112, val_acc:0.910]
Epoch [42/120    avg_loss:0.101, val_acc:0.959]
Epoch [43/120    avg_loss:0.087, val_acc:0.944]
Epoch [44/120    avg_loss:0.077, val_acc:0.958]
Epoch [45/120    avg_loss:0.094, val_acc:0.962]
Epoch [46/120    avg_loss:0.076, val_acc:0.953]
Epoch [47/120    avg_loss:0.059, val_acc:0.958]
Epoch [48/120    avg_loss:0.066, val_acc:0.957]
Epoch [49/120    avg_loss:0.058, val_acc:0.963]
Epoch [50/120    avg_loss:0.046, val_acc:0.966]
Epoch [51/120    avg_loss:0.040, val_acc:0.967]
Epoch [52/120    avg_loss:0.049, val_acc:0.944]
Epoch [53/120    avg_loss:0.044, val_acc:0.969]
Epoch [54/120    avg_loss:0.052, val_acc:0.957]
Epoch [55/120    avg_loss:0.041, val_acc:0.967]
Epoch [56/120    avg_loss:0.056, val_acc:0.949]
Epoch [57/120    avg_loss:0.055, val_acc:0.966]
Epoch [58/120    avg_loss:0.052, val_acc:0.962]
Epoch [59/120    avg_loss:0.052, val_acc:0.960]
Epoch [60/120    avg_loss:0.084, val_acc:0.952]
Epoch [61/120    avg_loss:0.106, val_acc:0.928]
Epoch [62/120    avg_loss:0.110, val_acc:0.951]
Epoch [63/120    avg_loss:0.078, val_acc:0.943]
Epoch [64/120    avg_loss:0.060, val_acc:0.963]
Epoch [65/120    avg_loss:0.042, val_acc:0.966]
Epoch [66/120    avg_loss:0.031, val_acc:0.974]
Epoch [67/120    avg_loss:0.034, val_acc:0.960]
Epoch [68/120    avg_loss:0.031, val_acc:0.972]
Epoch [69/120    avg_loss:0.030, val_acc:0.967]
Epoch [70/120    avg_loss:0.041, val_acc:0.967]
Epoch [71/120    avg_loss:0.039, val_acc:0.941]
Epoch [72/120    avg_loss:0.032, val_acc:0.962]
Epoch [73/120    avg_loss:0.034, val_acc:0.972]
Epoch [74/120    avg_loss:0.026, val_acc:0.974]
Epoch [75/120    avg_loss:0.024, val_acc:0.973]
Epoch [76/120    avg_loss:0.046, val_acc:0.959]
Epoch [77/120    avg_loss:0.038, val_acc:0.966]
Epoch [78/120    avg_loss:0.034, val_acc:0.963]
Epoch [79/120    avg_loss:0.030, val_acc:0.974]
Epoch [80/120    avg_loss:0.034, val_acc:0.974]
Epoch [81/120    avg_loss:0.031, val_acc:0.974]
Epoch [82/120    avg_loss:0.021, val_acc:0.973]
Epoch [83/120    avg_loss:0.017, val_acc:0.964]
Epoch [84/120    avg_loss:0.024, val_acc:0.973]
Epoch [85/120    avg_loss:0.018, val_acc:0.973]
Epoch [86/120    avg_loss:0.017, val_acc:0.974]
Epoch [87/120    avg_loss:0.013, val_acc:0.978]
Epoch [88/120    avg_loss:0.021, val_acc:0.975]
Epoch [89/120    avg_loss:0.030, val_acc:0.972]
Epoch [90/120    avg_loss:0.046, val_acc:0.965]
Epoch [91/120    avg_loss:0.017, val_acc:0.971]
Epoch [92/120    avg_loss:0.018, val_acc:0.974]
Epoch [93/120    avg_loss:0.024, val_acc:0.946]
Epoch [94/120    avg_loss:0.016, val_acc:0.980]
Epoch [95/120    avg_loss:0.012, val_acc:0.972]
Epoch [96/120    avg_loss:0.016, val_acc:0.970]
Epoch [97/120    avg_loss:0.020, val_acc:0.976]
Epoch [98/120    avg_loss:0.031, val_acc:0.966]
Epoch [99/120    avg_loss:0.017, val_acc:0.980]
Epoch [100/120    avg_loss:0.024, val_acc:0.973]
Epoch [101/120    avg_loss:0.015, val_acc:0.976]
Epoch [102/120    avg_loss:0.013, val_acc:0.976]
Epoch [103/120    avg_loss:0.014, val_acc:0.973]
Epoch [104/120    avg_loss:0.016, val_acc:0.978]
Epoch [105/120    avg_loss:0.010, val_acc:0.980]
Epoch [106/120    avg_loss:0.015, val_acc:0.980]
Epoch [107/120    avg_loss:0.012, val_acc:0.980]
Epoch [108/120    avg_loss:0.015, val_acc:0.978]
Epoch [109/120    avg_loss:0.012, val_acc:0.981]
Epoch [110/120    avg_loss:0.013, val_acc:0.978]
Epoch [111/120    avg_loss:0.011, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.984]
Epoch [113/120    avg_loss:0.010, val_acc:0.971]
Epoch [114/120    avg_loss:0.019, val_acc:0.982]
Epoch [115/120    avg_loss:0.010, val_acc:0.975]
Epoch [116/120    avg_loss:0.021, val_acc:0.984]
Epoch [117/120    avg_loss:0.020, val_acc:0.978]
Epoch [118/120    avg_loss:0.018, val_acc:0.978]
Epoch [119/120    avg_loss:0.037, val_acc:0.963]
Epoch [120/120    avg_loss:0.022, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1239    0    5    0    1    0    0    0   13   27    0    0
     0    0    0]
 [   0    0    2  710    6    0    0    0    0    0   11   12    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    3    0    0    2    0    0    0    0  863    5    0    0
     0    2    0]
 [   0    0    7    2    0    0    0    0    0    0   25 2175    0    1
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0   17    1  508    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    0   38    0    0    0    0    0    0    0
     3  306    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.71273712737127

F1 scores:
[       nan 0.975      0.97674419 0.97127223 0.97482838 0.99655568
 0.9704142  0.98039216 1.         0.97142857 0.95623269 0.9817197
 0.96761905 0.99730458 0.99516058 0.91891892 0.98224852]

Kappa:
0.9739199676397107
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4dc112d8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.686, val_acc:0.407]
Epoch [2/120    avg_loss:2.324, val_acc:0.525]
Epoch [3/120    avg_loss:2.116, val_acc:0.553]
Epoch [4/120    avg_loss:1.956, val_acc:0.580]
Epoch [5/120    avg_loss:1.765, val_acc:0.578]
Epoch [6/120    avg_loss:1.616, val_acc:0.610]
Epoch [7/120    avg_loss:1.512, val_acc:0.590]
Epoch [8/120    avg_loss:1.333, val_acc:0.643]
Epoch [9/120    avg_loss:1.185, val_acc:0.670]
Epoch [10/120    avg_loss:1.024, val_acc:0.665]
Epoch [11/120    avg_loss:0.961, val_acc:0.722]
Epoch [12/120    avg_loss:0.866, val_acc:0.735]
Epoch [13/120    avg_loss:0.706, val_acc:0.759]
Epoch [14/120    avg_loss:0.646, val_acc:0.745]
Epoch [15/120    avg_loss:0.599, val_acc:0.809]
Epoch [16/120    avg_loss:0.559, val_acc:0.778]
Epoch [17/120    avg_loss:0.589, val_acc:0.786]
Epoch [18/120    avg_loss:0.502, val_acc:0.805]
Epoch [19/120    avg_loss:0.431, val_acc:0.849]
Epoch [20/120    avg_loss:0.391, val_acc:0.852]
Epoch [21/120    avg_loss:0.424, val_acc:0.833]
Epoch [22/120    avg_loss:0.420, val_acc:0.809]
Epoch [23/120    avg_loss:0.344, val_acc:0.866]
Epoch [24/120    avg_loss:0.287, val_acc:0.881]
Epoch [25/120    avg_loss:0.214, val_acc:0.883]
Epoch [26/120    avg_loss:0.219, val_acc:0.890]
Epoch [27/120    avg_loss:0.235, val_acc:0.848]
Epoch [28/120    avg_loss:0.241, val_acc:0.890]
Epoch [29/120    avg_loss:0.250, val_acc:0.859]
Epoch [30/120    avg_loss:0.339, val_acc:0.872]
Epoch [31/120    avg_loss:0.246, val_acc:0.902]
Epoch [32/120    avg_loss:0.179, val_acc:0.914]
Epoch [33/120    avg_loss:0.163, val_acc:0.896]
Epoch [34/120    avg_loss:0.175, val_acc:0.912]
Epoch [35/120    avg_loss:0.126, val_acc:0.919]
Epoch [36/120    avg_loss:0.149, val_acc:0.911]
Epoch [37/120    avg_loss:0.103, val_acc:0.926]
Epoch [38/120    avg_loss:0.112, val_acc:0.911]
Epoch [39/120    avg_loss:0.135, val_acc:0.931]
Epoch [40/120    avg_loss:0.131, val_acc:0.946]
Epoch [41/120    avg_loss:0.109, val_acc:0.929]
Epoch [42/120    avg_loss:0.085, val_acc:0.951]
Epoch [43/120    avg_loss:0.070, val_acc:0.943]
Epoch [44/120    avg_loss:0.070, val_acc:0.945]
Epoch [45/120    avg_loss:0.066, val_acc:0.933]
Epoch [46/120    avg_loss:0.059, val_acc:0.948]
Epoch [47/120    avg_loss:0.067, val_acc:0.945]
Epoch [48/120    avg_loss:0.071, val_acc:0.931]
Epoch [49/120    avg_loss:0.048, val_acc:0.941]
Epoch [50/120    avg_loss:0.058, val_acc:0.948]
Epoch [51/120    avg_loss:0.057, val_acc:0.947]
Epoch [52/120    avg_loss:0.050, val_acc:0.953]
Epoch [53/120    avg_loss:0.056, val_acc:0.954]
Epoch [54/120    avg_loss:0.037, val_acc:0.952]
Epoch [55/120    avg_loss:0.046, val_acc:0.961]
Epoch [56/120    avg_loss:0.063, val_acc:0.938]
Epoch [57/120    avg_loss:0.054, val_acc:0.950]
Epoch [58/120    avg_loss:0.065, val_acc:0.935]
Epoch [59/120    avg_loss:0.055, val_acc:0.950]
Epoch [60/120    avg_loss:0.048, val_acc:0.950]
Epoch [61/120    avg_loss:0.059, val_acc:0.953]
Epoch [62/120    avg_loss:0.049, val_acc:0.955]
Epoch [63/120    avg_loss:0.052, val_acc:0.945]
Epoch [64/120    avg_loss:0.033, val_acc:0.964]
Epoch [65/120    avg_loss:0.031, val_acc:0.957]
Epoch [66/120    avg_loss:0.038, val_acc:0.956]
Epoch [67/120    avg_loss:0.025, val_acc:0.968]
Epoch [68/120    avg_loss:0.021, val_acc:0.972]
Epoch [69/120    avg_loss:0.026, val_acc:0.961]
Epoch [70/120    avg_loss:0.021, val_acc:0.969]
Epoch [71/120    avg_loss:0.024, val_acc:0.969]
Epoch [72/120    avg_loss:0.028, val_acc:0.963]
Epoch [73/120    avg_loss:0.025, val_acc:0.973]
Epoch [74/120    avg_loss:0.029, val_acc:0.975]
Epoch [75/120    avg_loss:0.022, val_acc:0.976]
Epoch [76/120    avg_loss:0.021, val_acc:0.970]
Epoch [77/120    avg_loss:0.021, val_acc:0.971]
Epoch [78/120    avg_loss:0.025, val_acc:0.965]
Epoch [79/120    avg_loss:0.017, val_acc:0.978]
Epoch [80/120    avg_loss:0.018, val_acc:0.977]
Epoch [81/120    avg_loss:0.016, val_acc:0.975]
Epoch [82/120    avg_loss:0.022, val_acc:0.959]
Epoch [83/120    avg_loss:0.040, val_acc:0.975]
Epoch [84/120    avg_loss:0.024, val_acc:0.975]
Epoch [85/120    avg_loss:0.026, val_acc:0.966]
Epoch [86/120    avg_loss:0.021, val_acc:0.977]
Epoch [87/120    avg_loss:0.014, val_acc:0.975]
Epoch [88/120    avg_loss:0.013, val_acc:0.976]
Epoch [89/120    avg_loss:0.018, val_acc:0.967]
Epoch [90/120    avg_loss:0.026, val_acc:0.977]
Epoch [91/120    avg_loss:0.017, val_acc:0.977]
Epoch [92/120    avg_loss:0.012, val_acc:0.974]
Epoch [93/120    avg_loss:0.010, val_acc:0.971]
Epoch [94/120    avg_loss:0.009, val_acc:0.973]
Epoch [95/120    avg_loss:0.012, val_acc:0.977]
Epoch [96/120    avg_loss:0.011, val_acc:0.976]
Epoch [97/120    avg_loss:0.009, val_acc:0.979]
Epoch [98/120    avg_loss:0.008, val_acc:0.978]
Epoch [99/120    avg_loss:0.009, val_acc:0.979]
Epoch [100/120    avg_loss:0.009, val_acc:0.979]
Epoch [101/120    avg_loss:0.008, val_acc:0.978]
Epoch [102/120    avg_loss:0.007, val_acc:0.978]
Epoch [103/120    avg_loss:0.008, val_acc:0.979]
Epoch [104/120    avg_loss:0.010, val_acc:0.976]
Epoch [105/120    avg_loss:0.009, val_acc:0.978]
Epoch [106/120    avg_loss:0.008, val_acc:0.978]
Epoch [107/120    avg_loss:0.008, val_acc:0.980]
Epoch [108/120    avg_loss:0.010, val_acc:0.980]
Epoch [109/120    avg_loss:0.008, val_acc:0.982]
Epoch [110/120    avg_loss:0.007, val_acc:0.981]
Epoch [111/120    avg_loss:0.010, val_acc:0.980]
Epoch [112/120    avg_loss:0.010, val_acc:0.981]
Epoch [113/120    avg_loss:0.008, val_acc:0.979]
Epoch [114/120    avg_loss:0.008, val_acc:0.979]
Epoch [115/120    avg_loss:0.008, val_acc:0.979]
Epoch [116/120    avg_loss:0.007, val_acc:0.977]
Epoch [117/120    avg_loss:0.008, val_acc:0.979]
Epoch [118/120    avg_loss:0.009, val_acc:0.979]
Epoch [119/120    avg_loss:0.008, val_acc:0.980]
Epoch [120/120    avg_loss:0.008, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1258    3    0    0    0    0    0    1    2   20    0    0
     0    1    0]
 [   0    0    0  730    1    0    1    0    0    2    0    1   11    0
     1    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    1    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    1    0    0    0    1  848   19    1    0
     0    4    0]
 [   0    0    9    0    0    1    5    0    0    2    5 2182    3    0
     0    3    0]
 [   0    0    0    3    0    0    0    0    0    0    0    1  524    0
     1    1    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1125   13    0]
 [   0    0    0    0    0    0   25    0    0    1    0    0    0    0
    38  283    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.85365853658537

F1 scores:
[       nan 0.96296296 0.98550725 0.98316498 0.99294118 0.99308756
 0.97619048 1.         0.99883586 0.81818182 0.97864974 0.9842129
 0.97579143 1.         0.97529259 0.86809816 0.97076023]

Kappa:
0.9755206710580742
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4b17cbe898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.683, val_acc:0.385]
Epoch [2/120    avg_loss:2.368, val_acc:0.480]
Epoch [3/120    avg_loss:2.133, val_acc:0.519]
Epoch [4/120    avg_loss:1.989, val_acc:0.534]
Epoch [5/120    avg_loss:1.840, val_acc:0.599]
Epoch [6/120    avg_loss:1.673, val_acc:0.625]
Epoch [7/120    avg_loss:1.582, val_acc:0.666]
Epoch [8/120    avg_loss:1.408, val_acc:0.690]
Epoch [9/120    avg_loss:1.327, val_acc:0.706]
Epoch [10/120    avg_loss:1.193, val_acc:0.715]
Epoch [11/120    avg_loss:1.097, val_acc:0.729]
Epoch [12/120    avg_loss:0.974, val_acc:0.728]
Epoch [13/120    avg_loss:0.886, val_acc:0.788]
Epoch [14/120    avg_loss:0.790, val_acc:0.780]
Epoch [15/120    avg_loss:0.710, val_acc:0.827]
Epoch [16/120    avg_loss:0.625, val_acc:0.817]
Epoch [17/120    avg_loss:0.729, val_acc:0.847]
Epoch [18/120    avg_loss:0.543, val_acc:0.857]
Epoch [19/120    avg_loss:0.547, val_acc:0.851]
Epoch [20/120    avg_loss:0.506, val_acc:0.818]
Epoch [21/120    avg_loss:0.453, val_acc:0.858]
Epoch [22/120    avg_loss:0.374, val_acc:0.866]
Epoch [23/120    avg_loss:0.416, val_acc:0.850]
Epoch [24/120    avg_loss:0.306, val_acc:0.890]
Epoch [25/120    avg_loss:0.274, val_acc:0.882]
Epoch [26/120    avg_loss:0.236, val_acc:0.905]
Epoch [27/120    avg_loss:0.214, val_acc:0.873]
Epoch [28/120    avg_loss:0.228, val_acc:0.893]
Epoch [29/120    avg_loss:0.221, val_acc:0.894]
Epoch [30/120    avg_loss:0.178, val_acc:0.916]
Epoch [31/120    avg_loss:0.185, val_acc:0.927]
Epoch [32/120    avg_loss:0.145, val_acc:0.938]
Epoch [33/120    avg_loss:0.142, val_acc:0.929]
Epoch [34/120    avg_loss:0.120, val_acc:0.939]
Epoch [35/120    avg_loss:0.135, val_acc:0.944]
Epoch [36/120    avg_loss:0.129, val_acc:0.933]
Epoch [37/120    avg_loss:0.132, val_acc:0.926]
Epoch [38/120    avg_loss:0.096, val_acc:0.950]
Epoch [39/120    avg_loss:0.118, val_acc:0.922]
Epoch [40/120    avg_loss:0.099, val_acc:0.954]
Epoch [41/120    avg_loss:0.091, val_acc:0.933]
Epoch [42/120    avg_loss:0.092, val_acc:0.939]
Epoch [43/120    avg_loss:0.068, val_acc:0.964]
Epoch [44/120    avg_loss:0.121, val_acc:0.938]
Epoch [45/120    avg_loss:0.167, val_acc:0.928]
Epoch [46/120    avg_loss:0.119, val_acc:0.960]
Epoch [47/120    avg_loss:0.145, val_acc:0.943]
Epoch [48/120    avg_loss:0.098, val_acc:0.937]
Epoch [49/120    avg_loss:0.126, val_acc:0.952]
Epoch [50/120    avg_loss:0.068, val_acc:0.963]
Epoch [51/120    avg_loss:0.063, val_acc:0.967]
Epoch [52/120    avg_loss:0.067, val_acc:0.962]
Epoch [53/120    avg_loss:0.055, val_acc:0.962]
Epoch [54/120    avg_loss:0.053, val_acc:0.946]
Epoch [55/120    avg_loss:0.041, val_acc:0.973]
Epoch [56/120    avg_loss:0.037, val_acc:0.969]
Epoch [57/120    avg_loss:0.046, val_acc:0.973]
Epoch [58/120    avg_loss:0.041, val_acc:0.967]
Epoch [59/120    avg_loss:0.048, val_acc:0.958]
Epoch [60/120    avg_loss:0.042, val_acc:0.969]
Epoch [61/120    avg_loss:0.035, val_acc:0.976]
Epoch [62/120    avg_loss:0.034, val_acc:0.975]
Epoch [63/120    avg_loss:0.031, val_acc:0.973]
Epoch [64/120    avg_loss:0.039, val_acc:0.967]
Epoch [65/120    avg_loss:0.036, val_acc:0.971]
Epoch [66/120    avg_loss:0.035, val_acc:0.971]
Epoch [67/120    avg_loss:0.044, val_acc:0.959]
Epoch [68/120    avg_loss:0.037, val_acc:0.967]
Epoch [69/120    avg_loss:0.026, val_acc:0.976]
Epoch [70/120    avg_loss:0.025, val_acc:0.980]
Epoch [71/120    avg_loss:0.021, val_acc:0.973]
Epoch [72/120    avg_loss:0.023, val_acc:0.968]
Epoch [73/120    avg_loss:0.022, val_acc:0.978]
Epoch [74/120    avg_loss:0.021, val_acc:0.985]
Epoch [75/120    avg_loss:0.020, val_acc:0.981]
Epoch [76/120    avg_loss:0.019, val_acc:0.978]
Epoch [77/120    avg_loss:0.016, val_acc:0.975]
Epoch [78/120    avg_loss:0.028, val_acc:0.975]
Epoch [79/120    avg_loss:0.028, val_acc:0.970]
Epoch [80/120    avg_loss:0.029, val_acc:0.968]
Epoch [81/120    avg_loss:0.020, val_acc:0.979]
Epoch [82/120    avg_loss:0.026, val_acc:0.974]
Epoch [83/120    avg_loss:0.019, val_acc:0.978]
Epoch [84/120    avg_loss:0.017, val_acc:0.948]
Epoch [85/120    avg_loss:0.020, val_acc:0.980]
Epoch [86/120    avg_loss:0.014, val_acc:0.979]
Epoch [87/120    avg_loss:0.019, val_acc:0.978]
Epoch [88/120    avg_loss:0.017, val_acc:0.976]
Epoch [89/120    avg_loss:0.014, val_acc:0.976]
Epoch [90/120    avg_loss:0.015, val_acc:0.976]
Epoch [91/120    avg_loss:0.009, val_acc:0.979]
Epoch [92/120    avg_loss:0.010, val_acc:0.979]
Epoch [93/120    avg_loss:0.011, val_acc:0.981]
Epoch [94/120    avg_loss:0.012, val_acc:0.979]
Epoch [95/120    avg_loss:0.011, val_acc:0.976]
Epoch [96/120    avg_loss:0.011, val_acc:0.978]
Epoch [97/120    avg_loss:0.011, val_acc:0.976]
Epoch [98/120    avg_loss:0.011, val_acc:0.979]
Epoch [99/120    avg_loss:0.011, val_acc:0.979]
Epoch [100/120    avg_loss:0.014, val_acc:0.975]
Epoch [101/120    avg_loss:0.009, val_acc:0.975]
Epoch [102/120    avg_loss:0.011, val_acc:0.975]
Epoch [103/120    avg_loss:0.011, val_acc:0.976]
Epoch [104/120    avg_loss:0.009, val_acc:0.980]
Epoch [105/120    avg_loss:0.011, val_acc:0.979]
Epoch [106/120    avg_loss:0.012, val_acc:0.978]
Epoch [107/120    avg_loss:0.010, val_acc:0.978]
Epoch [108/120    avg_loss:0.010, val_acc:0.978]
Epoch [109/120    avg_loss:0.009, val_acc:0.978]
Epoch [110/120    avg_loss:0.010, val_acc:0.979]
Epoch [111/120    avg_loss:0.013, val_acc:0.979]
Epoch [112/120    avg_loss:0.009, val_acc:0.979]
Epoch [113/120    avg_loss:0.011, val_acc:0.979]
Epoch [114/120    avg_loss:0.012, val_acc:0.979]
Epoch [115/120    avg_loss:0.008, val_acc:0.979]
Epoch [116/120    avg_loss:0.010, val_acc:0.979]
Epoch [117/120    avg_loss:0.009, val_acc:0.979]
Epoch [118/120    avg_loss:0.010, val_acc:0.979]
Epoch [119/120    avg_loss:0.011, val_acc:0.979]
Epoch [120/120    avg_loss:0.011, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1265    1    0    0    1    0    0    0    6   12    0    0
     0    0    0]
 [   0    0    0  737    1    0    0    0    0    4    0    0    4    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    4    0    0    1    0    0    0    0  863    6    0    0
     0    1    0]
 [   0    8    7    3    0    0    2    0    0    2   17 2161    9    1
     0    0    0]
 [   0    0    2    0    1    0    0    0    0    0    3   10  514    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
     5  341    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.51490514905149

F1 scores:
[       nan 0.8988764  0.98712446 0.9905914  0.9953271  0.99655568
 0.99619772 0.98039216 0.99883586 0.82926829 0.97790368 0.98204953
 0.96707432 0.99462366 0.99163364 0.96875    0.98224852]

Kappa:
0.9830793539440077
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6b3ba3e8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.692, val_acc:0.331]
Epoch [2/120    avg_loss:2.385, val_acc:0.406]
Epoch [3/120    avg_loss:2.194, val_acc:0.486]
Epoch [4/120    avg_loss:1.974, val_acc:0.584]
Epoch [5/120    avg_loss:1.836, val_acc:0.575]
Epoch [6/120    avg_loss:1.682, val_acc:0.609]
Epoch [7/120    avg_loss:1.566, val_acc:0.633]
Epoch [8/120    avg_loss:1.458, val_acc:0.671]
Epoch [9/120    avg_loss:1.329, val_acc:0.673]
Epoch [10/120    avg_loss:1.139, val_acc:0.714]
Epoch [11/120    avg_loss:0.985, val_acc:0.732]
Epoch [12/120    avg_loss:0.880, val_acc:0.699]
Epoch [13/120    avg_loss:0.813, val_acc:0.754]
Epoch [14/120    avg_loss:0.745, val_acc:0.787]
Epoch [15/120    avg_loss:0.729, val_acc:0.803]
Epoch [16/120    avg_loss:0.627, val_acc:0.768]
Epoch [17/120    avg_loss:0.529, val_acc:0.802]
Epoch [18/120    avg_loss:0.484, val_acc:0.809]
Epoch [19/120    avg_loss:0.411, val_acc:0.826]
Epoch [20/120    avg_loss:0.447, val_acc:0.811]
Epoch [21/120    avg_loss:0.439, val_acc:0.828]
Epoch [22/120    avg_loss:0.430, val_acc:0.822]
Epoch [23/120    avg_loss:0.355, val_acc:0.846]
Epoch [24/120    avg_loss:0.349, val_acc:0.875]
Epoch [25/120    avg_loss:0.281, val_acc:0.857]
Epoch [26/120    avg_loss:0.270, val_acc:0.840]
Epoch [27/120    avg_loss:0.269, val_acc:0.892]
Epoch [28/120    avg_loss:0.214, val_acc:0.890]
Epoch [29/120    avg_loss:0.204, val_acc:0.894]
Epoch [30/120    avg_loss:0.211, val_acc:0.895]
Epoch [31/120    avg_loss:0.158, val_acc:0.916]
Epoch [32/120    avg_loss:0.142, val_acc:0.901]
Epoch [33/120    avg_loss:0.176, val_acc:0.850]
Epoch [34/120    avg_loss:0.210, val_acc:0.887]
Epoch [35/120    avg_loss:0.173, val_acc:0.907]
Epoch [36/120    avg_loss:0.139, val_acc:0.924]
Epoch [37/120    avg_loss:0.122, val_acc:0.906]
Epoch [38/120    avg_loss:0.151, val_acc:0.915]
Epoch [39/120    avg_loss:0.110, val_acc:0.934]
Epoch [40/120    avg_loss:0.097, val_acc:0.916]
Epoch [41/120    avg_loss:0.103, val_acc:0.932]
Epoch [42/120    avg_loss:0.081, val_acc:0.928]
Epoch [43/120    avg_loss:0.132, val_acc:0.935]
Epoch [44/120    avg_loss:0.120, val_acc:0.932]
Epoch [45/120    avg_loss:0.105, val_acc:0.930]
Epoch [46/120    avg_loss:0.092, val_acc:0.941]
Epoch [47/120    avg_loss:0.077, val_acc:0.953]
Epoch [48/120    avg_loss:0.064, val_acc:0.946]
Epoch [49/120    avg_loss:0.073, val_acc:0.946]
Epoch [50/120    avg_loss:0.085, val_acc:0.946]
Epoch [51/120    avg_loss:0.071, val_acc:0.954]
Epoch [52/120    avg_loss:0.057, val_acc:0.954]
Epoch [53/120    avg_loss:0.126, val_acc:0.895]
Epoch [54/120    avg_loss:0.212, val_acc:0.919]
Epoch [55/120    avg_loss:0.132, val_acc:0.939]
Epoch [56/120    avg_loss:0.104, val_acc:0.936]
Epoch [57/120    avg_loss:0.093, val_acc:0.948]
Epoch [58/120    avg_loss:0.061, val_acc:0.948]
Epoch [59/120    avg_loss:0.062, val_acc:0.944]
Epoch [60/120    avg_loss:0.069, val_acc:0.939]
Epoch [61/120    avg_loss:0.096, val_acc:0.933]
Epoch [62/120    avg_loss:0.073, val_acc:0.950]
Epoch [63/120    avg_loss:0.054, val_acc:0.955]
Epoch [64/120    avg_loss:0.050, val_acc:0.967]
Epoch [65/120    avg_loss:0.037, val_acc:0.967]
Epoch [66/120    avg_loss:0.033, val_acc:0.956]
Epoch [67/120    avg_loss:0.057, val_acc:0.953]
Epoch [68/120    avg_loss:0.043, val_acc:0.958]
Epoch [69/120    avg_loss:0.057, val_acc:0.945]
Epoch [70/120    avg_loss:0.047, val_acc:0.957]
Epoch [71/120    avg_loss:0.036, val_acc:0.969]
Epoch [72/120    avg_loss:0.038, val_acc:0.959]
Epoch [73/120    avg_loss:0.028, val_acc:0.968]
Epoch [74/120    avg_loss:0.027, val_acc:0.966]
Epoch [75/120    avg_loss:0.052, val_acc:0.958]
Epoch [76/120    avg_loss:0.046, val_acc:0.954]
Epoch [77/120    avg_loss:0.033, val_acc:0.961]
Epoch [78/120    avg_loss:0.037, val_acc:0.965]
Epoch [79/120    avg_loss:0.029, val_acc:0.974]
Epoch [80/120    avg_loss:0.033, val_acc:0.957]
Epoch [81/120    avg_loss:0.034, val_acc:0.975]
Epoch [82/120    avg_loss:0.029, val_acc:0.968]
Epoch [83/120    avg_loss:0.031, val_acc:0.974]
Epoch [84/120    avg_loss:0.038, val_acc:0.958]
Epoch [85/120    avg_loss:0.030, val_acc:0.970]
Epoch [86/120    avg_loss:0.029, val_acc:0.969]
Epoch [87/120    avg_loss:0.025, val_acc:0.966]
Epoch [88/120    avg_loss:0.023, val_acc:0.968]
Epoch [89/120    avg_loss:0.027, val_acc:0.974]
Epoch [90/120    avg_loss:0.018, val_acc:0.977]
Epoch [91/120    avg_loss:0.021, val_acc:0.969]
Epoch [92/120    avg_loss:0.033, val_acc:0.972]
Epoch [93/120    avg_loss:0.022, val_acc:0.965]
Epoch [94/120    avg_loss:0.021, val_acc:0.974]
Epoch [95/120    avg_loss:0.022, val_acc:0.973]
Epoch [96/120    avg_loss:0.014, val_acc:0.969]
Epoch [97/120    avg_loss:0.019, val_acc:0.967]
Epoch [98/120    avg_loss:0.018, val_acc:0.968]
Epoch [99/120    avg_loss:0.015, val_acc:0.969]
Epoch [100/120    avg_loss:0.019, val_acc:0.965]
Epoch [101/120    avg_loss:0.029, val_acc:0.973]
Epoch [102/120    avg_loss:0.027, val_acc:0.968]
Epoch [103/120    avg_loss:0.034, val_acc:0.955]
Epoch [104/120    avg_loss:0.024, val_acc:0.969]
Epoch [105/120    avg_loss:0.017, val_acc:0.976]
Epoch [106/120    avg_loss:0.017, val_acc:0.977]
Epoch [107/120    avg_loss:0.010, val_acc:0.977]
Epoch [108/120    avg_loss:0.014, val_acc:0.977]
Epoch [109/120    avg_loss:0.013, val_acc:0.975]
Epoch [110/120    avg_loss:0.012, val_acc:0.977]
Epoch [111/120    avg_loss:0.010, val_acc:0.976]
Epoch [112/120    avg_loss:0.009, val_acc:0.976]
Epoch [113/120    avg_loss:0.011, val_acc:0.976]
Epoch [114/120    avg_loss:0.009, val_acc:0.979]
Epoch [115/120    avg_loss:0.010, val_acc:0.975]
Epoch [116/120    avg_loss:0.009, val_acc:0.975]
Epoch [117/120    avg_loss:0.015, val_acc:0.976]
Epoch [118/120    avg_loss:0.012, val_acc:0.977]
Epoch [119/120    avg_loss:0.009, val_acc:0.977]
Epoch [120/120    avg_loss:0.010, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1260    2    4    0    0    0    0    0    6   13    0    0
     0    0    0]
 [   0    0    0  734    1    0    0    0    0    2    0    2    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    2    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    1    0    0    3    5    0    0    0  852   12    0    0
     0    2    0]
 [   0    0    9    6    0    0    1    0    0    0    7 2182    3    1
     0    1    0]
 [   0    0    0    7    0    1    0    0    0    0    0    0  525    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1134    5    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    29  309    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.3089430894309

F1 scores:
[       nan 0.98765432 0.98630137 0.98128342 0.98839907 0.98731257
 0.98642534 0.96153846 0.99883586 0.89473684 0.97874785 0.98733032
 0.97765363 0.99730458 0.98266898 0.93072289 0.98203593]

Kappa:
0.9807181767045722
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f989dbee7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.662, val_acc:0.390]
Epoch [2/120    avg_loss:2.323, val_acc:0.503]
Epoch [3/120    avg_loss:2.096, val_acc:0.571]
Epoch [4/120    avg_loss:1.939, val_acc:0.571]
Epoch [5/120    avg_loss:1.812, val_acc:0.582]
Epoch [6/120    avg_loss:1.653, val_acc:0.596]
Epoch [7/120    avg_loss:1.538, val_acc:0.532]
Epoch [8/120    avg_loss:1.440, val_acc:0.618]
Epoch [9/120    avg_loss:1.321, val_acc:0.631]
Epoch [10/120    avg_loss:1.180, val_acc:0.689]
Epoch [11/120    avg_loss:1.098, val_acc:0.678]
Epoch [12/120    avg_loss:1.036, val_acc:0.676]
Epoch [13/120    avg_loss:0.992, val_acc:0.719]
Epoch [14/120    avg_loss:0.848, val_acc:0.742]
Epoch [15/120    avg_loss:0.806, val_acc:0.736]
Epoch [16/120    avg_loss:0.744, val_acc:0.772]
Epoch [17/120    avg_loss:0.627, val_acc:0.771]
Epoch [18/120    avg_loss:0.596, val_acc:0.749]
Epoch [19/120    avg_loss:0.618, val_acc:0.756]
Epoch [20/120    avg_loss:0.488, val_acc:0.785]
Epoch [21/120    avg_loss:0.431, val_acc:0.803]
Epoch [22/120    avg_loss:0.378, val_acc:0.836]
Epoch [23/120    avg_loss:0.421, val_acc:0.841]
Epoch [24/120    avg_loss:0.390, val_acc:0.836]
Epoch [25/120    avg_loss:0.420, val_acc:0.853]
Epoch [26/120    avg_loss:0.285, val_acc:0.874]
Epoch [27/120    avg_loss:0.250, val_acc:0.878]
Epoch [28/120    avg_loss:0.206, val_acc:0.900]
Epoch [29/120    avg_loss:0.235, val_acc:0.890]
Epoch [30/120    avg_loss:0.304, val_acc:0.867]
Epoch [31/120    avg_loss:0.255, val_acc:0.909]
Epoch [32/120    avg_loss:0.182, val_acc:0.908]
Epoch [33/120    avg_loss:0.167, val_acc:0.905]
Epoch [34/120    avg_loss:0.148, val_acc:0.918]
Epoch [35/120    avg_loss:0.144, val_acc:0.916]
Epoch [36/120    avg_loss:0.182, val_acc:0.923]
Epoch [37/120    avg_loss:0.141, val_acc:0.923]
Epoch [38/120    avg_loss:0.123, val_acc:0.932]
Epoch [39/120    avg_loss:0.145, val_acc:0.908]
Epoch [40/120    avg_loss:0.138, val_acc:0.940]
Epoch [41/120    avg_loss:0.118, val_acc:0.902]
Epoch [42/120    avg_loss:0.260, val_acc:0.916]
Epoch [43/120    avg_loss:0.175, val_acc:0.904]
Epoch [44/120    avg_loss:0.120, val_acc:0.934]
Epoch [45/120    avg_loss:0.111, val_acc:0.946]
Epoch [46/120    avg_loss:0.087, val_acc:0.938]
Epoch [47/120    avg_loss:0.076, val_acc:0.956]
Epoch [48/120    avg_loss:0.073, val_acc:0.953]
Epoch [49/120    avg_loss:0.064, val_acc:0.945]
Epoch [50/120    avg_loss:0.077, val_acc:0.949]
Epoch [51/120    avg_loss:0.060, val_acc:0.959]
Epoch [52/120    avg_loss:0.060, val_acc:0.955]
Epoch [53/120    avg_loss:0.057, val_acc:0.950]
Epoch [54/120    avg_loss:0.056, val_acc:0.964]
Epoch [55/120    avg_loss:0.053, val_acc:0.958]
Epoch [56/120    avg_loss:0.046, val_acc:0.963]
Epoch [57/120    avg_loss:0.057, val_acc:0.955]
Epoch [58/120    avg_loss:0.056, val_acc:0.954]
Epoch [59/120    avg_loss:0.070, val_acc:0.935]
Epoch [60/120    avg_loss:0.073, val_acc:0.949]
Epoch [61/120    avg_loss:0.053, val_acc:0.958]
Epoch [62/120    avg_loss:0.052, val_acc:0.963]
Epoch [63/120    avg_loss:0.044, val_acc:0.966]
Epoch [64/120    avg_loss:0.046, val_acc:0.967]
Epoch [65/120    avg_loss:0.051, val_acc:0.968]
Epoch [66/120    avg_loss:0.038, val_acc:0.939]
Epoch [67/120    avg_loss:0.034, val_acc:0.971]
Epoch [68/120    avg_loss:0.037, val_acc:0.968]
Epoch [69/120    avg_loss:0.032, val_acc:0.965]
Epoch [70/120    avg_loss:0.042, val_acc:0.957]
Epoch [71/120    avg_loss:0.040, val_acc:0.958]
Epoch [72/120    avg_loss:0.034, val_acc:0.965]
Epoch [73/120    avg_loss:0.024, val_acc:0.971]
Epoch [74/120    avg_loss:0.026, val_acc:0.972]
Epoch [75/120    avg_loss:0.025, val_acc:0.976]
Epoch [76/120    avg_loss:0.024, val_acc:0.975]
Epoch [77/120    avg_loss:0.018, val_acc:0.972]
Epoch [78/120    avg_loss:0.023, val_acc:0.966]
Epoch [79/120    avg_loss:0.031, val_acc:0.970]
Epoch [80/120    avg_loss:0.022, val_acc:0.974]
Epoch [81/120    avg_loss:0.041, val_acc:0.966]
Epoch [82/120    avg_loss:0.026, val_acc:0.972]
Epoch [83/120    avg_loss:0.018, val_acc:0.979]
Epoch [84/120    avg_loss:0.023, val_acc:0.963]
Epoch [85/120    avg_loss:0.026, val_acc:0.977]
Epoch [86/120    avg_loss:0.023, val_acc:0.975]
Epoch [87/120    avg_loss:0.022, val_acc:0.980]
Epoch [88/120    avg_loss:0.026, val_acc:0.971]
Epoch [89/120    avg_loss:0.023, val_acc:0.965]
Epoch [90/120    avg_loss:0.027, val_acc:0.974]
Epoch [91/120    avg_loss:0.022, val_acc:0.968]
Epoch [92/120    avg_loss:0.017, val_acc:0.972]
Epoch [93/120    avg_loss:0.029, val_acc:0.966]
Epoch [94/120    avg_loss:0.060, val_acc:0.953]
Epoch [95/120    avg_loss:0.054, val_acc:0.954]
Epoch [96/120    avg_loss:0.039, val_acc:0.963]
Epoch [97/120    avg_loss:0.047, val_acc:0.968]
Epoch [98/120    avg_loss:0.034, val_acc:0.964]
Epoch [99/120    avg_loss:0.032, val_acc:0.969]
Epoch [100/120    avg_loss:0.018, val_acc:0.976]
Epoch [101/120    avg_loss:0.014, val_acc:0.975]
Epoch [102/120    avg_loss:0.013, val_acc:0.976]
Epoch [103/120    avg_loss:0.016, val_acc:0.977]
Epoch [104/120    avg_loss:0.014, val_acc:0.975]
Epoch [105/120    avg_loss:0.010, val_acc:0.977]
Epoch [106/120    avg_loss:0.012, val_acc:0.977]
Epoch [107/120    avg_loss:0.015, val_acc:0.977]
Epoch [108/120    avg_loss:0.013, val_acc:0.978]
Epoch [109/120    avg_loss:0.013, val_acc:0.978]
Epoch [110/120    avg_loss:0.009, val_acc:0.980]
Epoch [111/120    avg_loss:0.014, val_acc:0.980]
Epoch [112/120    avg_loss:0.009, val_acc:0.979]
Epoch [113/120    avg_loss:0.012, val_acc:0.979]
Epoch [114/120    avg_loss:0.011, val_acc:0.978]
Epoch [115/120    avg_loss:0.012, val_acc:0.979]
Epoch [116/120    avg_loss:0.011, val_acc:0.979]
Epoch [117/120    avg_loss:0.009, val_acc:0.978]
Epoch [118/120    avg_loss:0.011, val_acc:0.979]
Epoch [119/120    avg_loss:0.008, val_acc:0.979]
Epoch [120/120    avg_loss:0.012, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1237    2    3    6    2    0    0    1    9   24    1    0
     0    0    0]
 [   0    0    0  729    1    0    1    0    0    6    0    6    4    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    5  863    6    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    3   11 2181   15    0
     0    0    0]
 [   0    0    0    3    0    1    0    0    0    0    3    1  523    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    5    0    0    0    0
  1130    4    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
     3  343    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
98.41734417344173

F1 scores:
[       nan 0.975      0.98096749 0.98380567 0.98834499 0.98623853
 0.9969651  1.         0.997669   0.62068966 0.97901305 0.98509485
 0.96494465 1.         0.99253404 0.98847262 0.95757576]

Kappa:
0.9819637397046446
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb21f12d828>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.678, val_acc:0.476]
Epoch [2/120    avg_loss:2.330, val_acc:0.467]
Epoch [3/120    avg_loss:2.124, val_acc:0.508]
Epoch [4/120    avg_loss:1.925, val_acc:0.550]
Epoch [5/120    avg_loss:1.831, val_acc:0.595]
Epoch [6/120    avg_loss:1.705, val_acc:0.576]
Epoch [7/120    avg_loss:1.534, val_acc:0.626]
Epoch [8/120    avg_loss:1.482, val_acc:0.599]
Epoch [9/120    avg_loss:1.288, val_acc:0.635]
Epoch [10/120    avg_loss:1.129, val_acc:0.659]
Epoch [11/120    avg_loss:1.055, val_acc:0.676]
Epoch [12/120    avg_loss:0.918, val_acc:0.714]
Epoch [13/120    avg_loss:0.844, val_acc:0.715]
Epoch [14/120    avg_loss:0.794, val_acc:0.751]
Epoch [15/120    avg_loss:0.780, val_acc:0.702]
Epoch [16/120    avg_loss:0.616, val_acc:0.767]
Epoch [17/120    avg_loss:0.624, val_acc:0.808]
Epoch [18/120    avg_loss:0.912, val_acc:0.731]
Epoch [19/120    avg_loss:0.634, val_acc:0.777]
Epoch [20/120    avg_loss:0.563, val_acc:0.805]
Epoch [21/120    avg_loss:0.455, val_acc:0.818]
Epoch [22/120    avg_loss:0.420, val_acc:0.822]
Epoch [23/120    avg_loss:0.418, val_acc:0.850]
Epoch [24/120    avg_loss:0.325, val_acc:0.853]
Epoch [25/120    avg_loss:0.296, val_acc:0.869]
Epoch [26/120    avg_loss:0.297, val_acc:0.886]
Epoch [27/120    avg_loss:0.236, val_acc:0.871]
Epoch [28/120    avg_loss:0.319, val_acc:0.878]
Epoch [29/120    avg_loss:0.299, val_acc:0.868]
Epoch [30/120    avg_loss:0.227, val_acc:0.902]
Epoch [31/120    avg_loss:0.169, val_acc:0.918]
Epoch [32/120    avg_loss:0.148, val_acc:0.918]
Epoch [33/120    avg_loss:0.155, val_acc:0.932]
Epoch [34/120    avg_loss:0.134, val_acc:0.938]
Epoch [35/120    avg_loss:0.117, val_acc:0.910]
Epoch [36/120    avg_loss:0.150, val_acc:0.931]
Epoch [37/120    avg_loss:0.178, val_acc:0.894]
Epoch [38/120    avg_loss:0.151, val_acc:0.908]
Epoch [39/120    avg_loss:0.113, val_acc:0.928]
Epoch [40/120    avg_loss:0.123, val_acc:0.895]
Epoch [41/120    avg_loss:0.137, val_acc:0.929]
Epoch [42/120    avg_loss:0.105, val_acc:0.945]
Epoch [43/120    avg_loss:0.077, val_acc:0.947]
Epoch [44/120    avg_loss:0.072, val_acc:0.941]
Epoch [45/120    avg_loss:0.142, val_acc:0.901]
Epoch [46/120    avg_loss:0.105, val_acc:0.941]
Epoch [47/120    avg_loss:0.096, val_acc:0.949]
Epoch [48/120    avg_loss:0.086, val_acc:0.957]
Epoch [49/120    avg_loss:0.076, val_acc:0.942]
Epoch [50/120    avg_loss:0.080, val_acc:0.951]
Epoch [51/120    avg_loss:0.087, val_acc:0.933]
Epoch [52/120    avg_loss:0.074, val_acc:0.942]
Epoch [53/120    avg_loss:0.059, val_acc:0.955]
Epoch [54/120    avg_loss:0.058, val_acc:0.952]
Epoch [55/120    avg_loss:0.051, val_acc:0.964]
Epoch [56/120    avg_loss:0.039, val_acc:0.969]
Epoch [57/120    avg_loss:0.042, val_acc:0.948]
Epoch [58/120    avg_loss:0.064, val_acc:0.944]
Epoch [59/120    avg_loss:0.050, val_acc:0.963]
Epoch [60/120    avg_loss:0.721, val_acc:0.682]
Epoch [61/120    avg_loss:0.602, val_acc:0.816]
Epoch [62/120    avg_loss:0.316, val_acc:0.854]
Epoch [63/120    avg_loss:0.215, val_acc:0.925]
Epoch [64/120    avg_loss:0.213, val_acc:0.915]
Epoch [65/120    avg_loss:0.133, val_acc:0.932]
Epoch [66/120    avg_loss:0.129, val_acc:0.936]
Epoch [67/120    avg_loss:0.092, val_acc:0.949]
Epoch [68/120    avg_loss:0.070, val_acc:0.951]
Epoch [69/120    avg_loss:0.064, val_acc:0.934]
Epoch [70/120    avg_loss:0.056, val_acc:0.966]
Epoch [71/120    avg_loss:0.048, val_acc:0.967]
Epoch [72/120    avg_loss:0.042, val_acc:0.967]
Epoch [73/120    avg_loss:0.040, val_acc:0.970]
Epoch [74/120    avg_loss:0.037, val_acc:0.968]
Epoch [75/120    avg_loss:0.039, val_acc:0.967]
Epoch [76/120    avg_loss:0.035, val_acc:0.969]
Epoch [77/120    avg_loss:0.042, val_acc:0.967]
Epoch [78/120    avg_loss:0.034, val_acc:0.969]
Epoch [79/120    avg_loss:0.031, val_acc:0.971]
Epoch [80/120    avg_loss:0.036, val_acc:0.967]
Epoch [81/120    avg_loss:0.037, val_acc:0.969]
Epoch [82/120    avg_loss:0.036, val_acc:0.965]
Epoch [83/120    avg_loss:0.037, val_acc:0.971]
Epoch [84/120    avg_loss:0.042, val_acc:0.969]
Epoch [85/120    avg_loss:0.033, val_acc:0.970]
Epoch [86/120    avg_loss:0.030, val_acc:0.972]
Epoch [87/120    avg_loss:0.032, val_acc:0.971]
Epoch [88/120    avg_loss:0.033, val_acc:0.968]
Epoch [89/120    avg_loss:0.031, val_acc:0.972]
Epoch [90/120    avg_loss:0.025, val_acc:0.972]
Epoch [91/120    avg_loss:0.026, val_acc:0.973]
Epoch [92/120    avg_loss:0.032, val_acc:0.972]
Epoch [93/120    avg_loss:0.029, val_acc:0.975]
Epoch [94/120    avg_loss:0.031, val_acc:0.973]
Epoch [95/120    avg_loss:0.028, val_acc:0.972]
Epoch [96/120    avg_loss:0.030, val_acc:0.971]
Epoch [97/120    avg_loss:0.030, val_acc:0.971]
Epoch [98/120    avg_loss:0.029, val_acc:0.972]
Epoch [99/120    avg_loss:0.028, val_acc:0.975]
Epoch [100/120    avg_loss:0.024, val_acc:0.974]
Epoch [101/120    avg_loss:0.024, val_acc:0.974]
Epoch [102/120    avg_loss:0.024, val_acc:0.973]
Epoch [103/120    avg_loss:0.029, val_acc:0.972]
Epoch [104/120    avg_loss:0.026, val_acc:0.972]
Epoch [105/120    avg_loss:0.021, val_acc:0.975]
Epoch [106/120    avg_loss:0.023, val_acc:0.972]
Epoch [107/120    avg_loss:0.023, val_acc:0.974]
Epoch [108/120    avg_loss:0.027, val_acc:0.973]
Epoch [109/120    avg_loss:0.026, val_acc:0.973]
Epoch [110/120    avg_loss:0.024, val_acc:0.973]
Epoch [111/120    avg_loss:0.024, val_acc:0.972]
Epoch [112/120    avg_loss:0.023, val_acc:0.973]
Epoch [113/120    avg_loss:0.018, val_acc:0.973]
Epoch [114/120    avg_loss:0.025, val_acc:0.972]
Epoch [115/120    avg_loss:0.022, val_acc:0.973]
Epoch [116/120    avg_loss:0.023, val_acc:0.975]
Epoch [117/120    avg_loss:0.023, val_acc:0.973]
Epoch [118/120    avg_loss:0.022, val_acc:0.973]
Epoch [119/120    avg_loss:0.021, val_acc:0.974]
Epoch [120/120    avg_loss:0.021, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1244    2    5    0    1    0    0    0    5   28    0    0
     0    0    0]
 [   0    0    0  710    0    0    0    0    0   15    4    2   13    3
     0    0    0]
 [   0    0    0    3  210    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    1    0    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    7    0    0    4    6    0    0    0  844   12    0    0
     0    2    0]
 [   0    0   25    0    0    0    4    0    0    3   14 2162    2    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    1    0    2  523    0
     3    0    4]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    6    0    2    0    0    0    0    0
  1123    8    0]
 [   0    0    0    0    0    1   21    0    0    0    0    0    0    0
    11  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.45257452574526

F1 scores:
[       nan 0.93506494 0.97111632 0.97127223 0.98130841 0.98737084
 0.96965211 1.         0.99651568 0.60714286 0.96622782 0.97850192
 0.97574627 0.98924731 0.98551996 0.93591654 0.97674419]

Kappa:
0.9709637522888459
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f81d514e8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.686, val_acc:0.317]
Epoch [2/120    avg_loss:2.345, val_acc:0.473]
Epoch [3/120    avg_loss:2.169, val_acc:0.512]
Epoch [4/120    avg_loss:2.029, val_acc:0.522]
Epoch [5/120    avg_loss:1.813, val_acc:0.576]
Epoch [6/120    avg_loss:1.641, val_acc:0.614]
Epoch [7/120    avg_loss:1.515, val_acc:0.628]
Epoch [8/120    avg_loss:1.320, val_acc:0.681]
Epoch [9/120    avg_loss:1.174, val_acc:0.709]
Epoch [10/120    avg_loss:1.072, val_acc:0.734]
Epoch [11/120    avg_loss:0.906, val_acc:0.758]
Epoch [12/120    avg_loss:0.849, val_acc:0.710]
Epoch [13/120    avg_loss:0.741, val_acc:0.804]
Epoch [14/120    avg_loss:0.652, val_acc:0.757]
Epoch [15/120    avg_loss:0.582, val_acc:0.809]
Epoch [16/120    avg_loss:0.579, val_acc:0.839]
Epoch [17/120    avg_loss:0.481, val_acc:0.832]
Epoch [18/120    avg_loss:0.414, val_acc:0.843]
Epoch [19/120    avg_loss:0.365, val_acc:0.860]
Epoch [20/120    avg_loss:0.327, val_acc:0.870]
Epoch [21/120    avg_loss:0.356, val_acc:0.863]
Epoch [22/120    avg_loss:0.392, val_acc:0.861]
Epoch [23/120    avg_loss:0.333, val_acc:0.865]
Epoch [24/120    avg_loss:0.265, val_acc:0.867]
Epoch [25/120    avg_loss:0.267, val_acc:0.922]
Epoch [26/120    avg_loss:0.205, val_acc:0.901]
Epoch [27/120    avg_loss:0.181, val_acc:0.929]
Epoch [28/120    avg_loss:0.161, val_acc:0.936]
Epoch [29/120    avg_loss:0.170, val_acc:0.938]
Epoch [30/120    avg_loss:0.204, val_acc:0.919]
Epoch [31/120    avg_loss:0.153, val_acc:0.898]
Epoch [32/120    avg_loss:0.163, val_acc:0.929]
Epoch [33/120    avg_loss:0.169, val_acc:0.906]
Epoch [34/120    avg_loss:0.190, val_acc:0.910]
Epoch [35/120    avg_loss:0.150, val_acc:0.919]
Epoch [36/120    avg_loss:0.118, val_acc:0.933]
Epoch [37/120    avg_loss:0.123, val_acc:0.941]
Epoch [38/120    avg_loss:0.092, val_acc:0.934]
Epoch [39/120    avg_loss:0.088, val_acc:0.925]
Epoch [40/120    avg_loss:0.095, val_acc:0.940]
Epoch [41/120    avg_loss:0.079, val_acc:0.957]
Epoch [42/120    avg_loss:0.108, val_acc:0.965]
Epoch [43/120    avg_loss:0.109, val_acc:0.956]
Epoch [44/120    avg_loss:0.087, val_acc:0.957]
Epoch [45/120    avg_loss:0.087, val_acc:0.953]
Epoch [46/120    avg_loss:0.081, val_acc:0.947]
Epoch [47/120    avg_loss:0.072, val_acc:0.960]
Epoch [48/120    avg_loss:0.067, val_acc:0.954]
Epoch [49/120    avg_loss:0.057, val_acc:0.969]
Epoch [50/120    avg_loss:0.055, val_acc:0.946]
Epoch [51/120    avg_loss:0.067, val_acc:0.953]
Epoch [52/120    avg_loss:0.064, val_acc:0.960]
Epoch [53/120    avg_loss:0.050, val_acc:0.963]
Epoch [54/120    avg_loss:0.067, val_acc:0.950]
Epoch [55/120    avg_loss:0.047, val_acc:0.964]
Epoch [56/120    avg_loss:0.040, val_acc:0.948]
Epoch [57/120    avg_loss:0.033, val_acc:0.968]
Epoch [58/120    avg_loss:0.042, val_acc:0.973]
Epoch [59/120    avg_loss:0.036, val_acc:0.968]
Epoch [60/120    avg_loss:0.038, val_acc:0.968]
Epoch [61/120    avg_loss:0.055, val_acc:0.955]
Epoch [62/120    avg_loss:0.045, val_acc:0.958]
Epoch [63/120    avg_loss:0.064, val_acc:0.950]
Epoch [64/120    avg_loss:0.052, val_acc:0.967]
Epoch [65/120    avg_loss:0.042, val_acc:0.963]
Epoch [66/120    avg_loss:0.034, val_acc:0.975]
Epoch [67/120    avg_loss:0.034, val_acc:0.954]
Epoch [68/120    avg_loss:0.065, val_acc:0.963]
Epoch [69/120    avg_loss:0.165, val_acc:0.961]
Epoch [70/120    avg_loss:0.089, val_acc:0.948]
Epoch [71/120    avg_loss:0.054, val_acc:0.968]
Epoch [72/120    avg_loss:0.073, val_acc:0.954]
Epoch [73/120    avg_loss:0.063, val_acc:0.969]
Epoch [74/120    avg_loss:0.036, val_acc:0.957]
Epoch [75/120    avg_loss:0.025, val_acc:0.974]
Epoch [76/120    avg_loss:0.038, val_acc:0.944]
Epoch [77/120    avg_loss:0.037, val_acc:0.976]
Epoch [78/120    avg_loss:0.028, val_acc:0.978]
Epoch [79/120    avg_loss:0.034, val_acc:0.968]
Epoch [80/120    avg_loss:0.022, val_acc:0.975]
Epoch [81/120    avg_loss:0.021, val_acc:0.978]
Epoch [82/120    avg_loss:0.025, val_acc:0.981]
Epoch [83/120    avg_loss:0.016, val_acc:0.983]
Epoch [84/120    avg_loss:0.020, val_acc:0.981]
Epoch [85/120    avg_loss:0.024, val_acc:0.965]
Epoch [86/120    avg_loss:0.061, val_acc:0.967]
Epoch [87/120    avg_loss:0.027, val_acc:0.974]
Epoch [88/120    avg_loss:0.029, val_acc:0.980]
Epoch [89/120    avg_loss:0.021, val_acc:0.976]
Epoch [90/120    avg_loss:0.021, val_acc:0.967]
Epoch [91/120    avg_loss:0.016, val_acc:0.978]
Epoch [92/120    avg_loss:0.026, val_acc:0.964]
Epoch [93/120    avg_loss:0.018, val_acc:0.979]
Epoch [94/120    avg_loss:0.014, val_acc:0.980]
Epoch [95/120    avg_loss:0.015, val_acc:0.977]
Epoch [96/120    avg_loss:0.015, val_acc:0.970]
Epoch [97/120    avg_loss:0.012, val_acc:0.976]
Epoch [98/120    avg_loss:0.010, val_acc:0.976]
Epoch [99/120    avg_loss:0.008, val_acc:0.979]
Epoch [100/120    avg_loss:0.010, val_acc:0.980]
Epoch [101/120    avg_loss:0.010, val_acc:0.979]
Epoch [102/120    avg_loss:0.007, val_acc:0.980]
Epoch [103/120    avg_loss:0.008, val_acc:0.981]
Epoch [104/120    avg_loss:0.010, val_acc:0.983]
Epoch [105/120    avg_loss:0.009, val_acc:0.984]
Epoch [106/120    avg_loss:0.012, val_acc:0.983]
Epoch [107/120    avg_loss:0.009, val_acc:0.983]
Epoch [108/120    avg_loss:0.008, val_acc:0.983]
Epoch [109/120    avg_loss:0.008, val_acc:0.983]
Epoch [110/120    avg_loss:0.008, val_acc:0.984]
Epoch [111/120    avg_loss:0.014, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.984]
Epoch [113/120    avg_loss:0.008, val_acc:0.984]
Epoch [114/120    avg_loss:0.009, val_acc:0.985]
Epoch [115/120    avg_loss:0.009, val_acc:0.982]
Epoch [116/120    avg_loss:0.006, val_acc:0.984]
Epoch [117/120    avg_loss:0.009, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.982]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.009, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1247    4    4    0    0    0    0    0    9   16    5    0
     0    0    0]
 [   0    0    1  726    0    4    0    0    0    7    3    0    6    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  430    0    3    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    2  842   25    0    0
     0    0    0]
 [   0    0    8    0    0    0    0    0    0    0   18 2176    7    1
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0    0  530    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    1    0    0    0
  1125    6    0]
 [   0    0    0    0    0    0    1    0    0    1    0    0    0    0
    37  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.8970189701897

F1 scores:
[       nan 0.975      0.97919121 0.98174442 0.98834499 0.98173516
 0.99847793 0.94339623 1.         0.73913043 0.96228571 0.9830585
 0.97785978 0.99730458 0.97698654 0.92911011 0.98809524]

Kappa:
0.9760226359500062
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8388918898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.687, val_acc:0.338]
Epoch [2/120    avg_loss:2.287, val_acc:0.479]
Epoch [3/120    avg_loss:2.072, val_acc:0.532]
Epoch [4/120    avg_loss:1.889, val_acc:0.558]
Epoch [5/120    avg_loss:1.702, val_acc:0.614]
Epoch [6/120    avg_loss:1.551, val_acc:0.639]
Epoch [7/120    avg_loss:1.349, val_acc:0.650]
Epoch [8/120    avg_loss:1.217, val_acc:0.644]
Epoch [9/120    avg_loss:1.122, val_acc:0.690]
Epoch [10/120    avg_loss:0.955, val_acc:0.686]
Epoch [11/120    avg_loss:0.888, val_acc:0.705]
Epoch [12/120    avg_loss:0.836, val_acc:0.729]
Epoch [13/120    avg_loss:0.729, val_acc:0.724]
Epoch [14/120    avg_loss:0.652, val_acc:0.720]
Epoch [15/120    avg_loss:0.592, val_acc:0.799]
Epoch [16/120    avg_loss:0.514, val_acc:0.824]
Epoch [17/120    avg_loss:0.498, val_acc:0.816]
Epoch [18/120    avg_loss:0.451, val_acc:0.809]
Epoch [19/120    avg_loss:0.405, val_acc:0.853]
Epoch [20/120    avg_loss:0.345, val_acc:0.863]
Epoch [21/120    avg_loss:0.450, val_acc:0.837]
Epoch [22/120    avg_loss:0.621, val_acc:0.722]
Epoch [23/120    avg_loss:0.410, val_acc:0.849]
Epoch [24/120    avg_loss:0.331, val_acc:0.861]
Epoch [25/120    avg_loss:0.277, val_acc:0.890]
Epoch [26/120    avg_loss:0.278, val_acc:0.878]
Epoch [27/120    avg_loss:0.219, val_acc:0.907]
Epoch [28/120    avg_loss:0.172, val_acc:0.900]
Epoch [29/120    avg_loss:0.186, val_acc:0.916]
Epoch [30/120    avg_loss:0.163, val_acc:0.922]
Epoch [31/120    avg_loss:0.160, val_acc:0.909]
Epoch [32/120    avg_loss:0.153, val_acc:0.922]
Epoch [33/120    avg_loss:0.146, val_acc:0.904]
Epoch [34/120    avg_loss:0.125, val_acc:0.910]
Epoch [35/120    avg_loss:0.138, val_acc:0.933]
Epoch [36/120    avg_loss:0.103, val_acc:0.926]
Epoch [37/120    avg_loss:0.118, val_acc:0.931]
Epoch [38/120    avg_loss:0.116, val_acc:0.917]
Epoch [39/120    avg_loss:0.119, val_acc:0.928]
Epoch [40/120    avg_loss:0.107, val_acc:0.943]
Epoch [41/120    avg_loss:0.108, val_acc:0.936]
Epoch [42/120    avg_loss:0.084, val_acc:0.921]
Epoch [43/120    avg_loss:0.090, val_acc:0.943]
Epoch [44/120    avg_loss:0.066, val_acc:0.941]
Epoch [45/120    avg_loss:0.075, val_acc:0.947]
Epoch [46/120    avg_loss:0.065, val_acc:0.944]
Epoch [47/120    avg_loss:0.077, val_acc:0.958]
Epoch [48/120    avg_loss:0.055, val_acc:0.956]
Epoch [49/120    avg_loss:0.064, val_acc:0.959]
Epoch [50/120    avg_loss:0.058, val_acc:0.959]
Epoch [51/120    avg_loss:0.061, val_acc:0.941]
Epoch [52/120    avg_loss:0.061, val_acc:0.923]
Epoch [53/120    avg_loss:0.048, val_acc:0.953]
Epoch [54/120    avg_loss:0.063, val_acc:0.952]
Epoch [55/120    avg_loss:0.066, val_acc:0.958]
Epoch [56/120    avg_loss:0.045, val_acc:0.958]
Epoch [57/120    avg_loss:0.039, val_acc:0.963]
Epoch [58/120    avg_loss:0.159, val_acc:0.905]
Epoch [59/120    avg_loss:0.109, val_acc:0.924]
Epoch [60/120    avg_loss:0.085, val_acc:0.935]
Epoch [61/120    avg_loss:0.059, val_acc:0.949]
Epoch [62/120    avg_loss:0.051, val_acc:0.956]
Epoch [63/120    avg_loss:0.046, val_acc:0.958]
Epoch [64/120    avg_loss:0.047, val_acc:0.957]
Epoch [65/120    avg_loss:0.039, val_acc:0.957]
Epoch [66/120    avg_loss:0.031, val_acc:0.954]
Epoch [67/120    avg_loss:0.032, val_acc:0.968]
Epoch [68/120    avg_loss:0.046, val_acc:0.962]
Epoch [69/120    avg_loss:0.036, val_acc:0.958]
Epoch [70/120    avg_loss:0.029, val_acc:0.968]
Epoch [71/120    avg_loss:0.022, val_acc:0.962]
Epoch [72/120    avg_loss:0.034, val_acc:0.964]
Epoch [73/120    avg_loss:0.032, val_acc:0.955]
Epoch [74/120    avg_loss:0.031, val_acc:0.966]
Epoch [75/120    avg_loss:0.019, val_acc:0.966]
Epoch [76/120    avg_loss:0.018, val_acc:0.963]
Epoch [77/120    avg_loss:0.020, val_acc:0.960]
Epoch [78/120    avg_loss:0.024, val_acc:0.968]
Epoch [79/120    avg_loss:0.028, val_acc:0.959]
Epoch [80/120    avg_loss:0.022, val_acc:0.970]
Epoch [81/120    avg_loss:0.027, val_acc:0.964]
Epoch [82/120    avg_loss:0.022, val_acc:0.969]
Epoch [83/120    avg_loss:0.019, val_acc:0.965]
Epoch [84/120    avg_loss:0.017, val_acc:0.974]
Epoch [85/120    avg_loss:0.015, val_acc:0.972]
Epoch [86/120    avg_loss:0.017, val_acc:0.971]
Epoch [87/120    avg_loss:0.024, val_acc:0.968]
Epoch [88/120    avg_loss:0.018, val_acc:0.974]
Epoch [89/120    avg_loss:0.021, val_acc:0.974]
Epoch [90/120    avg_loss:0.014, val_acc:0.969]
Epoch [91/120    avg_loss:0.015, val_acc:0.970]
Epoch [92/120    avg_loss:0.014, val_acc:0.974]
Epoch [93/120    avg_loss:0.016, val_acc:0.967]
Epoch [94/120    avg_loss:0.017, val_acc:0.973]
Epoch [95/120    avg_loss:0.014, val_acc:0.973]
Epoch [96/120    avg_loss:0.015, val_acc:0.971]
Epoch [97/120    avg_loss:0.012, val_acc:0.973]
Epoch [98/120    avg_loss:0.011, val_acc:0.962]
Epoch [99/120    avg_loss:0.032, val_acc:0.950]
Epoch [100/120    avg_loss:0.069, val_acc:0.942]
Epoch [101/120    avg_loss:0.064, val_acc:0.962]
Epoch [102/120    avg_loss:0.033, val_acc:0.958]
Epoch [103/120    avg_loss:0.024, val_acc:0.970]
Epoch [104/120    avg_loss:0.036, val_acc:0.958]
Epoch [105/120    avg_loss:0.021, val_acc:0.963]
Epoch [106/120    avg_loss:0.018, val_acc:0.966]
Epoch [107/120    avg_loss:0.017, val_acc:0.970]
Epoch [108/120    avg_loss:0.011, val_acc:0.974]
Epoch [109/120    avg_loss:0.013, val_acc:0.975]
Epoch [110/120    avg_loss:0.013, val_acc:0.973]
Epoch [111/120    avg_loss:0.010, val_acc:0.973]
Epoch [112/120    avg_loss:0.013, val_acc:0.971]
Epoch [113/120    avg_loss:0.012, val_acc:0.972]
Epoch [114/120    avg_loss:0.014, val_acc:0.972]
Epoch [115/120    avg_loss:0.011, val_acc:0.973]
Epoch [116/120    avg_loss:0.010, val_acc:0.972]
Epoch [117/120    avg_loss:0.011, val_acc:0.972]
Epoch [118/120    avg_loss:0.011, val_acc:0.972]
Epoch [119/120    avg_loss:0.011, val_acc:0.973]
Epoch [120/120    avg_loss:0.011, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1255    7    0    0    1    0    0    0    3   18    1    0
     0    0    0]
 [   0    0    0  724    2    0    0    0    0    4    1    1   15    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  429    0    2    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    1  857   10    0    0
     0    1    0]
 [   0    0    1    6    0    0    0    0    0    5   17 2161   18    1
     0    1    0]
 [   0    0    0    8    1    0    0    0    0    0    1    4  513    0
     0    5    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0    0
  1125   13    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    30  312    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.7560975609756

F1 scores:
[       nan 0.95238095 0.98547311 0.97050938 0.99065421 0.99305556
 0.99316629 0.96153846 0.99649942 0.76595745 0.97663818 0.9807125
 0.94562212 0.99730458 0.97911227 0.91899853 0.97005988]

Kappa:
0.9744276919260928
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4d66ebb8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.715, val_acc:0.311]
Epoch [2/120    avg_loss:2.353, val_acc:0.420]
Epoch [3/120    avg_loss:2.158, val_acc:0.511]
Epoch [4/120    avg_loss:2.010, val_acc:0.574]
Epoch [5/120    avg_loss:1.883, val_acc:0.572]
Epoch [6/120    avg_loss:1.732, val_acc:0.599]
Epoch [7/120    avg_loss:1.591, val_acc:0.639]
Epoch [8/120    avg_loss:1.443, val_acc:0.654]
Epoch [9/120    avg_loss:1.293, val_acc:0.651]
Epoch [10/120    avg_loss:1.167, val_acc:0.668]
Epoch [11/120    avg_loss:1.055, val_acc:0.702]
Epoch [12/120    avg_loss:0.913, val_acc:0.750]
Epoch [13/120    avg_loss:0.845, val_acc:0.764]
Epoch [14/120    avg_loss:0.779, val_acc:0.756]
Epoch [15/120    avg_loss:0.714, val_acc:0.755]
Epoch [16/120    avg_loss:0.643, val_acc:0.801]
Epoch [17/120    avg_loss:0.580, val_acc:0.827]
Epoch [18/120    avg_loss:0.489, val_acc:0.809]
Epoch [19/120    avg_loss:0.440, val_acc:0.822]
Epoch [20/120    avg_loss:0.372, val_acc:0.854]
Epoch [21/120    avg_loss:0.400, val_acc:0.873]
Epoch [22/120    avg_loss:0.370, val_acc:0.877]
Epoch [23/120    avg_loss:0.352, val_acc:0.854]
Epoch [24/120    avg_loss:0.299, val_acc:0.854]
Epoch [25/120    avg_loss:0.315, val_acc:0.895]
Epoch [26/120    avg_loss:0.243, val_acc:0.885]
Epoch [27/120    avg_loss:0.225, val_acc:0.899]
Epoch [28/120    avg_loss:0.276, val_acc:0.884]
Epoch [29/120    avg_loss:0.289, val_acc:0.878]
Epoch [30/120    avg_loss:0.278, val_acc:0.901]
Epoch [31/120    avg_loss:0.206, val_acc:0.917]
Epoch [32/120    avg_loss:0.171, val_acc:0.917]
Epoch [33/120    avg_loss:0.153, val_acc:0.902]
Epoch [34/120    avg_loss:0.172, val_acc:0.876]
Epoch [35/120    avg_loss:0.210, val_acc:0.915]
Epoch [36/120    avg_loss:0.165, val_acc:0.923]
Epoch [37/120    avg_loss:0.276, val_acc:0.886]
Epoch [38/120    avg_loss:0.323, val_acc:0.888]
Epoch [39/120    avg_loss:0.233, val_acc:0.901]
Epoch [40/120    avg_loss:0.143, val_acc:0.916]
Epoch [41/120    avg_loss:0.125, val_acc:0.912]
Epoch [42/120    avg_loss:0.112, val_acc:0.924]
Epoch [43/120    avg_loss:0.128, val_acc:0.935]
Epoch [44/120    avg_loss:0.087, val_acc:0.932]
Epoch [45/120    avg_loss:0.081, val_acc:0.946]
Epoch [46/120    avg_loss:0.085, val_acc:0.925]
Epoch [47/120    avg_loss:0.105, val_acc:0.919]
Epoch [48/120    avg_loss:0.093, val_acc:0.933]
Epoch [49/120    avg_loss:0.134, val_acc:0.926]
Epoch [50/120    avg_loss:0.091, val_acc:0.939]
Epoch [51/120    avg_loss:0.093, val_acc:0.939]
Epoch [52/120    avg_loss:0.112, val_acc:0.911]
Epoch [53/120    avg_loss:0.120, val_acc:0.933]
Epoch [54/120    avg_loss:0.099, val_acc:0.938]
Epoch [55/120    avg_loss:0.064, val_acc:0.947]
Epoch [56/120    avg_loss:0.057, val_acc:0.951]
Epoch [57/120    avg_loss:0.056, val_acc:0.939]
Epoch [58/120    avg_loss:0.066, val_acc:0.963]
Epoch [59/120    avg_loss:0.070, val_acc:0.943]
Epoch [60/120    avg_loss:0.049, val_acc:0.958]
Epoch [61/120    avg_loss:0.048, val_acc:0.957]
Epoch [62/120    avg_loss:0.033, val_acc:0.956]
Epoch [63/120    avg_loss:0.052, val_acc:0.952]
Epoch [64/120    avg_loss:0.043, val_acc:0.951]
Epoch [65/120    avg_loss:0.043, val_acc:0.949]
Epoch [66/120    avg_loss:0.049, val_acc:0.949]
Epoch [67/120    avg_loss:0.062, val_acc:0.952]
Epoch [68/120    avg_loss:0.050, val_acc:0.957]
Epoch [69/120    avg_loss:0.033, val_acc:0.958]
Epoch [70/120    avg_loss:0.029, val_acc:0.963]
Epoch [71/120    avg_loss:0.035, val_acc:0.968]
Epoch [72/120    avg_loss:0.038, val_acc:0.957]
Epoch [73/120    avg_loss:0.033, val_acc:0.958]
Epoch [74/120    avg_loss:0.037, val_acc:0.951]
Epoch [75/120    avg_loss:0.043, val_acc:0.966]
Epoch [76/120    avg_loss:0.028, val_acc:0.960]
Epoch [77/120    avg_loss:0.033, val_acc:0.946]
Epoch [78/120    avg_loss:0.025, val_acc:0.968]
Epoch [79/120    avg_loss:0.028, val_acc:0.957]
Epoch [80/120    avg_loss:0.038, val_acc:0.966]
Epoch [81/120    avg_loss:0.020, val_acc:0.965]
Epoch [82/120    avg_loss:0.021, val_acc:0.964]
Epoch [83/120    avg_loss:0.020, val_acc:0.968]
Epoch [84/120    avg_loss:0.022, val_acc:0.969]
Epoch [85/120    avg_loss:0.030, val_acc:0.964]
Epoch [86/120    avg_loss:0.020, val_acc:0.967]
Epoch [87/120    avg_loss:0.022, val_acc:0.969]
Epoch [88/120    avg_loss:0.018, val_acc:0.972]
Epoch [89/120    avg_loss:0.025, val_acc:0.970]
Epoch [90/120    avg_loss:0.026, val_acc:0.964]
Epoch [91/120    avg_loss:0.019, val_acc:0.968]
Epoch [92/120    avg_loss:0.017, val_acc:0.972]
Epoch [93/120    avg_loss:0.015, val_acc:0.973]
Epoch [94/120    avg_loss:0.018, val_acc:0.966]
Epoch [95/120    avg_loss:0.015, val_acc:0.970]
Epoch [96/120    avg_loss:0.014, val_acc:0.968]
Epoch [97/120    avg_loss:0.026, val_acc:0.965]
Epoch [98/120    avg_loss:0.022, val_acc:0.963]
Epoch [99/120    avg_loss:0.039, val_acc:0.969]
Epoch [100/120    avg_loss:0.029, val_acc:0.965]
Epoch [101/120    avg_loss:0.020, val_acc:0.968]
Epoch [102/120    avg_loss:0.022, val_acc:0.969]
Epoch [103/120    avg_loss:0.013, val_acc:0.968]
Epoch [104/120    avg_loss:0.017, val_acc:0.972]
Epoch [105/120    avg_loss:0.019, val_acc:0.967]
Epoch [106/120    avg_loss:0.018, val_acc:0.972]
Epoch [107/120    avg_loss:0.010, val_acc:0.974]
Epoch [108/120    avg_loss:0.010, val_acc:0.974]
Epoch [109/120    avg_loss:0.008, val_acc:0.973]
Epoch [110/120    avg_loss:0.010, val_acc:0.974]
Epoch [111/120    avg_loss:0.008, val_acc:0.974]
Epoch [112/120    avg_loss:0.010, val_acc:0.973]
Epoch [113/120    avg_loss:0.008, val_acc:0.974]
Epoch [114/120    avg_loss:0.012, val_acc:0.972]
Epoch [115/120    avg_loss:0.008, val_acc:0.972]
Epoch [116/120    avg_loss:0.007, val_acc:0.973]
Epoch [117/120    avg_loss:0.009, val_acc:0.972]
Epoch [118/120    avg_loss:0.007, val_acc:0.972]
Epoch [119/120    avg_loss:0.007, val_acc:0.971]
Epoch [120/120    avg_loss:0.010, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1245    0   13    0    1    0    0    0    6   20    0    0
     0    0    0]
 [   0    0    1  727    0    0    0    0    0    5    0    0   14    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   15    0    0    3    0    0    0    0  853    3    0    0
     1    0    0]
 [   0    0    9    0    0    2    1    0    0    0   15 2177    5    1
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0  527    0
     0    2    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    63  283    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.77777777777777

F1 scores:
[       nan 0.96202532 0.97455969 0.98643148 0.97038724 0.98737084
 0.99543379 1.         0.99883856 0.82926829 0.97207977 0.98685403
 0.97232472 0.99730458 0.96884336 0.89556962 0.95857988]

Kappa:
0.9746587366485261
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f34d0a427f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.694, val_acc:0.202]
Epoch [2/120    avg_loss:2.377, val_acc:0.430]
Epoch [3/120    avg_loss:2.160, val_acc:0.529]
Epoch [4/120    avg_loss:2.003, val_acc:0.532]
Epoch [5/120    avg_loss:1.806, val_acc:0.566]
Epoch [6/120    avg_loss:1.643, val_acc:0.562]
Epoch [7/120    avg_loss:1.458, val_acc:0.627]
Epoch [8/120    avg_loss:1.304, val_acc:0.633]
Epoch [9/120    avg_loss:1.245, val_acc:0.649]
Epoch [10/120    avg_loss:1.071, val_acc:0.672]
Epoch [11/120    avg_loss:1.053, val_acc:0.698]
Epoch [12/120    avg_loss:0.970, val_acc:0.717]
Epoch [13/120    avg_loss:0.816, val_acc:0.765]
Epoch [14/120    avg_loss:0.761, val_acc:0.769]
Epoch [15/120    avg_loss:0.692, val_acc:0.802]
Epoch [16/120    avg_loss:0.634, val_acc:0.807]
Epoch [17/120    avg_loss:0.604, val_acc:0.803]
Epoch [18/120    avg_loss:0.555, val_acc:0.822]
Epoch [19/120    avg_loss:0.511, val_acc:0.837]
Epoch [20/120    avg_loss:0.454, val_acc:0.821]
Epoch [21/120    avg_loss:0.437, val_acc:0.832]
Epoch [22/120    avg_loss:0.458, val_acc:0.857]
Epoch [23/120    avg_loss:0.362, val_acc:0.859]
Epoch [24/120    avg_loss:0.366, val_acc:0.885]
Epoch [25/120    avg_loss:0.333, val_acc:0.845]
Epoch [26/120    avg_loss:0.289, val_acc:0.864]
Epoch [27/120    avg_loss:0.278, val_acc:0.833]
Epoch [28/120    avg_loss:0.308, val_acc:0.888]
Epoch [29/120    avg_loss:0.259, val_acc:0.896]
Epoch [30/120    avg_loss:0.238, val_acc:0.911]
Epoch [31/120    avg_loss:0.214, val_acc:0.916]
Epoch [32/120    avg_loss:0.175, val_acc:0.899]
Epoch [33/120    avg_loss:0.168, val_acc:0.917]
Epoch [34/120    avg_loss:0.153, val_acc:0.930]
Epoch [35/120    avg_loss:0.155, val_acc:0.890]
Epoch [36/120    avg_loss:0.200, val_acc:0.927]
Epoch [37/120    avg_loss:0.212, val_acc:0.893]
Epoch [38/120    avg_loss:0.179, val_acc:0.909]
Epoch [39/120    avg_loss:0.131, val_acc:0.921]
Epoch [40/120    avg_loss:0.129, val_acc:0.923]
Epoch [41/120    avg_loss:0.141, val_acc:0.923]
Epoch [42/120    avg_loss:0.225, val_acc:0.929]
Epoch [43/120    avg_loss:0.125, val_acc:0.939]
Epoch [44/120    avg_loss:0.126, val_acc:0.928]
Epoch [45/120    avg_loss:0.104, val_acc:0.927]
Epoch [46/120    avg_loss:0.141, val_acc:0.920]
Epoch [47/120    avg_loss:0.128, val_acc:0.912]
Epoch [48/120    avg_loss:0.106, val_acc:0.938]
Epoch [49/120    avg_loss:0.081, val_acc:0.942]
Epoch [50/120    avg_loss:0.115, val_acc:0.945]
Epoch [51/120    avg_loss:0.077, val_acc:0.944]
Epoch [52/120    avg_loss:0.068, val_acc:0.948]
Epoch [53/120    avg_loss:0.071, val_acc:0.948]
Epoch [54/120    avg_loss:0.074, val_acc:0.931]
Epoch [55/120    avg_loss:0.081, val_acc:0.952]
Epoch [56/120    avg_loss:0.091, val_acc:0.932]
Epoch [57/120    avg_loss:0.081, val_acc:0.935]
Epoch [58/120    avg_loss:0.081, val_acc:0.947]
Epoch [59/120    avg_loss:0.060, val_acc:0.943]
Epoch [60/120    avg_loss:0.114, val_acc:0.946]
Epoch [61/120    avg_loss:0.080, val_acc:0.953]
Epoch [62/120    avg_loss:0.061, val_acc:0.951]
Epoch [63/120    avg_loss:0.054, val_acc:0.948]
Epoch [64/120    avg_loss:0.048, val_acc:0.961]
Epoch [65/120    avg_loss:0.038, val_acc:0.961]
Epoch [66/120    avg_loss:0.040, val_acc:0.963]
Epoch [67/120    avg_loss:0.042, val_acc:0.963]
Epoch [68/120    avg_loss:0.048, val_acc:0.950]
Epoch [69/120    avg_loss:0.045, val_acc:0.966]
Epoch [70/120    avg_loss:0.041, val_acc:0.964]
Epoch [71/120    avg_loss:0.036, val_acc:0.966]
Epoch [72/120    avg_loss:0.038, val_acc:0.959]
Epoch [73/120    avg_loss:0.037, val_acc:0.966]
Epoch [74/120    avg_loss:0.040, val_acc:0.956]
Epoch [75/120    avg_loss:0.041, val_acc:0.965]
Epoch [76/120    avg_loss:0.035, val_acc:0.961]
Epoch [77/120    avg_loss:0.031, val_acc:0.966]
Epoch [78/120    avg_loss:0.093, val_acc:0.927]
Epoch [79/120    avg_loss:0.090, val_acc:0.956]
Epoch [80/120    avg_loss:0.052, val_acc:0.955]
Epoch [81/120    avg_loss:0.037, val_acc:0.958]
Epoch [82/120    avg_loss:0.023, val_acc:0.964]
Epoch [83/120    avg_loss:0.039, val_acc:0.962]
Epoch [84/120    avg_loss:0.044, val_acc:0.956]
Epoch [85/120    avg_loss:0.041, val_acc:0.942]
Epoch [86/120    avg_loss:0.096, val_acc:0.955]
Epoch [87/120    avg_loss:0.039, val_acc:0.947]
Epoch [88/120    avg_loss:0.057, val_acc:0.945]
Epoch [89/120    avg_loss:0.151, val_acc:0.956]
Epoch [90/120    avg_loss:0.047, val_acc:0.961]
Epoch [91/120    avg_loss:0.038, val_acc:0.961]
Epoch [92/120    avg_loss:0.033, val_acc:0.964]
Epoch [93/120    avg_loss:0.029, val_acc:0.967]
Epoch [94/120    avg_loss:0.026, val_acc:0.971]
Epoch [95/120    avg_loss:0.024, val_acc:0.971]
Epoch [96/120    avg_loss:0.021, val_acc:0.971]
Epoch [97/120    avg_loss:0.023, val_acc:0.974]
Epoch [98/120    avg_loss:0.026, val_acc:0.977]
Epoch [99/120    avg_loss:0.026, val_acc:0.976]
Epoch [100/120    avg_loss:0.020, val_acc:0.970]
Epoch [101/120    avg_loss:0.026, val_acc:0.975]
Epoch [102/120    avg_loss:0.018, val_acc:0.973]
Epoch [103/120    avg_loss:0.025, val_acc:0.974]
Epoch [104/120    avg_loss:0.019, val_acc:0.975]
Epoch [105/120    avg_loss:0.021, val_acc:0.975]
Epoch [106/120    avg_loss:0.022, val_acc:0.976]
Epoch [107/120    avg_loss:0.022, val_acc:0.978]
Epoch [108/120    avg_loss:0.018, val_acc:0.977]
Epoch [109/120    avg_loss:0.019, val_acc:0.979]
Epoch [110/120    avg_loss:0.019, val_acc:0.977]
Epoch [111/120    avg_loss:0.020, val_acc:0.975]
Epoch [112/120    avg_loss:0.033, val_acc:0.979]
Epoch [113/120    avg_loss:0.019, val_acc:0.976]
Epoch [114/120    avg_loss:0.018, val_acc:0.973]
Epoch [115/120    avg_loss:0.025, val_acc:0.978]
Epoch [116/120    avg_loss:0.016, val_acc:0.980]
Epoch [117/120    avg_loss:0.019, val_acc:0.977]
Epoch [118/120    avg_loss:0.018, val_acc:0.978]
Epoch [119/120    avg_loss:0.013, val_acc:0.977]
Epoch [120/120    avg_loss:0.021, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    0    0    0    0    0    0    0    0    6    0    0    0
     0    0    0]
 [   0    0 1260    0    3    0    1    0    0    0    5   15    1    0
     0    0    0]
 [   0    0    0  701    6    8    0    0    0    6    1    1   24    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   15    0    0    3    1    0    0    0  847    1    1    0
     3    4    0]
 [   0    0   23    0    0    1    8    0    0    0   15 2161    2    0
     0    0    0]
 [   0    0    0    1    0    2    0    0    0    0    9    1  516    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   14    1    0    1    0    1    0    0    0
  1122    0    0]
 [   0    0    0    0    0    0   44    0    0    0    0    0    0    0
     7  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.34417344173441

F1 scores:
[       nan 0.92105263 0.97560976 0.96756384 0.97931034 0.96420582
 0.95982469 1.         0.99883856 0.82926829 0.96304719 0.98473456
 0.95644115 1.         0.98637363 0.91499227 0.97109827]

Kappa:
0.9697382788409015
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f715e9e1898>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.708, val_acc:0.330]
Epoch [2/120    avg_loss:2.341, val_acc:0.422]
Epoch [3/120    avg_loss:2.107, val_acc:0.489]
Epoch [4/120    avg_loss:1.955, val_acc:0.525]
Epoch [5/120    avg_loss:1.775, val_acc:0.562]
Epoch [6/120    avg_loss:1.668, val_acc:0.632]
Epoch [7/120    avg_loss:1.474, val_acc:0.651]
Epoch [8/120    avg_loss:1.345, val_acc:0.683]
Epoch [9/120    avg_loss:1.274, val_acc:0.643]
Epoch [10/120    avg_loss:1.098, val_acc:0.691]
Epoch [11/120    avg_loss:0.954, val_acc:0.747]
Epoch [12/120    avg_loss:0.860, val_acc:0.670]
Epoch [13/120    avg_loss:0.854, val_acc:0.753]
Epoch [14/120    avg_loss:0.689, val_acc:0.774]
Epoch [15/120    avg_loss:0.640, val_acc:0.787]
Epoch [16/120    avg_loss:0.627, val_acc:0.787]
Epoch [17/120    avg_loss:0.582, val_acc:0.830]
Epoch [18/120    avg_loss:0.485, val_acc:0.854]
Epoch [19/120    avg_loss:0.414, val_acc:0.790]
Epoch [20/120    avg_loss:0.396, val_acc:0.848]
Epoch [21/120    avg_loss:0.402, val_acc:0.870]
Epoch [22/120    avg_loss:0.329, val_acc:0.876]
Epoch [23/120    avg_loss:0.291, val_acc:0.855]
Epoch [24/120    avg_loss:0.274, val_acc:0.892]
Epoch [25/120    avg_loss:0.233, val_acc:0.902]
Epoch [26/120    avg_loss:0.211, val_acc:0.886]
Epoch [27/120    avg_loss:0.230, val_acc:0.890]
Epoch [28/120    avg_loss:0.240, val_acc:0.896]
Epoch [29/120    avg_loss:0.237, val_acc:0.901]
Epoch [30/120    avg_loss:0.212, val_acc:0.894]
Epoch [31/120    avg_loss:0.189, val_acc:0.926]
Epoch [32/120    avg_loss:0.148, val_acc:0.915]
Epoch [33/120    avg_loss:0.152, val_acc:0.928]
Epoch [34/120    avg_loss:0.173, val_acc:0.912]
Epoch [35/120    avg_loss:0.173, val_acc:0.911]
Epoch [36/120    avg_loss:0.163, val_acc:0.925]
Epoch [37/120    avg_loss:0.176, val_acc:0.920]
Epoch [38/120    avg_loss:0.131, val_acc:0.917]
Epoch [39/120    avg_loss:0.126, val_acc:0.932]
Epoch [40/120    avg_loss:0.091, val_acc:0.929]
Epoch [41/120    avg_loss:0.130, val_acc:0.938]
Epoch [42/120    avg_loss:0.144, val_acc:0.913]
Epoch [43/120    avg_loss:0.112, val_acc:0.936]
Epoch [44/120    avg_loss:0.104, val_acc:0.937]
Epoch [45/120    avg_loss:0.162, val_acc:0.894]
Epoch [46/120    avg_loss:0.108, val_acc:0.944]
Epoch [47/120    avg_loss:0.113, val_acc:0.902]
Epoch [48/120    avg_loss:0.127, val_acc:0.939]
Epoch [49/120    avg_loss:0.086, val_acc:0.963]
Epoch [50/120    avg_loss:0.092, val_acc:0.959]
Epoch [51/120    avg_loss:0.070, val_acc:0.941]
Epoch [52/120    avg_loss:0.067, val_acc:0.960]
Epoch [53/120    avg_loss:0.077, val_acc:0.950]
Epoch [54/120    avg_loss:0.095, val_acc:0.949]
Epoch [55/120    avg_loss:0.102, val_acc:0.951]
Epoch [56/120    avg_loss:0.092, val_acc:0.951]
Epoch [57/120    avg_loss:0.069, val_acc:0.955]
Epoch [58/120    avg_loss:0.063, val_acc:0.959]
Epoch [59/120    avg_loss:0.061, val_acc:0.956]
Epoch [60/120    avg_loss:0.079, val_acc:0.938]
Epoch [61/120    avg_loss:0.086, val_acc:0.953]
Epoch [62/120    avg_loss:0.055, val_acc:0.960]
Epoch [63/120    avg_loss:0.043, val_acc:0.962]
Epoch [64/120    avg_loss:0.040, val_acc:0.966]
Epoch [65/120    avg_loss:0.035, val_acc:0.965]
Epoch [66/120    avg_loss:0.030, val_acc:0.965]
Epoch [67/120    avg_loss:0.032, val_acc:0.966]
Epoch [68/120    avg_loss:0.030, val_acc:0.967]
Epoch [69/120    avg_loss:0.028, val_acc:0.968]
Epoch [70/120    avg_loss:0.030, val_acc:0.967]
Epoch [71/120    avg_loss:0.028, val_acc:0.969]
Epoch [72/120    avg_loss:0.029, val_acc:0.967]
Epoch [73/120    avg_loss:0.032, val_acc:0.967]
Epoch [74/120    avg_loss:0.032, val_acc:0.972]
Epoch [75/120    avg_loss:0.025, val_acc:0.970]
Epoch [76/120    avg_loss:0.031, val_acc:0.970]
Epoch [77/120    avg_loss:0.025, val_acc:0.968]
Epoch [78/120    avg_loss:0.035, val_acc:0.970]
Epoch [79/120    avg_loss:0.028, val_acc:0.973]
Epoch [80/120    avg_loss:0.030, val_acc:0.971]
Epoch [81/120    avg_loss:0.027, val_acc:0.974]
Epoch [82/120    avg_loss:0.027, val_acc:0.971]
Epoch [83/120    avg_loss:0.027, val_acc:0.973]
Epoch [84/120    avg_loss:0.021, val_acc:0.973]
Epoch [85/120    avg_loss:0.023, val_acc:0.972]
Epoch [86/120    avg_loss:0.026, val_acc:0.970]
Epoch [87/120    avg_loss:0.027, val_acc:0.972]
Epoch [88/120    avg_loss:0.023, val_acc:0.976]
Epoch [89/120    avg_loss:0.029, val_acc:0.975]
Epoch [90/120    avg_loss:0.024, val_acc:0.972]
Epoch [91/120    avg_loss:0.031, val_acc:0.974]
Epoch [92/120    avg_loss:0.023, val_acc:0.976]
Epoch [93/120    avg_loss:0.024, val_acc:0.973]
Epoch [94/120    avg_loss:0.026, val_acc:0.973]
Epoch [95/120    avg_loss:0.021, val_acc:0.974]
Epoch [96/120    avg_loss:0.026, val_acc:0.973]
Epoch [97/120    avg_loss:0.021, val_acc:0.975]
Epoch [98/120    avg_loss:0.020, val_acc:0.972]
Epoch [99/120    avg_loss:0.021, val_acc:0.974]
Epoch [100/120    avg_loss:0.024, val_acc:0.973]
Epoch [101/120    avg_loss:0.025, val_acc:0.976]
Epoch [102/120    avg_loss:0.026, val_acc:0.976]
Epoch [103/120    avg_loss:0.024, val_acc:0.971]
Epoch [104/120    avg_loss:0.025, val_acc:0.974]
Epoch [105/120    avg_loss:0.031, val_acc:0.976]
Epoch [106/120    avg_loss:0.021, val_acc:0.973]
Epoch [107/120    avg_loss:0.018, val_acc:0.972]
Epoch [108/120    avg_loss:0.022, val_acc:0.974]
Epoch [109/120    avg_loss:0.022, val_acc:0.979]
Epoch [110/120    avg_loss:0.022, val_acc:0.972]
Epoch [111/120    avg_loss:0.024, val_acc:0.976]
Epoch [112/120    avg_loss:0.021, val_acc:0.978]
Epoch [113/120    avg_loss:0.023, val_acc:0.973]
Epoch [114/120    avg_loss:0.021, val_acc:0.976]
Epoch [115/120    avg_loss:0.022, val_acc:0.974]
Epoch [116/120    avg_loss:0.019, val_acc:0.975]
Epoch [117/120    avg_loss:0.022, val_acc:0.976]
Epoch [118/120    avg_loss:0.018, val_acc:0.976]
Epoch [119/120    avg_loss:0.022, val_acc:0.978]
Epoch [120/120    avg_loss:0.022, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1246    2    1    0    1    0    0    0    7   21    7    0
     0    0    0]
 [   0    0    2  712    5    1    0    0    0    8    0    0   19    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    4    0    0    2    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    3    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   16    0    0    3    2    0    0    0  844    9    0    0
     0    1    0]
 [   0    0    7    3    0    0    1    0    0    1    7 2185    4    2
     0    0    0]
 [   0    0    0    0    4    0    0    0    0    0    1    0  525    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0    0    0
  1130    7    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    52  276    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.4308943089431

F1 scores:
[       nan 1.         0.97496088 0.9726776  0.97706422 0.9849711
 0.97451274 1.         1.         0.73913043 0.97235023 0.98667871
 0.9624198  0.99462366 0.97120756 0.8748019  0.97076023]

Kappa:
0.9706991189612756
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fca00756898>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.688, val_acc:0.545]
Epoch [2/120    avg_loss:2.363, val_acc:0.473]
Epoch [3/120    avg_loss:2.130, val_acc:0.580]
Epoch [4/120    avg_loss:1.947, val_acc:0.575]
Epoch [5/120    avg_loss:1.782, val_acc:0.606]
Epoch [6/120    avg_loss:1.625, val_acc:0.636]
Epoch [7/120    avg_loss:1.444, val_acc:0.632]
Epoch [8/120    avg_loss:1.256, val_acc:0.662]
Epoch [9/120    avg_loss:1.138, val_acc:0.706]
Epoch [10/120    avg_loss:1.086, val_acc:0.682]
Epoch [11/120    avg_loss:0.999, val_acc:0.719]
Epoch [12/120    avg_loss:0.895, val_acc:0.739]
Epoch [13/120    avg_loss:0.769, val_acc:0.739]
Epoch [14/120    avg_loss:0.697, val_acc:0.764]
Epoch [15/120    avg_loss:0.831, val_acc:0.756]
Epoch [16/120    avg_loss:0.621, val_acc:0.784]
Epoch [17/120    avg_loss:0.511, val_acc:0.804]
Epoch [18/120    avg_loss:0.524, val_acc:0.834]
Epoch [19/120    avg_loss:0.476, val_acc:0.837]
Epoch [20/120    avg_loss:0.446, val_acc:0.827]
Epoch [21/120    avg_loss:0.421, val_acc:0.826]
Epoch [22/120    avg_loss:0.364, val_acc:0.825]
Epoch [23/120    avg_loss:0.345, val_acc:0.811]
Epoch [24/120    avg_loss:0.285, val_acc:0.876]
Epoch [25/120    avg_loss:0.295, val_acc:0.860]
Epoch [26/120    avg_loss:0.281, val_acc:0.896]
Epoch [27/120    avg_loss:0.260, val_acc:0.911]
Epoch [28/120    avg_loss:0.330, val_acc:0.844]
Epoch [29/120    avg_loss:0.282, val_acc:0.869]
Epoch [30/120    avg_loss:0.276, val_acc:0.886]
Epoch [31/120    avg_loss:0.293, val_acc:0.904]
Epoch [32/120    avg_loss:0.216, val_acc:0.904]
Epoch [33/120    avg_loss:0.173, val_acc:0.923]
Epoch [34/120    avg_loss:0.178, val_acc:0.909]
Epoch [35/120    avg_loss:0.176, val_acc:0.915]
Epoch [36/120    avg_loss:0.207, val_acc:0.894]
Epoch [37/120    avg_loss:0.193, val_acc:0.918]
Epoch [38/120    avg_loss:0.139, val_acc:0.912]
Epoch [39/120    avg_loss:0.159, val_acc:0.913]
Epoch [40/120    avg_loss:0.150, val_acc:0.922]
Epoch [41/120    avg_loss:0.152, val_acc:0.920]
Epoch [42/120    avg_loss:0.123, val_acc:0.938]
Epoch [43/120    avg_loss:0.109, val_acc:0.919]
Epoch [44/120    avg_loss:0.094, val_acc:0.937]
Epoch [45/120    avg_loss:0.096, val_acc:0.937]
Epoch [46/120    avg_loss:0.108, val_acc:0.922]
Epoch [47/120    avg_loss:0.112, val_acc:0.927]
Epoch [48/120    avg_loss:0.107, val_acc:0.944]
Epoch [49/120    avg_loss:0.088, val_acc:0.941]
Epoch [50/120    avg_loss:0.086, val_acc:0.923]
Epoch [51/120    avg_loss:0.080, val_acc:0.950]
Epoch [52/120    avg_loss:0.073, val_acc:0.954]
Epoch [53/120    avg_loss:0.076, val_acc:0.950]
Epoch [54/120    avg_loss:0.059, val_acc:0.949]
Epoch [55/120    avg_loss:0.070, val_acc:0.941]
Epoch [56/120    avg_loss:0.092, val_acc:0.928]
Epoch [57/120    avg_loss:0.064, val_acc:0.954]
Epoch [58/120    avg_loss:0.073, val_acc:0.944]
Epoch [59/120    avg_loss:0.065, val_acc:0.953]
Epoch [60/120    avg_loss:0.046, val_acc:0.959]
Epoch [61/120    avg_loss:0.071, val_acc:0.953]
Epoch [62/120    avg_loss:0.061, val_acc:0.947]
Epoch [63/120    avg_loss:0.045, val_acc:0.956]
Epoch [64/120    avg_loss:0.064, val_acc:0.948]
Epoch [65/120    avg_loss:0.066, val_acc:0.949]
Epoch [66/120    avg_loss:0.055, val_acc:0.921]
Epoch [67/120    avg_loss:0.059, val_acc:0.948]
Epoch [68/120    avg_loss:0.049, val_acc:0.952]
Epoch [69/120    avg_loss:0.051, val_acc:0.948]
Epoch [70/120    avg_loss:0.098, val_acc:0.933]
Epoch [71/120    avg_loss:0.175, val_acc:0.934]
Epoch [72/120    avg_loss:0.137, val_acc:0.928]
Epoch [73/120    avg_loss:0.085, val_acc:0.949]
Epoch [74/120    avg_loss:0.052, val_acc:0.959]
Epoch [75/120    avg_loss:0.043, val_acc:0.957]
Epoch [76/120    avg_loss:0.045, val_acc:0.959]
Epoch [77/120    avg_loss:0.057, val_acc:0.958]
Epoch [78/120    avg_loss:0.051, val_acc:0.962]
Epoch [79/120    avg_loss:0.046, val_acc:0.958]
Epoch [80/120    avg_loss:0.038, val_acc:0.963]
Epoch [81/120    avg_loss:0.036, val_acc:0.965]
Epoch [82/120    avg_loss:0.039, val_acc:0.963]
Epoch [83/120    avg_loss:0.035, val_acc:0.962]
Epoch [84/120    avg_loss:0.033, val_acc:0.963]
Epoch [85/120    avg_loss:0.032, val_acc:0.964]
Epoch [86/120    avg_loss:0.045, val_acc:0.968]
Epoch [87/120    avg_loss:0.035, val_acc:0.966]
Epoch [88/120    avg_loss:0.034, val_acc:0.968]
Epoch [89/120    avg_loss:0.036, val_acc:0.965]
Epoch [90/120    avg_loss:0.033, val_acc:0.962]
Epoch [91/120    avg_loss:0.031, val_acc:0.969]
Epoch [92/120    avg_loss:0.030, val_acc:0.968]
Epoch [93/120    avg_loss:0.040, val_acc:0.966]
Epoch [94/120    avg_loss:0.043, val_acc:0.969]
Epoch [95/120    avg_loss:0.030, val_acc:0.968]
Epoch [96/120    avg_loss:0.032, val_acc:0.968]
Epoch [97/120    avg_loss:0.034, val_acc:0.969]
Epoch [98/120    avg_loss:0.034, val_acc:0.966]
Epoch [99/120    avg_loss:0.030, val_acc:0.970]
Epoch [100/120    avg_loss:0.033, val_acc:0.971]
Epoch [101/120    avg_loss:0.031, val_acc:0.972]
Epoch [102/120    avg_loss:0.031, val_acc:0.972]
Epoch [103/120    avg_loss:0.028, val_acc:0.967]
Epoch [104/120    avg_loss:0.033, val_acc:0.969]
Epoch [105/120    avg_loss:0.027, val_acc:0.971]
Epoch [106/120    avg_loss:0.030, val_acc:0.971]
Epoch [107/120    avg_loss:0.025, val_acc:0.971]
Epoch [108/120    avg_loss:0.029, val_acc:0.969]
Epoch [109/120    avg_loss:0.029, val_acc:0.971]
Epoch [110/120    avg_loss:0.026, val_acc:0.971]
Epoch [111/120    avg_loss:0.028, val_acc:0.972]
Epoch [112/120    avg_loss:0.028, val_acc:0.969]
Epoch [113/120    avg_loss:0.030, val_acc:0.972]
Epoch [114/120    avg_loss:0.028, val_acc:0.969]
Epoch [115/120    avg_loss:0.024, val_acc:0.971]
Epoch [116/120    avg_loss:0.031, val_acc:0.970]
Epoch [117/120    avg_loss:0.025, val_acc:0.970]
Epoch [118/120    avg_loss:0.027, val_acc:0.971]
Epoch [119/120    avg_loss:0.033, val_acc:0.970]
Epoch [120/120    avg_loss:0.026, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1245    5    0    0    2    0    0    1    9   20    1    0
     0    2    0]
 [   0    0    0  676    0   13    0    0    0    7    0    1   45    5
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  420    0    6    0    0    0    0    0    0
     9    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0    6    0    0    3    0    0    0    0  843   13    9    0
     1    0    0]
 [   0    0    8    0    0    0    3    0    1    0   10 2180    7    1
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    5  524    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1132    6    0]
 [   0    0    0    0    0    0   32    0    0    0    0    0    0    0
     8  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.23577235772358

F1 scores:
[       nan 0.96202532 0.97877358 0.94677871 1.         0.96219931
 0.96879643 0.89285714 0.99883856 0.76190476 0.96840896 0.98419865
 0.93404635 0.98404255 0.98692241 0.92749245 0.98823529]

Kappa:
0.9684888238553552
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff47e79e908>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.699, val_acc:0.327]
Epoch [2/120    avg_loss:2.351, val_acc:0.416]
Epoch [3/120    avg_loss:2.167, val_acc:0.543]
Epoch [4/120    avg_loss:1.977, val_acc:0.585]
Epoch [5/120    avg_loss:1.850, val_acc:0.592]
Epoch [6/120    avg_loss:1.691, val_acc:0.610]
Epoch [7/120    avg_loss:1.545, val_acc:0.677]
Epoch [8/120    avg_loss:1.363, val_acc:0.691]
Epoch [9/120    avg_loss:1.241, val_acc:0.692]
Epoch [10/120    avg_loss:1.090, val_acc:0.672]
Epoch [11/120    avg_loss:1.003, val_acc:0.718]
Epoch [12/120    avg_loss:0.918, val_acc:0.743]
Epoch [13/120    avg_loss:0.729, val_acc:0.766]
Epoch [14/120    avg_loss:0.690, val_acc:0.784]
Epoch [15/120    avg_loss:0.716, val_acc:0.794]
Epoch [16/120    avg_loss:0.600, val_acc:0.812]
Epoch [17/120    avg_loss:0.512, val_acc:0.815]
Epoch [18/120    avg_loss:0.499, val_acc:0.802]
Epoch [19/120    avg_loss:0.467, val_acc:0.806]
Epoch [20/120    avg_loss:0.396, val_acc:0.839]
Epoch [21/120    avg_loss:0.423, val_acc:0.846]
Epoch [22/120    avg_loss:0.324, val_acc:0.860]
Epoch [23/120    avg_loss:0.305, val_acc:0.851]
Epoch [24/120    avg_loss:0.335, val_acc:0.857]
Epoch [25/120    avg_loss:0.316, val_acc:0.870]
Epoch [26/120    avg_loss:0.305, val_acc:0.804]
Epoch [27/120    avg_loss:0.296, val_acc:0.876]
Epoch [28/120    avg_loss:0.239, val_acc:0.877]
Epoch [29/120    avg_loss:0.220, val_acc:0.894]
Epoch [30/120    avg_loss:0.215, val_acc:0.890]
Epoch [31/120    avg_loss:0.216, val_acc:0.849]
Epoch [32/120    avg_loss:0.228, val_acc:0.891]
Epoch [33/120    avg_loss:0.200, val_acc:0.875]
Epoch [34/120    avg_loss:0.215, val_acc:0.892]
Epoch [35/120    avg_loss:0.208, val_acc:0.901]
Epoch [36/120    avg_loss:0.217, val_acc:0.879]
Epoch [37/120    avg_loss:0.267, val_acc:0.879]
Epoch [38/120    avg_loss:0.187, val_acc:0.890]
Epoch [39/120    avg_loss:0.169, val_acc:0.901]
Epoch [40/120    avg_loss:0.184, val_acc:0.925]
Epoch [41/120    avg_loss:0.140, val_acc:0.935]
Epoch [42/120    avg_loss:0.156, val_acc:0.928]
Epoch [43/120    avg_loss:0.106, val_acc:0.921]
Epoch [44/120    avg_loss:0.087, val_acc:0.935]
Epoch [45/120    avg_loss:0.102, val_acc:0.913]
Epoch [46/120    avg_loss:0.114, val_acc:0.937]
Epoch [47/120    avg_loss:0.098, val_acc:0.905]
Epoch [48/120    avg_loss:0.086, val_acc:0.946]
Epoch [49/120    avg_loss:0.081, val_acc:0.946]
Epoch [50/120    avg_loss:0.102, val_acc:0.921]
Epoch [51/120    avg_loss:0.101, val_acc:0.907]
Epoch [52/120    avg_loss:0.114, val_acc:0.932]
Epoch [53/120    avg_loss:0.089, val_acc:0.931]
Epoch [54/120    avg_loss:0.089, val_acc:0.951]
Epoch [55/120    avg_loss:0.071, val_acc:0.950]
Epoch [56/120    avg_loss:0.066, val_acc:0.949]
Epoch [57/120    avg_loss:0.063, val_acc:0.949]
Epoch [58/120    avg_loss:0.064, val_acc:0.939]
Epoch [59/120    avg_loss:0.055, val_acc:0.960]
Epoch [60/120    avg_loss:0.050, val_acc:0.952]
Epoch [61/120    avg_loss:0.045, val_acc:0.960]
Epoch [62/120    avg_loss:0.040, val_acc:0.948]
Epoch [63/120    avg_loss:0.057, val_acc:0.944]
Epoch [64/120    avg_loss:0.049, val_acc:0.946]
Epoch [65/120    avg_loss:0.051, val_acc:0.955]
Epoch [66/120    avg_loss:0.053, val_acc:0.960]
Epoch [67/120    avg_loss:0.051, val_acc:0.953]
Epoch [68/120    avg_loss:0.053, val_acc:0.944]
Epoch [69/120    avg_loss:0.038, val_acc:0.963]
Epoch [70/120    avg_loss:0.055, val_acc:0.953]
Epoch [71/120    avg_loss:0.035, val_acc:0.954]
Epoch [72/120    avg_loss:0.041, val_acc:0.967]
Epoch [73/120    avg_loss:0.054, val_acc:0.948]
Epoch [74/120    avg_loss:0.049, val_acc:0.954]
Epoch [75/120    avg_loss:0.039, val_acc:0.959]
Epoch [76/120    avg_loss:0.039, val_acc:0.966]
Epoch [77/120    avg_loss:0.035, val_acc:0.959]
Epoch [78/120    avg_loss:0.036, val_acc:0.967]
Epoch [79/120    avg_loss:0.029, val_acc:0.963]
Epoch [80/120    avg_loss:0.041, val_acc:0.957]
Epoch [81/120    avg_loss:0.037, val_acc:0.963]
Epoch [82/120    avg_loss:0.062, val_acc:0.942]
Epoch [83/120    avg_loss:0.048, val_acc:0.968]
Epoch [84/120    avg_loss:0.061, val_acc:0.943]
Epoch [85/120    avg_loss:0.064, val_acc:0.956]
Epoch [86/120    avg_loss:0.044, val_acc:0.965]
Epoch [87/120    avg_loss:0.036, val_acc:0.975]
Epoch [88/120    avg_loss:0.032, val_acc:0.966]
Epoch [89/120    avg_loss:0.032, val_acc:0.974]
Epoch [90/120    avg_loss:0.023, val_acc:0.974]
Epoch [91/120    avg_loss:0.025, val_acc:0.964]
Epoch [92/120    avg_loss:0.037, val_acc:0.974]
Epoch [93/120    avg_loss:0.021, val_acc:0.962]
Epoch [94/120    avg_loss:0.047, val_acc:0.962]
Epoch [95/120    avg_loss:0.036, val_acc:0.960]
Epoch [96/120    avg_loss:0.045, val_acc:0.962]
Epoch [97/120    avg_loss:0.031, val_acc:0.971]
Epoch [98/120    avg_loss:0.028, val_acc:0.959]
Epoch [99/120    avg_loss:0.022, val_acc:0.965]
Epoch [100/120    avg_loss:0.024, val_acc:0.971]
Epoch [101/120    avg_loss:0.020, val_acc:0.976]
Epoch [102/120    avg_loss:0.017, val_acc:0.979]
Epoch [103/120    avg_loss:0.017, val_acc:0.976]
Epoch [104/120    avg_loss:0.016, val_acc:0.980]
Epoch [105/120    avg_loss:0.016, val_acc:0.978]
Epoch [106/120    avg_loss:0.014, val_acc:0.980]
Epoch [107/120    avg_loss:0.012, val_acc:0.979]
Epoch [108/120    avg_loss:0.014, val_acc:0.979]
Epoch [109/120    avg_loss:0.012, val_acc:0.978]
Epoch [110/120    avg_loss:0.012, val_acc:0.979]
Epoch [111/120    avg_loss:0.013, val_acc:0.976]
Epoch [112/120    avg_loss:0.013, val_acc:0.979]
Epoch [113/120    avg_loss:0.009, val_acc:0.980]
Epoch [114/120    avg_loss:0.016, val_acc:0.979]
Epoch [115/120    avg_loss:0.011, val_acc:0.980]
Epoch [116/120    avg_loss:0.012, val_acc:0.978]
Epoch [117/120    avg_loss:0.014, val_acc:0.976]
Epoch [118/120    avg_loss:0.014, val_acc:0.976]
Epoch [119/120    avg_loss:0.012, val_acc:0.976]
Epoch [120/120    avg_loss:0.011, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1251    5    0    0    2    0    0    0   11   15    0    0
     0    1    0]
 [   0    0    1  717    0    3    0    0    0   12    2    0    2   10
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    9    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   13    2    0    7    1    0    0    0  820   28    0    0
     1    3    0]
 [   0    0    4    0    0    0    1    0    0    1   15 2187    1    0
     0    1    0]
 [   0    0    2    6    2    3    0    0    0    0    0   10  507    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1134    4    0]
 [   0    0    0    0    0    0   24    0    0    0    0    0    0    0
     0  323    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.66937669376694

F1 scores:
[       nan 0.97619048 0.97887324 0.96957404 0.9953271  0.97365407
 0.97761194 0.84745763 0.997669   0.66666667 0.9512761  0.98247978
 0.97033493 0.97368421 0.99736148 0.94860499 0.98224852]

Kappa:
0.9734224275248174
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1df89ea828>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.664, val_acc:0.272]
Epoch [2/120    avg_loss:2.357, val_acc:0.502]
Epoch [3/120    avg_loss:2.183, val_acc:0.531]
Epoch [4/120    avg_loss:1.988, val_acc:0.561]
Epoch [5/120    avg_loss:1.829, val_acc:0.567]
Epoch [6/120    avg_loss:1.684, val_acc:0.616]
Epoch [7/120    avg_loss:1.561, val_acc:0.642]
Epoch [8/120    avg_loss:1.403, val_acc:0.675]
Epoch [9/120    avg_loss:1.252, val_acc:0.691]
Epoch [10/120    avg_loss:1.097, val_acc:0.696]
Epoch [11/120    avg_loss:1.014, val_acc:0.706]
Epoch [12/120    avg_loss:0.916, val_acc:0.747]
Epoch [13/120    avg_loss:0.772, val_acc:0.744]
Epoch [14/120    avg_loss:0.724, val_acc:0.762]
Epoch [15/120    avg_loss:0.727, val_acc:0.780]
Epoch [16/120    avg_loss:0.691, val_acc:0.718]
Epoch [17/120    avg_loss:0.595, val_acc:0.804]
Epoch [18/120    avg_loss:0.523, val_acc:0.812]
Epoch [19/120    avg_loss:0.526, val_acc:0.839]
Epoch [20/120    avg_loss:0.496, val_acc:0.763]
Epoch [21/120    avg_loss:0.433, val_acc:0.831]
Epoch [22/120    avg_loss:0.348, val_acc:0.873]
Epoch [23/120    avg_loss:0.323, val_acc:0.842]
Epoch [24/120    avg_loss:0.354, val_acc:0.865]
Epoch [25/120    avg_loss:0.345, val_acc:0.869]
Epoch [26/120    avg_loss:0.338, val_acc:0.838]
Epoch [27/120    avg_loss:0.318, val_acc:0.893]
Epoch [28/120    avg_loss:0.242, val_acc:0.881]
Epoch [29/120    avg_loss:0.233, val_acc:0.897]
Epoch [30/120    avg_loss:0.223, val_acc:0.893]
Epoch [31/120    avg_loss:0.224, val_acc:0.904]
Epoch [32/120    avg_loss:0.177, val_acc:0.893]
Epoch [33/120    avg_loss:0.201, val_acc:0.903]
Epoch [34/120    avg_loss:0.154, val_acc:0.929]
Epoch [35/120    avg_loss:0.141, val_acc:0.913]
Epoch [36/120    avg_loss:0.143, val_acc:0.920]
Epoch [37/120    avg_loss:0.138, val_acc:0.923]
Epoch [38/120    avg_loss:0.150, val_acc:0.929]
Epoch [39/120    avg_loss:0.250, val_acc:0.842]
Epoch [40/120    avg_loss:0.230, val_acc:0.920]
Epoch [41/120    avg_loss:0.153, val_acc:0.885]
Epoch [42/120    avg_loss:0.144, val_acc:0.932]
Epoch [43/120    avg_loss:0.132, val_acc:0.938]
Epoch [44/120    avg_loss:0.120, val_acc:0.946]
Epoch [45/120    avg_loss:0.161, val_acc:0.915]
Epoch [46/120    avg_loss:0.105, val_acc:0.931]
Epoch [47/120    avg_loss:0.123, val_acc:0.931]
Epoch [48/120    avg_loss:0.087, val_acc:0.951]
Epoch [49/120    avg_loss:0.096, val_acc:0.953]
Epoch [50/120    avg_loss:0.085, val_acc:0.943]
Epoch [51/120    avg_loss:0.121, val_acc:0.944]
Epoch [52/120    avg_loss:0.093, val_acc:0.946]
Epoch [53/120    avg_loss:0.072, val_acc:0.951]
Epoch [54/120    avg_loss:0.071, val_acc:0.943]
Epoch [55/120    avg_loss:0.074, val_acc:0.952]
Epoch [56/120    avg_loss:0.060, val_acc:0.950]
Epoch [57/120    avg_loss:0.058, val_acc:0.928]
Epoch [58/120    avg_loss:0.056, val_acc:0.956]
Epoch [59/120    avg_loss:0.053, val_acc:0.953]
Epoch [60/120    avg_loss:0.058, val_acc:0.954]
Epoch [61/120    avg_loss:0.055, val_acc:0.962]
Epoch [62/120    avg_loss:0.049, val_acc:0.951]
Epoch [63/120    avg_loss:0.054, val_acc:0.954]
Epoch [64/120    avg_loss:0.051, val_acc:0.965]
Epoch [65/120    avg_loss:0.059, val_acc:0.950]
Epoch [66/120    avg_loss:0.065, val_acc:0.957]
Epoch [67/120    avg_loss:0.056, val_acc:0.960]
Epoch [68/120    avg_loss:0.046, val_acc:0.948]
Epoch [69/120    avg_loss:0.078, val_acc:0.952]
Epoch [70/120    avg_loss:0.062, val_acc:0.942]
Epoch [71/120    avg_loss:0.049, val_acc:0.962]
Epoch [72/120    avg_loss:0.044, val_acc:0.962]
Epoch [73/120    avg_loss:0.035, val_acc:0.975]
Epoch [74/120    avg_loss:0.036, val_acc:0.975]
Epoch [75/120    avg_loss:0.044, val_acc:0.958]
Epoch [76/120    avg_loss:0.057, val_acc:0.943]
Epoch [77/120    avg_loss:0.044, val_acc:0.966]
Epoch [78/120    avg_loss:0.036, val_acc:0.963]
Epoch [79/120    avg_loss:0.037, val_acc:0.973]
Epoch [80/120    avg_loss:0.046, val_acc:0.951]
Epoch [81/120    avg_loss:0.043, val_acc:0.968]
Epoch [82/120    avg_loss:0.031, val_acc:0.954]
Epoch [83/120    avg_loss:0.060, val_acc:0.888]
Epoch [84/120    avg_loss:0.231, val_acc:0.887]
Epoch [85/120    avg_loss:0.113, val_acc:0.946]
Epoch [86/120    avg_loss:0.080, val_acc:0.943]
Epoch [87/120    avg_loss:0.070, val_acc:0.964]
Epoch [88/120    avg_loss:0.042, val_acc:0.972]
Epoch [89/120    avg_loss:0.035, val_acc:0.973]
Epoch [90/120    avg_loss:0.031, val_acc:0.969]
Epoch [91/120    avg_loss:0.029, val_acc:0.971]
Epoch [92/120    avg_loss:0.031, val_acc:0.971]
Epoch [93/120    avg_loss:0.026, val_acc:0.971]
Epoch [94/120    avg_loss:0.032, val_acc:0.968]
Epoch [95/120    avg_loss:0.028, val_acc:0.970]
Epoch [96/120    avg_loss:0.026, val_acc:0.970]
Epoch [97/120    avg_loss:0.026, val_acc:0.970]
Epoch [98/120    avg_loss:0.026, val_acc:0.971]
Epoch [99/120    avg_loss:0.028, val_acc:0.972]
Epoch [100/120    avg_loss:0.024, val_acc:0.971]
Epoch [101/120    avg_loss:0.027, val_acc:0.971]
Epoch [102/120    avg_loss:0.026, val_acc:0.971]
Epoch [103/120    avg_loss:0.021, val_acc:0.972]
Epoch [104/120    avg_loss:0.021, val_acc:0.972]
Epoch [105/120    avg_loss:0.026, val_acc:0.972]
Epoch [106/120    avg_loss:0.026, val_acc:0.972]
Epoch [107/120    avg_loss:0.023, val_acc:0.973]
Epoch [108/120    avg_loss:0.021, val_acc:0.973]
Epoch [109/120    avg_loss:0.022, val_acc:0.973]
Epoch [110/120    avg_loss:0.023, val_acc:0.972]
Epoch [111/120    avg_loss:0.021, val_acc:0.972]
Epoch [112/120    avg_loss:0.029, val_acc:0.972]
Epoch [113/120    avg_loss:0.022, val_acc:0.972]
Epoch [114/120    avg_loss:0.027, val_acc:0.972]
Epoch [115/120    avg_loss:0.025, val_acc:0.972]
Epoch [116/120    avg_loss:0.026, val_acc:0.972]
Epoch [117/120    avg_loss:0.023, val_acc:0.972]
Epoch [118/120    avg_loss:0.027, val_acc:0.972]
Epoch [119/120    avg_loss:0.023, val_acc:0.972]
Epoch [120/120    avg_loss:0.023, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1238    5    0    1    2    0    0    3    7   28    1    0
     0    0    0]
 [   0    0    0  679    0   15    0    0    0   14    1    0   36    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    3    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    5    6    0    0    0  838   20    0    0
     0    1    0]
 [   0    0    2    1    0    2    1    0    0    1   15 2173    5    2
     3    5    0]
 [   0    0    0    0    3    9    0    0    0    0    2    0  516    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   15    0    0    0    0    1    0    0    0
  1116    7    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
    47  283    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.67208672086721

F1 scores:
[       nan 0.98765432 0.97865613 0.94832402 0.99300699 0.9452954
 0.97524381 1.         1.         0.64285714 0.96321839 0.98015336
 0.94332724 0.98930481 0.96623377 0.87751938 0.97619048]

Kappa:
0.962063277099525
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f36b636f898>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.684, val_acc:0.486]
Epoch [2/120    avg_loss:2.338, val_acc:0.496]
Epoch [3/120    avg_loss:2.122, val_acc:0.519]
Epoch [4/120    avg_loss:1.923, val_acc:0.590]
Epoch [5/120    avg_loss:1.811, val_acc:0.631]
Epoch [6/120    avg_loss:1.689, val_acc:0.631]
Epoch [7/120    avg_loss:1.508, val_acc:0.667]
Epoch [8/120    avg_loss:1.397, val_acc:0.668]
Epoch [9/120    avg_loss:1.241, val_acc:0.689]
Epoch [10/120    avg_loss:1.075, val_acc:0.747]
Epoch [11/120    avg_loss:0.995, val_acc:0.784]
Epoch [12/120    avg_loss:0.853, val_acc:0.798]
Epoch [13/120    avg_loss:0.693, val_acc:0.815]
Epoch [14/120    avg_loss:0.739, val_acc:0.829]
Epoch [15/120    avg_loss:0.679, val_acc:0.811]
Epoch [16/120    avg_loss:0.614, val_acc:0.839]
Epoch [17/120    avg_loss:0.532, val_acc:0.858]
Epoch [18/120    avg_loss:0.464, val_acc:0.879]
Epoch [19/120    avg_loss:0.424, val_acc:0.884]
Epoch [20/120    avg_loss:0.402, val_acc:0.882]
Epoch [21/120    avg_loss:0.386, val_acc:0.868]
Epoch [22/120    avg_loss:0.318, val_acc:0.915]
Epoch [23/120    avg_loss:0.275, val_acc:0.905]
Epoch [24/120    avg_loss:0.261, val_acc:0.880]
Epoch [25/120    avg_loss:0.305, val_acc:0.924]
Epoch [26/120    avg_loss:0.290, val_acc:0.919]
Epoch [27/120    avg_loss:0.237, val_acc:0.927]
Epoch [28/120    avg_loss:0.213, val_acc:0.924]
Epoch [29/120    avg_loss:0.222, val_acc:0.941]
Epoch [30/120    avg_loss:0.197, val_acc:0.925]
Epoch [31/120    avg_loss:0.182, val_acc:0.920]
Epoch [32/120    avg_loss:0.206, val_acc:0.915]
Epoch [33/120    avg_loss:0.181, val_acc:0.944]
Epoch [34/120    avg_loss:0.169, val_acc:0.949]
Epoch [35/120    avg_loss:0.136, val_acc:0.952]
Epoch [36/120    avg_loss:0.115, val_acc:0.934]
Epoch [37/120    avg_loss:0.122, val_acc:0.962]
Epoch [38/120    avg_loss:0.105, val_acc:0.931]
Epoch [39/120    avg_loss:0.115, val_acc:0.957]
Epoch [40/120    avg_loss:0.117, val_acc:0.950]
Epoch [41/120    avg_loss:0.111, val_acc:0.959]
Epoch [42/120    avg_loss:0.088, val_acc:0.958]
Epoch [43/120    avg_loss:0.094, val_acc:0.957]
Epoch [44/120    avg_loss:0.090, val_acc:0.957]
Epoch [45/120    avg_loss:0.078, val_acc:0.966]
Epoch [46/120    avg_loss:0.070, val_acc:0.957]
Epoch [47/120    avg_loss:0.071, val_acc:0.967]
Epoch [48/120    avg_loss:0.059, val_acc:0.964]
Epoch [49/120    avg_loss:0.062, val_acc:0.967]
Epoch [50/120    avg_loss:0.083, val_acc:0.967]
Epoch [51/120    avg_loss:0.080, val_acc:0.955]
Epoch [52/120    avg_loss:0.059, val_acc:0.951]
Epoch [53/120    avg_loss:0.057, val_acc:0.955]
Epoch [54/120    avg_loss:0.062, val_acc:0.967]
Epoch [55/120    avg_loss:0.048, val_acc:0.974]
Epoch [56/120    avg_loss:0.039, val_acc:0.975]
Epoch [57/120    avg_loss:0.057, val_acc:0.978]
Epoch [58/120    avg_loss:0.049, val_acc:0.960]
Epoch [59/120    avg_loss:0.060, val_acc:0.966]
Epoch [60/120    avg_loss:0.046, val_acc:0.952]
Epoch [61/120    avg_loss:0.053, val_acc:0.980]
Epoch [62/120    avg_loss:0.070, val_acc:0.944]
Epoch [63/120    avg_loss:0.105, val_acc:0.975]
Epoch [64/120    avg_loss:0.047, val_acc:0.975]
Epoch [65/120    avg_loss:0.048, val_acc:0.971]
Epoch [66/120    avg_loss:0.049, val_acc:0.975]
Epoch [67/120    avg_loss:0.041, val_acc:0.973]
Epoch [68/120    avg_loss:0.046, val_acc:0.968]
Epoch [69/120    avg_loss:0.034, val_acc:0.973]
Epoch [70/120    avg_loss:0.034, val_acc:0.974]
Epoch [71/120    avg_loss:0.039, val_acc:0.983]
Epoch [72/120    avg_loss:0.042, val_acc:0.976]
Epoch [73/120    avg_loss:0.027, val_acc:0.976]
Epoch [74/120    avg_loss:0.030, val_acc:0.982]
Epoch [75/120    avg_loss:0.026, val_acc:0.984]
Epoch [76/120    avg_loss:0.018, val_acc:0.975]
Epoch [77/120    avg_loss:0.020, val_acc:0.981]
Epoch [78/120    avg_loss:0.027, val_acc:0.979]
Epoch [79/120    avg_loss:0.040, val_acc:0.979]
Epoch [80/120    avg_loss:0.032, val_acc:0.980]
Epoch [81/120    avg_loss:0.029, val_acc:0.979]
Epoch [82/120    avg_loss:0.020, val_acc:0.981]
Epoch [83/120    avg_loss:0.020, val_acc:0.979]
Epoch [84/120    avg_loss:0.024, val_acc:0.985]
Epoch [85/120    avg_loss:0.025, val_acc:0.988]
Epoch [86/120    avg_loss:0.021, val_acc:0.983]
Epoch [87/120    avg_loss:0.016, val_acc:0.982]
Epoch [88/120    avg_loss:0.016, val_acc:0.987]
Epoch [89/120    avg_loss:0.018, val_acc:0.988]
Epoch [90/120    avg_loss:0.014, val_acc:0.987]
Epoch [91/120    avg_loss:0.014, val_acc:0.986]
Epoch [92/120    avg_loss:0.013, val_acc:0.982]
Epoch [93/120    avg_loss:0.025, val_acc:0.986]
Epoch [94/120    avg_loss:0.019, val_acc:0.984]
Epoch [95/120    avg_loss:0.018, val_acc:0.987]
Epoch [96/120    avg_loss:0.013, val_acc:0.989]
Epoch [97/120    avg_loss:0.018, val_acc:0.988]
Epoch [98/120    avg_loss:0.025, val_acc:0.976]
Epoch [99/120    avg_loss:0.128, val_acc:0.912]
Epoch [100/120    avg_loss:0.330, val_acc:0.931]
Epoch [101/120    avg_loss:0.135, val_acc:0.959]
Epoch [102/120    avg_loss:0.100, val_acc:0.959]
Epoch [103/120    avg_loss:0.066, val_acc:0.972]
Epoch [104/120    avg_loss:0.050, val_acc:0.967]
Epoch [105/120    avg_loss:0.044, val_acc:0.985]
Epoch [106/120    avg_loss:0.031, val_acc:0.978]
Epoch [107/120    avg_loss:0.024, val_acc:0.984]
Epoch [108/120    avg_loss:0.027, val_acc:0.978]
Epoch [109/120    avg_loss:0.040, val_acc:0.980]
Epoch [110/120    avg_loss:0.022, val_acc:0.980]
Epoch [111/120    avg_loss:0.022, val_acc:0.984]
Epoch [112/120    avg_loss:0.025, val_acc:0.985]
Epoch [113/120    avg_loss:0.018, val_acc:0.984]
Epoch [114/120    avg_loss:0.016, val_acc:0.984]
Epoch [115/120    avg_loss:0.017, val_acc:0.987]
Epoch [116/120    avg_loss:0.017, val_acc:0.987]
Epoch [117/120    avg_loss:0.015, val_acc:0.987]
Epoch [118/120    avg_loss:0.014, val_acc:0.986]
Epoch [119/120    avg_loss:0.016, val_acc:0.987]
Epoch [120/120    avg_loss:0.018, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1260    1    0    0    0    0    0    3    9   11    1    0
     0    0    0]
 [   0    0    0  723    2    0    0    0    0    6    0    0   16    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    2    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    1    0    6    0    0    0    0  843   16    0    0
     0    1    0]
 [   0    0    8    1    0    1    2    0    2    0    8 2187    0    1
     0    0    0]
 [   0    0    1    0    0   13    0    0    0    0    1    3  509    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    0    0    0    0
  1131    1    0]
 [   0    0    0    0    0    0    2    0    0    7    0    0    0    0
    27  311    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.00542005420054

F1 scores:
[       nan 0.95121951 0.98360656 0.98167006 0.9953271  0.96868009
 0.99314547 1.         0.9941928  0.67924528 0.97008055 0.98758185
 0.96037736 0.99730458 0.98305085 0.94242424 0.96      ]

Kappa:
0.9772583726207206
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8a45aa1860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.668, val_acc:0.287]
Epoch [2/120    avg_loss:2.348, val_acc:0.478]
Epoch [3/120    avg_loss:2.128, val_acc:0.563]
Epoch [4/120    avg_loss:1.937, val_acc:0.550]
Epoch [5/120    avg_loss:1.776, val_acc:0.597]
Epoch [6/120    avg_loss:1.674, val_acc:0.606]
Epoch [7/120    avg_loss:1.525, val_acc:0.667]
Epoch [8/120    avg_loss:1.408, val_acc:0.674]
Epoch [9/120    avg_loss:1.282, val_acc:0.641]
Epoch [10/120    avg_loss:1.140, val_acc:0.681]
Epoch [11/120    avg_loss:1.016, val_acc:0.719]
Epoch [12/120    avg_loss:0.944, val_acc:0.755]
Epoch [13/120    avg_loss:0.868, val_acc:0.741]
Epoch [14/120    avg_loss:0.806, val_acc:0.738]
Epoch [15/120    avg_loss:0.668, val_acc:0.772]
Epoch [16/120    avg_loss:0.577, val_acc:0.772]
Epoch [17/120    avg_loss:0.657, val_acc:0.774]
Epoch [18/120    avg_loss:0.601, val_acc:0.792]
Epoch [19/120    avg_loss:0.534, val_acc:0.830]
Epoch [20/120    avg_loss:0.454, val_acc:0.857]
Epoch [21/120    avg_loss:0.387, val_acc:0.813]
Epoch [22/120    avg_loss:0.399, val_acc:0.816]
Epoch [23/120    avg_loss:0.394, val_acc:0.847]
Epoch [24/120    avg_loss:0.324, val_acc:0.839]
Epoch [25/120    avg_loss:0.319, val_acc:0.837]
Epoch [26/120    avg_loss:0.367, val_acc:0.832]
Epoch [27/120    avg_loss:0.330, val_acc:0.860]
Epoch [28/120    avg_loss:0.393, val_acc:0.855]
Epoch [29/120    avg_loss:0.328, val_acc:0.853]
Epoch [30/120    avg_loss:0.274, val_acc:0.856]
Epoch [31/120    avg_loss:0.331, val_acc:0.870]
Epoch [32/120    avg_loss:0.202, val_acc:0.890]
Epoch [33/120    avg_loss:0.188, val_acc:0.907]
Epoch [34/120    avg_loss:0.186, val_acc:0.921]
Epoch [35/120    avg_loss:0.184, val_acc:0.904]
Epoch [36/120    avg_loss:0.145, val_acc:0.918]
Epoch [37/120    avg_loss:0.166, val_acc:0.889]
Epoch [38/120    avg_loss:0.138, val_acc:0.926]
Epoch [39/120    avg_loss:0.147, val_acc:0.911]
Epoch [40/120    avg_loss:0.195, val_acc:0.875]
Epoch [41/120    avg_loss:0.348, val_acc:0.869]
Epoch [42/120    avg_loss:0.194, val_acc:0.879]
Epoch [43/120    avg_loss:0.145, val_acc:0.894]
Epoch [44/120    avg_loss:0.119, val_acc:0.921]
Epoch [45/120    avg_loss:0.115, val_acc:0.911]
Epoch [46/120    avg_loss:0.101, val_acc:0.937]
Epoch [47/120    avg_loss:0.099, val_acc:0.923]
Epoch [48/120    avg_loss:0.113, val_acc:0.908]
Epoch [49/120    avg_loss:0.094, val_acc:0.928]
Epoch [50/120    avg_loss:0.072, val_acc:0.933]
Epoch [51/120    avg_loss:0.068, val_acc:0.948]
Epoch [52/120    avg_loss:0.074, val_acc:0.943]
Epoch [53/120    avg_loss:0.087, val_acc:0.925]
Epoch [54/120    avg_loss:0.068, val_acc:0.947]
Epoch [55/120    avg_loss:0.058, val_acc:0.949]
Epoch [56/120    avg_loss:0.092, val_acc:0.933]
Epoch [57/120    avg_loss:0.084, val_acc:0.953]
Epoch [58/120    avg_loss:0.068, val_acc:0.952]
Epoch [59/120    avg_loss:0.062, val_acc:0.950]
Epoch [60/120    avg_loss:0.047, val_acc:0.952]
Epoch [61/120    avg_loss:0.071, val_acc:0.942]
Epoch [62/120    avg_loss:0.101, val_acc:0.946]
Epoch [63/120    avg_loss:0.060, val_acc:0.938]
Epoch [64/120    avg_loss:0.044, val_acc:0.953]
Epoch [65/120    avg_loss:0.067, val_acc:0.941]
Epoch [66/120    avg_loss:0.052, val_acc:0.957]
Epoch [67/120    avg_loss:0.052, val_acc:0.949]
Epoch [68/120    avg_loss:0.044, val_acc:0.959]
Epoch [69/120    avg_loss:0.035, val_acc:0.958]
Epoch [70/120    avg_loss:0.035, val_acc:0.963]
Epoch [71/120    avg_loss:0.036, val_acc:0.958]
Epoch [72/120    avg_loss:0.038, val_acc:0.963]
Epoch [73/120    avg_loss:0.029, val_acc:0.963]
Epoch [74/120    avg_loss:0.033, val_acc:0.959]
Epoch [75/120    avg_loss:0.058, val_acc:0.962]
Epoch [76/120    avg_loss:0.075, val_acc:0.931]
Epoch [77/120    avg_loss:0.114, val_acc:0.922]
Epoch [78/120    avg_loss:0.081, val_acc:0.946]
Epoch [79/120    avg_loss:0.037, val_acc:0.956]
Epoch [80/120    avg_loss:0.048, val_acc:0.968]
Epoch [81/120    avg_loss:0.044, val_acc:0.963]
Epoch [82/120    avg_loss:0.063, val_acc:0.956]
Epoch [83/120    avg_loss:0.063, val_acc:0.958]
Epoch [84/120    avg_loss:0.054, val_acc:0.953]
Epoch [85/120    avg_loss:0.044, val_acc:0.950]
Epoch [86/120    avg_loss:0.030, val_acc:0.966]
Epoch [87/120    avg_loss:0.041, val_acc:0.947]
Epoch [88/120    avg_loss:0.034, val_acc:0.967]
Epoch [89/120    avg_loss:0.038, val_acc:0.955]
Epoch [90/120    avg_loss:0.056, val_acc:0.958]
Epoch [91/120    avg_loss:0.040, val_acc:0.963]
Epoch [92/120    avg_loss:0.028, val_acc:0.972]
Epoch [93/120    avg_loss:0.029, val_acc:0.966]
Epoch [94/120    avg_loss:0.028, val_acc:0.943]
Epoch [95/120    avg_loss:0.031, val_acc:0.965]
Epoch [96/120    avg_loss:0.016, val_acc:0.970]
Epoch [97/120    avg_loss:0.024, val_acc:0.965]
Epoch [98/120    avg_loss:0.039, val_acc:0.952]
Epoch [99/120    avg_loss:0.028, val_acc:0.956]
Epoch [100/120    avg_loss:0.026, val_acc:0.958]
Epoch [101/120    avg_loss:0.036, val_acc:0.957]
Epoch [102/120    avg_loss:0.035, val_acc:0.967]
Epoch [103/120    avg_loss:0.022, val_acc:0.965]
Epoch [104/120    avg_loss:0.021, val_acc:0.966]
Epoch [105/120    avg_loss:0.025, val_acc:0.975]
Epoch [106/120    avg_loss:0.022, val_acc:0.966]
Epoch [107/120    avg_loss:0.024, val_acc:0.971]
Epoch [108/120    avg_loss:0.018, val_acc:0.979]
Epoch [109/120    avg_loss:0.024, val_acc:0.978]
Epoch [110/120    avg_loss:0.017, val_acc:0.973]
Epoch [111/120    avg_loss:0.015, val_acc:0.981]
Epoch [112/120    avg_loss:0.016, val_acc:0.974]
Epoch [113/120    avg_loss:0.017, val_acc:0.976]
Epoch [114/120    avg_loss:0.013, val_acc:0.978]
Epoch [115/120    avg_loss:0.012, val_acc:0.975]
Epoch [116/120    avg_loss:0.013, val_acc:0.973]
Epoch [117/120    avg_loss:0.019, val_acc:0.976]
Epoch [118/120    avg_loss:0.025, val_acc:0.968]
Epoch [119/120    avg_loss:0.017, val_acc:0.972]
Epoch [120/120    avg_loss:0.013, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1219    5    1    0    0    0    0    0   21   37    2    0
     0    0    0]
 [   0    0    0  681   20    3    0    0    0    3    0    2   38    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    2    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    2    0    3    1    0    0    0  848   13    0    0
     0    6    0]
 [   0    0    5    0    0    0    0    0    5    0    3 2192    5    0
     0    0    0]
 [   0    0    0    7    0    0    0    0    0    1    0    0  521    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1129    8    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
     9  321    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.47425474254743

F1 scores:
[       nan 0.975      0.97092792 0.9445215  0.95302013 0.99084668
 0.9850075  1.         0.98957126 0.9        0.96914286 0.98428379
 0.94469628 1.         0.99165569 0.93722628 0.98823529]

Kappa:
0.971205865672304
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff1f0d93898>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.736, val_acc:0.325]
Epoch [2/120    avg_loss:2.376, val_acc:0.476]
Epoch [3/120    avg_loss:2.169, val_acc:0.558]
Epoch [4/120    avg_loss:2.017, val_acc:0.589]
Epoch [5/120    avg_loss:1.857, val_acc:0.601]
Epoch [6/120    avg_loss:1.734, val_acc:0.614]
Epoch [7/120    avg_loss:1.598, val_acc:0.646]
Epoch [8/120    avg_loss:1.390, val_acc:0.671]
Epoch [9/120    avg_loss:1.296, val_acc:0.713]
Epoch [10/120    avg_loss:1.152, val_acc:0.704]
Epoch [11/120    avg_loss:1.022, val_acc:0.718]
Epoch [12/120    avg_loss:0.936, val_acc:0.752]
Epoch [13/120    avg_loss:0.816, val_acc:0.756]
Epoch [14/120    avg_loss:0.804, val_acc:0.736]
Epoch [15/120    avg_loss:0.752, val_acc:0.778]
Epoch [16/120    avg_loss:0.603, val_acc:0.802]
Epoch [17/120    avg_loss:0.499, val_acc:0.830]
Epoch [18/120    avg_loss:0.539, val_acc:0.800]
Epoch [19/120    avg_loss:0.513, val_acc:0.842]
Epoch [20/120    avg_loss:0.428, val_acc:0.858]
Epoch [21/120    avg_loss:0.333, val_acc:0.871]
Epoch [22/120    avg_loss:0.401, val_acc:0.839]
Epoch [23/120    avg_loss:0.495, val_acc:0.813]
Epoch [24/120    avg_loss:0.414, val_acc:0.809]
Epoch [25/120    avg_loss:0.356, val_acc:0.888]
Epoch [26/120    avg_loss:0.305, val_acc:0.886]
Epoch [27/120    avg_loss:0.371, val_acc:0.897]
Epoch [28/120    avg_loss:0.240, val_acc:0.922]
Epoch [29/120    avg_loss:0.216, val_acc:0.928]
Epoch [30/120    avg_loss:0.183, val_acc:0.921]
Epoch [31/120    avg_loss:0.195, val_acc:0.917]
Epoch [32/120    avg_loss:0.169, val_acc:0.912]
Epoch [33/120    avg_loss:0.173, val_acc:0.942]
Epoch [34/120    avg_loss:0.162, val_acc:0.924]
Epoch [35/120    avg_loss:0.145, val_acc:0.932]
Epoch [36/120    avg_loss:0.126, val_acc:0.947]
Epoch [37/120    avg_loss:0.133, val_acc:0.868]
Epoch [38/120    avg_loss:0.131, val_acc:0.915]
Epoch [39/120    avg_loss:0.146, val_acc:0.901]
Epoch [40/120    avg_loss:0.159, val_acc:0.937]
Epoch [41/120    avg_loss:0.100, val_acc:0.947]
Epoch [42/120    avg_loss:0.114, val_acc:0.954]
Epoch [43/120    avg_loss:0.130, val_acc:0.956]
Epoch [44/120    avg_loss:0.084, val_acc:0.963]
Epoch [45/120    avg_loss:0.098, val_acc:0.959]
Epoch [46/120    avg_loss:0.096, val_acc:0.972]
Epoch [47/120    avg_loss:0.060, val_acc:0.972]
Epoch [48/120    avg_loss:0.070, val_acc:0.962]
Epoch [49/120    avg_loss:0.115, val_acc:0.951]
Epoch [50/120    avg_loss:0.104, val_acc:0.958]
Epoch [51/120    avg_loss:0.081, val_acc:0.950]
Epoch [52/120    avg_loss:0.092, val_acc:0.922]
Epoch [53/120    avg_loss:0.100, val_acc:0.948]
Epoch [54/120    avg_loss:0.084, val_acc:0.933]
Epoch [55/120    avg_loss:0.064, val_acc:0.972]
Epoch [56/120    avg_loss:0.068, val_acc:0.963]
Epoch [57/120    avg_loss:0.069, val_acc:0.968]
Epoch [58/120    avg_loss:0.076, val_acc:0.962]
Epoch [59/120    avg_loss:0.077, val_acc:0.960]
Epoch [60/120    avg_loss:0.071, val_acc:0.949]
Epoch [61/120    avg_loss:0.072, val_acc:0.949]
Epoch [62/120    avg_loss:0.048, val_acc:0.967]
Epoch [63/120    avg_loss:0.072, val_acc:0.933]
Epoch [64/120    avg_loss:0.077, val_acc:0.963]
Epoch [65/120    avg_loss:0.057, val_acc:0.963]
Epoch [66/120    avg_loss:0.058, val_acc:0.969]
Epoch [67/120    avg_loss:0.040, val_acc:0.959]
Epoch [68/120    avg_loss:0.031, val_acc:0.975]
Epoch [69/120    avg_loss:0.036, val_acc:0.970]
Epoch [70/120    avg_loss:0.043, val_acc:0.975]
Epoch [71/120    avg_loss:0.043, val_acc:0.958]
Epoch [72/120    avg_loss:0.035, val_acc:0.973]
Epoch [73/120    avg_loss:0.085, val_acc:0.959]
Epoch [74/120    avg_loss:0.056, val_acc:0.964]
Epoch [75/120    avg_loss:0.077, val_acc:0.971]
Epoch [76/120    avg_loss:0.061, val_acc:0.971]
Epoch [77/120    avg_loss:0.093, val_acc:0.952]
Epoch [78/120    avg_loss:0.052, val_acc:0.965]
Epoch [79/120    avg_loss:0.037, val_acc:0.974]
Epoch [80/120    avg_loss:0.049, val_acc:0.960]
Epoch [81/120    avg_loss:0.038, val_acc:0.972]
Epoch [82/120    avg_loss:0.036, val_acc:0.979]
Epoch [83/120    avg_loss:0.035, val_acc:0.963]
Epoch [84/120    avg_loss:0.025, val_acc:0.978]
Epoch [85/120    avg_loss:0.029, val_acc:0.974]
Epoch [86/120    avg_loss:0.031, val_acc:0.978]
Epoch [87/120    avg_loss:0.022, val_acc:0.956]
Epoch [88/120    avg_loss:0.037, val_acc:0.967]
Epoch [89/120    avg_loss:0.023, val_acc:0.981]
Epoch [90/120    avg_loss:0.024, val_acc:0.982]
Epoch [91/120    avg_loss:0.031, val_acc:0.960]
Epoch [92/120    avg_loss:0.031, val_acc:0.967]
Epoch [93/120    avg_loss:0.029, val_acc:0.966]
Epoch [94/120    avg_loss:0.023, val_acc:0.969]
Epoch [95/120    avg_loss:0.022, val_acc:0.957]
Epoch [96/120    avg_loss:0.057, val_acc:0.827]
Epoch [97/120    avg_loss:0.738, val_acc:0.829]
Epoch [98/120    avg_loss:0.554, val_acc:0.870]
Epoch [99/120    avg_loss:0.224, val_acc:0.934]
Epoch [100/120    avg_loss:0.243, val_acc:0.911]
Epoch [101/120    avg_loss:0.170, val_acc:0.938]
Epoch [102/120    avg_loss:0.175, val_acc:0.908]
Epoch [103/120    avg_loss:0.145, val_acc:0.946]
Epoch [104/120    avg_loss:0.070, val_acc:0.957]
Epoch [105/120    avg_loss:0.074, val_acc:0.962]
Epoch [106/120    avg_loss:0.055, val_acc:0.963]
Epoch [107/120    avg_loss:0.055, val_acc:0.967]
Epoch [108/120    avg_loss:0.051, val_acc:0.968]
Epoch [109/120    avg_loss:0.048, val_acc:0.968]
Epoch [110/120    avg_loss:0.050, val_acc:0.968]
Epoch [111/120    avg_loss:0.044, val_acc:0.967]
Epoch [112/120    avg_loss:0.051, val_acc:0.967]
Epoch [113/120    avg_loss:0.043, val_acc:0.968]
Epoch [114/120    avg_loss:0.044, val_acc:0.967]
Epoch [115/120    avg_loss:0.038, val_acc:0.968]
Epoch [116/120    avg_loss:0.043, val_acc:0.966]
Epoch [117/120    avg_loss:0.037, val_acc:0.966]
Epoch [118/120    avg_loss:0.042, val_acc:0.966]
Epoch [119/120    avg_loss:0.041, val_acc:0.966]
Epoch [120/120    avg_loss:0.041, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1245    5    3    0    2    0    0    0   10   16    1    0
     0    3    0]
 [   0    0    0  698    1    8    0    0    0   31    0    0    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    2    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   34    0    0    9    5    0    0    1  820    1    0    0
     1    4    0]
 [   0    0   18    0    0    4    4    1    0    1   36 2127   11    2
     4    2    0]
 [   0    0    0    3    0    0    0    0    0    1    0    3  518    0
     0    7    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    3    2    0    0    0    1    0    0    1
  1120   12    0]
 [   0    0    1    0    0    0   19    0    0    0    0    0    0    0
     5  322    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.67208672086721

F1 scores:
[       nan 0.975      0.96362229 0.96077082 0.99069767 0.96279594
 0.97164179 0.98039216 0.99883586 0.5        0.94090648 0.97613584
 0.96282528 0.98924731 0.98245614 0.92395983 0.97619048]

Kappa:
0.9621208458271379
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fab852d6898>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.680, val_acc:0.334]
Epoch [2/120    avg_loss:2.368, val_acc:0.494]
Epoch [3/120    avg_loss:2.123, val_acc:0.517]
Epoch [4/120    avg_loss:1.954, val_acc:0.567]
Epoch [5/120    avg_loss:1.782, val_acc:0.577]
Epoch [6/120    avg_loss:1.628, val_acc:0.598]
Epoch [7/120    avg_loss:1.520, val_acc:0.640]
Epoch [8/120    avg_loss:1.338, val_acc:0.670]
Epoch [9/120    avg_loss:1.226, val_acc:0.661]
Epoch [10/120    avg_loss:1.111, val_acc:0.716]
Epoch [11/120    avg_loss:0.936, val_acc:0.741]
Epoch [12/120    avg_loss:0.885, val_acc:0.747]
Epoch [13/120    avg_loss:0.824, val_acc:0.724]
Epoch [14/120    avg_loss:0.738, val_acc:0.764]
Epoch [15/120    avg_loss:0.698, val_acc:0.766]
Epoch [16/120    avg_loss:0.625, val_acc:0.823]
Epoch [17/120    avg_loss:0.533, val_acc:0.835]
Epoch [18/120    avg_loss:0.529, val_acc:0.822]
Epoch [19/120    avg_loss:0.451, val_acc:0.854]
Epoch [20/120    avg_loss:0.422, val_acc:0.768]
Epoch [21/120    avg_loss:0.478, val_acc:0.851]
Epoch [22/120    avg_loss:0.438, val_acc:0.861]
Epoch [23/120    avg_loss:0.314, val_acc:0.860]
Epoch [24/120    avg_loss:0.344, val_acc:0.851]
Epoch [25/120    avg_loss:0.281, val_acc:0.875]
Epoch [26/120    avg_loss:0.248, val_acc:0.907]
Epoch [27/120    avg_loss:0.240, val_acc:0.890]
Epoch [28/120    avg_loss:0.286, val_acc:0.904]
Epoch [29/120    avg_loss:0.204, val_acc:0.915]
Epoch [30/120    avg_loss:0.201, val_acc:0.884]
Epoch [31/120    avg_loss:0.178, val_acc:0.887]
Epoch [32/120    avg_loss:0.188, val_acc:0.882]
Epoch [33/120    avg_loss:0.245, val_acc:0.879]
Epoch [34/120    avg_loss:0.204, val_acc:0.903]
Epoch [35/120    avg_loss:0.151, val_acc:0.915]
Epoch [36/120    avg_loss:0.159, val_acc:0.896]
Epoch [37/120    avg_loss:0.153, val_acc:0.932]
Epoch [38/120    avg_loss:0.140, val_acc:0.916]
Epoch [39/120    avg_loss:0.195, val_acc:0.921]
Epoch [40/120    avg_loss:0.146, val_acc:0.936]
Epoch [41/120    avg_loss:0.135, val_acc:0.921]
Epoch [42/120    avg_loss:0.108, val_acc:0.937]
Epoch [43/120    avg_loss:0.131, val_acc:0.934]
Epoch [44/120    avg_loss:0.120, val_acc:0.934]
Epoch [45/120    avg_loss:0.103, val_acc:0.946]
Epoch [46/120    avg_loss:0.101, val_acc:0.916]
Epoch [47/120    avg_loss:0.181, val_acc:0.911]
Epoch [48/120    avg_loss:0.138, val_acc:0.917]
Epoch [49/120    avg_loss:0.091, val_acc:0.947]
Epoch [50/120    avg_loss:0.082, val_acc:0.950]
Epoch [51/120    avg_loss:0.071, val_acc:0.937]
Epoch [52/120    avg_loss:0.065, val_acc:0.957]
Epoch [53/120    avg_loss:0.061, val_acc:0.955]
Epoch [54/120    avg_loss:0.070, val_acc:0.958]
Epoch [55/120    avg_loss:0.055, val_acc:0.951]
Epoch [56/120    avg_loss:0.073, val_acc:0.959]
Epoch [57/120    avg_loss:0.076, val_acc:0.959]
Epoch [58/120    avg_loss:0.055, val_acc:0.954]
Epoch [59/120    avg_loss:0.062, val_acc:0.960]
Epoch [60/120    avg_loss:0.045, val_acc:0.960]
Epoch [61/120    avg_loss:0.043, val_acc:0.952]
Epoch [62/120    avg_loss:0.049, val_acc:0.957]
Epoch [63/120    avg_loss:0.037, val_acc:0.958]
Epoch [64/120    avg_loss:0.043, val_acc:0.963]
Epoch [65/120    avg_loss:0.060, val_acc:0.938]
Epoch [66/120    avg_loss:0.052, val_acc:0.956]
Epoch [67/120    avg_loss:0.063, val_acc:0.967]
Epoch [68/120    avg_loss:0.051, val_acc:0.967]
Epoch [69/120    avg_loss:0.031, val_acc:0.956]
Epoch [70/120    avg_loss:0.033, val_acc:0.964]
Epoch [71/120    avg_loss:0.035, val_acc:0.967]
Epoch [72/120    avg_loss:0.048, val_acc:0.950]
Epoch [73/120    avg_loss:0.041, val_acc:0.955]
Epoch [74/120    avg_loss:0.045, val_acc:0.959]
Epoch [75/120    avg_loss:0.059, val_acc:0.955]
Epoch [76/120    avg_loss:0.035, val_acc:0.968]
Epoch [77/120    avg_loss:0.043, val_acc:0.956]
Epoch [78/120    avg_loss:0.038, val_acc:0.952]
Epoch [79/120    avg_loss:0.055, val_acc:0.944]
Epoch [80/120    avg_loss:0.044, val_acc:0.967]
Epoch [81/120    avg_loss:0.027, val_acc:0.965]
Epoch [82/120    avg_loss:0.023, val_acc:0.968]
Epoch [83/120    avg_loss:0.026, val_acc:0.972]
Epoch [84/120    avg_loss:0.029, val_acc:0.974]
Epoch [85/120    avg_loss:0.021, val_acc:0.963]
Epoch [86/120    avg_loss:0.017, val_acc:0.968]
Epoch [87/120    avg_loss:0.027, val_acc:0.967]
Epoch [88/120    avg_loss:0.028, val_acc:0.973]
Epoch [89/120    avg_loss:0.030, val_acc:0.972]
Epoch [90/120    avg_loss:0.018, val_acc:0.963]
Epoch [91/120    avg_loss:0.013, val_acc:0.967]
Epoch [92/120    avg_loss:0.024, val_acc:0.974]
Epoch [93/120    avg_loss:0.032, val_acc:0.967]
Epoch [94/120    avg_loss:0.022, val_acc:0.971]
Epoch [95/120    avg_loss:0.020, val_acc:0.969]
Epoch [96/120    avg_loss:0.029, val_acc:0.971]
Epoch [97/120    avg_loss:0.019, val_acc:0.972]
Epoch [98/120    avg_loss:0.016, val_acc:0.971]
Epoch [99/120    avg_loss:0.012, val_acc:0.975]
Epoch [100/120    avg_loss:0.015, val_acc:0.975]
Epoch [101/120    avg_loss:0.016, val_acc:0.976]
Epoch [102/120    avg_loss:0.023, val_acc:0.972]
Epoch [103/120    avg_loss:0.024, val_acc:0.970]
Epoch [104/120    avg_loss:0.090, val_acc:0.828]
Epoch [105/120    avg_loss:0.312, val_acc:0.905]
Epoch [106/120    avg_loss:0.197, val_acc:0.908]
Epoch [107/120    avg_loss:0.198, val_acc:0.913]
Epoch [108/120    avg_loss:0.089, val_acc:0.942]
Epoch [109/120    avg_loss:0.061, val_acc:0.949]
Epoch [110/120    avg_loss:0.061, val_acc:0.947]
Epoch [111/120    avg_loss:0.087, val_acc:0.958]
Epoch [112/120    avg_loss:0.059, val_acc:0.958]
Epoch [113/120    avg_loss:0.071, val_acc:0.951]
Epoch [114/120    avg_loss:0.051, val_acc:0.959]
Epoch [115/120    avg_loss:0.044, val_acc:0.964]
Epoch [116/120    avg_loss:0.033, val_acc:0.972]
Epoch [117/120    avg_loss:0.034, val_acc:0.974]
Epoch [118/120    avg_loss:0.028, val_acc:0.975]
Epoch [119/120    avg_loss:0.031, val_acc:0.976]
Epoch [120/120    avg_loss:0.029, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1237    5    3    0    0    0    0    0   10   30    0    0
     0    0    0]
 [   0    0    0  704    1   11    0    0    0    3    3    0   21    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  421    0    0    0    0    0    0    0    0
    13    1    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   12    2    0    3    2    0    0    0  856    0    0    0
     0    0    0]
 [   0    0   12    0    0    1    5    0    0    0   23 2158    7    3
     1    0    0]
 [   0    0    0    0    0    3    0    0    0    0    8    5  500    0
     0    0   18]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1128   10    0]
 [   0    0    0    0    0    0   21    0    0    0    0    0    0    0
     9  317    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.22493224932249

F1 scores:
[       nan 0.98765432 0.97172035 0.96570645 0.99069767 0.96338673
 0.97837435 1.         1.         0.86486486 0.9634215  0.98024074
 0.93896714 0.98143236 0.98472283 0.93925926 0.8972973 ]

Kappa:
0.9683795143813225
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc2ffd7c7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.684, val_acc:0.370]
Epoch [2/120    avg_loss:2.398, val_acc:0.462]
Epoch [3/120    avg_loss:2.198, val_acc:0.476]
Epoch [4/120    avg_loss:2.044, val_acc:0.537]
Epoch [5/120    avg_loss:1.897, val_acc:0.601]
Epoch [6/120    avg_loss:1.814, val_acc:0.596]
Epoch [7/120    avg_loss:1.687, val_acc:0.608]
Epoch [8/120    avg_loss:1.545, val_acc:0.614]
Epoch [9/120    avg_loss:1.428, val_acc:0.652]
Epoch [10/120    avg_loss:1.228, val_acc:0.712]
Epoch [11/120    avg_loss:1.155, val_acc:0.707]
Epoch [12/120    avg_loss:1.073, val_acc:0.680]
Epoch [13/120    avg_loss:1.036, val_acc:0.705]
Epoch [14/120    avg_loss:0.913, val_acc:0.757]
Epoch [15/120    avg_loss:0.877, val_acc:0.696]
Epoch [16/120    avg_loss:0.830, val_acc:0.769]
Epoch [17/120    avg_loss:0.745, val_acc:0.771]
Epoch [18/120    avg_loss:0.691, val_acc:0.759]
Epoch [19/120    avg_loss:0.664, val_acc:0.741]
Epoch [20/120    avg_loss:0.643, val_acc:0.800]
Epoch [21/120    avg_loss:0.594, val_acc:0.822]
Epoch [22/120    avg_loss:0.522, val_acc:0.803]
Epoch [23/120    avg_loss:0.448, val_acc:0.842]
Epoch [24/120    avg_loss:0.426, val_acc:0.834]
Epoch [25/120    avg_loss:0.385, val_acc:0.840]
Epoch [26/120    avg_loss:0.418, val_acc:0.846]
Epoch [27/120    avg_loss:0.360, val_acc:0.858]
Epoch [28/120    avg_loss:0.346, val_acc:0.866]
Epoch [29/120    avg_loss:0.319, val_acc:0.859]
Epoch [30/120    avg_loss:0.310, val_acc:0.863]
Epoch [31/120    avg_loss:0.296, val_acc:0.894]
Epoch [32/120    avg_loss:0.279, val_acc:0.882]
Epoch [33/120    avg_loss:0.267, val_acc:0.898]
Epoch [34/120    avg_loss:0.277, val_acc:0.884]
Epoch [35/120    avg_loss:0.250, val_acc:0.901]
Epoch [36/120    avg_loss:0.205, val_acc:0.880]
Epoch [37/120    avg_loss:0.221, val_acc:0.891]
Epoch [38/120    avg_loss:0.185, val_acc:0.908]
Epoch [39/120    avg_loss:0.165, val_acc:0.894]
Epoch [40/120    avg_loss:0.165, val_acc:0.912]
Epoch [41/120    avg_loss:0.168, val_acc:0.922]
Epoch [42/120    avg_loss:0.167, val_acc:0.907]
Epoch [43/120    avg_loss:0.160, val_acc:0.927]
Epoch [44/120    avg_loss:0.188, val_acc:0.911]
Epoch [45/120    avg_loss:0.160, val_acc:0.940]
Epoch [46/120    avg_loss:0.130, val_acc:0.910]
Epoch [47/120    avg_loss:0.159, val_acc:0.930]
Epoch [48/120    avg_loss:0.130, val_acc:0.903]
Epoch [49/120    avg_loss:0.241, val_acc:0.856]
Epoch [50/120    avg_loss:0.309, val_acc:0.907]
Epoch [51/120    avg_loss:0.188, val_acc:0.908]
Epoch [52/120    avg_loss:0.144, val_acc:0.926]
Epoch [53/120    avg_loss:0.133, val_acc:0.930]
Epoch [54/120    avg_loss:0.171, val_acc:0.928]
Epoch [55/120    avg_loss:0.130, val_acc:0.907]
Epoch [56/120    avg_loss:0.108, val_acc:0.925]
Epoch [57/120    avg_loss:0.103, val_acc:0.943]
Epoch [58/120    avg_loss:0.088, val_acc:0.938]
Epoch [59/120    avg_loss:0.089, val_acc:0.943]
Epoch [60/120    avg_loss:0.123, val_acc:0.934]
Epoch [61/120    avg_loss:0.124, val_acc:0.928]
Epoch [62/120    avg_loss:0.092, val_acc:0.934]
Epoch [63/120    avg_loss:0.076, val_acc:0.944]
Epoch [64/120    avg_loss:0.086, val_acc:0.935]
Epoch [65/120    avg_loss:0.090, val_acc:0.946]
Epoch [66/120    avg_loss:0.069, val_acc:0.950]
Epoch [67/120    avg_loss:0.058, val_acc:0.959]
Epoch [68/120    avg_loss:0.056, val_acc:0.955]
Epoch [69/120    avg_loss:0.053, val_acc:0.946]
Epoch [70/120    avg_loss:0.061, val_acc:0.952]
Epoch [71/120    avg_loss:0.076, val_acc:0.962]
Epoch [72/120    avg_loss:0.064, val_acc:0.956]
Epoch [73/120    avg_loss:0.054, val_acc:0.956]
Epoch [74/120    avg_loss:0.038, val_acc:0.955]
Epoch [75/120    avg_loss:0.039, val_acc:0.958]
Epoch [76/120    avg_loss:0.042, val_acc:0.956]
Epoch [77/120    avg_loss:0.046, val_acc:0.950]
Epoch [78/120    avg_loss:0.061, val_acc:0.954]
Epoch [79/120    avg_loss:0.043, val_acc:0.954]
Epoch [80/120    avg_loss:0.043, val_acc:0.966]
Epoch [81/120    avg_loss:0.058, val_acc:0.946]
Epoch [82/120    avg_loss:0.062, val_acc:0.964]
Epoch [83/120    avg_loss:0.047, val_acc:0.964]
Epoch [84/120    avg_loss:0.050, val_acc:0.958]
Epoch [85/120    avg_loss:0.033, val_acc:0.964]
Epoch [86/120    avg_loss:0.032, val_acc:0.958]
Epoch [87/120    avg_loss:0.030, val_acc:0.964]
Epoch [88/120    avg_loss:0.021, val_acc:0.963]
Epoch [89/120    avg_loss:0.035, val_acc:0.961]
Epoch [90/120    avg_loss:0.030, val_acc:0.966]
Epoch [91/120    avg_loss:0.034, val_acc:0.956]
Epoch [92/120    avg_loss:0.043, val_acc:0.963]
Epoch [93/120    avg_loss:0.033, val_acc:0.970]
Epoch [94/120    avg_loss:0.050, val_acc:0.955]
Epoch [95/120    avg_loss:0.032, val_acc:0.955]
Epoch [96/120    avg_loss:0.040, val_acc:0.963]
Epoch [97/120    avg_loss:0.035, val_acc:0.963]
Epoch [98/120    avg_loss:0.023, val_acc:0.964]
Epoch [99/120    avg_loss:0.027, val_acc:0.965]
Epoch [100/120    avg_loss:0.022, val_acc:0.963]
Epoch [101/120    avg_loss:0.026, val_acc:0.972]
Epoch [102/120    avg_loss:0.020, val_acc:0.964]
Epoch [103/120    avg_loss:0.038, val_acc:0.971]
Epoch [104/120    avg_loss:0.022, val_acc:0.974]
Epoch [105/120    avg_loss:0.017, val_acc:0.970]
Epoch [106/120    avg_loss:0.022, val_acc:0.970]
Epoch [107/120    avg_loss:0.026, val_acc:0.973]
Epoch [108/120    avg_loss:0.030, val_acc:0.966]
Epoch [109/120    avg_loss:0.019, val_acc:0.973]
Epoch [110/120    avg_loss:0.017, val_acc:0.971]
Epoch [111/120    avg_loss:0.031, val_acc:0.971]
Epoch [112/120    avg_loss:0.022, val_acc:0.965]
Epoch [113/120    avg_loss:0.019, val_acc:0.975]
Epoch [114/120    avg_loss:0.017, val_acc:0.973]
Epoch [115/120    avg_loss:0.025, val_acc:0.970]
Epoch [116/120    avg_loss:0.019, val_acc:0.973]
Epoch [117/120    avg_loss:0.015, val_acc:0.973]
Epoch [118/120    avg_loss:0.021, val_acc:0.971]
Epoch [119/120    avg_loss:0.019, val_acc:0.973]
Epoch [120/120    avg_loss:0.019, val_acc:0.948]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1244   18    0    0    4    0    0    1   10    2    6    0
     0    0    0]
 [   0    0    8  684    2   10    2    0    0   22    2    0   11    6
     0    0    0]
 [   0    0    0    0  212    0    1    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    3    0    7    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   39   60    0    6    3    0    0    0  753    5    0    0
     0    9    0]
 [   0    0  166    0    0    0    9    1    9    0   27 1984   10    4
     0    0    0]
 [   0    0    0    0    1    7    0    0    0    0   13   20  491    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    6    0    1    0    2    1    0    0
  1129    0    0]
 [   0    0    0    0    0    0   34    0    0    0    0    0    0    0
    64  249    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
93.27913279132791

F1 scores:
[       nan 0.98765432 0.90736689 0.90596026 0.99065421 0.96262741
 0.9570284  0.92592593 0.98850575 0.5        0.89483066 0.93983894
 0.92992424 0.97368421 0.96826758 0.8231405  0.97005988]

Kappa:
0.9235680495123489
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb9c470d8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.739, val_acc:0.287]
Epoch [2/120    avg_loss:2.414, val_acc:0.556]
Epoch [3/120    avg_loss:2.165, val_acc:0.577]
Epoch [4/120    avg_loss:2.018, val_acc:0.599]
Epoch [5/120    avg_loss:1.890, val_acc:0.618]
Epoch [6/120    avg_loss:1.747, val_acc:0.634]
Epoch [7/120    avg_loss:1.629, val_acc:0.639]
Epoch [8/120    avg_loss:1.601, val_acc:0.632]
Epoch [9/120    avg_loss:1.456, val_acc:0.628]
Epoch [10/120    avg_loss:1.365, val_acc:0.679]
Epoch [11/120    avg_loss:1.211, val_acc:0.710]
Epoch [12/120    avg_loss:1.140, val_acc:0.768]
Epoch [13/120    avg_loss:0.951, val_acc:0.784]
Epoch [14/120    avg_loss:0.813, val_acc:0.778]
Epoch [15/120    avg_loss:0.750, val_acc:0.819]
Epoch [16/120    avg_loss:0.674, val_acc:0.803]
Epoch [17/120    avg_loss:0.641, val_acc:0.842]
Epoch [18/120    avg_loss:0.618, val_acc:0.835]
Epoch [19/120    avg_loss:0.550, val_acc:0.833]
Epoch [20/120    avg_loss:0.472, val_acc:0.831]
Epoch [21/120    avg_loss:0.469, val_acc:0.843]
Epoch [22/120    avg_loss:0.434, val_acc:0.834]
Epoch [23/120    avg_loss:0.416, val_acc:0.862]
Epoch [24/120    avg_loss:0.345, val_acc:0.842]
Epoch [25/120    avg_loss:0.334, val_acc:0.885]
Epoch [26/120    avg_loss:0.350, val_acc:0.866]
Epoch [27/120    avg_loss:0.287, val_acc:0.879]
Epoch [28/120    avg_loss:0.288, val_acc:0.865]
Epoch [29/120    avg_loss:0.372, val_acc:0.858]
Epoch [30/120    avg_loss:0.314, val_acc:0.869]
Epoch [31/120    avg_loss:0.254, val_acc:0.884]
Epoch [32/120    avg_loss:0.325, val_acc:0.875]
Epoch [33/120    avg_loss:0.246, val_acc:0.891]
Epoch [34/120    avg_loss:0.272, val_acc:0.869]
Epoch [35/120    avg_loss:0.355, val_acc:0.869]
Epoch [36/120    avg_loss:0.234, val_acc:0.890]
Epoch [37/120    avg_loss:0.244, val_acc:0.918]
Epoch [38/120    avg_loss:0.196, val_acc:0.895]
Epoch [39/120    avg_loss:0.190, val_acc:0.934]
Epoch [40/120    avg_loss:0.153, val_acc:0.910]
Epoch [41/120    avg_loss:0.143, val_acc:0.913]
Epoch [42/120    avg_loss:0.153, val_acc:0.920]
Epoch [43/120    avg_loss:0.160, val_acc:0.926]
Epoch [44/120    avg_loss:0.129, val_acc:0.933]
Epoch [45/120    avg_loss:0.101, val_acc:0.917]
Epoch [46/120    avg_loss:0.192, val_acc:0.925]
Epoch [47/120    avg_loss:0.220, val_acc:0.890]
Epoch [48/120    avg_loss:0.170, val_acc:0.901]
Epoch [49/120    avg_loss:0.152, val_acc:0.925]
Epoch [50/120    avg_loss:0.121, val_acc:0.921]
Epoch [51/120    avg_loss:0.159, val_acc:0.921]
Epoch [52/120    avg_loss:0.124, val_acc:0.935]
Epoch [53/120    avg_loss:0.150, val_acc:0.919]
Epoch [54/120    avg_loss:0.104, val_acc:0.938]
Epoch [55/120    avg_loss:0.093, val_acc:0.922]
Epoch [56/120    avg_loss:0.074, val_acc:0.929]
Epoch [57/120    avg_loss:0.085, val_acc:0.911]
Epoch [58/120    avg_loss:0.087, val_acc:0.932]
Epoch [59/120    avg_loss:0.091, val_acc:0.926]
Epoch [60/120    avg_loss:0.093, val_acc:0.939]
Epoch [61/120    avg_loss:0.074, val_acc:0.922]
Epoch [62/120    avg_loss:0.076, val_acc:0.944]
Epoch [63/120    avg_loss:0.083, val_acc:0.943]
Epoch [64/120    avg_loss:0.071, val_acc:0.935]
Epoch [65/120    avg_loss:0.083, val_acc:0.934]
Epoch [66/120    avg_loss:0.072, val_acc:0.939]
Epoch [67/120    avg_loss:0.050, val_acc:0.952]
Epoch [68/120    avg_loss:0.042, val_acc:0.954]
Epoch [69/120    avg_loss:0.058, val_acc:0.935]
Epoch [70/120    avg_loss:0.067, val_acc:0.942]
Epoch [71/120    avg_loss:0.043, val_acc:0.951]
Epoch [72/120    avg_loss:0.044, val_acc:0.937]
Epoch [73/120    avg_loss:0.055, val_acc:0.944]
Epoch [74/120    avg_loss:0.062, val_acc:0.940]
Epoch [75/120    avg_loss:0.088, val_acc:0.942]
Epoch [76/120    avg_loss:0.059, val_acc:0.944]
Epoch [77/120    avg_loss:0.043, val_acc:0.950]
Epoch [78/120    avg_loss:0.079, val_acc:0.924]
Epoch [79/120    avg_loss:0.080, val_acc:0.946]
Epoch [80/120    avg_loss:0.049, val_acc:0.947]
Epoch [81/120    avg_loss:0.041, val_acc:0.940]
Epoch [82/120    avg_loss:0.034, val_acc:0.948]
Epoch [83/120    avg_loss:0.038, val_acc:0.946]
Epoch [84/120    avg_loss:0.030, val_acc:0.949]
Epoch [85/120    avg_loss:0.030, val_acc:0.953]
Epoch [86/120    avg_loss:0.025, val_acc:0.952]
Epoch [87/120    avg_loss:0.029, val_acc:0.952]
Epoch [88/120    avg_loss:0.025, val_acc:0.953]
Epoch [89/120    avg_loss:0.023, val_acc:0.952]
Epoch [90/120    avg_loss:0.027, val_acc:0.953]
Epoch [91/120    avg_loss:0.035, val_acc:0.954]
Epoch [92/120    avg_loss:0.028, val_acc:0.957]
Epoch [93/120    avg_loss:0.027, val_acc:0.956]
Epoch [94/120    avg_loss:0.024, val_acc:0.957]
Epoch [95/120    avg_loss:0.026, val_acc:0.958]
Epoch [96/120    avg_loss:0.022, val_acc:0.957]
Epoch [97/120    avg_loss:0.022, val_acc:0.956]
Epoch [98/120    avg_loss:0.025, val_acc:0.956]
Epoch [99/120    avg_loss:0.024, val_acc:0.955]
Epoch [100/120    avg_loss:0.023, val_acc:0.956]
Epoch [101/120    avg_loss:0.024, val_acc:0.957]
Epoch [102/120    avg_loss:0.026, val_acc:0.958]
Epoch [103/120    avg_loss:0.024, val_acc:0.962]
Epoch [104/120    avg_loss:0.020, val_acc:0.963]
Epoch [105/120    avg_loss:0.024, val_acc:0.960]
Epoch [106/120    avg_loss:0.024, val_acc:0.960]
Epoch [107/120    avg_loss:0.022, val_acc:0.962]
Epoch [108/120    avg_loss:0.022, val_acc:0.963]
Epoch [109/120    avg_loss:0.025, val_acc:0.963]
Epoch [110/120    avg_loss:0.021, val_acc:0.962]
Epoch [111/120    avg_loss:0.016, val_acc:0.966]
Epoch [112/120    avg_loss:0.024, val_acc:0.967]
Epoch [113/120    avg_loss:0.025, val_acc:0.965]
Epoch [114/120    avg_loss:0.024, val_acc:0.965]
Epoch [115/120    avg_loss:0.025, val_acc:0.966]
Epoch [116/120    avg_loss:0.020, val_acc:0.964]
Epoch [117/120    avg_loss:0.022, val_acc:0.965]
Epoch [118/120    avg_loss:0.021, val_acc:0.962]
Epoch [119/120    avg_loss:0.020, val_acc:0.962]
Epoch [120/120    avg_loss:0.022, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    1 1240    9    1    0    1    0    0    0    7   26    0    0
     0    0    0]
 [   0    0    5  726    1    5    0    0    0    8    0    1    0    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    1    0    7    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    2    0    0   14    0    0    0    0
     0    0    0]
 [   0    0   32   76    0    6    0    0    0    0  746    4    0    0
     1   10    0]
 [   0    0   13    0    0    0   12    0    0    0   23 2158    1    2
     1    0    0]
 [   0    0    0   18    4    3    0    0    0    0    8    4  489    0
     0    1    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    4    0    0    2    0    0    0    0    0    2    0    0
  1124    7    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    34  313    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.09756097560975

F1 scores:
[       nan 0.97560976 0.96124031 0.92015209 0.98611111 0.97031963
 0.98718915 0.98039216 1.         0.59574468 0.89933695 0.97935103
 0.95321637 0.99462366 0.97696654 0.92194404 0.94797688]

Kappa:
0.9555210170426828
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcc4007f898>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.753, val_acc:0.400]
Epoch [2/120    avg_loss:2.379, val_acc:0.515]
Epoch [3/120    avg_loss:2.243, val_acc:0.513]
Epoch [4/120    avg_loss:2.026, val_acc:0.536]
Epoch [5/120    avg_loss:1.898, val_acc:0.550]
Epoch [6/120    avg_loss:1.769, val_acc:0.600]
Epoch [7/120    avg_loss:1.672, val_acc:0.635]
Epoch [8/120    avg_loss:1.568, val_acc:0.645]
Epoch [9/120    avg_loss:1.386, val_acc:0.691]
Epoch [10/120    avg_loss:1.274, val_acc:0.720]
Epoch [11/120    avg_loss:1.081, val_acc:0.695]
Epoch [12/120    avg_loss:0.967, val_acc:0.746]
Epoch [13/120    avg_loss:0.845, val_acc:0.776]
Epoch [14/120    avg_loss:0.755, val_acc:0.799]
Epoch [15/120    avg_loss:0.716, val_acc:0.768]
Epoch [16/120    avg_loss:0.725, val_acc:0.795]
Epoch [17/120    avg_loss:0.654, val_acc:0.753]
Epoch [18/120    avg_loss:0.552, val_acc:0.845]
Epoch [19/120    avg_loss:0.519, val_acc:0.857]
Epoch [20/120    avg_loss:0.533, val_acc:0.832]
Epoch [21/120    avg_loss:0.462, val_acc:0.837]
Epoch [22/120    avg_loss:0.402, val_acc:0.863]
Epoch [23/120    avg_loss:0.339, val_acc:0.837]
Epoch [24/120    avg_loss:0.359, val_acc:0.862]
Epoch [25/120    avg_loss:0.371, val_acc:0.860]
Epoch [26/120    avg_loss:0.312, val_acc:0.890]
Epoch [27/120    avg_loss:0.279, val_acc:0.884]
Epoch [28/120    avg_loss:0.243, val_acc:0.898]
Epoch [29/120    avg_loss:0.221, val_acc:0.909]
Epoch [30/120    avg_loss:0.207, val_acc:0.887]
Epoch [31/120    avg_loss:0.244, val_acc:0.911]
Epoch [32/120    avg_loss:0.264, val_acc:0.904]
Epoch [33/120    avg_loss:0.214, val_acc:0.916]
Epoch [34/120    avg_loss:0.174, val_acc:0.909]
Epoch [35/120    avg_loss:0.184, val_acc:0.930]
Epoch [36/120    avg_loss:0.175, val_acc:0.920]
Epoch [37/120    avg_loss:0.149, val_acc:0.921]
Epoch [38/120    avg_loss:0.198, val_acc:0.927]
Epoch [39/120    avg_loss:0.155, val_acc:0.911]
Epoch [40/120    avg_loss:0.155, val_acc:0.932]
Epoch [41/120    avg_loss:0.154, val_acc:0.944]
Epoch [42/120    avg_loss:0.137, val_acc:0.933]
Epoch [43/120    avg_loss:0.119, val_acc:0.927]
Epoch [44/120    avg_loss:0.113, val_acc:0.942]
Epoch [45/120    avg_loss:0.153, val_acc:0.935]
Epoch [46/120    avg_loss:0.110, val_acc:0.950]
Epoch [47/120    avg_loss:0.134, val_acc:0.942]
Epoch [48/120    avg_loss:0.118, val_acc:0.941]
Epoch [49/120    avg_loss:0.100, val_acc:0.946]
Epoch [50/120    avg_loss:0.088, val_acc:0.954]
Epoch [51/120    avg_loss:0.071, val_acc:0.943]
Epoch [52/120    avg_loss:0.081, val_acc:0.952]
Epoch [53/120    avg_loss:0.072, val_acc:0.954]
Epoch [54/120    avg_loss:0.060, val_acc:0.953]
Epoch [55/120    avg_loss:0.064, val_acc:0.940]
Epoch [56/120    avg_loss:0.063, val_acc:0.943]
Epoch [57/120    avg_loss:0.099, val_acc:0.913]
Epoch [58/120    avg_loss:0.080, val_acc:0.940]
Epoch [59/120    avg_loss:0.062, val_acc:0.958]
Epoch [60/120    avg_loss:0.073, val_acc:0.922]
Epoch [61/120    avg_loss:0.084, val_acc:0.931]
Epoch [62/120    avg_loss:0.124, val_acc:0.942]
Epoch [63/120    avg_loss:0.085, val_acc:0.948]
Epoch [64/120    avg_loss:0.070, val_acc:0.942]
Epoch [65/120    avg_loss:0.058, val_acc:0.938]
Epoch [66/120    avg_loss:0.060, val_acc:0.942]
Epoch [67/120    avg_loss:0.051, val_acc:0.951]
Epoch [68/120    avg_loss:0.046, val_acc:0.950]
Epoch [69/120    avg_loss:0.048, val_acc:0.956]
Epoch [70/120    avg_loss:0.064, val_acc:0.950]
Epoch [71/120    avg_loss:0.074, val_acc:0.945]
Epoch [72/120    avg_loss:0.130, val_acc:0.929]
Epoch [73/120    avg_loss:0.097, val_acc:0.950]
Epoch [74/120    avg_loss:0.069, val_acc:0.953]
Epoch [75/120    avg_loss:0.057, val_acc:0.957]
Epoch [76/120    avg_loss:0.058, val_acc:0.956]
Epoch [77/120    avg_loss:0.054, val_acc:0.959]
Epoch [78/120    avg_loss:0.049, val_acc:0.965]
Epoch [79/120    avg_loss:0.042, val_acc:0.965]
Epoch [80/120    avg_loss:0.045, val_acc:0.966]
Epoch [81/120    avg_loss:0.054, val_acc:0.966]
Epoch [82/120    avg_loss:0.046, val_acc:0.967]
Epoch [83/120    avg_loss:0.041, val_acc:0.969]
Epoch [84/120    avg_loss:0.041, val_acc:0.961]
Epoch [85/120    avg_loss:0.040, val_acc:0.965]
Epoch [86/120    avg_loss:0.037, val_acc:0.966]
Epoch [87/120    avg_loss:0.035, val_acc:0.966]
Epoch [88/120    avg_loss:0.044, val_acc:0.963]
Epoch [89/120    avg_loss:0.036, val_acc:0.966]
Epoch [90/120    avg_loss:0.035, val_acc:0.968]
Epoch [91/120    avg_loss:0.033, val_acc:0.968]
Epoch [92/120    avg_loss:0.031, val_acc:0.969]
Epoch [93/120    avg_loss:0.032, val_acc:0.967]
Epoch [94/120    avg_loss:0.038, val_acc:0.967]
Epoch [95/120    avg_loss:0.031, val_acc:0.967]
Epoch [96/120    avg_loss:0.024, val_acc:0.968]
Epoch [97/120    avg_loss:0.029, val_acc:0.969]
Epoch [98/120    avg_loss:0.028, val_acc:0.969]
Epoch [99/120    avg_loss:0.029, val_acc:0.973]
Epoch [100/120    avg_loss:0.034, val_acc:0.969]
Epoch [101/120    avg_loss:0.023, val_acc:0.969]
Epoch [102/120    avg_loss:0.033, val_acc:0.969]
Epoch [103/120    avg_loss:0.025, val_acc:0.969]
Epoch [104/120    avg_loss:0.029, val_acc:0.967]
Epoch [105/120    avg_loss:0.026, val_acc:0.970]
Epoch [106/120    avg_loss:0.025, val_acc:0.971]
Epoch [107/120    avg_loss:0.025, val_acc:0.971]
Epoch [108/120    avg_loss:0.025, val_acc:0.974]
Epoch [109/120    avg_loss:0.026, val_acc:0.968]
Epoch [110/120    avg_loss:0.026, val_acc:0.970]
Epoch [111/120    avg_loss:0.026, val_acc:0.970]
Epoch [112/120    avg_loss:0.023, val_acc:0.969]
Epoch [113/120    avg_loss:0.022, val_acc:0.973]
Epoch [114/120    avg_loss:0.024, val_acc:0.974]
Epoch [115/120    avg_loss:0.024, val_acc:0.971]
Epoch [116/120    avg_loss:0.026, val_acc:0.967]
Epoch [117/120    avg_loss:0.026, val_acc:0.970]
Epoch [118/120    avg_loss:0.025, val_acc:0.968]
Epoch [119/120    avg_loss:0.026, val_acc:0.969]
Epoch [120/120    avg_loss:0.034, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1236    1    0    0    2    0    0    6   11   26    1    0
     0    2    0]
 [   0    0    1  705    0   24    0    0    0   11    0    1    4    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   17   43    0    6    1    0    0    0  804    4    0    0
     0    0    0]
 [   0    0   11    0    0    2    7    0    0    0   23 2161    4    1
     1    0    0]
 [   0    0    4   14    3   11    0    0    0    0    8    6  481    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    3    0    0    0
  1131    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    74  273    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.09756097560975

F1 scores:
[       nan 0.975      0.9678935  0.93377483 0.99300699 0.94194962
 0.99168556 1.         0.99649942 0.62962963 0.93163384 0.98026763
 0.93398058 0.99730458 0.96296296 0.8778135  0.94797688]

Kappa:
0.9555047837582733
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f65227de898>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.701, val_acc:0.466]
Epoch [2/120    avg_loss:2.395, val_acc:0.432]
Epoch [3/120    avg_loss:2.212, val_acc:0.400]
Epoch [4/120    avg_loss:2.093, val_acc:0.490]
Epoch [5/120    avg_loss:1.946, val_acc:0.576]
Epoch [6/120    avg_loss:1.823, val_acc:0.621]
Epoch [7/120    avg_loss:1.733, val_acc:0.625]
Epoch [8/120    avg_loss:1.572, val_acc:0.658]
Epoch [9/120    avg_loss:1.501, val_acc:0.684]
Epoch [10/120    avg_loss:1.385, val_acc:0.600]
Epoch [11/120    avg_loss:1.274, val_acc:0.695]
Epoch [12/120    avg_loss:1.151, val_acc:0.696]
Epoch [13/120    avg_loss:1.026, val_acc:0.693]
Epoch [14/120    avg_loss:0.940, val_acc:0.776]
Epoch [15/120    avg_loss:0.936, val_acc:0.788]
Epoch [16/120    avg_loss:0.912, val_acc:0.765]
Epoch [17/120    avg_loss:0.807, val_acc:0.807]
Epoch [18/120    avg_loss:0.696, val_acc:0.814]
Epoch [19/120    avg_loss:0.688, val_acc:0.822]
Epoch [20/120    avg_loss:0.609, val_acc:0.811]
Epoch [21/120    avg_loss:0.581, val_acc:0.822]
Epoch [22/120    avg_loss:0.598, val_acc:0.797]
Epoch [23/120    avg_loss:0.477, val_acc:0.860]
Epoch [24/120    avg_loss:0.488, val_acc:0.855]
Epoch [25/120    avg_loss:0.421, val_acc:0.873]
Epoch [26/120    avg_loss:0.397, val_acc:0.836]
Epoch [27/120    avg_loss:0.383, val_acc:0.894]
Epoch [28/120    avg_loss:0.301, val_acc:0.891]
Epoch [29/120    avg_loss:0.307, val_acc:0.850]
Epoch [30/120    avg_loss:0.412, val_acc:0.852]
Epoch [31/120    avg_loss:0.358, val_acc:0.899]
Epoch [32/120    avg_loss:0.315, val_acc:0.871]
Epoch [33/120    avg_loss:0.291, val_acc:0.883]
Epoch [34/120    avg_loss:0.284, val_acc:0.897]
Epoch [35/120    avg_loss:0.222, val_acc:0.916]
Epoch [36/120    avg_loss:0.202, val_acc:0.918]
Epoch [37/120    avg_loss:0.191, val_acc:0.906]
Epoch [38/120    avg_loss:0.194, val_acc:0.911]
Epoch [39/120    avg_loss:0.195, val_acc:0.914]
Epoch [40/120    avg_loss:0.200, val_acc:0.933]
Epoch [41/120    avg_loss:0.189, val_acc:0.918]
Epoch [42/120    avg_loss:0.204, val_acc:0.936]
Epoch [43/120    avg_loss:0.183, val_acc:0.936]
Epoch [44/120    avg_loss:0.164, val_acc:0.935]
Epoch [45/120    avg_loss:0.129, val_acc:0.940]
Epoch [46/120    avg_loss:0.130, val_acc:0.940]
Epoch [47/120    avg_loss:0.132, val_acc:0.950]
Epoch [48/120    avg_loss:0.134, val_acc:0.941]
Epoch [49/120    avg_loss:0.120, val_acc:0.942]
Epoch [50/120    avg_loss:0.111, val_acc:0.900]
Epoch [51/120    avg_loss:0.140, val_acc:0.943]
Epoch [52/120    avg_loss:0.108, val_acc:0.948]
Epoch [53/120    avg_loss:0.135, val_acc:0.927]
Epoch [54/120    avg_loss:0.157, val_acc:0.939]
Epoch [55/120    avg_loss:0.118, val_acc:0.945]
Epoch [56/120    avg_loss:0.118, val_acc:0.950]
Epoch [57/120    avg_loss:0.089, val_acc:0.959]
Epoch [58/120    avg_loss:0.084, val_acc:0.953]
Epoch [59/120    avg_loss:0.083, val_acc:0.956]
Epoch [60/120    avg_loss:0.066, val_acc:0.963]
Epoch [61/120    avg_loss:0.070, val_acc:0.957]
Epoch [62/120    avg_loss:0.064, val_acc:0.934]
Epoch [63/120    avg_loss:0.076, val_acc:0.951]
Epoch [64/120    avg_loss:0.081, val_acc:0.965]
Epoch [65/120    avg_loss:0.053, val_acc:0.961]
Epoch [66/120    avg_loss:0.056, val_acc:0.955]
Epoch [67/120    avg_loss:0.064, val_acc:0.962]
Epoch [68/120    avg_loss:0.070, val_acc:0.957]
Epoch [69/120    avg_loss:0.062, val_acc:0.964]
Epoch [70/120    avg_loss:0.103, val_acc:0.941]
Epoch [71/120    avg_loss:0.209, val_acc:0.908]
Epoch [72/120    avg_loss:0.255, val_acc:0.946]
Epoch [73/120    avg_loss:0.119, val_acc:0.959]
Epoch [74/120    avg_loss:0.079, val_acc:0.951]
Epoch [75/120    avg_loss:0.068, val_acc:0.957]
Epoch [76/120    avg_loss:0.074, val_acc:0.961]
Epoch [77/120    avg_loss:0.077, val_acc:0.951]
Epoch [78/120    avg_loss:0.057, val_acc:0.961]
Epoch [79/120    avg_loss:0.039, val_acc:0.965]
Epoch [80/120    avg_loss:0.038, val_acc:0.968]
Epoch [81/120    avg_loss:0.040, val_acc:0.970]
Epoch [82/120    avg_loss:0.039, val_acc:0.970]
Epoch [83/120    avg_loss:0.039, val_acc:0.973]
Epoch [84/120    avg_loss:0.033, val_acc:0.971]
Epoch [85/120    avg_loss:0.041, val_acc:0.973]
Epoch [86/120    avg_loss:0.036, val_acc:0.974]
Epoch [87/120    avg_loss:0.034, val_acc:0.973]
Epoch [88/120    avg_loss:0.042, val_acc:0.974]
Epoch [89/120    avg_loss:0.034, val_acc:0.975]
Epoch [90/120    avg_loss:0.031, val_acc:0.976]
Epoch [91/120    avg_loss:0.034, val_acc:0.976]
Epoch [92/120    avg_loss:0.032, val_acc:0.974]
Epoch [93/120    avg_loss:0.036, val_acc:0.975]
Epoch [94/120    avg_loss:0.030, val_acc:0.975]
Epoch [95/120    avg_loss:0.036, val_acc:0.975]
Epoch [96/120    avg_loss:0.036, val_acc:0.976]
Epoch [97/120    avg_loss:0.030, val_acc:0.976]
Epoch [98/120    avg_loss:0.037, val_acc:0.975]
Epoch [99/120    avg_loss:0.031, val_acc:0.975]
Epoch [100/120    avg_loss:0.035, val_acc:0.975]
Epoch [101/120    avg_loss:0.034, val_acc:0.976]
Epoch [102/120    avg_loss:0.034, val_acc:0.975]
Epoch [103/120    avg_loss:0.025, val_acc:0.976]
Epoch [104/120    avg_loss:0.033, val_acc:0.976]
Epoch [105/120    avg_loss:0.036, val_acc:0.976]
Epoch [106/120    avg_loss:0.027, val_acc:0.976]
Epoch [107/120    avg_loss:0.030, val_acc:0.976]
Epoch [108/120    avg_loss:0.028, val_acc:0.976]
Epoch [109/120    avg_loss:0.028, val_acc:0.976]
Epoch [110/120    avg_loss:0.029, val_acc:0.976]
Epoch [111/120    avg_loss:0.031, val_acc:0.975]
Epoch [112/120    avg_loss:0.029, val_acc:0.974]
Epoch [113/120    avg_loss:0.030, val_acc:0.976]
Epoch [114/120    avg_loss:0.026, val_acc:0.973]
Epoch [115/120    avg_loss:0.026, val_acc:0.974]
Epoch [116/120    avg_loss:0.027, val_acc:0.975]
Epoch [117/120    avg_loss:0.024, val_acc:0.975]
Epoch [118/120    avg_loss:0.030, val_acc:0.975]
Epoch [119/120    avg_loss:0.029, val_acc:0.975]
Epoch [120/120    avg_loss:0.029, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1249    2    0    0    4    0    0    0    4   23    3    0
     0    0    0]
 [   0    0    0  720    0   12    0    0    0    6    0    0    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    2    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7   75    0    3    0    0    0    0  779    8    0    0
     0    3    0]
 [   0    0    9    0    0    1   15    0    5    0   10 2165    3    2
     0    0    0]
 [   0    0    0    7    8    8    0    0    0    0    5    1  500    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    3    0    1    0    0    0
  1123    8    0]
 [   0    0    0    0    0    0   26    0    0    3    0    0    0    0
    67  251    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
96.07588075880759

F1 scores:
[       nan 0.975      0.97960784 0.92843327 0.98156682 0.96188341
 0.96688742 0.96153846 0.99078341 0.73469388 0.92959427 0.9825278
 0.94966762 0.99462366 0.96436239 0.82430213 0.94674556]

Kappa:
0.9552681188330161
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2de7093908>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.660, val_acc:0.370]
Epoch [2/120    avg_loss:2.347, val_acc:0.432]
Epoch [3/120    avg_loss:2.125, val_acc:0.508]
Epoch [4/120    avg_loss:2.002, val_acc:0.572]
Epoch [5/120    avg_loss:1.850, val_acc:0.576]
Epoch [6/120    avg_loss:1.690, val_acc:0.607]
Epoch [7/120    avg_loss:1.572, val_acc:0.617]
Epoch [8/120    avg_loss:1.432, val_acc:0.620]
Epoch [9/120    avg_loss:1.299, val_acc:0.628]
Epoch [10/120    avg_loss:1.177, val_acc:0.688]
Epoch [11/120    avg_loss:1.066, val_acc:0.730]
Epoch [12/120    avg_loss:0.957, val_acc:0.749]
Epoch [13/120    avg_loss:0.943, val_acc:0.716]
Epoch [14/120    avg_loss:0.849, val_acc:0.731]
Epoch [15/120    avg_loss:0.785, val_acc:0.766]
Epoch [16/120    avg_loss:0.769, val_acc:0.758]
Epoch [17/120    avg_loss:0.685, val_acc:0.745]
Epoch [18/120    avg_loss:0.619, val_acc:0.793]
Epoch [19/120    avg_loss:0.538, val_acc:0.834]
Epoch [20/120    avg_loss:0.489, val_acc:0.839]
Epoch [21/120    avg_loss:0.487, val_acc:0.821]
Epoch [22/120    avg_loss:0.437, val_acc:0.830]
Epoch [23/120    avg_loss:0.414, val_acc:0.713]
Epoch [24/120    avg_loss:0.449, val_acc:0.854]
Epoch [25/120    avg_loss:0.376, val_acc:0.849]
Epoch [26/120    avg_loss:0.342, val_acc:0.896]
Epoch [27/120    avg_loss:0.299, val_acc:0.861]
Epoch [28/120    avg_loss:0.318, val_acc:0.864]
Epoch [29/120    avg_loss:0.308, val_acc:0.873]
Epoch [30/120    avg_loss:0.342, val_acc:0.826]
Epoch [31/120    avg_loss:0.349, val_acc:0.867]
Epoch [32/120    avg_loss:0.274, val_acc:0.862]
Epoch [33/120    avg_loss:0.227, val_acc:0.906]
Epoch [34/120    avg_loss:0.261, val_acc:0.879]
Epoch [35/120    avg_loss:0.281, val_acc:0.866]
Epoch [36/120    avg_loss:0.213, val_acc:0.896]
Epoch [37/120    avg_loss:0.195, val_acc:0.876]
Epoch [38/120    avg_loss:0.190, val_acc:0.895]
Epoch [39/120    avg_loss:0.178, val_acc:0.920]
Epoch [40/120    avg_loss:0.166, val_acc:0.912]
Epoch [41/120    avg_loss:0.246, val_acc:0.893]
Epoch [42/120    avg_loss:0.180, val_acc:0.918]
Epoch [43/120    avg_loss:0.129, val_acc:0.931]
Epoch [44/120    avg_loss:0.133, val_acc:0.936]
Epoch [45/120    avg_loss:0.132, val_acc:0.929]
Epoch [46/120    avg_loss:0.094, val_acc:0.947]
Epoch [47/120    avg_loss:0.161, val_acc:0.853]
Epoch [48/120    avg_loss:0.202, val_acc:0.913]
Epoch [49/120    avg_loss:0.122, val_acc:0.912]
Epoch [50/120    avg_loss:0.144, val_acc:0.929]
Epoch [51/120    avg_loss:0.127, val_acc:0.928]
Epoch [52/120    avg_loss:0.095, val_acc:0.940]
Epoch [53/120    avg_loss:0.079, val_acc:0.950]
Epoch [54/120    avg_loss:0.091, val_acc:0.944]
Epoch [55/120    avg_loss:0.075, val_acc:0.931]
Epoch [56/120    avg_loss:0.079, val_acc:0.942]
Epoch [57/120    avg_loss:0.074, val_acc:0.924]
Epoch [58/120    avg_loss:0.082, val_acc:0.945]
Epoch [59/120    avg_loss:0.096, val_acc:0.939]
Epoch [60/120    avg_loss:0.099, val_acc:0.930]
Epoch [61/120    avg_loss:0.072, val_acc:0.948]
Epoch [62/120    avg_loss:0.053, val_acc:0.951]
Epoch [63/120    avg_loss:0.055, val_acc:0.957]
Epoch [64/120    avg_loss:0.061, val_acc:0.928]
Epoch [65/120    avg_loss:0.056, val_acc:0.950]
Epoch [66/120    avg_loss:0.058, val_acc:0.941]
Epoch [67/120    avg_loss:0.065, val_acc:0.919]
Epoch [68/120    avg_loss:0.085, val_acc:0.936]
Epoch [69/120    avg_loss:0.064, val_acc:0.947]
Epoch [70/120    avg_loss:0.054, val_acc:0.950]
Epoch [71/120    avg_loss:0.058, val_acc:0.941]
Epoch [72/120    avg_loss:0.056, val_acc:0.950]
Epoch [73/120    avg_loss:0.058, val_acc:0.946]
Epoch [74/120    avg_loss:0.049, val_acc:0.941]
Epoch [75/120    avg_loss:0.060, val_acc:0.945]
Epoch [76/120    avg_loss:0.061, val_acc:0.958]
Epoch [77/120    avg_loss:0.049, val_acc:0.959]
Epoch [78/120    avg_loss:0.034, val_acc:0.962]
Epoch [79/120    avg_loss:0.045, val_acc:0.959]
Epoch [80/120    avg_loss:0.054, val_acc:0.950]
Epoch [81/120    avg_loss:0.067, val_acc:0.918]
Epoch [82/120    avg_loss:0.249, val_acc:0.901]
Epoch [83/120    avg_loss:0.130, val_acc:0.922]
Epoch [84/120    avg_loss:0.083, val_acc:0.933]
Epoch [85/120    avg_loss:0.108, val_acc:0.888]
Epoch [86/120    avg_loss:0.156, val_acc:0.925]
Epoch [87/120    avg_loss:0.075, val_acc:0.950]
Epoch [88/120    avg_loss:0.063, val_acc:0.947]
Epoch [89/120    avg_loss:0.049, val_acc:0.943]
Epoch [90/120    avg_loss:0.050, val_acc:0.954]
Epoch [91/120    avg_loss:0.044, val_acc:0.948]
Epoch [92/120    avg_loss:0.033, val_acc:0.957]
Epoch [93/120    avg_loss:0.033, val_acc:0.962]
Epoch [94/120    avg_loss:0.037, val_acc:0.963]
Epoch [95/120    avg_loss:0.028, val_acc:0.963]
Epoch [96/120    avg_loss:0.027, val_acc:0.962]
Epoch [97/120    avg_loss:0.026, val_acc:0.962]
Epoch [98/120    avg_loss:0.024, val_acc:0.965]
Epoch [99/120    avg_loss:0.028, val_acc:0.965]
Epoch [100/120    avg_loss:0.031, val_acc:0.964]
Epoch [101/120    avg_loss:0.031, val_acc:0.964]
Epoch [102/120    avg_loss:0.025, val_acc:0.967]
Epoch [103/120    avg_loss:0.025, val_acc:0.970]
Epoch [104/120    avg_loss:0.034, val_acc:0.966]
Epoch [105/120    avg_loss:0.022, val_acc:0.965]
Epoch [106/120    avg_loss:0.040, val_acc:0.969]
Epoch [107/120    avg_loss:0.031, val_acc:0.970]
Epoch [108/120    avg_loss:0.028, val_acc:0.969]
Epoch [109/120    avg_loss:0.023, val_acc:0.969]
Epoch [110/120    avg_loss:0.022, val_acc:0.970]
Epoch [111/120    avg_loss:0.023, val_acc:0.967]
Epoch [112/120    avg_loss:0.023, val_acc:0.969]
Epoch [113/120    avg_loss:0.025, val_acc:0.971]
Epoch [114/120    avg_loss:0.021, val_acc:0.970]
Epoch [115/120    avg_loss:0.024, val_acc:0.969]
Epoch [116/120    avg_loss:0.021, val_acc:0.969]
Epoch [117/120    avg_loss:0.027, val_acc:0.969]
Epoch [118/120    avg_loss:0.019, val_acc:0.970]
Epoch [119/120    avg_loss:0.018, val_acc:0.969]
Epoch [120/120    avg_loss:0.019, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1238    1    0    0    0    0    0    1    9   32    4    0
     0    0    0]
 [   0    0    3  702    0   17    0    0    0    4    2    1   18    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   36   68    0    6    0    0    0    0  753    6    6    0
     0    0    0]
 [   0    0   18    0    0    2    3    0    2    0   23 2161    0    1
     0    0    0]
 [   0    0    0    4    5    3    0    0    0    0    2   16  498    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    1    0    0    0    4    0    0    1    0    3    2    0    0
  1127    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    37  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.130081300813

F1 scores:
[       nan 0.95       0.95968992 0.92247043 0.98839907 0.96222222
 0.99543379 0.98039216 0.99652375 0.85714286 0.90179641 0.97562077
 0.93873704 0.99730458 0.97829861 0.94224924 0.95953757]

Kappa:
0.9558703611754317
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1184cdc898>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.715, val_acc:0.363]
Epoch [2/120    avg_loss:2.435, val_acc:0.413]
Epoch [3/120    avg_loss:2.224, val_acc:0.467]
Epoch [4/120    avg_loss:2.064, val_acc:0.519]
Epoch [5/120    avg_loss:1.918, val_acc:0.529]
Epoch [6/120    avg_loss:1.811, val_acc:0.570]
Epoch [7/120    avg_loss:1.684, val_acc:0.609]
Epoch [8/120    avg_loss:1.530, val_acc:0.618]
Epoch [9/120    avg_loss:1.430, val_acc:0.660]
Epoch [10/120    avg_loss:1.291, val_acc:0.708]
Epoch [11/120    avg_loss:1.148, val_acc:0.719]
Epoch [12/120    avg_loss:1.037, val_acc:0.736]
Epoch [13/120    avg_loss:0.960, val_acc:0.742]
Epoch [14/120    avg_loss:0.956, val_acc:0.749]
Epoch [15/120    avg_loss:0.810, val_acc:0.808]
Epoch [16/120    avg_loss:0.752, val_acc:0.789]
Epoch [17/120    avg_loss:0.653, val_acc:0.779]
Epoch [18/120    avg_loss:0.606, val_acc:0.799]
Epoch [19/120    avg_loss:0.598, val_acc:0.827]
Epoch [20/120    avg_loss:0.539, val_acc:0.866]
Epoch [21/120    avg_loss:0.457, val_acc:0.861]
Epoch [22/120    avg_loss:0.476, val_acc:0.864]
Epoch [23/120    avg_loss:0.447, val_acc:0.856]
Epoch [24/120    avg_loss:0.333, val_acc:0.884]
Epoch [25/120    avg_loss:0.341, val_acc:0.899]
Epoch [26/120    avg_loss:0.432, val_acc:0.849]
Epoch [27/120    avg_loss:0.331, val_acc:0.877]
Epoch [28/120    avg_loss:0.281, val_acc:0.895]
Epoch [29/120    avg_loss:0.311, val_acc:0.891]
Epoch [30/120    avg_loss:0.238, val_acc:0.917]
Epoch [31/120    avg_loss:0.226, val_acc:0.890]
Epoch [32/120    avg_loss:0.205, val_acc:0.923]
Epoch [33/120    avg_loss:0.172, val_acc:0.919]
Epoch [34/120    avg_loss:0.161, val_acc:0.924]
Epoch [35/120    avg_loss:0.197, val_acc:0.911]
Epoch [36/120    avg_loss:0.173, val_acc:0.921]
Epoch [37/120    avg_loss:0.169, val_acc:0.912]
Epoch [38/120    avg_loss:0.184, val_acc:0.924]
Epoch [39/120    avg_loss:0.152, val_acc:0.925]
Epoch [40/120    avg_loss:0.137, val_acc:0.933]
Epoch [41/120    avg_loss:0.099, val_acc:0.932]
Epoch [42/120    avg_loss:0.107, val_acc:0.927]
Epoch [43/120    avg_loss:0.107, val_acc:0.939]
Epoch [44/120    avg_loss:0.096, val_acc:0.945]
Epoch [45/120    avg_loss:0.102, val_acc:0.939]
Epoch [46/120    avg_loss:0.110, val_acc:0.942]
Epoch [47/120    avg_loss:0.108, val_acc:0.944]
Epoch [48/120    avg_loss:0.131, val_acc:0.927]
Epoch [49/120    avg_loss:0.116, val_acc:0.944]
Epoch [50/120    avg_loss:0.088, val_acc:0.938]
Epoch [51/120    avg_loss:0.085, val_acc:0.950]
Epoch [52/120    avg_loss:0.084, val_acc:0.934]
Epoch [53/120    avg_loss:0.084, val_acc:0.945]
Epoch [54/120    avg_loss:0.085, val_acc:0.948]
Epoch [55/120    avg_loss:0.080, val_acc:0.938]
Epoch [56/120    avg_loss:0.081, val_acc:0.961]
Epoch [57/120    avg_loss:0.070, val_acc:0.951]
Epoch [58/120    avg_loss:0.096, val_acc:0.899]
Epoch [59/120    avg_loss:0.164, val_acc:0.930]
Epoch [60/120    avg_loss:0.094, val_acc:0.952]
Epoch [61/120    avg_loss:0.068, val_acc:0.946]
Epoch [62/120    avg_loss:0.087, val_acc:0.947]
Epoch [63/120    avg_loss:0.090, val_acc:0.948]
Epoch [64/120    avg_loss:0.064, val_acc:0.956]
Epoch [65/120    avg_loss:0.069, val_acc:0.932]
Epoch [66/120    avg_loss:0.086, val_acc:0.950]
Epoch [67/120    avg_loss:0.058, val_acc:0.934]
Epoch [68/120    avg_loss:0.066, val_acc:0.950]
Epoch [69/120    avg_loss:0.061, val_acc:0.953]
Epoch [70/120    avg_loss:0.043, val_acc:0.959]
Epoch [71/120    avg_loss:0.039, val_acc:0.961]
Epoch [72/120    avg_loss:0.036, val_acc:0.962]
Epoch [73/120    avg_loss:0.035, val_acc:0.962]
Epoch [74/120    avg_loss:0.030, val_acc:0.965]
Epoch [75/120    avg_loss:0.033, val_acc:0.963]
Epoch [76/120    avg_loss:0.033, val_acc:0.964]
Epoch [77/120    avg_loss:0.030, val_acc:0.964]
Epoch [78/120    avg_loss:0.033, val_acc:0.964]
Epoch [79/120    avg_loss:0.029, val_acc:0.961]
Epoch [80/120    avg_loss:0.034, val_acc:0.962]
Epoch [81/120    avg_loss:0.028, val_acc:0.963]
Epoch [82/120    avg_loss:0.031, val_acc:0.964]
Epoch [83/120    avg_loss:0.027, val_acc:0.964]
Epoch [84/120    avg_loss:0.028, val_acc:0.964]
Epoch [85/120    avg_loss:0.035, val_acc:0.963]
Epoch [86/120    avg_loss:0.028, val_acc:0.963]
Epoch [87/120    avg_loss:0.028, val_acc:0.963]
Epoch [88/120    avg_loss:0.023, val_acc:0.962]
Epoch [89/120    avg_loss:0.034, val_acc:0.963]
Epoch [90/120    avg_loss:0.024, val_acc:0.963]
Epoch [91/120    avg_loss:0.028, val_acc:0.963]
Epoch [92/120    avg_loss:0.032, val_acc:0.963]
Epoch [93/120    avg_loss:0.033, val_acc:0.963]
Epoch [94/120    avg_loss:0.030, val_acc:0.963]
Epoch [95/120    avg_loss:0.028, val_acc:0.963]
Epoch [96/120    avg_loss:0.026, val_acc:0.963]
Epoch [97/120    avg_loss:0.027, val_acc:0.963]
Epoch [98/120    avg_loss:0.025, val_acc:0.963]
Epoch [99/120    avg_loss:0.027, val_acc:0.963]
Epoch [100/120    avg_loss:0.031, val_acc:0.963]
Epoch [101/120    avg_loss:0.028, val_acc:0.963]
Epoch [102/120    avg_loss:0.023, val_acc:0.963]
Epoch [103/120    avg_loss:0.024, val_acc:0.963]
Epoch [104/120    avg_loss:0.028, val_acc:0.963]
Epoch [105/120    avg_loss:0.030, val_acc:0.963]
Epoch [106/120    avg_loss:0.024, val_acc:0.963]
Epoch [107/120    avg_loss:0.023, val_acc:0.963]
Epoch [108/120    avg_loss:0.026, val_acc:0.963]
Epoch [109/120    avg_loss:0.023, val_acc:0.963]
Epoch [110/120    avg_loss:0.025, val_acc:0.963]
Epoch [111/120    avg_loss:0.027, val_acc:0.963]
Epoch [112/120    avg_loss:0.024, val_acc:0.963]
Epoch [113/120    avg_loss:0.026, val_acc:0.963]
Epoch [114/120    avg_loss:0.025, val_acc:0.963]
Epoch [115/120    avg_loss:0.030, val_acc:0.963]
Epoch [116/120    avg_loss:0.029, val_acc:0.963]
Epoch [117/120    avg_loss:0.027, val_acc:0.963]
Epoch [118/120    avg_loss:0.026, val_acc:0.963]
Epoch [119/120    avg_loss:0.032, val_acc:0.963]
Epoch [120/120    avg_loss:0.028, val_acc:0.963]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1222    7    0    0    1    0    0    0    6   47    2    0
     0    0    0]
 [   0    0    2  711    0   15    0    0    0    6    1    0   12    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0   39   69    0    7    0    0    0    0  755    4    0    0
     0    1    0]
 [   0    0   15    0    0    0    4    0    2    0   22 2165    0    2
     0    0    0]
 [   0    0    0    6    5    8    0    0    0    0   11    0  496    0
     1    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    1    0    0    0
  1130    6    0]
 [   0    0    0    0    0    0   20    0    0    0    0    0    0    0
    50  277    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.70731707317073

F1 scores:
[       nan 0.94871795 0.95357004 0.92337662 0.98839907 0.9632107
 0.97827715 1.         0.99537037 0.71428571 0.90149254 0.97764732
 0.94656489 0.99462366 0.97371822 0.87797147 0.95402299]

Kappa:
0.9510429804464208
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7fa861c860>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.684, val_acc:0.418]
Epoch [2/120    avg_loss:2.373, val_acc:0.529]
Epoch [3/120    avg_loss:2.186, val_acc:0.542]
Epoch [4/120    avg_loss:2.018, val_acc:0.559]
Epoch [5/120    avg_loss:1.879, val_acc:0.545]
Epoch [6/120    avg_loss:1.715, val_acc:0.583]
Epoch [7/120    avg_loss:1.559, val_acc:0.604]
Epoch [8/120    avg_loss:1.518, val_acc:0.617]
Epoch [9/120    avg_loss:1.359, val_acc:0.668]
Epoch [10/120    avg_loss:1.261, val_acc:0.685]
Epoch [11/120    avg_loss:1.165, val_acc:0.686]
Epoch [12/120    avg_loss:1.049, val_acc:0.718]
Epoch [13/120    avg_loss:0.897, val_acc:0.720]
Epoch [14/120    avg_loss:0.849, val_acc:0.732]
Epoch [15/120    avg_loss:0.738, val_acc:0.777]
Epoch [16/120    avg_loss:0.644, val_acc:0.785]
Epoch [17/120    avg_loss:0.910, val_acc:0.671]
Epoch [18/120    avg_loss:0.756, val_acc:0.804]
Epoch [19/120    avg_loss:0.613, val_acc:0.796]
Epoch [20/120    avg_loss:0.507, val_acc:0.861]
Epoch [21/120    avg_loss:0.484, val_acc:0.836]
Epoch [22/120    avg_loss:0.491, val_acc:0.841]
Epoch [23/120    avg_loss:0.462, val_acc:0.852]
Epoch [24/120    avg_loss:0.435, val_acc:0.897]
Epoch [25/120    avg_loss:0.439, val_acc:0.843]
Epoch [26/120    avg_loss:0.460, val_acc:0.817]
Epoch [27/120    avg_loss:0.461, val_acc:0.867]
Epoch [28/120    avg_loss:0.331, val_acc:0.875]
Epoch [29/120    avg_loss:0.276, val_acc:0.883]
Epoch [30/120    avg_loss:0.272, val_acc:0.907]
Epoch [31/120    avg_loss:0.235, val_acc:0.904]
Epoch [32/120    avg_loss:0.223, val_acc:0.900]
Epoch [33/120    avg_loss:0.260, val_acc:0.913]
Epoch [34/120    avg_loss:0.253, val_acc:0.908]
Epoch [35/120    avg_loss:0.216, val_acc:0.889]
Epoch [36/120    avg_loss:0.232, val_acc:0.920]
Epoch [37/120    avg_loss:0.190, val_acc:0.932]
Epoch [38/120    avg_loss:0.155, val_acc:0.939]
Epoch [39/120    avg_loss:0.141, val_acc:0.931]
Epoch [40/120    avg_loss:0.122, val_acc:0.925]
Epoch [41/120    avg_loss:0.119, val_acc:0.929]
Epoch [42/120    avg_loss:0.117, val_acc:0.925]
Epoch [43/120    avg_loss:0.126, val_acc:0.930]
Epoch [44/120    avg_loss:0.146, val_acc:0.936]
Epoch [45/120    avg_loss:0.130, val_acc:0.932]
Epoch [46/120    avg_loss:0.164, val_acc:0.929]
Epoch [47/120    avg_loss:0.117, val_acc:0.945]
Epoch [48/120    avg_loss:0.089, val_acc:0.933]
Epoch [49/120    avg_loss:0.111, val_acc:0.943]
Epoch [50/120    avg_loss:0.122, val_acc:0.933]
Epoch [51/120    avg_loss:0.112, val_acc:0.948]
Epoch [52/120    avg_loss:0.109, val_acc:0.939]
Epoch [53/120    avg_loss:0.098, val_acc:0.955]
Epoch [54/120    avg_loss:0.083, val_acc:0.940]
Epoch [55/120    avg_loss:0.108, val_acc:0.946]
Epoch [56/120    avg_loss:0.106, val_acc:0.932]
Epoch [57/120    avg_loss:0.081, val_acc:0.959]
Epoch [58/120    avg_loss:0.067, val_acc:0.944]
Epoch [59/120    avg_loss:0.080, val_acc:0.958]
Epoch [60/120    avg_loss:0.063, val_acc:0.950]
Epoch [61/120    avg_loss:0.079, val_acc:0.954]
Epoch [62/120    avg_loss:0.087, val_acc:0.953]
Epoch [63/120    avg_loss:0.083, val_acc:0.944]
Epoch [64/120    avg_loss:0.075, val_acc:0.961]
Epoch [65/120    avg_loss:0.066, val_acc:0.965]
Epoch [66/120    avg_loss:0.056, val_acc:0.970]
Epoch [67/120    avg_loss:0.049, val_acc:0.965]
Epoch [68/120    avg_loss:0.063, val_acc:0.962]
Epoch [69/120    avg_loss:0.047, val_acc:0.967]
Epoch [70/120    avg_loss:0.053, val_acc:0.957]
Epoch [71/120    avg_loss:0.062, val_acc:0.966]
Epoch [72/120    avg_loss:0.084, val_acc:0.945]
Epoch [73/120    avg_loss:0.107, val_acc:0.956]
Epoch [74/120    avg_loss:0.065, val_acc:0.959]
Epoch [75/120    avg_loss:0.049, val_acc:0.963]
Epoch [76/120    avg_loss:0.043, val_acc:0.958]
Epoch [77/120    avg_loss:0.048, val_acc:0.957]
Epoch [78/120    avg_loss:0.048, val_acc:0.967]
Epoch [79/120    avg_loss:0.036, val_acc:0.970]
Epoch [80/120    avg_loss:0.040, val_acc:0.964]
Epoch [81/120    avg_loss:0.052, val_acc:0.961]
Epoch [82/120    avg_loss:0.079, val_acc:0.955]
Epoch [83/120    avg_loss:0.066, val_acc:0.959]
Epoch [84/120    avg_loss:0.044, val_acc:0.965]
Epoch [85/120    avg_loss:0.050, val_acc:0.962]
Epoch [86/120    avg_loss:0.086, val_acc:0.966]
Epoch [87/120    avg_loss:0.049, val_acc:0.966]
Epoch [88/120    avg_loss:0.057, val_acc:0.964]
Epoch [89/120    avg_loss:0.030, val_acc:0.963]
Epoch [90/120    avg_loss:0.029, val_acc:0.970]
Epoch [91/120    avg_loss:0.032, val_acc:0.966]
Epoch [92/120    avg_loss:0.031, val_acc:0.969]
Epoch [93/120    avg_loss:0.033, val_acc:0.968]
Epoch [94/120    avg_loss:0.026, val_acc:0.973]
Epoch [95/120    avg_loss:0.032, val_acc:0.973]
Epoch [96/120    avg_loss:0.042, val_acc:0.946]
Epoch [97/120    avg_loss:0.032, val_acc:0.970]
Epoch [98/120    avg_loss:0.036, val_acc:0.977]
Epoch [99/120    avg_loss:0.021, val_acc:0.978]
Epoch [100/120    avg_loss:0.028, val_acc:0.979]
Epoch [101/120    avg_loss:0.022, val_acc:0.978]
Epoch [102/120    avg_loss:0.038, val_acc:0.969]
Epoch [103/120    avg_loss:0.049, val_acc:0.922]
Epoch [104/120    avg_loss:0.105, val_acc:0.952]
Epoch [105/120    avg_loss:0.050, val_acc:0.947]
Epoch [106/120    avg_loss:0.045, val_acc:0.962]
Epoch [107/120    avg_loss:0.034, val_acc:0.978]
Epoch [108/120    avg_loss:0.028, val_acc:0.973]
Epoch [109/120    avg_loss:0.027, val_acc:0.973]
Epoch [110/120    avg_loss:0.023, val_acc:0.975]
Epoch [111/120    avg_loss:0.022, val_acc:0.968]
Epoch [112/120    avg_loss:0.026, val_acc:0.968]
Epoch [113/120    avg_loss:0.035, val_acc:0.957]
Epoch [114/120    avg_loss:0.024, val_acc:0.981]
Epoch [115/120    avg_loss:0.016, val_acc:0.980]
Epoch [116/120    avg_loss:0.015, val_acc:0.980]
Epoch [117/120    avg_loss:0.016, val_acc:0.981]
Epoch [118/120    avg_loss:0.015, val_acc:0.979]
Epoch [119/120    avg_loss:0.011, val_acc:0.978]
Epoch [120/120    avg_loss:0.017, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1229    4    0    0    0    0    0    0    8   38    6    0
     0    0    0]
 [   0    0    2  696    1   24    0    0    0   17    0    0    7    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    3    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0    2   62    0    6    0    0    0    0  799    3    0    0
     1    2    0]
 [   0    0    7    0    0    0    3    0    0    0   20 2178    1    1
     0    0    0]
 [   0    0    0   12   13    8    0    0    0    5   11   14  468    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    2    1    0    0
  1132    0    0]
 [   0    0    0    1    0    0   29    0    0    1    0    0    0    0
    29  287    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.0650406504065

F1 scores:
[       nan 0.93506494 0.97346535 0.91458607 0.96818182 0.94701987
 0.97546468 0.94339623 1.         0.55172414 0.92906977 0.9799775
 0.9185476  0.99730458 0.98306557 0.90251572 0.97647059]

Kappa:
0.9551295337370754
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb4ec261860>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.716, val_acc:0.412]
Epoch [2/120    avg_loss:2.395, val_acc:0.508]
Epoch [3/120    avg_loss:2.186, val_acc:0.520]
Epoch [4/120    avg_loss:2.049, val_acc:0.575]
Epoch [5/120    avg_loss:1.907, val_acc:0.565]
Epoch [6/120    avg_loss:1.793, val_acc:0.589]
Epoch [7/120    avg_loss:1.677, val_acc:0.639]
Epoch [8/120    avg_loss:1.518, val_acc:0.658]
Epoch [9/120    avg_loss:1.370, val_acc:0.700]
Epoch [10/120    avg_loss:1.244, val_acc:0.723]
Epoch [11/120    avg_loss:1.179, val_acc:0.696]
Epoch [12/120    avg_loss:1.061, val_acc:0.738]
Epoch [13/120    avg_loss:0.922, val_acc:0.739]
Epoch [14/120    avg_loss:0.822, val_acc:0.751]
Epoch [15/120    avg_loss:0.823, val_acc:0.769]
Epoch [16/120    avg_loss:0.693, val_acc:0.803]
Epoch [17/120    avg_loss:0.656, val_acc:0.798]
Epoch [18/120    avg_loss:0.606, val_acc:0.803]
Epoch [19/120    avg_loss:0.641, val_acc:0.821]
Epoch [20/120    avg_loss:0.541, val_acc:0.852]
Epoch [21/120    avg_loss:0.510, val_acc:0.850]
Epoch [22/120    avg_loss:0.466, val_acc:0.841]
Epoch [23/120    avg_loss:0.462, val_acc:0.819]
Epoch [24/120    avg_loss:0.450, val_acc:0.866]
Epoch [25/120    avg_loss:0.358, val_acc:0.823]
Epoch [26/120    avg_loss:0.359, val_acc:0.846]
Epoch [27/120    avg_loss:0.341, val_acc:0.864]
Epoch [28/120    avg_loss:0.365, val_acc:0.883]
Epoch [29/120    avg_loss:0.307, val_acc:0.893]
Epoch [30/120    avg_loss:0.266, val_acc:0.883]
Epoch [31/120    avg_loss:0.265, val_acc:0.862]
Epoch [32/120    avg_loss:0.262, val_acc:0.896]
Epoch [33/120    avg_loss:0.258, val_acc:0.908]
Epoch [34/120    avg_loss:0.221, val_acc:0.906]
Epoch [35/120    avg_loss:0.240, val_acc:0.910]
Epoch [36/120    avg_loss:0.206, val_acc:0.929]
Epoch [37/120    avg_loss:0.198, val_acc:0.910]
Epoch [38/120    avg_loss:0.216, val_acc:0.857]
Epoch [39/120    avg_loss:0.186, val_acc:0.907]
Epoch [40/120    avg_loss:0.173, val_acc:0.909]
Epoch [41/120    avg_loss:0.148, val_acc:0.931]
Epoch [42/120    avg_loss:0.143, val_acc:0.924]
Epoch [43/120    avg_loss:0.131, val_acc:0.923]
Epoch [44/120    avg_loss:0.112, val_acc:0.929]
Epoch [45/120    avg_loss:0.160, val_acc:0.936]
Epoch [46/120    avg_loss:0.124, val_acc:0.924]
Epoch [47/120    avg_loss:0.131, val_acc:0.943]
Epoch [48/120    avg_loss:0.113, val_acc:0.939]
Epoch [49/120    avg_loss:0.141, val_acc:0.927]
Epoch [50/120    avg_loss:0.141, val_acc:0.917]
Epoch [51/120    avg_loss:0.130, val_acc:0.940]
Epoch [52/120    avg_loss:0.113, val_acc:0.941]
Epoch [53/120    avg_loss:0.121, val_acc:0.899]
Epoch [54/120    avg_loss:0.142, val_acc:0.935]
Epoch [55/120    avg_loss:0.144, val_acc:0.919]
Epoch [56/120    avg_loss:0.178, val_acc:0.919]
Epoch [57/120    avg_loss:0.112, val_acc:0.944]
Epoch [58/120    avg_loss:0.101, val_acc:0.951]
Epoch [59/120    avg_loss:0.108, val_acc:0.946]
Epoch [60/120    avg_loss:0.087, val_acc:0.952]
Epoch [61/120    avg_loss:0.104, val_acc:0.958]
Epoch [62/120    avg_loss:0.122, val_acc:0.951]
Epoch [63/120    avg_loss:0.096, val_acc:0.951]
Epoch [64/120    avg_loss:0.071, val_acc:0.943]
Epoch [65/120    avg_loss:0.093, val_acc:0.947]
Epoch [66/120    avg_loss:0.096, val_acc:0.955]
Epoch [67/120    avg_loss:0.085, val_acc:0.963]
Epoch [68/120    avg_loss:0.060, val_acc:0.944]
Epoch [69/120    avg_loss:0.082, val_acc:0.947]
Epoch [70/120    avg_loss:0.068, val_acc:0.940]
Epoch [71/120    avg_loss:0.066, val_acc:0.956]
Epoch [72/120    avg_loss:0.054, val_acc:0.958]
Epoch [73/120    avg_loss:0.042, val_acc:0.966]
Epoch [74/120    avg_loss:0.065, val_acc:0.938]
Epoch [75/120    avg_loss:0.058, val_acc:0.959]
Epoch [76/120    avg_loss:0.042, val_acc:0.959]
Epoch [77/120    avg_loss:0.055, val_acc:0.950]
Epoch [78/120    avg_loss:0.046, val_acc:0.945]
Epoch [79/120    avg_loss:0.039, val_acc:0.965]
Epoch [80/120    avg_loss:0.043, val_acc:0.955]
Epoch [81/120    avg_loss:0.042, val_acc:0.961]
Epoch [82/120    avg_loss:0.029, val_acc:0.970]
Epoch [83/120    avg_loss:0.032, val_acc:0.969]
Epoch [84/120    avg_loss:0.034, val_acc:0.966]
Epoch [85/120    avg_loss:0.036, val_acc:0.959]
Epoch [86/120    avg_loss:0.031, val_acc:0.963]
Epoch [87/120    avg_loss:0.030, val_acc:0.970]
Epoch [88/120    avg_loss:0.030, val_acc:0.970]
Epoch [89/120    avg_loss:0.033, val_acc:0.956]
Epoch [90/120    avg_loss:0.034, val_acc:0.969]
Epoch [91/120    avg_loss:0.028, val_acc:0.969]
Epoch [92/120    avg_loss:0.029, val_acc:0.964]
Epoch [93/120    avg_loss:0.040, val_acc:0.959]
Epoch [94/120    avg_loss:0.050, val_acc:0.961]
Epoch [95/120    avg_loss:0.039, val_acc:0.959]
Epoch [96/120    avg_loss:0.028, val_acc:0.973]
Epoch [97/120    avg_loss:0.025, val_acc:0.973]
Epoch [98/120    avg_loss:0.021, val_acc:0.973]
Epoch [99/120    avg_loss:0.026, val_acc:0.966]
Epoch [100/120    avg_loss:0.024, val_acc:0.971]
Epoch [101/120    avg_loss:0.021, val_acc:0.973]
Epoch [102/120    avg_loss:0.025, val_acc:0.968]
Epoch [103/120    avg_loss:0.020, val_acc:0.966]
Epoch [104/120    avg_loss:0.033, val_acc:0.971]
Epoch [105/120    avg_loss:0.024, val_acc:0.938]
Epoch [106/120    avg_loss:0.038, val_acc:0.974]
Epoch [107/120    avg_loss:0.034, val_acc:0.973]
Epoch [108/120    avg_loss:0.027, val_acc:0.971]
Epoch [109/120    avg_loss:0.022, val_acc:0.961]
Epoch [110/120    avg_loss:0.025, val_acc:0.974]
Epoch [111/120    avg_loss:0.020, val_acc:0.970]
Epoch [112/120    avg_loss:0.023, val_acc:0.975]
Epoch [113/120    avg_loss:0.018, val_acc:0.974]
Epoch [114/120    avg_loss:0.015, val_acc:0.973]
Epoch [115/120    avg_loss:0.027, val_acc:0.967]
Epoch [116/120    avg_loss:0.022, val_acc:0.968]
Epoch [117/120    avg_loss:0.020, val_acc:0.966]
Epoch [118/120    avg_loss:0.018, val_acc:0.971]
Epoch [119/120    avg_loss:0.028, val_acc:0.968]
Epoch [120/120    avg_loss:0.022, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1222    8    0    0    5    0    0    0   13   35    2    0
     0    0    0]
 [   0    0    0  718    1    4    0    0    0   13    0    0   11    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  419    0    2    0    1    0    1    0    0
    12    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   28   45    0    2    1    0    0    0  783   15    0    0
     0    1    0]
 [   0    0    8    0    0    0    2    0    0    0   15 2181    4    0
     0    0    0]
 [   0    0    0   12    2    2    0    0    0    0    3   13  497    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    1    1    0    0    0    0    0
  1133    2    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    36  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.4769647696477

F1 scores:
[       nan 0.975      0.96031434 0.93733681 0.99300699 0.96990741
 0.98646617 0.94339623 0.995338   0.66666667 0.92717584 0.97890485
 0.94576594 1.         0.97672414 0.92638037 0.97109827]

Kappa:
0.9598046135193732
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fad691f78d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.727, val_acc:0.296]
Epoch [2/120    avg_loss:2.379, val_acc:0.500]
Epoch [3/120    avg_loss:2.165, val_acc:0.552]
Epoch [4/120    avg_loss:2.010, val_acc:0.577]
Epoch [5/120    avg_loss:1.927, val_acc:0.578]
Epoch [6/120    avg_loss:1.774, val_acc:0.607]
Epoch [7/120    avg_loss:1.695, val_acc:0.622]
Epoch [8/120    avg_loss:1.576, val_acc:0.618]
Epoch [9/120    avg_loss:1.467, val_acc:0.662]
Epoch [10/120    avg_loss:1.346, val_acc:0.679]
Epoch [11/120    avg_loss:1.214, val_acc:0.645]
Epoch [12/120    avg_loss:1.090, val_acc:0.708]
Epoch [13/120    avg_loss:0.969, val_acc:0.732]
Epoch [14/120    avg_loss:0.875, val_acc:0.772]
Epoch [15/120    avg_loss:0.882, val_acc:0.791]
Epoch [16/120    avg_loss:0.761, val_acc:0.812]
Epoch [17/120    avg_loss:0.671, val_acc:0.809]
Epoch [18/120    avg_loss:0.608, val_acc:0.842]
Epoch [19/120    avg_loss:0.527, val_acc:0.840]
Epoch [20/120    avg_loss:0.486, val_acc:0.754]
Epoch [21/120    avg_loss:0.451, val_acc:0.832]
Epoch [22/120    avg_loss:0.408, val_acc:0.818]
Epoch [23/120    avg_loss:0.579, val_acc:0.837]
Epoch [24/120    avg_loss:0.513, val_acc:0.844]
Epoch [25/120    avg_loss:0.506, val_acc:0.840]
Epoch [26/120    avg_loss:0.382, val_acc:0.888]
Epoch [27/120    avg_loss:0.378, val_acc:0.854]
Epoch [28/120    avg_loss:0.370, val_acc:0.871]
Epoch [29/120    avg_loss:0.319, val_acc:0.876]
Epoch [30/120    avg_loss:0.271, val_acc:0.875]
Epoch [31/120    avg_loss:0.268, val_acc:0.901]
Epoch [32/120    avg_loss:0.244, val_acc:0.882]
Epoch [33/120    avg_loss:0.241, val_acc:0.908]
Epoch [34/120    avg_loss:0.223, val_acc:0.925]
Epoch [35/120    avg_loss:0.323, val_acc:0.868]
Epoch [36/120    avg_loss:0.248, val_acc:0.889]
Epoch [37/120    avg_loss:0.222, val_acc:0.907]
Epoch [38/120    avg_loss:0.174, val_acc:0.921]
Epoch [39/120    avg_loss:0.201, val_acc:0.882]
Epoch [40/120    avg_loss:0.227, val_acc:0.913]
Epoch [41/120    avg_loss:0.200, val_acc:0.920]
Epoch [42/120    avg_loss:0.208, val_acc:0.908]
Epoch [43/120    avg_loss:0.167, val_acc:0.896]
Epoch [44/120    avg_loss:0.147, val_acc:0.920]
Epoch [45/120    avg_loss:0.131, val_acc:0.928]
Epoch [46/120    avg_loss:0.120, val_acc:0.932]
Epoch [47/120    avg_loss:0.113, val_acc:0.927]
Epoch [48/120    avg_loss:0.107, val_acc:0.922]
Epoch [49/120    avg_loss:0.102, val_acc:0.936]
Epoch [50/120    avg_loss:0.097, val_acc:0.918]
Epoch [51/120    avg_loss:0.102, val_acc:0.932]
Epoch [52/120    avg_loss:0.114, val_acc:0.921]
Epoch [53/120    avg_loss:0.092, val_acc:0.954]
Epoch [54/120    avg_loss:0.088, val_acc:0.941]
Epoch [55/120    avg_loss:0.096, val_acc:0.916]
Epoch [56/120    avg_loss:0.110, val_acc:0.934]
Epoch [57/120    avg_loss:0.083, val_acc:0.930]
Epoch [58/120    avg_loss:0.088, val_acc:0.936]
Epoch [59/120    avg_loss:0.077, val_acc:0.935]
Epoch [60/120    avg_loss:0.067, val_acc:0.955]
Epoch [61/120    avg_loss:0.063, val_acc:0.950]
Epoch [62/120    avg_loss:0.060, val_acc:0.945]
Epoch [63/120    avg_loss:0.069, val_acc:0.944]
Epoch [64/120    avg_loss:0.104, val_acc:0.933]
Epoch [65/120    avg_loss:0.106, val_acc:0.938]
Epoch [66/120    avg_loss:0.078, val_acc:0.939]
Epoch [67/120    avg_loss:0.076, val_acc:0.947]
Epoch [68/120    avg_loss:0.077, val_acc:0.943]
Epoch [69/120    avg_loss:0.059, val_acc:0.950]
Epoch [70/120    avg_loss:0.045, val_acc:0.948]
Epoch [71/120    avg_loss:0.048, val_acc:0.958]
Epoch [72/120    avg_loss:0.041, val_acc:0.953]
Epoch [73/120    avg_loss:0.047, val_acc:0.953]
Epoch [74/120    avg_loss:0.049, val_acc:0.956]
Epoch [75/120    avg_loss:0.044, val_acc:0.955]
Epoch [76/120    avg_loss:0.042, val_acc:0.954]
Epoch [77/120    avg_loss:0.048, val_acc:0.954]
Epoch [78/120    avg_loss:0.052, val_acc:0.950]
Epoch [79/120    avg_loss:0.039, val_acc:0.957]
Epoch [80/120    avg_loss:0.051, val_acc:0.962]
Epoch [81/120    avg_loss:0.042, val_acc:0.956]
Epoch [82/120    avg_loss:0.043, val_acc:0.954]
Epoch [83/120    avg_loss:0.038, val_acc:0.958]
Epoch [84/120    avg_loss:0.037, val_acc:0.957]
Epoch [85/120    avg_loss:0.046, val_acc:0.958]
Epoch [86/120    avg_loss:0.083, val_acc:0.947]
Epoch [87/120    avg_loss:0.085, val_acc:0.939]
Epoch [88/120    avg_loss:0.077, val_acc:0.940]
Epoch [89/120    avg_loss:0.055, val_acc:0.953]
Epoch [90/120    avg_loss:0.040, val_acc:0.959]
Epoch [91/120    avg_loss:0.044, val_acc:0.968]
Epoch [92/120    avg_loss:0.043, val_acc:0.970]
Epoch [93/120    avg_loss:0.043, val_acc:0.938]
Epoch [94/120    avg_loss:0.075, val_acc:0.948]
Epoch [95/120    avg_loss:0.053, val_acc:0.947]
Epoch [96/120    avg_loss:0.048, val_acc:0.948]
Epoch [97/120    avg_loss:0.046, val_acc:0.957]
Epoch [98/120    avg_loss:0.048, val_acc:0.962]
Epoch [99/120    avg_loss:0.033, val_acc:0.963]
Epoch [100/120    avg_loss:0.027, val_acc:0.952]
Epoch [101/120    avg_loss:0.044, val_acc:0.954]
Epoch [102/120    avg_loss:0.041, val_acc:0.956]
Epoch [103/120    avg_loss:0.036, val_acc:0.966]
Epoch [104/120    avg_loss:0.026, val_acc:0.963]
Epoch [105/120    avg_loss:0.028, val_acc:0.956]
Epoch [106/120    avg_loss:0.026, val_acc:0.967]
Epoch [107/120    avg_loss:0.025, val_acc:0.970]
Epoch [108/120    avg_loss:0.019, val_acc:0.974]
Epoch [109/120    avg_loss:0.021, val_acc:0.973]
Epoch [110/120    avg_loss:0.016, val_acc:0.974]
Epoch [111/120    avg_loss:0.017, val_acc:0.973]
Epoch [112/120    avg_loss:0.014, val_acc:0.973]
Epoch [113/120    avg_loss:0.015, val_acc:0.973]
Epoch [114/120    avg_loss:0.016, val_acc:0.971]
Epoch [115/120    avg_loss:0.014, val_acc:0.973]
Epoch [116/120    avg_loss:0.016, val_acc:0.971]
Epoch [117/120    avg_loss:0.023, val_acc:0.975]
Epoch [118/120    avg_loss:0.013, val_acc:0.974]
Epoch [119/120    avg_loss:0.014, val_acc:0.974]
Epoch [120/120    avg_loss:0.013, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    1 1254    2    0    0    1    0    0    0    2   23    1    0
     0    1    0]
 [   0    0    1  713    0   15    0    0    0   13    0    0    1    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    2    0    5    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    5   35    0    4    0    0    0    0  804   17    0    0
     0   10    0]
 [   0    0    4    0    0    0    5    0    0    0    6 2190    3    2
     0    0    0]
 [   0    0    1    4    8   10    0    0    0    1    5    0  497    0
     0    0    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    3    0    3    1    0    0
  1125    1    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    77  254    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.6178861788618

F1 scores:
[       nan 0.96296296 0.98314387 0.95003331 0.98156682 0.95206243
 0.98353293 0.96153846 0.99535963 0.62962963 0.94811321 0.98626435
 0.95761079 0.98404255 0.96071734 0.82871126 0.95454545]

Kappa:
0.9614203534778683
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8329888898>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.677, val_acc:0.317]
Epoch [2/120    avg_loss:2.390, val_acc:0.416]
Epoch [3/120    avg_loss:2.193, val_acc:0.429]
Epoch [4/120    avg_loss:2.032, val_acc:0.501]
Epoch [5/120    avg_loss:1.950, val_acc:0.497]
Epoch [6/120    avg_loss:1.815, val_acc:0.635]
Epoch [7/120    avg_loss:1.692, val_acc:0.635]
Epoch [8/120    avg_loss:1.523, val_acc:0.675]
Epoch [9/120    avg_loss:1.375, val_acc:0.670]
Epoch [10/120    avg_loss:1.263, val_acc:0.685]
Epoch [11/120    avg_loss:1.176, val_acc:0.689]
Epoch [12/120    avg_loss:1.048, val_acc:0.681]
Epoch [13/120    avg_loss:0.951, val_acc:0.757]
Epoch [14/120    avg_loss:0.884, val_acc:0.774]
Epoch [15/120    avg_loss:0.818, val_acc:0.746]
Epoch [16/120    avg_loss:0.759, val_acc:0.812]
Epoch [17/120    avg_loss:0.658, val_acc:0.815]
Epoch [18/120    avg_loss:0.609, val_acc:0.791]
Epoch [19/120    avg_loss:0.607, val_acc:0.770]
Epoch [20/120    avg_loss:0.612, val_acc:0.823]
Epoch [21/120    avg_loss:0.469, val_acc:0.841]
Epoch [22/120    avg_loss:0.519, val_acc:0.838]
Epoch [23/120    avg_loss:0.428, val_acc:0.871]
Epoch [24/120    avg_loss:0.461, val_acc:0.764]
Epoch [25/120    avg_loss:0.513, val_acc:0.822]
Epoch [26/120    avg_loss:0.489, val_acc:0.867]
Epoch [27/120    avg_loss:0.399, val_acc:0.874]
Epoch [28/120    avg_loss:0.396, val_acc:0.865]
Epoch [29/120    avg_loss:0.318, val_acc:0.891]
Epoch [30/120    avg_loss:0.287, val_acc:0.896]
Epoch [31/120    avg_loss:0.298, val_acc:0.902]
Epoch [32/120    avg_loss:0.247, val_acc:0.899]
Epoch [33/120    avg_loss:0.250, val_acc:0.904]
Epoch [34/120    avg_loss:0.319, val_acc:0.877]
Epoch [35/120    avg_loss:0.413, val_acc:0.887]
Epoch [36/120    avg_loss:0.280, val_acc:0.895]
Epoch [37/120    avg_loss:0.262, val_acc:0.916]
Epoch [38/120    avg_loss:0.215, val_acc:0.923]
Epoch [39/120    avg_loss:0.237, val_acc:0.918]
Epoch [40/120    avg_loss:0.236, val_acc:0.928]
Epoch [41/120    avg_loss:0.199, val_acc:0.913]
Epoch [42/120    avg_loss:0.165, val_acc:0.916]
Epoch [43/120    avg_loss:0.177, val_acc:0.918]
Epoch [44/120    avg_loss:0.155, val_acc:0.938]
Epoch [45/120    avg_loss:0.181, val_acc:0.909]
Epoch [46/120    avg_loss:0.162, val_acc:0.924]
Epoch [47/120    avg_loss:0.187, val_acc:0.910]
Epoch [48/120    avg_loss:0.181, val_acc:0.929]
Epoch [49/120    avg_loss:0.173, val_acc:0.923]
Epoch [50/120    avg_loss:0.170, val_acc:0.924]
Epoch [51/120    avg_loss:0.110, val_acc:0.936]
Epoch [52/120    avg_loss:0.097, val_acc:0.936]
Epoch [53/120    avg_loss:0.107, val_acc:0.944]
Epoch [54/120    avg_loss:0.099, val_acc:0.943]
Epoch [55/120    avg_loss:0.119, val_acc:0.943]
Epoch [56/120    avg_loss:0.075, val_acc:0.956]
Epoch [57/120    avg_loss:0.073, val_acc:0.945]
Epoch [58/120    avg_loss:0.091, val_acc:0.946]
Epoch [59/120    avg_loss:0.085, val_acc:0.944]
Epoch [60/120    avg_loss:0.144, val_acc:0.927]
Epoch [61/120    avg_loss:0.120, val_acc:0.929]
Epoch [62/120    avg_loss:0.077, val_acc:0.942]
Epoch [63/120    avg_loss:0.069, val_acc:0.942]
Epoch [64/120    avg_loss:0.052, val_acc:0.956]
Epoch [65/120    avg_loss:0.078, val_acc:0.940]
Epoch [66/120    avg_loss:0.072, val_acc:0.946]
Epoch [67/120    avg_loss:0.054, val_acc:0.955]
Epoch [68/120    avg_loss:0.077, val_acc:0.950]
Epoch [69/120    avg_loss:0.105, val_acc:0.940]
Epoch [70/120    avg_loss:0.060, val_acc:0.950]
Epoch [71/120    avg_loss:0.060, val_acc:0.955]
Epoch [72/120    avg_loss:0.070, val_acc:0.955]
Epoch [73/120    avg_loss:0.054, val_acc:0.945]
Epoch [74/120    avg_loss:0.052, val_acc:0.930]
Epoch [75/120    avg_loss:0.054, val_acc:0.961]
Epoch [76/120    avg_loss:0.050, val_acc:0.950]
Epoch [77/120    avg_loss:0.051, val_acc:0.954]
Epoch [78/120    avg_loss:0.078, val_acc:0.931]
Epoch [79/120    avg_loss:0.088, val_acc:0.943]
Epoch [80/120    avg_loss:0.070, val_acc:0.959]
Epoch [81/120    avg_loss:0.041, val_acc:0.958]
Epoch [82/120    avg_loss:0.061, val_acc:0.953]
Epoch [83/120    avg_loss:0.056, val_acc:0.958]
Epoch [84/120    avg_loss:0.051, val_acc:0.956]
Epoch [85/120    avg_loss:0.043, val_acc:0.952]
Epoch [86/120    avg_loss:0.060, val_acc:0.947]
Epoch [87/120    avg_loss:0.049, val_acc:0.966]
Epoch [88/120    avg_loss:0.039, val_acc:0.956]
Epoch [89/120    avg_loss:0.044, val_acc:0.962]
Epoch [90/120    avg_loss:0.028, val_acc:0.962]
Epoch [91/120    avg_loss:0.031, val_acc:0.963]
Epoch [92/120    avg_loss:0.034, val_acc:0.958]
Epoch [93/120    avg_loss:0.035, val_acc:0.957]
Epoch [94/120    avg_loss:0.026, val_acc:0.963]
Epoch [95/120    avg_loss:0.034, val_acc:0.962]
Epoch [96/120    avg_loss:0.043, val_acc:0.945]
Epoch [97/120    avg_loss:0.056, val_acc:0.961]
Epoch [98/120    avg_loss:0.038, val_acc:0.962]
Epoch [99/120    avg_loss:0.033, val_acc:0.945]
Epoch [100/120    avg_loss:0.034, val_acc:0.958]
Epoch [101/120    avg_loss:0.029, val_acc:0.965]
Epoch [102/120    avg_loss:0.022, val_acc:0.963]
Epoch [103/120    avg_loss:0.020, val_acc:0.969]
Epoch [104/120    avg_loss:0.020, val_acc:0.970]
Epoch [105/120    avg_loss:0.016, val_acc:0.970]
Epoch [106/120    avg_loss:0.016, val_acc:0.970]
Epoch [107/120    avg_loss:0.018, val_acc:0.969]
Epoch [108/120    avg_loss:0.016, val_acc:0.967]
Epoch [109/120    avg_loss:0.019, val_acc:0.969]
Epoch [110/120    avg_loss:0.015, val_acc:0.971]
Epoch [111/120    avg_loss:0.015, val_acc:0.971]
Epoch [112/120    avg_loss:0.015, val_acc:0.971]
Epoch [113/120    avg_loss:0.015, val_acc:0.971]
Epoch [114/120    avg_loss:0.017, val_acc:0.974]
Epoch [115/120    avg_loss:0.014, val_acc:0.973]
Epoch [116/120    avg_loss:0.015, val_acc:0.975]
Epoch [117/120    avg_loss:0.014, val_acc:0.975]
Epoch [118/120    avg_loss:0.015, val_acc:0.977]
Epoch [119/120    avg_loss:0.014, val_acc:0.974]
Epoch [120/120    avg_loss:0.018, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1252    2    4    0    3    0    0    0    6   18    0    0
     0    0    0]
 [   0    0    1  698    2   14    0    0    0   12    3    2   15    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    3    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   16   39    0    6    1    0    0    0  803    9    0    0
     0    1    0]
 [   0    0   11    0    0    1    1    0    0    0   11 2186    0    0
     0    0    0]
 [   0    0    0    0    5    4    0    0    0    0   11   10  494    0
     0    1    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   11    0    0    0    0    3    0    0    0
  1124    1    0]
 [   0    0    0    0    0    0   41    0    0    0    0    0    0    0
    59  247    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.21680216802169

F1 scores:
[       nan 0.975      0.97621832 0.93943472 0.97482838 0.94983278
 0.96541575 1.         1.         0.65306122 0.9369895  0.98557259
 0.94364852 1.         0.96563574 0.82747069 0.93714286]

Kappa:
0.9568403967853577
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f143b5c28d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.699, val_acc:0.333]
Epoch [2/120    avg_loss:2.369, val_acc:0.418]
Epoch [3/120    avg_loss:2.197, val_acc:0.471]
Epoch [4/120    avg_loss:2.007, val_acc:0.526]
Epoch [5/120    avg_loss:1.846, val_acc:0.579]
Epoch [6/120    avg_loss:1.700, val_acc:0.601]
Epoch [7/120    avg_loss:1.592, val_acc:0.619]
Epoch [8/120    avg_loss:1.419, val_acc:0.655]
Epoch [9/120    avg_loss:1.299, val_acc:0.650]
Epoch [10/120    avg_loss:1.116, val_acc:0.670]
Epoch [11/120    avg_loss:1.066, val_acc:0.720]
Epoch [12/120    avg_loss:0.961, val_acc:0.671]
Epoch [13/120    avg_loss:0.888, val_acc:0.726]
Epoch [14/120    avg_loss:0.772, val_acc:0.751]
Epoch [15/120    avg_loss:0.719, val_acc:0.764]
Epoch [16/120    avg_loss:0.782, val_acc:0.767]
Epoch [17/120    avg_loss:0.600, val_acc:0.806]
Epoch [18/120    avg_loss:0.515, val_acc:0.807]
Epoch [19/120    avg_loss:0.450, val_acc:0.834]
Epoch [20/120    avg_loss:0.515, val_acc:0.796]
Epoch [21/120    avg_loss:0.529, val_acc:0.795]
Epoch [22/120    avg_loss:0.486, val_acc:0.842]
Epoch [23/120    avg_loss:0.437, val_acc:0.841]
Epoch [24/120    avg_loss:0.389, val_acc:0.864]
Epoch [25/120    avg_loss:0.351, val_acc:0.856]
Epoch [26/120    avg_loss:0.335, val_acc:0.873]
Epoch [27/120    avg_loss:0.311, val_acc:0.861]
Epoch [28/120    avg_loss:0.299, val_acc:0.880]
Epoch [29/120    avg_loss:0.275, val_acc:0.883]
Epoch [30/120    avg_loss:0.269, val_acc:0.881]
Epoch [31/120    avg_loss:0.263, val_acc:0.887]
Epoch [32/120    avg_loss:0.242, val_acc:0.910]
Epoch [33/120    avg_loss:0.205, val_acc:0.911]
Epoch [34/120    avg_loss:0.202, val_acc:0.898]
Epoch [35/120    avg_loss:0.225, val_acc:0.901]
Epoch [36/120    avg_loss:0.210, val_acc:0.910]
Epoch [37/120    avg_loss:0.219, val_acc:0.925]
Epoch [38/120    avg_loss:0.181, val_acc:0.922]
Epoch [39/120    avg_loss:0.196, val_acc:0.921]
Epoch [40/120    avg_loss:0.159, val_acc:0.922]
Epoch [41/120    avg_loss:0.164, val_acc:0.944]
Epoch [42/120    avg_loss:0.151, val_acc:0.935]
Epoch [43/120    avg_loss:0.166, val_acc:0.911]
Epoch [44/120    avg_loss:0.185, val_acc:0.918]
Epoch [45/120    avg_loss:0.174, val_acc:0.937]
Epoch [46/120    avg_loss:0.136, val_acc:0.948]
Epoch [47/120    avg_loss:0.118, val_acc:0.939]
Epoch [48/120    avg_loss:0.136, val_acc:0.932]
Epoch [49/120    avg_loss:0.223, val_acc:0.881]
Epoch [50/120    avg_loss:0.274, val_acc:0.912]
Epoch [51/120    avg_loss:0.173, val_acc:0.936]
Epoch [52/120    avg_loss:0.145, val_acc:0.922]
Epoch [53/120    avg_loss:0.149, val_acc:0.940]
Epoch [54/120    avg_loss:0.118, val_acc:0.947]
Epoch [55/120    avg_loss:0.107, val_acc:0.947]
Epoch [56/120    avg_loss:0.118, val_acc:0.937]
Epoch [57/120    avg_loss:0.099, val_acc:0.952]
Epoch [58/120    avg_loss:0.107, val_acc:0.947]
Epoch [59/120    avg_loss:0.081, val_acc:0.949]
Epoch [60/120    avg_loss:0.128, val_acc:0.947]
Epoch [61/120    avg_loss:0.097, val_acc:0.958]
Epoch [62/120    avg_loss:0.090, val_acc:0.958]
Epoch [63/120    avg_loss:0.073, val_acc:0.958]
Epoch [64/120    avg_loss:0.063, val_acc:0.966]
Epoch [65/120    avg_loss:0.084, val_acc:0.950]
Epoch [66/120    avg_loss:0.080, val_acc:0.953]
Epoch [67/120    avg_loss:0.078, val_acc:0.961]
Epoch [68/120    avg_loss:0.067, val_acc:0.957]
Epoch [69/120    avg_loss:0.068, val_acc:0.950]
Epoch [70/120    avg_loss:0.088, val_acc:0.965]
Epoch [71/120    avg_loss:0.070, val_acc:0.953]
Epoch [72/120    avg_loss:0.085, val_acc:0.953]
Epoch [73/120    avg_loss:0.098, val_acc:0.962]
Epoch [74/120    avg_loss:0.077, val_acc:0.977]
Epoch [75/120    avg_loss:0.093, val_acc:0.948]
Epoch [76/120    avg_loss:0.066, val_acc:0.953]
Epoch [77/120    avg_loss:0.053, val_acc:0.953]
Epoch [78/120    avg_loss:0.057, val_acc:0.954]
Epoch [79/120    avg_loss:0.056, val_acc:0.967]
Epoch [80/120    avg_loss:0.064, val_acc:0.956]
Epoch [81/120    avg_loss:0.054, val_acc:0.964]
Epoch [82/120    avg_loss:0.055, val_acc:0.945]
Epoch [83/120    avg_loss:0.060, val_acc:0.973]
Epoch [84/120    avg_loss:0.053, val_acc:0.961]
Epoch [85/120    avg_loss:0.063, val_acc:0.972]
Epoch [86/120    avg_loss:0.062, val_acc:0.958]
Epoch [87/120    avg_loss:0.063, val_acc:0.965]
Epoch [88/120    avg_loss:0.035, val_acc:0.967]
Epoch [89/120    avg_loss:0.038, val_acc:0.968]
Epoch [90/120    avg_loss:0.044, val_acc:0.971]
Epoch [91/120    avg_loss:0.036, val_acc:0.973]
Epoch [92/120    avg_loss:0.032, val_acc:0.972]
Epoch [93/120    avg_loss:0.035, val_acc:0.976]
Epoch [94/120    avg_loss:0.031, val_acc:0.975]
Epoch [95/120    avg_loss:0.029, val_acc:0.979]
Epoch [96/120    avg_loss:0.025, val_acc:0.977]
Epoch [97/120    avg_loss:0.029, val_acc:0.981]
Epoch [98/120    avg_loss:0.026, val_acc:0.979]
Epoch [99/120    avg_loss:0.026, val_acc:0.980]
Epoch [100/120    avg_loss:0.027, val_acc:0.981]
Epoch [101/120    avg_loss:0.024, val_acc:0.981]
Epoch [102/120    avg_loss:0.022, val_acc:0.982]
Epoch [103/120    avg_loss:0.028, val_acc:0.980]
Epoch [104/120    avg_loss:0.030, val_acc:0.981]
Epoch [105/120    avg_loss:0.022, val_acc:0.981]
Epoch [106/120    avg_loss:0.025, val_acc:0.980]
Epoch [107/120    avg_loss:0.022, val_acc:0.981]
Epoch [108/120    avg_loss:0.025, val_acc:0.980]
Epoch [109/120    avg_loss:0.028, val_acc:0.981]
Epoch [110/120    avg_loss:0.024, val_acc:0.983]
Epoch [111/120    avg_loss:0.026, val_acc:0.983]
Epoch [112/120    avg_loss:0.025, val_acc:0.983]
Epoch [113/120    avg_loss:0.020, val_acc:0.982]
Epoch [114/120    avg_loss:0.019, val_acc:0.981]
Epoch [115/120    avg_loss:0.025, val_acc:0.981]
Epoch [116/120    avg_loss:0.023, val_acc:0.981]
Epoch [117/120    avg_loss:0.020, val_acc:0.982]
Epoch [118/120    avg_loss:0.025, val_acc:0.982]
Epoch [119/120    avg_loss:0.025, val_acc:0.981]
Epoch [120/120    avg_loss:0.019, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1246    4    0    0    3    0    0    0    5   23    4    0
     0    0    0]
 [   0    0    1  705    4   14    0    0    0   11    0    0    8    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    6    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    7    0    0    0    0    0    0  423    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   21   83    0    3    0    0    0    0  753    9    0    0
     3    3    0]
 [   0    0    9    0    0    0   10    0    0    0   11 2174    3    3
     0    0    0]
 [   0    0    3   21    0    1    0    0    0    0   11    2  487    0
     0    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0    0
  1137    1    0]
 [   0    0    0    0    0    0   29    0    0    0    0    0    0    0
    54  264    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.71815718157181

F1 scores:
[       nan 0.87058824 0.97153996 0.90384615 0.99069767 0.97045455
 0.96674058 1.         0.99179367 0.67924528 0.90777577 0.98326549
 0.93834297 0.98143236 0.9738758  0.85853659 0.93714286]

Kappa:
0.9511704858825266
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f11e23a1860>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.744, val_acc:0.400]
Epoch [2/120    avg_loss:2.434, val_acc:0.419]
Epoch [3/120    avg_loss:2.166, val_acc:0.466]
Epoch [4/120    avg_loss:2.002, val_acc:0.466]
Epoch [5/120    avg_loss:1.896, val_acc:0.538]
Epoch [6/120    avg_loss:1.722, val_acc:0.571]
Epoch [7/120    avg_loss:1.619, val_acc:0.571]
Epoch [8/120    avg_loss:1.472, val_acc:0.609]
Epoch [9/120    avg_loss:1.280, val_acc:0.660]
Epoch [10/120    avg_loss:1.214, val_acc:0.663]
Epoch [11/120    avg_loss:1.033, val_acc:0.690]
Epoch [12/120    avg_loss:0.953, val_acc:0.672]
Epoch [13/120    avg_loss:0.915, val_acc:0.735]
Epoch [14/120    avg_loss:0.885, val_acc:0.747]
Epoch [15/120    avg_loss:0.761, val_acc:0.757]
Epoch [16/120    avg_loss:0.702, val_acc:0.755]
Epoch [17/120    avg_loss:0.692, val_acc:0.747]
Epoch [18/120    avg_loss:0.604, val_acc:0.786]
Epoch [19/120    avg_loss:0.591, val_acc:0.783]
Epoch [20/120    avg_loss:0.512, val_acc:0.843]
Epoch [21/120    avg_loss:0.457, val_acc:0.831]
Epoch [22/120    avg_loss:0.429, val_acc:0.830]
Epoch [23/120    avg_loss:0.448, val_acc:0.807]
Epoch [24/120    avg_loss:0.401, val_acc:0.847]
Epoch [25/120    avg_loss:0.394, val_acc:0.843]
Epoch [26/120    avg_loss:0.393, val_acc:0.873]
Epoch [27/120    avg_loss:0.321, val_acc:0.865]
Epoch [28/120    avg_loss:0.323, val_acc:0.873]
Epoch [29/120    avg_loss:0.296, val_acc:0.867]
Epoch [30/120    avg_loss:0.275, val_acc:0.861]
Epoch [31/120    avg_loss:0.265, val_acc:0.876]
Epoch [32/120    avg_loss:0.291, val_acc:0.882]
Epoch [33/120    avg_loss:0.266, val_acc:0.873]
Epoch [34/120    avg_loss:0.263, val_acc:0.902]
Epoch [35/120    avg_loss:0.261, val_acc:0.892]
Epoch [36/120    avg_loss:0.227, val_acc:0.887]
Epoch [37/120    avg_loss:0.216, val_acc:0.926]
Epoch [38/120    avg_loss:0.189, val_acc:0.921]
Epoch [39/120    avg_loss:0.192, val_acc:0.917]
Epoch [40/120    avg_loss:0.205, val_acc:0.904]
Epoch [41/120    avg_loss:0.174, val_acc:0.911]
Epoch [42/120    avg_loss:0.181, val_acc:0.892]
Epoch [43/120    avg_loss:0.171, val_acc:0.925]
Epoch [44/120    avg_loss:0.135, val_acc:0.913]
Epoch [45/120    avg_loss:0.145, val_acc:0.927]
Epoch [46/120    avg_loss:0.128, val_acc:0.932]
Epoch [47/120    avg_loss:0.184, val_acc:0.892]
Epoch [48/120    avg_loss:0.167, val_acc:0.927]
Epoch [49/120    avg_loss:0.207, val_acc:0.896]
Epoch [50/120    avg_loss:0.172, val_acc:0.916]
Epoch [51/120    avg_loss:0.149, val_acc:0.881]
Epoch [52/120    avg_loss:0.142, val_acc:0.922]
Epoch [53/120    avg_loss:0.128, val_acc:0.938]
Epoch [54/120    avg_loss:0.135, val_acc:0.909]
Epoch [55/120    avg_loss:0.108, val_acc:0.937]
Epoch [56/120    avg_loss:0.101, val_acc:0.939]
Epoch [57/120    avg_loss:0.172, val_acc:0.887]
Epoch [58/120    avg_loss:0.245, val_acc:0.887]
Epoch [59/120    avg_loss:0.160, val_acc:0.894]
Epoch [60/120    avg_loss:0.124, val_acc:0.938]
Epoch [61/120    avg_loss:0.124, val_acc:0.926]
Epoch [62/120    avg_loss:0.153, val_acc:0.914]
Epoch [63/120    avg_loss:0.165, val_acc:0.937]
Epoch [64/120    avg_loss:0.092, val_acc:0.952]
Epoch [65/120    avg_loss:0.096, val_acc:0.944]
Epoch [66/120    avg_loss:0.097, val_acc:0.932]
Epoch [67/120    avg_loss:0.088, val_acc:0.948]
Epoch [68/120    avg_loss:0.093, val_acc:0.947]
Epoch [69/120    avg_loss:0.092, val_acc:0.937]
Epoch [70/120    avg_loss:0.066, val_acc:0.949]
Epoch [71/120    avg_loss:0.109, val_acc:0.936]
Epoch [72/120    avg_loss:0.082, val_acc:0.957]
Epoch [73/120    avg_loss:0.070, val_acc:0.950]
Epoch [74/120    avg_loss:0.124, val_acc:0.949]
Epoch [75/120    avg_loss:0.078, val_acc:0.959]
Epoch [76/120    avg_loss:0.069, val_acc:0.957]
Epoch [77/120    avg_loss:0.074, val_acc:0.939]
Epoch [78/120    avg_loss:0.119, val_acc:0.926]
Epoch [79/120    avg_loss:0.118, val_acc:0.937]
Epoch [80/120    avg_loss:0.101, val_acc:0.948]
Epoch [81/120    avg_loss:0.070, val_acc:0.947]
Epoch [82/120    avg_loss:0.085, val_acc:0.946]
Epoch [83/120    avg_loss:0.072, val_acc:0.943]
Epoch [84/120    avg_loss:0.069, val_acc:0.949]
Epoch [85/120    avg_loss:0.073, val_acc:0.959]
Epoch [86/120    avg_loss:0.063, val_acc:0.958]
Epoch [87/120    avg_loss:0.055, val_acc:0.952]
Epoch [88/120    avg_loss:0.061, val_acc:0.940]
Epoch [89/120    avg_loss:0.053, val_acc:0.961]
Epoch [90/120    avg_loss:0.044, val_acc:0.961]
Epoch [91/120    avg_loss:0.037, val_acc:0.965]
Epoch [92/120    avg_loss:0.030, val_acc:0.958]
Epoch [93/120    avg_loss:0.034, val_acc:0.962]
Epoch [94/120    avg_loss:0.048, val_acc:0.958]
Epoch [95/120    avg_loss:0.039, val_acc:0.953]
Epoch [96/120    avg_loss:0.030, val_acc:0.966]
Epoch [97/120    avg_loss:0.034, val_acc:0.964]
Epoch [98/120    avg_loss:0.035, val_acc:0.964]
Epoch [99/120    avg_loss:0.031, val_acc:0.967]
Epoch [100/120    avg_loss:0.041, val_acc:0.957]
Epoch [101/120    avg_loss:0.031, val_acc:0.956]
Epoch [102/120    avg_loss:0.041, val_acc:0.962]
Epoch [103/120    avg_loss:0.028, val_acc:0.959]
Epoch [104/120    avg_loss:0.040, val_acc:0.957]
Epoch [105/120    avg_loss:0.045, val_acc:0.950]
Epoch [106/120    avg_loss:0.037, val_acc:0.953]
Epoch [107/120    avg_loss:0.034, val_acc:0.948]
Epoch [108/120    avg_loss:0.030, val_acc:0.963]
Epoch [109/120    avg_loss:0.028, val_acc:0.965]
Epoch [110/120    avg_loss:0.027, val_acc:0.955]
Epoch [111/120    avg_loss:0.046, val_acc:0.962]
Epoch [112/120    avg_loss:0.039, val_acc:0.945]
Epoch [113/120    avg_loss:0.027, val_acc:0.957]
Epoch [114/120    avg_loss:0.029, val_acc:0.959]
Epoch [115/120    avg_loss:0.023, val_acc:0.966]
Epoch [116/120    avg_loss:0.020, val_acc:0.966]
Epoch [117/120    avg_loss:0.020, val_acc:0.965]
Epoch [118/120    avg_loss:0.022, val_acc:0.967]
Epoch [119/120    avg_loss:0.021, val_acc:0.970]
Epoch [120/120    avg_loss:0.017, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    2 1238    0    0    0    1    0    0    1    7   27    9    0
     0    0    0]
 [   0    0    1  708    2    4    0    0    0   11    0    0   21    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    3    0    6    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    5    0    0    0    0    0    0  425    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0   14   90    0    1    0    0    0    0  763    0    0    0
     0    7    0]
 [   0    0   33    0    0    0   17    0    1    0   14 2134    8    3
     0    0    0]
 [   0    0    0    1   21    1    0    0    0    0    9    0  499    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    2    0    1    0    3    0    0    0
  1133    0    0]
 [   0    0    1    0    0    0   54    0    0    1    0    0    0    0
    99  192    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.62330623306234

F1 scores:
[       nan 0.89655172 0.96267496 0.91591203 0.94877506 0.98034682
 0.94440433 0.94339623 0.99183197 0.57692308 0.91213389 0.97576589
 0.92750929 0.9919571  0.9549094  0.7032967  0.9704142 ]

Kappa:
0.9387149408704187
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8d260f8860>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.725, val_acc:0.347]
Epoch [2/120    avg_loss:2.388, val_acc:0.423]
Epoch [3/120    avg_loss:2.209, val_acc:0.506]
Epoch [4/120    avg_loss:2.048, val_acc:0.536]
Epoch [5/120    avg_loss:1.910, val_acc:0.515]
Epoch [6/120    avg_loss:1.817, val_acc:0.593]
Epoch [7/120    avg_loss:1.677, val_acc:0.597]
Epoch [8/120    avg_loss:1.562, val_acc:0.630]
Epoch [9/120    avg_loss:1.431, val_acc:0.633]
Epoch [10/120    avg_loss:1.311, val_acc:0.666]
Epoch [11/120    avg_loss:1.158, val_acc:0.690]
Epoch [12/120    avg_loss:1.033, val_acc:0.726]
Epoch [13/120    avg_loss:0.991, val_acc:0.702]
Epoch [14/120    avg_loss:0.880, val_acc:0.722]
Epoch [15/120    avg_loss:0.848, val_acc:0.714]
Epoch [16/120    avg_loss:0.827, val_acc:0.690]
Epoch [17/120    avg_loss:0.770, val_acc:0.760]
Epoch [18/120    avg_loss:0.766, val_acc:0.712]
Epoch [19/120    avg_loss:0.702, val_acc:0.755]
Epoch [20/120    avg_loss:0.585, val_acc:0.823]
Epoch [21/120    avg_loss:0.494, val_acc:0.813]
Epoch [22/120    avg_loss:0.504, val_acc:0.837]
Epoch [23/120    avg_loss:0.553, val_acc:0.813]
Epoch [24/120    avg_loss:0.462, val_acc:0.795]
Epoch [25/120    avg_loss:0.423, val_acc:0.829]
Epoch [26/120    avg_loss:0.432, val_acc:0.858]
Epoch [27/120    avg_loss:0.400, val_acc:0.801]
Epoch [28/120    avg_loss:0.388, val_acc:0.839]
Epoch [29/120    avg_loss:0.368, val_acc:0.824]
Epoch [30/120    avg_loss:0.405, val_acc:0.846]
Epoch [31/120    avg_loss:0.428, val_acc:0.794]
Epoch [32/120    avg_loss:0.345, val_acc:0.856]
Epoch [33/120    avg_loss:0.306, val_acc:0.901]
Epoch [34/120    avg_loss:0.245, val_acc:0.887]
Epoch [35/120    avg_loss:0.331, val_acc:0.880]
Epoch [36/120    avg_loss:0.248, val_acc:0.902]
Epoch [37/120    avg_loss:0.311, val_acc:0.877]
Epoch [38/120    avg_loss:0.249, val_acc:0.876]
Epoch [39/120    avg_loss:0.232, val_acc:0.886]
Epoch [40/120    avg_loss:0.217, val_acc:0.911]
Epoch [41/120    avg_loss:0.202, val_acc:0.901]
Epoch [42/120    avg_loss:0.179, val_acc:0.925]
Epoch [43/120    avg_loss:0.173, val_acc:0.904]
Epoch [44/120    avg_loss:0.200, val_acc:0.893]
Epoch [45/120    avg_loss:0.228, val_acc:0.895]
Epoch [46/120    avg_loss:0.205, val_acc:0.898]
Epoch [47/120    avg_loss:0.197, val_acc:0.910]
Epoch [48/120    avg_loss:0.144, val_acc:0.919]
Epoch [49/120    avg_loss:0.146, val_acc:0.930]
Epoch [50/120    avg_loss:0.139, val_acc:0.909]
Epoch [51/120    avg_loss:0.130, val_acc:0.919]
Epoch [52/120    avg_loss:0.156, val_acc:0.912]
Epoch [53/120    avg_loss:0.156, val_acc:0.890]
Epoch [54/120    avg_loss:0.230, val_acc:0.909]
Epoch [55/120    avg_loss:0.134, val_acc:0.921]
Epoch [56/120    avg_loss:0.095, val_acc:0.926]
Epoch [57/120    avg_loss:0.094, val_acc:0.940]
Epoch [58/120    avg_loss:0.096, val_acc:0.944]
Epoch [59/120    avg_loss:0.097, val_acc:0.940]
Epoch [60/120    avg_loss:0.082, val_acc:0.927]
Epoch [61/120    avg_loss:0.098, val_acc:0.923]
Epoch [62/120    avg_loss:0.108, val_acc:0.930]
Epoch [63/120    avg_loss:0.111, val_acc:0.923]
Epoch [64/120    avg_loss:0.096, val_acc:0.932]
Epoch [65/120    avg_loss:0.293, val_acc:0.863]
Epoch [66/120    avg_loss:0.216, val_acc:0.929]
Epoch [67/120    avg_loss:0.144, val_acc:0.928]
Epoch [68/120    avg_loss:0.100, val_acc:0.936]
Epoch [69/120    avg_loss:0.117, val_acc:0.949]
Epoch [70/120    avg_loss:0.086, val_acc:0.940]
Epoch [71/120    avg_loss:0.080, val_acc:0.961]
Epoch [72/120    avg_loss:0.075, val_acc:0.935]
Epoch [73/120    avg_loss:0.071, val_acc:0.954]
Epoch [74/120    avg_loss:0.071, val_acc:0.956]
Epoch [75/120    avg_loss:0.059, val_acc:0.949]
Epoch [76/120    avg_loss:0.048, val_acc:0.962]
Epoch [77/120    avg_loss:0.047, val_acc:0.957]
Epoch [78/120    avg_loss:0.054, val_acc:0.963]
Epoch [79/120    avg_loss:0.059, val_acc:0.957]
Epoch [80/120    avg_loss:0.061, val_acc:0.958]
Epoch [81/120    avg_loss:0.050, val_acc:0.957]
Epoch [82/120    avg_loss:0.061, val_acc:0.952]
Epoch [83/120    avg_loss:0.046, val_acc:0.959]
Epoch [84/120    avg_loss:0.051, val_acc:0.966]
Epoch [85/120    avg_loss:0.064, val_acc:0.945]
Epoch [86/120    avg_loss:0.056, val_acc:0.962]
Epoch [87/120    avg_loss:0.090, val_acc:0.918]
Epoch [88/120    avg_loss:0.128, val_acc:0.919]
Epoch [89/120    avg_loss:0.078, val_acc:0.949]
Epoch [90/120    avg_loss:0.078, val_acc:0.939]
Epoch [91/120    avg_loss:0.090, val_acc:0.945]
Epoch [92/120    avg_loss:0.060, val_acc:0.957]
Epoch [93/120    avg_loss:0.054, val_acc:0.965]
Epoch [94/120    avg_loss:0.059, val_acc:0.958]
Epoch [95/120    avg_loss:0.031, val_acc:0.962]
Epoch [96/120    avg_loss:0.042, val_acc:0.962]
Epoch [97/120    avg_loss:0.066, val_acc:0.882]
Epoch [98/120    avg_loss:0.101, val_acc:0.943]
Epoch [99/120    avg_loss:0.050, val_acc:0.950]
Epoch [100/120    avg_loss:0.048, val_acc:0.954]
Epoch [101/120    avg_loss:0.044, val_acc:0.959]
Epoch [102/120    avg_loss:0.045, val_acc:0.956]
Epoch [103/120    avg_loss:0.045, val_acc:0.964]
Epoch [104/120    avg_loss:0.034, val_acc:0.964]
Epoch [105/120    avg_loss:0.031, val_acc:0.964]
Epoch [106/120    avg_loss:0.036, val_acc:0.968]
Epoch [107/120    avg_loss:0.030, val_acc:0.966]
Epoch [108/120    avg_loss:0.029, val_acc:0.967]
Epoch [109/120    avg_loss:0.033, val_acc:0.968]
Epoch [110/120    avg_loss:0.031, val_acc:0.966]
Epoch [111/120    avg_loss:0.036, val_acc:0.967]
Epoch [112/120    avg_loss:0.040, val_acc:0.972]
Epoch [113/120    avg_loss:0.038, val_acc:0.971]
Epoch [114/120    avg_loss:0.039, val_acc:0.971]
Epoch [115/120    avg_loss:0.031, val_acc:0.967]
Epoch [116/120    avg_loss:0.032, val_acc:0.970]
Epoch [117/120    avg_loss:0.036, val_acc:0.965]
Epoch [118/120    avg_loss:0.031, val_acc:0.968]
Epoch [119/120    avg_loss:0.032, val_acc:0.968]
Epoch [120/120    avg_loss:0.025, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1224    6    0    0    3    0    0    0   14   38    0    0
     0    0    0]
 [   0    0    2  731    0    1    0    0    0    6    0    0    3    4
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  417    0    0    0    6    0    0    0    0
    12    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   11    0    0    4    0
     0    0    0]
 [   0    0   17   90    0    8    0    0    0    0  747    0    0    0
     0   13    0]
 [   0    0   20    0    0    1    1    0    0    0    5 2181    1    1
     0    0    0]
 [   0    0    0   24    9    1    0    0    0    0   16   14  463    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    0    0    3    0    0    0
  1126    0    0]
 [   0    0    0    0    0    1   25    0    0    1    0    0    0    0
    97  223    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.87262872628726

F1 scores:
[       nan 0.93975904 0.96075353 0.91432145 0.97695853 0.95423341
 0.97622585 1.         0.99649942 0.52380952 0.89891697 0.98176907
 0.92139303 0.98666667 0.94860994 0.76500858 0.96      ]

Kappa:
0.9414758683696626
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4250c308d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.704, val_acc:0.428]
Epoch [2/120    avg_loss:2.360, val_acc:0.519]
Epoch [3/120    avg_loss:2.148, val_acc:0.515]
Epoch [4/120    avg_loss:1.962, val_acc:0.546]
Epoch [5/120    avg_loss:1.855, val_acc:0.552]
Epoch [6/120    avg_loss:1.735, val_acc:0.578]
Epoch [7/120    avg_loss:1.642, val_acc:0.611]
Epoch [8/120    avg_loss:1.531, val_acc:0.639]
Epoch [9/120    avg_loss:1.415, val_acc:0.670]
Epoch [10/120    avg_loss:1.263, val_acc:0.673]
Epoch [11/120    avg_loss:1.059, val_acc:0.670]
Epoch [12/120    avg_loss:0.946, val_acc:0.733]
Epoch [13/120    avg_loss:0.881, val_acc:0.714]
Epoch [14/120    avg_loss:0.836, val_acc:0.744]
Epoch [15/120    avg_loss:0.769, val_acc:0.764]
Epoch [16/120    avg_loss:0.725, val_acc:0.764]
Epoch [17/120    avg_loss:0.602, val_acc:0.802]
Epoch [18/120    avg_loss:0.540, val_acc:0.809]
Epoch [19/120    avg_loss:0.535, val_acc:0.779]
Epoch [20/120    avg_loss:0.594, val_acc:0.820]
Epoch [21/120    avg_loss:0.502, val_acc:0.809]
Epoch [22/120    avg_loss:0.510, val_acc:0.858]
Epoch [23/120    avg_loss:0.428, val_acc:0.858]
Epoch [24/120    avg_loss:0.374, val_acc:0.810]
Epoch [25/120    avg_loss:0.412, val_acc:0.863]
Epoch [26/120    avg_loss:0.359, val_acc:0.868]
Epoch [27/120    avg_loss:0.317, val_acc:0.868]
Epoch [28/120    avg_loss:0.304, val_acc:0.875]
Epoch [29/120    avg_loss:0.281, val_acc:0.872]
Epoch [30/120    avg_loss:0.269, val_acc:0.886]
Epoch [31/120    avg_loss:0.264, val_acc:0.864]
Epoch [32/120    avg_loss:0.277, val_acc:0.881]
Epoch [33/120    avg_loss:0.273, val_acc:0.855]
Epoch [34/120    avg_loss:0.303, val_acc:0.890]
Epoch [35/120    avg_loss:0.281, val_acc:0.899]
Epoch [36/120    avg_loss:0.229, val_acc:0.889]
Epoch [37/120    avg_loss:0.215, val_acc:0.887]
Epoch [38/120    avg_loss:0.191, val_acc:0.905]
Epoch [39/120    avg_loss:0.162, val_acc:0.887]
Epoch [40/120    avg_loss:0.288, val_acc:0.895]
Epoch [41/120    avg_loss:0.214, val_acc:0.892]
Epoch [42/120    avg_loss:0.222, val_acc:0.894]
Epoch [43/120    avg_loss:0.153, val_acc:0.917]
Epoch [44/120    avg_loss:0.146, val_acc:0.926]
Epoch [45/120    avg_loss:0.153, val_acc:0.923]
Epoch [46/120    avg_loss:0.119, val_acc:0.928]
Epoch [47/120    avg_loss:0.143, val_acc:0.910]
Epoch [48/120    avg_loss:0.157, val_acc:0.919]
Epoch [49/120    avg_loss:0.153, val_acc:0.921]
Epoch [50/120    avg_loss:0.151, val_acc:0.913]
Epoch [51/120    avg_loss:0.131, val_acc:0.909]
Epoch [52/120    avg_loss:0.101, val_acc:0.937]
Epoch [53/120    avg_loss:0.099, val_acc:0.944]
Epoch [54/120    avg_loss:0.112, val_acc:0.918]
Epoch [55/120    avg_loss:0.114, val_acc:0.925]
Epoch [56/120    avg_loss:0.131, val_acc:0.929]
Epoch [57/120    avg_loss:0.106, val_acc:0.936]
Epoch [58/120    avg_loss:0.105, val_acc:0.929]
Epoch [59/120    avg_loss:0.105, val_acc:0.931]
Epoch [60/120    avg_loss:0.144, val_acc:0.920]
Epoch [61/120    avg_loss:0.156, val_acc:0.936]
Epoch [62/120    avg_loss:0.128, val_acc:0.934]
Epoch [63/120    avg_loss:0.093, val_acc:0.941]
Epoch [64/120    avg_loss:0.120, val_acc:0.929]
Epoch [65/120    avg_loss:0.143, val_acc:0.935]
Epoch [66/120    avg_loss:0.090, val_acc:0.935]
Epoch [67/120    avg_loss:0.077, val_acc:0.945]
Epoch [68/120    avg_loss:0.064, val_acc:0.945]
Epoch [69/120    avg_loss:0.057, val_acc:0.953]
Epoch [70/120    avg_loss:0.052, val_acc:0.952]
Epoch [71/120    avg_loss:0.049, val_acc:0.952]
Epoch [72/120    avg_loss:0.046, val_acc:0.949]
Epoch [73/120    avg_loss:0.057, val_acc:0.950]
Epoch [74/120    avg_loss:0.058, val_acc:0.954]
Epoch [75/120    avg_loss:0.050, val_acc:0.956]
Epoch [76/120    avg_loss:0.045, val_acc:0.955]
Epoch [77/120    avg_loss:0.045, val_acc:0.955]
Epoch [78/120    avg_loss:0.041, val_acc:0.955]
Epoch [79/120    avg_loss:0.050, val_acc:0.957]
Epoch [80/120    avg_loss:0.045, val_acc:0.956]
Epoch [81/120    avg_loss:0.044, val_acc:0.956]
Epoch [82/120    avg_loss:0.042, val_acc:0.958]
Epoch [83/120    avg_loss:0.042, val_acc:0.956]
Epoch [84/120    avg_loss:0.044, val_acc:0.957]
Epoch [85/120    avg_loss:0.041, val_acc:0.959]
Epoch [86/120    avg_loss:0.042, val_acc:0.959]
Epoch [87/120    avg_loss:0.046, val_acc:0.958]
Epoch [88/120    avg_loss:0.041, val_acc:0.958]
Epoch [89/120    avg_loss:0.048, val_acc:0.959]
Epoch [90/120    avg_loss:0.038, val_acc:0.958]
Epoch [91/120    avg_loss:0.040, val_acc:0.956]
Epoch [92/120    avg_loss:0.042, val_acc:0.958]
Epoch [93/120    avg_loss:0.040, val_acc:0.958]
Epoch [94/120    avg_loss:0.038, val_acc:0.955]
Epoch [95/120    avg_loss:0.042, val_acc:0.961]
Epoch [96/120    avg_loss:0.038, val_acc:0.958]
Epoch [97/120    avg_loss:0.041, val_acc:0.957]
Epoch [98/120    avg_loss:0.043, val_acc:0.961]
Epoch [99/120    avg_loss:0.039, val_acc:0.957]
Epoch [100/120    avg_loss:0.042, val_acc:0.961]
Epoch [101/120    avg_loss:0.040, val_acc:0.961]
Epoch [102/120    avg_loss:0.037, val_acc:0.958]
Epoch [103/120    avg_loss:0.043, val_acc:0.958]
Epoch [104/120    avg_loss:0.043, val_acc:0.962]
Epoch [105/120    avg_loss:0.040, val_acc:0.959]
Epoch [106/120    avg_loss:0.041, val_acc:0.961]
Epoch [107/120    avg_loss:0.038, val_acc:0.961]
Epoch [108/120    avg_loss:0.043, val_acc:0.962]
Epoch [109/120    avg_loss:0.043, val_acc:0.961]
Epoch [110/120    avg_loss:0.037, val_acc:0.962]
Epoch [111/120    avg_loss:0.036, val_acc:0.961]
Epoch [112/120    avg_loss:0.036, val_acc:0.963]
Epoch [113/120    avg_loss:0.039, val_acc:0.963]
Epoch [114/120    avg_loss:0.035, val_acc:0.962]
Epoch [115/120    avg_loss:0.037, val_acc:0.963]
Epoch [116/120    avg_loss:0.037, val_acc:0.961]
Epoch [117/120    avg_loss:0.036, val_acc:0.963]
Epoch [118/120    avg_loss:0.038, val_acc:0.957]
Epoch [119/120    avg_loss:0.035, val_acc:0.958]
Epoch [120/120    avg_loss:0.035, val_acc:0.962]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1193    2    0    0    2    0    0    0   14   72    1    0
     0    1    0]
 [   0    0    2  707    2    7    0    0    0    9    0    0   18    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    4    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    5    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   44   90    0    4    0    0    0    0  723    6    2    0
     0    6    0]
 [   0    0   12    0    0    0   10    0    0    0   10 2172    3    3
     0    0    0]
 [   0    0    0    8    1    6    0    0    0    0   13    2  496    0
     0    0    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    1    0    0
  1134    0    0]
 [   0    0    0    0    0    0   20    0    0    2    0    0    1    0
   113  211    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.3089430894309

F1 scores:
[       nan 0.94871795 0.94085174 0.90990991 0.99300699 0.97155859
 0.97164179 0.92592593 0.995338   0.70833333 0.88063337 0.97224709
 0.93496701 0.98666667 0.94935119 0.74690265 0.94252874]

Kappa:
0.9350184154238337
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f269f6cd8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.740, val_acc:0.367]
Epoch [2/120    avg_loss:2.433, val_acc:0.343]
Epoch [3/120    avg_loss:2.211, val_acc:0.524]
Epoch [4/120    avg_loss:2.045, val_acc:0.543]
Epoch [5/120    avg_loss:1.903, val_acc:0.631]
Epoch [6/120    avg_loss:1.814, val_acc:0.627]
Epoch [7/120    avg_loss:1.693, val_acc:0.668]
Epoch [8/120    avg_loss:1.546, val_acc:0.667]
Epoch [9/120    avg_loss:1.399, val_acc:0.677]
Epoch [10/120    avg_loss:1.233, val_acc:0.673]
Epoch [11/120    avg_loss:1.151, val_acc:0.697]
Epoch [12/120    avg_loss:0.996, val_acc:0.727]
Epoch [13/120    avg_loss:0.949, val_acc:0.691]
Epoch [14/120    avg_loss:0.844, val_acc:0.771]
Epoch [15/120    avg_loss:0.766, val_acc:0.742]
Epoch [16/120    avg_loss:0.702, val_acc:0.784]
Epoch [17/120    avg_loss:0.618, val_acc:0.825]
Epoch [18/120    avg_loss:0.544, val_acc:0.830]
Epoch [19/120    avg_loss:0.546, val_acc:0.800]
Epoch [20/120    avg_loss:0.529, val_acc:0.818]
Epoch [21/120    avg_loss:0.459, val_acc:0.855]
Epoch [22/120    avg_loss:0.406, val_acc:0.859]
Epoch [23/120    avg_loss:0.368, val_acc:0.866]
Epoch [24/120    avg_loss:0.353, val_acc:0.830]
Epoch [25/120    avg_loss:0.516, val_acc:0.811]
Epoch [26/120    avg_loss:0.450, val_acc:0.839]
Epoch [27/120    avg_loss:0.432, val_acc:0.884]
Epoch [28/120    avg_loss:0.356, val_acc:0.845]
Epoch [29/120    avg_loss:0.329, val_acc:0.887]
Epoch [30/120    avg_loss:0.267, val_acc:0.892]
Epoch [31/120    avg_loss:0.271, val_acc:0.885]
Epoch [32/120    avg_loss:0.288, val_acc:0.903]
Epoch [33/120    avg_loss:0.298, val_acc:0.908]
Epoch [34/120    avg_loss:0.245, val_acc:0.909]
Epoch [35/120    avg_loss:0.294, val_acc:0.901]
Epoch [36/120    avg_loss:0.235, val_acc:0.900]
Epoch [37/120    avg_loss:0.253, val_acc:0.921]
Epoch [38/120    avg_loss:0.197, val_acc:0.909]
Epoch [39/120    avg_loss:0.211, val_acc:0.918]
Epoch [40/120    avg_loss:0.177, val_acc:0.920]
Epoch [41/120    avg_loss:0.211, val_acc:0.916]
Epoch [42/120    avg_loss:0.154, val_acc:0.930]
Epoch [43/120    avg_loss:0.166, val_acc:0.935]
Epoch [44/120    avg_loss:0.140, val_acc:0.932]
Epoch [45/120    avg_loss:0.146, val_acc:0.904]
Epoch [46/120    avg_loss:0.144, val_acc:0.941]
Epoch [47/120    avg_loss:0.143, val_acc:0.928]
Epoch [48/120    avg_loss:0.156, val_acc:0.901]
Epoch [49/120    avg_loss:0.154, val_acc:0.919]
Epoch [50/120    avg_loss:0.194, val_acc:0.930]
Epoch [51/120    avg_loss:0.151, val_acc:0.930]
Epoch [52/120    avg_loss:0.137, val_acc:0.934]
Epoch [53/120    avg_loss:0.148, val_acc:0.936]
Epoch [54/120    avg_loss:0.131, val_acc:0.925]
Epoch [55/120    avg_loss:0.117, val_acc:0.939]
Epoch [56/120    avg_loss:0.102, val_acc:0.945]
Epoch [57/120    avg_loss:0.118, val_acc:0.947]
Epoch [58/120    avg_loss:0.111, val_acc:0.955]
Epoch [59/120    avg_loss:0.089, val_acc:0.952]
Epoch [60/120    avg_loss:0.083, val_acc:0.952]
Epoch [61/120    avg_loss:0.190, val_acc:0.937]
Epoch [62/120    avg_loss:0.110, val_acc:0.946]
Epoch [63/120    avg_loss:0.109, val_acc:0.949]
Epoch [64/120    avg_loss:0.125, val_acc:0.947]
Epoch [65/120    avg_loss:0.103, val_acc:0.955]
Epoch [66/120    avg_loss:0.084, val_acc:0.953]
Epoch [67/120    avg_loss:0.071, val_acc:0.947]
Epoch [68/120    avg_loss:0.079, val_acc:0.954]
Epoch [69/120    avg_loss:0.074, val_acc:0.952]
Epoch [70/120    avg_loss:0.065, val_acc:0.954]
Epoch [71/120    avg_loss:0.057, val_acc:0.949]
Epoch [72/120    avg_loss:0.066, val_acc:0.959]
Epoch [73/120    avg_loss:0.067, val_acc:0.950]
Epoch [74/120    avg_loss:0.077, val_acc:0.952]
Epoch [75/120    avg_loss:0.063, val_acc:0.970]
Epoch [76/120    avg_loss:0.054, val_acc:0.949]
Epoch [77/120    avg_loss:0.048, val_acc:0.958]
Epoch [78/120    avg_loss:0.106, val_acc:0.927]
Epoch [79/120    avg_loss:0.114, val_acc:0.953]
Epoch [80/120    avg_loss:0.091, val_acc:0.950]
Epoch [81/120    avg_loss:0.092, val_acc:0.940]
Epoch [82/120    avg_loss:0.082, val_acc:0.949]
Epoch [83/120    avg_loss:0.072, val_acc:0.957]
Epoch [84/120    avg_loss:0.055, val_acc:0.966]
Epoch [85/120    avg_loss:0.047, val_acc:0.963]
Epoch [86/120    avg_loss:0.040, val_acc:0.971]
Epoch [87/120    avg_loss:0.056, val_acc:0.957]
Epoch [88/120    avg_loss:0.055, val_acc:0.964]
Epoch [89/120    avg_loss:0.042, val_acc:0.966]
Epoch [90/120    avg_loss:0.048, val_acc:0.972]
Epoch [91/120    avg_loss:0.045, val_acc:0.971]
Epoch [92/120    avg_loss:0.039, val_acc:0.971]
Epoch [93/120    avg_loss:0.035, val_acc:0.973]
Epoch [94/120    avg_loss:0.042, val_acc:0.963]
Epoch [95/120    avg_loss:0.035, val_acc:0.975]
Epoch [96/120    avg_loss:0.042, val_acc:0.964]
Epoch [97/120    avg_loss:0.053, val_acc:0.971]
Epoch [98/120    avg_loss:0.060, val_acc:0.950]
Epoch [99/120    avg_loss:0.049, val_acc:0.968]
Epoch [100/120    avg_loss:0.032, val_acc:0.973]
Epoch [101/120    avg_loss:0.029, val_acc:0.973]
Epoch [102/120    avg_loss:0.027, val_acc:0.970]
Epoch [103/120    avg_loss:0.026, val_acc:0.970]
Epoch [104/120    avg_loss:0.026, val_acc:0.974]
Epoch [105/120    avg_loss:0.028, val_acc:0.974]
Epoch [106/120    avg_loss:0.030, val_acc:0.966]
Epoch [107/120    avg_loss:0.035, val_acc:0.970]
Epoch [108/120    avg_loss:0.046, val_acc:0.965]
Epoch [109/120    avg_loss:0.033, val_acc:0.968]
Epoch [110/120    avg_loss:0.022, val_acc:0.976]
Epoch [111/120    avg_loss:0.025, val_acc:0.976]
Epoch [112/120    avg_loss:0.022, val_acc:0.976]
Epoch [113/120    avg_loss:0.015, val_acc:0.979]
Epoch [114/120    avg_loss:0.017, val_acc:0.980]
Epoch [115/120    avg_loss:0.020, val_acc:0.981]
Epoch [116/120    avg_loss:0.025, val_acc:0.981]
Epoch [117/120    avg_loss:0.018, val_acc:0.979]
Epoch [118/120    avg_loss:0.019, val_acc:0.977]
Epoch [119/120    avg_loss:0.016, val_acc:0.979]
Epoch [120/120    avg_loss:0.020, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1239    2    0    1    0    0    0    0    3   37    2    0
     0    1    0]
 [   0    0    0  707    8    8    0    0    0   10    0    0   13    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  416    0    5    0    6    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   12    0    0    5    0
     0    0    0]
 [   0    0   22   90    0    2    0    0    0    0  758    3    0    0
     0    0    0]
 [   0    0   14    0    0    0   11    0    1    0    4 2176    0    4
     0    0    0]
 [   0    0    0   35    7    6    0    0    0    0    9    0  476    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    4    2    0    0
  1133    0    0]
 [   0    0    0    0    0    0   11    0    0    2    0    0    0    5
    48  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.69647696476964

F1 scores:
[       nan 0.98765432 0.96796875 0.89437065 0.96598639 0.95852535
 0.98127341 0.90909091 0.99883856 0.5        0.9165659  0.98239278
 0.92248062 0.9762533  0.97294976 0.89348172 0.98203593]

Kappa:
0.950919503549939
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0f73182898>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.662, val_acc:0.328]
Epoch [2/120    avg_loss:2.342, val_acc:0.413]
Epoch [3/120    avg_loss:2.163, val_acc:0.518]
Epoch [4/120    avg_loss:2.027, val_acc:0.589]
Epoch [5/120    avg_loss:1.879, val_acc:0.587]
Epoch [6/120    avg_loss:1.723, val_acc:0.620]
Epoch [7/120    avg_loss:1.632, val_acc:0.641]
Epoch [8/120    avg_loss:1.549, val_acc:0.640]
Epoch [9/120    avg_loss:1.354, val_acc:0.684]
Epoch [10/120    avg_loss:1.250, val_acc:0.686]
Epoch [11/120    avg_loss:1.143, val_acc:0.668]
Epoch [12/120    avg_loss:0.988, val_acc:0.720]
Epoch [13/120    avg_loss:0.923, val_acc:0.713]
Epoch [14/120    avg_loss:0.854, val_acc:0.732]
Epoch [15/120    avg_loss:0.763, val_acc:0.739]
Epoch [16/120    avg_loss:0.678, val_acc:0.774]
Epoch [17/120    avg_loss:0.624, val_acc:0.825]
Epoch [18/120    avg_loss:0.615, val_acc:0.802]
Epoch [19/120    avg_loss:0.595, val_acc:0.782]
Epoch [20/120    avg_loss:0.530, val_acc:0.843]
Epoch [21/120    avg_loss:0.512, val_acc:0.823]
Epoch [22/120    avg_loss:0.461, val_acc:0.839]
Epoch [23/120    avg_loss:0.497, val_acc:0.813]
Epoch [24/120    avg_loss:0.510, val_acc:0.831]
Epoch [25/120    avg_loss:0.441, val_acc:0.856]
Epoch [26/120    avg_loss:0.370, val_acc:0.824]
Epoch [27/120    avg_loss:0.419, val_acc:0.823]
Epoch [28/120    avg_loss:0.337, val_acc:0.872]
Epoch [29/120    avg_loss:0.299, val_acc:0.881]
Epoch [30/120    avg_loss:0.257, val_acc:0.868]
Epoch [31/120    avg_loss:0.291, val_acc:0.900]
Epoch [32/120    avg_loss:0.241, val_acc:0.898]
Epoch [33/120    avg_loss:0.248, val_acc:0.895]
Epoch [34/120    avg_loss:0.226, val_acc:0.912]
Epoch [35/120    avg_loss:0.193, val_acc:0.914]
Epoch [36/120    avg_loss:0.187, val_acc:0.909]
Epoch [37/120    avg_loss:0.275, val_acc:0.901]
Epoch [38/120    avg_loss:0.288, val_acc:0.925]
Epoch [39/120    avg_loss:0.187, val_acc:0.926]
Epoch [40/120    avg_loss:0.210, val_acc:0.907]
Epoch [41/120    avg_loss:0.196, val_acc:0.895]
Epoch [42/120    avg_loss:0.349, val_acc:0.917]
Epoch [43/120    avg_loss:0.201, val_acc:0.926]
Epoch [44/120    avg_loss:0.168, val_acc:0.917]
Epoch [45/120    avg_loss:0.155, val_acc:0.922]
Epoch [46/120    avg_loss:0.156, val_acc:0.935]
Epoch [47/120    avg_loss:0.157, val_acc:0.927]
Epoch [48/120    avg_loss:0.177, val_acc:0.918]
Epoch [49/120    avg_loss:0.142, val_acc:0.932]
Epoch [50/120    avg_loss:0.119, val_acc:0.935]
Epoch [51/120    avg_loss:0.112, val_acc:0.944]
Epoch [52/120    avg_loss:0.129, val_acc:0.930]
Epoch [53/120    avg_loss:0.110, val_acc:0.946]
Epoch [54/120    avg_loss:0.083, val_acc:0.953]
Epoch [55/120    avg_loss:0.098, val_acc:0.945]
Epoch [56/120    avg_loss:0.144, val_acc:0.937]
Epoch [57/120    avg_loss:0.127, val_acc:0.920]
Epoch [58/120    avg_loss:0.110, val_acc:0.947]
Epoch [59/120    avg_loss:0.163, val_acc:0.925]
Epoch [60/120    avg_loss:0.108, val_acc:0.936]
Epoch [61/120    avg_loss:0.097, val_acc:0.953]
Epoch [62/120    avg_loss:0.161, val_acc:0.943]
Epoch [63/120    avg_loss:0.101, val_acc:0.945]
Epoch [64/120    avg_loss:0.089, val_acc:0.944]
Epoch [65/120    avg_loss:0.077, val_acc:0.966]
Epoch [66/120    avg_loss:0.082, val_acc:0.963]
Epoch [67/120    avg_loss:0.063, val_acc:0.975]
Epoch [68/120    avg_loss:0.097, val_acc:0.943]
Epoch [69/120    avg_loss:0.070, val_acc:0.964]
Epoch [70/120    avg_loss:0.050, val_acc:0.972]
Epoch [71/120    avg_loss:0.055, val_acc:0.964]
Epoch [72/120    avg_loss:0.070, val_acc:0.953]
Epoch [73/120    avg_loss:0.061, val_acc:0.967]
Epoch [74/120    avg_loss:0.066, val_acc:0.959]
Epoch [75/120    avg_loss:0.064, val_acc:0.955]
Epoch [76/120    avg_loss:0.101, val_acc:0.938]
Epoch [77/120    avg_loss:0.073, val_acc:0.964]
Epoch [78/120    avg_loss:0.079, val_acc:0.961]
Epoch [79/120    avg_loss:0.066, val_acc:0.970]
Epoch [80/120    avg_loss:0.058, val_acc:0.971]
Epoch [81/120    avg_loss:0.038, val_acc:0.973]
Epoch [82/120    avg_loss:0.037, val_acc:0.974]
Epoch [83/120    avg_loss:0.031, val_acc:0.973]
Epoch [84/120    avg_loss:0.039, val_acc:0.973]
Epoch [85/120    avg_loss:0.032, val_acc:0.974]
Epoch [86/120    avg_loss:0.036, val_acc:0.974]
Epoch [87/120    avg_loss:0.033, val_acc:0.975]
Epoch [88/120    avg_loss:0.037, val_acc:0.975]
Epoch [89/120    avg_loss:0.027, val_acc:0.977]
Epoch [90/120    avg_loss:0.030, val_acc:0.977]
Epoch [91/120    avg_loss:0.032, val_acc:0.977]
Epoch [92/120    avg_loss:0.031, val_acc:0.977]
Epoch [93/120    avg_loss:0.030, val_acc:0.977]
Epoch [94/120    avg_loss:0.026, val_acc:0.981]
Epoch [95/120    avg_loss:0.030, val_acc:0.980]
Epoch [96/120    avg_loss:0.029, val_acc:0.976]
Epoch [97/120    avg_loss:0.034, val_acc:0.979]
Epoch [98/120    avg_loss:0.029, val_acc:0.979]
Epoch [99/120    avg_loss:0.029, val_acc:0.979]
Epoch [100/120    avg_loss:0.031, val_acc:0.979]
Epoch [101/120    avg_loss:0.026, val_acc:0.977]
Epoch [102/120    avg_loss:0.030, val_acc:0.976]
Epoch [103/120    avg_loss:0.028, val_acc:0.977]
Epoch [104/120    avg_loss:0.030, val_acc:0.977]
Epoch [105/120    avg_loss:0.028, val_acc:0.976]
Epoch [106/120    avg_loss:0.033, val_acc:0.979]
Epoch [107/120    avg_loss:0.027, val_acc:0.977]
Epoch [108/120    avg_loss:0.025, val_acc:0.979]
Epoch [109/120    avg_loss:0.035, val_acc:0.980]
Epoch [110/120    avg_loss:0.029, val_acc:0.979]
Epoch [111/120    avg_loss:0.030, val_acc:0.979]
Epoch [112/120    avg_loss:0.026, val_acc:0.977]
Epoch [113/120    avg_loss:0.030, val_acc:0.977]
Epoch [114/120    avg_loss:0.027, val_acc:0.977]
Epoch [115/120    avg_loss:0.036, val_acc:0.977]
Epoch [116/120    avg_loss:0.029, val_acc:0.977]
Epoch [117/120    avg_loss:0.024, val_acc:0.976]
Epoch [118/120    avg_loss:0.024, val_acc:0.976]
Epoch [119/120    avg_loss:0.027, val_acc:0.976]
Epoch [120/120    avg_loss:0.024, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1232    4    1    0    2    0    0    0    4   28    6    0
     0    8    0]
 [   0    0    0  706    0   20    0    0    0    6    0    0   13    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  416    0    0    0    4    0    0    0    0
    15    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    4    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   14    0    0    3    0
     0    0    0]
 [   0    0   30   90    0    5    0    0    0    0  737    1    0    0
     2   10    0]
 [   0    0   28    0    0    2   10    0    0    0   15 2152    0    3
     0    0    0]
 [   0    0    4    7    2    7    0    0    0    0   19    0  488    0
     1    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    3    1    0    0
  1131    0    0]
 [   0    0    0    0    0    0    4    0    0    7    0    0    0    0
    85  251    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.88346883468834

F1 scores:
[       nan 0.98765432 0.95540907 0.90803859 0.99300699 0.93693694
 0.98338369 1.         0.99883856 0.57142857 0.89117291 0.97907188
 0.93307839 0.98666667 0.95242105 0.81493506 0.95348837]

Kappa:
0.9416601118814939
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9bb6966898>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.684, val_acc:0.227]
Epoch [2/120    avg_loss:2.393, val_acc:0.381]
Epoch [3/120    avg_loss:2.161, val_acc:0.464]
Epoch [4/120    avg_loss:2.006, val_acc:0.546]
Epoch [5/120    avg_loss:1.910, val_acc:0.574]
Epoch [6/120    avg_loss:1.805, val_acc:0.593]
Epoch [7/120    avg_loss:1.725, val_acc:0.644]
Epoch [8/120    avg_loss:1.534, val_acc:0.655]
Epoch [9/120    avg_loss:1.341, val_acc:0.670]
Epoch [10/120    avg_loss:1.258, val_acc:0.694]
Epoch [11/120    avg_loss:1.095, val_acc:0.679]
Epoch [12/120    avg_loss:1.010, val_acc:0.718]
Epoch [13/120    avg_loss:0.924, val_acc:0.736]
Epoch [14/120    avg_loss:0.844, val_acc:0.766]
Epoch [15/120    avg_loss:0.775, val_acc:0.753]
Epoch [16/120    avg_loss:0.702, val_acc:0.779]
Epoch [17/120    avg_loss:0.680, val_acc:0.770]
Epoch [18/120    avg_loss:0.626, val_acc:0.759]
Epoch [19/120    avg_loss:0.621, val_acc:0.777]
Epoch [20/120    avg_loss:0.516, val_acc:0.822]
Epoch [21/120    avg_loss:0.565, val_acc:0.814]
Epoch [22/120    avg_loss:0.457, val_acc:0.832]
Epoch [23/120    avg_loss:0.460, val_acc:0.822]
Epoch [24/120    avg_loss:0.427, val_acc:0.825]
Epoch [25/120    avg_loss:0.358, val_acc:0.875]
Epoch [26/120    avg_loss:0.369, val_acc:0.831]
Epoch [27/120    avg_loss:0.367, val_acc:0.840]
Epoch [28/120    avg_loss:0.360, val_acc:0.839]
Epoch [29/120    avg_loss:0.373, val_acc:0.878]
Epoch [30/120    avg_loss:0.436, val_acc:0.837]
Epoch [31/120    avg_loss:0.355, val_acc:0.846]
Epoch [32/120    avg_loss:0.297, val_acc:0.893]
Epoch [33/120    avg_loss:0.284, val_acc:0.865]
Epoch [34/120    avg_loss:0.310, val_acc:0.896]
Epoch [35/120    avg_loss:0.241, val_acc:0.907]
Epoch [36/120    avg_loss:0.227, val_acc:0.889]
Epoch [37/120    avg_loss:0.215, val_acc:0.912]
Epoch [38/120    avg_loss:0.202, val_acc:0.894]
Epoch [39/120    avg_loss:0.180, val_acc:0.909]
Epoch [40/120    avg_loss:0.178, val_acc:0.912]
Epoch [41/120    avg_loss:0.225, val_acc:0.909]
Epoch [42/120    avg_loss:0.252, val_acc:0.887]
Epoch [43/120    avg_loss:0.201, val_acc:0.934]
Epoch [44/120    avg_loss:0.159, val_acc:0.919]
Epoch [45/120    avg_loss:0.169, val_acc:0.902]
Epoch [46/120    avg_loss:0.184, val_acc:0.907]
Epoch [47/120    avg_loss:0.173, val_acc:0.911]
Epoch [48/120    avg_loss:0.147, val_acc:0.902]
Epoch [49/120    avg_loss:0.169, val_acc:0.932]
Epoch [50/120    avg_loss:0.271, val_acc:0.890]
Epoch [51/120    avg_loss:0.345, val_acc:0.833]
Epoch [52/120    avg_loss:0.271, val_acc:0.894]
Epoch [53/120    avg_loss:0.196, val_acc:0.890]
Epoch [54/120    avg_loss:0.184, val_acc:0.911]
Epoch [55/120    avg_loss:0.176, val_acc:0.899]
Epoch [56/120    avg_loss:0.135, val_acc:0.929]
Epoch [57/120    avg_loss:0.113, val_acc:0.931]
Epoch [58/120    avg_loss:0.099, val_acc:0.936]
Epoch [59/120    avg_loss:0.099, val_acc:0.935]
Epoch [60/120    avg_loss:0.099, val_acc:0.940]
Epoch [61/120    avg_loss:0.097, val_acc:0.940]
Epoch [62/120    avg_loss:0.092, val_acc:0.939]
Epoch [63/120    avg_loss:0.086, val_acc:0.943]
Epoch [64/120    avg_loss:0.095, val_acc:0.939]
Epoch [65/120    avg_loss:0.079, val_acc:0.943]
Epoch [66/120    avg_loss:0.080, val_acc:0.944]
Epoch [67/120    avg_loss:0.090, val_acc:0.944]
Epoch [68/120    avg_loss:0.077, val_acc:0.943]
Epoch [69/120    avg_loss:0.089, val_acc:0.941]
Epoch [70/120    avg_loss:0.077, val_acc:0.946]
Epoch [71/120    avg_loss:0.078, val_acc:0.948]
Epoch [72/120    avg_loss:0.074, val_acc:0.948]
Epoch [73/120    avg_loss:0.082, val_acc:0.947]
Epoch [74/120    avg_loss:0.078, val_acc:0.946]
Epoch [75/120    avg_loss:0.070, val_acc:0.947]
Epoch [76/120    avg_loss:0.078, val_acc:0.954]
Epoch [77/120    avg_loss:0.078, val_acc:0.952]
Epoch [78/120    avg_loss:0.080, val_acc:0.947]
Epoch [79/120    avg_loss:0.075, val_acc:0.953]
Epoch [80/120    avg_loss:0.075, val_acc:0.949]
Epoch [81/120    avg_loss:0.083, val_acc:0.948]
Epoch [82/120    avg_loss:0.072, val_acc:0.952]
Epoch [83/120    avg_loss:0.074, val_acc:0.949]
Epoch [84/120    avg_loss:0.076, val_acc:0.949]
Epoch [85/120    avg_loss:0.072, val_acc:0.953]
Epoch [86/120    avg_loss:0.079, val_acc:0.952]
Epoch [87/120    avg_loss:0.069, val_acc:0.950]
Epoch [88/120    avg_loss:0.082, val_acc:0.954]
Epoch [89/120    avg_loss:0.065, val_acc:0.953]
Epoch [90/120    avg_loss:0.067, val_acc:0.952]
Epoch [91/120    avg_loss:0.079, val_acc:0.953]
Epoch [92/120    avg_loss:0.066, val_acc:0.948]
Epoch [93/120    avg_loss:0.067, val_acc:0.954]
Epoch [94/120    avg_loss:0.068, val_acc:0.950]
Epoch [95/120    avg_loss:0.063, val_acc:0.952]
Epoch [96/120    avg_loss:0.069, val_acc:0.950]
Epoch [97/120    avg_loss:0.073, val_acc:0.946]
Epoch [98/120    avg_loss:0.062, val_acc:0.950]
Epoch [99/120    avg_loss:0.056, val_acc:0.952]
Epoch [100/120    avg_loss:0.067, val_acc:0.953]
Epoch [101/120    avg_loss:0.060, val_acc:0.953]
Epoch [102/120    avg_loss:0.071, val_acc:0.953]
Epoch [103/120    avg_loss:0.061, val_acc:0.952]
Epoch [104/120    avg_loss:0.059, val_acc:0.952]
Epoch [105/120    avg_loss:0.065, val_acc:0.950]
Epoch [106/120    avg_loss:0.068, val_acc:0.961]
Epoch [107/120    avg_loss:0.060, val_acc:0.958]
Epoch [108/120    avg_loss:0.059, val_acc:0.957]
Epoch [109/120    avg_loss:0.065, val_acc:0.953]
Epoch [110/120    avg_loss:0.055, val_acc:0.957]
Epoch [111/120    avg_loss:0.060, val_acc:0.958]
Epoch [112/120    avg_loss:0.058, val_acc:0.952]
Epoch [113/120    avg_loss:0.061, val_acc:0.956]
Epoch [114/120    avg_loss:0.059, val_acc:0.955]
Epoch [115/120    avg_loss:0.053, val_acc:0.957]
Epoch [116/120    avg_loss:0.073, val_acc:0.957]
Epoch [117/120    avg_loss:0.062, val_acc:0.958]
Epoch [118/120    avg_loss:0.053, val_acc:0.953]
Epoch [119/120    avg_loss:0.060, val_acc:0.956]
Epoch [120/120    avg_loss:0.060, val_acc:0.957]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    2 1187    2    0    0    4    0    0    6    2   73    0    0
     0    9    0]
 [   0    0    0  672    0   14    0    0    0   20    0    0   39    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    4    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  647    0    0    3    0    2    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    5    0    0   11    0    0    1    0
     0    0    0]
 [   0    0   34   88    0    8    2    0    0    0  728    5    0    0
     0   10    0]
 [   0    0   54    0    0    1   16    0    1    0   10 2117    3    5
     3    0    0]
 [   0    0    1   22    4   11    0    0    0    0    4    0  478    0
     0    2   12]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    1    6    1    1    0    0
  1126    0    0]
 [   0    0    0    0    0    0   34    0    0   11    0    0    0    0
   117  185    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
92.72628726287263

F1 scores:
[       nan 0.95121951 0.92698165 0.8772846  0.99069767 0.94888889
 0.94798535 1.         0.99767981 0.27848101 0.89765721 0.96052632
 0.90616114 0.98143236 0.94068505 0.66907776 0.93333333]

Kappa:
0.917066513085912
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7fd7947898>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.719, val_acc:0.377]
Epoch [2/120    avg_loss:2.403, val_acc:0.492]
Epoch [3/120    avg_loss:2.182, val_acc:0.503]
Epoch [4/120    avg_loss:2.067, val_acc:0.547]
Epoch [5/120    avg_loss:1.910, val_acc:0.547]
Epoch [6/120    avg_loss:1.830, val_acc:0.574]
Epoch [7/120    avg_loss:1.694, val_acc:0.605]
Epoch [8/120    avg_loss:1.577, val_acc:0.619]
Epoch [9/120    avg_loss:1.490, val_acc:0.628]
Epoch [10/120    avg_loss:1.342, val_acc:0.660]
Epoch [11/120    avg_loss:1.233, val_acc:0.677]
Epoch [12/120    avg_loss:1.047, val_acc:0.713]
Epoch [13/120    avg_loss:0.969, val_acc:0.739]
Epoch [14/120    avg_loss:0.890, val_acc:0.758]
Epoch [15/120    avg_loss:0.808, val_acc:0.752]
Epoch [16/120    avg_loss:0.737, val_acc:0.749]
Epoch [17/120    avg_loss:0.663, val_acc:0.828]
Epoch [18/120    avg_loss:0.623, val_acc:0.782]
Epoch [19/120    avg_loss:0.552, val_acc:0.819]
Epoch [20/120    avg_loss:0.497, val_acc:0.811]
Epoch [21/120    avg_loss:0.457, val_acc:0.840]
Epoch [22/120    avg_loss:0.403, val_acc:0.837]
Epoch [23/120    avg_loss:0.369, val_acc:0.827]
Epoch [24/120    avg_loss:0.335, val_acc:0.850]
Epoch [25/120    avg_loss:0.434, val_acc:0.836]
Epoch [26/120    avg_loss:0.347, val_acc:0.861]
Epoch [27/120    avg_loss:0.313, val_acc:0.878]
Epoch [28/120    avg_loss:0.312, val_acc:0.882]
Epoch [29/120    avg_loss:0.295, val_acc:0.839]
Epoch [30/120    avg_loss:0.313, val_acc:0.861]
Epoch [31/120    avg_loss:0.296, val_acc:0.878]
Epoch [32/120    avg_loss:0.264, val_acc:0.889]
Epoch [33/120    avg_loss:0.296, val_acc:0.883]
Epoch [34/120    avg_loss:0.276, val_acc:0.842]
Epoch [35/120    avg_loss:0.333, val_acc:0.885]
Epoch [36/120    avg_loss:0.285, val_acc:0.884]
Epoch [37/120    avg_loss:0.231, val_acc:0.894]
Epoch [38/120    avg_loss:0.213, val_acc:0.900]
Epoch [39/120    avg_loss:0.215, val_acc:0.900]
Epoch [40/120    avg_loss:0.205, val_acc:0.910]
Epoch [41/120    avg_loss:0.184, val_acc:0.903]
Epoch [42/120    avg_loss:0.218, val_acc:0.894]
Epoch [43/120    avg_loss:0.154, val_acc:0.917]
Epoch [44/120    avg_loss:0.158, val_acc:0.895]
Epoch [45/120    avg_loss:0.167, val_acc:0.905]
Epoch [46/120    avg_loss:0.169, val_acc:0.919]
Epoch [47/120    avg_loss:0.181, val_acc:0.919]
Epoch [48/120    avg_loss:0.142, val_acc:0.930]
Epoch [49/120    avg_loss:0.122, val_acc:0.921]
Epoch [50/120    avg_loss:0.146, val_acc:0.916]
Epoch [51/120    avg_loss:0.159, val_acc:0.920]
Epoch [52/120    avg_loss:0.119, val_acc:0.943]
Epoch [53/120    avg_loss:0.112, val_acc:0.934]
Epoch [54/120    avg_loss:0.109, val_acc:0.936]
Epoch [55/120    avg_loss:0.126, val_acc:0.940]
Epoch [56/120    avg_loss:0.094, val_acc:0.934]
Epoch [57/120    avg_loss:0.094, val_acc:0.935]
Epoch [58/120    avg_loss:0.111, val_acc:0.950]
Epoch [59/120    avg_loss:0.116, val_acc:0.935]
Epoch [60/120    avg_loss:0.096, val_acc:0.939]
Epoch [61/120    avg_loss:0.087, val_acc:0.947]
Epoch [62/120    avg_loss:0.103, val_acc:0.940]
Epoch [63/120    avg_loss:0.104, val_acc:0.931]
Epoch [64/120    avg_loss:0.075, val_acc:0.947]
Epoch [65/120    avg_loss:0.072, val_acc:0.959]
Epoch [66/120    avg_loss:0.077, val_acc:0.954]
Epoch [67/120    avg_loss:0.058, val_acc:0.947]
Epoch [68/120    avg_loss:0.053, val_acc:0.959]
Epoch [69/120    avg_loss:0.073, val_acc:0.944]
Epoch [70/120    avg_loss:0.069, val_acc:0.945]
Epoch [71/120    avg_loss:0.064, val_acc:0.955]
Epoch [72/120    avg_loss:0.051, val_acc:0.949]
Epoch [73/120    avg_loss:0.055, val_acc:0.953]
Epoch [74/120    avg_loss:0.052, val_acc:0.955]
Epoch [75/120    avg_loss:0.072, val_acc:0.941]
Epoch [76/120    avg_loss:0.077, val_acc:0.965]
Epoch [77/120    avg_loss:0.077, val_acc:0.939]
Epoch [78/120    avg_loss:0.085, val_acc:0.946]
Epoch [79/120    avg_loss:0.091, val_acc:0.955]
Epoch [80/120    avg_loss:0.091, val_acc:0.955]
Epoch [81/120    avg_loss:0.058, val_acc:0.962]
Epoch [82/120    avg_loss:0.038, val_acc:0.947]
Epoch [83/120    avg_loss:0.050, val_acc:0.962]
Epoch [84/120    avg_loss:0.050, val_acc:0.953]
Epoch [85/120    avg_loss:0.051, val_acc:0.955]
Epoch [86/120    avg_loss:0.062, val_acc:0.953]
Epoch [87/120    avg_loss:0.076, val_acc:0.946]
Epoch [88/120    avg_loss:0.103, val_acc:0.937]
Epoch [89/120    avg_loss:0.078, val_acc:0.952]
Epoch [90/120    avg_loss:0.055, val_acc:0.957]
Epoch [91/120    avg_loss:0.045, val_acc:0.961]
Epoch [92/120    avg_loss:0.032, val_acc:0.965]
Epoch [93/120    avg_loss:0.040, val_acc:0.963]
Epoch [94/120    avg_loss:0.038, val_acc:0.963]
Epoch [95/120    avg_loss:0.036, val_acc:0.965]
Epoch [96/120    avg_loss:0.031, val_acc:0.964]
Epoch [97/120    avg_loss:0.033, val_acc:0.965]
Epoch [98/120    avg_loss:0.040, val_acc:0.964]
Epoch [99/120    avg_loss:0.033, val_acc:0.966]
Epoch [100/120    avg_loss:0.035, val_acc:0.967]
Epoch [101/120    avg_loss:0.027, val_acc:0.968]
Epoch [102/120    avg_loss:0.026, val_acc:0.968]
Epoch [103/120    avg_loss:0.032, val_acc:0.968]
Epoch [104/120    avg_loss:0.029, val_acc:0.968]
Epoch [105/120    avg_loss:0.024, val_acc:0.968]
Epoch [106/120    avg_loss:0.032, val_acc:0.966]
Epoch [107/120    avg_loss:0.026, val_acc:0.967]
Epoch [108/120    avg_loss:0.029, val_acc:0.967]
Epoch [109/120    avg_loss:0.029, val_acc:0.968]
Epoch [110/120    avg_loss:0.026, val_acc:0.968]
Epoch [111/120    avg_loss:0.026, val_acc:0.967]
Epoch [112/120    avg_loss:0.022, val_acc:0.967]
Epoch [113/120    avg_loss:0.027, val_acc:0.967]
Epoch [114/120    avg_loss:0.023, val_acc:0.968]
Epoch [115/120    avg_loss:0.022, val_acc:0.966]
Epoch [116/120    avg_loss:0.024, val_acc:0.967]
Epoch [117/120    avg_loss:0.025, val_acc:0.970]
Epoch [118/120    avg_loss:0.022, val_acc:0.970]
Epoch [119/120    avg_loss:0.026, val_acc:0.968]
Epoch [120/120    avg_loss:0.023, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1244    0    0    0    7    0    0    0    3   13    1    0
     0   17    0]
 [   0    0    3  713    1    6    0    0    0   13    0    0   11    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    5    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   18   90    0    1    1    0    0    0  754    0    0    0
     0   11    0]
 [   0    0   19    0    0    3   13    0    1    0   19 2148    1    5
     1    0    0]
 [   0    0    0   24    6    0    0    0    0    0   18    1  479    0
     0    0    6]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    1    0    0
  1136    0    0]
 [   0    0    0    0    0    1    0    0    0    3    0    0    0    0
   123  220    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.07859078590786

F1 scores:
[       nan 0.96202532 0.96847022 0.90597205 0.98383372 0.97945205
 0.98426966 0.98039216 0.99767981 0.60714286 0.90137478 0.98239195
 0.93281402 0.98395722 0.94706128 0.7394958  0.96551724]

Kappa:
0.9438996842260081
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f366fa4c860>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.706, val_acc:0.412]
Epoch [2/120    avg_loss:2.424, val_acc:0.450]
Epoch [3/120    avg_loss:2.259, val_acc:0.476]
Epoch [4/120    avg_loss:2.080, val_acc:0.511]
Epoch [5/120    avg_loss:1.957, val_acc:0.575]
Epoch [6/120    avg_loss:1.831, val_acc:0.544]
Epoch [7/120    avg_loss:1.740, val_acc:0.622]
Epoch [8/120    avg_loss:1.591, val_acc:0.650]
Epoch [9/120    avg_loss:1.445, val_acc:0.633]
Epoch [10/120    avg_loss:1.288, val_acc:0.685]
Epoch [11/120    avg_loss:1.143, val_acc:0.699]
Epoch [12/120    avg_loss:1.115, val_acc:0.708]
Epoch [13/120    avg_loss:0.986, val_acc:0.741]
Epoch [14/120    avg_loss:0.926, val_acc:0.744]
Epoch [15/120    avg_loss:0.864, val_acc:0.774]
Epoch [16/120    avg_loss:0.787, val_acc:0.713]
Epoch [17/120    avg_loss:0.799, val_acc:0.750]
Epoch [18/120    avg_loss:0.726, val_acc:0.783]
Epoch [19/120    avg_loss:0.656, val_acc:0.791]
Epoch [20/120    avg_loss:0.679, val_acc:0.779]
Epoch [21/120    avg_loss:0.576, val_acc:0.818]
Epoch [22/120    avg_loss:0.593, val_acc:0.827]
Epoch [23/120    avg_loss:0.550, val_acc:0.818]
Epoch [24/120    avg_loss:0.464, val_acc:0.819]
Epoch [25/120    avg_loss:0.512, val_acc:0.828]
Epoch [26/120    avg_loss:0.425, val_acc:0.833]
Epoch [27/120    avg_loss:0.413, val_acc:0.841]
Epoch [28/120    avg_loss:0.376, val_acc:0.857]
Epoch [29/120    avg_loss:0.331, val_acc:0.860]
Epoch [30/120    avg_loss:0.360, val_acc:0.851]
Epoch [31/120    avg_loss:0.374, val_acc:0.856]
Epoch [32/120    avg_loss:0.318, val_acc:0.849]
Epoch [33/120    avg_loss:0.296, val_acc:0.874]
Epoch [34/120    avg_loss:0.289, val_acc:0.842]
Epoch [35/120    avg_loss:0.337, val_acc:0.861]
Epoch [36/120    avg_loss:0.284, val_acc:0.865]
Epoch [37/120    avg_loss:0.259, val_acc:0.896]
Epoch [38/120    avg_loss:0.271, val_acc:0.880]
Epoch [39/120    avg_loss:0.214, val_acc:0.900]
Epoch [40/120    avg_loss:0.191, val_acc:0.891]
Epoch [41/120    avg_loss:0.192, val_acc:0.896]
Epoch [42/120    avg_loss:0.164, val_acc:0.899]
Epoch [43/120    avg_loss:0.225, val_acc:0.849]
Epoch [44/120    avg_loss:0.216, val_acc:0.887]
Epoch [45/120    avg_loss:0.218, val_acc:0.904]
Epoch [46/120    avg_loss:0.193, val_acc:0.893]
Epoch [47/120    avg_loss:0.180, val_acc:0.907]
Epoch [48/120    avg_loss:0.153, val_acc:0.912]
Epoch [49/120    avg_loss:0.123, val_acc:0.916]
Epoch [50/120    avg_loss:0.120, val_acc:0.923]
Epoch [51/120    avg_loss:0.132, val_acc:0.917]
Epoch [52/120    avg_loss:0.113, val_acc:0.921]
Epoch [53/120    avg_loss:0.133, val_acc:0.910]
Epoch [54/120    avg_loss:0.153, val_acc:0.926]
Epoch [55/120    avg_loss:0.129, val_acc:0.908]
Epoch [56/120    avg_loss:0.130, val_acc:0.916]
Epoch [57/120    avg_loss:0.143, val_acc:0.936]
Epoch [58/120    avg_loss:0.120, val_acc:0.913]
Epoch [59/120    avg_loss:0.146, val_acc:0.919]
Epoch [60/120    avg_loss:0.154, val_acc:0.923]
Epoch [61/120    avg_loss:0.123, val_acc:0.923]
Epoch [62/120    avg_loss:0.122, val_acc:0.904]
Epoch [63/120    avg_loss:0.294, val_acc:0.902]
Epoch [64/120    avg_loss:0.168, val_acc:0.910]
Epoch [65/120    avg_loss:0.120, val_acc:0.914]
Epoch [66/120    avg_loss:0.140, val_acc:0.909]
Epoch [67/120    avg_loss:0.133, val_acc:0.930]
Epoch [68/120    avg_loss:0.122, val_acc:0.902]
Epoch [69/120    avg_loss:0.152, val_acc:0.910]
Epoch [70/120    avg_loss:0.133, val_acc:0.922]
Epoch [71/120    avg_loss:0.092, val_acc:0.936]
Epoch [72/120    avg_loss:0.081, val_acc:0.939]
Epoch [73/120    avg_loss:0.067, val_acc:0.944]
Epoch [74/120    avg_loss:0.064, val_acc:0.944]
Epoch [75/120    avg_loss:0.064, val_acc:0.948]
Epoch [76/120    avg_loss:0.074, val_acc:0.946]
Epoch [77/120    avg_loss:0.065, val_acc:0.949]
Epoch [78/120    avg_loss:0.058, val_acc:0.948]
Epoch [79/120    avg_loss:0.062, val_acc:0.947]
Epoch [80/120    avg_loss:0.060, val_acc:0.946]
Epoch [81/120    avg_loss:0.059, val_acc:0.946]
Epoch [82/120    avg_loss:0.061, val_acc:0.945]
Epoch [83/120    avg_loss:0.071, val_acc:0.947]
Epoch [84/120    avg_loss:0.061, val_acc:0.939]
Epoch [85/120    avg_loss:0.051, val_acc:0.943]
Epoch [86/120    avg_loss:0.063, val_acc:0.947]
Epoch [87/120    avg_loss:0.055, val_acc:0.943]
Epoch [88/120    avg_loss:0.051, val_acc:0.946]
Epoch [89/120    avg_loss:0.049, val_acc:0.946]
Epoch [90/120    avg_loss:0.045, val_acc:0.947]
Epoch [91/120    avg_loss:0.043, val_acc:0.947]
Epoch [92/120    avg_loss:0.048, val_acc:0.947]
Epoch [93/120    avg_loss:0.051, val_acc:0.946]
Epoch [94/120    avg_loss:0.050, val_acc:0.946]
Epoch [95/120    avg_loss:0.043, val_acc:0.947]
Epoch [96/120    avg_loss:0.043, val_acc:0.946]
Epoch [97/120    avg_loss:0.047, val_acc:0.947]
Epoch [98/120    avg_loss:0.050, val_acc:0.946]
Epoch [99/120    avg_loss:0.048, val_acc:0.946]
Epoch [100/120    avg_loss:0.053, val_acc:0.946]
Epoch [101/120    avg_loss:0.043, val_acc:0.945]
Epoch [102/120    avg_loss:0.052, val_acc:0.946]
Epoch [103/120    avg_loss:0.058, val_acc:0.945]
Epoch [104/120    avg_loss:0.050, val_acc:0.945]
Epoch [105/120    avg_loss:0.050, val_acc:0.945]
Epoch [106/120    avg_loss:0.060, val_acc:0.945]
Epoch [107/120    avg_loss:0.049, val_acc:0.945]
Epoch [108/120    avg_loss:0.053, val_acc:0.945]
Epoch [109/120    avg_loss:0.056, val_acc:0.946]
Epoch [110/120    avg_loss:0.050, val_acc:0.947]
Epoch [111/120    avg_loss:0.047, val_acc:0.944]
Epoch [112/120    avg_loss:0.048, val_acc:0.944]
Epoch [113/120    avg_loss:0.055, val_acc:0.944]
Epoch [114/120    avg_loss:0.050, val_acc:0.944]
Epoch [115/120    avg_loss:0.049, val_acc:0.944]
Epoch [116/120    avg_loss:0.048, val_acc:0.944]
Epoch [117/120    avg_loss:0.054, val_acc:0.944]
Epoch [118/120    avg_loss:0.050, val_acc:0.944]
Epoch [119/120    avg_loss:0.044, val_acc:0.944]
Epoch [120/120    avg_loss:0.047, val_acc:0.944]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1205    0    5    0    3    0    0    0    9   63    0    0
     0    0    0]
 [   0    0    2  680    3   17    1    0    0   12    0    0   29    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    3    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  653    0    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  425    0    0    0    0    0
     0    1    0]
 [   0    0    0    1    0    0    4    0    0   11    0    0    2    0
     0    0    0]
 [   0    0   33   85    0    6    3    0    0    0  709   30    0    0
     1    8    0]
 [   0    0   36    0    0    4   12    0    0    0    7 2137    9    3
     2    0    0]
 [   0    0    2   21    2    3    0    0    0    0   13    0  480    0
     0    0   13]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   13    0    0    2    0    3    3    0    0
  1118    0    0]
 [   0    0    0    0    0    0   90    0    0   11    0    0    1    0
    81  164    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
92.73712737127371

F1 scores:
[       nan 0.91566265 0.94030433 0.88657106 0.97706422 0.94597574
 0.91777934 1.         0.99183197 0.39285714 0.87584929 0.96196264
 0.90909091 0.98404255 0.95270558 0.63076923 0.92222222]

Kappa:
0.9171300874544224
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f844de0d898>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.746, val_acc:0.345]
Epoch [2/120    avg_loss:2.458, val_acc:0.380]
Epoch [3/120    avg_loss:2.249, val_acc:0.430]
Epoch [4/120    avg_loss:2.083, val_acc:0.539]
Epoch [5/120    avg_loss:1.915, val_acc:0.561]
Epoch [6/120    avg_loss:1.758, val_acc:0.592]
Epoch [7/120    avg_loss:1.663, val_acc:0.611]
Epoch [8/120    avg_loss:1.531, val_acc:0.639]
Epoch [9/120    avg_loss:1.408, val_acc:0.669]
Epoch [10/120    avg_loss:1.244, val_acc:0.664]
Epoch [11/120    avg_loss:1.115, val_acc:0.713]
Epoch [12/120    avg_loss:1.012, val_acc:0.714]
Epoch [13/120    avg_loss:0.972, val_acc:0.704]
Epoch [14/120    avg_loss:0.860, val_acc:0.723]
Epoch [15/120    avg_loss:0.776, val_acc:0.725]
Epoch [16/120    avg_loss:0.763, val_acc:0.735]
Epoch [17/120    avg_loss:0.701, val_acc:0.756]
Epoch [18/120    avg_loss:0.618, val_acc:0.751]
Epoch [19/120    avg_loss:0.639, val_acc:0.764]
Epoch [20/120    avg_loss:0.529, val_acc:0.803]
Epoch [21/120    avg_loss:0.517, val_acc:0.818]
Epoch [22/120    avg_loss:0.485, val_acc:0.813]
Epoch [23/120    avg_loss:0.513, val_acc:0.832]
Epoch [24/120    avg_loss:0.465, val_acc:0.800]
Epoch [25/120    avg_loss:0.455, val_acc:0.812]
Epoch [26/120    avg_loss:0.432, val_acc:0.806]
Epoch [27/120    avg_loss:0.381, val_acc:0.843]
Epoch [28/120    avg_loss:0.350, val_acc:0.855]
Epoch [29/120    avg_loss:0.272, val_acc:0.880]
Epoch [30/120    avg_loss:0.287, val_acc:0.852]
Epoch [31/120    avg_loss:0.277, val_acc:0.845]
Epoch [32/120    avg_loss:0.339, val_acc:0.858]
Epoch [33/120    avg_loss:0.292, val_acc:0.896]
Epoch [34/120    avg_loss:0.460, val_acc:0.778]
Epoch [35/120    avg_loss:0.548, val_acc:0.813]
Epoch [36/120    avg_loss:0.359, val_acc:0.868]
Epoch [37/120    avg_loss:0.311, val_acc:0.858]
Epoch [38/120    avg_loss:0.246, val_acc:0.885]
Epoch [39/120    avg_loss:0.229, val_acc:0.881]
Epoch [40/120    avg_loss:0.204, val_acc:0.877]
Epoch [41/120    avg_loss:0.192, val_acc:0.896]
Epoch [42/120    avg_loss:0.209, val_acc:0.904]
Epoch [43/120    avg_loss:0.213, val_acc:0.911]
Epoch [44/120    avg_loss:0.206, val_acc:0.898]
Epoch [45/120    avg_loss:0.204, val_acc:0.911]
Epoch [46/120    avg_loss:0.193, val_acc:0.880]
Epoch [47/120    avg_loss:0.163, val_acc:0.913]
Epoch [48/120    avg_loss:0.153, val_acc:0.927]
Epoch [49/120    avg_loss:0.146, val_acc:0.914]
Epoch [50/120    avg_loss:0.126, val_acc:0.938]
Epoch [51/120    avg_loss:0.136, val_acc:0.919]
Epoch [52/120    avg_loss:0.130, val_acc:0.921]
Epoch [53/120    avg_loss:0.122, val_acc:0.940]
Epoch [54/120    avg_loss:0.155, val_acc:0.927]
Epoch [55/120    avg_loss:0.132, val_acc:0.894]
Epoch [56/120    avg_loss:0.121, val_acc:0.917]
Epoch [57/120    avg_loss:0.100, val_acc:0.943]
Epoch [58/120    avg_loss:0.109, val_acc:0.937]
Epoch [59/120    avg_loss:0.105, val_acc:0.944]
Epoch [60/120    avg_loss:0.152, val_acc:0.909]
Epoch [61/120    avg_loss:0.156, val_acc:0.927]
Epoch [62/120    avg_loss:0.121, val_acc:0.938]
Epoch [63/120    avg_loss:0.111, val_acc:0.948]
Epoch [64/120    avg_loss:0.073, val_acc:0.950]
Epoch [65/120    avg_loss:0.081, val_acc:0.941]
Epoch [66/120    avg_loss:0.084, val_acc:0.941]
Epoch [67/120    avg_loss:0.106, val_acc:0.934]
Epoch [68/120    avg_loss:0.118, val_acc:0.928]
Epoch [69/120    avg_loss:0.122, val_acc:0.941]
Epoch [70/120    avg_loss:0.077, val_acc:0.926]
Epoch [71/120    avg_loss:0.075, val_acc:0.947]
Epoch [72/120    avg_loss:0.067, val_acc:0.936]
Epoch [73/120    avg_loss:0.077, val_acc:0.945]
Epoch [74/120    avg_loss:0.069, val_acc:0.944]
Epoch [75/120    avg_loss:0.083, val_acc:0.945]
Epoch [76/120    avg_loss:0.067, val_acc:0.958]
Epoch [77/120    avg_loss:0.084, val_acc:0.940]
Epoch [78/120    avg_loss:0.085, val_acc:0.940]
Epoch [79/120    avg_loss:0.078, val_acc:0.957]
Epoch [80/120    avg_loss:0.054, val_acc:0.954]
Epoch [81/120    avg_loss:0.056, val_acc:0.959]
Epoch [82/120    avg_loss:0.049, val_acc:0.962]
Epoch [83/120    avg_loss:0.053, val_acc:0.956]
Epoch [84/120    avg_loss:0.064, val_acc:0.955]
Epoch [85/120    avg_loss:0.054, val_acc:0.955]
Epoch [86/120    avg_loss:0.070, val_acc:0.950]
Epoch [87/120    avg_loss:0.060, val_acc:0.962]
Epoch [88/120    avg_loss:0.064, val_acc:0.948]
Epoch [89/120    avg_loss:0.058, val_acc:0.948]
Epoch [90/120    avg_loss:0.085, val_acc:0.955]
Epoch [91/120    avg_loss:0.060, val_acc:0.954]
Epoch [92/120    avg_loss:0.047, val_acc:0.950]
Epoch [93/120    avg_loss:0.063, val_acc:0.947]
Epoch [94/120    avg_loss:0.076, val_acc:0.957]
Epoch [95/120    avg_loss:0.056, val_acc:0.952]
Epoch [96/120    avg_loss:0.056, val_acc:0.952]
Epoch [97/120    avg_loss:0.057, val_acc:0.953]
Epoch [98/120    avg_loss:0.052, val_acc:0.962]
Epoch [99/120    avg_loss:0.040, val_acc:0.956]
Epoch [100/120    avg_loss:0.044, val_acc:0.959]
Epoch [101/120    avg_loss:0.038, val_acc:0.965]
Epoch [102/120    avg_loss:0.032, val_acc:0.966]
Epoch [103/120    avg_loss:0.045, val_acc:0.964]
Epoch [104/120    avg_loss:0.037, val_acc:0.963]
Epoch [105/120    avg_loss:0.031, val_acc:0.965]
Epoch [106/120    avg_loss:0.039, val_acc:0.949]
Epoch [107/120    avg_loss:0.046, val_acc:0.964]
Epoch [108/120    avg_loss:0.036, val_acc:0.968]
Epoch [109/120    avg_loss:0.029, val_acc:0.967]
Epoch [110/120    avg_loss:0.038, val_acc:0.966]
Epoch [111/120    avg_loss:0.029, val_acc:0.963]
Epoch [112/120    avg_loss:0.032, val_acc:0.962]
Epoch [113/120    avg_loss:0.041, val_acc:0.948]
Epoch [114/120    avg_loss:0.056, val_acc:0.949]
Epoch [115/120    avg_loss:0.048, val_acc:0.955]
Epoch [116/120    avg_loss:0.041, val_acc:0.959]
Epoch [117/120    avg_loss:0.031, val_acc:0.968]
Epoch [118/120    avg_loss:0.039, val_acc:0.956]
Epoch [119/120    avg_loss:0.034, val_acc:0.967]
Epoch [120/120    avg_loss:0.038, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1241    3    0    0    8    0    0    0    8   17    5    0
     0    3    0]
 [   0    0    1  696    3    0    1    0    0    3    0    0   43    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0  428    0    2    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   14    0    0    0    0    0    0  416    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    6    0    0    8    0    0    1    0
     0    0    0]
 [   0    0   37   88    0    0    2    0    0    0  743    0    0    0
     0    5    0]
 [   0    0   26    0    0    2   21    0    3    0   16 2115    0    3
    24    0    0]
 [   0    0    2   22    7    0    0    0    0    0   17    0  480    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   12    0    0    0    0    1    0    0    0
  1126    0    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    54  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.75338753387534

F1 scores:
[       nan 0.82978723 0.95719244 0.89230769 0.97706422 0.97605473
 0.96617647 0.96153846 0.97997644 0.48484848 0.89464178 0.97420544
 0.90310442 0.9919571  0.9611609  0.890625   0.96551724]

Kappa:
0.9402531367370842
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbbb5bcd7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.772, val_acc:0.219]
Epoch [2/120    avg_loss:2.435, val_acc:0.362]
Epoch [3/120    avg_loss:2.254, val_acc:0.478]
Epoch [4/120    avg_loss:2.109, val_acc:0.528]
Epoch [5/120    avg_loss:1.944, val_acc:0.551]
Epoch [6/120    avg_loss:1.808, val_acc:0.540]
Epoch [7/120    avg_loss:1.709, val_acc:0.592]
Epoch [8/120    avg_loss:1.587, val_acc:0.618]
Epoch [9/120    avg_loss:1.484, val_acc:0.633]
Epoch [10/120    avg_loss:1.368, val_acc:0.676]
Epoch [11/120    avg_loss:1.264, val_acc:0.658]
Epoch [12/120    avg_loss:1.193, val_acc:0.701]
Epoch [13/120    avg_loss:1.142, val_acc:0.708]
Epoch [14/120    avg_loss:1.021, val_acc:0.768]
Epoch [15/120    avg_loss:0.990, val_acc:0.709]
Epoch [16/120    avg_loss:0.861, val_acc:0.755]
Epoch [17/120    avg_loss:0.863, val_acc:0.716]
Epoch [18/120    avg_loss:0.728, val_acc:0.816]
Epoch [19/120    avg_loss:0.624, val_acc:0.816]
Epoch [20/120    avg_loss:0.533, val_acc:0.814]
Epoch [21/120    avg_loss:0.462, val_acc:0.852]
Epoch [22/120    avg_loss:0.428, val_acc:0.863]
Epoch [23/120    avg_loss:0.434, val_acc:0.846]
Epoch [24/120    avg_loss:0.370, val_acc:0.853]
Epoch [25/120    avg_loss:0.361, val_acc:0.880]
Epoch [26/120    avg_loss:0.309, val_acc:0.855]
Epoch [27/120    avg_loss:0.286, val_acc:0.884]
Epoch [28/120    avg_loss:0.260, val_acc:0.892]
Epoch [29/120    avg_loss:0.243, val_acc:0.903]
Epoch [30/120    avg_loss:0.207, val_acc:0.917]
Epoch [31/120    avg_loss:0.247, val_acc:0.910]
Epoch [32/120    avg_loss:0.212, val_acc:0.922]
Epoch [33/120    avg_loss:0.208, val_acc:0.896]
Epoch [34/120    avg_loss:0.207, val_acc:0.912]
Epoch [35/120    avg_loss:0.184, val_acc:0.905]
Epoch [36/120    avg_loss:0.158, val_acc:0.922]
Epoch [37/120    avg_loss:0.128, val_acc:0.933]
Epoch [38/120    avg_loss:0.135, val_acc:0.935]
Epoch [39/120    avg_loss:0.234, val_acc:0.914]
Epoch [40/120    avg_loss:0.146, val_acc:0.919]
Epoch [41/120    avg_loss:0.129, val_acc:0.943]
Epoch [42/120    avg_loss:0.120, val_acc:0.947]
Epoch [43/120    avg_loss:0.088, val_acc:0.943]
Epoch [44/120    avg_loss:0.079, val_acc:0.949]
Epoch [45/120    avg_loss:0.075, val_acc:0.949]
Epoch [46/120    avg_loss:0.118, val_acc:0.940]
Epoch [47/120    avg_loss:0.147, val_acc:0.946]
Epoch [48/120    avg_loss:0.109, val_acc:0.956]
Epoch [49/120    avg_loss:0.094, val_acc:0.946]
Epoch [50/120    avg_loss:0.087, val_acc:0.953]
Epoch [51/120    avg_loss:0.075, val_acc:0.942]
Epoch [52/120    avg_loss:0.075, val_acc:0.932]
Epoch [53/120    avg_loss:0.057, val_acc:0.948]
Epoch [54/120    avg_loss:0.048, val_acc:0.959]
Epoch [55/120    avg_loss:0.061, val_acc:0.942]
Epoch [56/120    avg_loss:0.055, val_acc:0.968]
Epoch [57/120    avg_loss:0.055, val_acc:0.970]
Epoch [58/120    avg_loss:0.071, val_acc:0.943]
Epoch [59/120    avg_loss:0.062, val_acc:0.952]
Epoch [60/120    avg_loss:0.053, val_acc:0.947]
Epoch [61/120    avg_loss:0.048, val_acc:0.964]
Epoch [62/120    avg_loss:0.038, val_acc:0.971]
Epoch [63/120    avg_loss:0.039, val_acc:0.963]
Epoch [64/120    avg_loss:0.035, val_acc:0.969]
Epoch [65/120    avg_loss:0.027, val_acc:0.973]
Epoch [66/120    avg_loss:0.031, val_acc:0.964]
Epoch [67/120    avg_loss:0.032, val_acc:0.961]
Epoch [68/120    avg_loss:0.037, val_acc:0.968]
Epoch [69/120    avg_loss:0.058, val_acc:0.965]
Epoch [70/120    avg_loss:0.031, val_acc:0.968]
Epoch [71/120    avg_loss:0.032, val_acc:0.959]
Epoch [72/120    avg_loss:0.050, val_acc:0.964]
Epoch [73/120    avg_loss:0.033, val_acc:0.963]
Epoch [74/120    avg_loss:0.029, val_acc:0.973]
Epoch [75/120    avg_loss:0.025, val_acc:0.972]
Epoch [76/120    avg_loss:0.021, val_acc:0.976]
Epoch [77/120    avg_loss:0.025, val_acc:0.971]
Epoch [78/120    avg_loss:0.036, val_acc:0.947]
Epoch [79/120    avg_loss:0.031, val_acc:0.973]
Epoch [80/120    avg_loss:0.030, val_acc:0.978]
Epoch [81/120    avg_loss:0.031, val_acc:0.970]
Epoch [82/120    avg_loss:0.028, val_acc:0.970]
Epoch [83/120    avg_loss:0.018, val_acc:0.969]
Epoch [84/120    avg_loss:0.019, val_acc:0.971]
Epoch [85/120    avg_loss:0.017, val_acc:0.976]
Epoch [86/120    avg_loss:0.014, val_acc:0.974]
Epoch [87/120    avg_loss:0.023, val_acc:0.969]
Epoch [88/120    avg_loss:0.028, val_acc:0.968]
Epoch [89/120    avg_loss:0.027, val_acc:0.965]
Epoch [90/120    avg_loss:0.036, val_acc:0.967]
Epoch [91/120    avg_loss:0.022, val_acc:0.973]
Epoch [92/120    avg_loss:0.024, val_acc:0.965]
Epoch [93/120    avg_loss:0.022, val_acc:0.970]
Epoch [94/120    avg_loss:0.016, val_acc:0.973]
Epoch [95/120    avg_loss:0.012, val_acc:0.976]
Epoch [96/120    avg_loss:0.015, val_acc:0.977]
Epoch [97/120    avg_loss:0.011, val_acc:0.978]
Epoch [98/120    avg_loss:0.010, val_acc:0.978]
Epoch [99/120    avg_loss:0.011, val_acc:0.979]
Epoch [100/120    avg_loss:0.013, val_acc:0.978]
Epoch [101/120    avg_loss:0.010, val_acc:0.979]
Epoch [102/120    avg_loss:0.010, val_acc:0.978]
Epoch [103/120    avg_loss:0.013, val_acc:0.980]
Epoch [104/120    avg_loss:0.010, val_acc:0.979]
Epoch [105/120    avg_loss:0.010, val_acc:0.979]
Epoch [106/120    avg_loss:0.008, val_acc:0.980]
Epoch [107/120    avg_loss:0.010, val_acc:0.980]
Epoch [108/120    avg_loss:0.011, val_acc:0.980]
Epoch [109/120    avg_loss:0.008, val_acc:0.980]
Epoch [110/120    avg_loss:0.010, val_acc:0.981]
Epoch [111/120    avg_loss:0.009, val_acc:0.981]
Epoch [112/120    avg_loss:0.011, val_acc:0.980]
Epoch [113/120    avg_loss:0.011, val_acc:0.981]
Epoch [114/120    avg_loss:0.009, val_acc:0.979]
Epoch [115/120    avg_loss:0.011, val_acc:0.979]
Epoch [116/120    avg_loss:0.011, val_acc:0.979]
Epoch [117/120    avg_loss:0.011, val_acc:0.979]
Epoch [118/120    avg_loss:0.010, val_acc:0.978]
Epoch [119/120    avg_loss:0.011, val_acc:0.981]
Epoch [120/120    avg_loss:0.010, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1264    0    0    0    0    0    0    0    2   19    0    0
     0    0    0]
 [   0    0    1  727    5    1    0    0    0    3    0    7    2    1
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    1    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  852   18    1    0
     0    0    0]
 [   0    0    4    0    0    1    1    0    0    2   13 2167   21    0
     0    1    0]
 [   0    0    0    3    2    0    0    0    0    0    0    4  520    0
     1    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    63  284    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.71273712737127

F1 scores:
[       nan 0.95121951 0.98827209 0.98309669 0.97911833 0.99076212
 0.99696049 1.         0.997669   0.85       0.97706422 0.97921374
 0.96296296 0.99730458 0.96575342 0.88062016 0.97619048]

Kappa:
0.9739102332684304
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb8d1aa48d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.743, val_acc:0.236]
Epoch [2/120    avg_loss:2.476, val_acc:0.403]
Epoch [3/120    avg_loss:2.272, val_acc:0.496]
Epoch [4/120    avg_loss:2.083, val_acc:0.492]
Epoch [5/120    avg_loss:1.964, val_acc:0.546]
Epoch [6/120    avg_loss:1.850, val_acc:0.554]
Epoch [7/120    avg_loss:1.749, val_acc:0.560]
Epoch [8/120    avg_loss:1.610, val_acc:0.575]
Epoch [9/120    avg_loss:1.520, val_acc:0.623]
Epoch [10/120    avg_loss:1.399, val_acc:0.634]
Epoch [11/120    avg_loss:1.274, val_acc:0.645]
Epoch [12/120    avg_loss:1.199, val_acc:0.554]
Epoch [13/120    avg_loss:1.105, val_acc:0.694]
Epoch [14/120    avg_loss:0.968, val_acc:0.733]
Epoch [15/120    avg_loss:0.824, val_acc:0.723]
Epoch [16/120    avg_loss:0.791, val_acc:0.726]
Epoch [17/120    avg_loss:0.775, val_acc:0.757]
Epoch [18/120    avg_loss:0.676, val_acc:0.782]
Epoch [19/120    avg_loss:0.665, val_acc:0.734]
Epoch [20/120    avg_loss:0.583, val_acc:0.776]
Epoch [21/120    avg_loss:0.495, val_acc:0.802]
Epoch [22/120    avg_loss:0.474, val_acc:0.815]
Epoch [23/120    avg_loss:0.426, val_acc:0.810]
Epoch [24/120    avg_loss:0.348, val_acc:0.803]
Epoch [25/120    avg_loss:0.358, val_acc:0.825]
Epoch [26/120    avg_loss:0.357, val_acc:0.842]
Epoch [27/120    avg_loss:0.315, val_acc:0.802]
Epoch [28/120    avg_loss:0.388, val_acc:0.849]
Epoch [29/120    avg_loss:0.296, val_acc:0.861]
Epoch [30/120    avg_loss:0.249, val_acc:0.857]
Epoch [31/120    avg_loss:0.283, val_acc:0.786]
Epoch [32/120    avg_loss:0.309, val_acc:0.844]
Epoch [33/120    avg_loss:0.316, val_acc:0.844]
Epoch [34/120    avg_loss:0.212, val_acc:0.883]
Epoch [35/120    avg_loss:0.312, val_acc:0.829]
Epoch [36/120    avg_loss:0.263, val_acc:0.847]
Epoch [37/120    avg_loss:0.252, val_acc:0.877]
Epoch [38/120    avg_loss:0.198, val_acc:0.898]
Epoch [39/120    avg_loss:0.187, val_acc:0.891]
Epoch [40/120    avg_loss:0.136, val_acc:0.906]
Epoch [41/120    avg_loss:0.130, val_acc:0.895]
Epoch [42/120    avg_loss:0.136, val_acc:0.907]
Epoch [43/120    avg_loss:0.163, val_acc:0.892]
Epoch [44/120    avg_loss:0.158, val_acc:0.917]
Epoch [45/120    avg_loss:0.121, val_acc:0.912]
Epoch [46/120    avg_loss:0.105, val_acc:0.903]
Epoch [47/120    avg_loss:0.080, val_acc:0.927]
Epoch [48/120    avg_loss:0.080, val_acc:0.921]
Epoch [49/120    avg_loss:0.074, val_acc:0.928]
Epoch [50/120    avg_loss:0.067, val_acc:0.943]
Epoch [51/120    avg_loss:0.070, val_acc:0.934]
Epoch [52/120    avg_loss:0.080, val_acc:0.940]
Epoch [53/120    avg_loss:0.062, val_acc:0.951]
Epoch [54/120    avg_loss:0.074, val_acc:0.927]
Epoch [55/120    avg_loss:0.065, val_acc:0.946]
Epoch [56/120    avg_loss:0.054, val_acc:0.951]
Epoch [57/120    avg_loss:0.058, val_acc:0.945]
Epoch [58/120    avg_loss:0.054, val_acc:0.952]
Epoch [59/120    avg_loss:0.053, val_acc:0.948]
Epoch [60/120    avg_loss:0.052, val_acc:0.928]
Epoch [61/120    avg_loss:0.044, val_acc:0.944]
Epoch [62/120    avg_loss:0.047, val_acc:0.944]
Epoch [63/120    avg_loss:0.039, val_acc:0.958]
Epoch [64/120    avg_loss:0.037, val_acc:0.955]
Epoch [65/120    avg_loss:0.034, val_acc:0.951]
Epoch [66/120    avg_loss:0.036, val_acc:0.947]
Epoch [67/120    avg_loss:0.037, val_acc:0.947]
Epoch [68/120    avg_loss:0.040, val_acc:0.961]
Epoch [69/120    avg_loss:0.032, val_acc:0.955]
Epoch [70/120    avg_loss:0.043, val_acc:0.951]
Epoch [71/120    avg_loss:0.031, val_acc:0.954]
Epoch [72/120    avg_loss:0.045, val_acc:0.949]
Epoch [73/120    avg_loss:0.035, val_acc:0.965]
Epoch [74/120    avg_loss:0.036, val_acc:0.950]
Epoch [75/120    avg_loss:0.036, val_acc:0.957]
Epoch [76/120    avg_loss:0.070, val_acc:0.931]
Epoch [77/120    avg_loss:0.047, val_acc:0.954]
Epoch [78/120    avg_loss:0.057, val_acc:0.960]
Epoch [79/120    avg_loss:0.029, val_acc:0.957]
Epoch [80/120    avg_loss:0.028, val_acc:0.952]
Epoch [81/120    avg_loss:0.022, val_acc:0.967]
Epoch [82/120    avg_loss:0.021, val_acc:0.960]
Epoch [83/120    avg_loss:0.018, val_acc:0.968]
Epoch [84/120    avg_loss:0.023, val_acc:0.968]
Epoch [85/120    avg_loss:0.014, val_acc:0.972]
Epoch [86/120    avg_loss:0.019, val_acc:0.964]
Epoch [87/120    avg_loss:0.018, val_acc:0.966]
Epoch [88/120    avg_loss:0.022, val_acc:0.970]
Epoch [89/120    avg_loss:0.028, val_acc:0.969]
Epoch [90/120    avg_loss:0.017, val_acc:0.967]
Epoch [91/120    avg_loss:0.023, val_acc:0.965]
Epoch [92/120    avg_loss:0.021, val_acc:0.965]
Epoch [93/120    avg_loss:0.020, val_acc:0.967]
Epoch [94/120    avg_loss:0.021, val_acc:0.958]
Epoch [95/120    avg_loss:0.021, val_acc:0.957]
Epoch [96/120    avg_loss:0.020, val_acc:0.967]
Epoch [97/120    avg_loss:0.040, val_acc:0.966]
Epoch [98/120    avg_loss:0.039, val_acc:0.951]
Epoch [99/120    avg_loss:0.024, val_acc:0.960]
Epoch [100/120    avg_loss:0.016, val_acc:0.966]
Epoch [101/120    avg_loss:0.012, val_acc:0.965]
Epoch [102/120    avg_loss:0.013, val_acc:0.967]
Epoch [103/120    avg_loss:0.013, val_acc:0.967]
Epoch [104/120    avg_loss:0.017, val_acc:0.965]
Epoch [105/120    avg_loss:0.010, val_acc:0.965]
Epoch [106/120    avg_loss:0.011, val_acc:0.966]
Epoch [107/120    avg_loss:0.010, val_acc:0.967]
Epoch [108/120    avg_loss:0.013, val_acc:0.970]
Epoch [109/120    avg_loss:0.010, val_acc:0.970]
Epoch [110/120    avg_loss:0.011, val_acc:0.968]
Epoch [111/120    avg_loss:0.010, val_acc:0.968]
Epoch [112/120    avg_loss:0.009, val_acc:0.968]
Epoch [113/120    avg_loss:0.010, val_acc:0.968]
Epoch [114/120    avg_loss:0.012, val_acc:0.968]
Epoch [115/120    avg_loss:0.012, val_acc:0.968]
Epoch [116/120    avg_loss:0.009, val_acc:0.968]
Epoch [117/120    avg_loss:0.013, val_acc:0.968]
Epoch [118/120    avg_loss:0.008, val_acc:0.968]
Epoch [119/120    avg_loss:0.013, val_acc:0.968]
Epoch [120/120    avg_loss:0.010, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    2    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1259   12    2    0    0    0    0    0    3    9    0    0
     0    0    0]
 [   0    0    0  729    4    0    0    0    0    0    1    6    7    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    3    0    1    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    0    0
     0    1    0]
 [   0    0   11    1    0    2    0    0    0    0  843   17    1    0
     0    0    0]
 [   0    0   12    1    0    0    0    0    0    1   32 2149   11    1
     0    3    0]
 [   0    0    0    6    0    0    0    0    0    0    0    4  522    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1125   13    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    65  274    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.20325203252033

F1 scores:
[       nan 0.975      0.98091157 0.9739479  0.9837587  0.9837963
 0.99014405 0.94339623 0.997669   0.91891892 0.96123147 0.97704024
 0.96756256 0.99730458 0.96359743 0.85893417 0.97619048]

Kappa:
0.9681111515787282
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe111dcc828>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.732, val_acc:0.255]
Epoch [2/120    avg_loss:2.465, val_acc:0.320]
Epoch [3/120    avg_loss:2.268, val_acc:0.411]
Epoch [4/120    avg_loss:2.096, val_acc:0.505]
Epoch [5/120    avg_loss:1.990, val_acc:0.570]
Epoch [6/120    avg_loss:1.806, val_acc:0.598]
Epoch [7/120    avg_loss:1.615, val_acc:0.618]
Epoch [8/120    avg_loss:1.512, val_acc:0.637]
Epoch [9/120    avg_loss:1.359, val_acc:0.665]
Epoch [10/120    avg_loss:1.226, val_acc:0.665]
Epoch [11/120    avg_loss:1.120, val_acc:0.705]
Epoch [12/120    avg_loss:0.990, val_acc:0.723]
Epoch [13/120    avg_loss:0.909, val_acc:0.724]
Epoch [14/120    avg_loss:0.816, val_acc:0.767]
Epoch [15/120    avg_loss:0.723, val_acc:0.742]
Epoch [16/120    avg_loss:0.665, val_acc:0.758]
Epoch [17/120    avg_loss:0.566, val_acc:0.814]
Epoch [18/120    avg_loss:0.519, val_acc:0.761]
Epoch [19/120    avg_loss:0.437, val_acc:0.836]
Epoch [20/120    avg_loss:0.445, val_acc:0.795]
Epoch [21/120    avg_loss:0.439, val_acc:0.841]
Epoch [22/120    avg_loss:0.355, val_acc:0.820]
Epoch [23/120    avg_loss:0.370, val_acc:0.838]
Epoch [24/120    avg_loss:0.363, val_acc:0.833]
Epoch [25/120    avg_loss:0.301, val_acc:0.854]
Epoch [26/120    avg_loss:0.274, val_acc:0.819]
Epoch [27/120    avg_loss:0.286, val_acc:0.856]
Epoch [28/120    avg_loss:0.219, val_acc:0.890]
Epoch [29/120    avg_loss:0.193, val_acc:0.899]
Epoch [30/120    avg_loss:0.193, val_acc:0.898]
Epoch [31/120    avg_loss:0.163, val_acc:0.922]
Epoch [32/120    avg_loss:0.151, val_acc:0.912]
Epoch [33/120    avg_loss:0.167, val_acc:0.911]
Epoch [34/120    avg_loss:0.150, val_acc:0.919]
Epoch [35/120    avg_loss:0.150, val_acc:0.940]
Epoch [36/120    avg_loss:0.138, val_acc:0.908]
Epoch [37/120    avg_loss:0.153, val_acc:0.925]
Epoch [38/120    avg_loss:0.141, val_acc:0.903]
Epoch [39/120    avg_loss:0.123, val_acc:0.919]
Epoch [40/120    avg_loss:0.152, val_acc:0.919]
Epoch [41/120    avg_loss:0.119, val_acc:0.935]
Epoch [42/120    avg_loss:0.107, val_acc:0.934]
Epoch [43/120    avg_loss:0.098, val_acc:0.940]
Epoch [44/120    avg_loss:0.080, val_acc:0.939]
Epoch [45/120    avg_loss:0.070, val_acc:0.954]
Epoch [46/120    avg_loss:0.065, val_acc:0.956]
Epoch [47/120    avg_loss:0.063, val_acc:0.938]
Epoch [48/120    avg_loss:0.090, val_acc:0.944]
Epoch [49/120    avg_loss:0.102, val_acc:0.940]
Epoch [50/120    avg_loss:0.083, val_acc:0.954]
Epoch [51/120    avg_loss:0.072, val_acc:0.943]
Epoch [52/120    avg_loss:0.073, val_acc:0.961]
Epoch [53/120    avg_loss:0.058, val_acc:0.950]
Epoch [54/120    avg_loss:0.050, val_acc:0.954]
Epoch [55/120    avg_loss:0.046, val_acc:0.965]
Epoch [56/120    avg_loss:0.064, val_acc:0.947]
Epoch [57/120    avg_loss:0.116, val_acc:0.945]
Epoch [58/120    avg_loss:0.080, val_acc:0.915]
Epoch [59/120    avg_loss:0.098, val_acc:0.931]
Epoch [60/120    avg_loss:0.186, val_acc:0.939]
Epoch [61/120    avg_loss:0.154, val_acc:0.920]
Epoch [62/120    avg_loss:0.092, val_acc:0.943]
Epoch [63/120    avg_loss:0.057, val_acc:0.951]
Epoch [64/120    avg_loss:0.051, val_acc:0.947]
Epoch [65/120    avg_loss:0.069, val_acc:0.941]
Epoch [66/120    avg_loss:0.065, val_acc:0.951]
Epoch [67/120    avg_loss:0.061, val_acc:0.956]
Epoch [68/120    avg_loss:0.051, val_acc:0.967]
Epoch [69/120    avg_loss:0.030, val_acc:0.969]
Epoch [70/120    avg_loss:0.041, val_acc:0.965]
Epoch [71/120    avg_loss:0.052, val_acc:0.946]
Epoch [72/120    avg_loss:0.042, val_acc:0.966]
Epoch [73/120    avg_loss:0.027, val_acc:0.976]
Epoch [74/120    avg_loss:0.034, val_acc:0.957]
Epoch [75/120    avg_loss:0.031, val_acc:0.965]
Epoch [76/120    avg_loss:0.034, val_acc:0.967]
Epoch [77/120    avg_loss:0.048, val_acc:0.952]
Epoch [78/120    avg_loss:0.035, val_acc:0.972]
Epoch [79/120    avg_loss:0.032, val_acc:0.966]
Epoch [80/120    avg_loss:0.049, val_acc:0.979]
Epoch [81/120    avg_loss:0.042, val_acc:0.980]
Epoch [82/120    avg_loss:0.026, val_acc:0.977]
Epoch [83/120    avg_loss:0.026, val_acc:0.968]
Epoch [84/120    avg_loss:0.024, val_acc:0.975]
Epoch [85/120    avg_loss:0.023, val_acc:0.974]
Epoch [86/120    avg_loss:0.026, val_acc:0.977]
Epoch [87/120    avg_loss:0.019, val_acc:0.972]
Epoch [88/120    avg_loss:0.020, val_acc:0.976]
Epoch [89/120    avg_loss:0.030, val_acc:0.966]
Epoch [90/120    avg_loss:0.022, val_acc:0.975]
Epoch [91/120    avg_loss:0.015, val_acc:0.974]
Epoch [92/120    avg_loss:0.017, val_acc:0.970]
Epoch [93/120    avg_loss:0.020, val_acc:0.972]
Epoch [94/120    avg_loss:0.018, val_acc:0.975]
Epoch [95/120    avg_loss:0.013, val_acc:0.978]
Epoch [96/120    avg_loss:0.011, val_acc:0.983]
Epoch [97/120    avg_loss:0.013, val_acc:0.982]
Epoch [98/120    avg_loss:0.012, val_acc:0.982]
Epoch [99/120    avg_loss:0.010, val_acc:0.982]
Epoch [100/120    avg_loss:0.010, val_acc:0.983]
Epoch [101/120    avg_loss:0.009, val_acc:0.982]
Epoch [102/120    avg_loss:0.008, val_acc:0.982]
Epoch [103/120    avg_loss:0.012, val_acc:0.982]
Epoch [104/120    avg_loss:0.011, val_acc:0.982]
Epoch [105/120    avg_loss:0.010, val_acc:0.982]
Epoch [106/120    avg_loss:0.010, val_acc:0.983]
Epoch [107/120    avg_loss:0.011, val_acc:0.982]
Epoch [108/120    avg_loss:0.010, val_acc:0.983]
Epoch [109/120    avg_loss:0.008, val_acc:0.984]
Epoch [110/120    avg_loss:0.009, val_acc:0.982]
Epoch [111/120    avg_loss:0.009, val_acc:0.982]
Epoch [112/120    avg_loss:0.008, val_acc:0.982]
Epoch [113/120    avg_loss:0.010, val_acc:0.983]
Epoch [114/120    avg_loss:0.008, val_acc:0.983]
Epoch [115/120    avg_loss:0.010, val_acc:0.982]
Epoch [116/120    avg_loss:0.008, val_acc:0.982]
Epoch [117/120    avg_loss:0.011, val_acc:0.983]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.008, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    2    0    0    0    0    0    0    0    0
     0    1    0]
 [   0    0 1258    1    9    0    0    0    0    0    0   17    0    0
     0    0    0]
 [   0    0    0  718   12    0    0    0    0    1    4    6    5    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0   12    0    0    0    0    0    0    0  848   13    1    0
     0    1    0]
 [   0    0    7    0    0    0    0    0    0    0    5 2181   17    0
     0    0    0]
 [   0    0    2    1    0    0    0    0    0    0    2    4  522    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    1    1    0  183
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    54  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.7560975609756

F1 scores:
[       nan 0.92682927 0.98127925 0.97886844 0.95302013 0.99424626
 0.99468489 1.         0.99649942 0.91891892 0.97752161 0.98376184
 0.96577243 0.99186992 0.97164948 0.89302326 0.97619048]

Kappa:
0.9744032858804486
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f274b84e898>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.730, val_acc:0.275]
Epoch [2/120    avg_loss:2.452, val_acc:0.409]
Epoch [3/120    avg_loss:2.235, val_acc:0.485]
Epoch [4/120    avg_loss:2.059, val_acc:0.524]
Epoch [5/120    avg_loss:1.950, val_acc:0.527]
Epoch [6/120    avg_loss:1.799, val_acc:0.560]
Epoch [7/120    avg_loss:1.678, val_acc:0.593]
Epoch [8/120    avg_loss:1.542, val_acc:0.600]
Epoch [9/120    avg_loss:1.430, val_acc:0.639]
Epoch [10/120    avg_loss:1.277, val_acc:0.654]
Epoch [11/120    avg_loss:1.161, val_acc:0.675]
Epoch [12/120    avg_loss:1.040, val_acc:0.709]
Epoch [13/120    avg_loss:0.971, val_acc:0.714]
Epoch [14/120    avg_loss:0.877, val_acc:0.751]
Epoch [15/120    avg_loss:0.769, val_acc:0.758]
Epoch [16/120    avg_loss:0.722, val_acc:0.811]
Epoch [17/120    avg_loss:0.627, val_acc:0.783]
Epoch [18/120    avg_loss:0.535, val_acc:0.783]
Epoch [19/120    avg_loss:0.483, val_acc:0.798]
Epoch [20/120    avg_loss:0.423, val_acc:0.839]
Epoch [21/120    avg_loss:0.376, val_acc:0.817]
Epoch [22/120    avg_loss:0.340, val_acc:0.845]
Epoch [23/120    avg_loss:0.313, val_acc:0.851]
Epoch [24/120    avg_loss:0.290, val_acc:0.883]
Epoch [25/120    avg_loss:0.276, val_acc:0.857]
Epoch [26/120    avg_loss:0.274, val_acc:0.850]
Epoch [27/120    avg_loss:0.322, val_acc:0.869]
Epoch [28/120    avg_loss:0.281, val_acc:0.865]
Epoch [29/120    avg_loss:0.265, val_acc:0.896]
Epoch [30/120    avg_loss:0.192, val_acc:0.896]
Epoch [31/120    avg_loss:0.198, val_acc:0.912]
Epoch [32/120    avg_loss:0.161, val_acc:0.924]
Epoch [33/120    avg_loss:0.155, val_acc:0.908]
Epoch [34/120    avg_loss:0.144, val_acc:0.932]
Epoch [35/120    avg_loss:0.171, val_acc:0.916]
Epoch [36/120    avg_loss:0.149, val_acc:0.918]
Epoch [37/120    avg_loss:0.126, val_acc:0.918]
Epoch [38/120    avg_loss:0.110, val_acc:0.948]
Epoch [39/120    avg_loss:0.103, val_acc:0.952]
Epoch [40/120    avg_loss:0.109, val_acc:0.933]
Epoch [41/120    avg_loss:0.095, val_acc:0.939]
Epoch [42/120    avg_loss:0.077, val_acc:0.964]
Epoch [43/120    avg_loss:0.069, val_acc:0.951]
Epoch [44/120    avg_loss:0.119, val_acc:0.926]
Epoch [45/120    avg_loss:0.151, val_acc:0.939]
Epoch [46/120    avg_loss:0.087, val_acc:0.954]
Epoch [47/120    avg_loss:0.108, val_acc:0.943]
Epoch [48/120    avg_loss:0.141, val_acc:0.928]
Epoch [49/120    avg_loss:0.123, val_acc:0.941]
Epoch [50/120    avg_loss:0.097, val_acc:0.935]
Epoch [51/120    avg_loss:0.145, val_acc:0.930]
Epoch [52/120    avg_loss:0.110, val_acc:0.950]
Epoch [53/120    avg_loss:0.084, val_acc:0.939]
Epoch [54/120    avg_loss:0.087, val_acc:0.956]
Epoch [55/120    avg_loss:0.058, val_acc:0.969]
Epoch [56/120    avg_loss:0.065, val_acc:0.960]
Epoch [57/120    avg_loss:0.063, val_acc:0.951]
Epoch [58/120    avg_loss:0.096, val_acc:0.911]
Epoch [59/120    avg_loss:0.073, val_acc:0.954]
Epoch [60/120    avg_loss:0.053, val_acc:0.968]
Epoch [61/120    avg_loss:0.049, val_acc:0.970]
Epoch [62/120    avg_loss:0.038, val_acc:0.964]
Epoch [63/120    avg_loss:0.033, val_acc:0.955]
Epoch [64/120    avg_loss:0.039, val_acc:0.959]
Epoch [65/120    avg_loss:0.040, val_acc:0.967]
Epoch [66/120    avg_loss:0.027, val_acc:0.967]
Epoch [67/120    avg_loss:0.036, val_acc:0.968]
Epoch [68/120    avg_loss:0.030, val_acc:0.963]
Epoch [69/120    avg_loss:0.025, val_acc:0.970]
Epoch [70/120    avg_loss:0.032, val_acc:0.956]
Epoch [71/120    avg_loss:0.041, val_acc:0.951]
Epoch [72/120    avg_loss:0.040, val_acc:0.966]
Epoch [73/120    avg_loss:0.028, val_acc:0.956]
Epoch [74/120    avg_loss:0.029, val_acc:0.973]
Epoch [75/120    avg_loss:0.025, val_acc:0.969]
Epoch [76/120    avg_loss:0.022, val_acc:0.972]
Epoch [77/120    avg_loss:0.022, val_acc:0.975]
Epoch [78/120    avg_loss:0.018, val_acc:0.974]
Epoch [79/120    avg_loss:0.024, val_acc:0.970]
Epoch [80/120    avg_loss:0.024, val_acc:0.970]
Epoch [81/120    avg_loss:0.019, val_acc:0.967]
Epoch [82/120    avg_loss:0.016, val_acc:0.971]
Epoch [83/120    avg_loss:0.024, val_acc:0.965]
Epoch [84/120    avg_loss:0.018, val_acc:0.976]
Epoch [85/120    avg_loss:0.016, val_acc:0.974]
Epoch [86/120    avg_loss:0.014, val_acc:0.976]
Epoch [87/120    avg_loss:0.018, val_acc:0.971]
Epoch [88/120    avg_loss:0.032, val_acc:0.949]
Epoch [89/120    avg_loss:0.022, val_acc:0.972]
Epoch [90/120    avg_loss:0.013, val_acc:0.970]
Epoch [91/120    avg_loss:0.014, val_acc:0.965]
Epoch [92/120    avg_loss:0.017, val_acc:0.974]
Epoch [93/120    avg_loss:0.016, val_acc:0.973]
Epoch [94/120    avg_loss:0.014, val_acc:0.974]
Epoch [95/120    avg_loss:0.010, val_acc:0.975]
Epoch [96/120    avg_loss:0.011, val_acc:0.970]
Epoch [97/120    avg_loss:0.010, val_acc:0.975]
Epoch [98/120    avg_loss:0.010, val_acc:0.969]
Epoch [99/120    avg_loss:0.027, val_acc:0.966]
Epoch [100/120    avg_loss:0.018, val_acc:0.974]
Epoch [101/120    avg_loss:0.016, val_acc:0.975]
Epoch [102/120    avg_loss:0.011, val_acc:0.975]
Epoch [103/120    avg_loss:0.012, val_acc:0.976]
Epoch [104/120    avg_loss:0.010, val_acc:0.976]
Epoch [105/120    avg_loss:0.012, val_acc:0.976]
Epoch [106/120    avg_loss:0.010, val_acc:0.976]
Epoch [107/120    avg_loss:0.012, val_acc:0.976]
Epoch [108/120    avg_loss:0.010, val_acc:0.975]
Epoch [109/120    avg_loss:0.012, val_acc:0.975]
Epoch [110/120    avg_loss:0.008, val_acc:0.976]
Epoch [111/120    avg_loss:0.008, val_acc:0.975]
Epoch [112/120    avg_loss:0.008, val_acc:0.975]
Epoch [113/120    avg_loss:0.010, val_acc:0.977]
Epoch [114/120    avg_loss:0.010, val_acc:0.976]
Epoch [115/120    avg_loss:0.009, val_acc:0.976]
Epoch [116/120    avg_loss:0.008, val_acc:0.976]
Epoch [117/120    avg_loss:0.012, val_acc:0.976]
Epoch [118/120    avg_loss:0.009, val_acc:0.975]
Epoch [119/120    avg_loss:0.008, val_acc:0.974]
Epoch [120/120    avg_loss:0.010, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1250    2    1    0    1    0    0    1    3   27    0    0
     0    0    0]
 [   0    0    9  714   13    0    0    0    0    1    3    1    5    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    1    0    0    0    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    8    0    0    1    0    0    0    0  824   34    5    0
     2    1    0]
 [   0    0   12    0    0    0    0    0    0    3   14 2163   14    1
     0    3    0]
 [   0    0    2    0    0    2    0    0    0    0    0    1  522    0
     0    6    1]
 [   0    0    0    0    0    0    0    0    0    0    0    2    0  183
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    30  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.38753387533875

F1 scores:
[       nan 0.98765432 0.97427903 0.97607656 0.96818182 0.98845266
 0.99392097 1.         1.         0.85       0.95813953 0.97410493
 0.96577243 0.99186992 0.97611811 0.9154519  0.98809524]

Kappa:
0.9702047742050653
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd0fc5d68d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.723, val_acc:0.342]
Epoch [2/120    avg_loss:2.420, val_acc:0.496]
Epoch [3/120    avg_loss:2.212, val_acc:0.518]
Epoch [4/120    avg_loss:2.050, val_acc:0.545]
Epoch [5/120    avg_loss:1.881, val_acc:0.571]
Epoch [6/120    avg_loss:1.761, val_acc:0.575]
Epoch [7/120    avg_loss:1.648, val_acc:0.606]
Epoch [8/120    avg_loss:1.542, val_acc:0.591]
Epoch [9/120    avg_loss:1.433, val_acc:0.609]
Epoch [10/120    avg_loss:1.293, val_acc:0.617]
Epoch [11/120    avg_loss:1.211, val_acc:0.666]
Epoch [12/120    avg_loss:1.050, val_acc:0.676]
Epoch [13/120    avg_loss:1.023, val_acc:0.678]
Epoch [14/120    avg_loss:0.926, val_acc:0.693]
Epoch [15/120    avg_loss:0.751, val_acc:0.694]
Epoch [16/120    avg_loss:0.704, val_acc:0.797]
Epoch [17/120    avg_loss:0.596, val_acc:0.799]
Epoch [18/120    avg_loss:0.572, val_acc:0.791]
Epoch [19/120    avg_loss:0.633, val_acc:0.806]
Epoch [20/120    avg_loss:0.504, val_acc:0.807]
Epoch [21/120    avg_loss:0.396, val_acc:0.840]
Epoch [22/120    avg_loss:0.375, val_acc:0.842]
Epoch [23/120    avg_loss:0.350, val_acc:0.853]
Epoch [24/120    avg_loss:0.384, val_acc:0.871]
Epoch [25/120    avg_loss:0.332, val_acc:0.877]
Epoch [26/120    avg_loss:0.271, val_acc:0.857]
Epoch [27/120    avg_loss:0.266, val_acc:0.885]
Epoch [28/120    avg_loss:0.255, val_acc:0.909]
Epoch [29/120    avg_loss:0.232, val_acc:0.905]
Epoch [30/120    avg_loss:0.196, val_acc:0.907]
Epoch [31/120    avg_loss:0.174, val_acc:0.898]
Epoch [32/120    avg_loss:0.171, val_acc:0.918]
Epoch [33/120    avg_loss:0.133, val_acc:0.912]
Epoch [34/120    avg_loss:0.120, val_acc:0.923]
Epoch [35/120    avg_loss:0.124, val_acc:0.947]
Epoch [36/120    avg_loss:0.129, val_acc:0.935]
Epoch [37/120    avg_loss:0.112, val_acc:0.938]
Epoch [38/120    avg_loss:0.108, val_acc:0.918]
Epoch [39/120    avg_loss:0.105, val_acc:0.915]
Epoch [40/120    avg_loss:0.114, val_acc:0.908]
Epoch [41/120    avg_loss:0.109, val_acc:0.946]
Epoch [42/120    avg_loss:0.096, val_acc:0.931]
Epoch [43/120    avg_loss:0.083, val_acc:0.946]
Epoch [44/120    avg_loss:0.095, val_acc:0.944]
Epoch [45/120    avg_loss:0.228, val_acc:0.860]
Epoch [46/120    avg_loss:0.272, val_acc:0.905]
Epoch [47/120    avg_loss:0.210, val_acc:0.896]
Epoch [48/120    avg_loss:0.145, val_acc:0.932]
Epoch [49/120    avg_loss:0.099, val_acc:0.940]
Epoch [50/120    avg_loss:0.084, val_acc:0.944]
Epoch [51/120    avg_loss:0.084, val_acc:0.944]
Epoch [52/120    avg_loss:0.084, val_acc:0.948]
Epoch [53/120    avg_loss:0.076, val_acc:0.949]
Epoch [54/120    avg_loss:0.075, val_acc:0.950]
Epoch [55/120    avg_loss:0.073, val_acc:0.945]
Epoch [56/120    avg_loss:0.068, val_acc:0.949]
Epoch [57/120    avg_loss:0.066, val_acc:0.952]
Epoch [58/120    avg_loss:0.063, val_acc:0.952]
Epoch [59/120    avg_loss:0.062, val_acc:0.953]
Epoch [60/120    avg_loss:0.054, val_acc:0.954]
Epoch [61/120    avg_loss:0.070, val_acc:0.954]
Epoch [62/120    avg_loss:0.058, val_acc:0.953]
Epoch [63/120    avg_loss:0.053, val_acc:0.954]
Epoch [64/120    avg_loss:0.059, val_acc:0.954]
Epoch [65/120    avg_loss:0.053, val_acc:0.950]
Epoch [66/120    avg_loss:0.054, val_acc:0.955]
Epoch [67/120    avg_loss:0.054, val_acc:0.955]
Epoch [68/120    avg_loss:0.048, val_acc:0.952]
Epoch [69/120    avg_loss:0.051, val_acc:0.957]
Epoch [70/120    avg_loss:0.048, val_acc:0.954]
Epoch [71/120    avg_loss:0.049, val_acc:0.954]
Epoch [72/120    avg_loss:0.049, val_acc:0.954]
Epoch [73/120    avg_loss:0.044, val_acc:0.954]
Epoch [74/120    avg_loss:0.050, val_acc:0.957]
Epoch [75/120    avg_loss:0.054, val_acc:0.956]
Epoch [76/120    avg_loss:0.056, val_acc:0.954]
Epoch [77/120    avg_loss:0.043, val_acc:0.956]
Epoch [78/120    avg_loss:0.041, val_acc:0.959]
Epoch [79/120    avg_loss:0.049, val_acc:0.957]
Epoch [80/120    avg_loss:0.042, val_acc:0.956]
Epoch [81/120    avg_loss:0.042, val_acc:0.956]
Epoch [82/120    avg_loss:0.044, val_acc:0.959]
Epoch [83/120    avg_loss:0.048, val_acc:0.964]
Epoch [84/120    avg_loss:0.040, val_acc:0.966]
Epoch [85/120    avg_loss:0.039, val_acc:0.963]
Epoch [86/120    avg_loss:0.041, val_acc:0.964]
Epoch [87/120    avg_loss:0.050, val_acc:0.956]
Epoch [88/120    avg_loss:0.045, val_acc:0.964]
Epoch [89/120    avg_loss:0.041, val_acc:0.964]
Epoch [90/120    avg_loss:0.044, val_acc:0.963]
Epoch [91/120    avg_loss:0.048, val_acc:0.964]
Epoch [92/120    avg_loss:0.040, val_acc:0.964]
Epoch [93/120    avg_loss:0.042, val_acc:0.965]
Epoch [94/120    avg_loss:0.033, val_acc:0.967]
Epoch [95/120    avg_loss:0.037, val_acc:0.967]
Epoch [96/120    avg_loss:0.035, val_acc:0.964]
Epoch [97/120    avg_loss:0.038, val_acc:0.966]
Epoch [98/120    avg_loss:0.031, val_acc:0.966]
Epoch [99/120    avg_loss:0.034, val_acc:0.967]
Epoch [100/120    avg_loss:0.035, val_acc:0.961]
Epoch [101/120    avg_loss:0.037, val_acc:0.966]
Epoch [102/120    avg_loss:0.033, val_acc:0.964]
Epoch [103/120    avg_loss:0.038, val_acc:0.966]
Epoch [104/120    avg_loss:0.036, val_acc:0.965]
Epoch [105/120    avg_loss:0.033, val_acc:0.967]
Epoch [106/120    avg_loss:0.037, val_acc:0.968]
Epoch [107/120    avg_loss:0.035, val_acc:0.966]
Epoch [108/120    avg_loss:0.037, val_acc:0.965]
Epoch [109/120    avg_loss:0.033, val_acc:0.964]
Epoch [110/120    avg_loss:0.038, val_acc:0.969]
Epoch [111/120    avg_loss:0.037, val_acc:0.967]
Epoch [112/120    avg_loss:0.036, val_acc:0.966]
Epoch [113/120    avg_loss:0.032, val_acc:0.963]
Epoch [114/120    avg_loss:0.031, val_acc:0.967]
Epoch [115/120    avg_loss:0.034, val_acc:0.966]
Epoch [116/120    avg_loss:0.030, val_acc:0.967]
Epoch [117/120    avg_loss:0.033, val_acc:0.970]
Epoch [118/120    avg_loss:0.031, val_acc:0.968]
Epoch [119/120    avg_loss:0.030, val_acc:0.967]
Epoch [120/120    avg_loss:0.036, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1254    8    1    1    0    0    0    0    4   17    0    0
     0    0    0]
 [   0    0    0  735    1    1    0    0    0    1    2    7    0    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  427    0    5    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    1    0    0   15    0    1    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  824   45    1    0
     0    0    0]
 [   0    0   13    0    0    1    0    0    0    0   31 2145   20    0
     0    0    0]
 [   0    0    0    7    0    0    0    0    0    0    0    0  525    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1121   18    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    64  283    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.10569105691057

F1 scores:
[       nan 0.97560976 0.98083692 0.98130841 0.99297424 0.98728324
 0.99847793 0.90909091 0.99883586 0.88235294 0.94876223 0.96905354
 0.96952909 1.         0.9638865  0.87211094 0.98203593]

Kappa:
0.9669867447573248
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fda8b63d860>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.720, val_acc:0.333]
Epoch [2/120    avg_loss:2.410, val_acc:0.481]
Epoch [3/120    avg_loss:2.218, val_acc:0.514]
Epoch [4/120    avg_loss:2.057, val_acc:0.548]
Epoch [5/120    avg_loss:1.890, val_acc:0.570]
Epoch [6/120    avg_loss:1.765, val_acc:0.601]
Epoch [7/120    avg_loss:1.651, val_acc:0.597]
Epoch [8/120    avg_loss:1.473, val_acc:0.627]
Epoch [9/120    avg_loss:1.321, val_acc:0.636]
Epoch [10/120    avg_loss:1.190, val_acc:0.662]
Epoch [11/120    avg_loss:1.112, val_acc:0.698]
Epoch [12/120    avg_loss:1.013, val_acc:0.678]
Epoch [13/120    avg_loss:0.839, val_acc:0.717]
Epoch [14/120    avg_loss:0.810, val_acc:0.751]
Epoch [15/120    avg_loss:0.712, val_acc:0.757]
Epoch [16/120    avg_loss:0.710, val_acc:0.772]
Epoch [17/120    avg_loss:0.638, val_acc:0.771]
Epoch [18/120    avg_loss:0.534, val_acc:0.799]
Epoch [19/120    avg_loss:0.445, val_acc:0.811]
Epoch [20/120    avg_loss:0.407, val_acc:0.849]
Epoch [21/120    avg_loss:0.388, val_acc:0.843]
Epoch [22/120    avg_loss:0.372, val_acc:0.842]
Epoch [23/120    avg_loss:0.323, val_acc:0.869]
Epoch [24/120    avg_loss:0.333, val_acc:0.847]
Epoch [25/120    avg_loss:0.296, val_acc:0.871]
Epoch [26/120    avg_loss:0.269, val_acc:0.897]
Epoch [27/120    avg_loss:0.212, val_acc:0.896]
Epoch [28/120    avg_loss:0.203, val_acc:0.903]
Epoch [29/120    avg_loss:0.212, val_acc:0.841]
Epoch [30/120    avg_loss:0.246, val_acc:0.904]
Epoch [31/120    avg_loss:0.185, val_acc:0.893]
Epoch [32/120    avg_loss:0.193, val_acc:0.907]
Epoch [33/120    avg_loss:0.273, val_acc:0.894]
Epoch [34/120    avg_loss:0.283, val_acc:0.868]
Epoch [35/120    avg_loss:0.192, val_acc:0.914]
Epoch [36/120    avg_loss:0.175, val_acc:0.890]
Epoch [37/120    avg_loss:0.153, val_acc:0.915]
Epoch [38/120    avg_loss:0.134, val_acc:0.920]
Epoch [39/120    avg_loss:0.113, val_acc:0.919]
Epoch [40/120    avg_loss:0.099, val_acc:0.915]
Epoch [41/120    avg_loss:0.094, val_acc:0.936]
Epoch [42/120    avg_loss:0.116, val_acc:0.929]
Epoch [43/120    avg_loss:0.163, val_acc:0.922]
Epoch [44/120    avg_loss:0.107, val_acc:0.925]
Epoch [45/120    avg_loss:0.088, val_acc:0.943]
Epoch [46/120    avg_loss:0.070, val_acc:0.940]
Epoch [47/120    avg_loss:0.071, val_acc:0.931]
Epoch [48/120    avg_loss:0.066, val_acc:0.936]
Epoch [49/120    avg_loss:0.063, val_acc:0.939]
Epoch [50/120    avg_loss:0.057, val_acc:0.942]
Epoch [51/120    avg_loss:0.066, val_acc:0.942]
Epoch [52/120    avg_loss:0.049, val_acc:0.953]
Epoch [53/120    avg_loss:0.053, val_acc:0.945]
Epoch [54/120    avg_loss:0.047, val_acc:0.957]
Epoch [55/120    avg_loss:0.047, val_acc:0.942]
Epoch [56/120    avg_loss:0.041, val_acc:0.951]
Epoch [57/120    avg_loss:0.045, val_acc:0.930]
Epoch [58/120    avg_loss:0.063, val_acc:0.938]
Epoch [59/120    avg_loss:0.048, val_acc:0.957]
Epoch [60/120    avg_loss:0.038, val_acc:0.946]
Epoch [61/120    avg_loss:0.041, val_acc:0.935]
Epoch [62/120    avg_loss:0.042, val_acc:0.948]
Epoch [63/120    avg_loss:0.058, val_acc:0.953]
Epoch [64/120    avg_loss:0.041, val_acc:0.954]
Epoch [65/120    avg_loss:0.031, val_acc:0.964]
Epoch [66/120    avg_loss:0.043, val_acc:0.952]
Epoch [67/120    avg_loss:0.045, val_acc:0.949]
Epoch [68/120    avg_loss:0.035, val_acc:0.963]
Epoch [69/120    avg_loss:0.037, val_acc:0.945]
Epoch [70/120    avg_loss:0.039, val_acc:0.959]
Epoch [71/120    avg_loss:0.030, val_acc:0.967]
Epoch [72/120    avg_loss:0.022, val_acc:0.963]
Epoch [73/120    avg_loss:0.039, val_acc:0.958]
Epoch [74/120    avg_loss:0.030, val_acc:0.934]
Epoch [75/120    avg_loss:0.050, val_acc:0.958]
Epoch [76/120    avg_loss:0.045, val_acc:0.960]
Epoch [77/120    avg_loss:0.024, val_acc:0.943]
Epoch [78/120    avg_loss:0.038, val_acc:0.928]
Epoch [79/120    avg_loss:0.044, val_acc:0.958]
Epoch [80/120    avg_loss:0.039, val_acc:0.957]
Epoch [81/120    avg_loss:0.022, val_acc:0.964]
Epoch [82/120    avg_loss:0.020, val_acc:0.966]
Epoch [83/120    avg_loss:0.018, val_acc:0.966]
Epoch [84/120    avg_loss:0.023, val_acc:0.966]
Epoch [85/120    avg_loss:0.018, val_acc:0.969]
Epoch [86/120    avg_loss:0.012, val_acc:0.972]
Epoch [87/120    avg_loss:0.013, val_acc:0.973]
Epoch [88/120    avg_loss:0.011, val_acc:0.973]
Epoch [89/120    avg_loss:0.010, val_acc:0.973]
Epoch [90/120    avg_loss:0.015, val_acc:0.973]
Epoch [91/120    avg_loss:0.010, val_acc:0.972]
Epoch [92/120    avg_loss:0.011, val_acc:0.972]
Epoch [93/120    avg_loss:0.016, val_acc:0.972]
Epoch [94/120    avg_loss:0.012, val_acc:0.972]
Epoch [95/120    avg_loss:0.013, val_acc:0.972]
Epoch [96/120    avg_loss:0.009, val_acc:0.971]
Epoch [97/120    avg_loss:0.013, val_acc:0.972]
Epoch [98/120    avg_loss:0.011, val_acc:0.973]
Epoch [99/120    avg_loss:0.013, val_acc:0.973]
Epoch [100/120    avg_loss:0.014, val_acc:0.971]
Epoch [101/120    avg_loss:0.010, val_acc:0.971]
Epoch [102/120    avg_loss:0.012, val_acc:0.972]
Epoch [103/120    avg_loss:0.009, val_acc:0.972]
Epoch [104/120    avg_loss:0.010, val_acc:0.972]
Epoch [105/120    avg_loss:0.011, val_acc:0.973]
Epoch [106/120    avg_loss:0.011, val_acc:0.974]
Epoch [107/120    avg_loss:0.009, val_acc:0.974]
Epoch [108/120    avg_loss:0.009, val_acc:0.973]
Epoch [109/120    avg_loss:0.009, val_acc:0.972]
Epoch [110/120    avg_loss:0.012, val_acc:0.973]
Epoch [111/120    avg_loss:0.009, val_acc:0.974]
Epoch [112/120    avg_loss:0.012, val_acc:0.974]
Epoch [113/120    avg_loss:0.010, val_acc:0.973]
Epoch [114/120    avg_loss:0.009, val_acc:0.974]
Epoch [115/120    avg_loss:0.008, val_acc:0.973]
Epoch [116/120    avg_loss:0.009, val_acc:0.973]
Epoch [117/120    avg_loss:0.008, val_acc:0.974]
Epoch [118/120    avg_loss:0.009, val_acc:0.973]
Epoch [119/120    avg_loss:0.009, val_acc:0.974]
Epoch [120/120    avg_loss:0.012, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1247   10    7    0    0    0    0    0    4   17    0    0
     0    0    0]
 [   0    0    0  720    4    0    2    0    0    6    2   11    2    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    5    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    0    0    0    0    0  845   19    3    0
     0    0    0]
 [   0    0    7    0    0    9    3    0    0    0   28 2144   19    0
     0    0    0]
 [   0    0    0    6    0    0    0    0    0    0    0    2  522    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1119   19    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    35  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.20325203252033

F1 scores:
[       nan 0.98765432 0.97919121 0.9703504  0.97247706 0.98514286
 0.98190045 1.         0.997669   0.8372093  0.96296296 0.97277677
 0.96221198 1.         0.97389034 0.89655172 0.97005988]

Kappa:
0.9681253593103877
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa7120d0828>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.723, val_acc:0.183]
Epoch [2/120    avg_loss:2.402, val_acc:0.389]
Epoch [3/120    avg_loss:2.230, val_acc:0.508]
Epoch [4/120    avg_loss:2.059, val_acc:0.520]
Epoch [5/120    avg_loss:1.949, val_acc:0.576]
Epoch [6/120    avg_loss:1.792, val_acc:0.600]
Epoch [7/120    avg_loss:1.689, val_acc:0.601]
Epoch [8/120    avg_loss:1.571, val_acc:0.654]
Epoch [9/120    avg_loss:1.444, val_acc:0.648]
Epoch [10/120    avg_loss:1.297, val_acc:0.645]
Epoch [11/120    avg_loss:1.195, val_acc:0.709]
Epoch [12/120    avg_loss:1.094, val_acc:0.698]
Epoch [13/120    avg_loss:0.958, val_acc:0.723]
Epoch [14/120    avg_loss:0.867, val_acc:0.752]
Epoch [15/120    avg_loss:0.869, val_acc:0.766]
Epoch [16/120    avg_loss:0.710, val_acc:0.786]
Epoch [17/120    avg_loss:0.735, val_acc:0.779]
Epoch [18/120    avg_loss:0.635, val_acc:0.766]
Epoch [19/120    avg_loss:0.576, val_acc:0.799]
Epoch [20/120    avg_loss:0.531, val_acc:0.805]
Epoch [21/120    avg_loss:0.514, val_acc:0.820]
Epoch [22/120    avg_loss:0.411, val_acc:0.853]
Epoch [23/120    avg_loss:0.381, val_acc:0.858]
Epoch [24/120    avg_loss:0.345, val_acc:0.870]
Epoch [25/120    avg_loss:0.297, val_acc:0.898]
Epoch [26/120    avg_loss:0.308, val_acc:0.896]
Epoch [27/120    avg_loss:0.246, val_acc:0.901]
Epoch [28/120    avg_loss:0.221, val_acc:0.883]
Epoch [29/120    avg_loss:0.210, val_acc:0.891]
Epoch [30/120    avg_loss:0.208, val_acc:0.918]
Epoch [31/120    avg_loss:0.164, val_acc:0.918]
Epoch [32/120    avg_loss:0.206, val_acc:0.914]
Epoch [33/120    avg_loss:0.192, val_acc:0.925]
Epoch [34/120    avg_loss:0.151, val_acc:0.893]
Epoch [35/120    avg_loss:0.148, val_acc:0.940]
Epoch [36/120    avg_loss:0.122, val_acc:0.942]
Epoch [37/120    avg_loss:0.129, val_acc:0.939]
Epoch [38/120    avg_loss:0.145, val_acc:0.947]
Epoch [39/120    avg_loss:0.142, val_acc:0.901]
Epoch [40/120    avg_loss:0.098, val_acc:0.939]
Epoch [41/120    avg_loss:0.124, val_acc:0.954]
Epoch [42/120    avg_loss:0.098, val_acc:0.946]
Epoch [43/120    avg_loss:0.109, val_acc:0.951]
Epoch [44/120    avg_loss:0.107, val_acc:0.946]
Epoch [45/120    avg_loss:0.100, val_acc:0.950]
Epoch [46/120    avg_loss:0.087, val_acc:0.946]
Epoch [47/120    avg_loss:0.069, val_acc:0.944]
Epoch [48/120    avg_loss:0.075, val_acc:0.961]
Epoch [49/120    avg_loss:0.071, val_acc:0.957]
Epoch [50/120    avg_loss:0.056, val_acc:0.968]
Epoch [51/120    avg_loss:0.059, val_acc:0.964]
Epoch [52/120    avg_loss:0.063, val_acc:0.963]
Epoch [53/120    avg_loss:0.061, val_acc:0.955]
Epoch [54/120    avg_loss:0.051, val_acc:0.958]
Epoch [55/120    avg_loss:0.047, val_acc:0.969]
Epoch [56/120    avg_loss:0.036, val_acc:0.969]
Epoch [57/120    avg_loss:0.046, val_acc:0.960]
Epoch [58/120    avg_loss:0.071, val_acc:0.961]
Epoch [59/120    avg_loss:0.059, val_acc:0.967]
Epoch [60/120    avg_loss:0.072, val_acc:0.956]
Epoch [61/120    avg_loss:0.070, val_acc:0.961]
Epoch [62/120    avg_loss:0.060, val_acc:0.957]
Epoch [63/120    avg_loss:0.051, val_acc:0.959]
Epoch [64/120    avg_loss:0.059, val_acc:0.968]
Epoch [65/120    avg_loss:0.045, val_acc:0.968]
Epoch [66/120    avg_loss:0.057, val_acc:0.976]
Epoch [67/120    avg_loss:0.047, val_acc:0.956]
Epoch [68/120    avg_loss:0.062, val_acc:0.950]
Epoch [69/120    avg_loss:0.052, val_acc:0.971]
Epoch [70/120    avg_loss:0.085, val_acc:0.960]
Epoch [71/120    avg_loss:0.056, val_acc:0.948]
Epoch [72/120    avg_loss:0.054, val_acc:0.969]
Epoch [73/120    avg_loss:0.030, val_acc:0.977]
Epoch [74/120    avg_loss:0.026, val_acc:0.971]
Epoch [75/120    avg_loss:0.029, val_acc:0.969]
Epoch [76/120    avg_loss:0.058, val_acc:0.953]
Epoch [77/120    avg_loss:0.038, val_acc:0.976]
Epoch [78/120    avg_loss:0.042, val_acc:0.953]
Epoch [79/120    avg_loss:0.042, val_acc:0.972]
Epoch [80/120    avg_loss:0.041, val_acc:0.972]
Epoch [81/120    avg_loss:0.032, val_acc:0.978]
Epoch [82/120    avg_loss:0.022, val_acc:0.979]
Epoch [83/120    avg_loss:0.029, val_acc:0.978]
Epoch [84/120    avg_loss:0.022, val_acc:0.972]
Epoch [85/120    avg_loss:0.032, val_acc:0.971]
Epoch [86/120    avg_loss:0.019, val_acc:0.980]
Epoch [87/120    avg_loss:0.013, val_acc:0.978]
Epoch [88/120    avg_loss:0.012, val_acc:0.979]
Epoch [89/120    avg_loss:0.036, val_acc:0.963]
Epoch [90/120    avg_loss:0.068, val_acc:0.969]
Epoch [91/120    avg_loss:0.046, val_acc:0.958]
Epoch [92/120    avg_loss:0.036, val_acc:0.973]
Epoch [93/120    avg_loss:0.030, val_acc:0.976]
Epoch [94/120    avg_loss:0.021, val_acc:0.975]
Epoch [95/120    avg_loss:0.021, val_acc:0.978]
Epoch [96/120    avg_loss:0.026, val_acc:0.985]
Epoch [97/120    avg_loss:0.015, val_acc:0.980]
Epoch [98/120    avg_loss:0.013, val_acc:0.982]
Epoch [99/120    avg_loss:0.014, val_acc:0.982]
Epoch [100/120    avg_loss:0.013, val_acc:0.971]
Epoch [101/120    avg_loss:0.013, val_acc:0.981]
Epoch [102/120    avg_loss:0.010, val_acc:0.984]
Epoch [103/120    avg_loss:0.011, val_acc:0.979]
Epoch [104/120    avg_loss:0.012, val_acc:0.981]
Epoch [105/120    avg_loss:0.010, val_acc:0.982]
Epoch [106/120    avg_loss:0.009, val_acc:0.983]
Epoch [107/120    avg_loss:0.012, val_acc:0.977]
Epoch [108/120    avg_loss:0.010, val_acc:0.982]
Epoch [109/120    avg_loss:0.011, val_acc:0.983]
Epoch [110/120    avg_loss:0.008, val_acc:0.983]
Epoch [111/120    avg_loss:0.009, val_acc:0.985]
Epoch [112/120    avg_loss:0.007, val_acc:0.985]
Epoch [113/120    avg_loss:0.007, val_acc:0.985]
Epoch [114/120    avg_loss:0.011, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.006, val_acc:0.985]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1274    1    0    0    1    0    0    0    1    8    0    0
     0    0    0]
 [   0    0    2  721    0    0    1    0    0    3    0   11    8    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    3    4    0    0    1    0    0    0
     1    0    0]
 [   0    0    1    0    0    0  656    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    1    0    0    0    0  852   18    0    0
     2    0    0]
 [   0    0    5    0    0    0    0    0    0    2   12 2169   22    0
     0    0    0]
 [   0    0    1    4    0    0    0    0    0    0    0    6  520    0
     2    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1126   12    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    65  280    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.7560975609756

F1 scores:
[       nan 0.975      0.99143969 0.97895451 1.         0.98725377
 0.99393939 0.92592593 1.         0.87804878 0.97762478 0.98100407
 0.95940959 1.         0.9640411  0.87636933 0.99408284]

Kappa:
0.9744040266306695
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa44582f898>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.729, val_acc:0.325]
Epoch [2/120    avg_loss:2.402, val_acc:0.432]
Epoch [3/120    avg_loss:2.228, val_acc:0.498]
Epoch [4/120    avg_loss:2.055, val_acc:0.550]
Epoch [5/120    avg_loss:1.905, val_acc:0.553]
Epoch [6/120    avg_loss:1.755, val_acc:0.568]
Epoch [7/120    avg_loss:1.642, val_acc:0.584]
Epoch [8/120    avg_loss:1.512, val_acc:0.606]
Epoch [9/120    avg_loss:1.392, val_acc:0.606]
Epoch [10/120    avg_loss:1.351, val_acc:0.577]
Epoch [11/120    avg_loss:1.361, val_acc:0.610]
Epoch [12/120    avg_loss:1.238, val_acc:0.622]
Epoch [13/120    avg_loss:1.050, val_acc:0.682]
Epoch [14/120    avg_loss:0.937, val_acc:0.711]
Epoch [15/120    avg_loss:0.944, val_acc:0.720]
Epoch [16/120    avg_loss:0.785, val_acc:0.751]
Epoch [17/120    avg_loss:0.687, val_acc:0.796]
Epoch [18/120    avg_loss:0.605, val_acc:0.811]
Epoch [19/120    avg_loss:0.718, val_acc:0.727]
Epoch [20/120    avg_loss:0.706, val_acc:0.789]
Epoch [21/120    avg_loss:0.588, val_acc:0.784]
Epoch [22/120    avg_loss:0.506, val_acc:0.810]
Epoch [23/120    avg_loss:0.409, val_acc:0.878]
Epoch [24/120    avg_loss:0.375, val_acc:0.878]
Epoch [25/120    avg_loss:0.338, val_acc:0.894]
Epoch [26/120    avg_loss:0.290, val_acc:0.908]
Epoch [27/120    avg_loss:0.273, val_acc:0.892]
Epoch [28/120    avg_loss:0.258, val_acc:0.889]
Epoch [29/120    avg_loss:0.268, val_acc:0.882]
Epoch [30/120    avg_loss:0.285, val_acc:0.924]
Epoch [31/120    avg_loss:0.214, val_acc:0.921]
Epoch [32/120    avg_loss:0.187, val_acc:0.930]
Epoch [33/120    avg_loss:0.181, val_acc:0.936]
Epoch [34/120    avg_loss:0.172, val_acc:0.943]
Epoch [35/120    avg_loss:0.135, val_acc:0.929]
Epoch [36/120    avg_loss:0.141, val_acc:0.928]
Epoch [37/120    avg_loss:0.161, val_acc:0.941]
Epoch [38/120    avg_loss:0.144, val_acc:0.935]
Epoch [39/120    avg_loss:0.134, val_acc:0.897]
Epoch [40/120    avg_loss:1.483, val_acc:0.777]
Epoch [41/120    avg_loss:0.639, val_acc:0.801]
Epoch [42/120    avg_loss:0.348, val_acc:0.882]
Epoch [43/120    avg_loss:0.274, val_acc:0.915]
Epoch [44/120    avg_loss:0.208, val_acc:0.922]
Epoch [45/120    avg_loss:0.156, val_acc:0.926]
Epoch [46/120    avg_loss:0.158, val_acc:0.916]
Epoch [47/120    avg_loss:0.188, val_acc:0.906]
Epoch [48/120    avg_loss:0.146, val_acc:0.928]
Epoch [49/120    avg_loss:0.122, val_acc:0.944]
Epoch [50/120    avg_loss:0.111, val_acc:0.944]
Epoch [51/120    avg_loss:0.104, val_acc:0.946]
Epoch [52/120    avg_loss:0.099, val_acc:0.946]
Epoch [53/120    avg_loss:0.102, val_acc:0.948]
Epoch [54/120    avg_loss:0.092, val_acc:0.946]
Epoch [55/120    avg_loss:0.094, val_acc:0.947]
Epoch [56/120    avg_loss:0.089, val_acc:0.949]
Epoch [57/120    avg_loss:0.079, val_acc:0.948]
Epoch [58/120    avg_loss:0.088, val_acc:0.946]
Epoch [59/120    avg_loss:0.087, val_acc:0.949]
Epoch [60/120    avg_loss:0.085, val_acc:0.951]
Epoch [61/120    avg_loss:0.080, val_acc:0.952]
Epoch [62/120    avg_loss:0.072, val_acc:0.951]
Epoch [63/120    avg_loss:0.082, val_acc:0.952]
Epoch [64/120    avg_loss:0.083, val_acc:0.952]
Epoch [65/120    avg_loss:0.073, val_acc:0.953]
Epoch [66/120    avg_loss:0.076, val_acc:0.952]
Epoch [67/120    avg_loss:0.076, val_acc:0.956]
Epoch [68/120    avg_loss:0.075, val_acc:0.954]
Epoch [69/120    avg_loss:0.070, val_acc:0.958]
Epoch [70/120    avg_loss:0.075, val_acc:0.955]
Epoch [71/120    avg_loss:0.067, val_acc:0.955]
Epoch [72/120    avg_loss:0.071, val_acc:0.954]
Epoch [73/120    avg_loss:0.063, val_acc:0.956]
Epoch [74/120    avg_loss:0.071, val_acc:0.957]
Epoch [75/120    avg_loss:0.076, val_acc:0.956]
Epoch [76/120    avg_loss:0.063, val_acc:0.954]
Epoch [77/120    avg_loss:0.073, val_acc:0.956]
Epoch [78/120    avg_loss:0.069, val_acc:0.959]
Epoch [79/120    avg_loss:0.062, val_acc:0.958]
Epoch [80/120    avg_loss:0.062, val_acc:0.957]
Epoch [81/120    avg_loss:0.065, val_acc:0.957]
Epoch [82/120    avg_loss:0.062, val_acc:0.958]
Epoch [83/120    avg_loss:0.063, val_acc:0.958]
Epoch [84/120    avg_loss:0.065, val_acc:0.957]
Epoch [85/120    avg_loss:0.067, val_acc:0.958]
Epoch [86/120    avg_loss:0.070, val_acc:0.960]
Epoch [87/120    avg_loss:0.069, val_acc:0.957]
Epoch [88/120    avg_loss:0.065, val_acc:0.959]
Epoch [89/120    avg_loss:0.057, val_acc:0.959]
Epoch [90/120    avg_loss:0.063, val_acc:0.957]
Epoch [91/120    avg_loss:0.050, val_acc:0.960]
Epoch [92/120    avg_loss:0.063, val_acc:0.961]
Epoch [93/120    avg_loss:0.062, val_acc:0.961]
Epoch [94/120    avg_loss:0.056, val_acc:0.960]
Epoch [95/120    avg_loss:0.051, val_acc:0.963]
Epoch [96/120    avg_loss:0.058, val_acc:0.961]
Epoch [97/120    avg_loss:0.058, val_acc:0.959]
Epoch [98/120    avg_loss:0.070, val_acc:0.961]
Epoch [99/120    avg_loss:0.054, val_acc:0.961]
Epoch [100/120    avg_loss:0.060, val_acc:0.959]
Epoch [101/120    avg_loss:0.053, val_acc:0.961]
Epoch [102/120    avg_loss:0.054, val_acc:0.963]
Epoch [103/120    avg_loss:0.047, val_acc:0.959]
Epoch [104/120    avg_loss:0.053, val_acc:0.958]
Epoch [105/120    avg_loss:0.053, val_acc:0.961]
Epoch [106/120    avg_loss:0.057, val_acc:0.964]
Epoch [107/120    avg_loss:0.051, val_acc:0.965]
Epoch [108/120    avg_loss:0.047, val_acc:0.958]
Epoch [109/120    avg_loss:0.052, val_acc:0.961]
Epoch [110/120    avg_loss:0.047, val_acc:0.961]
Epoch [111/120    avg_loss:0.061, val_acc:0.960]
Epoch [112/120    avg_loss:0.045, val_acc:0.959]
Epoch [113/120    avg_loss:0.051, val_acc:0.960]
Epoch [114/120    avg_loss:0.044, val_acc:0.963]
Epoch [115/120    avg_loss:0.048, val_acc:0.959]
Epoch [116/120    avg_loss:0.044, val_acc:0.960]
Epoch [117/120    avg_loss:0.048, val_acc:0.963]
Epoch [118/120    avg_loss:0.047, val_acc:0.960]
Epoch [119/120    avg_loss:0.044, val_acc:0.964]
Epoch [120/120    avg_loss:0.044, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1252    6    7    0    0    0    0    0    6   14    0    0
     0    0    0]
 [   0    0    0  728    1    1    1    0    0    1    5    8    2    0
     0    0    0]
 [   0    0    0    2  209    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   12    0    0    1    0    0    0    1  825   35    0    0
     0    1    0]
 [   0    0   27    0    0    1    0    0    0    1   47 2119   11    0
     0    3    1]
 [   0    0    0    5    1    0    0    0    0    0    2    0  520    0
     0    3    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1127   12    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    78  269    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.6829268292683

F1 scores:
[       nan 0.98765432 0.97167249 0.97849462 0.96983759 0.99427262
 0.99771516 1.         1.         0.92307692 0.9375     0.96603602
 0.97287184 0.99728997 0.96078431 0.84724409 0.97674419]

Kappa:
0.962182494419545
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f881e38c8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.774, val_acc:0.179]
Epoch [2/120    avg_loss:2.454, val_acc:0.427]
Epoch [3/120    avg_loss:2.245, val_acc:0.441]
Epoch [4/120    avg_loss:2.109, val_acc:0.476]
Epoch [5/120    avg_loss:1.924, val_acc:0.420]
Epoch [6/120    avg_loss:1.822, val_acc:0.469]
Epoch [7/120    avg_loss:1.717, val_acc:0.515]
Epoch [8/120    avg_loss:1.591, val_acc:0.594]
Epoch [9/120    avg_loss:1.495, val_acc:0.584]
Epoch [10/120    avg_loss:1.412, val_acc:0.584]
Epoch [11/120    avg_loss:1.275, val_acc:0.620]
Epoch [12/120    avg_loss:1.203, val_acc:0.664]
Epoch [13/120    avg_loss:1.091, val_acc:0.654]
Epoch [14/120    avg_loss:0.933, val_acc:0.736]
Epoch [15/120    avg_loss:0.891, val_acc:0.745]
Epoch [16/120    avg_loss:0.765, val_acc:0.730]
Epoch [17/120    avg_loss:0.773, val_acc:0.742]
Epoch [18/120    avg_loss:0.707, val_acc:0.748]
Epoch [19/120    avg_loss:0.612, val_acc:0.770]
Epoch [20/120    avg_loss:0.594, val_acc:0.830]
Epoch [21/120    avg_loss:0.478, val_acc:0.769]
Epoch [22/120    avg_loss:0.507, val_acc:0.811]
Epoch [23/120    avg_loss:0.458, val_acc:0.831]
Epoch [24/120    avg_loss:0.451, val_acc:0.811]
Epoch [25/120    avg_loss:0.360, val_acc:0.806]
Epoch [26/120    avg_loss:0.365, val_acc:0.884]
Epoch [27/120    avg_loss:0.315, val_acc:0.873]
Epoch [28/120    avg_loss:0.265, val_acc:0.891]
Epoch [29/120    avg_loss:0.234, val_acc:0.882]
Epoch [30/120    avg_loss:0.298, val_acc:0.898]
Epoch [31/120    avg_loss:0.215, val_acc:0.917]
Epoch [32/120    avg_loss:0.201, val_acc:0.891]
Epoch [33/120    avg_loss:0.212, val_acc:0.882]
Epoch [34/120    avg_loss:0.171, val_acc:0.927]
Epoch [35/120    avg_loss:0.193, val_acc:0.900]
Epoch [36/120    avg_loss:0.164, val_acc:0.929]
Epoch [37/120    avg_loss:0.128, val_acc:0.929]
Epoch [38/120    avg_loss:0.137, val_acc:0.933]
Epoch [39/120    avg_loss:0.136, val_acc:0.940]
Epoch [40/120    avg_loss:0.106, val_acc:0.935]
Epoch [41/120    avg_loss:0.101, val_acc:0.931]
Epoch [42/120    avg_loss:0.097, val_acc:0.953]
Epoch [43/120    avg_loss:0.082, val_acc:0.949]
Epoch [44/120    avg_loss:0.106, val_acc:0.949]
Epoch [45/120    avg_loss:0.115, val_acc:0.947]
Epoch [46/120    avg_loss:0.111, val_acc:0.950]
Epoch [47/120    avg_loss:0.086, val_acc:0.939]
Epoch [48/120    avg_loss:0.097, val_acc:0.954]
Epoch [49/120    avg_loss:0.104, val_acc:0.922]
Epoch [50/120    avg_loss:0.083, val_acc:0.941]
Epoch [51/120    avg_loss:0.081, val_acc:0.873]
Epoch [52/120    avg_loss:0.137, val_acc:0.938]
Epoch [53/120    avg_loss:0.078, val_acc:0.959]
Epoch [54/120    avg_loss:0.077, val_acc:0.954]
Epoch [55/120    avg_loss:0.060, val_acc:0.964]
Epoch [56/120    avg_loss:0.054, val_acc:0.968]
Epoch [57/120    avg_loss:0.052, val_acc:0.966]
Epoch [58/120    avg_loss:0.053, val_acc:0.959]
Epoch [59/120    avg_loss:0.067, val_acc:0.965]
Epoch [60/120    avg_loss:0.042, val_acc:0.972]
Epoch [61/120    avg_loss:0.047, val_acc:0.969]
Epoch [62/120    avg_loss:0.110, val_acc:0.944]
Epoch [63/120    avg_loss:0.060, val_acc:0.950]
Epoch [64/120    avg_loss:0.062, val_acc:0.959]
Epoch [65/120    avg_loss:0.047, val_acc:0.963]
Epoch [66/120    avg_loss:0.037, val_acc:0.966]
Epoch [67/120    avg_loss:0.032, val_acc:0.971]
Epoch [68/120    avg_loss:0.035, val_acc:0.964]
Epoch [69/120    avg_loss:0.033, val_acc:0.971]
Epoch [70/120    avg_loss:0.028, val_acc:0.969]
Epoch [71/120    avg_loss:0.036, val_acc:0.971]
Epoch [72/120    avg_loss:0.029, val_acc:0.968]
Epoch [73/120    avg_loss:0.028, val_acc:0.960]
Epoch [74/120    avg_loss:0.024, val_acc:0.978]
Epoch [75/120    avg_loss:0.020, val_acc:0.978]
Epoch [76/120    avg_loss:0.020, val_acc:0.982]
Epoch [77/120    avg_loss:0.022, val_acc:0.977]
Epoch [78/120    avg_loss:0.017, val_acc:0.979]
Epoch [79/120    avg_loss:0.018, val_acc:0.980]
Epoch [80/120    avg_loss:0.018, val_acc:0.977]
Epoch [81/120    avg_loss:0.017, val_acc:0.979]
Epoch [82/120    avg_loss:0.017, val_acc:0.980]
Epoch [83/120    avg_loss:0.019, val_acc:0.979]
Epoch [84/120    avg_loss:0.017, val_acc:0.978]
Epoch [85/120    avg_loss:0.015, val_acc:0.979]
Epoch [86/120    avg_loss:0.016, val_acc:0.979]
Epoch [87/120    avg_loss:0.018, val_acc:0.978]
Epoch [88/120    avg_loss:0.018, val_acc:0.979]
Epoch [89/120    avg_loss:0.018, val_acc:0.977]
Epoch [90/120    avg_loss:0.016, val_acc:0.977]
Epoch [91/120    avg_loss:0.018, val_acc:0.977]
Epoch [92/120    avg_loss:0.019, val_acc:0.977]
Epoch [93/120    avg_loss:0.019, val_acc:0.977]
Epoch [94/120    avg_loss:0.017, val_acc:0.978]
Epoch [95/120    avg_loss:0.015, val_acc:0.978]
Epoch [96/120    avg_loss:0.018, val_acc:0.978]
Epoch [97/120    avg_loss:0.016, val_acc:0.979]
Epoch [98/120    avg_loss:0.019, val_acc:0.980]
Epoch [99/120    avg_loss:0.015, val_acc:0.980]
Epoch [100/120    avg_loss:0.015, val_acc:0.981]
Epoch [101/120    avg_loss:0.016, val_acc:0.981]
Epoch [102/120    avg_loss:0.016, val_acc:0.980]
Epoch [103/120    avg_loss:0.015, val_acc:0.981]
Epoch [104/120    avg_loss:0.016, val_acc:0.980]
Epoch [105/120    avg_loss:0.016, val_acc:0.981]
Epoch [106/120    avg_loss:0.016, val_acc:0.981]
Epoch [107/120    avg_loss:0.017, val_acc:0.981]
Epoch [108/120    avg_loss:0.015, val_acc:0.981]
Epoch [109/120    avg_loss:0.020, val_acc:0.981]
Epoch [110/120    avg_loss:0.015, val_acc:0.981]
Epoch [111/120    avg_loss:0.016, val_acc:0.981]
Epoch [112/120    avg_loss:0.015, val_acc:0.981]
Epoch [113/120    avg_loss:0.018, val_acc:0.980]
Epoch [114/120    avg_loss:0.020, val_acc:0.980]
Epoch [115/120    avg_loss:0.014, val_acc:0.980]
Epoch [116/120    avg_loss:0.016, val_acc:0.980]
Epoch [117/120    avg_loss:0.014, val_acc:0.980]
Epoch [118/120    avg_loss:0.015, val_acc:0.980]
Epoch [119/120    avg_loss:0.016, val_acc:0.980]
Epoch [120/120    avg_loss:0.017, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0 1236    7   14    1    0    0    0    0    1   24    2    0
     0    0    0]
 [   0    0    3  726    8    1    0    0    0    2    1    1    4    0
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    3    0    0    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    1    1    0    0    0  835   31    0    0
     0    0    0]
 [   0    0   19    0    0    3    2    0    0    0    5 2168   12    0
     0    1    0]
 [   0    0    0    2    0    0    0    0    0    0    0    8  520    0
     1    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1123   16    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    62  275    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.02981029810299

F1 scores:
[       nan 0.975      0.96903175 0.97909643 0.94854586 0.98269896
 0.98642534 0.94339623 1.         0.88888889 0.97262667 0.97569757
 0.96654275 1.         0.96270896 0.859375   0.97005988]

Kappa:
0.9661141366609569
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd0180e47f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.738, val_acc:0.248]
Epoch [2/120    avg_loss:2.424, val_acc:0.384]
Epoch [3/120    avg_loss:2.235, val_acc:0.448]
Epoch [4/120    avg_loss:2.055, val_acc:0.487]
Epoch [5/120    avg_loss:1.927, val_acc:0.530]
Epoch [6/120    avg_loss:1.819, val_acc:0.552]
Epoch [7/120    avg_loss:1.673, val_acc:0.569]
Epoch [8/120    avg_loss:1.552, val_acc:0.592]
Epoch [9/120    avg_loss:1.417, val_acc:0.607]
Epoch [10/120    avg_loss:1.336, val_acc:0.610]
Epoch [11/120    avg_loss:1.199, val_acc:0.653]
Epoch [12/120    avg_loss:1.096, val_acc:0.680]
Epoch [13/120    avg_loss:0.962, val_acc:0.692]
Epoch [14/120    avg_loss:0.879, val_acc:0.711]
Epoch [15/120    avg_loss:0.770, val_acc:0.730]
Epoch [16/120    avg_loss:0.662, val_acc:0.753]
Epoch [17/120    avg_loss:0.633, val_acc:0.767]
Epoch [18/120    avg_loss:0.630, val_acc:0.773]
Epoch [19/120    avg_loss:0.508, val_acc:0.791]
Epoch [20/120    avg_loss:0.476, val_acc:0.814]
Epoch [21/120    avg_loss:0.355, val_acc:0.818]
Epoch [22/120    avg_loss:0.323, val_acc:0.861]
Epoch [23/120    avg_loss:0.298, val_acc:0.842]
Epoch [24/120    avg_loss:0.318, val_acc:0.836]
Epoch [25/120    avg_loss:0.291, val_acc:0.867]
Epoch [26/120    avg_loss:0.295, val_acc:0.856]
Epoch [27/120    avg_loss:0.280, val_acc:0.809]
Epoch [28/120    avg_loss:0.489, val_acc:0.757]
Epoch [29/120    avg_loss:0.397, val_acc:0.833]
Epoch [30/120    avg_loss:0.324, val_acc:0.863]
Epoch [31/120    avg_loss:0.260, val_acc:0.851]
Epoch [32/120    avg_loss:0.222, val_acc:0.883]
Epoch [33/120    avg_loss:0.155, val_acc:0.894]
Epoch [34/120    avg_loss:0.167, val_acc:0.899]
Epoch [35/120    avg_loss:0.154, val_acc:0.904]
Epoch [36/120    avg_loss:0.133, val_acc:0.908]
Epoch [37/120    avg_loss:0.130, val_acc:0.923]
Epoch [38/120    avg_loss:0.138, val_acc:0.919]
Epoch [39/120    avg_loss:0.124, val_acc:0.916]
Epoch [40/120    avg_loss:0.146, val_acc:0.909]
Epoch [41/120    avg_loss:0.132, val_acc:0.931]
Epoch [42/120    avg_loss:0.112, val_acc:0.933]
Epoch [43/120    avg_loss:0.100, val_acc:0.892]
Epoch [44/120    avg_loss:0.091, val_acc:0.932]
Epoch [45/120    avg_loss:0.092, val_acc:0.931]
Epoch [46/120    avg_loss:0.088, val_acc:0.940]
Epoch [47/120    avg_loss:0.083, val_acc:0.927]
Epoch [48/120    avg_loss:0.089, val_acc:0.910]
Epoch [49/120    avg_loss:0.141, val_acc:0.915]
Epoch [50/120    avg_loss:0.099, val_acc:0.917]
Epoch [51/120    avg_loss:0.075, val_acc:0.948]
Epoch [52/120    avg_loss:0.069, val_acc:0.934]
Epoch [53/120    avg_loss:0.069, val_acc:0.952]
Epoch [54/120    avg_loss:0.079, val_acc:0.942]
Epoch [55/120    avg_loss:0.063, val_acc:0.940]
Epoch [56/120    avg_loss:0.063, val_acc:0.952]
Epoch [57/120    avg_loss:0.051, val_acc:0.948]
Epoch [58/120    avg_loss:0.058, val_acc:0.939]
Epoch [59/120    avg_loss:0.062, val_acc:0.890]
Epoch [60/120    avg_loss:0.061, val_acc:0.939]
Epoch [61/120    avg_loss:0.051, val_acc:0.955]
Epoch [62/120    avg_loss:0.041, val_acc:0.945]
Epoch [63/120    avg_loss:0.044, val_acc:0.943]
Epoch [64/120    avg_loss:0.065, val_acc:0.934]
Epoch [65/120    avg_loss:0.044, val_acc:0.954]
Epoch [66/120    avg_loss:0.051, val_acc:0.955]
Epoch [67/120    avg_loss:0.045, val_acc:0.948]
Epoch [68/120    avg_loss:0.034, val_acc:0.958]
Epoch [69/120    avg_loss:0.033, val_acc:0.952]
Epoch [70/120    avg_loss:0.035, val_acc:0.964]
Epoch [71/120    avg_loss:0.032, val_acc:0.947]
Epoch [72/120    avg_loss:0.027, val_acc:0.963]
Epoch [73/120    avg_loss:0.019, val_acc:0.964]
Epoch [74/120    avg_loss:0.028, val_acc:0.957]
Epoch [75/120    avg_loss:0.020, val_acc:0.960]
Epoch [76/120    avg_loss:0.017, val_acc:0.961]
Epoch [77/120    avg_loss:0.017, val_acc:0.966]
Epoch [78/120    avg_loss:0.020, val_acc:0.966]
Epoch [79/120    avg_loss:0.025, val_acc:0.957]
Epoch [80/120    avg_loss:0.030, val_acc:0.953]
Epoch [81/120    avg_loss:0.020, val_acc:0.963]
Epoch [82/120    avg_loss:0.017, val_acc:0.967]
Epoch [83/120    avg_loss:0.026, val_acc:0.963]
Epoch [84/120    avg_loss:0.030, val_acc:0.940]
Epoch [85/120    avg_loss:0.029, val_acc:0.967]
Epoch [86/120    avg_loss:0.023, val_acc:0.966]
Epoch [87/120    avg_loss:0.017, val_acc:0.963]
Epoch [88/120    avg_loss:0.020, val_acc:0.961]
Epoch [89/120    avg_loss:0.017, val_acc:0.972]
Epoch [90/120    avg_loss:0.018, val_acc:0.960]
Epoch [91/120    avg_loss:0.015, val_acc:0.967]
Epoch [92/120    avg_loss:0.012, val_acc:0.971]
Epoch [93/120    avg_loss:0.013, val_acc:0.967]
Epoch [94/120    avg_loss:0.015, val_acc:0.966]
Epoch [95/120    avg_loss:0.013, val_acc:0.969]
Epoch [96/120    avg_loss:0.014, val_acc:0.967]
Epoch [97/120    avg_loss:0.011, val_acc:0.974]
Epoch [98/120    avg_loss:0.025, val_acc:0.955]
Epoch [99/120    avg_loss:0.024, val_acc:0.955]
Epoch [100/120    avg_loss:0.087, val_acc:0.866]
Epoch [101/120    avg_loss:0.176, val_acc:0.905]
Epoch [102/120    avg_loss:0.226, val_acc:0.841]
Epoch [103/120    avg_loss:0.229, val_acc:0.910]
Epoch [104/120    avg_loss:0.108, val_acc:0.941]
Epoch [105/120    avg_loss:0.082, val_acc:0.932]
Epoch [106/120    avg_loss:0.078, val_acc:0.915]
Epoch [107/120    avg_loss:0.134, val_acc:0.945]
Epoch [108/120    avg_loss:0.086, val_acc:0.936]
Epoch [109/120    avg_loss:0.075, val_acc:0.943]
Epoch [110/120    avg_loss:0.071, val_acc:0.934]
Epoch [111/120    avg_loss:0.063, val_acc:0.943]
Epoch [112/120    avg_loss:0.043, val_acc:0.945]
Epoch [113/120    avg_loss:0.043, val_acc:0.952]
Epoch [114/120    avg_loss:0.038, val_acc:0.955]
Epoch [115/120    avg_loss:0.034, val_acc:0.955]
Epoch [116/120    avg_loss:0.032, val_acc:0.958]
Epoch [117/120    avg_loss:0.029, val_acc:0.956]
Epoch [118/120    avg_loss:0.026, val_acc:0.956]
Epoch [119/120    avg_loss:0.028, val_acc:0.958]
Epoch [120/120    avg_loss:0.028, val_acc:0.960]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    1    0    0    0    0    0    2    0    0    0    1    0
     0    0    0]
 [   0    0 1249    6    3    0    0    0    0    0    4   23    0    0
     0    0    0]
 [   0    0    2  724    7    0    0    0    0    0    2    8    4    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  849   22    0    0
     0    0    0]
 [   0    0   25    0    0    1    0    0    0    0   40 2130   13    0
     1    0    0]
 [   0    0    2    2    0    1    0    0    0    0    1    0  523    0
     4    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    36  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
97.25745257452574

F1 scores:
[       nan 0.94871795 0.97274143 0.97837838 0.97471264 0.99655568
 0.99015897 1.         0.99767981 0.97142857 0.95878035 0.96906278
 0.96672828 1.         0.97527115 0.90799397 0.95705521]

Kappa:
0.9687379188734445
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc0e2da98d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.749, val_acc:0.385]
Epoch [2/120    avg_loss:2.484, val_acc:0.486]
Epoch [3/120    avg_loss:2.275, val_acc:0.489]
Epoch [4/120    avg_loss:2.086, val_acc:0.515]
Epoch [5/120    avg_loss:1.952, val_acc:0.565]
Epoch [6/120    avg_loss:1.797, val_acc:0.567]
Epoch [7/120    avg_loss:1.661, val_acc:0.623]
Epoch [8/120    avg_loss:1.532, val_acc:0.619]
Epoch [9/120    avg_loss:1.398, val_acc:0.680]
Epoch [10/120    avg_loss:1.280, val_acc:0.674]
Epoch [11/120    avg_loss:1.199, val_acc:0.683]
Epoch [12/120    avg_loss:1.116, val_acc:0.718]
Epoch [13/120    avg_loss:0.938, val_acc:0.752]
Epoch [14/120    avg_loss:0.865, val_acc:0.730]
Epoch [15/120    avg_loss:0.806, val_acc:0.743]
Epoch [16/120    avg_loss:0.682, val_acc:0.808]
Epoch [17/120    avg_loss:0.649, val_acc:0.787]
Epoch [18/120    avg_loss:0.671, val_acc:0.785]
Epoch [19/120    avg_loss:0.641, val_acc:0.815]
Epoch [20/120    avg_loss:0.601, val_acc:0.829]
Epoch [21/120    avg_loss:0.488, val_acc:0.849]
Epoch [22/120    avg_loss:0.477, val_acc:0.811]
Epoch [23/120    avg_loss:0.418, val_acc:0.860]
Epoch [24/120    avg_loss:0.348, val_acc:0.892]
Epoch [25/120    avg_loss:0.318, val_acc:0.897]
Epoch [26/120    avg_loss:0.275, val_acc:0.883]
Epoch [27/120    avg_loss:0.272, val_acc:0.883]
Epoch [28/120    avg_loss:0.294, val_acc:0.889]
Epoch [29/120    avg_loss:0.228, val_acc:0.897]
Epoch [30/120    avg_loss:0.240, val_acc:0.893]
Epoch [31/120    avg_loss:0.201, val_acc:0.902]
Epoch [32/120    avg_loss:0.189, val_acc:0.927]
Epoch [33/120    avg_loss:0.185, val_acc:0.912]
Epoch [34/120    avg_loss:0.166, val_acc:0.934]
Epoch [35/120    avg_loss:0.176, val_acc:0.915]
Epoch [36/120    avg_loss:0.151, val_acc:0.933]
Epoch [37/120    avg_loss:0.137, val_acc:0.901]
Epoch [38/120    avg_loss:0.122, val_acc:0.945]
Epoch [39/120    avg_loss:0.132, val_acc:0.936]
Epoch [40/120    avg_loss:0.107, val_acc:0.918]
Epoch [41/120    avg_loss:0.117, val_acc:0.948]
Epoch [42/120    avg_loss:0.102, val_acc:0.959]
Epoch [43/120    avg_loss:0.098, val_acc:0.934]
Epoch [44/120    avg_loss:0.104, val_acc:0.908]
Epoch [45/120    avg_loss:0.109, val_acc:0.945]
Epoch [46/120    avg_loss:0.078, val_acc:0.947]
Epoch [47/120    avg_loss:0.072, val_acc:0.957]
Epoch [48/120    avg_loss:0.098, val_acc:0.936]
Epoch [49/120    avg_loss:0.109, val_acc:0.952]
Epoch [50/120    avg_loss:0.087, val_acc:0.935]
Epoch [51/120    avg_loss:0.077, val_acc:0.958]
Epoch [52/120    avg_loss:0.079, val_acc:0.959]
Epoch [53/120    avg_loss:0.071, val_acc:0.957]
Epoch [54/120    avg_loss:0.067, val_acc:0.960]
Epoch [55/120    avg_loss:0.062, val_acc:0.961]
Epoch [56/120    avg_loss:0.073, val_acc:0.956]
Epoch [57/120    avg_loss:0.065, val_acc:0.965]
Epoch [58/120    avg_loss:0.051, val_acc:0.957]
Epoch [59/120    avg_loss:0.050, val_acc:0.949]
Epoch [60/120    avg_loss:0.057, val_acc:0.951]
Epoch [61/120    avg_loss:0.056, val_acc:0.951]
Epoch [62/120    avg_loss:0.077, val_acc:0.948]
Epoch [63/120    avg_loss:0.091, val_acc:0.923]
Epoch [64/120    avg_loss:0.091, val_acc:0.952]
Epoch [65/120    avg_loss:0.083, val_acc:0.953]
Epoch [66/120    avg_loss:0.104, val_acc:0.943]
Epoch [67/120    avg_loss:0.174, val_acc:0.934]
Epoch [68/120    avg_loss:0.119, val_acc:0.931]
Epoch [69/120    avg_loss:0.081, val_acc:0.945]
Epoch [70/120    avg_loss:0.076, val_acc:0.966]
Epoch [71/120    avg_loss:0.059, val_acc:0.952]
Epoch [72/120    avg_loss:0.058, val_acc:0.956]
Epoch [73/120    avg_loss:0.073, val_acc:0.938]
Epoch [74/120    avg_loss:0.084, val_acc:0.959]
Epoch [75/120    avg_loss:0.046, val_acc:0.960]
Epoch [76/120    avg_loss:0.039, val_acc:0.966]
Epoch [77/120    avg_loss:0.042, val_acc:0.966]
Epoch [78/120    avg_loss:0.044, val_acc:0.964]
Epoch [79/120    avg_loss:0.076, val_acc:0.938]
Epoch [80/120    avg_loss:0.050, val_acc:0.964]
Epoch [81/120    avg_loss:0.050, val_acc:0.953]
Epoch [82/120    avg_loss:0.036, val_acc:0.961]
Epoch [83/120    avg_loss:0.028, val_acc:0.971]
Epoch [84/120    avg_loss:0.026, val_acc:0.972]
Epoch [85/120    avg_loss:0.025, val_acc:0.961]
Epoch [86/120    avg_loss:0.032, val_acc:0.967]
Epoch [87/120    avg_loss:0.025, val_acc:0.972]
Epoch [88/120    avg_loss:0.028, val_acc:0.965]
Epoch [89/120    avg_loss:0.032, val_acc:0.969]
Epoch [90/120    avg_loss:0.035, val_acc:0.968]
Epoch [91/120    avg_loss:0.025, val_acc:0.975]
Epoch [92/120    avg_loss:0.027, val_acc:0.972]
Epoch [93/120    avg_loss:0.021, val_acc:0.964]
Epoch [94/120    avg_loss:0.022, val_acc:0.976]
Epoch [95/120    avg_loss:0.017, val_acc:0.976]
Epoch [96/120    avg_loss:0.022, val_acc:0.970]
Epoch [97/120    avg_loss:0.021, val_acc:0.975]
Epoch [98/120    avg_loss:0.018, val_acc:0.969]
Epoch [99/120    avg_loss:0.021, val_acc:0.974]
Epoch [100/120    avg_loss:0.018, val_acc:0.975]
Epoch [101/120    avg_loss:0.024, val_acc:0.975]
Epoch [102/120    avg_loss:0.016, val_acc:0.978]
Epoch [103/120    avg_loss:0.017, val_acc:0.976]
Epoch [104/120    avg_loss:0.015, val_acc:0.976]
Epoch [105/120    avg_loss:0.024, val_acc:0.972]
Epoch [106/120    avg_loss:0.035, val_acc:0.974]
Epoch [107/120    avg_loss:0.018, val_acc:0.975]
Epoch [108/120    avg_loss:0.020, val_acc:0.974]
Epoch [109/120    avg_loss:0.014, val_acc:0.979]
Epoch [110/120    avg_loss:0.010, val_acc:0.975]
Epoch [111/120    avg_loss:0.012, val_acc:0.981]
Epoch [112/120    avg_loss:0.011, val_acc:0.978]
Epoch [113/120    avg_loss:0.010, val_acc:0.978]
Epoch [114/120    avg_loss:0.018, val_acc:0.976]
Epoch [115/120    avg_loss:0.019, val_acc:0.977]
Epoch [116/120    avg_loss:0.018, val_acc:0.938]
Epoch [117/120    avg_loss:0.020, val_acc:0.974]
Epoch [118/120    avg_loss:0.022, val_acc:0.972]
Epoch [119/120    avg_loss:0.017, val_acc:0.977]
Epoch [120/120    avg_loss:0.014, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1249    4    3    0    0    0    0    0    1   28    0    0
     0    0    0]
 [   0    0    1  723    0    0    0    0    0    2    3    3   11    3
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    1    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   13    0    0    5    0
     0    0    0]
 [   0    0    7    0    0    2    0    0    0    0  828   36    0    0
     0    2    0]
 [   0    0   17    1    0    0    2    0    0    0    5 2178    6    1
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    3    1  517    0
     3    2    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1135    3    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    34  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.57181571815718

F1 scores:
[       nan 1.         0.97616256 0.98033898 0.99300699 0.98623853
 0.98417483 1.         1.         0.76470588 0.96503497 0.97690065
 0.96275605 0.98930481 0.97971515 0.91411043 0.97647059]

Kappa:
0.9722897341095122
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0cc7adc860>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.695, val_acc:0.401]
Epoch [2/120    avg_loss:2.395, val_acc:0.486]
Epoch [3/120    avg_loss:2.230, val_acc:0.523]
Epoch [4/120    avg_loss:2.091, val_acc:0.533]
Epoch [5/120    avg_loss:1.973, val_acc:0.518]
Epoch [6/120    avg_loss:1.865, val_acc:0.614]
Epoch [7/120    avg_loss:1.745, val_acc:0.608]
Epoch [8/120    avg_loss:1.619, val_acc:0.630]
Epoch [9/120    avg_loss:1.520, val_acc:0.655]
Epoch [10/120    avg_loss:1.426, val_acc:0.647]
Epoch [11/120    avg_loss:1.294, val_acc:0.683]
Epoch [12/120    avg_loss:1.173, val_acc:0.681]
Epoch [13/120    avg_loss:1.079, val_acc:0.739]
Epoch [14/120    avg_loss:0.994, val_acc:0.740]
Epoch [15/120    avg_loss:0.862, val_acc:0.776]
Epoch [16/120    avg_loss:0.737, val_acc:0.790]
Epoch [17/120    avg_loss:0.706, val_acc:0.793]
Epoch [18/120    avg_loss:0.639, val_acc:0.808]
Epoch [19/120    avg_loss:0.559, val_acc:0.838]
Epoch [20/120    avg_loss:0.567, val_acc:0.814]
Epoch [21/120    avg_loss:0.511, val_acc:0.831]
Epoch [22/120    avg_loss:0.455, val_acc:0.852]
Epoch [23/120    avg_loss:0.386, val_acc:0.829]
Epoch [24/120    avg_loss:0.345, val_acc:0.860]
Epoch [25/120    avg_loss:0.337, val_acc:0.875]
Epoch [26/120    avg_loss:0.273, val_acc:0.880]
Epoch [27/120    avg_loss:0.240, val_acc:0.899]
Epoch [28/120    avg_loss:0.215, val_acc:0.906]
Epoch [29/120    avg_loss:0.206, val_acc:0.901]
Epoch [30/120    avg_loss:0.207, val_acc:0.909]
Epoch [31/120    avg_loss:0.176, val_acc:0.888]
Epoch [32/120    avg_loss:0.198, val_acc:0.902]
Epoch [33/120    avg_loss:0.151, val_acc:0.920]
Epoch [34/120    avg_loss:0.171, val_acc:0.911]
Epoch [35/120    avg_loss:0.145, val_acc:0.907]
Epoch [36/120    avg_loss:0.146, val_acc:0.919]
Epoch [37/120    avg_loss:0.132, val_acc:0.946]
Epoch [38/120    avg_loss:0.100, val_acc:0.932]
Epoch [39/120    avg_loss:0.104, val_acc:0.923]
Epoch [40/120    avg_loss:0.107, val_acc:0.939]
Epoch [41/120    avg_loss:0.097, val_acc:0.934]
Epoch [42/120    avg_loss:0.106, val_acc:0.918]
Epoch [43/120    avg_loss:0.140, val_acc:0.905]
Epoch [44/120    avg_loss:0.107, val_acc:0.950]
Epoch [45/120    avg_loss:0.190, val_acc:0.890]
Epoch [46/120    avg_loss:0.347, val_acc:0.863]
Epoch [47/120    avg_loss:0.260, val_acc:0.858]
Epoch [48/120    avg_loss:0.227, val_acc:0.906]
Epoch [49/120    avg_loss:0.148, val_acc:0.926]
Epoch [50/120    avg_loss:0.110, val_acc:0.921]
Epoch [51/120    avg_loss:0.092, val_acc:0.942]
Epoch [52/120    avg_loss:0.089, val_acc:0.934]
Epoch [53/120    avg_loss:0.080, val_acc:0.939]
Epoch [54/120    avg_loss:0.071, val_acc:0.952]
Epoch [55/120    avg_loss:0.064, val_acc:0.946]
Epoch [56/120    avg_loss:0.055, val_acc:0.950]
Epoch [57/120    avg_loss:0.071, val_acc:0.958]
Epoch [58/120    avg_loss:0.060, val_acc:0.938]
Epoch [59/120    avg_loss:0.056, val_acc:0.962]
Epoch [60/120    avg_loss:0.035, val_acc:0.950]
Epoch [61/120    avg_loss:0.058, val_acc:0.956]
Epoch [62/120    avg_loss:0.044, val_acc:0.964]
Epoch [63/120    avg_loss:0.051, val_acc:0.953]
Epoch [64/120    avg_loss:0.070, val_acc:0.963]
Epoch [65/120    avg_loss:0.058, val_acc:0.949]
Epoch [66/120    avg_loss:0.054, val_acc:0.961]
Epoch [67/120    avg_loss:0.042, val_acc:0.963]
Epoch [68/120    avg_loss:0.036, val_acc:0.969]
Epoch [69/120    avg_loss:0.037, val_acc:0.968]
Epoch [70/120    avg_loss:0.035, val_acc:0.964]
Epoch [71/120    avg_loss:0.030, val_acc:0.970]
Epoch [72/120    avg_loss:0.045, val_acc:0.956]
Epoch [73/120    avg_loss:0.047, val_acc:0.963]
Epoch [74/120    avg_loss:0.033, val_acc:0.974]
Epoch [75/120    avg_loss:0.045, val_acc:0.959]
Epoch [76/120    avg_loss:0.039, val_acc:0.964]
Epoch [77/120    avg_loss:0.040, val_acc:0.960]
Epoch [78/120    avg_loss:0.044, val_acc:0.960]
Epoch [79/120    avg_loss:0.031, val_acc:0.959]
Epoch [80/120    avg_loss:0.031, val_acc:0.968]
Epoch [81/120    avg_loss:0.032, val_acc:0.964]
Epoch [82/120    avg_loss:0.026, val_acc:0.977]
Epoch [83/120    avg_loss:0.026, val_acc:0.973]
Epoch [84/120    avg_loss:0.031, val_acc:0.961]
Epoch [85/120    avg_loss:0.031, val_acc:0.969]
Epoch [86/120    avg_loss:0.030, val_acc:0.971]
Epoch [87/120    avg_loss:0.026, val_acc:0.962]
Epoch [88/120    avg_loss:0.019, val_acc:0.973]
Epoch [89/120    avg_loss:0.015, val_acc:0.981]
Epoch [90/120    avg_loss:0.020, val_acc:0.981]
Epoch [91/120    avg_loss:0.017, val_acc:0.972]
Epoch [92/120    avg_loss:0.018, val_acc:0.972]
Epoch [93/120    avg_loss:0.019, val_acc:0.974]
Epoch [94/120    avg_loss:0.016, val_acc:0.971]
Epoch [95/120    avg_loss:0.011, val_acc:0.975]
Epoch [96/120    avg_loss:0.021, val_acc:0.972]
Epoch [97/120    avg_loss:0.023, val_acc:0.970]
Epoch [98/120    avg_loss:0.017, val_acc:0.972]
Epoch [99/120    avg_loss:0.026, val_acc:0.970]
Epoch [100/120    avg_loss:0.023, val_acc:0.966]
Epoch [101/120    avg_loss:0.015, val_acc:0.973]
Epoch [102/120    avg_loss:0.017, val_acc:0.973]
Epoch [103/120    avg_loss:0.017, val_acc:0.975]
Epoch [104/120    avg_loss:0.015, val_acc:0.977]
Epoch [105/120    avg_loss:0.010, val_acc:0.977]
Epoch [106/120    avg_loss:0.012, val_acc:0.980]
Epoch [107/120    avg_loss:0.010, val_acc:0.980]
Epoch [108/120    avg_loss:0.010, val_acc:0.980]
Epoch [109/120    avg_loss:0.012, val_acc:0.983]
Epoch [110/120    avg_loss:0.009, val_acc:0.983]
Epoch [111/120    avg_loss:0.016, val_acc:0.984]
Epoch [112/120    avg_loss:0.009, val_acc:0.983]
Epoch [113/120    avg_loss:0.008, val_acc:0.982]
Epoch [114/120    avg_loss:0.009, val_acc:0.983]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.011, val_acc:0.984]
Epoch [117/120    avg_loss:0.011, val_acc:0.983]
Epoch [118/120    avg_loss:0.010, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.984]
Epoch [120/120    avg_loss:0.009, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1237    9    5    0    1    0    0    0   13   20    0    0
     0    0    0]
 [   0    0    0  722    4    4    0    0    0    7    4    4    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    3    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    3    0    0    0    1  849   12    0    0
     0    0    0]
 [   0    0    8    0    0    0    2    1    7    2    5 2173   11    1
     0    0    0]
 [   0    0    0    3    0    1    0    0    0    0    1    7  517    0
     0    4    1]
 [   0    0    0    0    0    0    0    0    0    1    0    1    0  183
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    1    0    0
  1135    2    0]
 [   0    0    0    0    0    0    2    0    0    8    0    0    0    0
     5  332    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    7    0
     0    0   77]]

Accuracy:
97.91869918699187

F1 scores:
[       nan 0.975      0.97401575 0.97501688 0.97931034 0.98627002
 0.99394856 0.92592593 0.98845266 0.57692308 0.97084048 0.98125988
 0.96365331 0.99186992 0.9960509  0.96934307 0.95061728]

Kappa:
0.9762768269280944
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f167b1bc898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.717, val_acc:0.303]
Epoch [2/120    avg_loss:2.453, val_acc:0.481]
Epoch [3/120    avg_loss:2.316, val_acc:0.496]
Epoch [4/120    avg_loss:2.127, val_acc:0.492]
Epoch [5/120    avg_loss:2.015, val_acc:0.512]
Epoch [6/120    avg_loss:1.905, val_acc:0.543]
Epoch [7/120    avg_loss:1.772, val_acc:0.605]
Epoch [8/120    avg_loss:1.688, val_acc:0.619]
Epoch [9/120    avg_loss:1.599, val_acc:0.622]
Epoch [10/120    avg_loss:1.475, val_acc:0.621]
Epoch [11/120    avg_loss:1.383, val_acc:0.655]
Epoch [12/120    avg_loss:1.257, val_acc:0.657]
Epoch [13/120    avg_loss:1.127, val_acc:0.680]
Epoch [14/120    avg_loss:1.012, val_acc:0.706]
Epoch [15/120    avg_loss:0.916, val_acc:0.688]
Epoch [16/120    avg_loss:0.835, val_acc:0.718]
Epoch [17/120    avg_loss:0.797, val_acc:0.722]
Epoch [18/120    avg_loss:0.755, val_acc:0.719]
Epoch [19/120    avg_loss:0.665, val_acc:0.746]
Epoch [20/120    avg_loss:0.795, val_acc:0.716]
Epoch [21/120    avg_loss:0.652, val_acc:0.777]
Epoch [22/120    avg_loss:0.509, val_acc:0.775]
Epoch [23/120    avg_loss:0.496, val_acc:0.769]
Epoch [24/120    avg_loss:0.436, val_acc:0.804]
Epoch [25/120    avg_loss:0.410, val_acc:0.821]
Epoch [26/120    avg_loss:0.367, val_acc:0.798]
Epoch [27/120    avg_loss:0.352, val_acc:0.797]
Epoch [28/120    avg_loss:0.382, val_acc:0.833]
Epoch [29/120    avg_loss:0.327, val_acc:0.858]
Epoch [30/120    avg_loss:0.267, val_acc:0.881]
Epoch [31/120    avg_loss:0.251, val_acc:0.858]
Epoch [32/120    avg_loss:0.223, val_acc:0.887]
Epoch [33/120    avg_loss:0.211, val_acc:0.899]
Epoch [34/120    avg_loss:0.203, val_acc:0.920]
Epoch [35/120    avg_loss:0.183, val_acc:0.909]
Epoch [36/120    avg_loss:0.168, val_acc:0.916]
Epoch [37/120    avg_loss:0.153, val_acc:0.904]
Epoch [38/120    avg_loss:0.139, val_acc:0.917]
Epoch [39/120    avg_loss:0.121, val_acc:0.913]
Epoch [40/120    avg_loss:0.120, val_acc:0.914]
Epoch [41/120    avg_loss:0.112, val_acc:0.906]
Epoch [42/120    avg_loss:0.096, val_acc:0.932]
Epoch [43/120    avg_loss:0.091, val_acc:0.943]
Epoch [44/120    avg_loss:0.089, val_acc:0.914]
Epoch [45/120    avg_loss:0.111, val_acc:0.905]
Epoch [46/120    avg_loss:0.089, val_acc:0.936]
Epoch [47/120    avg_loss:0.082, val_acc:0.942]
Epoch [48/120    avg_loss:0.128, val_acc:0.914]
Epoch [49/120    avg_loss:0.101, val_acc:0.938]
Epoch [50/120    avg_loss:0.089, val_acc:0.926]
Epoch [51/120    avg_loss:0.085, val_acc:0.920]
Epoch [52/120    avg_loss:0.064, val_acc:0.933]
Epoch [53/120    avg_loss:0.079, val_acc:0.927]
Epoch [54/120    avg_loss:0.069, val_acc:0.942]
Epoch [55/120    avg_loss:0.075, val_acc:0.907]
Epoch [56/120    avg_loss:0.110, val_acc:0.913]
Epoch [57/120    avg_loss:0.084, val_acc:0.949]
Epoch [58/120    avg_loss:0.046, val_acc:0.949]
Epoch [59/120    avg_loss:0.050, val_acc:0.956]
Epoch [60/120    avg_loss:0.044, val_acc:0.958]
Epoch [61/120    avg_loss:0.041, val_acc:0.959]
Epoch [62/120    avg_loss:0.043, val_acc:0.956]
Epoch [63/120    avg_loss:0.048, val_acc:0.961]
Epoch [64/120    avg_loss:0.038, val_acc:0.963]
Epoch [65/120    avg_loss:0.041, val_acc:0.962]
Epoch [66/120    avg_loss:0.041, val_acc:0.963]
Epoch [67/120    avg_loss:0.034, val_acc:0.963]
Epoch [68/120    avg_loss:0.039, val_acc:0.961]
Epoch [69/120    avg_loss:0.042, val_acc:0.960]
Epoch [70/120    avg_loss:0.039, val_acc:0.960]
Epoch [71/120    avg_loss:0.032, val_acc:0.960]
Epoch [72/120    avg_loss:0.033, val_acc:0.962]
Epoch [73/120    avg_loss:0.035, val_acc:0.961]
Epoch [74/120    avg_loss:0.036, val_acc:0.962]
Epoch [75/120    avg_loss:0.033, val_acc:0.963]
Epoch [76/120    avg_loss:0.034, val_acc:0.963]
Epoch [77/120    avg_loss:0.031, val_acc:0.962]
Epoch [78/120    avg_loss:0.030, val_acc:0.967]
Epoch [79/120    avg_loss:0.029, val_acc:0.963]
Epoch [80/120    avg_loss:0.033, val_acc:0.966]
Epoch [81/120    avg_loss:0.035, val_acc:0.966]
Epoch [82/120    avg_loss:0.028, val_acc:0.968]
Epoch [83/120    avg_loss:0.032, val_acc:0.966]
Epoch [84/120    avg_loss:0.029, val_acc:0.967]
Epoch [85/120    avg_loss:0.029, val_acc:0.968]
Epoch [86/120    avg_loss:0.031, val_acc:0.963]
Epoch [87/120    avg_loss:0.028, val_acc:0.967]
Epoch [88/120    avg_loss:0.034, val_acc:0.964]
Epoch [89/120    avg_loss:0.029, val_acc:0.966]
Epoch [90/120    avg_loss:0.026, val_acc:0.967]
Epoch [91/120    avg_loss:0.030, val_acc:0.964]
Epoch [92/120    avg_loss:0.028, val_acc:0.967]
Epoch [93/120    avg_loss:0.029, val_acc:0.967]
Epoch [94/120    avg_loss:0.026, val_acc:0.968]
Epoch [95/120    avg_loss:0.028, val_acc:0.967]
Epoch [96/120    avg_loss:0.029, val_acc:0.970]
Epoch [97/120    avg_loss:0.027, val_acc:0.969]
Epoch [98/120    avg_loss:0.025, val_acc:0.964]
Epoch [99/120    avg_loss:0.028, val_acc:0.968]
Epoch [100/120    avg_loss:0.026, val_acc:0.968]
Epoch [101/120    avg_loss:0.024, val_acc:0.967]
Epoch [102/120    avg_loss:0.028, val_acc:0.968]
Epoch [103/120    avg_loss:0.025, val_acc:0.968]
Epoch [104/120    avg_loss:0.027, val_acc:0.970]
Epoch [105/120    avg_loss:0.021, val_acc:0.969]
Epoch [106/120    avg_loss:0.028, val_acc:0.967]
Epoch [107/120    avg_loss:0.028, val_acc:0.969]
Epoch [108/120    avg_loss:0.025, val_acc:0.967]
Epoch [109/120    avg_loss:0.025, val_acc:0.968]
Epoch [110/120    avg_loss:0.024, val_acc:0.966]
Epoch [111/120    avg_loss:0.024, val_acc:0.968]
Epoch [112/120    avg_loss:0.023, val_acc:0.968]
Epoch [113/120    avg_loss:0.029, val_acc:0.968]
Epoch [114/120    avg_loss:0.027, val_acc:0.968]
Epoch [115/120    avg_loss:0.025, val_acc:0.968]
Epoch [116/120    avg_loss:0.024, val_acc:0.969]
Epoch [117/120    avg_loss:0.023, val_acc:0.970]
Epoch [118/120    avg_loss:0.024, val_acc:0.969]
Epoch [119/120    avg_loss:0.028, val_acc:0.961]
Epoch [120/120    avg_loss:0.025, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1239    0    4    0    0    0    0    1    6   35    0    0
     0    0    0]
 [   0    0    0  714    1    1    0    0    0    4    3    2   20    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   19    0    0    0    1    0    0    0  841   12    1    0
     0    1    0]
 [   0    0   10    0    0    1    2    0    0    0   24 2168    4    0
     0    1    0]
 [   0    0    0    7    0    0    0    0    0    0    6    0  519    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    0    1    0
  1118   17    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    19  315    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.4308943089431

F1 scores:
[       nan 0.94871795 0.9706228  0.97275204 0.98839907 0.99192618
 0.98568199 1.         1.         0.87804878 0.95459705 0.97922313
 0.96022202 0.99462366 0.97941305 0.92375367 0.98203593]

Kappa:
0.9707083324295487
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4a9615e8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.735, val_acc:0.327]
Epoch [2/120    avg_loss:2.418, val_acc:0.484]
Epoch [3/120    avg_loss:2.252, val_acc:0.564]
Epoch [4/120    avg_loss:2.089, val_acc:0.568]
Epoch [5/120    avg_loss:1.931, val_acc:0.591]
Epoch [6/120    avg_loss:1.816, val_acc:0.593]
Epoch [7/120    avg_loss:1.719, val_acc:0.604]
Epoch [8/120    avg_loss:1.640, val_acc:0.628]
Epoch [9/120    avg_loss:1.511, val_acc:0.659]
Epoch [10/120    avg_loss:1.385, val_acc:0.698]
Epoch [11/120    avg_loss:1.212, val_acc:0.714]
Epoch [12/120    avg_loss:1.103, val_acc:0.683]
Epoch [13/120    avg_loss:1.095, val_acc:0.725]
Epoch [14/120    avg_loss:0.951, val_acc:0.757]
Epoch [15/120    avg_loss:0.810, val_acc:0.775]
Epoch [16/120    avg_loss:0.832, val_acc:0.720]
Epoch [17/120    avg_loss:0.765, val_acc:0.780]
Epoch [18/120    avg_loss:0.615, val_acc:0.819]
Epoch [19/120    avg_loss:0.594, val_acc:0.806]
Epoch [20/120    avg_loss:0.552, val_acc:0.825]
Epoch [21/120    avg_loss:0.493, val_acc:0.822]
Epoch [22/120    avg_loss:0.428, val_acc:0.872]
Epoch [23/120    avg_loss:0.373, val_acc:0.864]
Epoch [24/120    avg_loss:0.388, val_acc:0.871]
Epoch [25/120    avg_loss:0.346, val_acc:0.860]
Epoch [26/120    avg_loss:0.308, val_acc:0.853]
Epoch [27/120    avg_loss:0.256, val_acc:0.879]
Epoch [28/120    avg_loss:0.226, val_acc:0.907]
Epoch [29/120    avg_loss:0.203, val_acc:0.926]
Epoch [30/120    avg_loss:0.201, val_acc:0.920]
Epoch [31/120    avg_loss:0.202, val_acc:0.909]
Epoch [32/120    avg_loss:0.172, val_acc:0.915]
Epoch [33/120    avg_loss:0.196, val_acc:0.928]
Epoch [34/120    avg_loss:0.174, val_acc:0.921]
Epoch [35/120    avg_loss:0.141, val_acc:0.934]
Epoch [36/120    avg_loss:0.154, val_acc:0.915]
Epoch [37/120    avg_loss:0.175, val_acc:0.918]
Epoch [38/120    avg_loss:0.165, val_acc:0.926]
Epoch [39/120    avg_loss:0.146, val_acc:0.924]
Epoch [40/120    avg_loss:0.128, val_acc:0.941]
Epoch [41/120    avg_loss:0.106, val_acc:0.943]
Epoch [42/120    avg_loss:0.105, val_acc:0.940]
Epoch [43/120    avg_loss:0.083, val_acc:0.946]
Epoch [44/120    avg_loss:0.111, val_acc:0.886]
Epoch [45/120    avg_loss:0.173, val_acc:0.912]
Epoch [46/120    avg_loss:0.129, val_acc:0.936]
Epoch [47/120    avg_loss:0.098, val_acc:0.960]
Epoch [48/120    avg_loss:0.072, val_acc:0.958]
Epoch [49/120    avg_loss:0.066, val_acc:0.955]
Epoch [50/120    avg_loss:0.052, val_acc:0.960]
Epoch [51/120    avg_loss:0.068, val_acc:0.952]
Epoch [52/120    avg_loss:0.046, val_acc:0.944]
Epoch [53/120    avg_loss:0.061, val_acc:0.959]
Epoch [54/120    avg_loss:0.043, val_acc:0.960]
Epoch [55/120    avg_loss:0.055, val_acc:0.966]
Epoch [56/120    avg_loss:0.062, val_acc:0.951]
Epoch [57/120    avg_loss:0.085, val_acc:0.949]
Epoch [58/120    avg_loss:0.052, val_acc:0.967]
Epoch [59/120    avg_loss:0.051, val_acc:0.954]
Epoch [60/120    avg_loss:0.061, val_acc:0.951]
Epoch [61/120    avg_loss:0.040, val_acc:0.966]
Epoch [62/120    avg_loss:0.068, val_acc:0.947]
Epoch [63/120    avg_loss:0.057, val_acc:0.958]
Epoch [64/120    avg_loss:0.046, val_acc:0.963]
Epoch [65/120    avg_loss:0.052, val_acc:0.963]
Epoch [66/120    avg_loss:0.050, val_acc:0.971]
Epoch [67/120    avg_loss:0.029, val_acc:0.956]
Epoch [68/120    avg_loss:0.027, val_acc:0.972]
Epoch [69/120    avg_loss:0.031, val_acc:0.960]
Epoch [70/120    avg_loss:0.034, val_acc:0.968]
Epoch [71/120    avg_loss:0.040, val_acc:0.954]
Epoch [72/120    avg_loss:0.035, val_acc:0.964]
Epoch [73/120    avg_loss:0.041, val_acc:0.960]
Epoch [74/120    avg_loss:0.032, val_acc:0.968]
Epoch [75/120    avg_loss:0.025, val_acc:0.963]
Epoch [76/120    avg_loss:0.017, val_acc:0.966]
Epoch [77/120    avg_loss:0.021, val_acc:0.965]
Epoch [78/120    avg_loss:0.019, val_acc:0.980]
Epoch [79/120    avg_loss:0.023, val_acc:0.965]
Epoch [80/120    avg_loss:0.017, val_acc:0.972]
Epoch [81/120    avg_loss:0.021, val_acc:0.973]
Epoch [82/120    avg_loss:0.017, val_acc:0.977]
Epoch [83/120    avg_loss:0.017, val_acc:0.972]
Epoch [84/120    avg_loss:0.015, val_acc:0.974]
Epoch [85/120    avg_loss:0.017, val_acc:0.976]
Epoch [86/120    avg_loss:0.014, val_acc:0.966]
Epoch [87/120    avg_loss:0.019, val_acc:0.946]
Epoch [88/120    avg_loss:0.020, val_acc:0.974]
Epoch [89/120    avg_loss:0.022, val_acc:0.977]
Epoch [90/120    avg_loss:0.019, val_acc:0.972]
Epoch [91/120    avg_loss:0.044, val_acc:0.945]
Epoch [92/120    avg_loss:0.046, val_acc:0.966]
Epoch [93/120    avg_loss:0.024, val_acc:0.969]
Epoch [94/120    avg_loss:0.017, val_acc:0.971]
Epoch [95/120    avg_loss:0.017, val_acc:0.971]
Epoch [96/120    avg_loss:0.013, val_acc:0.972]
Epoch [97/120    avg_loss:0.014, val_acc:0.973]
Epoch [98/120    avg_loss:0.012, val_acc:0.971]
Epoch [99/120    avg_loss:0.015, val_acc:0.973]
Epoch [100/120    avg_loss:0.012, val_acc:0.970]
Epoch [101/120    avg_loss:0.012, val_acc:0.973]
Epoch [102/120    avg_loss:0.011, val_acc:0.971]
Epoch [103/120    avg_loss:0.008, val_acc:0.973]
Epoch [104/120    avg_loss:0.014, val_acc:0.973]
Epoch [105/120    avg_loss:0.016, val_acc:0.973]
Epoch [106/120    avg_loss:0.015, val_acc:0.973]
Epoch [107/120    avg_loss:0.009, val_acc:0.973]
Epoch [108/120    avg_loss:0.012, val_acc:0.973]
Epoch [109/120    avg_loss:0.012, val_acc:0.973]
Epoch [110/120    avg_loss:0.011, val_acc:0.973]
Epoch [111/120    avg_loss:0.011, val_acc:0.973]
Epoch [112/120    avg_loss:0.010, val_acc:0.973]
Epoch [113/120    avg_loss:0.010, val_acc:0.973]
Epoch [114/120    avg_loss:0.013, val_acc:0.973]
Epoch [115/120    avg_loss:0.011, val_acc:0.973]
Epoch [116/120    avg_loss:0.010, val_acc:0.973]
Epoch [117/120    avg_loss:0.013, val_acc:0.973]
Epoch [118/120    avg_loss:0.015, val_acc:0.973]
Epoch [119/120    avg_loss:0.012, val_acc:0.973]
Epoch [120/120    avg_loss:0.011, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0 1241    1    0    0    0    0    0    4   11   28    0    0
     0    0    0]
 [   0    0    1  726    0    1    1    0    0    9    3    4    2    0
     0    0    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    1    2    0    0    1  844   11    5    0
     0    1    0]
 [   0    0    7    0    0    1    2    0    0    3    4 2193    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    4    7  511    0
     0    6    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1134    5    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    11  336    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.26558265582656

F1 scores:
[       nan 0.98765432 0.97562893 0.98240866 0.99528302 0.99656357
 0.99544765 1.         0.99649942 0.67924528 0.96955773 0.98495396
 0.96597353 1.         0.99256018 0.96690647 0.97647059]

Kappa:
0.9802193568790262
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4b66c14860>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.694, val_acc:0.334]
Epoch [2/120    avg_loss:2.372, val_acc:0.482]
Epoch [3/120    avg_loss:2.242, val_acc:0.467]
Epoch [4/120    avg_loss:2.063, val_acc:0.491]
Epoch [5/120    avg_loss:1.898, val_acc:0.571]
Epoch [6/120    avg_loss:1.816, val_acc:0.599]
Epoch [7/120    avg_loss:1.667, val_acc:0.568]
Epoch [8/120    avg_loss:1.545, val_acc:0.591]
Epoch [9/120    avg_loss:1.433, val_acc:0.614]
Epoch [10/120    avg_loss:1.300, val_acc:0.665]
Epoch [11/120    avg_loss:1.214, val_acc:0.690]
Epoch [12/120    avg_loss:1.095, val_acc:0.720]
Epoch [13/120    avg_loss:0.917, val_acc:0.776]
Epoch [14/120    avg_loss:0.827, val_acc:0.762]
Epoch [15/120    avg_loss:0.742, val_acc:0.802]
Epoch [16/120    avg_loss:0.608, val_acc:0.828]
Epoch [17/120    avg_loss:0.594, val_acc:0.769]
Epoch [18/120    avg_loss:0.548, val_acc:0.825]
Epoch [19/120    avg_loss:0.507, val_acc:0.841]
Epoch [20/120    avg_loss:0.471, val_acc:0.829]
Epoch [21/120    avg_loss:0.425, val_acc:0.873]
Epoch [22/120    avg_loss:0.425, val_acc:0.861]
Epoch [23/120    avg_loss:0.352, val_acc:0.879]
Epoch [24/120    avg_loss:0.322, val_acc:0.884]
Epoch [25/120    avg_loss:0.288, val_acc:0.873]
Epoch [26/120    avg_loss:0.285, val_acc:0.899]
Epoch [27/120    avg_loss:0.229, val_acc:0.915]
Epoch [28/120    avg_loss:0.187, val_acc:0.912]
Epoch [29/120    avg_loss:0.214, val_acc:0.899]
Epoch [30/120    avg_loss:0.174, val_acc:0.908]
Epoch [31/120    avg_loss:0.151, val_acc:0.901]
Epoch [32/120    avg_loss:0.175, val_acc:0.903]
Epoch [33/120    avg_loss:0.164, val_acc:0.908]
Epoch [34/120    avg_loss:0.137, val_acc:0.939]
Epoch [35/120    avg_loss:0.138, val_acc:0.926]
Epoch [36/120    avg_loss:0.130, val_acc:0.927]
Epoch [37/120    avg_loss:0.128, val_acc:0.932]
Epoch [38/120    avg_loss:0.119, val_acc:0.940]
Epoch [39/120    avg_loss:0.108, val_acc:0.934]
Epoch [40/120    avg_loss:0.089, val_acc:0.943]
Epoch [41/120    avg_loss:0.084, val_acc:0.953]
Epoch [42/120    avg_loss:0.092, val_acc:0.944]
Epoch [43/120    avg_loss:0.068, val_acc:0.948]
Epoch [44/120    avg_loss:0.066, val_acc:0.945]
Epoch [45/120    avg_loss:0.066, val_acc:0.955]
Epoch [46/120    avg_loss:0.094, val_acc:0.955]
Epoch [47/120    avg_loss:0.104, val_acc:0.934]
Epoch [48/120    avg_loss:0.085, val_acc:0.950]
Epoch [49/120    avg_loss:0.104, val_acc:0.944]
Epoch [50/120    avg_loss:0.086, val_acc:0.948]
Epoch [51/120    avg_loss:0.074, val_acc:0.944]
Epoch [52/120    avg_loss:0.071, val_acc:0.959]
Epoch [53/120    avg_loss:0.055, val_acc:0.971]
Epoch [54/120    avg_loss:0.054, val_acc:0.963]
Epoch [55/120    avg_loss:0.044, val_acc:0.961]
Epoch [56/120    avg_loss:0.057, val_acc:0.957]
Epoch [57/120    avg_loss:0.048, val_acc:0.958]
Epoch [58/120    avg_loss:0.046, val_acc:0.959]
Epoch [59/120    avg_loss:0.076, val_acc:0.940]
Epoch [60/120    avg_loss:0.076, val_acc:0.956]
Epoch [61/120    avg_loss:0.063, val_acc:0.955]
Epoch [62/120    avg_loss:0.073, val_acc:0.964]
Epoch [63/120    avg_loss:0.067, val_acc:0.943]
Epoch [64/120    avg_loss:0.061, val_acc:0.963]
Epoch [65/120    avg_loss:0.048, val_acc:0.966]
Epoch [66/120    avg_loss:0.063, val_acc:0.962]
Epoch [67/120    avg_loss:0.032, val_acc:0.972]
Epoch [68/120    avg_loss:0.033, val_acc:0.970]
Epoch [69/120    avg_loss:0.031, val_acc:0.972]
Epoch [70/120    avg_loss:0.033, val_acc:0.971]
Epoch [71/120    avg_loss:0.033, val_acc:0.971]
Epoch [72/120    avg_loss:0.031, val_acc:0.971]
Epoch [73/120    avg_loss:0.031, val_acc:0.973]
Epoch [74/120    avg_loss:0.023, val_acc:0.973]
Epoch [75/120    avg_loss:0.026, val_acc:0.973]
Epoch [76/120    avg_loss:0.028, val_acc:0.974]
Epoch [77/120    avg_loss:0.024, val_acc:0.971]
Epoch [78/120    avg_loss:0.024, val_acc:0.971]
Epoch [79/120    avg_loss:0.023, val_acc:0.974]
Epoch [80/120    avg_loss:0.025, val_acc:0.972]
Epoch [81/120    avg_loss:0.028, val_acc:0.971]
Epoch [82/120    avg_loss:0.027, val_acc:0.973]
Epoch [83/120    avg_loss:0.025, val_acc:0.973]
Epoch [84/120    avg_loss:0.025, val_acc:0.974]
Epoch [85/120    avg_loss:0.025, val_acc:0.975]
Epoch [86/120    avg_loss:0.026, val_acc:0.973]
Epoch [87/120    avg_loss:0.025, val_acc:0.973]
Epoch [88/120    avg_loss:0.023, val_acc:0.976]
Epoch [89/120    avg_loss:0.026, val_acc:0.975]
Epoch [90/120    avg_loss:0.026, val_acc:0.976]
Epoch [91/120    avg_loss:0.022, val_acc:0.973]
Epoch [92/120    avg_loss:0.020, val_acc:0.972]
Epoch [93/120    avg_loss:0.025, val_acc:0.971]
Epoch [94/120    avg_loss:0.020, val_acc:0.974]
Epoch [95/120    avg_loss:0.026, val_acc:0.975]
Epoch [96/120    avg_loss:0.022, val_acc:0.973]
Epoch [97/120    avg_loss:0.025, val_acc:0.978]
Epoch [98/120    avg_loss:0.026, val_acc:0.977]
Epoch [99/120    avg_loss:0.027, val_acc:0.976]
Epoch [100/120    avg_loss:0.021, val_acc:0.977]
Epoch [101/120    avg_loss:0.022, val_acc:0.974]
Epoch [102/120    avg_loss:0.023, val_acc:0.974]
Epoch [103/120    avg_loss:0.023, val_acc:0.974]
Epoch [104/120    avg_loss:0.020, val_acc:0.977]
Epoch [105/120    avg_loss:0.022, val_acc:0.976]
Epoch [106/120    avg_loss:0.024, val_acc:0.977]
Epoch [107/120    avg_loss:0.018, val_acc:0.977]
Epoch [108/120    avg_loss:0.019, val_acc:0.975]
Epoch [109/120    avg_loss:0.022, val_acc:0.976]
Epoch [110/120    avg_loss:0.017, val_acc:0.977]
Epoch [111/120    avg_loss:0.023, val_acc:0.977]
Epoch [112/120    avg_loss:0.021, val_acc:0.977]
Epoch [113/120    avg_loss:0.023, val_acc:0.977]
Epoch [114/120    avg_loss:0.019, val_acc:0.977]
Epoch [115/120    avg_loss:0.020, val_acc:0.977]
Epoch [116/120    avg_loss:0.023, val_acc:0.978]
Epoch [117/120    avg_loss:0.021, val_acc:0.977]
Epoch [118/120    avg_loss:0.019, val_acc:0.976]
Epoch [119/120    avg_loss:0.018, val_acc:0.977]
Epoch [120/120    avg_loss:0.018, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1250    1    0    0    0    0    0    1    4   28    1    0
     0    0    0]
 [   0    0    2  731    1    0    0    0    0    6    2    1    3    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    5    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  422    0    0    0    8    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   12    0    0    0    0    0    0    0  846   17    0    0
     0    0    0]
 [   0    0   15    0    0    0    4    0    1    1   33 2150    6    0
     0    0    0]
 [   0    0    0    5    2    0    0    0    0    0    0    0  522    0
     0    2    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1125   14    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    19  315    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.64769647696477

F1 scores:
[       nan 0.975      0.975039   0.9851752  0.99300699 0.99190751
 0.98646617 0.90909091 0.989449   0.8        0.96027242 0.9759419
 0.97116279 0.99728997 0.98511384 0.92783505 0.97647059]

Kappa:
0.9731902596600344
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcf31994860>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.657, val_acc:0.386]
Epoch [2/120    avg_loss:2.401, val_acc:0.517]
Epoch [3/120    avg_loss:2.185, val_acc:0.565]
Epoch [4/120    avg_loss:2.031, val_acc:0.509]
Epoch [5/120    avg_loss:1.938, val_acc:0.591]
Epoch [6/120    avg_loss:1.803, val_acc:0.553]
Epoch [7/120    avg_loss:1.663, val_acc:0.647]
Epoch [8/120    avg_loss:1.549, val_acc:0.656]
Epoch [9/120    avg_loss:1.500, val_acc:0.673]
Epoch [10/120    avg_loss:1.359, val_acc:0.679]
Epoch [11/120    avg_loss:1.219, val_acc:0.693]
Epoch [12/120    avg_loss:1.112, val_acc:0.719]
Epoch [13/120    avg_loss:1.018, val_acc:0.716]
Epoch [14/120    avg_loss:0.914, val_acc:0.733]
Epoch [15/120    avg_loss:0.882, val_acc:0.734]
Epoch [16/120    avg_loss:0.813, val_acc:0.776]
Epoch [17/120    avg_loss:0.765, val_acc:0.793]
Epoch [18/120    avg_loss:0.662, val_acc:0.824]
Epoch [19/120    avg_loss:0.599, val_acc:0.816]
Epoch [20/120    avg_loss:0.522, val_acc:0.810]
Epoch [21/120    avg_loss:0.489, val_acc:0.864]
Epoch [22/120    avg_loss:0.511, val_acc:0.808]
Epoch [23/120    avg_loss:0.463, val_acc:0.820]
Epoch [24/120    avg_loss:0.441, val_acc:0.866]
Epoch [25/120    avg_loss:0.421, val_acc:0.863]
Epoch [26/120    avg_loss:0.370, val_acc:0.855]
Epoch [27/120    avg_loss:0.291, val_acc:0.894]
Epoch [28/120    avg_loss:0.280, val_acc:0.869]
Epoch [29/120    avg_loss:0.267, val_acc:0.895]
Epoch [30/120    avg_loss:0.212, val_acc:0.916]
Epoch [31/120    avg_loss:0.203, val_acc:0.929]
Epoch [32/120    avg_loss:0.211, val_acc:0.903]
Epoch [33/120    avg_loss:0.225, val_acc:0.918]
Epoch [34/120    avg_loss:0.200, val_acc:0.930]
Epoch [35/120    avg_loss:0.158, val_acc:0.930]
Epoch [36/120    avg_loss:0.167, val_acc:0.935]
Epoch [37/120    avg_loss:0.160, val_acc:0.941]
Epoch [38/120    avg_loss:0.136, val_acc:0.923]
Epoch [39/120    avg_loss:0.210, val_acc:0.938]
Epoch [40/120    avg_loss:0.138, val_acc:0.919]
Epoch [41/120    avg_loss:0.173, val_acc:0.923]
Epoch [42/120    avg_loss:0.207, val_acc:0.925]
Epoch [43/120    avg_loss:0.145, val_acc:0.901]
Epoch [44/120    avg_loss:0.124, val_acc:0.941]
Epoch [45/120    avg_loss:0.093, val_acc:0.944]
Epoch [46/120    avg_loss:0.108, val_acc:0.934]
Epoch [47/120    avg_loss:0.104, val_acc:0.940]
Epoch [48/120    avg_loss:0.096, val_acc:0.948]
Epoch [49/120    avg_loss:0.080, val_acc:0.953]
Epoch [50/120    avg_loss:0.090, val_acc:0.923]
Epoch [51/120    avg_loss:0.087, val_acc:0.953]
Epoch [52/120    avg_loss:0.072, val_acc:0.963]
Epoch [53/120    avg_loss:0.060, val_acc:0.959]
Epoch [54/120    avg_loss:0.074, val_acc:0.931]
Epoch [55/120    avg_loss:0.090, val_acc:0.952]
Epoch [56/120    avg_loss:0.055, val_acc:0.957]
Epoch [57/120    avg_loss:0.058, val_acc:0.965]
Epoch [58/120    avg_loss:0.064, val_acc:0.961]
Epoch [59/120    avg_loss:0.052, val_acc:0.951]
Epoch [60/120    avg_loss:0.059, val_acc:0.969]
Epoch [61/120    avg_loss:0.037, val_acc:0.975]
Epoch [62/120    avg_loss:0.057, val_acc:0.961]
Epoch [63/120    avg_loss:0.044, val_acc:0.971]
Epoch [64/120    avg_loss:0.037, val_acc:0.960]
Epoch [65/120    avg_loss:0.040, val_acc:0.971]
Epoch [66/120    avg_loss:0.030, val_acc:0.969]
Epoch [67/120    avg_loss:0.046, val_acc:0.960]
Epoch [68/120    avg_loss:0.060, val_acc:0.947]
Epoch [69/120    avg_loss:0.073, val_acc:0.963]
Epoch [70/120    avg_loss:0.046, val_acc:0.963]
Epoch [71/120    avg_loss:0.036, val_acc:0.967]
Epoch [72/120    avg_loss:0.026, val_acc:0.977]
Epoch [73/120    avg_loss:0.024, val_acc:0.976]
Epoch [74/120    avg_loss:0.025, val_acc:0.968]
Epoch [75/120    avg_loss:0.022, val_acc:0.970]
Epoch [76/120    avg_loss:0.028, val_acc:0.957]
Epoch [77/120    avg_loss:0.033, val_acc:0.957]
Epoch [78/120    avg_loss:0.031, val_acc:0.968]
Epoch [79/120    avg_loss:0.030, val_acc:0.972]
Epoch [80/120    avg_loss:0.021, val_acc:0.980]
Epoch [81/120    avg_loss:0.019, val_acc:0.980]
Epoch [82/120    avg_loss:0.019, val_acc:0.981]
Epoch [83/120    avg_loss:0.026, val_acc:0.972]
Epoch [84/120    avg_loss:0.060, val_acc:0.953]
Epoch [85/120    avg_loss:0.048, val_acc:0.954]
Epoch [86/120    avg_loss:0.032, val_acc:0.973]
Epoch [87/120    avg_loss:0.019, val_acc:0.972]
Epoch [88/120    avg_loss:0.018, val_acc:0.976]
Epoch [89/120    avg_loss:0.017, val_acc:0.976]
Epoch [90/120    avg_loss:0.016, val_acc:0.976]
Epoch [91/120    avg_loss:0.024, val_acc:0.975]
Epoch [92/120    avg_loss:0.016, val_acc:0.976]
Epoch [93/120    avg_loss:0.019, val_acc:0.971]
Epoch [94/120    avg_loss:0.024, val_acc:0.961]
Epoch [95/120    avg_loss:0.017, val_acc:0.978]
Epoch [96/120    avg_loss:0.012, val_acc:0.977]
Epoch [97/120    avg_loss:0.010, val_acc:0.978]
Epoch [98/120    avg_loss:0.015, val_acc:0.979]
Epoch [99/120    avg_loss:0.013, val_acc:0.976]
Epoch [100/120    avg_loss:0.011, val_acc:0.978]
Epoch [101/120    avg_loss:0.013, val_acc:0.978]
Epoch [102/120    avg_loss:0.012, val_acc:0.978]
Epoch [103/120    avg_loss:0.011, val_acc:0.978]
Epoch [104/120    avg_loss:0.009, val_acc:0.979]
Epoch [105/120    avg_loss:0.011, val_acc:0.980]
Epoch [106/120    avg_loss:0.011, val_acc:0.981]
Epoch [107/120    avg_loss:0.009, val_acc:0.981]
Epoch [108/120    avg_loss:0.011, val_acc:0.979]
Epoch [109/120    avg_loss:0.008, val_acc:0.979]
Epoch [110/120    avg_loss:0.010, val_acc:0.977]
Epoch [111/120    avg_loss:0.009, val_acc:0.977]
Epoch [112/120    avg_loss:0.010, val_acc:0.979]
Epoch [113/120    avg_loss:0.013, val_acc:0.983]
Epoch [114/120    avg_loss:0.010, val_acc:0.982]
Epoch [115/120    avg_loss:0.009, val_acc:0.981]
Epoch [116/120    avg_loss:0.010, val_acc:0.980]
Epoch [117/120    avg_loss:0.010, val_acc:0.979]
Epoch [118/120    avg_loss:0.009, val_acc:0.979]
Epoch [119/120    avg_loss:0.009, val_acc:0.980]
Epoch [120/120    avg_loss:0.009, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    2    2    0    3    0    0    0    3   14    0    0
     0    0    0]
 [   0    0    2  728    2    3    0    0    0    6    2    0    3    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    5    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    2    0    0    0  858   10    0    0
     0    0    0]
 [   0    0    2    2    0    3    2    0    0    0    6 2191    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0   10    2  520    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1129    9    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    53  289    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.13550135501355

F1 scores:
[       nan 0.98765432 0.98669797 0.98444895 0.99069767 0.98504028
 0.98869631 0.90909091 0.99883856 0.76190476 0.97833523 0.9898351
 0.98020735 0.99730458 0.97243755 0.89473684 0.99408284]

Kappa:
0.9787346621388532
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f274e9d88d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.758, val_acc:0.244]
Epoch [2/120    avg_loss:2.458, val_acc:0.418]
Epoch [3/120    avg_loss:2.231, val_acc:0.441]
Epoch [4/120    avg_loss:2.111, val_acc:0.512]
Epoch [5/120    avg_loss:1.909, val_acc:0.548]
Epoch [6/120    avg_loss:1.757, val_acc:0.530]
Epoch [7/120    avg_loss:1.644, val_acc:0.528]
Epoch [8/120    avg_loss:1.517, val_acc:0.545]
Epoch [9/120    avg_loss:1.413, val_acc:0.573]
Epoch [10/120    avg_loss:1.283, val_acc:0.634]
Epoch [11/120    avg_loss:1.225, val_acc:0.677]
Epoch [12/120    avg_loss:1.059, val_acc:0.682]
Epoch [13/120    avg_loss:0.910, val_acc:0.709]
Epoch [14/120    avg_loss:0.792, val_acc:0.736]
Epoch [15/120    avg_loss:0.859, val_acc:0.759]
Epoch [16/120    avg_loss:0.845, val_acc:0.705]
Epoch [17/120    avg_loss:0.731, val_acc:0.719]
Epoch [18/120    avg_loss:0.680, val_acc:0.770]
Epoch [19/120    avg_loss:0.590, val_acc:0.804]
Epoch [20/120    avg_loss:0.535, val_acc:0.792]
Epoch [21/120    avg_loss:0.646, val_acc:0.765]
Epoch [22/120    avg_loss:0.516, val_acc:0.814]
Epoch [23/120    avg_loss:0.401, val_acc:0.844]
Epoch [24/120    avg_loss:0.350, val_acc:0.852]
Epoch [25/120    avg_loss:0.311, val_acc:0.864]
Epoch [26/120    avg_loss:0.322, val_acc:0.873]
Epoch [27/120    avg_loss:0.295, val_acc:0.878]
Epoch [28/120    avg_loss:0.263, val_acc:0.856]
Epoch [29/120    avg_loss:0.290, val_acc:0.869]
Epoch [30/120    avg_loss:0.253, val_acc:0.871]
Epoch [31/120    avg_loss:0.212, val_acc:0.898]
Epoch [32/120    avg_loss:0.211, val_acc:0.885]
Epoch [33/120    avg_loss:0.213, val_acc:0.899]
Epoch [34/120    avg_loss:0.231, val_acc:0.891]
Epoch [35/120    avg_loss:0.169, val_acc:0.913]
Epoch [36/120    avg_loss:0.158, val_acc:0.906]
Epoch [37/120    avg_loss:0.126, val_acc:0.932]
Epoch [38/120    avg_loss:0.126, val_acc:0.923]
Epoch [39/120    avg_loss:0.113, val_acc:0.921]
Epoch [40/120    avg_loss:0.109, val_acc:0.930]
Epoch [41/120    avg_loss:0.099, val_acc:0.925]
Epoch [42/120    avg_loss:0.096, val_acc:0.920]
Epoch [43/120    avg_loss:0.118, val_acc:0.918]
Epoch [44/120    avg_loss:0.110, val_acc:0.927]
Epoch [45/120    avg_loss:0.096, val_acc:0.925]
Epoch [46/120    avg_loss:0.080, val_acc:0.944]
Epoch [47/120    avg_loss:0.135, val_acc:0.861]
Epoch [48/120    avg_loss:0.353, val_acc:0.893]
Epoch [49/120    avg_loss:0.194, val_acc:0.916]
Epoch [50/120    avg_loss:0.177, val_acc:0.909]
Epoch [51/120    avg_loss:0.126, val_acc:0.928]
Epoch [52/120    avg_loss:0.183, val_acc:0.923]
Epoch [53/120    avg_loss:0.134, val_acc:0.918]
Epoch [54/120    avg_loss:0.095, val_acc:0.922]
Epoch [55/120    avg_loss:0.101, val_acc:0.912]
Epoch [56/120    avg_loss:0.116, val_acc:0.922]
Epoch [57/120    avg_loss:0.084, val_acc:0.928]
Epoch [58/120    avg_loss:0.079, val_acc:0.927]
Epoch [59/120    avg_loss:0.075, val_acc:0.927]
Epoch [60/120    avg_loss:0.064, val_acc:0.932]
Epoch [61/120    avg_loss:0.050, val_acc:0.944]
Epoch [62/120    avg_loss:0.056, val_acc:0.944]
Epoch [63/120    avg_loss:0.053, val_acc:0.941]
Epoch [64/120    avg_loss:0.046, val_acc:0.940]
Epoch [65/120    avg_loss:0.045, val_acc:0.940]
Epoch [66/120    avg_loss:0.047, val_acc:0.942]
Epoch [67/120    avg_loss:0.046, val_acc:0.944]
Epoch [68/120    avg_loss:0.039, val_acc:0.944]
Epoch [69/120    avg_loss:0.042, val_acc:0.949]
Epoch [70/120    avg_loss:0.043, val_acc:0.946]
Epoch [71/120    avg_loss:0.048, val_acc:0.952]
Epoch [72/120    avg_loss:0.051, val_acc:0.941]
Epoch [73/120    avg_loss:0.044, val_acc:0.946]
Epoch [74/120    avg_loss:0.041, val_acc:0.949]
Epoch [75/120    avg_loss:0.039, val_acc:0.949]
Epoch [76/120    avg_loss:0.041, val_acc:0.947]
Epoch [77/120    avg_loss:0.040, val_acc:0.949]
Epoch [78/120    avg_loss:0.037, val_acc:0.947]
Epoch [79/120    avg_loss:0.041, val_acc:0.950]
Epoch [80/120    avg_loss:0.042, val_acc:0.954]
Epoch [81/120    avg_loss:0.047, val_acc:0.957]
Epoch [82/120    avg_loss:0.039, val_acc:0.954]
Epoch [83/120    avg_loss:0.039, val_acc:0.950]
Epoch [84/120    avg_loss:0.066, val_acc:0.952]
Epoch [85/120    avg_loss:0.042, val_acc:0.952]
Epoch [86/120    avg_loss:0.035, val_acc:0.957]
Epoch [87/120    avg_loss:0.037, val_acc:0.952]
Epoch [88/120    avg_loss:0.035, val_acc:0.952]
Epoch [89/120    avg_loss:0.036, val_acc:0.950]
Epoch [90/120    avg_loss:0.035, val_acc:0.954]
Epoch [91/120    avg_loss:0.037, val_acc:0.955]
Epoch [92/120    avg_loss:0.035, val_acc:0.953]
Epoch [93/120    avg_loss:0.039, val_acc:0.953]
Epoch [94/120    avg_loss:0.034, val_acc:0.956]
Epoch [95/120    avg_loss:0.035, val_acc:0.956]
Epoch [96/120    avg_loss:0.034, val_acc:0.961]
Epoch [97/120    avg_loss:0.036, val_acc:0.959]
Epoch [98/120    avg_loss:0.033, val_acc:0.955]
Epoch [99/120    avg_loss:0.039, val_acc:0.955]
Epoch [100/120    avg_loss:0.032, val_acc:0.958]
Epoch [101/120    avg_loss:0.033, val_acc:0.959]
Epoch [102/120    avg_loss:0.031, val_acc:0.956]
Epoch [103/120    avg_loss:0.043, val_acc:0.948]
Epoch [104/120    avg_loss:0.037, val_acc:0.953]
Epoch [105/120    avg_loss:0.035, val_acc:0.953]
Epoch [106/120    avg_loss:0.033, val_acc:0.955]
Epoch [107/120    avg_loss:0.033, val_acc:0.956]
Epoch [108/120    avg_loss:0.032, val_acc:0.953]
Epoch [109/120    avg_loss:0.029, val_acc:0.957]
Epoch [110/120    avg_loss:0.026, val_acc:0.957]
Epoch [111/120    avg_loss:0.037, val_acc:0.957]
Epoch [112/120    avg_loss:0.029, val_acc:0.956]
Epoch [113/120    avg_loss:0.031, val_acc:0.956]
Epoch [114/120    avg_loss:0.029, val_acc:0.955]
Epoch [115/120    avg_loss:0.027, val_acc:0.956]
Epoch [116/120    avg_loss:0.030, val_acc:0.956]
Epoch [117/120    avg_loss:0.032, val_acc:0.956]
Epoch [118/120    avg_loss:0.031, val_acc:0.957]
Epoch [119/120    avg_loss:0.033, val_acc:0.957]
Epoch [120/120    avg_loss:0.028, val_acc:0.957]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1230    2    2    1    0    0    0    0    6   44    0    0
     0    0    0]
 [   0    0    5  698    3    2    0    0    0    6    4    9   19    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    1    0    2    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   26    0    0    2    1    0    0    0  792   46    0    0
     6    2    0]
 [   0    0    4    0    0    1    0    0    0    1   20 2168   16    0
     0    0    0]
 [   0    0    0    2    0    1    0    0    0    0    3    2  522    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    1    0    0
  1128    9    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    1    0
    23  311    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.70460704607046

F1 scores:
[       nan 0.975      0.96470588 0.96342305 0.98839907 0.98273878
 0.98638427 0.98039216 0.99883856 0.8        0.9306698  0.96764115
 0.95516926 0.99730458 0.97789337 0.92974589 0.98224852]

Kappa:
0.9623902823592064
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f53cf04a828>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.755, val_acc:0.251]
Epoch [2/120    avg_loss:2.434, val_acc:0.483]
Epoch [3/120    avg_loss:2.240, val_acc:0.525]
Epoch [4/120    avg_loss:2.103, val_acc:0.543]
Epoch [5/120    avg_loss:1.988, val_acc:0.543]
Epoch [6/120    avg_loss:1.865, val_acc:0.560]
Epoch [7/120    avg_loss:1.718, val_acc:0.559]
Epoch [8/120    avg_loss:1.628, val_acc:0.607]
Epoch [9/120    avg_loss:1.546, val_acc:0.608]
Epoch [10/120    avg_loss:1.444, val_acc:0.666]
Epoch [11/120    avg_loss:1.310, val_acc:0.651]
Epoch [12/120    avg_loss:1.250, val_acc:0.666]
Epoch [13/120    avg_loss:1.169, val_acc:0.689]
Epoch [14/120    avg_loss:1.035, val_acc:0.731]
Epoch [15/120    avg_loss:0.927, val_acc:0.734]
Epoch [16/120    avg_loss:0.875, val_acc:0.746]
Epoch [17/120    avg_loss:0.748, val_acc:0.755]
Epoch [18/120    avg_loss:0.659, val_acc:0.761]
Epoch [19/120    avg_loss:0.636, val_acc:0.732]
Epoch [20/120    avg_loss:0.731, val_acc:0.784]
Epoch [21/120    avg_loss:0.672, val_acc:0.793]
Epoch [22/120    avg_loss:0.574, val_acc:0.836]
Epoch [23/120    avg_loss:0.557, val_acc:0.821]
Epoch [24/120    avg_loss:0.474, val_acc:0.823]
Epoch [25/120    avg_loss:0.456, val_acc:0.821]
Epoch [26/120    avg_loss:0.390, val_acc:0.860]
Epoch [27/120    avg_loss:0.349, val_acc:0.879]
Epoch [28/120    avg_loss:0.285, val_acc:0.908]
Epoch [29/120    avg_loss:0.284, val_acc:0.893]
Epoch [30/120    avg_loss:0.284, val_acc:0.900]
Epoch [31/120    avg_loss:0.274, val_acc:0.869]
Epoch [32/120    avg_loss:0.273, val_acc:0.907]
Epoch [33/120    avg_loss:0.258, val_acc:0.901]
Epoch [34/120    avg_loss:0.252, val_acc:0.909]
Epoch [35/120    avg_loss:0.226, val_acc:0.880]
Epoch [36/120    avg_loss:0.289, val_acc:0.925]
Epoch [37/120    avg_loss:0.187, val_acc:0.902]
Epoch [38/120    avg_loss:0.230, val_acc:0.904]
Epoch [39/120    avg_loss:0.188, val_acc:0.916]
Epoch [40/120    avg_loss:0.184, val_acc:0.927]
Epoch [41/120    avg_loss:0.165, val_acc:0.938]
Epoch [42/120    avg_loss:0.145, val_acc:0.928]
Epoch [43/120    avg_loss:0.160, val_acc:0.934]
Epoch [44/120    avg_loss:0.145, val_acc:0.929]
Epoch [45/120    avg_loss:0.147, val_acc:0.941]
Epoch [46/120    avg_loss:0.109, val_acc:0.947]
Epoch [47/120    avg_loss:0.094, val_acc:0.957]
Epoch [48/120    avg_loss:0.088, val_acc:0.960]
Epoch [49/120    avg_loss:0.084, val_acc:0.943]
Epoch [50/120    avg_loss:0.075, val_acc:0.957]
Epoch [51/120    avg_loss:0.087, val_acc:0.961]
Epoch [52/120    avg_loss:0.084, val_acc:0.953]
Epoch [53/120    avg_loss:0.078, val_acc:0.963]
Epoch [54/120    avg_loss:0.061, val_acc:0.959]
Epoch [55/120    avg_loss:0.056, val_acc:0.960]
Epoch [56/120    avg_loss:0.057, val_acc:0.971]
Epoch [57/120    avg_loss:0.049, val_acc:0.967]
Epoch [58/120    avg_loss:0.055, val_acc:0.955]
Epoch [59/120    avg_loss:0.049, val_acc:0.966]
Epoch [60/120    avg_loss:0.058, val_acc:0.956]
Epoch [61/120    avg_loss:0.050, val_acc:0.969]
Epoch [62/120    avg_loss:0.043, val_acc:0.957]
Epoch [63/120    avg_loss:0.098, val_acc:0.946]
Epoch [64/120    avg_loss:0.095, val_acc:0.950]
Epoch [65/120    avg_loss:0.088, val_acc:0.959]
Epoch [66/120    avg_loss:0.070, val_acc:0.953]
Epoch [67/120    avg_loss:0.056, val_acc:0.971]
Epoch [68/120    avg_loss:0.057, val_acc:0.962]
Epoch [69/120    avg_loss:0.051, val_acc:0.962]
Epoch [70/120    avg_loss:0.042, val_acc:0.966]
Epoch [71/120    avg_loss:0.044, val_acc:0.968]
Epoch [72/120    avg_loss:0.039, val_acc:0.976]
Epoch [73/120    avg_loss:0.031, val_acc:0.967]
Epoch [74/120    avg_loss:0.028, val_acc:0.976]
Epoch [75/120    avg_loss:0.030, val_acc:0.976]
Epoch [76/120    avg_loss:0.037, val_acc:0.968]
Epoch [77/120    avg_loss:0.035, val_acc:0.971]
Epoch [78/120    avg_loss:0.027, val_acc:0.973]
Epoch [79/120    avg_loss:0.033, val_acc:0.968]
Epoch [80/120    avg_loss:0.038, val_acc:0.972]
Epoch [81/120    avg_loss:0.031, val_acc:0.969]
Epoch [82/120    avg_loss:0.029, val_acc:0.972]
Epoch [83/120    avg_loss:0.023, val_acc:0.970]
Epoch [84/120    avg_loss:0.030, val_acc:0.980]
Epoch [85/120    avg_loss:0.023, val_acc:0.975]
Epoch [86/120    avg_loss:0.025, val_acc:0.983]
Epoch [87/120    avg_loss:0.026, val_acc:0.969]
Epoch [88/120    avg_loss:0.030, val_acc:0.971]
Epoch [89/120    avg_loss:0.025, val_acc:0.977]
Epoch [90/120    avg_loss:0.027, val_acc:0.982]
Epoch [91/120    avg_loss:0.033, val_acc:0.963]
Epoch [92/120    avg_loss:0.033, val_acc:0.983]
Epoch [93/120    avg_loss:0.023, val_acc:0.970]
Epoch [94/120    avg_loss:0.031, val_acc:0.958]
Epoch [95/120    avg_loss:0.063, val_acc:0.960]
Epoch [96/120    avg_loss:0.041, val_acc:0.956]
Epoch [97/120    avg_loss:0.043, val_acc:0.963]
Epoch [98/120    avg_loss:0.025, val_acc:0.973]
Epoch [99/120    avg_loss:0.022, val_acc:0.973]
Epoch [100/120    avg_loss:0.027, val_acc:0.972]
Epoch [101/120    avg_loss:0.039, val_acc:0.972]
Epoch [102/120    avg_loss:0.035, val_acc:0.964]
Epoch [103/120    avg_loss:0.023, val_acc:0.973]
Epoch [104/120    avg_loss:0.015, val_acc:0.981]
Epoch [105/120    avg_loss:0.013, val_acc:0.977]
Epoch [106/120    avg_loss:0.014, val_acc:0.980]
Epoch [107/120    avg_loss:0.012, val_acc:0.980]
Epoch [108/120    avg_loss:0.011, val_acc:0.981]
Epoch [109/120    avg_loss:0.010, val_acc:0.981]
Epoch [110/120    avg_loss:0.011, val_acc:0.982]
Epoch [111/120    avg_loss:0.012, val_acc:0.981]
Epoch [112/120    avg_loss:0.011, val_acc:0.982]
Epoch [113/120    avg_loss:0.010, val_acc:0.981]
Epoch [114/120    avg_loss:0.011, val_acc:0.980]
Epoch [115/120    avg_loss:0.010, val_acc:0.980]
Epoch [116/120    avg_loss:0.017, val_acc:0.981]
Epoch [117/120    avg_loss:0.011, val_acc:0.982]
Epoch [118/120    avg_loss:0.013, val_acc:0.981]
Epoch [119/120    avg_loss:0.009, val_acc:0.981]
Epoch [120/120    avg_loss:0.012, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    1 1246    0    0    0    0    0    0    2   15   21    0    0
     0    0    0]
 [   0    0    2  713    4    0    0    0    0   13    4    8    3    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  428    0    4    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   12    0    0    1    1    0    0    0  840   18    1    0
     1    1    0]
 [   0    0    2    0    0    0    1    0    0    3    9 2191    1    2
     0    1    0]
 [   0    0    0    0    0    4    0    0    0    2    2    1  516    0
     0    6    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1123   16    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    77  251    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.06233062330624

F1 scores:
[       nan 0.96296296 0.97840597 0.97604381 0.98834499 0.98504028
 0.98350825 0.92592593 0.99883586 0.61818182 0.96164854 0.9847191
 0.97634816 0.99191375 0.95860009 0.80707395 0.98245614]

Kappa:
0.966478082990584
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8db853d898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.694, val_acc:0.294]
Epoch [2/120    avg_loss:2.346, val_acc:0.451]
Epoch [3/120    avg_loss:2.193, val_acc:0.482]
Epoch [4/120    avg_loss:2.057, val_acc:0.486]
Epoch [5/120    avg_loss:1.920, val_acc:0.536]
Epoch [6/120    avg_loss:1.765, val_acc:0.551]
Epoch [7/120    avg_loss:1.724, val_acc:0.599]
Epoch [8/120    avg_loss:1.601, val_acc:0.613]
Epoch [9/120    avg_loss:1.529, val_acc:0.665]
Epoch [10/120    avg_loss:1.369, val_acc:0.688]
Epoch [11/120    avg_loss:1.267, val_acc:0.710]
Epoch [12/120    avg_loss:1.072, val_acc:0.702]
Epoch [13/120    avg_loss:1.062, val_acc:0.713]
Epoch [14/120    avg_loss:0.958, val_acc:0.747]
Epoch [15/120    avg_loss:0.806, val_acc:0.722]
Epoch [16/120    avg_loss:0.770, val_acc:0.768]
Epoch [17/120    avg_loss:0.715, val_acc:0.767]
Epoch [18/120    avg_loss:0.658, val_acc:0.738]
Epoch [19/120    avg_loss:0.630, val_acc:0.816]
Epoch [20/120    avg_loss:0.500, val_acc:0.787]
Epoch [21/120    avg_loss:0.478, val_acc:0.818]
Epoch [22/120    avg_loss:0.536, val_acc:0.780]
Epoch [23/120    avg_loss:0.456, val_acc:0.827]
Epoch [24/120    avg_loss:0.344, val_acc:0.864]
Epoch [25/120    avg_loss:0.405, val_acc:0.847]
Epoch [26/120    avg_loss:0.372, val_acc:0.883]
Epoch [27/120    avg_loss:0.344, val_acc:0.857]
Epoch [28/120    avg_loss:0.280, val_acc:0.908]
Epoch [29/120    avg_loss:0.254, val_acc:0.894]
Epoch [30/120    avg_loss:0.288, val_acc:0.876]
Epoch [31/120    avg_loss:0.258, val_acc:0.848]
Epoch [32/120    avg_loss:0.238, val_acc:0.880]
Epoch [33/120    avg_loss:0.202, val_acc:0.901]
Epoch [34/120    avg_loss:0.170, val_acc:0.927]
Epoch [35/120    avg_loss:0.171, val_acc:0.921]
Epoch [36/120    avg_loss:0.156, val_acc:0.919]
Epoch [37/120    avg_loss:0.145, val_acc:0.927]
Epoch [38/120    avg_loss:0.123, val_acc:0.933]
Epoch [39/120    avg_loss:0.138, val_acc:0.894]
Epoch [40/120    avg_loss:0.170, val_acc:0.920]
Epoch [41/120    avg_loss:0.156, val_acc:0.941]
Epoch [42/120    avg_loss:0.157, val_acc:0.925]
Epoch [43/120    avg_loss:0.152, val_acc:0.938]
Epoch [44/120    avg_loss:0.109, val_acc:0.933]
Epoch [45/120    avg_loss:0.101, val_acc:0.942]
Epoch [46/120    avg_loss:0.137, val_acc:0.947]
Epoch [47/120    avg_loss:0.116, val_acc:0.933]
Epoch [48/120    avg_loss:0.092, val_acc:0.950]
Epoch [49/120    avg_loss:0.090, val_acc:0.963]
Epoch [50/120    avg_loss:0.070, val_acc:0.949]
Epoch [51/120    avg_loss:0.067, val_acc:0.948]
Epoch [52/120    avg_loss:0.070, val_acc:0.946]
Epoch [53/120    avg_loss:0.099, val_acc:0.951]
Epoch [54/120    avg_loss:0.081, val_acc:0.932]
Epoch [55/120    avg_loss:0.068, val_acc:0.955]
Epoch [56/120    avg_loss:0.063, val_acc:0.944]
Epoch [57/120    avg_loss:0.061, val_acc:0.954]
Epoch [58/120    avg_loss:0.074, val_acc:0.945]
Epoch [59/120    avg_loss:0.058, val_acc:0.953]
Epoch [60/120    avg_loss:0.060, val_acc:0.953]
Epoch [61/120    avg_loss:0.056, val_acc:0.945]
Epoch [62/120    avg_loss:0.064, val_acc:0.955]
Epoch [63/120    avg_loss:0.050, val_acc:0.965]
Epoch [64/120    avg_loss:0.035, val_acc:0.968]
Epoch [65/120    avg_loss:0.034, val_acc:0.967]
Epoch [66/120    avg_loss:0.030, val_acc:0.967]
Epoch [67/120    avg_loss:0.032, val_acc:0.967]
Epoch [68/120    avg_loss:0.034, val_acc:0.970]
Epoch [69/120    avg_loss:0.037, val_acc:0.971]
Epoch [70/120    avg_loss:0.038, val_acc:0.973]
Epoch [71/120    avg_loss:0.025, val_acc:0.973]
Epoch [72/120    avg_loss:0.024, val_acc:0.969]
Epoch [73/120    avg_loss:0.036, val_acc:0.967]
Epoch [74/120    avg_loss:0.025, val_acc:0.969]
Epoch [75/120    avg_loss:0.028, val_acc:0.972]
Epoch [76/120    avg_loss:0.026, val_acc:0.971]
Epoch [77/120    avg_loss:0.028, val_acc:0.969]
Epoch [78/120    avg_loss:0.027, val_acc:0.968]
Epoch [79/120    avg_loss:0.025, val_acc:0.969]
Epoch [80/120    avg_loss:0.027, val_acc:0.970]
Epoch [81/120    avg_loss:0.029, val_acc:0.971]
Epoch [82/120    avg_loss:0.025, val_acc:0.969]
Epoch [83/120    avg_loss:0.025, val_acc:0.969]
Epoch [84/120    avg_loss:0.024, val_acc:0.971]
Epoch [85/120    avg_loss:0.021, val_acc:0.971]
Epoch [86/120    avg_loss:0.025, val_acc:0.971]
Epoch [87/120    avg_loss:0.025, val_acc:0.971]
Epoch [88/120    avg_loss:0.022, val_acc:0.972]
Epoch [89/120    avg_loss:0.025, val_acc:0.971]
Epoch [90/120    avg_loss:0.021, val_acc:0.970]
Epoch [91/120    avg_loss:0.025, val_acc:0.970]
Epoch [92/120    avg_loss:0.024, val_acc:0.970]
Epoch [93/120    avg_loss:0.024, val_acc:0.969]
Epoch [94/120    avg_loss:0.031, val_acc:0.969]
Epoch [95/120    avg_loss:0.026, val_acc:0.969]
Epoch [96/120    avg_loss:0.024, val_acc:0.970]
Epoch [97/120    avg_loss:0.024, val_acc:0.969]
Epoch [98/120    avg_loss:0.025, val_acc:0.969]
Epoch [99/120    avg_loss:0.022, val_acc:0.969]
Epoch [100/120    avg_loss:0.022, val_acc:0.969]
Epoch [101/120    avg_loss:0.025, val_acc:0.969]
Epoch [102/120    avg_loss:0.028, val_acc:0.969]
Epoch [103/120    avg_loss:0.022, val_acc:0.969]
Epoch [104/120    avg_loss:0.028, val_acc:0.969]
Epoch [105/120    avg_loss:0.025, val_acc:0.969]
Epoch [106/120    avg_loss:0.020, val_acc:0.969]
Epoch [107/120    avg_loss:0.029, val_acc:0.969]
Epoch [108/120    avg_loss:0.025, val_acc:0.969]
Epoch [109/120    avg_loss:0.023, val_acc:0.969]
Epoch [110/120    avg_loss:0.023, val_acc:0.969]
Epoch [111/120    avg_loss:0.023, val_acc:0.969]
Epoch [112/120    avg_loss:0.026, val_acc:0.969]
Epoch [113/120    avg_loss:0.023, val_acc:0.969]
Epoch [114/120    avg_loss:0.026, val_acc:0.969]
Epoch [115/120    avg_loss:0.022, val_acc:0.969]
Epoch [116/120    avg_loss:0.024, val_acc:0.969]
Epoch [117/120    avg_loss:0.021, val_acc:0.969]
Epoch [118/120    avg_loss:0.025, val_acc:0.969]
Epoch [119/120    avg_loss:0.024, val_acc:0.969]
Epoch [120/120    avg_loss:0.024, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1245    4    5    0    1    0    0    0    6   21    3    0
     0    0    0]
 [   0    0    0  688    4    0    0    0    0   12    2   36    5    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    9    1    0    3    3    0    0    2  830   24    2    0
     0    1    0]
 [   0    0    8    0    0    1    2    0    0    0   14 2165   18    2
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    3    1  522    0
     2    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0    0
  1129    9    0]
 [   0    0    0    0    0    0   31    0    0    1    0    0    0    0
    22  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.99728997289972

F1 scores:
[       nan 0.94871795 0.97762073 0.95489244 0.97695853 0.98969072
 0.97189349 1.         0.997669   0.68       0.95732411 0.97128757
 0.96044158 0.99462366 0.983878   0.89877301 0.98224852]

Kappa:
0.9657464514217771
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7650fce898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.738, val_acc:0.305]
Epoch [2/120    avg_loss:2.465, val_acc:0.429]
Epoch [3/120    avg_loss:2.269, val_acc:0.539]
Epoch [4/120    avg_loss:2.091, val_acc:0.548]
Epoch [5/120    avg_loss:1.972, val_acc:0.564]
Epoch [6/120    avg_loss:1.858, val_acc:0.621]
Epoch [7/120    avg_loss:1.784, val_acc:0.615]
Epoch [8/120    avg_loss:1.660, val_acc:0.631]
Epoch [9/120    avg_loss:1.537, val_acc:0.664]
Epoch [10/120    avg_loss:1.418, val_acc:0.661]
Epoch [11/120    avg_loss:1.309, val_acc:0.666]
Epoch [12/120    avg_loss:1.216, val_acc:0.689]
Epoch [13/120    avg_loss:1.070, val_acc:0.695]
Epoch [14/120    avg_loss:1.033, val_acc:0.707]
Epoch [15/120    avg_loss:0.967, val_acc:0.727]
Epoch [16/120    avg_loss:0.845, val_acc:0.759]
Epoch [17/120    avg_loss:0.784, val_acc:0.769]
Epoch [18/120    avg_loss:0.682, val_acc:0.791]
Epoch [19/120    avg_loss:0.650, val_acc:0.802]
Epoch [20/120    avg_loss:0.577, val_acc:0.830]
Epoch [21/120    avg_loss:0.505, val_acc:0.864]
Epoch [22/120    avg_loss:0.426, val_acc:0.876]
Epoch [23/120    avg_loss:0.425, val_acc:0.856]
Epoch [24/120    avg_loss:0.448, val_acc:0.825]
Epoch [25/120    avg_loss:0.385, val_acc:0.871]
Epoch [26/120    avg_loss:0.317, val_acc:0.898]
Epoch [27/120    avg_loss:0.299, val_acc:0.900]
Epoch [28/120    avg_loss:0.251, val_acc:0.920]
Epoch [29/120    avg_loss:0.231, val_acc:0.922]
Epoch [30/120    avg_loss:0.209, val_acc:0.908]
Epoch [31/120    avg_loss:0.212, val_acc:0.912]
Epoch [32/120    avg_loss:0.212, val_acc:0.925]
Epoch [33/120    avg_loss:0.198, val_acc:0.914]
Epoch [34/120    avg_loss:0.199, val_acc:0.927]
Epoch [35/120    avg_loss:0.180, val_acc:0.901]
Epoch [36/120    avg_loss:0.192, val_acc:0.931]
Epoch [37/120    avg_loss:0.220, val_acc:0.858]
Epoch [38/120    avg_loss:0.275, val_acc:0.899]
Epoch [39/120    avg_loss:0.183, val_acc:0.934]
Epoch [40/120    avg_loss:0.150, val_acc:0.945]
Epoch [41/120    avg_loss:0.182, val_acc:0.895]
Epoch [42/120    avg_loss:0.148, val_acc:0.936]
Epoch [43/120    avg_loss:0.117, val_acc:0.933]
Epoch [44/120    avg_loss:0.100, val_acc:0.953]
Epoch [45/120    avg_loss:0.088, val_acc:0.942]
Epoch [46/120    avg_loss:0.096, val_acc:0.957]
Epoch [47/120    avg_loss:0.095, val_acc:0.945]
Epoch [48/120    avg_loss:0.084, val_acc:0.946]
Epoch [49/120    avg_loss:0.097, val_acc:0.947]
Epoch [50/120    avg_loss:0.118, val_acc:0.944]
Epoch [51/120    avg_loss:0.084, val_acc:0.952]
Epoch [52/120    avg_loss:0.075, val_acc:0.958]
Epoch [53/120    avg_loss:0.069, val_acc:0.953]
Epoch [54/120    avg_loss:0.094, val_acc:0.940]
Epoch [55/120    avg_loss:0.077, val_acc:0.959]
Epoch [56/120    avg_loss:0.058, val_acc:0.954]
Epoch [57/120    avg_loss:0.068, val_acc:0.962]
Epoch [58/120    avg_loss:0.064, val_acc:0.959]
Epoch [59/120    avg_loss:0.051, val_acc:0.957]
Epoch [60/120    avg_loss:0.050, val_acc:0.960]
Epoch [61/120    avg_loss:0.068, val_acc:0.956]
Epoch [62/120    avg_loss:0.075, val_acc:0.966]
Epoch [63/120    avg_loss:0.070, val_acc:0.964]
Epoch [64/120    avg_loss:0.055, val_acc:0.958]
Epoch [65/120    avg_loss:0.045, val_acc:0.971]
Epoch [66/120    avg_loss:0.037, val_acc:0.972]
Epoch [67/120    avg_loss:0.034, val_acc:0.972]
Epoch [68/120    avg_loss:0.033, val_acc:0.973]
Epoch [69/120    avg_loss:0.038, val_acc:0.963]
Epoch [70/120    avg_loss:0.036, val_acc:0.969]
Epoch [71/120    avg_loss:0.033, val_acc:0.961]
Epoch [72/120    avg_loss:0.036, val_acc:0.975]
Epoch [73/120    avg_loss:0.037, val_acc:0.970]
Epoch [74/120    avg_loss:0.040, val_acc:0.973]
Epoch [75/120    avg_loss:0.031, val_acc:0.980]
Epoch [76/120    avg_loss:0.027, val_acc:0.971]
Epoch [77/120    avg_loss:0.025, val_acc:0.981]
Epoch [78/120    avg_loss:0.018, val_acc:0.980]
Epoch [79/120    avg_loss:0.032, val_acc:0.977]
Epoch [80/120    avg_loss:0.026, val_acc:0.978]
Epoch [81/120    avg_loss:0.028, val_acc:0.977]
Epoch [82/120    avg_loss:0.022, val_acc:0.977]
Epoch [83/120    avg_loss:0.047, val_acc:0.968]
Epoch [84/120    avg_loss:0.034, val_acc:0.968]
Epoch [85/120    avg_loss:0.029, val_acc:0.976]
Epoch [86/120    avg_loss:0.024, val_acc:0.974]
Epoch [87/120    avg_loss:0.023, val_acc:0.980]
Epoch [88/120    avg_loss:0.022, val_acc:0.980]
Epoch [89/120    avg_loss:0.022, val_acc:0.981]
Epoch [90/120    avg_loss:0.021, val_acc:0.981]
Epoch [91/120    avg_loss:0.016, val_acc:0.978]
Epoch [92/120    avg_loss:0.019, val_acc:0.974]
Epoch [93/120    avg_loss:0.015, val_acc:0.983]
Epoch [94/120    avg_loss:0.012, val_acc:0.982]
Epoch [95/120    avg_loss:0.017, val_acc:0.976]
Epoch [96/120    avg_loss:0.024, val_acc:0.977]
Epoch [97/120    avg_loss:0.039, val_acc:0.974]
Epoch [98/120    avg_loss:0.031, val_acc:0.961]
Epoch [99/120    avg_loss:0.027, val_acc:0.980]
Epoch [100/120    avg_loss:0.023, val_acc:0.982]
Epoch [101/120    avg_loss:0.036, val_acc:0.954]
Epoch [102/120    avg_loss:0.077, val_acc:0.964]
Epoch [103/120    avg_loss:0.030, val_acc:0.974]
Epoch [104/120    avg_loss:0.021, val_acc:0.985]
Epoch [105/120    avg_loss:0.018, val_acc:0.976]
Epoch [106/120    avg_loss:0.017, val_acc:0.970]
Epoch [107/120    avg_loss:0.014, val_acc:0.975]
Epoch [108/120    avg_loss:0.022, val_acc:0.975]
Epoch [109/120    avg_loss:0.016, val_acc:0.983]
Epoch [110/120    avg_loss:0.014, val_acc:0.978]
Epoch [111/120    avg_loss:0.011, val_acc:0.978]
Epoch [112/120    avg_loss:0.011, val_acc:0.977]
Epoch [113/120    avg_loss:0.017, val_acc:0.974]
Epoch [114/120    avg_loss:0.015, val_acc:0.977]
Epoch [115/120    avg_loss:0.014, val_acc:0.984]
Epoch [116/120    avg_loss:0.012, val_acc:0.983]
Epoch [117/120    avg_loss:0.011, val_acc:0.983]
Epoch [118/120    avg_loss:0.011, val_acc:0.983]
Epoch [119/120    avg_loss:0.009, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1248    1    4    0    4    0    0    0    8   20    0    0
     0    0    0]
 [   0    0    1  725    7    0    0    0    0    1    1    0   12    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    5    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   11    0    0    0    6    0    0    0  844   11    0    0
     0    3    0]
 [   0    0    5    0    0    0    2    0    0    1    7 2194    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    4   13  514    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1123   16    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
     8  329    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
98.08130081300813

F1 scores:
[       nan 0.94871795 0.97882353 0.98438561 0.97482838 0.99305556
 0.98277154 0.90909091 1.         0.89473684 0.96844521 0.98651079
 0.96344892 1.         0.98899163 0.94404591 0.96341463]

Kappa:
0.9781191971717593
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f87caa57908>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.769, val_acc:0.412]
Epoch [2/120    avg_loss:2.479, val_acc:0.450]
Epoch [3/120    avg_loss:2.257, val_acc:0.500]
Epoch [4/120    avg_loss:2.129, val_acc:0.537]
Epoch [5/120    avg_loss:1.995, val_acc:0.557]
Epoch [6/120    avg_loss:1.898, val_acc:0.594]
Epoch [7/120    avg_loss:1.753, val_acc:0.602]
Epoch [8/120    avg_loss:1.688, val_acc:0.637]
Epoch [9/120    avg_loss:1.558, val_acc:0.639]
Epoch [10/120    avg_loss:1.436, val_acc:0.678]
Epoch [11/120    avg_loss:1.322, val_acc:0.683]
Epoch [12/120    avg_loss:1.241, val_acc:0.691]
Epoch [13/120    avg_loss:1.133, val_acc:0.697]
Epoch [14/120    avg_loss:1.064, val_acc:0.733]
Epoch [15/120    avg_loss:0.905, val_acc:0.716]
Epoch [16/120    avg_loss:0.842, val_acc:0.737]
Epoch [17/120    avg_loss:0.804, val_acc:0.737]
Epoch [18/120    avg_loss:0.660, val_acc:0.774]
Epoch [19/120    avg_loss:0.624, val_acc:0.753]
Epoch [20/120    avg_loss:0.685, val_acc:0.753]
Epoch [21/120    avg_loss:0.754, val_acc:0.780]
Epoch [22/120    avg_loss:0.692, val_acc:0.754]
Epoch [23/120    avg_loss:0.576, val_acc:0.806]
Epoch [24/120    avg_loss:0.461, val_acc:0.825]
Epoch [25/120    avg_loss:0.444, val_acc:0.844]
Epoch [26/120    avg_loss:0.351, val_acc:0.863]
Epoch [27/120    avg_loss:0.333, val_acc:0.848]
Epoch [28/120    avg_loss:0.329, val_acc:0.892]
Epoch [29/120    avg_loss:0.301, val_acc:0.872]
Epoch [30/120    avg_loss:0.288, val_acc:0.855]
Epoch [31/120    avg_loss:0.252, val_acc:0.890]
Epoch [32/120    avg_loss:0.230, val_acc:0.900]
Epoch [33/120    avg_loss:0.316, val_acc:0.861]
Epoch [34/120    avg_loss:0.270, val_acc:0.873]
Epoch [35/120    avg_loss:0.220, val_acc:0.905]
Epoch [36/120    avg_loss:0.202, val_acc:0.857]
Epoch [37/120    avg_loss:0.221, val_acc:0.901]
Epoch [38/120    avg_loss:0.215, val_acc:0.881]
Epoch [39/120    avg_loss:0.183, val_acc:0.900]
Epoch [40/120    avg_loss:0.202, val_acc:0.929]
Epoch [41/120    avg_loss:0.148, val_acc:0.917]
Epoch [42/120    avg_loss:0.179, val_acc:0.914]
Epoch [43/120    avg_loss:0.149, val_acc:0.905]
Epoch [44/120    avg_loss:0.262, val_acc:0.859]
Epoch [45/120    avg_loss:0.280, val_acc:0.895]
Epoch [46/120    avg_loss:0.271, val_acc:0.897]
Epoch [47/120    avg_loss:0.196, val_acc:0.901]
Epoch [48/120    avg_loss:0.163, val_acc:0.921]
Epoch [49/120    avg_loss:0.140, val_acc:0.933]
Epoch [50/120    avg_loss:0.115, val_acc:0.931]
Epoch [51/120    avg_loss:0.099, val_acc:0.935]
Epoch [52/120    avg_loss:0.107, val_acc:0.939]
Epoch [53/120    avg_loss:0.088, val_acc:0.950]
Epoch [54/120    avg_loss:0.092, val_acc:0.939]
Epoch [55/120    avg_loss:0.084, val_acc:0.962]
Epoch [56/120    avg_loss:0.079, val_acc:0.939]
Epoch [57/120    avg_loss:0.081, val_acc:0.942]
Epoch [58/120    avg_loss:0.075, val_acc:0.953]
Epoch [59/120    avg_loss:0.063, val_acc:0.955]
Epoch [60/120    avg_loss:0.056, val_acc:0.940]
Epoch [61/120    avg_loss:0.065, val_acc:0.956]
Epoch [62/120    avg_loss:0.071, val_acc:0.949]
Epoch [63/120    avg_loss:0.079, val_acc:0.944]
Epoch [64/120    avg_loss:0.053, val_acc:0.963]
Epoch [65/120    avg_loss:0.059, val_acc:0.947]
Epoch [66/120    avg_loss:0.054, val_acc:0.958]
Epoch [67/120    avg_loss:0.042, val_acc:0.953]
Epoch [68/120    avg_loss:0.054, val_acc:0.932]
Epoch [69/120    avg_loss:0.080, val_acc:0.947]
Epoch [70/120    avg_loss:0.058, val_acc:0.952]
Epoch [71/120    avg_loss:0.048, val_acc:0.948]
Epoch [72/120    avg_loss:0.047, val_acc:0.958]
Epoch [73/120    avg_loss:0.056, val_acc:0.946]
Epoch [74/120    avg_loss:0.058, val_acc:0.911]
Epoch [75/120    avg_loss:0.044, val_acc:0.955]
Epoch [76/120    avg_loss:0.047, val_acc:0.958]
Epoch [77/120    avg_loss:0.042, val_acc:0.962]
Epoch [78/120    avg_loss:0.035, val_acc:0.964]
Epoch [79/120    avg_loss:0.028, val_acc:0.970]
Epoch [80/120    avg_loss:0.028, val_acc:0.972]
Epoch [81/120    avg_loss:0.034, val_acc:0.970]
Epoch [82/120    avg_loss:0.027, val_acc:0.972]
Epoch [83/120    avg_loss:0.030, val_acc:0.971]
Epoch [84/120    avg_loss:0.025, val_acc:0.968]
Epoch [85/120    avg_loss:0.028, val_acc:0.971]
Epoch [86/120    avg_loss:0.024, val_acc:0.972]
Epoch [87/120    avg_loss:0.022, val_acc:0.973]
Epoch [88/120    avg_loss:0.023, val_acc:0.971]
Epoch [89/120    avg_loss:0.028, val_acc:0.974]
Epoch [90/120    avg_loss:0.025, val_acc:0.974]
Epoch [91/120    avg_loss:0.025, val_acc:0.971]
Epoch [92/120    avg_loss:0.019, val_acc:0.972]
Epoch [93/120    avg_loss:0.022, val_acc:0.972]
Epoch [94/120    avg_loss:0.026, val_acc:0.972]
Epoch [95/120    avg_loss:0.025, val_acc:0.977]
Epoch [96/120    avg_loss:0.024, val_acc:0.973]
Epoch [97/120    avg_loss:0.023, val_acc:0.974]
Epoch [98/120    avg_loss:0.022, val_acc:0.974]
Epoch [99/120    avg_loss:0.022, val_acc:0.977]
Epoch [100/120    avg_loss:0.020, val_acc:0.976]
Epoch [101/120    avg_loss:0.019, val_acc:0.976]
Epoch [102/120    avg_loss:0.022, val_acc:0.973]
Epoch [103/120    avg_loss:0.024, val_acc:0.973]
Epoch [104/120    avg_loss:0.019, val_acc:0.973]
Epoch [105/120    avg_loss:0.020, val_acc:0.973]
Epoch [106/120    avg_loss:0.019, val_acc:0.977]
Epoch [107/120    avg_loss:0.025, val_acc:0.978]
Epoch [108/120    avg_loss:0.025, val_acc:0.974]
Epoch [109/120    avg_loss:0.017, val_acc:0.978]
Epoch [110/120    avg_loss:0.023, val_acc:0.977]
Epoch [111/120    avg_loss:0.019, val_acc:0.982]
Epoch [112/120    avg_loss:0.019, val_acc:0.976]
Epoch [113/120    avg_loss:0.024, val_acc:0.978]
Epoch [114/120    avg_loss:0.023, val_acc:0.980]
Epoch [115/120    avg_loss:0.017, val_acc:0.976]
Epoch [116/120    avg_loss:0.018, val_acc:0.975]
Epoch [117/120    avg_loss:0.020, val_acc:0.978]
Epoch [118/120    avg_loss:0.019, val_acc:0.976]
Epoch [119/120    avg_loss:0.021, val_acc:0.977]
Epoch [120/120    avg_loss:0.021, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1235    0    4    0    1    0    0    0   10   34    1    0
     0    0    0]
 [   0    0    0  708    1    4    0    0    0   10    6    0   17    1
     0    0    0]
 [   0    0    0    4  209    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    2    0    2    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    7    7    0    0    0  843   12    0    0
     1    1    0]
 [   0    0    4    0    0    4    6    0    0    0   16 2175    4    0
     1    0    0]
 [   0    0    0    1    0    3    0    0    0    0    2    7  515    0
     2    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   17    0    0    1    0    0    0    0    0
  1120    1    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
     6  316    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.42005420054201

F1 scores:
[       nan 1.         0.97705696 0.96919918 0.97892272 0.95555556
 0.9704142  0.96153846 0.99883856 0.72340426 0.96232877 0.97972973
 0.95992544 0.99730458 0.98721904 0.95037594 0.96470588]

Kappa:
0.9705906310273685
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f73a62a8860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.742, val_acc:0.166]
Epoch [2/120    avg_loss:2.459, val_acc:0.302]
Epoch [3/120    avg_loss:2.197, val_acc:0.510]
Epoch [4/120    avg_loss:2.029, val_acc:0.534]
Epoch [5/120    avg_loss:1.925, val_acc:0.577]
Epoch [6/120    avg_loss:1.760, val_acc:0.553]
Epoch [7/120    avg_loss:1.667, val_acc:0.583]
Epoch [8/120    avg_loss:1.521, val_acc:0.625]
Epoch [9/120    avg_loss:1.439, val_acc:0.617]
Epoch [10/120    avg_loss:1.346, val_acc:0.638]
Epoch [11/120    avg_loss:1.321, val_acc:0.670]
Epoch [12/120    avg_loss:1.185, val_acc:0.691]
Epoch [13/120    avg_loss:1.048, val_acc:0.735]
Epoch [14/120    avg_loss:0.985, val_acc:0.717]
Epoch [15/120    avg_loss:0.870, val_acc:0.733]
Epoch [16/120    avg_loss:0.795, val_acc:0.783]
Epoch [17/120    avg_loss:0.710, val_acc:0.808]
Epoch [18/120    avg_loss:0.641, val_acc:0.770]
Epoch [19/120    avg_loss:0.587, val_acc:0.796]
Epoch [20/120    avg_loss:0.572, val_acc:0.836]
Epoch [21/120    avg_loss:0.538, val_acc:0.846]
Epoch [22/120    avg_loss:0.462, val_acc:0.861]
Epoch [23/120    avg_loss:0.448, val_acc:0.851]
Epoch [24/120    avg_loss:0.455, val_acc:0.853]
Epoch [25/120    avg_loss:0.361, val_acc:0.867]
Epoch [26/120    avg_loss:0.338, val_acc:0.877]
Epoch [27/120    avg_loss:0.289, val_acc:0.877]
Epoch [28/120    avg_loss:0.315, val_acc:0.904]
Epoch [29/120    avg_loss:0.294, val_acc:0.911]
Epoch [30/120    avg_loss:0.267, val_acc:0.894]
Epoch [31/120    avg_loss:0.259, val_acc:0.926]
Epoch [32/120    avg_loss:0.216, val_acc:0.927]
Epoch [33/120    avg_loss:0.233, val_acc:0.912]
Epoch [34/120    avg_loss:0.208, val_acc:0.911]
Epoch [35/120    avg_loss:0.205, val_acc:0.914]
Epoch [36/120    avg_loss:0.272, val_acc:0.906]
Epoch [37/120    avg_loss:0.248, val_acc:0.918]
Epoch [38/120    avg_loss:0.211, val_acc:0.901]
Epoch [39/120    avg_loss:0.232, val_acc:0.933]
Epoch [40/120    avg_loss:0.151, val_acc:0.945]
Epoch [41/120    avg_loss:0.133, val_acc:0.944]
Epoch [42/120    avg_loss:0.144, val_acc:0.923]
Epoch [43/120    avg_loss:0.160, val_acc:0.942]
Epoch [44/120    avg_loss:0.142, val_acc:0.930]
Epoch [45/120    avg_loss:0.131, val_acc:0.941]
Epoch [46/120    avg_loss:0.087, val_acc:0.961]
Epoch [47/120    avg_loss:0.102, val_acc:0.952]
Epoch [48/120    avg_loss:0.086, val_acc:0.953]
Epoch [49/120    avg_loss:0.092, val_acc:0.957]
Epoch [50/120    avg_loss:0.092, val_acc:0.956]
Epoch [51/120    avg_loss:0.083, val_acc:0.963]
Epoch [52/120    avg_loss:0.074, val_acc:0.954]
Epoch [53/120    avg_loss:0.070, val_acc:0.949]
Epoch [54/120    avg_loss:0.085, val_acc:0.954]
Epoch [55/120    avg_loss:0.068, val_acc:0.950]
Epoch [56/120    avg_loss:0.070, val_acc:0.949]
Epoch [57/120    avg_loss:0.064, val_acc:0.964]
Epoch [58/120    avg_loss:0.065, val_acc:0.966]
Epoch [59/120    avg_loss:0.071, val_acc:0.955]
Epoch [60/120    avg_loss:0.071, val_acc:0.963]
Epoch [61/120    avg_loss:0.064, val_acc:0.963]
Epoch [62/120    avg_loss:0.054, val_acc:0.963]
Epoch [63/120    avg_loss:0.049, val_acc:0.956]
Epoch [64/120    avg_loss:0.080, val_acc:0.944]
Epoch [65/120    avg_loss:0.312, val_acc:0.915]
Epoch [66/120    avg_loss:0.169, val_acc:0.957]
Epoch [67/120    avg_loss:0.101, val_acc:0.943]
Epoch [68/120    avg_loss:0.087, val_acc:0.918]
Epoch [69/120    avg_loss:0.079, val_acc:0.957]
Epoch [70/120    avg_loss:0.067, val_acc:0.941]
Epoch [71/120    avg_loss:0.094, val_acc:0.960]
Epoch [72/120    avg_loss:0.073, val_acc:0.968]
Epoch [73/120    avg_loss:0.044, val_acc:0.972]
Epoch [74/120    avg_loss:0.051, val_acc:0.971]
Epoch [75/120    avg_loss:0.044, val_acc:0.971]
Epoch [76/120    avg_loss:0.039, val_acc:0.973]
Epoch [77/120    avg_loss:0.039, val_acc:0.971]
Epoch [78/120    avg_loss:0.034, val_acc:0.973]
Epoch [79/120    avg_loss:0.039, val_acc:0.970]
Epoch [80/120    avg_loss:0.042, val_acc:0.971]
Epoch [81/120    avg_loss:0.041, val_acc:0.970]
Epoch [82/120    avg_loss:0.040, val_acc:0.970]
Epoch [83/120    avg_loss:0.031, val_acc:0.972]
Epoch [84/120    avg_loss:0.035, val_acc:0.973]
Epoch [85/120    avg_loss:0.033, val_acc:0.972]
Epoch [86/120    avg_loss:0.040, val_acc:0.973]
Epoch [87/120    avg_loss:0.033, val_acc:0.973]
Epoch [88/120    avg_loss:0.031, val_acc:0.971]
Epoch [89/120    avg_loss:0.033, val_acc:0.972]
Epoch [90/120    avg_loss:0.034, val_acc:0.971]
Epoch [91/120    avg_loss:0.028, val_acc:0.972]
Epoch [92/120    avg_loss:0.037, val_acc:0.972]
Epoch [93/120    avg_loss:0.034, val_acc:0.974]
Epoch [94/120    avg_loss:0.030, val_acc:0.974]
Epoch [95/120    avg_loss:0.032, val_acc:0.968]
Epoch [96/120    avg_loss:0.030, val_acc:0.971]
Epoch [97/120    avg_loss:0.030, val_acc:0.971]
Epoch [98/120    avg_loss:0.028, val_acc:0.972]
Epoch [99/120    avg_loss:0.027, val_acc:0.974]
Epoch [100/120    avg_loss:0.029, val_acc:0.976]
Epoch [101/120    avg_loss:0.028, val_acc:0.972]
Epoch [102/120    avg_loss:0.028, val_acc:0.971]
Epoch [103/120    avg_loss:0.028, val_acc:0.972]
Epoch [104/120    avg_loss:0.028, val_acc:0.974]
Epoch [105/120    avg_loss:0.028, val_acc:0.975]
Epoch [106/120    avg_loss:0.028, val_acc:0.974]
Epoch [107/120    avg_loss:0.027, val_acc:0.976]
Epoch [108/120    avg_loss:0.027, val_acc:0.975]
Epoch [109/120    avg_loss:0.027, val_acc:0.971]
Epoch [110/120    avg_loss:0.027, val_acc:0.976]
Epoch [111/120    avg_loss:0.023, val_acc:0.972]
Epoch [112/120    avg_loss:0.031, val_acc:0.972]
Epoch [113/120    avg_loss:0.031, val_acc:0.975]
Epoch [114/120    avg_loss:0.028, val_acc:0.973]
Epoch [115/120    avg_loss:0.026, val_acc:0.975]
Epoch [116/120    avg_loss:0.026, val_acc:0.973]
Epoch [117/120    avg_loss:0.031, val_acc:0.973]
Epoch [118/120    avg_loss:0.025, val_acc:0.973]
Epoch [119/120    avg_loss:0.030, val_acc:0.975]
Epoch [120/120    avg_loss:0.025, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1240    2    0    3    3    0    0    0   10   25    2    0
     0    0    0]
 [   0    0    0  707    6    5    0    0    0   10    0    0   19    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    5    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    3    0    0    7    1    0    0    0  857    6    0    0
     0    1    0]
 [   0    0   10    0    0    0    4    0    0    0   37 2156    1    2
     0    0    0]
 [   0    0    0    2    0    1    0    0    0    0    2    0  523    0
     0    1    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    1    0    0    0
  1125    9    0]
 [   0    0    0    0    0    0   28    0    0    0    0    0    0    0
     4  315    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.53929539295393

F1 scores:
[       nan 0.98765432 0.97714736 0.96982167 0.98611111 0.97632469
 0.96798213 1.         0.99767442 0.73913043 0.96130118 0.97955475
 0.96672828 0.99462366 0.99075297 0.93610698 0.96511628]

Kappa:
0.9719675639652112
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe84a4577f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.726, val_acc:0.277]
Epoch [2/120    avg_loss:2.423, val_acc:0.500]
Epoch [3/120    avg_loss:2.236, val_acc:0.593]
Epoch [4/120    avg_loss:2.069, val_acc:0.605]
Epoch [5/120    avg_loss:1.931, val_acc:0.630]
Epoch [6/120    avg_loss:1.810, val_acc:0.615]
Epoch [7/120    avg_loss:1.640, val_acc:0.664]
Epoch [8/120    avg_loss:1.569, val_acc:0.683]
Epoch [9/120    avg_loss:1.462, val_acc:0.722]
Epoch [10/120    avg_loss:1.322, val_acc:0.673]
Epoch [11/120    avg_loss:1.234, val_acc:0.684]
Epoch [12/120    avg_loss:1.103, val_acc:0.724]
Epoch [13/120    avg_loss:0.983, val_acc:0.747]
Epoch [14/120    avg_loss:0.931, val_acc:0.736]
Epoch [15/120    avg_loss:0.854, val_acc:0.742]
Epoch [16/120    avg_loss:0.790, val_acc:0.745]
Epoch [17/120    avg_loss:0.857, val_acc:0.758]
Epoch [18/120    avg_loss:0.732, val_acc:0.808]
Epoch [19/120    avg_loss:0.607, val_acc:0.815]
Epoch [20/120    avg_loss:0.587, val_acc:0.775]
Epoch [21/120    avg_loss:0.575, val_acc:0.761]
Epoch [22/120    avg_loss:0.573, val_acc:0.820]
Epoch [23/120    avg_loss:0.501, val_acc:0.836]
Epoch [24/120    avg_loss:0.470, val_acc:0.822]
Epoch [25/120    avg_loss:0.446, val_acc:0.870]
Epoch [26/120    avg_loss:0.409, val_acc:0.807]
Epoch [27/120    avg_loss:0.491, val_acc:0.727]
Epoch [28/120    avg_loss:0.482, val_acc:0.879]
Epoch [29/120    avg_loss:0.383, val_acc:0.869]
Epoch [30/120    avg_loss:0.314, val_acc:0.887]
Epoch [31/120    avg_loss:0.287, val_acc:0.873]
Epoch [32/120    avg_loss:0.348, val_acc:0.875]
Epoch [33/120    avg_loss:0.275, val_acc:0.881]
Epoch [34/120    avg_loss:0.245, val_acc:0.894]
Epoch [35/120    avg_loss:0.229, val_acc:0.919]
Epoch [36/120    avg_loss:0.223, val_acc:0.926]
Epoch [37/120    avg_loss:0.214, val_acc:0.911]
Epoch [38/120    avg_loss:0.226, val_acc:0.912]
Epoch [39/120    avg_loss:0.176, val_acc:0.901]
Epoch [40/120    avg_loss:0.212, val_acc:0.779]
Epoch [41/120    avg_loss:0.376, val_acc:0.869]
Epoch [42/120    avg_loss:0.282, val_acc:0.885]
Epoch [43/120    avg_loss:0.196, val_acc:0.904]
Epoch [44/120    avg_loss:0.186, val_acc:0.918]
Epoch [45/120    avg_loss:0.174, val_acc:0.917]
Epoch [46/120    avg_loss:0.171, val_acc:0.915]
Epoch [47/120    avg_loss:0.147, val_acc:0.922]
Epoch [48/120    avg_loss:0.128, val_acc:0.941]
Epoch [49/120    avg_loss:0.132, val_acc:0.907]
Epoch [50/120    avg_loss:0.118, val_acc:0.939]
Epoch [51/120    avg_loss:0.103, val_acc:0.945]
Epoch [52/120    avg_loss:0.115, val_acc:0.936]
Epoch [53/120    avg_loss:0.108, val_acc:0.943]
Epoch [54/120    avg_loss:0.110, val_acc:0.942]
Epoch [55/120    avg_loss:0.111, val_acc:0.943]
Epoch [56/120    avg_loss:0.096, val_acc:0.945]
Epoch [57/120    avg_loss:0.090, val_acc:0.942]
Epoch [58/120    avg_loss:0.100, val_acc:0.932]
Epoch [59/120    avg_loss:0.088, val_acc:0.940]
Epoch [60/120    avg_loss:0.105, val_acc:0.942]
Epoch [61/120    avg_loss:0.078, val_acc:0.955]
Epoch [62/120    avg_loss:0.091, val_acc:0.940]
Epoch [63/120    avg_loss:0.072, val_acc:0.948]
Epoch [64/120    avg_loss:0.069, val_acc:0.960]
Epoch [65/120    avg_loss:0.060, val_acc:0.954]
Epoch [66/120    avg_loss:0.065, val_acc:0.950]
Epoch [67/120    avg_loss:0.067, val_acc:0.952]
Epoch [68/120    avg_loss:0.053, val_acc:0.959]
Epoch [69/120    avg_loss:0.051, val_acc:0.956]
Epoch [70/120    avg_loss:0.069, val_acc:0.940]
Epoch [71/120    avg_loss:0.071, val_acc:0.946]
Epoch [72/120    avg_loss:0.051, val_acc:0.958]
Epoch [73/120    avg_loss:0.054, val_acc:0.956]
Epoch [74/120    avg_loss:0.050, val_acc:0.958]
Epoch [75/120    avg_loss:0.042, val_acc:0.963]
Epoch [76/120    avg_loss:0.046, val_acc:0.960]
Epoch [77/120    avg_loss:0.068, val_acc:0.925]
Epoch [78/120    avg_loss:0.070, val_acc:0.946]
Epoch [79/120    avg_loss:0.055, val_acc:0.962]
Epoch [80/120    avg_loss:0.056, val_acc:0.960]
Epoch [81/120    avg_loss:0.036, val_acc:0.962]
Epoch [82/120    avg_loss:0.056, val_acc:0.956]
Epoch [83/120    avg_loss:0.042, val_acc:0.962]
Epoch [84/120    avg_loss:0.045, val_acc:0.957]
Epoch [85/120    avg_loss:0.039, val_acc:0.962]
Epoch [86/120    avg_loss:0.042, val_acc:0.960]
Epoch [87/120    avg_loss:0.043, val_acc:0.967]
Epoch [88/120    avg_loss:0.041, val_acc:0.953]
Epoch [89/120    avg_loss:0.046, val_acc:0.954]
Epoch [90/120    avg_loss:0.059, val_acc:0.956]
Epoch [91/120    avg_loss:0.038, val_acc:0.956]
Epoch [92/120    avg_loss:0.044, val_acc:0.952]
Epoch [93/120    avg_loss:0.037, val_acc:0.954]
Epoch [94/120    avg_loss:0.058, val_acc:0.955]
Epoch [95/120    avg_loss:0.044, val_acc:0.966]
Epoch [96/120    avg_loss:0.035, val_acc:0.954]
Epoch [97/120    avg_loss:0.051, val_acc:0.963]
Epoch [98/120    avg_loss:0.042, val_acc:0.964]
Epoch [99/120    avg_loss:0.042, val_acc:0.962]
Epoch [100/120    avg_loss:0.031, val_acc:0.960]
Epoch [101/120    avg_loss:0.029, val_acc:0.964]
Epoch [102/120    avg_loss:0.022, val_acc:0.969]
Epoch [103/120    avg_loss:0.021, val_acc:0.968]
Epoch [104/120    avg_loss:0.023, val_acc:0.966]
Epoch [105/120    avg_loss:0.018, val_acc:0.967]
Epoch [106/120    avg_loss:0.023, val_acc:0.971]
Epoch [107/120    avg_loss:0.019, val_acc:0.970]
Epoch [108/120    avg_loss:0.027, val_acc:0.968]
Epoch [109/120    avg_loss:0.020, val_acc:0.970]
Epoch [110/120    avg_loss:0.019, val_acc:0.969]
Epoch [111/120    avg_loss:0.022, val_acc:0.970]
Epoch [112/120    avg_loss:0.022, val_acc:0.972]
Epoch [113/120    avg_loss:0.021, val_acc:0.970]
Epoch [114/120    avg_loss:0.021, val_acc:0.970]
Epoch [115/120    avg_loss:0.022, val_acc:0.969]
Epoch [116/120    avg_loss:0.021, val_acc:0.970]
Epoch [117/120    avg_loss:0.018, val_acc:0.970]
Epoch [118/120    avg_loss:0.017, val_acc:0.971]
Epoch [119/120    avg_loss:0.018, val_acc:0.972]
Epoch [120/120    avg_loss:0.020, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1247    6    0    6    0    0    0    3   11   12    0    0
     0    0    0]
 [   0    0    2  682    1   19    1    0    0    9    1    1   30    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    6    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    8    0    0    0    0    0    0  421    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   42    0    0    5    1    0    0    0  810   17    0    0
     0    0    0]
 [   0    0   17    0    0    0    0    0    0    0    1 2186    6    0
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    5    5  512    0
     3    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  184
     1    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1133    4    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    17  316    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.00813008130082

F1 scores:
[       nan 0.87356322 0.96182029 0.95052265 0.99765808 0.94866071
 0.98644578 0.89285714 0.98826291 0.73469388 0.94903339 0.98623957
 0.9455217  0.99728997 0.98650414 0.94752624 0.98245614]

Kappa:
0.9658831991672934
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f819b4bb860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.783, val_acc:0.277]
Epoch [2/120    avg_loss:2.540, val_acc:0.332]
Epoch [3/120    avg_loss:2.308, val_acc:0.492]
Epoch [4/120    avg_loss:2.170, val_acc:0.506]
Epoch [5/120    avg_loss:1.994, val_acc:0.592]
Epoch [6/120    avg_loss:1.862, val_acc:0.588]
Epoch [7/120    avg_loss:1.731, val_acc:0.621]
Epoch [8/120    avg_loss:1.567, val_acc:0.639]
Epoch [9/120    avg_loss:1.415, val_acc:0.666]
Epoch [10/120    avg_loss:1.331, val_acc:0.691]
Epoch [11/120    avg_loss:1.125, val_acc:0.712]
Epoch [12/120    avg_loss:1.045, val_acc:0.700]
Epoch [13/120    avg_loss:1.035, val_acc:0.718]
Epoch [14/120    avg_loss:0.875, val_acc:0.735]
Epoch [15/120    avg_loss:0.770, val_acc:0.772]
Epoch [16/120    avg_loss:0.734, val_acc:0.791]
Epoch [17/120    avg_loss:0.655, val_acc:0.765]
Epoch [18/120    avg_loss:0.594, val_acc:0.807]
Epoch [19/120    avg_loss:0.566, val_acc:0.821]
Epoch [20/120    avg_loss:0.550, val_acc:0.786]
Epoch [21/120    avg_loss:0.481, val_acc:0.811]
Epoch [22/120    avg_loss:0.399, val_acc:0.853]
Epoch [23/120    avg_loss:0.421, val_acc:0.843]
Epoch [24/120    avg_loss:0.378, val_acc:0.859]
Epoch [25/120    avg_loss:0.307, val_acc:0.872]
Epoch [26/120    avg_loss:0.338, val_acc:0.851]
Epoch [27/120    avg_loss:0.313, val_acc:0.877]
Epoch [28/120    avg_loss:0.320, val_acc:0.857]
Epoch [29/120    avg_loss:0.244, val_acc:0.867]
Epoch [30/120    avg_loss:0.236, val_acc:0.890]
Epoch [31/120    avg_loss:0.243, val_acc:0.903]
Epoch [32/120    avg_loss:0.224, val_acc:0.898]
Epoch [33/120    avg_loss:0.197, val_acc:0.912]
Epoch [34/120    avg_loss:0.194, val_acc:0.906]
Epoch [35/120    avg_loss:0.179, val_acc:0.906]
Epoch [36/120    avg_loss:0.162, val_acc:0.913]
Epoch [37/120    avg_loss:0.171, val_acc:0.922]
Epoch [38/120    avg_loss:0.168, val_acc:0.900]
Epoch [39/120    avg_loss:0.168, val_acc:0.908]
Epoch [40/120    avg_loss:0.157, val_acc:0.912]
Epoch [41/120    avg_loss:0.121, val_acc:0.935]
Epoch [42/120    avg_loss:0.126, val_acc:0.931]
Epoch [43/120    avg_loss:0.101, val_acc:0.944]
Epoch [44/120    avg_loss:0.125, val_acc:0.922]
Epoch [45/120    avg_loss:0.142, val_acc:0.922]
Epoch [46/120    avg_loss:0.111, val_acc:0.941]
Epoch [47/120    avg_loss:0.110, val_acc:0.940]
Epoch [48/120    avg_loss:0.104, val_acc:0.933]
Epoch [49/120    avg_loss:0.077, val_acc:0.889]
Epoch [50/120    avg_loss:0.092, val_acc:0.940]
Epoch [51/120    avg_loss:0.080, val_acc:0.953]
Epoch [52/120    avg_loss:0.070, val_acc:0.954]
Epoch [53/120    avg_loss:0.060, val_acc:0.954]
Epoch [54/120    avg_loss:0.076, val_acc:0.953]
Epoch [55/120    avg_loss:0.127, val_acc:0.933]
Epoch [56/120    avg_loss:0.146, val_acc:0.928]
Epoch [57/120    avg_loss:0.111, val_acc:0.913]
Epoch [58/120    avg_loss:0.112, val_acc:0.953]
Epoch [59/120    avg_loss:0.090, val_acc:0.947]
Epoch [60/120    avg_loss:0.080, val_acc:0.938]
Epoch [61/120    avg_loss:0.195, val_acc:0.912]
Epoch [62/120    avg_loss:0.140, val_acc:0.944]
Epoch [63/120    avg_loss:0.100, val_acc:0.941]
Epoch [64/120    avg_loss:0.129, val_acc:0.940]
Epoch [65/120    avg_loss:0.166, val_acc:0.920]
Epoch [66/120    avg_loss:0.114, val_acc:0.904]
Epoch [67/120    avg_loss:0.130, val_acc:0.943]
Epoch [68/120    avg_loss:0.078, val_acc:0.944]
Epoch [69/120    avg_loss:0.076, val_acc:0.941]
Epoch [70/120    avg_loss:0.066, val_acc:0.948]
Epoch [71/120    avg_loss:0.052, val_acc:0.947]
Epoch [72/120    avg_loss:0.047, val_acc:0.954]
Epoch [73/120    avg_loss:0.052, val_acc:0.953]
Epoch [74/120    avg_loss:0.055, val_acc:0.957]
Epoch [75/120    avg_loss:0.050, val_acc:0.962]
Epoch [76/120    avg_loss:0.043, val_acc:0.961]
Epoch [77/120    avg_loss:0.045, val_acc:0.962]
Epoch [78/120    avg_loss:0.047, val_acc:0.962]
Epoch [79/120    avg_loss:0.055, val_acc:0.966]
Epoch [80/120    avg_loss:0.041, val_acc:0.963]
Epoch [81/120    avg_loss:0.041, val_acc:0.966]
Epoch [82/120    avg_loss:0.040, val_acc:0.968]
Epoch [83/120    avg_loss:0.045, val_acc:0.962]
Epoch [84/120    avg_loss:0.052, val_acc:0.964]
Epoch [85/120    avg_loss:0.042, val_acc:0.970]
Epoch [86/120    avg_loss:0.037, val_acc:0.966]
Epoch [87/120    avg_loss:0.034, val_acc:0.967]
Epoch [88/120    avg_loss:0.046, val_acc:0.963]
Epoch [89/120    avg_loss:0.037, val_acc:0.968]
Epoch [90/120    avg_loss:0.041, val_acc:0.966]
Epoch [91/120    avg_loss:0.032, val_acc:0.967]
Epoch [92/120    avg_loss:0.040, val_acc:0.966]
Epoch [93/120    avg_loss:0.034, val_acc:0.969]
Epoch [94/120    avg_loss:0.029, val_acc:0.964]
Epoch [95/120    avg_loss:0.032, val_acc:0.968]
Epoch [96/120    avg_loss:0.034, val_acc:0.966]
Epoch [97/120    avg_loss:0.035, val_acc:0.969]
Epoch [98/120    avg_loss:0.034, val_acc:0.970]
Epoch [99/120    avg_loss:0.036, val_acc:0.966]
Epoch [100/120    avg_loss:0.035, val_acc:0.968]
Epoch [101/120    avg_loss:0.034, val_acc:0.964]
Epoch [102/120    avg_loss:0.040, val_acc:0.970]
Epoch [103/120    avg_loss:0.033, val_acc:0.969]
Epoch [104/120    avg_loss:0.028, val_acc:0.972]
Epoch [105/120    avg_loss:0.033, val_acc:0.968]
Epoch [106/120    avg_loss:0.031, val_acc:0.968]
Epoch [107/120    avg_loss:0.035, val_acc:0.966]
Epoch [108/120    avg_loss:0.031, val_acc:0.969]
Epoch [109/120    avg_loss:0.027, val_acc:0.969]
Epoch [110/120    avg_loss:0.030, val_acc:0.967]
Epoch [111/120    avg_loss:0.026, val_acc:0.970]
Epoch [112/120    avg_loss:0.028, val_acc:0.968]
Epoch [113/120    avg_loss:0.029, val_acc:0.969]
Epoch [114/120    avg_loss:0.032, val_acc:0.971]
Epoch [115/120    avg_loss:0.030, val_acc:0.973]
Epoch [116/120    avg_loss:0.026, val_acc:0.972]
Epoch [117/120    avg_loss:0.026, val_acc:0.971]
Epoch [118/120    avg_loss:0.026, val_acc:0.974]
Epoch [119/120    avg_loss:0.027, val_acc:0.970]
Epoch [120/120    avg_loss:0.027, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1239    0    7    0    3    0    0    0    9   24    3    0
     0    0    0]
 [   0    0    1  686    1    0    0    0    0   10    4    0   45    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   23    0    0    6    0    0    0    1  834    6    2    0
     0    3    0]
 [   0    0    7    0    0    0    1    0    0    0   31 2168    1    2
     0    0    0]
 [   0    0    0    4    0    6    0    0    0    1    0    0  511    0
     3    0    9]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    1    0    0
  1120   17    0]
 [   0    0    0    0    0    0    9    0    0    5    0    0    0    0
     6  327    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.12737127371274

F1 scores:
[       nan 0.975      0.96986301 0.95410292 0.98156682 0.97949886
 0.98791541 0.98039216 1.         0.61538462 0.9498861  0.98299705
 0.93163172 0.99191375 0.98591549 0.94236311 0.94915254]

Kappa:
0.9672731741637431
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc0f20ec828>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.795, val_acc:0.322]
Epoch [2/120    avg_loss:2.501, val_acc:0.511]
Epoch [3/120    avg_loss:2.295, val_acc:0.530]
Epoch [4/120    avg_loss:2.158, val_acc:0.540]
Epoch [5/120    avg_loss:2.017, val_acc:0.564]
Epoch [6/120    avg_loss:1.881, val_acc:0.569]
Epoch [7/120    avg_loss:1.783, val_acc:0.594]
Epoch [8/120    avg_loss:1.654, val_acc:0.594]
Epoch [9/120    avg_loss:1.503, val_acc:0.608]
Epoch [10/120    avg_loss:1.398, val_acc:0.630]
Epoch [11/120    avg_loss:1.300, val_acc:0.622]
Epoch [12/120    avg_loss:1.170, val_acc:0.651]
Epoch [13/120    avg_loss:1.069, val_acc:0.688]
Epoch [14/120    avg_loss:0.984, val_acc:0.704]
Epoch [15/120    avg_loss:0.897, val_acc:0.738]
Epoch [16/120    avg_loss:0.752, val_acc:0.787]
Epoch [17/120    avg_loss:0.661, val_acc:0.767]
Epoch [18/120    avg_loss:0.683, val_acc:0.800]
Epoch [19/120    avg_loss:0.579, val_acc:0.779]
Epoch [20/120    avg_loss:0.563, val_acc:0.819]
Epoch [21/120    avg_loss:0.520, val_acc:0.793]
Epoch [22/120    avg_loss:0.587, val_acc:0.787]
Epoch [23/120    avg_loss:0.550, val_acc:0.801]
Epoch [24/120    avg_loss:0.403, val_acc:0.836]
Epoch [25/120    avg_loss:0.389, val_acc:0.847]
Epoch [26/120    avg_loss:0.348, val_acc:0.843]
Epoch [27/120    avg_loss:0.367, val_acc:0.848]
Epoch [28/120    avg_loss:0.295, val_acc:0.860]
Epoch [29/120    avg_loss:0.378, val_acc:0.864]
Epoch [30/120    avg_loss:0.304, val_acc:0.855]
Epoch [31/120    avg_loss:0.269, val_acc:0.849]
Epoch [32/120    avg_loss:0.247, val_acc:0.893]
Epoch [33/120    avg_loss:0.193, val_acc:0.916]
Epoch [34/120    avg_loss:0.173, val_acc:0.906]
Epoch [35/120    avg_loss:0.174, val_acc:0.907]
Epoch [36/120    avg_loss:0.167, val_acc:0.905]
Epoch [37/120    avg_loss:0.155, val_acc:0.921]
Epoch [38/120    avg_loss:0.148, val_acc:0.916]
Epoch [39/120    avg_loss:0.130, val_acc:0.930]
Epoch [40/120    avg_loss:0.135, val_acc:0.884]
Epoch [41/120    avg_loss:0.197, val_acc:0.906]
Epoch [42/120    avg_loss:0.445, val_acc:0.830]
Epoch [43/120    avg_loss:0.342, val_acc:0.853]
Epoch [44/120    avg_loss:0.237, val_acc:0.861]
Epoch [45/120    avg_loss:0.209, val_acc:0.894]
Epoch [46/120    avg_loss:0.128, val_acc:0.916]
Epoch [47/120    avg_loss:0.126, val_acc:0.933]
Epoch [48/120    avg_loss:0.123, val_acc:0.927]
Epoch [49/120    avg_loss:0.184, val_acc:0.918]
Epoch [50/120    avg_loss:0.134, val_acc:0.923]
Epoch [51/120    avg_loss:0.110, val_acc:0.941]
Epoch [52/120    avg_loss:0.120, val_acc:0.911]
Epoch [53/120    avg_loss:0.131, val_acc:0.920]
Epoch [54/120    avg_loss:0.092, val_acc:0.936]
Epoch [55/120    avg_loss:0.077, val_acc:0.943]
Epoch [56/120    avg_loss:0.072, val_acc:0.922]
Epoch [57/120    avg_loss:0.096, val_acc:0.936]
Epoch [58/120    avg_loss:0.089, val_acc:0.919]
Epoch [59/120    avg_loss:0.082, val_acc:0.919]
Epoch [60/120    avg_loss:0.082, val_acc:0.945]
Epoch [61/120    avg_loss:0.072, val_acc:0.949]
Epoch [62/120    avg_loss:0.061, val_acc:0.956]
Epoch [63/120    avg_loss:0.065, val_acc:0.940]
Epoch [64/120    avg_loss:0.058, val_acc:0.943]
Epoch [65/120    avg_loss:0.053, val_acc:0.949]
Epoch [66/120    avg_loss:0.064, val_acc:0.948]
Epoch [67/120    avg_loss:0.067, val_acc:0.918]
Epoch [68/120    avg_loss:0.072, val_acc:0.938]
Epoch [69/120    avg_loss:0.047, val_acc:0.952]
Epoch [70/120    avg_loss:0.038, val_acc:0.950]
Epoch [71/120    avg_loss:0.036, val_acc:0.957]
Epoch [72/120    avg_loss:0.045, val_acc:0.957]
Epoch [73/120    avg_loss:0.039, val_acc:0.963]
Epoch [74/120    avg_loss:0.045, val_acc:0.949]
Epoch [75/120    avg_loss:0.040, val_acc:0.959]
Epoch [76/120    avg_loss:0.041, val_acc:0.943]
Epoch [77/120    avg_loss:0.036, val_acc:0.955]
Epoch [78/120    avg_loss:0.039, val_acc:0.933]
Epoch [79/120    avg_loss:0.040, val_acc:0.962]
Epoch [80/120    avg_loss:0.027, val_acc:0.961]
Epoch [81/120    avg_loss:0.033, val_acc:0.961]
Epoch [82/120    avg_loss:0.030, val_acc:0.961]
Epoch [83/120    avg_loss:0.025, val_acc:0.961]
Epoch [84/120    avg_loss:0.033, val_acc:0.966]
Epoch [85/120    avg_loss:0.028, val_acc:0.957]
Epoch [86/120    avg_loss:0.023, val_acc:0.969]
Epoch [87/120    avg_loss:0.037, val_acc:0.962]
Epoch [88/120    avg_loss:0.033, val_acc:0.964]
Epoch [89/120    avg_loss:0.023, val_acc:0.945]
Epoch [90/120    avg_loss:0.024, val_acc:0.967]
Epoch [91/120    avg_loss:0.023, val_acc:0.970]
Epoch [92/120    avg_loss:0.037, val_acc:0.961]
Epoch [93/120    avg_loss:0.026, val_acc:0.959]
Epoch [94/120    avg_loss:0.022, val_acc:0.964]
Epoch [95/120    avg_loss:0.021, val_acc:0.960]
Epoch [96/120    avg_loss:0.022, val_acc:0.959]
Epoch [97/120    avg_loss:0.019, val_acc:0.968]
Epoch [98/120    avg_loss:0.018, val_acc:0.962]
Epoch [99/120    avg_loss:0.016, val_acc:0.961]
Epoch [100/120    avg_loss:0.014, val_acc:0.967]
Epoch [101/120    avg_loss:0.018, val_acc:0.966]
Epoch [102/120    avg_loss:0.015, val_acc:0.958]
Epoch [103/120    avg_loss:0.019, val_acc:0.956]
Epoch [104/120    avg_loss:0.014, val_acc:0.966]
Epoch [105/120    avg_loss:0.014, val_acc:0.969]
Epoch [106/120    avg_loss:0.011, val_acc:0.970]
Epoch [107/120    avg_loss:0.013, val_acc:0.967]
Epoch [108/120    avg_loss:0.015, val_acc:0.968]
Epoch [109/120    avg_loss:0.012, val_acc:0.970]
Epoch [110/120    avg_loss:0.014, val_acc:0.967]
Epoch [111/120    avg_loss:0.010, val_acc:0.966]
Epoch [112/120    avg_loss:0.014, val_acc:0.969]
Epoch [113/120    avg_loss:0.013, val_acc:0.968]
Epoch [114/120    avg_loss:0.014, val_acc:0.966]
Epoch [115/120    avg_loss:0.013, val_acc:0.966]
Epoch [116/120    avg_loss:0.014, val_acc:0.966]
Epoch [117/120    avg_loss:0.013, val_acc:0.966]
Epoch [118/120    avg_loss:0.012, val_acc:0.964]
Epoch [119/120    avg_loss:0.010, val_acc:0.963]
Epoch [120/120    avg_loss:0.009, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1257    1    0    0    0    0    0    2    4   19    2    0
     0    0    0]
 [   0    0    0  703    8   11    0    0    0    6    2    0   17    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    4    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   21    0    0    4    1    0    0    0  838    9    0    0
     0    2    0]
 [   0    0   10    0    0    2    3    0    0    0   12 2175    7    1
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0   13    3  512    0
     1    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    2    0    1    0    0    0
  1132    1    0]
 [   0    0    0    0    0    0   30    0    0    6    0    0    0    0
     9  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
97.44173441734418

F1 scores:
[       nan 0.96202532 0.97706957 0.96898691 0.98156682 0.96723164
 0.97257228 0.98039216 0.99767981 0.61538462 0.95881007 0.98483133
 0.95078923 0.99730458 0.99167762 0.92496172 0.95757576]

Kappa:
0.9708366396237705
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc7b8627898>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.769, val_acc:0.330]
Epoch [2/120    avg_loss:2.507, val_acc:0.492]
Epoch [3/120    avg_loss:2.282, val_acc:0.533]
Epoch [4/120    avg_loss:2.082, val_acc:0.568]
Epoch [5/120    avg_loss:1.977, val_acc:0.606]
Epoch [6/120    avg_loss:1.826, val_acc:0.631]
Epoch [7/120    avg_loss:1.665, val_acc:0.641]
Epoch [8/120    avg_loss:1.550, val_acc:0.670]
Epoch [9/120    avg_loss:1.392, val_acc:0.690]
Epoch [10/120    avg_loss:1.295, val_acc:0.666]
Epoch [11/120    avg_loss:1.211, val_acc:0.679]
Epoch [12/120    avg_loss:1.079, val_acc:0.733]
Epoch [13/120    avg_loss:0.971, val_acc:0.679]
Epoch [14/120    avg_loss:0.866, val_acc:0.736]
Epoch [15/120    avg_loss:0.831, val_acc:0.751]
Epoch [16/120    avg_loss:0.716, val_acc:0.776]
Epoch [17/120    avg_loss:0.639, val_acc:0.805]
Epoch [18/120    avg_loss:0.557, val_acc:0.811]
Epoch [19/120    avg_loss:0.669, val_acc:0.775]
Epoch [20/120    avg_loss:0.582, val_acc:0.825]
Epoch [21/120    avg_loss:0.516, val_acc:0.806]
Epoch [22/120    avg_loss:0.443, val_acc:0.835]
Epoch [23/120    avg_loss:0.518, val_acc:0.792]
Epoch [24/120    avg_loss:0.485, val_acc:0.825]
Epoch [25/120    avg_loss:0.378, val_acc:0.834]
Epoch [26/120    avg_loss:0.290, val_acc:0.865]
Epoch [27/120    avg_loss:0.266, val_acc:0.874]
Epoch [28/120    avg_loss:0.313, val_acc:0.827]
Epoch [29/120    avg_loss:0.286, val_acc:0.880]
Epoch [30/120    avg_loss:0.240, val_acc:0.893]
Epoch [31/120    avg_loss:0.259, val_acc:0.880]
Epoch [32/120    avg_loss:0.235, val_acc:0.904]
Epoch [33/120    avg_loss:0.211, val_acc:0.895]
Epoch [34/120    avg_loss:0.185, val_acc:0.904]
Epoch [35/120    avg_loss:0.171, val_acc:0.911]
Epoch [36/120    avg_loss:0.137, val_acc:0.906]
Epoch [37/120    avg_loss:0.194, val_acc:0.911]
Epoch [38/120    avg_loss:0.163, val_acc:0.926]
Epoch [39/120    avg_loss:0.135, val_acc:0.921]
Epoch [40/120    avg_loss:0.133, val_acc:0.904]
Epoch [41/120    avg_loss:0.154, val_acc:0.897]
Epoch [42/120    avg_loss:0.176, val_acc:0.902]
Epoch [43/120    avg_loss:0.164, val_acc:0.923]
Epoch [44/120    avg_loss:0.129, val_acc:0.932]
Epoch [45/120    avg_loss:0.119, val_acc:0.944]
Epoch [46/120    avg_loss:0.261, val_acc:0.853]
Epoch [47/120    avg_loss:0.314, val_acc:0.902]
Epoch [48/120    avg_loss:0.184, val_acc:0.906]
Epoch [49/120    avg_loss:0.139, val_acc:0.928]
Epoch [50/120    avg_loss:0.119, val_acc:0.950]
Epoch [51/120    avg_loss:0.154, val_acc:0.944]
Epoch [52/120    avg_loss:0.113, val_acc:0.934]
Epoch [53/120    avg_loss:0.117, val_acc:0.926]
Epoch [54/120    avg_loss:0.098, val_acc:0.939]
Epoch [55/120    avg_loss:0.091, val_acc:0.939]
Epoch [56/120    avg_loss:0.082, val_acc:0.944]
Epoch [57/120    avg_loss:0.086, val_acc:0.954]
Epoch [58/120    avg_loss:0.072, val_acc:0.938]
Epoch [59/120    avg_loss:0.078, val_acc:0.954]
Epoch [60/120    avg_loss:0.072, val_acc:0.950]
Epoch [61/120    avg_loss:0.075, val_acc:0.955]
Epoch [62/120    avg_loss:0.105, val_acc:0.948]
Epoch [63/120    avg_loss:0.063, val_acc:0.956]
Epoch [64/120    avg_loss:0.061, val_acc:0.956]
Epoch [65/120    avg_loss:0.055, val_acc:0.956]
Epoch [66/120    avg_loss:0.047, val_acc:0.958]
Epoch [67/120    avg_loss:0.053, val_acc:0.957]
Epoch [68/120    avg_loss:0.050, val_acc:0.961]
Epoch [69/120    avg_loss:0.046, val_acc:0.977]
Epoch [70/120    avg_loss:0.038, val_acc:0.968]
Epoch [71/120    avg_loss:0.048, val_acc:0.967]
Epoch [72/120    avg_loss:0.039, val_acc:0.968]
Epoch [73/120    avg_loss:0.046, val_acc:0.958]
Epoch [74/120    avg_loss:0.042, val_acc:0.962]
Epoch [75/120    avg_loss:0.035, val_acc:0.968]
Epoch [76/120    avg_loss:0.051, val_acc:0.960]
Epoch [77/120    avg_loss:0.075, val_acc:0.923]
Epoch [78/120    avg_loss:0.119, val_acc:0.955]
Epoch [79/120    avg_loss:0.079, val_acc:0.947]
Epoch [80/120    avg_loss:0.054, val_acc:0.966]
Epoch [81/120    avg_loss:0.045, val_acc:0.944]
Epoch [82/120    avg_loss:0.041, val_acc:0.972]
Epoch [83/120    avg_loss:0.027, val_acc:0.973]
Epoch [84/120    avg_loss:0.027, val_acc:0.972]
Epoch [85/120    avg_loss:0.028, val_acc:0.974]
Epoch [86/120    avg_loss:0.023, val_acc:0.973]
Epoch [87/120    avg_loss:0.035, val_acc:0.972]
Epoch [88/120    avg_loss:0.027, val_acc:0.976]
Epoch [89/120    avg_loss:0.027, val_acc:0.977]
Epoch [90/120    avg_loss:0.023, val_acc:0.976]
Epoch [91/120    avg_loss:0.017, val_acc:0.977]
Epoch [92/120    avg_loss:0.020, val_acc:0.977]
Epoch [93/120    avg_loss:0.024, val_acc:0.978]
Epoch [94/120    avg_loss:0.021, val_acc:0.976]
Epoch [95/120    avg_loss:0.021, val_acc:0.978]
Epoch [96/120    avg_loss:0.020, val_acc:0.977]
Epoch [97/120    avg_loss:0.021, val_acc:0.976]
Epoch [98/120    avg_loss:0.022, val_acc:0.977]
Epoch [99/120    avg_loss:0.020, val_acc:0.975]
Epoch [100/120    avg_loss:0.024, val_acc:0.974]
Epoch [101/120    avg_loss:0.021, val_acc:0.977]
Epoch [102/120    avg_loss:0.022, val_acc:0.977]
Epoch [103/120    avg_loss:0.020, val_acc:0.977]
Epoch [104/120    avg_loss:0.019, val_acc:0.976]
Epoch [105/120    avg_loss:0.017, val_acc:0.980]
Epoch [106/120    avg_loss:0.019, val_acc:0.978]
Epoch [107/120    avg_loss:0.018, val_acc:0.981]
Epoch [108/120    avg_loss:0.019, val_acc:0.981]
Epoch [109/120    avg_loss:0.019, val_acc:0.978]
Epoch [110/120    avg_loss:0.035, val_acc:0.973]
Epoch [111/120    avg_loss:0.020, val_acc:0.975]
Epoch [112/120    avg_loss:0.024, val_acc:0.976]
Epoch [113/120    avg_loss:0.023, val_acc:0.977]
Epoch [114/120    avg_loss:0.027, val_acc:0.976]
Epoch [115/120    avg_loss:0.016, val_acc:0.980]
Epoch [116/120    avg_loss:0.020, val_acc:0.980]
Epoch [117/120    avg_loss:0.016, val_acc:0.981]
Epoch [118/120    avg_loss:0.019, val_acc:0.977]
Epoch [119/120    avg_loss:0.017, val_acc:0.981]
Epoch [120/120    avg_loss:0.018, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1258    2    6    0    4    0    0    0    4   10    0    0
     0    1    0]
 [   0    0    1  707    4    1    0    0    0    7    1    1   20    5
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    4    0    3    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   15    0    0    5    0    0    0    0  844    5    0    0
     2    4    0]
 [   0    0   13    0    0    1    2    0    1    0   20 2168    3    2
     0    0    0]
 [   0    0    0    2    1    3    0    0    0    0    0    6  518    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1136    2    0]
 [   0    0    0    0    0    0   11    0    0    5    0    0    0    0
     8  323    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.81029810298104

F1 scores:
[       nan 0.96202532 0.97822706 0.96915696 0.97247706 0.97583429
 0.98646617 0.92592593 0.99767981 0.65306122 0.96622782 0.98545455
 0.96103896 0.98143236 0.99170668 0.95420975 0.97647059]

Kappa:
0.975049252436934
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3958ae7860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.768, val_acc:0.322]
Epoch [2/120    avg_loss:2.489, val_acc:0.431]
Epoch [3/120    avg_loss:2.276, val_acc:0.492]
Epoch [4/120    avg_loss:2.149, val_acc:0.525]
Epoch [5/120    avg_loss:2.000, val_acc:0.569]
Epoch [6/120    avg_loss:1.893, val_acc:0.584]
Epoch [7/120    avg_loss:1.762, val_acc:0.585]
Epoch [8/120    avg_loss:1.640, val_acc:0.574]
Epoch [9/120    avg_loss:1.573, val_acc:0.639]
Epoch [10/120    avg_loss:1.390, val_acc:0.657]
Epoch [11/120    avg_loss:1.283, val_acc:0.680]
Epoch [12/120    avg_loss:1.205, val_acc:0.690]
Epoch [13/120    avg_loss:1.118, val_acc:0.707]
Epoch [14/120    avg_loss:0.991, val_acc:0.748]
Epoch [15/120    avg_loss:0.929, val_acc:0.752]
Epoch [16/120    avg_loss:0.844, val_acc:0.790]
Epoch [17/120    avg_loss:0.736, val_acc:0.766]
Epoch [18/120    avg_loss:0.731, val_acc:0.822]
Epoch [19/120    avg_loss:0.667, val_acc:0.804]
Epoch [20/120    avg_loss:0.565, val_acc:0.814]
Epoch [21/120    avg_loss:0.511, val_acc:0.836]
Epoch [22/120    avg_loss:0.433, val_acc:0.851]
Epoch [23/120    avg_loss:0.460, val_acc:0.847]
Epoch [24/120    avg_loss:0.345, val_acc:0.874]
Epoch [25/120    avg_loss:0.303, val_acc:0.879]
Epoch [26/120    avg_loss:0.286, val_acc:0.880]
Epoch [27/120    avg_loss:0.258, val_acc:0.913]
Epoch [28/120    avg_loss:0.279, val_acc:0.888]
Epoch [29/120    avg_loss:0.277, val_acc:0.885]
Epoch [30/120    avg_loss:0.292, val_acc:0.867]
Epoch [31/120    avg_loss:0.247, val_acc:0.898]
Epoch [32/120    avg_loss:0.240, val_acc:0.905]
Epoch [33/120    avg_loss:0.204, val_acc:0.917]
Epoch [34/120    avg_loss:0.202, val_acc:0.904]
Epoch [35/120    avg_loss:0.169, val_acc:0.912]
Epoch [36/120    avg_loss:0.157, val_acc:0.928]
Epoch [37/120    avg_loss:0.237, val_acc:0.902]
Epoch [38/120    avg_loss:0.158, val_acc:0.918]
Epoch [39/120    avg_loss:0.149, val_acc:0.911]
Epoch [40/120    avg_loss:0.117, val_acc:0.932]
Epoch [41/120    avg_loss:0.122, val_acc:0.913]
Epoch [42/120    avg_loss:0.112, val_acc:0.926]
Epoch [43/120    avg_loss:0.140, val_acc:0.921]
Epoch [44/120    avg_loss:0.136, val_acc:0.913]
Epoch [45/120    avg_loss:0.127, val_acc:0.916]
Epoch [46/120    avg_loss:0.110, val_acc:0.930]
Epoch [47/120    avg_loss:0.094, val_acc:0.923]
Epoch [48/120    avg_loss:0.098, val_acc:0.941]
Epoch [49/120    avg_loss:0.092, val_acc:0.950]
Epoch [50/120    avg_loss:0.076, val_acc:0.945]
Epoch [51/120    avg_loss:0.073, val_acc:0.943]
Epoch [52/120    avg_loss:0.087, val_acc:0.945]
Epoch [53/120    avg_loss:0.073, val_acc:0.939]
Epoch [54/120    avg_loss:0.091, val_acc:0.949]
Epoch [55/120    avg_loss:0.074, val_acc:0.956]
Epoch [56/120    avg_loss:0.063, val_acc:0.927]
Epoch [57/120    avg_loss:0.084, val_acc:0.936]
Epoch [58/120    avg_loss:0.084, val_acc:0.936]
Epoch [59/120    avg_loss:0.072, val_acc:0.949]
Epoch [60/120    avg_loss:0.084, val_acc:0.917]
Epoch [61/120    avg_loss:0.080, val_acc:0.940]
Epoch [62/120    avg_loss:0.079, val_acc:0.940]
Epoch [63/120    avg_loss:0.067, val_acc:0.947]
Epoch [64/120    avg_loss:0.045, val_acc:0.950]
Epoch [65/120    avg_loss:0.073, val_acc:0.948]
Epoch [66/120    avg_loss:0.063, val_acc:0.943]
Epoch [67/120    avg_loss:0.066, val_acc:0.947]
Epoch [68/120    avg_loss:0.058, val_acc:0.952]
Epoch [69/120    avg_loss:0.051, val_acc:0.953]
Epoch [70/120    avg_loss:0.041, val_acc:0.957]
Epoch [71/120    avg_loss:0.041, val_acc:0.960]
Epoch [72/120    avg_loss:0.037, val_acc:0.960]
Epoch [73/120    avg_loss:0.032, val_acc:0.960]
Epoch [74/120    avg_loss:0.032, val_acc:0.960]
Epoch [75/120    avg_loss:0.030, val_acc:0.962]
Epoch [76/120    avg_loss:0.039, val_acc:0.958]
Epoch [77/120    avg_loss:0.033, val_acc:0.960]
Epoch [78/120    avg_loss:0.032, val_acc:0.956]
Epoch [79/120    avg_loss:0.030, val_acc:0.960]
Epoch [80/120    avg_loss:0.031, val_acc:0.958]
Epoch [81/120    avg_loss:0.025, val_acc:0.958]
Epoch [82/120    avg_loss:0.030, val_acc:0.959]
Epoch [83/120    avg_loss:0.026, val_acc:0.959]
Epoch [84/120    avg_loss:0.029, val_acc:0.961]
Epoch [85/120    avg_loss:0.027, val_acc:0.961]
Epoch [86/120    avg_loss:0.029, val_acc:0.960]
Epoch [87/120    avg_loss:0.030, val_acc:0.961]
Epoch [88/120    avg_loss:0.026, val_acc:0.960]
Epoch [89/120    avg_loss:0.026, val_acc:0.960]
Epoch [90/120    avg_loss:0.029, val_acc:0.960]
Epoch [91/120    avg_loss:0.028, val_acc:0.960]
Epoch [92/120    avg_loss:0.029, val_acc:0.960]
Epoch [93/120    avg_loss:0.028, val_acc:0.960]
Epoch [94/120    avg_loss:0.027, val_acc:0.960]
Epoch [95/120    avg_loss:0.025, val_acc:0.960]
Epoch [96/120    avg_loss:0.025, val_acc:0.961]
Epoch [97/120    avg_loss:0.026, val_acc:0.962]
Epoch [98/120    avg_loss:0.025, val_acc:0.961]
Epoch [99/120    avg_loss:0.029, val_acc:0.961]
Epoch [100/120    avg_loss:0.026, val_acc:0.961]
Epoch [101/120    avg_loss:0.023, val_acc:0.962]
Epoch [102/120    avg_loss:0.030, val_acc:0.962]
Epoch [103/120    avg_loss:0.024, val_acc:0.961]
Epoch [104/120    avg_loss:0.029, val_acc:0.960]
Epoch [105/120    avg_loss:0.028, val_acc:0.961]
Epoch [106/120    avg_loss:0.026, val_acc:0.961]
Epoch [107/120    avg_loss:0.027, val_acc:0.961]
Epoch [108/120    avg_loss:0.024, val_acc:0.961]
Epoch [109/120    avg_loss:0.036, val_acc:0.961]
Epoch [110/120    avg_loss:0.027, val_acc:0.961]
Epoch [111/120    avg_loss:0.026, val_acc:0.962]
Epoch [112/120    avg_loss:0.023, val_acc:0.962]
Epoch [113/120    avg_loss:0.028, val_acc:0.962]
Epoch [114/120    avg_loss:0.028, val_acc:0.962]
Epoch [115/120    avg_loss:0.024, val_acc:0.962]
Epoch [116/120    avg_loss:0.023, val_acc:0.962]
Epoch [117/120    avg_loss:0.024, val_acc:0.962]
Epoch [118/120    avg_loss:0.028, val_acc:0.962]
Epoch [119/120    avg_loss:0.028, val_acc:0.962]
Epoch [120/120    avg_loss:0.030, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1219    0    3    0    4    0    0    0    7   51    1    0
     0    0    0]
 [   0    0    2  692    2    9    0    0    0   11    1    3   27    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   15    0    2    0    0
     0    0    0]
 [   0    0   22    0    0    7    0    0    0    0  831   12    0    0
     0    3    0]
 [   0    0    1    0    0    4    3    0    0    0   20 2167   14    1
     0    0    0]
 [   0    0    5    1    0    4    0    0    0    0    1   11  507    0
     2    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    2    0    0   10    0    0    1    0    1    0    0    0
  1125    0    0]
 [   0    0    0    0    0    0   29    0    0    1    0    0    0    0
    13  304    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.66124661246613

F1 scores:
[       nan 0.975      0.96135647 0.96111111 0.98839907 0.95661846
 0.97032641 0.98039216 0.995338   0.6122449  0.95627158 0.97196681
 0.93370166 0.99730458 0.98727512 0.92966361 0.98245614]

Kappa:
0.9619227814699876
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffad41d5828>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.764, val_acc:0.226]
Epoch [2/120    avg_loss:2.486, val_acc:0.415]
Epoch [3/120    avg_loss:2.303, val_acc:0.502]
Epoch [4/120    avg_loss:2.158, val_acc:0.511]
Epoch [5/120    avg_loss:2.059, val_acc:0.527]
Epoch [6/120    avg_loss:1.951, val_acc:0.559]
Epoch [7/120    avg_loss:1.837, val_acc:0.575]
Epoch [8/120    avg_loss:1.723, val_acc:0.593]
Epoch [9/120    avg_loss:1.620, val_acc:0.603]
Epoch [10/120    avg_loss:1.488, val_acc:0.641]
Epoch [11/120    avg_loss:1.299, val_acc:0.661]
Epoch [12/120    avg_loss:1.196, val_acc:0.666]
Epoch [13/120    avg_loss:1.111, val_acc:0.695]
Epoch [14/120    avg_loss:1.017, val_acc:0.686]
Epoch [15/120    avg_loss:0.951, val_acc:0.706]
Epoch [16/120    avg_loss:0.870, val_acc:0.764]
Epoch [17/120    avg_loss:0.771, val_acc:0.755]
Epoch [18/120    avg_loss:0.742, val_acc:0.784]
Epoch [19/120    avg_loss:0.706, val_acc:0.800]
Epoch [20/120    avg_loss:0.643, val_acc:0.801]
Epoch [21/120    avg_loss:0.542, val_acc:0.837]
Epoch [22/120    avg_loss:0.497, val_acc:0.853]
Epoch [23/120    avg_loss:0.504, val_acc:0.834]
Epoch [24/120    avg_loss:0.451, val_acc:0.871]
Epoch [25/120    avg_loss:0.380, val_acc:0.869]
Epoch [26/120    avg_loss:0.403, val_acc:0.869]
Epoch [27/120    avg_loss:0.349, val_acc:0.848]
Epoch [28/120    avg_loss:0.350, val_acc:0.865]
Epoch [29/120    avg_loss:0.288, val_acc:0.900]
Epoch [30/120    avg_loss:0.265, val_acc:0.908]
Epoch [31/120    avg_loss:0.245, val_acc:0.907]
Epoch [32/120    avg_loss:0.218, val_acc:0.914]
Epoch [33/120    avg_loss:0.192, val_acc:0.927]
Epoch [34/120    avg_loss:0.190, val_acc:0.927]
Epoch [35/120    avg_loss:0.226, val_acc:0.919]
Epoch [36/120    avg_loss:0.188, val_acc:0.927]
Epoch [37/120    avg_loss:0.229, val_acc:0.905]
Epoch [38/120    avg_loss:0.202, val_acc:0.926]
Epoch [39/120    avg_loss:0.161, val_acc:0.914]
Epoch [40/120    avg_loss:0.150, val_acc:0.925]
Epoch [41/120    avg_loss:0.140, val_acc:0.946]
Epoch [42/120    avg_loss:0.126, val_acc:0.953]
Epoch [43/120    avg_loss:0.106, val_acc:0.949]
Epoch [44/120    avg_loss:0.104, val_acc:0.939]
Epoch [45/120    avg_loss:0.106, val_acc:0.953]
Epoch [46/120    avg_loss:0.121, val_acc:0.938]
Epoch [47/120    avg_loss:0.098, val_acc:0.960]
Epoch [48/120    avg_loss:0.098, val_acc:0.946]
Epoch [49/120    avg_loss:0.098, val_acc:0.945]
Epoch [50/120    avg_loss:0.098, val_acc:0.954]
Epoch [51/120    avg_loss:0.102, val_acc:0.946]
Epoch [52/120    avg_loss:0.104, val_acc:0.936]
Epoch [53/120    avg_loss:0.125, val_acc:0.952]
Epoch [54/120    avg_loss:0.193, val_acc:0.898]
Epoch [55/120    avg_loss:0.176, val_acc:0.907]
Epoch [56/120    avg_loss:0.118, val_acc:0.949]
Epoch [57/120    avg_loss:0.082, val_acc:0.947]
Epoch [58/120    avg_loss:0.081, val_acc:0.956]
Epoch [59/120    avg_loss:0.069, val_acc:0.957]
Epoch [60/120    avg_loss:0.070, val_acc:0.954]
Epoch [61/120    avg_loss:0.058, val_acc:0.964]
Epoch [62/120    avg_loss:0.053, val_acc:0.968]
Epoch [63/120    avg_loss:0.052, val_acc:0.970]
Epoch [64/120    avg_loss:0.052, val_acc:0.970]
Epoch [65/120    avg_loss:0.044, val_acc:0.972]
Epoch [66/120    avg_loss:0.045, val_acc:0.973]
Epoch [67/120    avg_loss:0.049, val_acc:0.970]
Epoch [68/120    avg_loss:0.042, val_acc:0.971]
Epoch [69/120    avg_loss:0.047, val_acc:0.971]
Epoch [70/120    avg_loss:0.040, val_acc:0.967]
Epoch [71/120    avg_loss:0.048, val_acc:0.968]
Epoch [72/120    avg_loss:0.043, val_acc:0.969]
Epoch [73/120    avg_loss:0.045, val_acc:0.969]
Epoch [74/120    avg_loss:0.037, val_acc:0.970]
Epoch [75/120    avg_loss:0.050, val_acc:0.969]
Epoch [76/120    avg_loss:0.041, val_acc:0.967]
Epoch [77/120    avg_loss:0.038, val_acc:0.971]
Epoch [78/120    avg_loss:0.036, val_acc:0.971]
Epoch [79/120    avg_loss:0.041, val_acc:0.970]
Epoch [80/120    avg_loss:0.043, val_acc:0.970]
Epoch [81/120    avg_loss:0.038, val_acc:0.970]
Epoch [82/120    avg_loss:0.037, val_acc:0.970]
Epoch [83/120    avg_loss:0.042, val_acc:0.970]
Epoch [84/120    avg_loss:0.038, val_acc:0.970]
Epoch [85/120    avg_loss:0.036, val_acc:0.971]
Epoch [86/120    avg_loss:0.042, val_acc:0.971]
Epoch [87/120    avg_loss:0.039, val_acc:0.971]
Epoch [88/120    avg_loss:0.042, val_acc:0.971]
Epoch [89/120    avg_loss:0.046, val_acc:0.971]
Epoch [90/120    avg_loss:0.034, val_acc:0.971]
Epoch [91/120    avg_loss:0.041, val_acc:0.971]
Epoch [92/120    avg_loss:0.038, val_acc:0.971]
Epoch [93/120    avg_loss:0.036, val_acc:0.971]
Epoch [94/120    avg_loss:0.037, val_acc:0.971]
Epoch [95/120    avg_loss:0.037, val_acc:0.971]
Epoch [96/120    avg_loss:0.037, val_acc:0.971]
Epoch [97/120    avg_loss:0.040, val_acc:0.971]
Epoch [98/120    avg_loss:0.035, val_acc:0.971]
Epoch [99/120    avg_loss:0.033, val_acc:0.971]
Epoch [100/120    avg_loss:0.040, val_acc:0.971]
Epoch [101/120    avg_loss:0.046, val_acc:0.971]
Epoch [102/120    avg_loss:0.040, val_acc:0.971]
Epoch [103/120    avg_loss:0.034, val_acc:0.971]
Epoch [104/120    avg_loss:0.042, val_acc:0.971]
Epoch [105/120    avg_loss:0.038, val_acc:0.971]
Epoch [106/120    avg_loss:0.034, val_acc:0.971]
Epoch [107/120    avg_loss:0.039, val_acc:0.971]
Epoch [108/120    avg_loss:0.047, val_acc:0.971]
Epoch [109/120    avg_loss:0.042, val_acc:0.971]
Epoch [110/120    avg_loss:0.040, val_acc:0.971]
Epoch [111/120    avg_loss:0.044, val_acc:0.971]
Epoch [112/120    avg_loss:0.042, val_acc:0.971]
Epoch [113/120    avg_loss:0.037, val_acc:0.971]
Epoch [114/120    avg_loss:0.036, val_acc:0.971]
Epoch [115/120    avg_loss:0.038, val_acc:0.971]
Epoch [116/120    avg_loss:0.045, val_acc:0.971]
Epoch [117/120    avg_loss:0.043, val_acc:0.971]
Epoch [118/120    avg_loss:0.037, val_acc:0.971]
Epoch [119/120    avg_loss:0.042, val_acc:0.971]
Epoch [120/120    avg_loss:0.039, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1227    5    0    0    1    0    0    0    5   47    0    0
     0    0    0]
 [   0    0    2  690   14    5    0    0    0    8    0    0   27    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    2    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   44    1    0    5    0    0    0    2  800   21    1    0
     0    1    0]
 [   0    0   10    0    0    0    1    1    4    0   21 2162    9    2
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    6    7  507    0
     3    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    0    1    0
  1135    0    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
    10  312    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.57452574525745

F1 scores:
[       nan 1.         0.95560748 0.95634096 0.96818182 0.97610922
 0.97834205 0.94339623 0.99303944 0.66666667 0.93567251 0.9721223
 0.93628809 0.9919571  0.99213287 0.94545455 0.96551724]

Kappa:
0.960936358344771
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff139e407f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.769, val_acc:0.386]
Epoch [2/120    avg_loss:2.505, val_acc:0.423]
Epoch [3/120    avg_loss:2.286, val_acc:0.452]
Epoch [4/120    avg_loss:2.130, val_acc:0.497]
Epoch [5/120    avg_loss:1.950, val_acc:0.533]
Epoch [6/120    avg_loss:1.863, val_acc:0.573]
Epoch [7/120    avg_loss:1.754, val_acc:0.596]
Epoch [8/120    avg_loss:1.625, val_acc:0.605]
Epoch [9/120    avg_loss:1.540, val_acc:0.655]
Epoch [10/120    avg_loss:1.405, val_acc:0.657]
Epoch [11/120    avg_loss:1.314, val_acc:0.696]
Epoch [12/120    avg_loss:1.140, val_acc:0.684]
Epoch [13/120    avg_loss:1.114, val_acc:0.700]
Epoch [14/120    avg_loss:1.064, val_acc:0.686]
Epoch [15/120    avg_loss:0.910, val_acc:0.712]
Epoch [16/120    avg_loss:0.865, val_acc:0.725]
Epoch [17/120    avg_loss:0.764, val_acc:0.737]
Epoch [18/120    avg_loss:0.749, val_acc:0.754]
Epoch [19/120    avg_loss:0.718, val_acc:0.792]
Epoch [20/120    avg_loss:0.607, val_acc:0.779]
Epoch [21/120    avg_loss:0.584, val_acc:0.792]
Epoch [22/120    avg_loss:0.481, val_acc:0.839]
Epoch [23/120    avg_loss:0.487, val_acc:0.846]
Epoch [24/120    avg_loss:0.466, val_acc:0.825]
Epoch [25/120    avg_loss:0.443, val_acc:0.839]
Epoch [26/120    avg_loss:0.438, val_acc:0.850]
Epoch [27/120    avg_loss:0.340, val_acc:0.886]
Epoch [28/120    avg_loss:0.305, val_acc:0.877]
Epoch [29/120    avg_loss:0.273, val_acc:0.876]
Epoch [30/120    avg_loss:0.282, val_acc:0.899]
Epoch [31/120    avg_loss:0.308, val_acc:0.893]
Epoch [32/120    avg_loss:0.327, val_acc:0.867]
Epoch [33/120    avg_loss:0.262, val_acc:0.885]
Epoch [34/120    avg_loss:0.256, val_acc:0.888]
Epoch [35/120    avg_loss:0.250, val_acc:0.899]
Epoch [36/120    avg_loss:0.204, val_acc:0.900]
Epoch [37/120    avg_loss:0.190, val_acc:0.920]
Epoch [38/120    avg_loss:0.169, val_acc:0.938]
Epoch [39/120    avg_loss:0.220, val_acc:0.877]
Epoch [40/120    avg_loss:0.161, val_acc:0.931]
Epoch [41/120    avg_loss:0.122, val_acc:0.927]
Epoch [42/120    avg_loss:0.111, val_acc:0.933]
Epoch [43/120    avg_loss:0.109, val_acc:0.920]
Epoch [44/120    avg_loss:0.119, val_acc:0.939]
Epoch [45/120    avg_loss:0.114, val_acc:0.938]
Epoch [46/120    avg_loss:0.111, val_acc:0.945]
Epoch [47/120    avg_loss:0.107, val_acc:0.946]
Epoch [48/120    avg_loss:0.105, val_acc:0.947]
Epoch [49/120    avg_loss:0.093, val_acc:0.953]
Epoch [50/120    avg_loss:0.121, val_acc:0.954]
Epoch [51/120    avg_loss:0.091, val_acc:0.938]
Epoch [52/120    avg_loss:0.089, val_acc:0.949]
Epoch [53/120    avg_loss:0.077, val_acc:0.956]
Epoch [54/120    avg_loss:0.082, val_acc:0.943]
Epoch [55/120    avg_loss:0.079, val_acc:0.944]
Epoch [56/120    avg_loss:0.073, val_acc:0.955]
Epoch [57/120    avg_loss:0.070, val_acc:0.954]
Epoch [58/120    avg_loss:0.085, val_acc:0.957]
Epoch [59/120    avg_loss:0.087, val_acc:0.952]
Epoch [60/120    avg_loss:0.071, val_acc:0.949]
Epoch [61/120    avg_loss:0.071, val_acc:0.950]
Epoch [62/120    avg_loss:0.057, val_acc:0.967]
Epoch [63/120    avg_loss:0.049, val_acc:0.963]
Epoch [64/120    avg_loss:0.071, val_acc:0.964]
Epoch [65/120    avg_loss:0.069, val_acc:0.949]
Epoch [66/120    avg_loss:0.086, val_acc:0.961]
Epoch [67/120    avg_loss:0.065, val_acc:0.957]
Epoch [68/120    avg_loss:0.061, val_acc:0.962]
Epoch [69/120    avg_loss:0.044, val_acc:0.966]
Epoch [70/120    avg_loss:0.061, val_acc:0.968]
Epoch [71/120    avg_loss:0.051, val_acc:0.969]
Epoch [72/120    avg_loss:0.046, val_acc:0.962]
Epoch [73/120    avg_loss:0.034, val_acc:0.962]
Epoch [74/120    avg_loss:0.037, val_acc:0.966]
Epoch [75/120    avg_loss:0.049, val_acc:0.972]
Epoch [76/120    avg_loss:0.048, val_acc:0.967]
Epoch [77/120    avg_loss:0.050, val_acc:0.969]
Epoch [78/120    avg_loss:0.033, val_acc:0.977]
Epoch [79/120    avg_loss:0.034, val_acc:0.974]
Epoch [80/120    avg_loss:0.029, val_acc:0.971]
Epoch [81/120    avg_loss:0.044, val_acc:0.958]
Epoch [82/120    avg_loss:0.079, val_acc:0.954]
Epoch [83/120    avg_loss:0.047, val_acc:0.971]
Epoch [84/120    avg_loss:0.035, val_acc:0.963]
Epoch [85/120    avg_loss:0.065, val_acc:0.961]
Epoch [86/120    avg_loss:0.065, val_acc:0.960]
Epoch [87/120    avg_loss:0.049, val_acc:0.966]
Epoch [88/120    avg_loss:0.046, val_acc:0.968]
Epoch [89/120    avg_loss:0.038, val_acc:0.966]
Epoch [90/120    avg_loss:0.033, val_acc:0.969]
Epoch [91/120    avg_loss:0.051, val_acc:0.973]
Epoch [92/120    avg_loss:0.035, val_acc:0.978]
Epoch [93/120    avg_loss:0.025, val_acc:0.975]
Epoch [94/120    avg_loss:0.019, val_acc:0.971]
Epoch [95/120    avg_loss:0.022, val_acc:0.972]
Epoch [96/120    avg_loss:0.018, val_acc:0.975]
Epoch [97/120    avg_loss:0.027, val_acc:0.973]
Epoch [98/120    avg_loss:0.022, val_acc:0.978]
Epoch [99/120    avg_loss:0.019, val_acc:0.976]
Epoch [100/120    avg_loss:0.019, val_acc:0.975]
Epoch [101/120    avg_loss:0.021, val_acc:0.976]
Epoch [102/120    avg_loss:0.018, val_acc:0.977]
Epoch [103/120    avg_loss:0.016, val_acc:0.977]
Epoch [104/120    avg_loss:0.020, val_acc:0.978]
Epoch [105/120    avg_loss:0.016, val_acc:0.980]
Epoch [106/120    avg_loss:0.019, val_acc:0.981]
Epoch [107/120    avg_loss:0.019, val_acc:0.977]
Epoch [108/120    avg_loss:0.016, val_acc:0.982]
Epoch [109/120    avg_loss:0.021, val_acc:0.980]
Epoch [110/120    avg_loss:0.023, val_acc:0.974]
Epoch [111/120    avg_loss:0.018, val_acc:0.976]
Epoch [112/120    avg_loss:0.019, val_acc:0.976]
Epoch [113/120    avg_loss:0.019, val_acc:0.976]
Epoch [114/120    avg_loss:0.019, val_acc:0.980]
Epoch [115/120    avg_loss:0.015, val_acc:0.980]
Epoch [116/120    avg_loss:0.015, val_acc:0.978]
Epoch [117/120    avg_loss:0.015, val_acc:0.978]
Epoch [118/120    avg_loss:0.020, val_acc:0.980]
Epoch [119/120    avg_loss:0.018, val_acc:0.980]
Epoch [120/120    avg_loss:0.018, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1235    5    1    1    0    0    0    0    9   31    0    0
     0    3    0]
 [   0    0    0  678    2    6    0    0    0   12    0    0   48    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    1    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    1    0    5    1    0    0    0  856    2    0    0
     0    4    0]
 [   0    0    4    0    0    0    3    0    0    0   14 2189    0    0
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    1    3    4  514    0
     0    1    4]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    1
  1132    5    0]
 [   0    0    0    0    0    0   33    0    0    0    0    0    0    0
     6  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.52845528455285

F1 scores:
[       nan 1.         0.97628458 0.9475891  0.99300699 0.97637795
 0.97113249 1.         0.99883856 0.73469388 0.97438816 0.98670273
 0.9379562  0.99459459 0.99385426 0.92215569 0.97674419]

Kappa:
0.9718276652487989
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1d9ea53828>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.768, val_acc:0.216]
Epoch [2/120    avg_loss:2.475, val_acc:0.428]
Epoch [3/120    avg_loss:2.268, val_acc:0.494]
Epoch [4/120    avg_loss:2.124, val_acc:0.477]
Epoch [5/120    avg_loss:1.967, val_acc:0.573]
Epoch [6/120    avg_loss:1.872, val_acc:0.571]
Epoch [7/120    avg_loss:1.707, val_acc:0.581]
Epoch [8/120    avg_loss:1.643, val_acc:0.616]
Epoch [9/120    avg_loss:1.523, val_acc:0.639]
Epoch [10/120    avg_loss:1.434, val_acc:0.641]
Epoch [11/120    avg_loss:1.321, val_acc:0.670]
Epoch [12/120    avg_loss:1.212, val_acc:0.713]
Epoch [13/120    avg_loss:1.081, val_acc:0.695]
Epoch [14/120    avg_loss:1.019, val_acc:0.721]
Epoch [15/120    avg_loss:0.889, val_acc:0.717]
Epoch [16/120    avg_loss:0.928, val_acc:0.749]
Epoch [17/120    avg_loss:0.746, val_acc:0.756]
Epoch [18/120    avg_loss:0.667, val_acc:0.770]
Epoch [19/120    avg_loss:0.631, val_acc:0.801]
Epoch [20/120    avg_loss:0.597, val_acc:0.818]
Epoch [21/120    avg_loss:0.553, val_acc:0.782]
Epoch [22/120    avg_loss:0.632, val_acc:0.801]
Epoch [23/120    avg_loss:0.576, val_acc:0.798]
Epoch [24/120    avg_loss:0.451, val_acc:0.831]
Epoch [25/120    avg_loss:0.440, val_acc:0.823]
Epoch [26/120    avg_loss:0.468, val_acc:0.817]
Epoch [27/120    avg_loss:0.399, val_acc:0.847]
Epoch [28/120    avg_loss:0.401, val_acc:0.849]
Epoch [29/120    avg_loss:0.288, val_acc:0.883]
Epoch [30/120    avg_loss:0.343, val_acc:0.872]
Epoch [31/120    avg_loss:0.325, val_acc:0.852]
Epoch [32/120    avg_loss:0.298, val_acc:0.877]
Epoch [33/120    avg_loss:0.250, val_acc:0.872]
Epoch [34/120    avg_loss:0.244, val_acc:0.874]
Epoch [35/120    avg_loss:0.205, val_acc:0.907]
Epoch [36/120    avg_loss:0.222, val_acc:0.887]
Epoch [37/120    avg_loss:0.283, val_acc:0.878]
Epoch [38/120    avg_loss:0.254, val_acc:0.869]
Epoch [39/120    avg_loss:0.211, val_acc:0.897]
Epoch [40/120    avg_loss:0.242, val_acc:0.894]
Epoch [41/120    avg_loss:0.161, val_acc:0.901]
Epoch [42/120    avg_loss:0.167, val_acc:0.920]
Epoch [43/120    avg_loss:0.178, val_acc:0.908]
Epoch [44/120    avg_loss:0.203, val_acc:0.903]
Epoch [45/120    avg_loss:0.160, val_acc:0.918]
Epoch [46/120    avg_loss:0.126, val_acc:0.928]
Epoch [47/120    avg_loss:0.113, val_acc:0.932]
Epoch [48/120    avg_loss:0.112, val_acc:0.930]
Epoch [49/120    avg_loss:0.118, val_acc:0.930]
Epoch [50/120    avg_loss:0.104, val_acc:0.935]
Epoch [51/120    avg_loss:0.137, val_acc:0.900]
Epoch [52/120    avg_loss:0.145, val_acc:0.921]
Epoch [53/120    avg_loss:0.125, val_acc:0.917]
Epoch [54/120    avg_loss:0.141, val_acc:0.918]
Epoch [55/120    avg_loss:0.113, val_acc:0.934]
Epoch [56/120    avg_loss:0.082, val_acc:0.933]
Epoch [57/120    avg_loss:0.097, val_acc:0.931]
Epoch [58/120    avg_loss:0.078, val_acc:0.949]
Epoch [59/120    avg_loss:0.088, val_acc:0.947]
Epoch [60/120    avg_loss:0.094, val_acc:0.932]
Epoch [61/120    avg_loss:0.076, val_acc:0.947]
Epoch [62/120    avg_loss:0.089, val_acc:0.932]
Epoch [63/120    avg_loss:0.086, val_acc:0.946]
Epoch [64/120    avg_loss:0.068, val_acc:0.946]
Epoch [65/120    avg_loss:0.068, val_acc:0.955]
Epoch [66/120    avg_loss:0.055, val_acc:0.957]
Epoch [67/120    avg_loss:0.048, val_acc:0.949]
Epoch [68/120    avg_loss:0.050, val_acc:0.952]
Epoch [69/120    avg_loss:0.056, val_acc:0.948]
Epoch [70/120    avg_loss:0.052, val_acc:0.949]
Epoch [71/120    avg_loss:0.053, val_acc:0.957]
Epoch [72/120    avg_loss:0.048, val_acc:0.960]
Epoch [73/120    avg_loss:0.063, val_acc:0.960]
Epoch [74/120    avg_loss:0.040, val_acc:0.954]
Epoch [75/120    avg_loss:0.047, val_acc:0.950]
Epoch [76/120    avg_loss:0.044, val_acc:0.954]
Epoch [77/120    avg_loss:0.055, val_acc:0.962]
Epoch [78/120    avg_loss:0.056, val_acc:0.953]
Epoch [79/120    avg_loss:0.048, val_acc:0.971]
Epoch [80/120    avg_loss:0.038, val_acc:0.947]
Epoch [81/120    avg_loss:0.047, val_acc:0.955]
Epoch [82/120    avg_loss:0.035, val_acc:0.969]
Epoch [83/120    avg_loss:0.032, val_acc:0.961]
Epoch [84/120    avg_loss:0.029, val_acc:0.969]
Epoch [85/120    avg_loss:0.031, val_acc:0.963]
Epoch [86/120    avg_loss:0.022, val_acc:0.969]
Epoch [87/120    avg_loss:0.026, val_acc:0.969]
Epoch [88/120    avg_loss:0.028, val_acc:0.968]
Epoch [89/120    avg_loss:0.030, val_acc:0.974]
Epoch [90/120    avg_loss:0.034, val_acc:0.958]
Epoch [91/120    avg_loss:0.032, val_acc:0.958]
Epoch [92/120    avg_loss:0.050, val_acc:0.970]
Epoch [93/120    avg_loss:0.045, val_acc:0.963]
Epoch [94/120    avg_loss:0.045, val_acc:0.952]
Epoch [95/120    avg_loss:0.035, val_acc:0.969]
Epoch [96/120    avg_loss:0.025, val_acc:0.970]
Epoch [97/120    avg_loss:0.030, val_acc:0.971]
Epoch [98/120    avg_loss:0.033, val_acc:0.966]
Epoch [99/120    avg_loss:0.025, val_acc:0.970]
Epoch [100/120    avg_loss:0.028, val_acc:0.970]
Epoch [101/120    avg_loss:0.033, val_acc:0.967]
Epoch [102/120    avg_loss:0.045, val_acc:0.958]
Epoch [103/120    avg_loss:0.072, val_acc:0.974]
Epoch [104/120    avg_loss:0.033, val_acc:0.976]
Epoch [105/120    avg_loss:0.022, val_acc:0.976]
Epoch [106/120    avg_loss:0.024, val_acc:0.978]
Epoch [107/120    avg_loss:0.021, val_acc:0.984]
Epoch [108/120    avg_loss:0.025, val_acc:0.980]
Epoch [109/120    avg_loss:0.018, val_acc:0.980]
Epoch [110/120    avg_loss:0.016, val_acc:0.980]
Epoch [111/120    avg_loss:0.021, val_acc:0.981]
Epoch [112/120    avg_loss:0.016, val_acc:0.981]
Epoch [113/120    avg_loss:0.019, val_acc:0.982]
Epoch [114/120    avg_loss:0.020, val_acc:0.980]
Epoch [115/120    avg_loss:0.017, val_acc:0.977]
Epoch [116/120    avg_loss:0.016, val_acc:0.981]
Epoch [117/120    avg_loss:0.015, val_acc:0.982]
Epoch [118/120    avg_loss:0.019, val_acc:0.982]
Epoch [119/120    avg_loss:0.016, val_acc:0.984]
Epoch [120/120    avg_loss:0.018, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1259    0    7    0    1    0    0    0    3   15    0    0
     0    0    0]
 [   0    0    0  685    3    7    0    0    0    4    0    0   48    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    5    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   12    0    0    5    0    0    0    0  840   18    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0   13 2184    8    0
     2    0    0]
 [   0    0    0    0    0    5    0    0    0    0   11    5  507    0
     2    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    3    0    0    0
  1131    0    0]
 [   0    0    0    0    0    0   22    0    0    0    0    0    0    0
     4  321    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.57181571815718

F1 scores:
[       nan 0.975      0.98397812 0.95670391 0.97706422 0.96839729
 0.98050975 1.         1.         0.74418605 0.96164854 0.9848929
 0.92181818 1.         0.99254059 0.96107784 0.97076023]

Kappa:
0.9723158243024272
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f588aa34898>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.737, val_acc:0.299]
Epoch [2/120    avg_loss:2.444, val_acc:0.346]
Epoch [3/120    avg_loss:2.258, val_acc:0.493]
Epoch [4/120    avg_loss:2.123, val_acc:0.518]
Epoch [5/120    avg_loss:2.022, val_acc:0.566]
Epoch [6/120    avg_loss:1.904, val_acc:0.585]
Epoch [7/120    avg_loss:1.791, val_acc:0.622]
Epoch [8/120    avg_loss:1.667, val_acc:0.661]
Epoch [9/120    avg_loss:1.549, val_acc:0.681]
Epoch [10/120    avg_loss:1.473, val_acc:0.670]
Epoch [11/120    avg_loss:1.357, val_acc:0.704]
Epoch [12/120    avg_loss:1.215, val_acc:0.690]
Epoch [13/120    avg_loss:1.105, val_acc:0.709]
Epoch [14/120    avg_loss:1.005, val_acc:0.766]
Epoch [15/120    avg_loss:0.893, val_acc:0.751]
Epoch [16/120    avg_loss:0.834, val_acc:0.787]
Epoch [17/120    avg_loss:0.803, val_acc:0.778]
Epoch [18/120    avg_loss:0.706, val_acc:0.798]
Epoch [19/120    avg_loss:0.652, val_acc:0.792]
Epoch [20/120    avg_loss:0.649, val_acc:0.838]
Epoch [21/120    avg_loss:0.562, val_acc:0.838]
Epoch [22/120    avg_loss:0.580, val_acc:0.850]
Epoch [23/120    avg_loss:0.539, val_acc:0.837]
Epoch [24/120    avg_loss:0.548, val_acc:0.872]
Epoch [25/120    avg_loss:0.461, val_acc:0.858]
Epoch [26/120    avg_loss:0.498, val_acc:0.805]
Epoch [27/120    avg_loss:0.582, val_acc:0.821]
Epoch [28/120    avg_loss:0.506, val_acc:0.805]
Epoch [29/120    avg_loss:0.382, val_acc:0.882]
Epoch [30/120    avg_loss:0.322, val_acc:0.908]
Epoch [31/120    avg_loss:0.333, val_acc:0.885]
Epoch [32/120    avg_loss:0.391, val_acc:0.869]
Epoch [33/120    avg_loss:0.342, val_acc:0.898]
Epoch [34/120    avg_loss:0.227, val_acc:0.915]
Epoch [35/120    avg_loss:0.210, val_acc:0.915]
Epoch [36/120    avg_loss:0.224, val_acc:0.923]
Epoch [37/120    avg_loss:0.218, val_acc:0.916]
Epoch [38/120    avg_loss:0.204, val_acc:0.924]
Epoch [39/120    avg_loss:0.178, val_acc:0.940]
Epoch [40/120    avg_loss:0.195, val_acc:0.922]
Epoch [41/120    avg_loss:0.161, val_acc:0.922]
Epoch [42/120    avg_loss:0.153, val_acc:0.917]
Epoch [43/120    avg_loss:0.167, val_acc:0.925]
Epoch [44/120    avg_loss:0.156, val_acc:0.943]
Epoch [45/120    avg_loss:0.142, val_acc:0.936]
Epoch [46/120    avg_loss:0.140, val_acc:0.945]
Epoch [47/120    avg_loss:0.129, val_acc:0.940]
Epoch [48/120    avg_loss:0.114, val_acc:0.956]
Epoch [49/120    avg_loss:0.140, val_acc:0.949]
Epoch [50/120    avg_loss:0.155, val_acc:0.945]
Epoch [51/120    avg_loss:0.112, val_acc:0.951]
Epoch [52/120    avg_loss:0.107, val_acc:0.948]
Epoch [53/120    avg_loss:0.125, val_acc:0.950]
Epoch [54/120    avg_loss:0.104, val_acc:0.950]
Epoch [55/120    avg_loss:0.106, val_acc:0.942]
Epoch [56/120    avg_loss:0.102, val_acc:0.955]
Epoch [57/120    avg_loss:0.079, val_acc:0.955]
Epoch [58/120    avg_loss:0.096, val_acc:0.943]
Epoch [59/120    avg_loss:0.107, val_acc:0.955]
Epoch [60/120    avg_loss:0.086, val_acc:0.943]
Epoch [61/120    avg_loss:0.100, val_acc:0.950]
Epoch [62/120    avg_loss:0.073, val_acc:0.960]
Epoch [63/120    avg_loss:0.058, val_acc:0.959]
Epoch [64/120    avg_loss:0.053, val_acc:0.956]
Epoch [65/120    avg_loss:0.050, val_acc:0.956]
Epoch [66/120    avg_loss:0.047, val_acc:0.959]
Epoch [67/120    avg_loss:0.045, val_acc:0.963]
Epoch [68/120    avg_loss:0.052, val_acc:0.960]
Epoch [69/120    avg_loss:0.050, val_acc:0.960]
Epoch [70/120    avg_loss:0.051, val_acc:0.959]
Epoch [71/120    avg_loss:0.052, val_acc:0.963]
Epoch [72/120    avg_loss:0.037, val_acc:0.965]
Epoch [73/120    avg_loss:0.040, val_acc:0.963]
Epoch [74/120    avg_loss:0.045, val_acc:0.968]
Epoch [75/120    avg_loss:0.052, val_acc:0.963]
Epoch [76/120    avg_loss:0.045, val_acc:0.964]
Epoch [77/120    avg_loss:0.038, val_acc:0.965]
Epoch [78/120    avg_loss:0.050, val_acc:0.964]
Epoch [79/120    avg_loss:0.043, val_acc:0.963]
Epoch [80/120    avg_loss:0.040, val_acc:0.962]
Epoch [81/120    avg_loss:0.050, val_acc:0.963]
Epoch [82/120    avg_loss:0.034, val_acc:0.969]
Epoch [83/120    avg_loss:0.048, val_acc:0.967]
Epoch [84/120    avg_loss:0.048, val_acc:0.967]
Epoch [85/120    avg_loss:0.043, val_acc:0.964]
Epoch [86/120    avg_loss:0.040, val_acc:0.965]
Epoch [87/120    avg_loss:0.047, val_acc:0.967]
Epoch [88/120    avg_loss:0.038, val_acc:0.969]
Epoch [89/120    avg_loss:0.041, val_acc:0.971]
Epoch [90/120    avg_loss:0.039, val_acc:0.972]
Epoch [91/120    avg_loss:0.041, val_acc:0.970]
Epoch [92/120    avg_loss:0.037, val_acc:0.965]
Epoch [93/120    avg_loss:0.040, val_acc:0.965]
Epoch [94/120    avg_loss:0.037, val_acc:0.971]
Epoch [95/120    avg_loss:0.040, val_acc:0.968]
Epoch [96/120    avg_loss:0.043, val_acc:0.969]
Epoch [97/120    avg_loss:0.034, val_acc:0.970]
Epoch [98/120    avg_loss:0.037, val_acc:0.965]
Epoch [99/120    avg_loss:0.032, val_acc:0.967]
Epoch [100/120    avg_loss:0.036, val_acc:0.965]
Epoch [101/120    avg_loss:0.037, val_acc:0.968]
Epoch [102/120    avg_loss:0.038, val_acc:0.970]
Epoch [103/120    avg_loss:0.039, val_acc:0.968]
Epoch [104/120    avg_loss:0.032, val_acc:0.969]
Epoch [105/120    avg_loss:0.036, val_acc:0.970]
Epoch [106/120    avg_loss:0.038, val_acc:0.971]
Epoch [107/120    avg_loss:0.036, val_acc:0.971]
Epoch [108/120    avg_loss:0.037, val_acc:0.971]
Epoch [109/120    avg_loss:0.032, val_acc:0.971]
Epoch [110/120    avg_loss:0.034, val_acc:0.971]
Epoch [111/120    avg_loss:0.036, val_acc:0.971]
Epoch [112/120    avg_loss:0.038, val_acc:0.971]
Epoch [113/120    avg_loss:0.035, val_acc:0.971]
Epoch [114/120    avg_loss:0.036, val_acc:0.971]
Epoch [115/120    avg_loss:0.035, val_acc:0.971]
Epoch [116/120    avg_loss:0.034, val_acc:0.971]
Epoch [117/120    avg_loss:0.037, val_acc:0.971]
Epoch [118/120    avg_loss:0.035, val_acc:0.971]
Epoch [119/120    avg_loss:0.034, val_acc:0.971]
Epoch [120/120    avg_loss:0.035, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1212    0    3    0    6    0    0    1   10   49    4    0
     0    0    0]
 [   0    0    2  685    3    9    0    0    0   17    0    0   28    2
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   41   60    0    7    0    0    0    0  746   19    0    0
     1    1    0]
 [   0    0   16    0    0    1    5    0    0    0   13 2167    6    2
     0    0    0]
 [   0    0    0    5   14    6    0    0    0    0   12    0  492    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    0    0    0    0    6    3    1    0
  1120    0    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    46  297    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.33875338753387

F1 scores:
[       nan 0.96202532 0.94835681 0.91516366 0.95515695 0.95758929
 0.98795181 0.98039216 1.         0.59259259 0.8960961  0.97415149
 0.92221181 0.98930481 0.96969697 0.92093023 0.97109827]

Kappa:
0.94684400385154
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f966a468898>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.794, val_acc:0.388]
Epoch [2/120    avg_loss:2.545, val_acc:0.412]
Epoch [3/120    avg_loss:2.319, val_acc:0.488]
Epoch [4/120    avg_loss:2.146, val_acc:0.518]
Epoch [5/120    avg_loss:2.066, val_acc:0.569]
Epoch [6/120    avg_loss:1.918, val_acc:0.570]
Epoch [7/120    avg_loss:1.819, val_acc:0.573]
Epoch [8/120    avg_loss:1.667, val_acc:0.609]
Epoch [9/120    avg_loss:1.564, val_acc:0.644]
Epoch [10/120    avg_loss:1.445, val_acc:0.651]
Epoch [11/120    avg_loss:1.379, val_acc:0.698]
Epoch [12/120    avg_loss:1.201, val_acc:0.702]
Epoch [13/120    avg_loss:1.080, val_acc:0.722]
Epoch [14/120    avg_loss:1.011, val_acc:0.702]
Epoch [15/120    avg_loss:0.877, val_acc:0.723]
Epoch [16/120    avg_loss:0.852, val_acc:0.756]
Epoch [17/120    avg_loss:0.774, val_acc:0.738]
Epoch [18/120    avg_loss:0.727, val_acc:0.772]
Epoch [19/120    avg_loss:0.690, val_acc:0.766]
Epoch [20/120    avg_loss:0.646, val_acc:0.771]
Epoch [21/120    avg_loss:0.539, val_acc:0.814]
Epoch [22/120    avg_loss:0.530, val_acc:0.831]
Epoch [23/120    avg_loss:0.494, val_acc:0.806]
Epoch [24/120    avg_loss:0.453, val_acc:0.872]
Epoch [25/120    avg_loss:0.411, val_acc:0.877]
Epoch [26/120    avg_loss:0.349, val_acc:0.858]
Epoch [27/120    avg_loss:0.309, val_acc:0.873]
Epoch [28/120    avg_loss:0.307, val_acc:0.884]
Epoch [29/120    avg_loss:0.282, val_acc:0.886]
Epoch [30/120    avg_loss:0.303, val_acc:0.867]
Epoch [31/120    avg_loss:0.278, val_acc:0.883]
Epoch [32/120    avg_loss:0.378, val_acc:0.853]
Epoch [33/120    avg_loss:0.328, val_acc:0.901]
Epoch [34/120    avg_loss:0.273, val_acc:0.882]
Epoch [35/120    avg_loss:0.213, val_acc:0.908]
Epoch [36/120    avg_loss:0.186, val_acc:0.912]
Epoch [37/120    avg_loss:0.179, val_acc:0.922]
Epoch [38/120    avg_loss:0.183, val_acc:0.924]
Epoch [39/120    avg_loss:0.165, val_acc:0.926]
Epoch [40/120    avg_loss:0.158, val_acc:0.894]
Epoch [41/120    avg_loss:0.182, val_acc:0.919]
Epoch [42/120    avg_loss:0.140, val_acc:0.915]
Epoch [43/120    avg_loss:0.171, val_acc:0.924]
Epoch [44/120    avg_loss:0.155, val_acc:0.922]
Epoch [45/120    avg_loss:0.145, val_acc:0.936]
Epoch [46/120    avg_loss:0.122, val_acc:0.938]
Epoch [47/120    avg_loss:0.124, val_acc:0.935]
Epoch [48/120    avg_loss:0.087, val_acc:0.941]
Epoch [49/120    avg_loss:0.107, val_acc:0.943]
Epoch [50/120    avg_loss:0.103, val_acc:0.943]
Epoch [51/120    avg_loss:0.108, val_acc:0.920]
Epoch [52/120    avg_loss:0.246, val_acc:0.860]
Epoch [53/120    avg_loss:0.315, val_acc:0.879]
Epoch [54/120    avg_loss:0.289, val_acc:0.907]
Epoch [55/120    avg_loss:0.200, val_acc:0.924]
Epoch [56/120    avg_loss:0.155, val_acc:0.942]
Epoch [57/120    avg_loss:0.110, val_acc:0.927]
Epoch [58/120    avg_loss:0.121, val_acc:0.925]
Epoch [59/120    avg_loss:0.112, val_acc:0.933]
Epoch [60/120    avg_loss:0.107, val_acc:0.944]
Epoch [61/120    avg_loss:0.069, val_acc:0.945]
Epoch [62/120    avg_loss:0.075, val_acc:0.924]
Epoch [63/120    avg_loss:0.077, val_acc:0.954]
Epoch [64/120    avg_loss:0.064, val_acc:0.953]
Epoch [65/120    avg_loss:0.071, val_acc:0.943]
Epoch [66/120    avg_loss:0.069, val_acc:0.954]
Epoch [67/120    avg_loss:0.055, val_acc:0.950]
Epoch [68/120    avg_loss:0.064, val_acc:0.963]
Epoch [69/120    avg_loss:0.068, val_acc:0.955]
Epoch [70/120    avg_loss:0.060, val_acc:0.958]
Epoch [71/120    avg_loss:0.061, val_acc:0.938]
Epoch [72/120    avg_loss:0.061, val_acc:0.956]
Epoch [73/120    avg_loss:0.059, val_acc:0.953]
Epoch [74/120    avg_loss:0.053, val_acc:0.955]
Epoch [75/120    avg_loss:0.048, val_acc:0.945]
Epoch [76/120    avg_loss:0.054, val_acc:0.956]
Epoch [77/120    avg_loss:0.044, val_acc:0.956]
Epoch [78/120    avg_loss:0.060, val_acc:0.950]
Epoch [79/120    avg_loss:0.069, val_acc:0.953]
Epoch [80/120    avg_loss:0.056, val_acc:0.955]
Epoch [81/120    avg_loss:0.063, val_acc:0.950]
Epoch [82/120    avg_loss:0.058, val_acc:0.953]
Epoch [83/120    avg_loss:0.037, val_acc:0.958]
Epoch [84/120    avg_loss:0.034, val_acc:0.955]
Epoch [85/120    avg_loss:0.030, val_acc:0.958]
Epoch [86/120    avg_loss:0.040, val_acc:0.961]
Epoch [87/120    avg_loss:0.035, val_acc:0.964]
Epoch [88/120    avg_loss:0.027, val_acc:0.968]
Epoch [89/120    avg_loss:0.029, val_acc:0.964]
Epoch [90/120    avg_loss:0.033, val_acc:0.965]
Epoch [91/120    avg_loss:0.027, val_acc:0.967]
Epoch [92/120    avg_loss:0.034, val_acc:0.965]
Epoch [93/120    avg_loss:0.026, val_acc:0.965]
Epoch [94/120    avg_loss:0.023, val_acc:0.964]
Epoch [95/120    avg_loss:0.025, val_acc:0.967]
Epoch [96/120    avg_loss:0.033, val_acc:0.969]
Epoch [97/120    avg_loss:0.027, val_acc:0.967]
Epoch [98/120    avg_loss:0.023, val_acc:0.968]
Epoch [99/120    avg_loss:0.023, val_acc:0.969]
Epoch [100/120    avg_loss:0.025, val_acc:0.969]
Epoch [101/120    avg_loss:0.021, val_acc:0.967]
Epoch [102/120    avg_loss:0.024, val_acc:0.967]
Epoch [103/120    avg_loss:0.025, val_acc:0.965]
Epoch [104/120    avg_loss:0.025, val_acc:0.967]
Epoch [105/120    avg_loss:0.027, val_acc:0.969]
Epoch [106/120    avg_loss:0.027, val_acc:0.970]
Epoch [107/120    avg_loss:0.024, val_acc:0.971]
Epoch [108/120    avg_loss:0.022, val_acc:0.970]
Epoch [109/120    avg_loss:0.025, val_acc:0.970]
Epoch [110/120    avg_loss:0.025, val_acc:0.971]
Epoch [111/120    avg_loss:0.024, val_acc:0.969]
Epoch [112/120    avg_loss:0.021, val_acc:0.967]
Epoch [113/120    avg_loss:0.023, val_acc:0.970]
Epoch [114/120    avg_loss:0.026, val_acc:0.969]
Epoch [115/120    avg_loss:0.024, val_acc:0.969]
Epoch [116/120    avg_loss:0.023, val_acc:0.969]
Epoch [117/120    avg_loss:0.027, val_acc:0.968]
Epoch [118/120    avg_loss:0.020, val_acc:0.967]
Epoch [119/120    avg_loss:0.020, val_acc:0.967]
Epoch [120/120    avg_loss:0.023, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1232    6    0    0    4    0    0    0    5   30    7    0
     0    1    0]
 [   0    0    4  688    1    9    2    0    0    5    0    0   35    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   13   71    0    4    0    0    0    0  764   14    0    0
     0    9    0]
 [   0    0   20    0    0    0   13    0    2    0   24 2136   13    2
     0    0    0]
 [   0    0    2   15    8    1    0    0    0    1    2    8  489    0
     0    0    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   14    0    0    2    0    2    1    0    0
  1120    0    0]
 [   0    0    1    0    0    0   15    0    0    1    0    0    0    0
    37  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.3821138211382

F1 scores:
[       nan 0.96202532 0.96362925 0.90111329 0.97931034 0.96536313
 0.97401633 1.         0.99186992 0.75555556 0.91223881 0.97090909
 0.9038817  0.98666667 0.97560976 0.90153846 0.95454545]

Kappa:
0.947388999806095
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f872a3898d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.722, val_acc:0.388]
Epoch [2/120    avg_loss:2.477, val_acc:0.446]
Epoch [3/120    avg_loss:2.277, val_acc:0.491]
Epoch [4/120    avg_loss:2.160, val_acc:0.512]
Epoch [5/120    avg_loss:1.993, val_acc:0.566]
Epoch [6/120    avg_loss:1.916, val_acc:0.580]
Epoch [7/120    avg_loss:1.812, val_acc:0.605]
Epoch [8/120    avg_loss:1.732, val_acc:0.618]
Epoch [9/120    avg_loss:1.644, val_acc:0.609]
Epoch [10/120    avg_loss:1.518, val_acc:0.644]
Epoch [11/120    avg_loss:1.332, val_acc:0.674]
Epoch [12/120    avg_loss:1.258, val_acc:0.677]
Epoch [13/120    avg_loss:1.142, val_acc:0.692]
Epoch [14/120    avg_loss:1.071, val_acc:0.702]
Epoch [15/120    avg_loss:0.990, val_acc:0.735]
Epoch [16/120    avg_loss:0.885, val_acc:0.780]
Epoch [17/120    avg_loss:0.791, val_acc:0.772]
Epoch [18/120    avg_loss:0.753, val_acc:0.805]
Epoch [19/120    avg_loss:0.763, val_acc:0.782]
Epoch [20/120    avg_loss:0.639, val_acc:0.779]
Epoch [21/120    avg_loss:0.597, val_acc:0.839]
Epoch [22/120    avg_loss:0.526, val_acc:0.853]
Epoch [23/120    avg_loss:0.478, val_acc:0.847]
Epoch [24/120    avg_loss:0.465, val_acc:0.856]
Epoch [25/120    avg_loss:0.421, val_acc:0.872]
Epoch [26/120    avg_loss:0.413, val_acc:0.883]
Epoch [27/120    avg_loss:0.387, val_acc:0.876]
Epoch [28/120    avg_loss:0.370, val_acc:0.882]
Epoch [29/120    avg_loss:0.360, val_acc:0.891]
Epoch [30/120    avg_loss:0.322, val_acc:0.871]
Epoch [31/120    avg_loss:0.321, val_acc:0.883]
Epoch [32/120    avg_loss:0.266, val_acc:0.901]
Epoch [33/120    avg_loss:0.217, val_acc:0.925]
Epoch [34/120    avg_loss:0.274, val_acc:0.874]
Epoch [35/120    avg_loss:0.373, val_acc:0.905]
Epoch [36/120    avg_loss:0.301, val_acc:0.885]
Epoch [37/120    avg_loss:0.397, val_acc:0.879]
Epoch [38/120    avg_loss:0.295, val_acc:0.920]
Epoch [39/120    avg_loss:0.195, val_acc:0.920]
Epoch [40/120    avg_loss:0.150, val_acc:0.938]
Epoch [41/120    avg_loss:0.200, val_acc:0.912]
Epoch [42/120    avg_loss:0.180, val_acc:0.940]
Epoch [43/120    avg_loss:0.178, val_acc:0.921]
Epoch [44/120    avg_loss:0.185, val_acc:0.932]
Epoch [45/120    avg_loss:0.160, val_acc:0.948]
Epoch [46/120    avg_loss:0.134, val_acc:0.943]
Epoch [47/120    avg_loss:0.113, val_acc:0.949]
Epoch [48/120    avg_loss:0.114, val_acc:0.953]
Epoch [49/120    avg_loss:0.114, val_acc:0.949]
Epoch [50/120    avg_loss:0.116, val_acc:0.950]
Epoch [51/120    avg_loss:0.099, val_acc:0.950]
Epoch [52/120    avg_loss:0.099, val_acc:0.953]
Epoch [53/120    avg_loss:0.077, val_acc:0.962]
Epoch [54/120    avg_loss:0.076, val_acc:0.946]
Epoch [55/120    avg_loss:0.112, val_acc:0.945]
Epoch [56/120    avg_loss:0.080, val_acc:0.951]
Epoch [57/120    avg_loss:0.088, val_acc:0.938]
Epoch [58/120    avg_loss:0.101, val_acc:0.952]
Epoch [59/120    avg_loss:0.064, val_acc:0.962]
Epoch [60/120    avg_loss:0.091, val_acc:0.963]
Epoch [61/120    avg_loss:0.076, val_acc:0.945]
Epoch [62/120    avg_loss:0.084, val_acc:0.949]
Epoch [63/120    avg_loss:0.072, val_acc:0.960]
Epoch [64/120    avg_loss:0.059, val_acc:0.969]
Epoch [65/120    avg_loss:0.062, val_acc:0.953]
Epoch [66/120    avg_loss:0.072, val_acc:0.949]
Epoch [67/120    avg_loss:0.072, val_acc:0.950]
Epoch [68/120    avg_loss:0.058, val_acc:0.963]
Epoch [69/120    avg_loss:0.050, val_acc:0.965]
Epoch [70/120    avg_loss:0.053, val_acc:0.968]
Epoch [71/120    avg_loss:0.063, val_acc:0.941]
Epoch [72/120    avg_loss:0.115, val_acc:0.948]
Epoch [73/120    avg_loss:0.071, val_acc:0.955]
Epoch [74/120    avg_loss:0.050, val_acc:0.963]
Epoch [75/120    avg_loss:0.047, val_acc:0.971]
Epoch [76/120    avg_loss:0.041, val_acc:0.968]
Epoch [77/120    avg_loss:0.041, val_acc:0.961]
Epoch [78/120    avg_loss:0.037, val_acc:0.970]
Epoch [79/120    avg_loss:0.033, val_acc:0.977]
Epoch [80/120    avg_loss:0.030, val_acc:0.977]
Epoch [81/120    avg_loss:0.059, val_acc:0.963]
Epoch [82/120    avg_loss:0.043, val_acc:0.962]
Epoch [83/120    avg_loss:0.037, val_acc:0.973]
Epoch [84/120    avg_loss:0.042, val_acc:0.973]
Epoch [85/120    avg_loss:0.039, val_acc:0.961]
Epoch [86/120    avg_loss:0.045, val_acc:0.967]
Epoch [87/120    avg_loss:0.043, val_acc:0.960]
Epoch [88/120    avg_loss:0.029, val_acc:0.975]
Epoch [89/120    avg_loss:0.034, val_acc:0.968]
Epoch [90/120    avg_loss:0.040, val_acc:0.968]
Epoch [91/120    avg_loss:0.044, val_acc:0.956]
Epoch [92/120    avg_loss:0.056, val_acc:0.969]
Epoch [93/120    avg_loss:0.065, val_acc:0.964]
Epoch [94/120    avg_loss:0.047, val_acc:0.970]
Epoch [95/120    avg_loss:0.038, val_acc:0.977]
Epoch [96/120    avg_loss:0.028, val_acc:0.975]
Epoch [97/120    avg_loss:0.024, val_acc:0.978]
Epoch [98/120    avg_loss:0.019, val_acc:0.977]
Epoch [99/120    avg_loss:0.023, val_acc:0.978]
Epoch [100/120    avg_loss:0.021, val_acc:0.977]
Epoch [101/120    avg_loss:0.023, val_acc:0.980]
Epoch [102/120    avg_loss:0.020, val_acc:0.979]
Epoch [103/120    avg_loss:0.020, val_acc:0.980]
Epoch [104/120    avg_loss:0.017, val_acc:0.980]
Epoch [105/120    avg_loss:0.018, val_acc:0.980]
Epoch [106/120    avg_loss:0.025, val_acc:0.981]
Epoch [107/120    avg_loss:0.020, val_acc:0.979]
Epoch [108/120    avg_loss:0.019, val_acc:0.980]
Epoch [109/120    avg_loss:0.017, val_acc:0.980]
Epoch [110/120    avg_loss:0.017, val_acc:0.982]
Epoch [111/120    avg_loss:0.021, val_acc:0.983]
Epoch [112/120    avg_loss:0.018, val_acc:0.980]
Epoch [113/120    avg_loss:0.020, val_acc:0.984]
Epoch [114/120    avg_loss:0.021, val_acc:0.984]
Epoch [115/120    avg_loss:0.017, val_acc:0.980]
Epoch [116/120    avg_loss:0.017, val_acc:0.980]
Epoch [117/120    avg_loss:0.017, val_acc:0.982]
Epoch [118/120    avg_loss:0.017, val_acc:0.982]
Epoch [119/120    avg_loss:0.017, val_acc:0.981]
Epoch [120/120    avg_loss:0.018, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1233    1    0    0    2    0    0    0   12   33    4    0
     0    0    0]
 [   0    0    1  692    5   22    0    0    0    3    0    3   20    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   25   75    0    8    0    0    0    0  761    3    1    0
     0    2    0]
 [   0    0    9    0    0    0    6    0    0    0   17 2174    4    0
     0    0    0]
 [   0    0    1    4    0    1    0    0    0    0    8   14  499    0
     2    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    1    0    3    1    0    0
  1126    0    0]
 [   0    0    0    0    0    0   11    0    0    6    0    0    0    0
    41  289    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.98915989159892

F1 scores:
[       nan 0.94871795 0.96554424 0.91112574 0.98839907 0.95709571
 0.98498498 1.         0.99883856 0.74418605 0.90595238 0.97949989
 0.93796992 0.99730458 0.97573657 0.90595611 0.97109827]

Kappa:
0.9542591097440446
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa5e35ae860>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.762, val_acc:0.278]
Epoch [2/120    avg_loss:2.487, val_acc:0.324]
Epoch [3/120    avg_loss:2.299, val_acc:0.454]
Epoch [4/120    avg_loss:2.137, val_acc:0.528]
Epoch [5/120    avg_loss:2.026, val_acc:0.531]
Epoch [6/120    avg_loss:1.869, val_acc:0.527]
Epoch [7/120    avg_loss:1.747, val_acc:0.532]
Epoch [8/120    avg_loss:1.706, val_acc:0.536]
Epoch [9/120    avg_loss:1.552, val_acc:0.575]
Epoch [10/120    avg_loss:1.466, val_acc:0.620]
Epoch [11/120    avg_loss:1.386, val_acc:0.593]
Epoch [12/120    avg_loss:1.242, val_acc:0.630]
Epoch [13/120    avg_loss:1.191, val_acc:0.634]
Epoch [14/120    avg_loss:1.050, val_acc:0.699]
Epoch [15/120    avg_loss:0.980, val_acc:0.720]
Epoch [16/120    avg_loss:0.868, val_acc:0.726]
Epoch [17/120    avg_loss:0.838, val_acc:0.751]
Epoch [18/120    avg_loss:0.763, val_acc:0.745]
Epoch [19/120    avg_loss:0.685, val_acc:0.787]
Epoch [20/120    avg_loss:0.625, val_acc:0.768]
Epoch [21/120    avg_loss:0.603, val_acc:0.814]
Epoch [22/120    avg_loss:0.533, val_acc:0.807]
Epoch [23/120    avg_loss:0.542, val_acc:0.810]
Epoch [24/120    avg_loss:0.530, val_acc:0.788]
Epoch [25/120    avg_loss:0.442, val_acc:0.839]
Epoch [26/120    avg_loss:0.428, val_acc:0.819]
Epoch [27/120    avg_loss:0.374, val_acc:0.848]
Epoch [28/120    avg_loss:0.310, val_acc:0.864]
Epoch [29/120    avg_loss:0.365, val_acc:0.839]
Epoch [30/120    avg_loss:0.368, val_acc:0.859]
Epoch [31/120    avg_loss:0.388, val_acc:0.847]
Epoch [32/120    avg_loss:0.287, val_acc:0.876]
Epoch [33/120    avg_loss:0.247, val_acc:0.874]
Epoch [34/120    avg_loss:0.257, val_acc:0.901]
Epoch [35/120    avg_loss:0.247, val_acc:0.863]
Epoch [36/120    avg_loss:0.294, val_acc:0.877]
Epoch [37/120    avg_loss:0.219, val_acc:0.905]
Epoch [38/120    avg_loss:0.206, val_acc:0.875]
Epoch [39/120    avg_loss:0.253, val_acc:0.883]
Epoch [40/120    avg_loss:0.196, val_acc:0.912]
Epoch [41/120    avg_loss:0.232, val_acc:0.883]
Epoch [42/120    avg_loss:0.201, val_acc:0.927]
Epoch [43/120    avg_loss:0.178, val_acc:0.911]
Epoch [44/120    avg_loss:0.175, val_acc:0.917]
Epoch [45/120    avg_loss:0.162, val_acc:0.904]
Epoch [46/120    avg_loss:0.170, val_acc:0.928]
Epoch [47/120    avg_loss:0.199, val_acc:0.912]
Epoch [48/120    avg_loss:0.129, val_acc:0.928]
Epoch [49/120    avg_loss:0.120, val_acc:0.926]
Epoch [50/120    avg_loss:0.112, val_acc:0.944]
Epoch [51/120    avg_loss:0.134, val_acc:0.935]
Epoch [52/120    avg_loss:0.111, val_acc:0.927]
Epoch [53/120    avg_loss:0.112, val_acc:0.935]
Epoch [54/120    avg_loss:0.117, val_acc:0.923]
Epoch [55/120    avg_loss:0.113, val_acc:0.934]
Epoch [56/120    avg_loss:0.120, val_acc:0.933]
Epoch [57/120    avg_loss:0.096, val_acc:0.909]
Epoch [58/120    avg_loss:0.142, val_acc:0.938]
Epoch [59/120    avg_loss:0.086, val_acc:0.940]
Epoch [60/120    avg_loss:0.085, val_acc:0.947]
Epoch [61/120    avg_loss:0.093, val_acc:0.938]
Epoch [62/120    avg_loss:0.095, val_acc:0.944]
Epoch [63/120    avg_loss:0.100, val_acc:0.906]
Epoch [64/120    avg_loss:0.103, val_acc:0.932]
Epoch [65/120    avg_loss:0.092, val_acc:0.943]
Epoch [66/120    avg_loss:0.101, val_acc:0.927]
Epoch [67/120    avg_loss:0.068, val_acc:0.949]
Epoch [68/120    avg_loss:0.078, val_acc:0.950]
Epoch [69/120    avg_loss:0.068, val_acc:0.953]
Epoch [70/120    avg_loss:0.059, val_acc:0.954]
Epoch [71/120    avg_loss:0.143, val_acc:0.927]
Epoch [72/120    avg_loss:0.083, val_acc:0.953]
Epoch [73/120    avg_loss:0.070, val_acc:0.929]
Epoch [74/120    avg_loss:0.089, val_acc:0.942]
Epoch [75/120    avg_loss:0.066, val_acc:0.938]
Epoch [76/120    avg_loss:0.055, val_acc:0.949]
Epoch [77/120    avg_loss:0.060, val_acc:0.945]
Epoch [78/120    avg_loss:0.067, val_acc:0.949]
Epoch [79/120    avg_loss:0.052, val_acc:0.957]
Epoch [80/120    avg_loss:0.046, val_acc:0.960]
Epoch [81/120    avg_loss:0.040, val_acc:0.962]
Epoch [82/120    avg_loss:0.053, val_acc:0.942]
Epoch [83/120    avg_loss:0.069, val_acc:0.962]
Epoch [84/120    avg_loss:0.056, val_acc:0.952]
Epoch [85/120    avg_loss:0.042, val_acc:0.960]
Epoch [86/120    avg_loss:0.047, val_acc:0.954]
Epoch [87/120    avg_loss:0.076, val_acc:0.934]
Epoch [88/120    avg_loss:0.051, val_acc:0.960]
Epoch [89/120    avg_loss:0.035, val_acc:0.957]
Epoch [90/120    avg_loss:0.037, val_acc:0.959]
Epoch [91/120    avg_loss:0.053, val_acc:0.960]
Epoch [92/120    avg_loss:0.048, val_acc:0.964]
Epoch [93/120    avg_loss:0.034, val_acc:0.960]
Epoch [94/120    avg_loss:0.033, val_acc:0.970]
Epoch [95/120    avg_loss:0.032, val_acc:0.950]
Epoch [96/120    avg_loss:0.033, val_acc:0.969]
Epoch [97/120    avg_loss:0.027, val_acc:0.968]
Epoch [98/120    avg_loss:0.030, val_acc:0.961]
Epoch [99/120    avg_loss:0.033, val_acc:0.956]
Epoch [100/120    avg_loss:0.035, val_acc:0.959]
Epoch [101/120    avg_loss:0.036, val_acc:0.958]
Epoch [102/120    avg_loss:0.028, val_acc:0.970]
Epoch [103/120    avg_loss:0.018, val_acc:0.969]
Epoch [104/120    avg_loss:0.019, val_acc:0.969]
Epoch [105/120    avg_loss:0.017, val_acc:0.964]
Epoch [106/120    avg_loss:0.029, val_acc:0.954]
Epoch [107/120    avg_loss:0.108, val_acc:0.935]
Epoch [108/120    avg_loss:0.060, val_acc:0.964]
Epoch [109/120    avg_loss:0.038, val_acc:0.960]
Epoch [110/120    avg_loss:0.032, val_acc:0.970]
Epoch [111/120    avg_loss:0.028, val_acc:0.968]
Epoch [112/120    avg_loss:0.050, val_acc:0.956]
Epoch [113/120    avg_loss:0.047, val_acc:0.964]
Epoch [114/120    avg_loss:0.031, val_acc:0.963]
Epoch [115/120    avg_loss:0.029, val_acc:0.971]
Epoch [116/120    avg_loss:0.028, val_acc:0.963]
Epoch [117/120    avg_loss:0.021, val_acc:0.955]
Epoch [118/120    avg_loss:0.028, val_acc:0.961]
Epoch [119/120    avg_loss:0.024, val_acc:0.967]
Epoch [120/120    avg_loss:0.020, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    3    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1232    0    3    0    3    0    0    0    4   35    3    0
     0    5    0]
 [   0    0   11  696    0   14    2    0    0    2    0    1   20    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    2    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    8    0    0    9    0    0    1    0
     0    0    0]
 [   0    0   44   59    0    4    0    0    0    0  754    8    4    0
     0    2    0]
 [   0    0    7    0    0    0    5    0    0    0    6 2186    3    3
     0    0    0]
 [   0    0    0   12    4    2    0    0    0    0   17    5  487    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    4    0    0    0    0    0    0    0    0    3    0    0    0
  1132    0    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    45  291    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.82655826558266

F1 scores:
[       nan 0.875      0.95429899 0.91941876 0.98383372 0.96942242
 0.97535474 0.98039216 1.         0.58064516 0.90734055 0.9831347
 0.92585551 0.98930481 0.97502153 0.90232558 0.96      ]

Kappa:
0.9523791965487362
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdcf6b47860>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.749, val_acc:0.345]
Epoch [2/120    avg_loss:2.454, val_acc:0.454]
Epoch [3/120    avg_loss:2.302, val_acc:0.484]
Epoch [4/120    avg_loss:2.150, val_acc:0.529]
Epoch [5/120    avg_loss:1.994, val_acc:0.584]
Epoch [6/120    avg_loss:1.862, val_acc:0.604]
Epoch [7/120    avg_loss:1.772, val_acc:0.637]
Epoch [8/120    avg_loss:1.650, val_acc:0.638]
Epoch [9/120    avg_loss:1.538, val_acc:0.648]
Epoch [10/120    avg_loss:1.441, val_acc:0.671]
Epoch [11/120    avg_loss:1.276, val_acc:0.688]
Epoch [12/120    avg_loss:1.164, val_acc:0.698]
Epoch [13/120    avg_loss:1.096, val_acc:0.729]
Epoch [14/120    avg_loss:0.988, val_acc:0.720]
Epoch [15/120    avg_loss:0.892, val_acc:0.750]
Epoch [16/120    avg_loss:0.789, val_acc:0.794]
Epoch [17/120    avg_loss:0.700, val_acc:0.785]
Epoch [18/120    avg_loss:0.685, val_acc:0.787]
Epoch [19/120    avg_loss:0.671, val_acc:0.794]
Epoch [20/120    avg_loss:0.701, val_acc:0.791]
Epoch [21/120    avg_loss:0.593, val_acc:0.815]
Epoch [22/120    avg_loss:0.526, val_acc:0.807]
Epoch [23/120    avg_loss:0.493, val_acc:0.865]
Epoch [24/120    avg_loss:0.422, val_acc:0.866]
Epoch [25/120    avg_loss:0.358, val_acc:0.858]
Epoch [26/120    avg_loss:0.346, val_acc:0.883]
Epoch [27/120    avg_loss:0.316, val_acc:0.894]
Epoch [28/120    avg_loss:0.330, val_acc:0.862]
Epoch [29/120    avg_loss:0.288, val_acc:0.883]
Epoch [30/120    avg_loss:0.248, val_acc:0.903]
Epoch [31/120    avg_loss:0.255, val_acc:0.897]
Epoch [32/120    avg_loss:0.206, val_acc:0.885]
Epoch [33/120    avg_loss:0.252, val_acc:0.902]
Epoch [34/120    avg_loss:0.404, val_acc:0.855]
Epoch [35/120    avg_loss:0.348, val_acc:0.879]
Epoch [36/120    avg_loss:0.286, val_acc:0.888]
Epoch [37/120    avg_loss:0.252, val_acc:0.896]
Epoch [38/120    avg_loss:0.224, val_acc:0.915]
Epoch [39/120    avg_loss:0.204, val_acc:0.913]
Epoch [40/120    avg_loss:0.161, val_acc:0.901]
Epoch [41/120    avg_loss:0.196, val_acc:0.924]
Epoch [42/120    avg_loss:0.188, val_acc:0.904]
Epoch [43/120    avg_loss:0.153, val_acc:0.924]
Epoch [44/120    avg_loss:0.159, val_acc:0.926]
Epoch [45/120    avg_loss:0.144, val_acc:0.922]
Epoch [46/120    avg_loss:0.163, val_acc:0.927]
Epoch [47/120    avg_loss:0.113, val_acc:0.932]
Epoch [48/120    avg_loss:0.104, val_acc:0.923]
Epoch [49/120    avg_loss:0.098, val_acc:0.940]
Epoch [50/120    avg_loss:0.089, val_acc:0.933]
Epoch [51/120    avg_loss:0.122, val_acc:0.933]
Epoch [52/120    avg_loss:0.135, val_acc:0.930]
Epoch [53/120    avg_loss:0.114, val_acc:0.930]
Epoch [54/120    avg_loss:0.117, val_acc:0.933]
Epoch [55/120    avg_loss:0.105, val_acc:0.921]
Epoch [56/120    avg_loss:0.089, val_acc:0.948]
Epoch [57/120    avg_loss:0.091, val_acc:0.943]
Epoch [58/120    avg_loss:0.106, val_acc:0.936]
Epoch [59/120    avg_loss:0.090, val_acc:0.939]
Epoch [60/120    avg_loss:0.069, val_acc:0.943]
Epoch [61/120    avg_loss:0.077, val_acc:0.941]
Epoch [62/120    avg_loss:0.100, val_acc:0.943]
Epoch [63/120    avg_loss:0.074, val_acc:0.951]
Epoch [64/120    avg_loss:0.060, val_acc:0.943]
Epoch [65/120    avg_loss:0.053, val_acc:0.959]
Epoch [66/120    avg_loss:0.062, val_acc:0.958]
Epoch [67/120    avg_loss:0.057, val_acc:0.936]
Epoch [68/120    avg_loss:0.066, val_acc:0.944]
Epoch [69/120    avg_loss:0.052, val_acc:0.951]
Epoch [70/120    avg_loss:0.053, val_acc:0.956]
Epoch [71/120    avg_loss:0.054, val_acc:0.936]
Epoch [72/120    avg_loss:0.077, val_acc:0.949]
Epoch [73/120    avg_loss:0.085, val_acc:0.951]
Epoch [74/120    avg_loss:0.077, val_acc:0.933]
Epoch [75/120    avg_loss:0.058, val_acc:0.956]
Epoch [76/120    avg_loss:0.041, val_acc:0.956]
Epoch [77/120    avg_loss:0.033, val_acc:0.970]
Epoch [78/120    avg_loss:0.045, val_acc:0.965]
Epoch [79/120    avg_loss:0.029, val_acc:0.967]
Epoch [80/120    avg_loss:0.030, val_acc:0.956]
Epoch [81/120    avg_loss:0.039, val_acc:0.958]
Epoch [82/120    avg_loss:0.046, val_acc:0.968]
Epoch [83/120    avg_loss:0.041, val_acc:0.950]
Epoch [84/120    avg_loss:0.040, val_acc:0.964]
Epoch [85/120    avg_loss:0.038, val_acc:0.963]
Epoch [86/120    avg_loss:0.042, val_acc:0.968]
Epoch [87/120    avg_loss:0.034, val_acc:0.968]
Epoch [88/120    avg_loss:0.037, val_acc:0.963]
Epoch [89/120    avg_loss:0.031, val_acc:0.968]
Epoch [90/120    avg_loss:0.039, val_acc:0.964]
Epoch [91/120    avg_loss:0.039, val_acc:0.972]
Epoch [92/120    avg_loss:0.026, val_acc:0.971]
Epoch [93/120    avg_loss:0.022, val_acc:0.971]
Epoch [94/120    avg_loss:0.020, val_acc:0.970]
Epoch [95/120    avg_loss:0.018, val_acc:0.970]
Epoch [96/120    avg_loss:0.021, val_acc:0.970]
Epoch [97/120    avg_loss:0.019, val_acc:0.971]
Epoch [98/120    avg_loss:0.017, val_acc:0.968]
Epoch [99/120    avg_loss:0.016, val_acc:0.968]
Epoch [100/120    avg_loss:0.019, val_acc:0.967]
Epoch [101/120    avg_loss:0.020, val_acc:0.968]
Epoch [102/120    avg_loss:0.015, val_acc:0.971]
Epoch [103/120    avg_loss:0.014, val_acc:0.970]
Epoch [104/120    avg_loss:0.021, val_acc:0.968]
Epoch [105/120    avg_loss:0.015, val_acc:0.968]
Epoch [106/120    avg_loss:0.017, val_acc:0.968]
Epoch [107/120    avg_loss:0.015, val_acc:0.968]
Epoch [108/120    avg_loss:0.018, val_acc:0.968]
Epoch [109/120    avg_loss:0.015, val_acc:0.968]
Epoch [110/120    avg_loss:0.019, val_acc:0.968]
Epoch [111/120    avg_loss:0.016, val_acc:0.970]
Epoch [112/120    avg_loss:0.015, val_acc:0.970]
Epoch [113/120    avg_loss:0.030, val_acc:0.970]
Epoch [114/120    avg_loss:0.016, val_acc:0.970]
Epoch [115/120    avg_loss:0.018, val_acc:0.969]
Epoch [116/120    avg_loss:0.022, val_acc:0.970]
Epoch [117/120    avg_loss:0.018, val_acc:0.970]
Epoch [118/120    avg_loss:0.017, val_acc:0.970]
Epoch [119/120    avg_loss:0.016, val_acc:0.970]
Epoch [120/120    avg_loss:0.016, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1239    6    0    0    0    0    0    0    2   36    2    0
     0    0    0]
 [   0    0    4  721    0   12    0    0    0    7    0    0    2    0
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    5    0    4    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   25   68    0    5    0    0    0    0  769    5    0    0
     2    1    0]
 [   0    0   11    0    0    0   12    0    4    0   18 2161    0    0
     4    0    0]
 [   0    0    0    6   14    2    0    0    0    1    5    2  496    0
     1    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    2    0    3    0    0    0
  1126    1    0]
 [   0    0    0    0    0    0    6    0    0    8    0    0    0    0
    43  290    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.15176151761517

F1 scores:
[       nan 0.94871795 0.96645866 0.93092318 0.96583144 0.95583239
 0.98496241 0.90909091 0.99307159 0.64285714 0.9176611  0.97893545
 0.95938104 1.         0.97027143 0.90766823 0.96      ]

Kappa:
0.956131444051257
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f58300998d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.760, val_acc:0.141]
Epoch [2/120    avg_loss:2.500, val_acc:0.289]
Epoch [3/120    avg_loss:2.337, val_acc:0.427]
Epoch [4/120    avg_loss:2.205, val_acc:0.498]
Epoch [5/120    avg_loss:2.053, val_acc:0.564]
Epoch [6/120    avg_loss:1.947, val_acc:0.594]
Epoch [7/120    avg_loss:1.837, val_acc:0.592]
Epoch [8/120    avg_loss:1.727, val_acc:0.624]
Epoch [9/120    avg_loss:1.638, val_acc:0.632]
Epoch [10/120    avg_loss:1.466, val_acc:0.647]
Epoch [11/120    avg_loss:1.357, val_acc:0.667]
Epoch [12/120    avg_loss:1.258, val_acc:0.689]
Epoch [13/120    avg_loss:1.132, val_acc:0.699]
Epoch [14/120    avg_loss:1.014, val_acc:0.739]
Epoch [15/120    avg_loss:0.917, val_acc:0.729]
Epoch [16/120    avg_loss:0.845, val_acc:0.752]
Epoch [17/120    avg_loss:0.785, val_acc:0.786]
Epoch [18/120    avg_loss:0.712, val_acc:0.777]
Epoch [19/120    avg_loss:0.679, val_acc:0.796]
Epoch [20/120    avg_loss:0.575, val_acc:0.825]
Epoch [21/120    avg_loss:0.544, val_acc:0.826]
Epoch [22/120    avg_loss:0.487, val_acc:0.842]
Epoch [23/120    avg_loss:0.456, val_acc:0.807]
Epoch [24/120    avg_loss:0.453, val_acc:0.830]
Epoch [25/120    avg_loss:0.447, val_acc:0.790]
Epoch [26/120    avg_loss:0.487, val_acc:0.842]
Epoch [27/120    avg_loss:0.369, val_acc:0.876]
Epoch [28/120    avg_loss:0.319, val_acc:0.881]
Epoch [29/120    avg_loss:0.307, val_acc:0.868]
Epoch [30/120    avg_loss:0.276, val_acc:0.891]
Epoch [31/120    avg_loss:0.284, val_acc:0.879]
Epoch [32/120    avg_loss:0.259, val_acc:0.884]
Epoch [33/120    avg_loss:0.288, val_acc:0.898]
Epoch [34/120    avg_loss:0.250, val_acc:0.907]
Epoch [35/120    avg_loss:0.231, val_acc:0.920]
Epoch [36/120    avg_loss:0.246, val_acc:0.904]
Epoch [37/120    avg_loss:0.281, val_acc:0.892]
Epoch [38/120    avg_loss:0.291, val_acc:0.872]
Epoch [39/120    avg_loss:0.189, val_acc:0.917]
Epoch [40/120    avg_loss:0.195, val_acc:0.917]
Epoch [41/120    avg_loss:0.168, val_acc:0.929]
Epoch [42/120    avg_loss:0.156, val_acc:0.929]
Epoch [43/120    avg_loss:0.166, val_acc:0.935]
Epoch [44/120    avg_loss:0.153, val_acc:0.927]
Epoch [45/120    avg_loss:0.142, val_acc:0.930]
Epoch [46/120    avg_loss:0.140, val_acc:0.934]
Epoch [47/120    avg_loss:0.191, val_acc:0.865]
Epoch [48/120    avg_loss:0.181, val_acc:0.922]
Epoch [49/120    avg_loss:0.170, val_acc:0.913]
Epoch [50/120    avg_loss:0.184, val_acc:0.933]
Epoch [51/120    avg_loss:0.133, val_acc:0.944]
Epoch [52/120    avg_loss:0.113, val_acc:0.938]
Epoch [53/120    avg_loss:0.123, val_acc:0.938]
Epoch [54/120    avg_loss:0.133, val_acc:0.902]
Epoch [55/120    avg_loss:0.118, val_acc:0.936]
Epoch [56/120    avg_loss:0.107, val_acc:0.936]
Epoch [57/120    avg_loss:0.127, val_acc:0.932]
Epoch [58/120    avg_loss:0.098, val_acc:0.939]
Epoch [59/120    avg_loss:0.085, val_acc:0.951]
Epoch [60/120    avg_loss:0.100, val_acc:0.930]
Epoch [61/120    avg_loss:0.102, val_acc:0.920]
Epoch [62/120    avg_loss:0.109, val_acc:0.950]
Epoch [63/120    avg_loss:0.129, val_acc:0.946]
Epoch [64/120    avg_loss:0.095, val_acc:0.951]
Epoch [65/120    avg_loss:0.114, val_acc:0.941]
Epoch [66/120    avg_loss:0.075, val_acc:0.943]
Epoch [67/120    avg_loss:0.079, val_acc:0.948]
Epoch [68/120    avg_loss:0.076, val_acc:0.949]
Epoch [69/120    avg_loss:0.080, val_acc:0.951]
Epoch [70/120    avg_loss:0.061, val_acc:0.954]
Epoch [71/120    avg_loss:0.060, val_acc:0.959]
Epoch [72/120    avg_loss:0.046, val_acc:0.954]
Epoch [73/120    avg_loss:0.057, val_acc:0.958]
Epoch [74/120    avg_loss:0.297, val_acc:0.894]
Epoch [75/120    avg_loss:0.216, val_acc:0.895]
Epoch [76/120    avg_loss:0.176, val_acc:0.924]
Epoch [77/120    avg_loss:0.129, val_acc:0.939]
Epoch [78/120    avg_loss:0.127, val_acc:0.932]
Epoch [79/120    avg_loss:0.102, val_acc:0.946]
Epoch [80/120    avg_loss:0.079, val_acc:0.949]
Epoch [81/120    avg_loss:0.075, val_acc:0.952]
Epoch [82/120    avg_loss:0.083, val_acc:0.949]
Epoch [83/120    avg_loss:0.073, val_acc:0.956]
Epoch [84/120    avg_loss:0.051, val_acc:0.952]
Epoch [85/120    avg_loss:0.051, val_acc:0.953]
Epoch [86/120    avg_loss:0.045, val_acc:0.955]
Epoch [87/120    avg_loss:0.038, val_acc:0.958]
Epoch [88/120    avg_loss:0.042, val_acc:0.959]
Epoch [89/120    avg_loss:0.044, val_acc:0.959]
Epoch [90/120    avg_loss:0.045, val_acc:0.960]
Epoch [91/120    avg_loss:0.037, val_acc:0.959]
Epoch [92/120    avg_loss:0.039, val_acc:0.960]
Epoch [93/120    avg_loss:0.041, val_acc:0.960]
Epoch [94/120    avg_loss:0.044, val_acc:0.960]
Epoch [95/120    avg_loss:0.039, val_acc:0.961]
Epoch [96/120    avg_loss:0.043, val_acc:0.961]
Epoch [97/120    avg_loss:0.044, val_acc:0.962]
Epoch [98/120    avg_loss:0.041, val_acc:0.964]
Epoch [99/120    avg_loss:0.031, val_acc:0.963]
Epoch [100/120    avg_loss:0.037, val_acc:0.960]
Epoch [101/120    avg_loss:0.036, val_acc:0.962]
Epoch [102/120    avg_loss:0.036, val_acc:0.963]
Epoch [103/120    avg_loss:0.039, val_acc:0.963]
Epoch [104/120    avg_loss:0.028, val_acc:0.963]
Epoch [105/120    avg_loss:0.042, val_acc:0.962]
Epoch [106/120    avg_loss:0.034, val_acc:0.963]
Epoch [107/120    avg_loss:0.037, val_acc:0.960]
Epoch [108/120    avg_loss:0.034, val_acc:0.958]
Epoch [109/120    avg_loss:0.046, val_acc:0.962]
Epoch [110/120    avg_loss:0.034, val_acc:0.962]
Epoch [111/120    avg_loss:0.034, val_acc:0.963]
Epoch [112/120    avg_loss:0.033, val_acc:0.963]
Epoch [113/120    avg_loss:0.032, val_acc:0.962]
Epoch [114/120    avg_loss:0.030, val_acc:0.962]
Epoch [115/120    avg_loss:0.029, val_acc:0.962]
Epoch [116/120    avg_loss:0.028, val_acc:0.962]
Epoch [117/120    avg_loss:0.031, val_acc:0.962]
Epoch [118/120    avg_loss:0.035, val_acc:0.962]
Epoch [119/120    avg_loss:0.034, val_acc:0.962]
Epoch [120/120    avg_loss:0.039, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    2    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1250    0    3    0    0    0    0    0    3   29    0    0
     0    0    0]
 [   0    0    2  693    1   19    0    0    0    6    0    0   22    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    1    0    2    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    2    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   14    0    0    1    0
     0    0    0]
 [   0    0   52   59    0    9    0    0    0    0  733   16    0    0
     1    5    0]
 [   0    0   21    0    0    0    6    0    0    0   13 2157    9    4
     0    0    0]
 [   0    0    8   30    4   11    0    0    0    0    5    0  475    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    0    0    2    0    3    0    2    0
  1123    0    0]
 [   0    0    0    0    0    0   23    0    0    4    0    0    0    0
    74  246    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.74254742547426

F1 scores:
[       nan 0.94871795 0.95419847 0.90469974 0.98156682 0.93846154
 0.97305389 0.98039216 0.99767981 0.63636364 0.89718482 0.97734481
 0.90909091 0.97883598 0.95696634 0.82274247 0.98203593]

Kappa:
0.9400227494299572
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc8cc6dc8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.740, val_acc:0.196]
Epoch [2/120    avg_loss:2.464, val_acc:0.401]
Epoch [3/120    avg_loss:2.290, val_acc:0.414]
Epoch [4/120    avg_loss:2.137, val_acc:0.502]
Epoch [5/120    avg_loss:1.984, val_acc:0.556]
Epoch [6/120    avg_loss:1.842, val_acc:0.575]
Epoch [7/120    avg_loss:1.763, val_acc:0.571]
Epoch [8/120    avg_loss:1.633, val_acc:0.607]
Epoch [9/120    avg_loss:1.521, val_acc:0.641]
Epoch [10/120    avg_loss:1.436, val_acc:0.643]
Epoch [11/120    avg_loss:1.315, val_acc:0.690]
Epoch [12/120    avg_loss:1.157, val_acc:0.693]
Epoch [13/120    avg_loss:1.090, val_acc:0.682]
Epoch [14/120    avg_loss:0.960, val_acc:0.729]
Epoch [15/120    avg_loss:0.958, val_acc:0.719]
Epoch [16/120    avg_loss:0.921, val_acc:0.740]
Epoch [17/120    avg_loss:0.926, val_acc:0.732]
Epoch [18/120    avg_loss:0.812, val_acc:0.734]
Epoch [19/120    avg_loss:0.723, val_acc:0.791]
Epoch [20/120    avg_loss:0.644, val_acc:0.807]
Epoch [21/120    avg_loss:0.608, val_acc:0.816]
Epoch [22/120    avg_loss:0.529, val_acc:0.816]
Epoch [23/120    avg_loss:0.538, val_acc:0.819]
Epoch [24/120    avg_loss:0.498, val_acc:0.830]
Epoch [25/120    avg_loss:0.454, val_acc:0.839]
Epoch [26/120    avg_loss:0.432, val_acc:0.855]
Epoch [27/120    avg_loss:0.403, val_acc:0.846]
Epoch [28/120    avg_loss:0.394, val_acc:0.831]
Epoch [29/120    avg_loss:0.389, val_acc:0.857]
Epoch [30/120    avg_loss:0.363, val_acc:0.868]
Epoch [31/120    avg_loss:0.355, val_acc:0.855]
Epoch [32/120    avg_loss:0.276, val_acc:0.884]
Epoch [33/120    avg_loss:0.278, val_acc:0.879]
Epoch [34/120    avg_loss:0.281, val_acc:0.882]
Epoch [35/120    avg_loss:0.254, val_acc:0.898]
Epoch [36/120    avg_loss:0.259, val_acc:0.887]
Epoch [37/120    avg_loss:0.276, val_acc:0.896]
Epoch [38/120    avg_loss:0.265, val_acc:0.911]
Epoch [39/120    avg_loss:0.212, val_acc:0.908]
Epoch [40/120    avg_loss:0.194, val_acc:0.894]
Epoch [41/120    avg_loss:0.229, val_acc:0.898]
Epoch [42/120    avg_loss:0.180, val_acc:0.914]
Epoch [43/120    avg_loss:0.178, val_acc:0.933]
Epoch [44/120    avg_loss:0.179, val_acc:0.925]
Epoch [45/120    avg_loss:0.242, val_acc:0.864]
Epoch [46/120    avg_loss:0.210, val_acc:0.916]
Epoch [47/120    avg_loss:0.154, val_acc:0.930]
Epoch [48/120    avg_loss:0.134, val_acc:0.930]
Epoch [49/120    avg_loss:0.130, val_acc:0.922]
Epoch [50/120    avg_loss:0.111, val_acc:0.938]
Epoch [51/120    avg_loss:0.106, val_acc:0.944]
Epoch [52/120    avg_loss:0.111, val_acc:0.940]
Epoch [53/120    avg_loss:0.112, val_acc:0.944]
Epoch [54/120    avg_loss:0.090, val_acc:0.939]
Epoch [55/120    avg_loss:0.089, val_acc:0.935]
Epoch [56/120    avg_loss:0.082, val_acc:0.934]
Epoch [57/120    avg_loss:0.145, val_acc:0.935]
Epoch [58/120    avg_loss:0.099, val_acc:0.948]
Epoch [59/120    avg_loss:0.078, val_acc:0.943]
Epoch [60/120    avg_loss:0.108, val_acc:0.935]
Epoch [61/120    avg_loss:0.111, val_acc:0.941]
Epoch [62/120    avg_loss:0.086, val_acc:0.949]
Epoch [63/120    avg_loss:0.064, val_acc:0.956]
Epoch [64/120    avg_loss:0.070, val_acc:0.949]
Epoch [65/120    avg_loss:0.072, val_acc:0.952]
Epoch [66/120    avg_loss:0.075, val_acc:0.949]
Epoch [67/120    avg_loss:0.064, val_acc:0.953]
Epoch [68/120    avg_loss:0.056, val_acc:0.959]
Epoch [69/120    avg_loss:0.059, val_acc:0.958]
Epoch [70/120    avg_loss:0.063, val_acc:0.953]
Epoch [71/120    avg_loss:0.045, val_acc:0.955]
Epoch [72/120    avg_loss:0.053, val_acc:0.962]
Epoch [73/120    avg_loss:0.048, val_acc:0.956]
Epoch [74/120    avg_loss:0.051, val_acc:0.954]
Epoch [75/120    avg_loss:0.050, val_acc:0.955]
Epoch [76/120    avg_loss:0.052, val_acc:0.960]
Epoch [77/120    avg_loss:0.052, val_acc:0.962]
Epoch [78/120    avg_loss:0.045, val_acc:0.962]
Epoch [79/120    avg_loss:0.047, val_acc:0.962]
Epoch [80/120    avg_loss:0.041, val_acc:0.961]
Epoch [81/120    avg_loss:0.041, val_acc:0.968]
Epoch [82/120    avg_loss:0.043, val_acc:0.961]
Epoch [83/120    avg_loss:0.045, val_acc:0.963]
Epoch [84/120    avg_loss:0.047, val_acc:0.963]
Epoch [85/120    avg_loss:0.031, val_acc:0.967]
Epoch [86/120    avg_loss:0.033, val_acc:0.965]
Epoch [87/120    avg_loss:0.047, val_acc:0.960]
Epoch [88/120    avg_loss:0.049, val_acc:0.967]
Epoch [89/120    avg_loss:0.048, val_acc:0.954]
Epoch [90/120    avg_loss:0.058, val_acc:0.955]
Epoch [91/120    avg_loss:0.045, val_acc:0.956]
Epoch [92/120    avg_loss:0.040, val_acc:0.972]
Epoch [93/120    avg_loss:0.036, val_acc:0.964]
Epoch [94/120    avg_loss:0.048, val_acc:0.965]
Epoch [95/120    avg_loss:0.040, val_acc:0.965]
Epoch [96/120    avg_loss:0.034, val_acc:0.964]
Epoch [97/120    avg_loss:0.042, val_acc:0.958]
Epoch [98/120    avg_loss:0.037, val_acc:0.964]
Epoch [99/120    avg_loss:0.028, val_acc:0.953]
Epoch [100/120    avg_loss:0.035, val_acc:0.970]
Epoch [101/120    avg_loss:0.026, val_acc:0.973]
Epoch [102/120    avg_loss:0.024, val_acc:0.971]
Epoch [103/120    avg_loss:0.024, val_acc:0.972]
Epoch [104/120    avg_loss:0.025, val_acc:0.971]
Epoch [105/120    avg_loss:0.030, val_acc:0.974]
Epoch [106/120    avg_loss:0.029, val_acc:0.967]
Epoch [107/120    avg_loss:0.024, val_acc:0.969]
Epoch [108/120    avg_loss:0.023, val_acc:0.974]
Epoch [109/120    avg_loss:0.038, val_acc:0.960]
Epoch [110/120    avg_loss:0.066, val_acc:0.948]
Epoch [111/120    avg_loss:0.073, val_acc:0.959]
Epoch [112/120    avg_loss:0.071, val_acc:0.954]
Epoch [113/120    avg_loss:0.111, val_acc:0.907]
Epoch [114/120    avg_loss:0.136, val_acc:0.943]
Epoch [115/120    avg_loss:0.066, val_acc:0.956]
Epoch [116/120    avg_loss:0.043, val_acc:0.968]
Epoch [117/120    avg_loss:0.041, val_acc:0.968]
Epoch [118/120    avg_loss:0.045, val_acc:0.960]
Epoch [119/120    avg_loss:0.048, val_acc:0.960]
Epoch [120/120    avg_loss:0.038, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1243    0    0    0    9    0    0    1    7   22    3    0
     0    0    0]
 [   0    0    3  657   13   22    0    0    0   10    0    0   40    2
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  431    0    4    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    2    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    9   73    0    7    0    0    0    0  772    1    0    0
     3   10    0]
 [   0    0   23    0    0    4   22    1    4    0   26 2118    7    5
     0    0    0]
 [   0    0    0   16    6   10    0    0    0    0    8    0  494    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    1    0    1    0    0    0
  1129    4    0]
 [   0    0    0    0    0    0   29    0    0    4    0    0    0    0
    56  258    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    7    0
     0    0   77]]

Accuracy:
94.66666666666667

F1 scores:
[       nan 0.96202532 0.96995708 0.87775551 0.95495495 0.9441402
 0.95098756 0.90909091 0.99421965 0.59574468 0.91252955 0.97312198
 0.90976059 0.98143236 0.96826758 0.83360258 0.95652174]

Kappa:
0.9392765606346695
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7ff607f908>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.741, val_acc:0.317]
Epoch [2/120    avg_loss:2.475, val_acc:0.394]
Epoch [3/120    avg_loss:2.304, val_acc:0.493]
Epoch [4/120    avg_loss:2.180, val_acc:0.549]
Epoch [5/120    avg_loss:2.034, val_acc:0.576]
Epoch [6/120    avg_loss:1.913, val_acc:0.623]
Epoch [7/120    avg_loss:1.806, val_acc:0.637]
Epoch [8/120    avg_loss:1.714, val_acc:0.661]
Epoch [9/120    avg_loss:1.576, val_acc:0.692]
Epoch [10/120    avg_loss:1.458, val_acc:0.710]
Epoch [11/120    avg_loss:1.311, val_acc:0.723]
Epoch [12/120    avg_loss:1.184, val_acc:0.728]
Epoch [13/120    avg_loss:1.084, val_acc:0.715]
Epoch [14/120    avg_loss:1.052, val_acc:0.762]
Epoch [15/120    avg_loss:0.951, val_acc:0.763]
Epoch [16/120    avg_loss:0.815, val_acc:0.758]
Epoch [17/120    avg_loss:0.717, val_acc:0.790]
Epoch [18/120    avg_loss:0.750, val_acc:0.752]
Epoch [19/120    avg_loss:0.683, val_acc:0.804]
Epoch [20/120    avg_loss:0.594, val_acc:0.810]
Epoch [21/120    avg_loss:0.510, val_acc:0.836]
Epoch [22/120    avg_loss:0.447, val_acc:0.823]
Epoch [23/120    avg_loss:0.483, val_acc:0.802]
Epoch [24/120    avg_loss:0.403, val_acc:0.833]
Epoch [25/120    avg_loss:0.394, val_acc:0.872]
Epoch [26/120    avg_loss:0.337, val_acc:0.860]
Epoch [27/120    avg_loss:0.354, val_acc:0.856]
Epoch [28/120    avg_loss:0.312, val_acc:0.885]
Epoch [29/120    avg_loss:0.269, val_acc:0.881]
Epoch [30/120    avg_loss:0.281, val_acc:0.901]
Epoch [31/120    avg_loss:0.291, val_acc:0.895]
Epoch [32/120    avg_loss:0.323, val_acc:0.895]
Epoch [33/120    avg_loss:0.249, val_acc:0.891]
Epoch [34/120    avg_loss:0.210, val_acc:0.903]
Epoch [35/120    avg_loss:0.218, val_acc:0.916]
Epoch [36/120    avg_loss:0.266, val_acc:0.894]
Epoch [37/120    avg_loss:0.210, val_acc:0.915]
Epoch [38/120    avg_loss:0.186, val_acc:0.913]
Epoch [39/120    avg_loss:0.183, val_acc:0.921]
Epoch [40/120    avg_loss:0.208, val_acc:0.860]
Epoch [41/120    avg_loss:0.239, val_acc:0.910]
Epoch [42/120    avg_loss:0.176, val_acc:0.901]
Epoch [43/120    avg_loss:0.186, val_acc:0.913]
Epoch [44/120    avg_loss:0.152, val_acc:0.925]
Epoch [45/120    avg_loss:0.136, val_acc:0.939]
Epoch [46/120    avg_loss:0.109, val_acc:0.927]
Epoch [47/120    avg_loss:0.153, val_acc:0.926]
Epoch [48/120    avg_loss:0.128, val_acc:0.931]
Epoch [49/120    avg_loss:0.106, val_acc:0.933]
Epoch [50/120    avg_loss:0.135, val_acc:0.912]
Epoch [51/120    avg_loss:0.132, val_acc:0.920]
Epoch [52/120    avg_loss:0.133, val_acc:0.925]
Epoch [53/120    avg_loss:0.103, val_acc:0.935]
Epoch [54/120    avg_loss:0.135, val_acc:0.936]
Epoch [55/120    avg_loss:0.114, val_acc:0.942]
Epoch [56/120    avg_loss:0.096, val_acc:0.924]
Epoch [57/120    avg_loss:0.134, val_acc:0.897]
Epoch [58/120    avg_loss:0.207, val_acc:0.923]
Epoch [59/120    avg_loss:0.107, val_acc:0.905]
Epoch [60/120    avg_loss:0.124, val_acc:0.915]
Epoch [61/120    avg_loss:0.094, val_acc:0.932]
Epoch [62/120    avg_loss:0.076, val_acc:0.943]
Epoch [63/120    avg_loss:0.063, val_acc:0.932]
Epoch [64/120    avg_loss:0.064, val_acc:0.944]
Epoch [65/120    avg_loss:0.047, val_acc:0.946]
Epoch [66/120    avg_loss:0.054, val_acc:0.948]
Epoch [67/120    avg_loss:0.042, val_acc:0.950]
Epoch [68/120    avg_loss:0.067, val_acc:0.952]
Epoch [69/120    avg_loss:0.062, val_acc:0.956]
Epoch [70/120    avg_loss:0.072, val_acc:0.935]
Epoch [71/120    avg_loss:0.077, val_acc:0.956]
Epoch [72/120    avg_loss:0.054, val_acc:0.946]
Epoch [73/120    avg_loss:0.042, val_acc:0.953]
Epoch [74/120    avg_loss:0.043, val_acc:0.962]
Epoch [75/120    avg_loss:0.051, val_acc:0.963]
Epoch [76/120    avg_loss:0.037, val_acc:0.956]
Epoch [77/120    avg_loss:0.037, val_acc:0.949]
Epoch [78/120    avg_loss:0.040, val_acc:0.951]
Epoch [79/120    avg_loss:0.050, val_acc:0.967]
Epoch [80/120    avg_loss:0.048, val_acc:0.952]
Epoch [81/120    avg_loss:0.076, val_acc:0.942]
Epoch [82/120    avg_loss:0.045, val_acc:0.951]
Epoch [83/120    avg_loss:0.037, val_acc:0.955]
Epoch [84/120    avg_loss:0.034, val_acc:0.949]
Epoch [85/120    avg_loss:0.031, val_acc:0.960]
Epoch [86/120    avg_loss:0.029, val_acc:0.960]
Epoch [87/120    avg_loss:0.026, val_acc:0.963]
Epoch [88/120    avg_loss:0.038, val_acc:0.963]
Epoch [89/120    avg_loss:0.026, val_acc:0.956]
Epoch [90/120    avg_loss:0.048, val_acc:0.953]
Epoch [91/120    avg_loss:0.083, val_acc:0.952]
Epoch [92/120    avg_loss:0.106, val_acc:0.954]
Epoch [93/120    avg_loss:0.072, val_acc:0.956]
Epoch [94/120    avg_loss:0.059, val_acc:0.962]
Epoch [95/120    avg_loss:0.040, val_acc:0.969]
Epoch [96/120    avg_loss:0.034, val_acc:0.969]
Epoch [97/120    avg_loss:0.035, val_acc:0.967]
Epoch [98/120    avg_loss:0.037, val_acc:0.961]
Epoch [99/120    avg_loss:0.036, val_acc:0.960]
Epoch [100/120    avg_loss:0.030, val_acc:0.963]
Epoch [101/120    avg_loss:0.030, val_acc:0.963]
Epoch [102/120    avg_loss:0.025, val_acc:0.964]
Epoch [103/120    avg_loss:0.028, val_acc:0.965]
Epoch [104/120    avg_loss:0.025, val_acc:0.964]
Epoch [105/120    avg_loss:0.025, val_acc:0.967]
Epoch [106/120    avg_loss:0.032, val_acc:0.968]
Epoch [107/120    avg_loss:0.028, val_acc:0.964]
Epoch [108/120    avg_loss:0.018, val_acc:0.967]
Epoch [109/120    avg_loss:0.023, val_acc:0.968]
Epoch [110/120    avg_loss:0.026, val_acc:0.968]
Epoch [111/120    avg_loss:0.022, val_acc:0.968]
Epoch [112/120    avg_loss:0.024, val_acc:0.969]
Epoch [113/120    avg_loss:0.025, val_acc:0.969]
Epoch [114/120    avg_loss:0.022, val_acc:0.969]
Epoch [115/120    avg_loss:0.021, val_acc:0.969]
Epoch [116/120    avg_loss:0.019, val_acc:0.969]
Epoch [117/120    avg_loss:0.026, val_acc:0.970]
Epoch [118/120    avg_loss:0.020, val_acc:0.970]
Epoch [119/120    avg_loss:0.018, val_acc:0.970]
Epoch [120/120    avg_loss:0.019, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    2    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1236    0    1    1    4    0    0    2    8   32    1    0
     0    0    0]
 [   0    0    0  709    2    9    0    0    0   15    0    0    9    2
     1    0    0]
 [   0    0    0    0  212    0    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    1    0    1    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4   52    0    6    0    0    0    0  793   11    0    0
     0    9    0]
 [   0    0   10    0    0    0    8    0    0    0   25 2163    0    4
     0    0    0]
 [   0    0    0    8   16    8    0    0    0    8    7    5  466    0
     0    4   12]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0   20    0    0   11    0    0    0    0
    44  272    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.85907859078591

F1 scores:
[       nan 0.94871795 0.97437919 0.9353562  0.95495495 0.96388262
 0.97546468 0.98039216 0.995338   0.48648649 0.92586106 0.97829037
 0.9182266  0.98404255 0.97634409 0.86075949 0.92134831]

Kappa:
0.9528007468517189
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f94f3935860>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.739, val_acc:0.258]
Epoch [2/120    avg_loss:2.452, val_acc:0.408]
Epoch [3/120    avg_loss:2.257, val_acc:0.490]
Epoch [4/120    avg_loss:2.121, val_acc:0.529]
Epoch [5/120    avg_loss:1.996, val_acc:0.595]
Epoch [6/120    avg_loss:1.892, val_acc:0.597]
Epoch [7/120    avg_loss:1.807, val_acc:0.615]
Epoch [8/120    avg_loss:1.731, val_acc:0.617]
Epoch [9/120    avg_loss:1.583, val_acc:0.641]
Epoch [10/120    avg_loss:1.467, val_acc:0.665]
Epoch [11/120    avg_loss:1.287, val_acc:0.685]
Epoch [12/120    avg_loss:1.204, val_acc:0.699]
Epoch [13/120    avg_loss:1.101, val_acc:0.721]
Epoch [14/120    avg_loss:0.989, val_acc:0.698]
Epoch [15/120    avg_loss:0.874, val_acc:0.714]
Epoch [16/120    avg_loss:0.847, val_acc:0.720]
Epoch [17/120    avg_loss:0.845, val_acc:0.759]
Epoch [18/120    avg_loss:0.775, val_acc:0.771]
Epoch [19/120    avg_loss:0.673, val_acc:0.773]
Epoch [20/120    avg_loss:0.644, val_acc:0.781]
Epoch [21/120    avg_loss:0.569, val_acc:0.806]
Epoch [22/120    avg_loss:0.599, val_acc:0.801]
Epoch [23/120    avg_loss:0.528, val_acc:0.827]
Epoch [24/120    avg_loss:0.511, val_acc:0.835]
Epoch [25/120    avg_loss:0.499, val_acc:0.815]
Epoch [26/120    avg_loss:0.447, val_acc:0.840]
Epoch [27/120    avg_loss:0.364, val_acc:0.843]
Epoch [28/120    avg_loss:0.364, val_acc:0.862]
Epoch [29/120    avg_loss:0.319, val_acc:0.853]
Epoch [30/120    avg_loss:0.309, val_acc:0.848]
Epoch [31/120    avg_loss:0.343, val_acc:0.872]
Epoch [32/120    avg_loss:0.294, val_acc:0.885]
Epoch [33/120    avg_loss:0.260, val_acc:0.871]
Epoch [34/120    avg_loss:0.241, val_acc:0.879]
Epoch [35/120    avg_loss:0.266, val_acc:0.864]
Epoch [36/120    avg_loss:0.223, val_acc:0.900]
Epoch [37/120    avg_loss:0.219, val_acc:0.890]
Epoch [38/120    avg_loss:0.241, val_acc:0.885]
Epoch [39/120    avg_loss:0.226, val_acc:0.903]
Epoch [40/120    avg_loss:0.226, val_acc:0.902]
Epoch [41/120    avg_loss:0.201, val_acc:0.868]
Epoch [42/120    avg_loss:0.204, val_acc:0.888]
Epoch [43/120    avg_loss:0.199, val_acc:0.901]
Epoch [44/120    avg_loss:0.193, val_acc:0.894]
Epoch [45/120    avg_loss:0.163, val_acc:0.915]
Epoch [46/120    avg_loss:0.157, val_acc:0.919]
Epoch [47/120    avg_loss:0.153, val_acc:0.912]
Epoch [48/120    avg_loss:0.144, val_acc:0.923]
Epoch [49/120    avg_loss:0.148, val_acc:0.903]
Epoch [50/120    avg_loss:0.165, val_acc:0.924]
Epoch [51/120    avg_loss:0.105, val_acc:0.920]
Epoch [52/120    avg_loss:0.093, val_acc:0.943]
Epoch [53/120    avg_loss:0.112, val_acc:0.932]
Epoch [54/120    avg_loss:0.096, val_acc:0.932]
Epoch [55/120    avg_loss:0.117, val_acc:0.932]
Epoch [56/120    avg_loss:0.111, val_acc:0.911]
Epoch [57/120    avg_loss:0.128, val_acc:0.912]
Epoch [58/120    avg_loss:0.098, val_acc:0.927]
Epoch [59/120    avg_loss:0.086, val_acc:0.929]
Epoch [60/120    avg_loss:0.096, val_acc:0.923]
Epoch [61/120    avg_loss:0.185, val_acc:0.902]
Epoch [62/120    avg_loss:0.161, val_acc:0.920]
Epoch [63/120    avg_loss:0.171, val_acc:0.922]
Epoch [64/120    avg_loss:0.104, val_acc:0.942]
Epoch [65/120    avg_loss:0.100, val_acc:0.913]
Epoch [66/120    avg_loss:0.090, val_acc:0.939]
Epoch [67/120    avg_loss:0.066, val_acc:0.948]
Epoch [68/120    avg_loss:0.056, val_acc:0.952]
Epoch [69/120    avg_loss:0.072, val_acc:0.953]
Epoch [70/120    avg_loss:0.057, val_acc:0.952]
Epoch [71/120    avg_loss:0.056, val_acc:0.954]
Epoch [72/120    avg_loss:0.055, val_acc:0.954]
Epoch [73/120    avg_loss:0.056, val_acc:0.956]
Epoch [74/120    avg_loss:0.058, val_acc:0.955]
Epoch [75/120    avg_loss:0.056, val_acc:0.953]
Epoch [76/120    avg_loss:0.057, val_acc:0.954]
Epoch [77/120    avg_loss:0.047, val_acc:0.954]
Epoch [78/120    avg_loss:0.051, val_acc:0.954]
Epoch [79/120    avg_loss:0.054, val_acc:0.955]
Epoch [80/120    avg_loss:0.054, val_acc:0.956]
Epoch [81/120    avg_loss:0.049, val_acc:0.953]
Epoch [82/120    avg_loss:0.050, val_acc:0.955]
Epoch [83/120    avg_loss:0.060, val_acc:0.954]
Epoch [84/120    avg_loss:0.048, val_acc:0.954]
Epoch [85/120    avg_loss:0.053, val_acc:0.956]
Epoch [86/120    avg_loss:0.053, val_acc:0.955]
Epoch [87/120    avg_loss:0.043, val_acc:0.960]
Epoch [88/120    avg_loss:0.051, val_acc:0.955]
Epoch [89/120    avg_loss:0.042, val_acc:0.956]
Epoch [90/120    avg_loss:0.046, val_acc:0.955]
Epoch [91/120    avg_loss:0.048, val_acc:0.960]
Epoch [92/120    avg_loss:0.041, val_acc:0.956]
Epoch [93/120    avg_loss:0.044, val_acc:0.958]
Epoch [94/120    avg_loss:0.043, val_acc:0.959]
Epoch [95/120    avg_loss:0.046, val_acc:0.956]
Epoch [96/120    avg_loss:0.040, val_acc:0.959]
Epoch [97/120    avg_loss:0.042, val_acc:0.956]
Epoch [98/120    avg_loss:0.040, val_acc:0.955]
Epoch [99/120    avg_loss:0.047, val_acc:0.955]
Epoch [100/120    avg_loss:0.052, val_acc:0.960]
Epoch [101/120    avg_loss:0.049, val_acc:0.958]
Epoch [102/120    avg_loss:0.045, val_acc:0.959]
Epoch [103/120    avg_loss:0.045, val_acc:0.959]
Epoch [104/120    avg_loss:0.044, val_acc:0.958]
Epoch [105/120    avg_loss:0.045, val_acc:0.962]
Epoch [106/120    avg_loss:0.038, val_acc:0.962]
Epoch [107/120    avg_loss:0.037, val_acc:0.958]
Epoch [108/120    avg_loss:0.053, val_acc:0.958]
Epoch [109/120    avg_loss:0.043, val_acc:0.960]
Epoch [110/120    avg_loss:0.042, val_acc:0.963]
Epoch [111/120    avg_loss:0.045, val_acc:0.962]
Epoch [112/120    avg_loss:0.040, val_acc:0.960]
Epoch [113/120    avg_loss:0.040, val_acc:0.961]
Epoch [114/120    avg_loss:0.040, val_acc:0.961]
Epoch [115/120    avg_loss:0.043, val_acc:0.959]
Epoch [116/120    avg_loss:0.040, val_acc:0.962]
Epoch [117/120    avg_loss:0.037, val_acc:0.962]
Epoch [118/120    avg_loss:0.036, val_acc:0.963]
Epoch [119/120    avg_loss:0.041, val_acc:0.963]
Epoch [120/120    avg_loss:0.035, val_acc:0.962]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    1 1216    4    0    0    4    0    0    0    8   43    1    0
     0    8    0]
 [   0    0    1  691    0    8    0    0    0   14    0    2   23    3
     0    5    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  420    0    1    0    5    0    1    0    0
     8    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   31   43    0    5    0    0    0    0  764   16    1    0
     3   12    0]
 [   0    0   15    0    0    0    9    0    3    0   22 2153    4    4
     0    0    0]
 [   0    0    0   36    0   10    0    0    0    0    5    1  478    0
     0    0    4]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    4    0    0    1    0    3    3    0    0
  1128    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    46  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.30623306233062

F1 scores:
[       nan 0.92307692 0.9544741  0.90741957 1.         0.95130238
 0.98867925 0.98039216 0.99537037 0.60377358 0.90844233 0.97178966
 0.91658677 0.9787234  0.9707401  0.89450223 0.96470588]

Kappa:
0.9464824882979074
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fda90724828>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.750, val_acc:0.179]
Epoch [2/120    avg_loss:2.498, val_acc:0.330]
Epoch [3/120    avg_loss:2.337, val_acc:0.513]
Epoch [4/120    avg_loss:2.167, val_acc:0.520]
Epoch [5/120    avg_loss:2.035, val_acc:0.581]
Epoch [6/120    avg_loss:1.899, val_acc:0.586]
Epoch [7/120    avg_loss:1.810, val_acc:0.599]
Epoch [8/120    avg_loss:1.631, val_acc:0.610]
Epoch [9/120    avg_loss:1.461, val_acc:0.637]
Epoch [10/120    avg_loss:1.331, val_acc:0.673]
Epoch [11/120    avg_loss:1.266, val_acc:0.672]
Epoch [12/120    avg_loss:1.211, val_acc:0.691]
Epoch [13/120    avg_loss:1.061, val_acc:0.720]
Epoch [14/120    avg_loss:0.952, val_acc:0.720]
Epoch [15/120    avg_loss:0.890, val_acc:0.765]
Epoch [16/120    avg_loss:0.805, val_acc:0.790]
Epoch [17/120    avg_loss:0.772, val_acc:0.762]
Epoch [18/120    avg_loss:0.704, val_acc:0.814]
Epoch [19/120    avg_loss:0.714, val_acc:0.772]
Epoch [20/120    avg_loss:0.670, val_acc:0.815]
Epoch [21/120    avg_loss:0.602, val_acc:0.835]
Epoch [22/120    avg_loss:0.534, val_acc:0.833]
Epoch [23/120    avg_loss:0.526, val_acc:0.834]
Epoch [24/120    avg_loss:0.433, val_acc:0.859]
Epoch [25/120    avg_loss:0.402, val_acc:0.897]
Epoch [26/120    avg_loss:0.354, val_acc:0.885]
Epoch [27/120    avg_loss:0.356, val_acc:0.848]
Epoch [28/120    avg_loss:0.358, val_acc:0.876]
Epoch [29/120    avg_loss:0.313, val_acc:0.890]
Epoch [30/120    avg_loss:0.279, val_acc:0.908]
Epoch [31/120    avg_loss:0.298, val_acc:0.873]
Epoch [32/120    avg_loss:0.302, val_acc:0.904]
Epoch [33/120    avg_loss:0.278, val_acc:0.913]
Epoch [34/120    avg_loss:0.240, val_acc:0.906]
Epoch [35/120    avg_loss:0.233, val_acc:0.885]
Epoch [36/120    avg_loss:0.278, val_acc:0.898]
Epoch [37/120    avg_loss:0.230, val_acc:0.913]
Epoch [38/120    avg_loss:0.187, val_acc:0.910]
Epoch [39/120    avg_loss:0.175, val_acc:0.917]
Epoch [40/120    avg_loss:0.218, val_acc:0.903]
Epoch [41/120    avg_loss:0.196, val_acc:0.930]
Epoch [42/120    avg_loss:0.170, val_acc:0.942]
Epoch [43/120    avg_loss:0.152, val_acc:0.917]
Epoch [44/120    avg_loss:0.158, val_acc:0.933]
Epoch [45/120    avg_loss:0.151, val_acc:0.932]
Epoch [46/120    avg_loss:0.127, val_acc:0.932]
Epoch [47/120    avg_loss:0.125, val_acc:0.927]
Epoch [48/120    avg_loss:0.123, val_acc:0.919]
Epoch [49/120    avg_loss:0.107, val_acc:0.935]
Epoch [50/120    avg_loss:0.106, val_acc:0.940]
Epoch [51/120    avg_loss:0.088, val_acc:0.952]
Epoch [52/120    avg_loss:0.101, val_acc:0.940]
Epoch [53/120    avg_loss:0.083, val_acc:0.950]
Epoch [54/120    avg_loss:0.075, val_acc:0.948]
Epoch [55/120    avg_loss:0.093, val_acc:0.941]
Epoch [56/120    avg_loss:0.091, val_acc:0.944]
Epoch [57/120    avg_loss:0.094, val_acc:0.946]
Epoch [58/120    avg_loss:0.082, val_acc:0.953]
Epoch [59/120    avg_loss:0.097, val_acc:0.940]
Epoch [60/120    avg_loss:0.106, val_acc:0.951]
Epoch [61/120    avg_loss:0.095, val_acc:0.942]
Epoch [62/120    avg_loss:0.097, val_acc:0.958]
Epoch [63/120    avg_loss:0.071, val_acc:0.939]
Epoch [64/120    avg_loss:0.091, val_acc:0.951]
Epoch [65/120    avg_loss:0.062, val_acc:0.954]
Epoch [66/120    avg_loss:0.080, val_acc:0.946]
Epoch [67/120    avg_loss:0.085, val_acc:0.948]
Epoch [68/120    avg_loss:0.067, val_acc:0.955]
Epoch [69/120    avg_loss:0.060, val_acc:0.945]
Epoch [70/120    avg_loss:0.073, val_acc:0.927]
Epoch [71/120    avg_loss:0.072, val_acc:0.950]
Epoch [72/120    avg_loss:0.066, val_acc:0.952]
Epoch [73/120    avg_loss:0.129, val_acc:0.917]
Epoch [74/120    avg_loss:0.209, val_acc:0.923]
Epoch [75/120    avg_loss:0.170, val_acc:0.932]
Epoch [76/120    avg_loss:0.104, val_acc:0.942]
Epoch [77/120    avg_loss:0.086, val_acc:0.950]
Epoch [78/120    avg_loss:0.068, val_acc:0.953]
Epoch [79/120    avg_loss:0.066, val_acc:0.954]
Epoch [80/120    avg_loss:0.059, val_acc:0.955]
Epoch [81/120    avg_loss:0.064, val_acc:0.958]
Epoch [82/120    avg_loss:0.053, val_acc:0.961]
Epoch [83/120    avg_loss:0.058, val_acc:0.963]
Epoch [84/120    avg_loss:0.053, val_acc:0.962]
Epoch [85/120    avg_loss:0.046, val_acc:0.962]
Epoch [86/120    avg_loss:0.042, val_acc:0.962]
Epoch [87/120    avg_loss:0.044, val_acc:0.964]
Epoch [88/120    avg_loss:0.043, val_acc:0.964]
Epoch [89/120    avg_loss:0.048, val_acc:0.962]
Epoch [90/120    avg_loss:0.054, val_acc:0.961]
Epoch [91/120    avg_loss:0.046, val_acc:0.962]
Epoch [92/120    avg_loss:0.040, val_acc:0.963]
Epoch [93/120    avg_loss:0.043, val_acc:0.962]
Epoch [94/120    avg_loss:0.045, val_acc:0.963]
Epoch [95/120    avg_loss:0.040, val_acc:0.967]
Epoch [96/120    avg_loss:0.037, val_acc:0.963]
Epoch [97/120    avg_loss:0.037, val_acc:0.963]
Epoch [98/120    avg_loss:0.044, val_acc:0.964]
Epoch [99/120    avg_loss:0.035, val_acc:0.964]
Epoch [100/120    avg_loss:0.039, val_acc:0.967]
Epoch [101/120    avg_loss:0.033, val_acc:0.964]
Epoch [102/120    avg_loss:0.035, val_acc:0.969]
Epoch [103/120    avg_loss:0.039, val_acc:0.968]
Epoch [104/120    avg_loss:0.036, val_acc:0.965]
Epoch [105/120    avg_loss:0.033, val_acc:0.967]
Epoch [106/120    avg_loss:0.037, val_acc:0.967]
Epoch [107/120    avg_loss:0.036, val_acc:0.965]
Epoch [108/120    avg_loss:0.042, val_acc:0.967]
Epoch [109/120    avg_loss:0.031, val_acc:0.968]
Epoch [110/120    avg_loss:0.038, val_acc:0.967]
Epoch [111/120    avg_loss:0.037, val_acc:0.968]
Epoch [112/120    avg_loss:0.033, val_acc:0.965]
Epoch [113/120    avg_loss:0.032, val_acc:0.969]
Epoch [114/120    avg_loss:0.031, val_acc:0.970]
Epoch [115/120    avg_loss:0.035, val_acc:0.969]
Epoch [116/120    avg_loss:0.036, val_acc:0.969]
Epoch [117/120    avg_loss:0.027, val_acc:0.970]
Epoch [118/120    avg_loss:0.033, val_acc:0.971]
Epoch [119/120    avg_loss:0.029, val_acc:0.969]
Epoch [120/120    avg_loss:0.033, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1223    4    1    1    3    0    0    0    8   44    0    0
     0    1    0]
 [   0    0    7  687    1   20    0    0    0    6    1    0   20    5
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  424    0    0    0    6    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   53   35    0    8    0    0    0    0  756   18    0    0
     2    3    0]
 [   0    0    8    0    0    0    9    0    2    0    4 2176    5    5
     1    0    0]
 [   0    0    0   22    5    6    0    0    0    0   12    8  477    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    3    0    3    3    0    0
  1123    0    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    38  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.5230352303523

F1 scores:
[       nan 0.96202532 0.94953416 0.91906355 0.98383372 0.94818082
 0.98198198 1.         0.98719441 0.79069767 0.90974729 0.97534738
 0.91466922 0.97368421 0.97398092 0.92165899 0.97674419]

Kappa:
0.9489181132789818
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f41d3a0a860>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.752, val_acc:0.366]
Epoch [2/120    avg_loss:2.466, val_acc:0.443]
Epoch [3/120    avg_loss:2.298, val_acc:0.443]
Epoch [4/120    avg_loss:2.145, val_acc:0.467]
Epoch [5/120    avg_loss:2.053, val_acc:0.547]
Epoch [6/120    avg_loss:1.961, val_acc:0.587]
Epoch [7/120    avg_loss:1.850, val_acc:0.594]
Epoch [8/120    avg_loss:1.832, val_acc:0.631]
Epoch [9/120    avg_loss:1.719, val_acc:0.633]
Epoch [10/120    avg_loss:1.586, val_acc:0.667]
Epoch [11/120    avg_loss:1.485, val_acc:0.675]
Epoch [12/120    avg_loss:1.358, val_acc:0.682]
Epoch [13/120    avg_loss:1.245, val_acc:0.700]
Epoch [14/120    avg_loss:1.147, val_acc:0.695]
Epoch [15/120    avg_loss:1.095, val_acc:0.709]
Epoch [16/120    avg_loss:1.011, val_acc:0.731]
Epoch [17/120    avg_loss:0.840, val_acc:0.763]
Epoch [18/120    avg_loss:0.809, val_acc:0.746]
Epoch [19/120    avg_loss:0.709, val_acc:0.810]
Epoch [20/120    avg_loss:0.664, val_acc:0.798]
Epoch [21/120    avg_loss:0.596, val_acc:0.790]
Epoch [22/120    avg_loss:0.530, val_acc:0.833]
Epoch [23/120    avg_loss:0.551, val_acc:0.821]
Epoch [24/120    avg_loss:0.527, val_acc:0.835]
Epoch [25/120    avg_loss:0.449, val_acc:0.840]
Epoch [26/120    avg_loss:0.456, val_acc:0.845]
Epoch [27/120    avg_loss:0.420, val_acc:0.805]
Epoch [28/120    avg_loss:0.456, val_acc:0.868]
Epoch [29/120    avg_loss:0.368, val_acc:0.866]
Epoch [30/120    avg_loss:0.334, val_acc:0.878]
Epoch [31/120    avg_loss:0.337, val_acc:0.886]
Epoch [32/120    avg_loss:0.304, val_acc:0.848]
Epoch [33/120    avg_loss:0.305, val_acc:0.863]
Epoch [34/120    avg_loss:0.354, val_acc:0.887]
Epoch [35/120    avg_loss:0.288, val_acc:0.874]
Epoch [36/120    avg_loss:0.265, val_acc:0.871]
Epoch [37/120    avg_loss:0.267, val_acc:0.903]
Epoch [38/120    avg_loss:0.257, val_acc:0.900]
Epoch [39/120    avg_loss:0.230, val_acc:0.910]
Epoch [40/120    avg_loss:0.244, val_acc:0.915]
Epoch [41/120    avg_loss:0.229, val_acc:0.916]
Epoch [42/120    avg_loss:0.219, val_acc:0.885]
Epoch [43/120    avg_loss:0.222, val_acc:0.902]
Epoch [44/120    avg_loss:0.171, val_acc:0.919]
Epoch [45/120    avg_loss:0.197, val_acc:0.922]
Epoch [46/120    avg_loss:0.175, val_acc:0.934]
Epoch [47/120    avg_loss:0.172, val_acc:0.913]
Epoch [48/120    avg_loss:0.157, val_acc:0.919]
Epoch [49/120    avg_loss:0.172, val_acc:0.934]
Epoch [50/120    avg_loss:0.168, val_acc:0.930]
Epoch [51/120    avg_loss:0.164, val_acc:0.925]
Epoch [52/120    avg_loss:0.177, val_acc:0.914]
Epoch [53/120    avg_loss:0.151, val_acc:0.913]
Epoch [54/120    avg_loss:0.127, val_acc:0.935]
Epoch [55/120    avg_loss:0.132, val_acc:0.921]
Epoch [56/120    avg_loss:0.146, val_acc:0.905]
Epoch [57/120    avg_loss:0.180, val_acc:0.914]
Epoch [58/120    avg_loss:0.136, val_acc:0.935]
Epoch [59/120    avg_loss:0.111, val_acc:0.939]
Epoch [60/120    avg_loss:0.117, val_acc:0.933]
Epoch [61/120    avg_loss:0.114, val_acc:0.934]
Epoch [62/120    avg_loss:0.111, val_acc:0.919]
Epoch [63/120    avg_loss:0.123, val_acc:0.940]
Epoch [64/120    avg_loss:0.116, val_acc:0.932]
Epoch [65/120    avg_loss:0.116, val_acc:0.933]
Epoch [66/120    avg_loss:0.102, val_acc:0.920]
Epoch [67/120    avg_loss:0.125, val_acc:0.932]
Epoch [68/120    avg_loss:0.087, val_acc:0.946]
Epoch [69/120    avg_loss:0.076, val_acc:0.940]
Epoch [70/120    avg_loss:0.075, val_acc:0.931]
Epoch [71/120    avg_loss:0.088, val_acc:0.943]
Epoch [72/120    avg_loss:0.115, val_acc:0.935]
Epoch [73/120    avg_loss:0.073, val_acc:0.952]
Epoch [74/120    avg_loss:0.073, val_acc:0.945]
Epoch [75/120    avg_loss:0.078, val_acc:0.944]
Epoch [76/120    avg_loss:0.062, val_acc:0.952]
Epoch [77/120    avg_loss:0.080, val_acc:0.955]
Epoch [78/120    avg_loss:0.077, val_acc:0.943]
Epoch [79/120    avg_loss:0.073, val_acc:0.949]
Epoch [80/120    avg_loss:0.056, val_acc:0.953]
Epoch [81/120    avg_loss:0.051, val_acc:0.952]
Epoch [82/120    avg_loss:0.057, val_acc:0.953]
Epoch [83/120    avg_loss:0.054, val_acc:0.951]
Epoch [84/120    avg_loss:0.043, val_acc:0.955]
Epoch [85/120    avg_loss:0.047, val_acc:0.948]
Epoch [86/120    avg_loss:0.060, val_acc:0.959]
Epoch [87/120    avg_loss:0.048, val_acc:0.941]
Epoch [88/120    avg_loss:0.044, val_acc:0.954]
Epoch [89/120    avg_loss:0.046, val_acc:0.959]
Epoch [90/120    avg_loss:0.040, val_acc:0.967]
Epoch [91/120    avg_loss:0.036, val_acc:0.967]
Epoch [92/120    avg_loss:0.041, val_acc:0.962]
Epoch [93/120    avg_loss:0.044, val_acc:0.955]
Epoch [94/120    avg_loss:0.049, val_acc:0.955]
Epoch [95/120    avg_loss:0.038, val_acc:0.962]
Epoch [96/120    avg_loss:0.039, val_acc:0.955]
Epoch [97/120    avg_loss:0.050, val_acc:0.956]
Epoch [98/120    avg_loss:0.071, val_acc:0.877]
Epoch [99/120    avg_loss:0.205, val_acc:0.939]
Epoch [100/120    avg_loss:0.190, val_acc:0.914]
Epoch [101/120    avg_loss:0.179, val_acc:0.924]
Epoch [102/120    avg_loss:0.107, val_acc:0.939]
Epoch [103/120    avg_loss:0.081, val_acc:0.939]
Epoch [104/120    avg_loss:0.071, val_acc:0.959]
Epoch [105/120    avg_loss:0.052, val_acc:0.961]
Epoch [106/120    avg_loss:0.042, val_acc:0.960]
Epoch [107/120    avg_loss:0.039, val_acc:0.960]
Epoch [108/120    avg_loss:0.036, val_acc:0.961]
Epoch [109/120    avg_loss:0.035, val_acc:0.961]
Epoch [110/120    avg_loss:0.039, val_acc:0.958]
Epoch [111/120    avg_loss:0.036, val_acc:0.956]
Epoch [112/120    avg_loss:0.035, val_acc:0.959]
Epoch [113/120    avg_loss:0.044, val_acc:0.959]
Epoch [114/120    avg_loss:0.041, val_acc:0.958]
Epoch [115/120    avg_loss:0.030, val_acc:0.961]
Epoch [116/120    avg_loss:0.034, val_acc:0.960]
Epoch [117/120    avg_loss:0.041, val_acc:0.961]
Epoch [118/120    avg_loss:0.032, val_acc:0.961]
Epoch [119/120    avg_loss:0.031, val_acc:0.962]
Epoch [120/120    avg_loss:0.032, val_acc:0.962]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    4 1181    7    1    0    3    0    0    1   10   55   14    0
     0    9    0]
 [   0    0    3  698    1   17    0    0    0   17    0    0   11    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  421    0    8    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    5    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    6    0    0   11    0    0    1    0
     0    0    0]
 [   0    0   10   90    0    4    0    0    0    0  762    6    0    0
     0    3    0]
 [   0    0    7    0    0    0    7    0    1    0    2 2180    7    4
     2    0    0]
 [   0    0    0    2    3    3    0    0    0    0   18    5  499    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   21    0    0    1    0    1    2    0    0
  1114    0    0]
 [   0    0    0    0    0    0   34    0    0    6    0    0    0    0
    83  224    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
94.44986449864498

F1 scores:
[       nan 0.94117647 0.95012068 0.90414508 0.98839907 0.9345172
 0.95799558 0.86206897 0.99767981 0.37288136 0.91312163 0.97692135
 0.93271028 0.98930481 0.95213675 0.76843911 0.95238095]

Kappa:
0.9366963860729154
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6dc8883908>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.752, val_acc:0.253]
Epoch [2/120    avg_loss:2.514, val_acc:0.448]
Epoch [3/120    avg_loss:2.271, val_acc:0.489]
Epoch [4/120    avg_loss:2.159, val_acc:0.463]
Epoch [5/120    avg_loss:2.054, val_acc:0.532]
Epoch [6/120    avg_loss:1.939, val_acc:0.544]
Epoch [7/120    avg_loss:1.844, val_acc:0.575]
Epoch [8/120    avg_loss:1.738, val_acc:0.589]
Epoch [9/120    avg_loss:1.633, val_acc:0.610]
Epoch [10/120    avg_loss:1.544, val_acc:0.615]
Epoch [11/120    avg_loss:1.409, val_acc:0.653]
Epoch [12/120    avg_loss:1.335, val_acc:0.652]
Epoch [13/120    avg_loss:1.205, val_acc:0.664]
Epoch [14/120    avg_loss:1.184, val_acc:0.682]
Epoch [15/120    avg_loss:1.086, val_acc:0.689]
Epoch [16/120    avg_loss:0.951, val_acc:0.710]
Epoch [17/120    avg_loss:0.890, val_acc:0.720]
Epoch [18/120    avg_loss:0.810, val_acc:0.737]
Epoch [19/120    avg_loss:0.801, val_acc:0.741]
Epoch [20/120    avg_loss:0.710, val_acc:0.724]
Epoch [21/120    avg_loss:0.726, val_acc:0.758]
Epoch [22/120    avg_loss:0.674, val_acc:0.777]
Epoch [23/120    avg_loss:0.607, val_acc:0.794]
Epoch [24/120    avg_loss:0.577, val_acc:0.786]
Epoch [25/120    avg_loss:0.574, val_acc:0.754]
Epoch [26/120    avg_loss:0.483, val_acc:0.827]
Epoch [27/120    avg_loss:0.469, val_acc:0.817]
Epoch [28/120    avg_loss:0.461, val_acc:0.816]
Epoch [29/120    avg_loss:0.414, val_acc:0.823]
Epoch [30/120    avg_loss:0.379, val_acc:0.831]
Epoch [31/120    avg_loss:0.357, val_acc:0.865]
Epoch [32/120    avg_loss:0.338, val_acc:0.844]
Epoch [33/120    avg_loss:0.339, val_acc:0.830]
Epoch [34/120    avg_loss:0.396, val_acc:0.844]
Epoch [35/120    avg_loss:0.312, val_acc:0.856]
Epoch [36/120    avg_loss:0.330, val_acc:0.879]
Epoch [37/120    avg_loss:0.298, val_acc:0.867]
Epoch [38/120    avg_loss:0.282, val_acc:0.855]
Epoch [39/120    avg_loss:0.253, val_acc:0.883]
Epoch [40/120    avg_loss:0.232, val_acc:0.890]
Epoch [41/120    avg_loss:0.244, val_acc:0.881]
Epoch [42/120    avg_loss:0.256, val_acc:0.885]
Epoch [43/120    avg_loss:0.203, val_acc:0.888]
Epoch [44/120    avg_loss:0.240, val_acc:0.856]
Epoch [45/120    avg_loss:0.239, val_acc:0.872]
Epoch [46/120    avg_loss:0.273, val_acc:0.863]
Epoch [47/120    avg_loss:0.239, val_acc:0.901]
Epoch [48/120    avg_loss:0.183, val_acc:0.895]
Epoch [49/120    avg_loss:0.177, val_acc:0.891]
Epoch [50/120    avg_loss:0.156, val_acc:0.912]
Epoch [51/120    avg_loss:0.164, val_acc:0.894]
Epoch [52/120    avg_loss:0.183, val_acc:0.905]
Epoch [53/120    avg_loss:0.195, val_acc:0.871]
Epoch [54/120    avg_loss:0.175, val_acc:0.900]
Epoch [55/120    avg_loss:0.133, val_acc:0.916]
Epoch [56/120    avg_loss:0.114, val_acc:0.916]
Epoch [57/120    avg_loss:0.098, val_acc:0.931]
Epoch [58/120    avg_loss:0.133, val_acc:0.905]
Epoch [59/120    avg_loss:0.182, val_acc:0.894]
Epoch [60/120    avg_loss:0.151, val_acc:0.896]
Epoch [61/120    avg_loss:0.121, val_acc:0.914]
Epoch [62/120    avg_loss:0.116, val_acc:0.934]
Epoch [63/120    avg_loss:0.096, val_acc:0.930]
Epoch [64/120    avg_loss:0.089, val_acc:0.926]
Epoch [65/120    avg_loss:0.101, val_acc:0.924]
Epoch [66/120    avg_loss:0.138, val_acc:0.903]
Epoch [67/120    avg_loss:0.104, val_acc:0.925]
Epoch [68/120    avg_loss:0.095, val_acc:0.921]
Epoch [69/120    avg_loss:0.121, val_acc:0.920]
Epoch [70/120    avg_loss:0.100, val_acc:0.935]
Epoch [71/120    avg_loss:0.072, val_acc:0.930]
Epoch [72/120    avg_loss:0.100, val_acc:0.929]
Epoch [73/120    avg_loss:0.079, val_acc:0.913]
Epoch [74/120    avg_loss:0.102, val_acc:0.911]
Epoch [75/120    avg_loss:0.132, val_acc:0.931]
Epoch [76/120    avg_loss:0.098, val_acc:0.932]
Epoch [77/120    avg_loss:0.074, val_acc:0.938]
Epoch [78/120    avg_loss:0.067, val_acc:0.920]
Epoch [79/120    avg_loss:0.072, val_acc:0.919]
Epoch [80/120    avg_loss:0.094, val_acc:0.922]
Epoch [81/120    avg_loss:0.067, val_acc:0.940]
Epoch [82/120    avg_loss:0.062, val_acc:0.940]
Epoch [83/120    avg_loss:0.057, val_acc:0.951]
Epoch [84/120    avg_loss:0.061, val_acc:0.951]
Epoch [85/120    avg_loss:0.053, val_acc:0.946]
Epoch [86/120    avg_loss:0.054, val_acc:0.945]
Epoch [87/120    avg_loss:0.052, val_acc:0.950]
Epoch [88/120    avg_loss:0.046, val_acc:0.951]
Epoch [89/120    avg_loss:0.049, val_acc:0.944]
Epoch [90/120    avg_loss:0.046, val_acc:0.952]
Epoch [91/120    avg_loss:0.048, val_acc:0.958]
Epoch [92/120    avg_loss:0.049, val_acc:0.948]
Epoch [93/120    avg_loss:0.049, val_acc:0.958]
Epoch [94/120    avg_loss:0.050, val_acc:0.940]
Epoch [95/120    avg_loss:0.040, val_acc:0.946]
Epoch [96/120    avg_loss:0.042, val_acc:0.949]
Epoch [97/120    avg_loss:0.037, val_acc:0.943]
Epoch [98/120    avg_loss:0.082, val_acc:0.925]
Epoch [99/120    avg_loss:0.081, val_acc:0.945]
Epoch [100/120    avg_loss:0.061, val_acc:0.942]
Epoch [101/120    avg_loss:0.055, val_acc:0.955]
Epoch [102/120    avg_loss:0.051, val_acc:0.948]
Epoch [103/120    avg_loss:0.064, val_acc:0.930]
Epoch [104/120    avg_loss:0.068, val_acc:0.943]
Epoch [105/120    avg_loss:0.052, val_acc:0.953]
Epoch [106/120    avg_loss:0.053, val_acc:0.956]
Epoch [107/120    avg_loss:0.038, val_acc:0.960]
Epoch [108/120    avg_loss:0.033, val_acc:0.960]
Epoch [109/120    avg_loss:0.033, val_acc:0.955]
Epoch [110/120    avg_loss:0.031, val_acc:0.956]
Epoch [111/120    avg_loss:0.029, val_acc:0.956]
Epoch [112/120    avg_loss:0.031, val_acc:0.956]
Epoch [113/120    avg_loss:0.028, val_acc:0.959]
Epoch [114/120    avg_loss:0.027, val_acc:0.960]
Epoch [115/120    avg_loss:0.026, val_acc:0.959]
Epoch [116/120    avg_loss:0.027, val_acc:0.960]
Epoch [117/120    avg_loss:0.029, val_acc:0.959]
Epoch [118/120    avg_loss:0.029, val_acc:0.959]
Epoch [119/120    avg_loss:0.027, val_acc:0.959]
Epoch [120/120    avg_loss:0.026, val_acc:0.960]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    3    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1222    1    0    0    2    0    0    0    2   45    1    0
     0   12    0]
 [   0    0    1  719    1   12    0    0    0    6    0    0    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    0    0    6    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0   41   90    0    5    0    0    0    0  724    9    0    0
     2    4    0]
 [   0    0   23    0    0    0    8    0    0    0    4 2161    5    2
     7    0    0]
 [   0    0    9   13    1    3    0    0    0    0   10    1  486    0
     0    3    8]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    2    2    1    0
  1132    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
   150  196    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.34146341463415

F1 scores:
[       nan 0.92105263 0.94582043 0.91359593 0.9953271  0.96153846
 0.98939394 1.         1.         0.63636364 0.89382716 0.97518051
 0.93913043 0.99191375 0.93015612 0.6975089  0.95454545]

Kappa:
0.9353871414447252
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f94c2bef828>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.728, val_acc:0.285]
Epoch [2/120    avg_loss:2.439, val_acc:0.449]
Epoch [3/120    avg_loss:2.261, val_acc:0.475]
Epoch [4/120    avg_loss:2.169, val_acc:0.508]
Epoch [5/120    avg_loss:2.013, val_acc:0.542]
Epoch [6/120    avg_loss:1.944, val_acc:0.575]
Epoch [7/120    avg_loss:1.803, val_acc:0.591]
Epoch [8/120    avg_loss:1.733, val_acc:0.589]
Epoch [9/120    avg_loss:1.664, val_acc:0.600]
Epoch [10/120    avg_loss:1.527, val_acc:0.613]
Epoch [11/120    avg_loss:1.426, val_acc:0.657]
Epoch [12/120    avg_loss:1.354, val_acc:0.645]
Epoch [13/120    avg_loss:1.239, val_acc:0.659]
Epoch [14/120    avg_loss:1.144, val_acc:0.696]
Epoch [15/120    avg_loss:1.094, val_acc:0.692]
Epoch [16/120    avg_loss:1.037, val_acc:0.716]
Epoch [17/120    avg_loss:0.924, val_acc:0.716]
Epoch [18/120    avg_loss:0.912, val_acc:0.713]
Epoch [19/120    avg_loss:0.843, val_acc:0.750]
Epoch [20/120    avg_loss:0.762, val_acc:0.780]
Epoch [21/120    avg_loss:0.703, val_acc:0.779]
Epoch [22/120    avg_loss:0.646, val_acc:0.791]
Epoch [23/120    avg_loss:0.653, val_acc:0.809]
Epoch [24/120    avg_loss:0.533, val_acc:0.814]
Epoch [25/120    avg_loss:0.581, val_acc:0.747]
Epoch [26/120    avg_loss:0.544, val_acc:0.816]
Epoch [27/120    avg_loss:0.479, val_acc:0.826]
Epoch [28/120    avg_loss:0.447, val_acc:0.846]
Epoch [29/120    avg_loss:0.493, val_acc:0.809]
Epoch [30/120    avg_loss:0.442, val_acc:0.836]
Epoch [31/120    avg_loss:0.413, val_acc:0.866]
Epoch [32/120    avg_loss:0.342, val_acc:0.852]
Epoch [33/120    avg_loss:0.331, val_acc:0.858]
Epoch [34/120    avg_loss:0.385, val_acc:0.814]
Epoch [35/120    avg_loss:0.378, val_acc:0.854]
Epoch [36/120    avg_loss:0.295, val_acc:0.898]
Epoch [37/120    avg_loss:0.254, val_acc:0.903]
Epoch [38/120    avg_loss:0.250, val_acc:0.888]
Epoch [39/120    avg_loss:0.258, val_acc:0.905]
Epoch [40/120    avg_loss:0.246, val_acc:0.881]
Epoch [41/120    avg_loss:0.234, val_acc:0.869]
Epoch [42/120    avg_loss:0.218, val_acc:0.909]
Epoch [43/120    avg_loss:0.233, val_acc:0.897]
Epoch [44/120    avg_loss:0.225, val_acc:0.912]
Epoch [45/120    avg_loss:0.219, val_acc:0.909]
Epoch [46/120    avg_loss:0.209, val_acc:0.918]
Epoch [47/120    avg_loss:0.182, val_acc:0.936]
Epoch [48/120    avg_loss:0.176, val_acc:0.928]
Epoch [49/120    avg_loss:0.162, val_acc:0.932]
Epoch [50/120    avg_loss:0.151, val_acc:0.924]
Epoch [51/120    avg_loss:0.161, val_acc:0.941]
Epoch [52/120    avg_loss:0.165, val_acc:0.932]
Epoch [53/120    avg_loss:0.206, val_acc:0.906]
Epoch [54/120    avg_loss:0.176, val_acc:0.926]
Epoch [55/120    avg_loss:0.180, val_acc:0.927]
Epoch [56/120    avg_loss:0.156, val_acc:0.933]
Epoch [57/120    avg_loss:0.129, val_acc:0.907]
Epoch [58/120    avg_loss:0.137, val_acc:0.949]
Epoch [59/120    avg_loss:0.115, val_acc:0.936]
Epoch [60/120    avg_loss:0.122, val_acc:0.938]
Epoch [61/120    avg_loss:0.132, val_acc:0.951]
Epoch [62/120    avg_loss:0.098, val_acc:0.948]
Epoch [63/120    avg_loss:0.136, val_acc:0.928]
Epoch [64/120    avg_loss:0.147, val_acc:0.912]
Epoch [65/120    avg_loss:0.140, val_acc:0.936]
Epoch [66/120    avg_loss:0.123, val_acc:0.934]
Epoch [67/120    avg_loss:0.103, val_acc:0.940]
Epoch [68/120    avg_loss:0.090, val_acc:0.949]
Epoch [69/120    avg_loss:0.091, val_acc:0.954]
Epoch [70/120    avg_loss:0.088, val_acc:0.944]
Epoch [71/120    avg_loss:0.104, val_acc:0.947]
Epoch [72/120    avg_loss:0.069, val_acc:0.957]
Epoch [73/120    avg_loss:0.072, val_acc:0.956]
Epoch [74/120    avg_loss:0.079, val_acc:0.956]
Epoch [75/120    avg_loss:0.060, val_acc:0.955]
Epoch [76/120    avg_loss:0.074, val_acc:0.959]
Epoch [77/120    avg_loss:0.070, val_acc:0.966]
Epoch [78/120    avg_loss:0.065, val_acc:0.961]
Epoch [79/120    avg_loss:0.062, val_acc:0.962]
Epoch [80/120    avg_loss:0.050, val_acc:0.961]
Epoch [81/120    avg_loss:0.055, val_acc:0.964]
Epoch [82/120    avg_loss:0.055, val_acc:0.964]
Epoch [83/120    avg_loss:0.053, val_acc:0.957]
Epoch [84/120    avg_loss:0.084, val_acc:0.961]
Epoch [85/120    avg_loss:0.080, val_acc:0.940]
Epoch [86/120    avg_loss:0.085, val_acc:0.956]
Epoch [87/120    avg_loss:0.081, val_acc:0.940]
Epoch [88/120    avg_loss:0.075, val_acc:0.956]
Epoch [89/120    avg_loss:0.135, val_acc:0.931]
Epoch [90/120    avg_loss:0.153, val_acc:0.938]
Epoch [91/120    avg_loss:0.091, val_acc:0.943]
Epoch [92/120    avg_loss:0.068, val_acc:0.954]
Epoch [93/120    avg_loss:0.060, val_acc:0.954]
Epoch [94/120    avg_loss:0.053, val_acc:0.956]
Epoch [95/120    avg_loss:0.059, val_acc:0.958]
Epoch [96/120    avg_loss:0.051, val_acc:0.961]
Epoch [97/120    avg_loss:0.061, val_acc:0.961]
Epoch [98/120    avg_loss:0.054, val_acc:0.959]
Epoch [99/120    avg_loss:0.051, val_acc:0.961]
Epoch [100/120    avg_loss:0.047, val_acc:0.962]
Epoch [101/120    avg_loss:0.045, val_acc:0.961]
Epoch [102/120    avg_loss:0.044, val_acc:0.957]
Epoch [103/120    avg_loss:0.046, val_acc:0.958]
Epoch [104/120    avg_loss:0.044, val_acc:0.958]
Epoch [105/120    avg_loss:0.041, val_acc:0.959]
Epoch [106/120    avg_loss:0.042, val_acc:0.961]
Epoch [107/120    avg_loss:0.049, val_acc:0.957]
Epoch [108/120    avg_loss:0.049, val_acc:0.957]
Epoch [109/120    avg_loss:0.036, val_acc:0.959]
Epoch [110/120    avg_loss:0.043, val_acc:0.957]
Epoch [111/120    avg_loss:0.048, val_acc:0.958]
Epoch [112/120    avg_loss:0.048, val_acc:0.958]
Epoch [113/120    avg_loss:0.048, val_acc:0.958]
Epoch [114/120    avg_loss:0.048, val_acc:0.958]
Epoch [115/120    avg_loss:0.045, val_acc:0.959]
Epoch [116/120    avg_loss:0.043, val_acc:0.959]
Epoch [117/120    avg_loss:0.048, val_acc:0.958]
Epoch [118/120    avg_loss:0.053, val_acc:0.959]
Epoch [119/120    avg_loss:0.047, val_acc:0.958]
Epoch [120/120    avg_loss:0.036, val_acc:0.959]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1184    6    0    0    5    0    0    0   19   71    0    0
     0    0    0]
 [   0    0    0  684    3   26    0    0    0   14    0    0   20    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    5    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   14    0    0    2    0
     0    0    0]
 [   0    0   39   83    0    7    7    0    0    0  717   16    0    0
     2    4    0]
 [   0    0   13    0    0    5   15    0    2    0    2 2141    5    5
    21    1    0]
 [   0    0    0   32    9   13    0    0    0    1    6    0  465    0
     0    0    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    2    0    3    1    1    0
  1131    0    0]
 [   0    0    0    0    0    0   48    0    0    0    0    0    0    0
    99  200    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
93.10569105691057

F1 scores:
[       nan 0.94871795 0.9393098  0.8814433  0.97260274 0.93435449
 0.94160058 0.90909091 0.99537037 0.57142857 0.88191882 0.96376322
 0.90466926 0.98666667 0.945257   0.72463768 0.94857143]

Kappa:
0.9213394709219693
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fad24294898>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.766, val_acc:0.232]
Epoch [2/120    avg_loss:2.526, val_acc:0.401]
Epoch [3/120    avg_loss:2.312, val_acc:0.479]
Epoch [4/120    avg_loss:2.164, val_acc:0.519]
Epoch [5/120    avg_loss:2.065, val_acc:0.546]
Epoch [6/120    avg_loss:1.957, val_acc:0.556]
Epoch [7/120    avg_loss:1.831, val_acc:0.602]
Epoch [8/120    avg_loss:1.699, val_acc:0.561]
Epoch [9/120    avg_loss:1.583, val_acc:0.617]
Epoch [10/120    avg_loss:1.474, val_acc:0.624]
Epoch [11/120    avg_loss:1.323, val_acc:0.662]
Epoch [12/120    avg_loss:1.225, val_acc:0.660]
Epoch [13/120    avg_loss:1.143, val_acc:0.699]
Epoch [14/120    avg_loss:1.077, val_acc:0.720]
Epoch [15/120    avg_loss:0.948, val_acc:0.722]
Epoch [16/120    avg_loss:0.867, val_acc:0.759]
Epoch [17/120    avg_loss:0.872, val_acc:0.739]
Epoch [18/120    avg_loss:0.730, val_acc:0.758]
Epoch [19/120    avg_loss:0.696, val_acc:0.787]
Epoch [20/120    avg_loss:0.649, val_acc:0.776]
Epoch [21/120    avg_loss:0.659, val_acc:0.776]
Epoch [22/120    avg_loss:0.612, val_acc:0.804]
Epoch [23/120    avg_loss:0.545, val_acc:0.814]
Epoch [24/120    avg_loss:0.561, val_acc:0.807]
Epoch [25/120    avg_loss:0.500, val_acc:0.840]
Epoch [26/120    avg_loss:0.451, val_acc:0.827]
Epoch [27/120    avg_loss:0.407, val_acc:0.846]
Epoch [28/120    avg_loss:0.362, val_acc:0.867]
Epoch [29/120    avg_loss:0.374, val_acc:0.859]
Epoch [30/120    avg_loss:0.370, val_acc:0.856]
Epoch [31/120    avg_loss:0.318, val_acc:0.866]
Epoch [32/120    avg_loss:0.356, val_acc:0.864]
Epoch [33/120    avg_loss:0.306, val_acc:0.871]
Epoch [34/120    avg_loss:0.308, val_acc:0.882]
Epoch [35/120    avg_loss:0.278, val_acc:0.887]
Epoch [36/120    avg_loss:0.244, val_acc:0.898]
Epoch [37/120    avg_loss:0.227, val_acc:0.894]
Epoch [38/120    avg_loss:0.257, val_acc:0.890]
Epoch [39/120    avg_loss:0.213, val_acc:0.906]
Epoch [40/120    avg_loss:0.213, val_acc:0.884]
Epoch [41/120    avg_loss:0.233, val_acc:0.872]
Epoch [42/120    avg_loss:0.263, val_acc:0.890]
Epoch [43/120    avg_loss:0.199, val_acc:0.923]
Epoch [44/120    avg_loss:0.169, val_acc:0.897]
Epoch [45/120    avg_loss:0.164, val_acc:0.883]
Epoch [46/120    avg_loss:0.161, val_acc:0.923]
Epoch [47/120    avg_loss:0.170, val_acc:0.906]
Epoch [48/120    avg_loss:0.159, val_acc:0.911]
Epoch [49/120    avg_loss:0.182, val_acc:0.895]
Epoch [50/120    avg_loss:0.171, val_acc:0.924]
Epoch [51/120    avg_loss:0.153, val_acc:0.931]
Epoch [52/120    avg_loss:0.131, val_acc:0.932]
Epoch [53/120    avg_loss:0.149, val_acc:0.914]
Epoch [54/120    avg_loss:0.148, val_acc:0.930]
Epoch [55/120    avg_loss:0.139, val_acc:0.915]
Epoch [56/120    avg_loss:0.202, val_acc:0.915]
Epoch [57/120    avg_loss:0.231, val_acc:0.890]
Epoch [58/120    avg_loss:0.149, val_acc:0.917]
Epoch [59/120    avg_loss:0.155, val_acc:0.920]
Epoch [60/120    avg_loss:0.184, val_acc:0.912]
Epoch [61/120    avg_loss:0.138, val_acc:0.921]
Epoch [62/120    avg_loss:0.105, val_acc:0.921]
Epoch [63/120    avg_loss:0.116, val_acc:0.905]
Epoch [64/120    avg_loss:0.124, val_acc:0.935]
Epoch [65/120    avg_loss:0.113, val_acc:0.949]
Epoch [66/120    avg_loss:0.106, val_acc:0.939]
Epoch [67/120    avg_loss:0.095, val_acc:0.952]
Epoch [68/120    avg_loss:0.070, val_acc:0.955]
Epoch [69/120    avg_loss:0.096, val_acc:0.946]
Epoch [70/120    avg_loss:0.074, val_acc:0.942]
Epoch [71/120    avg_loss:0.065, val_acc:0.944]
Epoch [72/120    avg_loss:0.069, val_acc:0.952]
Epoch [73/120    avg_loss:0.065, val_acc:0.953]
Epoch [74/120    avg_loss:0.068, val_acc:0.959]
Epoch [75/120    avg_loss:0.069, val_acc:0.952]
Epoch [76/120    avg_loss:0.064, val_acc:0.954]
Epoch [77/120    avg_loss:0.072, val_acc:0.944]
Epoch [78/120    avg_loss:0.074, val_acc:0.946]
Epoch [79/120    avg_loss:0.072, val_acc:0.953]
Epoch [80/120    avg_loss:0.071, val_acc:0.943]
Epoch [81/120    avg_loss:0.059, val_acc:0.954]
Epoch [82/120    avg_loss:0.052, val_acc:0.951]
Epoch [83/120    avg_loss:0.055, val_acc:0.949]
Epoch [84/120    avg_loss:0.059, val_acc:0.958]
Epoch [85/120    avg_loss:0.053, val_acc:0.960]
Epoch [86/120    avg_loss:0.050, val_acc:0.955]
Epoch [87/120    avg_loss:0.054, val_acc:0.953]
Epoch [88/120    avg_loss:0.060, val_acc:0.960]
Epoch [89/120    avg_loss:0.050, val_acc:0.961]
Epoch [90/120    avg_loss:0.052, val_acc:0.962]
Epoch [91/120    avg_loss:0.043, val_acc:0.965]
Epoch [92/120    avg_loss:0.041, val_acc:0.961]
Epoch [93/120    avg_loss:0.050, val_acc:0.968]
Epoch [94/120    avg_loss:0.039, val_acc:0.963]
Epoch [95/120    avg_loss:0.046, val_acc:0.968]
Epoch [96/120    avg_loss:0.049, val_acc:0.963]
Epoch [97/120    avg_loss:0.043, val_acc:0.960]
Epoch [98/120    avg_loss:0.037, val_acc:0.968]
Epoch [99/120    avg_loss:0.047, val_acc:0.964]
Epoch [100/120    avg_loss:0.050, val_acc:0.950]
Epoch [101/120    avg_loss:0.050, val_acc:0.963]
Epoch [102/120    avg_loss:0.039, val_acc:0.964]
Epoch [103/120    avg_loss:0.041, val_acc:0.961]
Epoch [104/120    avg_loss:0.043, val_acc:0.960]
Epoch [105/120    avg_loss:0.047, val_acc:0.964]
Epoch [106/120    avg_loss:0.047, val_acc:0.958]
Epoch [107/120    avg_loss:0.042, val_acc:0.970]
Epoch [108/120    avg_loss:0.024, val_acc:0.969]
Epoch [109/120    avg_loss:0.040, val_acc:0.953]
Epoch [110/120    avg_loss:0.039, val_acc:0.962]
Epoch [111/120    avg_loss:0.065, val_acc:0.965]
Epoch [112/120    avg_loss:0.056, val_acc:0.955]
Epoch [113/120    avg_loss:0.043, val_acc:0.965]
Epoch [114/120    avg_loss:0.040, val_acc:0.968]
Epoch [115/120    avg_loss:0.041, val_acc:0.975]
Epoch [116/120    avg_loss:0.044, val_acc:0.942]
Epoch [117/120    avg_loss:0.055, val_acc:0.972]
Epoch [118/120    avg_loss:0.032, val_acc:0.967]
Epoch [119/120    avg_loss:0.029, val_acc:0.971]
Epoch [120/120    avg_loss:0.032, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1206    2    0    0    0    0    0    6   19   52    0    0
     0    0    0]
 [   0    0    7  705    2    0    0    0    0   10    0    0   21    2
     0    0    0]
 [   0    0    0    0  212    0    1    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0  404    0    0    0    7    1    0    0    0
    22    0    0]
 [   0    0    1    0    0    0  654    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   30   89    0    1    1    0    0    0  750    0    3    0
     0    1    0]
 [   0    0   16    0    0    0    1    0    3    0    6 2174    7    3
     0    0    0]
 [   0    0    0   38    3    0    0    0    0    0   13    2  472    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    2    1    0
  1131    1    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    1    0
   109  224    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
94.38482384823848

F1 scores:
[       nan 0.975      0.94774067 0.89127686 0.98604651 0.96190476
 0.98568199 1.         0.99537037 0.61016949 0.89874176 0.97883836
 0.90334928 0.98666667 0.94210746 0.78184991 0.92857143]

Kappa:
0.9358990198186058
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd9c973d898>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.775, val_acc:0.388]
Epoch [2/120    avg_loss:2.516, val_acc:0.464]
Epoch [3/120    avg_loss:2.311, val_acc:0.453]
Epoch [4/120    avg_loss:2.159, val_acc:0.471]
Epoch [5/120    avg_loss:2.055, val_acc:0.546]
Epoch [6/120    avg_loss:1.963, val_acc:0.535]
Epoch [7/120    avg_loss:1.854, val_acc:0.580]
Epoch [8/120    avg_loss:1.724, val_acc:0.607]
Epoch [9/120    avg_loss:1.660, val_acc:0.613]
Epoch [10/120    avg_loss:1.537, val_acc:0.619]
Epoch [11/120    avg_loss:1.390, val_acc:0.661]
Epoch [12/120    avg_loss:1.278, val_acc:0.674]
Epoch [13/120    avg_loss:1.147, val_acc:0.676]
Epoch [14/120    avg_loss:1.099, val_acc:0.740]
Epoch [15/120    avg_loss:0.998, val_acc:0.708]
Epoch [16/120    avg_loss:0.890, val_acc:0.721]
Epoch [17/120    avg_loss:0.875, val_acc:0.727]
Epoch [18/120    avg_loss:0.772, val_acc:0.731]
Epoch [19/120    avg_loss:0.743, val_acc:0.741]
Epoch [20/120    avg_loss:0.698, val_acc:0.778]
Epoch [21/120    avg_loss:0.615, val_acc:0.790]
Epoch [22/120    avg_loss:0.568, val_acc:0.777]
Epoch [23/120    avg_loss:0.567, val_acc:0.769]
Epoch [24/120    avg_loss:0.499, val_acc:0.818]
Epoch [25/120    avg_loss:0.471, val_acc:0.857]
Epoch [26/120    avg_loss:0.471, val_acc:0.836]
Epoch [27/120    avg_loss:0.417, val_acc:0.835]
Epoch [28/120    avg_loss:0.374, val_acc:0.845]
Epoch [29/120    avg_loss:0.382, val_acc:0.842]
Epoch [30/120    avg_loss:0.378, val_acc:0.833]
Epoch [31/120    avg_loss:0.397, val_acc:0.855]
Epoch [32/120    avg_loss:0.366, val_acc:0.853]
Epoch [33/120    avg_loss:0.348, val_acc:0.845]
Epoch [34/120    avg_loss:0.308, val_acc:0.844]
Epoch [35/120    avg_loss:0.294, val_acc:0.850]
Epoch [36/120    avg_loss:0.296, val_acc:0.881]
Epoch [37/120    avg_loss:0.291, val_acc:0.844]
Epoch [38/120    avg_loss:0.276, val_acc:0.862]
Epoch [39/120    avg_loss:0.235, val_acc:0.877]
Epoch [40/120    avg_loss:0.208, val_acc:0.907]
Epoch [41/120    avg_loss:0.250, val_acc:0.885]
Epoch [42/120    avg_loss:0.204, val_acc:0.914]
Epoch [43/120    avg_loss:0.209, val_acc:0.871]
Epoch [44/120    avg_loss:0.292, val_acc:0.908]
Epoch [45/120    avg_loss:0.229, val_acc:0.907]
Epoch [46/120    avg_loss:0.224, val_acc:0.905]
Epoch [47/120    avg_loss:0.205, val_acc:0.926]
Epoch [48/120    avg_loss:0.198, val_acc:0.916]
Epoch [49/120    avg_loss:0.172, val_acc:0.925]
Epoch [50/120    avg_loss:0.163, val_acc:0.935]
Epoch [51/120    avg_loss:0.142, val_acc:0.936]
Epoch [52/120    avg_loss:0.144, val_acc:0.940]
Epoch [53/120    avg_loss:0.136, val_acc:0.939]
Epoch [54/120    avg_loss:0.138, val_acc:0.934]
Epoch [55/120    avg_loss:0.155, val_acc:0.924]
Epoch [56/120    avg_loss:0.107, val_acc:0.948]
Epoch [57/120    avg_loss:0.129, val_acc:0.940]
Epoch [58/120    avg_loss:0.184, val_acc:0.920]
Epoch [59/120    avg_loss:0.158, val_acc:0.931]
Epoch [60/120    avg_loss:0.120, val_acc:0.946]
Epoch [61/120    avg_loss:0.120, val_acc:0.915]
Epoch [62/120    avg_loss:0.142, val_acc:0.927]
Epoch [63/120    avg_loss:0.122, val_acc:0.951]
Epoch [64/120    avg_loss:0.106, val_acc:0.944]
Epoch [65/120    avg_loss:0.090, val_acc:0.938]
Epoch [66/120    avg_loss:0.091, val_acc:0.940]
Epoch [67/120    avg_loss:0.079, val_acc:0.942]
Epoch [68/120    avg_loss:0.080, val_acc:0.944]
Epoch [69/120    avg_loss:0.093, val_acc:0.941]
Epoch [70/120    avg_loss:0.095, val_acc:0.946]
Epoch [71/120    avg_loss:0.085, val_acc:0.945]
Epoch [72/120    avg_loss:0.127, val_acc:0.916]
Epoch [73/120    avg_loss:0.140, val_acc:0.939]
Epoch [74/120    avg_loss:0.094, val_acc:0.940]
Epoch [75/120    avg_loss:0.091, val_acc:0.948]
Epoch [76/120    avg_loss:0.080, val_acc:0.946]
Epoch [77/120    avg_loss:0.049, val_acc:0.955]
Epoch [78/120    avg_loss:0.052, val_acc:0.958]
Epoch [79/120    avg_loss:0.049, val_acc:0.960]
Epoch [80/120    avg_loss:0.050, val_acc:0.960]
Epoch [81/120    avg_loss:0.050, val_acc:0.958]
Epoch [82/120    avg_loss:0.056, val_acc:0.961]
Epoch [83/120    avg_loss:0.050, val_acc:0.962]
Epoch [84/120    avg_loss:0.049, val_acc:0.961]
Epoch [85/120    avg_loss:0.046, val_acc:0.964]
Epoch [86/120    avg_loss:0.043, val_acc:0.962]
Epoch [87/120    avg_loss:0.042, val_acc:0.963]
Epoch [88/120    avg_loss:0.044, val_acc:0.964]
Epoch [89/120    avg_loss:0.042, val_acc:0.962]
Epoch [90/120    avg_loss:0.046, val_acc:0.964]
Epoch [91/120    avg_loss:0.051, val_acc:0.965]
Epoch [92/120    avg_loss:0.050, val_acc:0.965]
Epoch [93/120    avg_loss:0.052, val_acc:0.967]
Epoch [94/120    avg_loss:0.043, val_acc:0.965]
Epoch [95/120    avg_loss:0.038, val_acc:0.967]
Epoch [96/120    avg_loss:0.042, val_acc:0.968]
Epoch [97/120    avg_loss:0.044, val_acc:0.964]
Epoch [98/120    avg_loss:0.044, val_acc:0.962]
Epoch [99/120    avg_loss:0.050, val_acc:0.969]
Epoch [100/120    avg_loss:0.038, val_acc:0.964]
Epoch [101/120    avg_loss:0.037, val_acc:0.965]
Epoch [102/120    avg_loss:0.041, val_acc:0.968]
Epoch [103/120    avg_loss:0.041, val_acc:0.967]
Epoch [104/120    avg_loss:0.044, val_acc:0.970]
Epoch [105/120    avg_loss:0.041, val_acc:0.965]
Epoch [106/120    avg_loss:0.039, val_acc:0.965]
Epoch [107/120    avg_loss:0.044, val_acc:0.971]
Epoch [108/120    avg_loss:0.040, val_acc:0.967]
Epoch [109/120    avg_loss:0.041, val_acc:0.968]
Epoch [110/120    avg_loss:0.041, val_acc:0.968]
Epoch [111/120    avg_loss:0.039, val_acc:0.970]
Epoch [112/120    avg_loss:0.038, val_acc:0.970]
Epoch [113/120    avg_loss:0.037, val_acc:0.970]
Epoch [114/120    avg_loss:0.038, val_acc:0.972]
Epoch [115/120    avg_loss:0.036, val_acc:0.970]
Epoch [116/120    avg_loss:0.044, val_acc:0.968]
Epoch [117/120    avg_loss:0.039, val_acc:0.969]
Epoch [118/120    avg_loss:0.037, val_acc:0.969]
Epoch [119/120    avg_loss:0.037, val_acc:0.971]
Epoch [120/120    avg_loss:0.037, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    8 1199    0    0    0    1    0    0    0   18   41   18    0
     0    0    0]
 [   0    0    6  703    0   20    0    0    0    6    0    0   10    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    6    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    1    0    0   14    0    0    1    0
     0    0    0]
 [   0    0   40   90    0    3    0    0    0    0  733    3    0    0
     2    4    0]
 [   0    0   19    0    0    3    2    0    0    0    5 2169    3    4
     5    0    0]
 [   0    0    0    9    3    2    0    0    0    0   15    5  482    0
     0    0   18]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   14    0    0    1    0    3    1    0    0
  1120    0    0]
 [   0    0    0    0    0    1   28    0    0    0    0    0    0   25
    83  210    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.13550135501355

F1 scores:
[       nan 0.88636364 0.94076108 0.90651193 0.99300699 0.94130676
 0.97317437 0.89285714 0.99883856 0.71794872 0.8879467  0.97856982
 0.91984733 0.92269327 0.95238095 0.7486631  0.90322581]

Kappa:
0.9331154921619461
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4e0aff9828>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.746, val_acc:0.355]
Epoch [2/120    avg_loss:2.502, val_acc:0.403]
Epoch [3/120    avg_loss:2.334, val_acc:0.432]
Epoch [4/120    avg_loss:2.163, val_acc:0.455]
Epoch [5/120    avg_loss:2.046, val_acc:0.527]
Epoch [6/120    avg_loss:1.954, val_acc:0.571]
Epoch [7/120    avg_loss:1.822, val_acc:0.608]
Epoch [8/120    avg_loss:1.693, val_acc:0.589]
Epoch [9/120    avg_loss:1.621, val_acc:0.616]
Epoch [10/120    avg_loss:1.492, val_acc:0.633]
Epoch [11/120    avg_loss:1.362, val_acc:0.670]
Epoch [12/120    avg_loss:1.231, val_acc:0.669]
Epoch [13/120    avg_loss:1.139, val_acc:0.698]
Epoch [14/120    avg_loss:1.043, val_acc:0.635]
Epoch [15/120    avg_loss:1.007, val_acc:0.689]
Epoch [16/120    avg_loss:0.925, val_acc:0.722]
Epoch [17/120    avg_loss:0.815, val_acc:0.766]
Epoch [18/120    avg_loss:0.736, val_acc:0.782]
Epoch [19/120    avg_loss:0.645, val_acc:0.782]
Epoch [20/120    avg_loss:0.586, val_acc:0.793]
Epoch [21/120    avg_loss:0.539, val_acc:0.809]
Epoch [22/120    avg_loss:0.616, val_acc:0.787]
Epoch [23/120    avg_loss:0.659, val_acc:0.777]
Epoch [24/120    avg_loss:0.559, val_acc:0.828]
Epoch [25/120    avg_loss:0.530, val_acc:0.775]
Epoch [26/120    avg_loss:0.527, val_acc:0.814]
Epoch [27/120    avg_loss:0.449, val_acc:0.800]
Epoch [28/120    avg_loss:0.510, val_acc:0.821]
Epoch [29/120    avg_loss:0.430, val_acc:0.839]
Epoch [30/120    avg_loss:0.372, val_acc:0.854]
Epoch [31/120    avg_loss:0.376, val_acc:0.844]
Epoch [32/120    avg_loss:0.356, val_acc:0.844]
Epoch [33/120    avg_loss:0.343, val_acc:0.851]
Epoch [34/120    avg_loss:0.408, val_acc:0.800]
Epoch [35/120    avg_loss:0.347, val_acc:0.858]
Epoch [36/120    avg_loss:0.308, val_acc:0.841]
Epoch [37/120    avg_loss:0.317, val_acc:0.867]
Epoch [38/120    avg_loss:0.257, val_acc:0.896]
Epoch [39/120    avg_loss:0.254, val_acc:0.887]
Epoch [40/120    avg_loss:0.235, val_acc:0.880]
Epoch [41/120    avg_loss:0.215, val_acc:0.889]
Epoch [42/120    avg_loss:0.200, val_acc:0.892]
Epoch [43/120    avg_loss:0.217, val_acc:0.875]
Epoch [44/120    avg_loss:0.194, val_acc:0.887]
Epoch [45/120    avg_loss:0.170, val_acc:0.902]
Epoch [46/120    avg_loss:0.159, val_acc:0.896]
Epoch [47/120    avg_loss:0.165, val_acc:0.904]
Epoch [48/120    avg_loss:0.187, val_acc:0.911]
Epoch [49/120    avg_loss:0.215, val_acc:0.876]
Epoch [50/120    avg_loss:0.236, val_acc:0.888]
Epoch [51/120    avg_loss:0.189, val_acc:0.913]
Epoch [52/120    avg_loss:0.138, val_acc:0.899]
Epoch [53/120    avg_loss:0.214, val_acc:0.892]
Epoch [54/120    avg_loss:0.161, val_acc:0.906]
Epoch [55/120    avg_loss:0.147, val_acc:0.898]
Epoch [56/120    avg_loss:0.127, val_acc:0.920]
Epoch [57/120    avg_loss:0.122, val_acc:0.911]
Epoch [58/120    avg_loss:0.109, val_acc:0.931]
Epoch [59/120    avg_loss:0.122, val_acc:0.910]
Epoch [60/120    avg_loss:0.117, val_acc:0.916]
Epoch [61/120    avg_loss:0.096, val_acc:0.929]
Epoch [62/120    avg_loss:0.086, val_acc:0.924]
Epoch [63/120    avg_loss:0.099, val_acc:0.907]
Epoch [64/120    avg_loss:0.103, val_acc:0.925]
Epoch [65/120    avg_loss:0.093, val_acc:0.924]
Epoch [66/120    avg_loss:0.112, val_acc:0.927]
Epoch [67/120    avg_loss:0.082, val_acc:0.924]
Epoch [68/120    avg_loss:0.083, val_acc:0.922]
Epoch [69/120    avg_loss:0.091, val_acc:0.935]
Epoch [70/120    avg_loss:0.077, val_acc:0.936]
Epoch [71/120    avg_loss:0.092, val_acc:0.938]
Epoch [72/120    avg_loss:0.082, val_acc:0.933]
Epoch [73/120    avg_loss:0.062, val_acc:0.942]
Epoch [74/120    avg_loss:0.062, val_acc:0.931]
Epoch [75/120    avg_loss:0.060, val_acc:0.940]
Epoch [76/120    avg_loss:0.053, val_acc:0.943]
Epoch [77/120    avg_loss:0.052, val_acc:0.925]
Epoch [78/120    avg_loss:0.056, val_acc:0.927]
Epoch [79/120    avg_loss:0.077, val_acc:0.935]
Epoch [80/120    avg_loss:0.051, val_acc:0.941]
Epoch [81/120    avg_loss:0.049, val_acc:0.949]
Epoch [82/120    avg_loss:0.046, val_acc:0.944]
Epoch [83/120    avg_loss:0.041, val_acc:0.936]
Epoch [84/120    avg_loss:0.066, val_acc:0.933]
Epoch [85/120    avg_loss:0.066, val_acc:0.927]
Epoch [86/120    avg_loss:0.049, val_acc:0.940]
Epoch [87/120    avg_loss:0.039, val_acc:0.936]
Epoch [88/120    avg_loss:0.049, val_acc:0.931]
Epoch [89/120    avg_loss:0.057, val_acc:0.933]
Epoch [90/120    avg_loss:0.103, val_acc:0.900]
Epoch [91/120    avg_loss:0.108, val_acc:0.934]
Epoch [92/120    avg_loss:0.065, val_acc:0.944]
Epoch [93/120    avg_loss:0.094, val_acc:0.938]
Epoch [94/120    avg_loss:0.073, val_acc:0.939]
Epoch [95/120    avg_loss:0.051, val_acc:0.947]
Epoch [96/120    avg_loss:0.037, val_acc:0.950]
Epoch [97/120    avg_loss:0.035, val_acc:0.954]
Epoch [98/120    avg_loss:0.031, val_acc:0.953]
Epoch [99/120    avg_loss:0.034, val_acc:0.953]
Epoch [100/120    avg_loss:0.033, val_acc:0.956]
Epoch [101/120    avg_loss:0.034, val_acc:0.956]
Epoch [102/120    avg_loss:0.035, val_acc:0.955]
Epoch [103/120    avg_loss:0.030, val_acc:0.955]
Epoch [104/120    avg_loss:0.031, val_acc:0.957]
Epoch [105/120    avg_loss:0.030, val_acc:0.957]
Epoch [106/120    avg_loss:0.028, val_acc:0.957]
Epoch [107/120    avg_loss:0.030, val_acc:0.956]
Epoch [108/120    avg_loss:0.032, val_acc:0.958]
Epoch [109/120    avg_loss:0.029, val_acc:0.959]
Epoch [110/120    avg_loss:0.032, val_acc:0.957]
Epoch [111/120    avg_loss:0.030, val_acc:0.959]
Epoch [112/120    avg_loss:0.034, val_acc:0.959]
Epoch [113/120    avg_loss:0.026, val_acc:0.961]
Epoch [114/120    avg_loss:0.026, val_acc:0.959]
Epoch [115/120    avg_loss:0.032, val_acc:0.961]
Epoch [116/120    avg_loss:0.029, val_acc:0.955]
Epoch [117/120    avg_loss:0.031, val_acc:0.954]
Epoch [118/120    avg_loss:0.031, val_acc:0.957]
Epoch [119/120    avg_loss:0.032, val_acc:0.955]
Epoch [120/120    avg_loss:0.028, val_acc:0.953]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1200   10    0    0    8    0    0    0   12   44    6    0
     0    5    0]
 [   0    0    1  720    1    2    0    0    0    6    0    0   12    5
     0    0    0]
 [   0    0    0    0  211    0    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    2    0    5    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   18    0    0    0    0    0    0  412    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0   14   75    0    7   13    0    0    0  755    4    0    0
     0    7    0]
 [   0    0    9    0    0    4   24    0    6    0   16 2144    4    3
     0    0    0]
 [   0    0    0    2    4   10    0    0    0    0    9    2  494    0
     0    0   13]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   16    0    0    1    0    2    0    3    0
  1117    0    0]
 [   0    0    0    0    0    0   43    0    0   13    0    0    0    0
    97  194    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.01626016260163

F1 scores:
[       nan 0.79591837 0.9565564  0.92426187 0.98368298 0.94196429
 0.93723252 0.96153846 0.97055359 0.48275862 0.90365051 0.97366031
 0.9382716  0.97883598 0.94701145 0.70162749 0.9281768 ]

Kappa:
0.9318082816124865
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7d89099860>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.729, val_acc:0.311]
Epoch [2/120    avg_loss:2.448, val_acc:0.404]
Epoch [3/120    avg_loss:2.284, val_acc:0.485]
Epoch [4/120    avg_loss:2.128, val_acc:0.512]
Epoch [5/120    avg_loss:2.023, val_acc:0.530]
Epoch [6/120    avg_loss:1.962, val_acc:0.515]
Epoch [7/120    avg_loss:1.854, val_acc:0.548]
Epoch [8/120    avg_loss:1.790, val_acc:0.562]
Epoch [9/120    avg_loss:1.684, val_acc:0.551]
Epoch [10/120    avg_loss:1.604, val_acc:0.584]
Epoch [11/120    avg_loss:1.482, val_acc:0.602]
Epoch [12/120    avg_loss:1.403, val_acc:0.635]
Epoch [13/120    avg_loss:1.305, val_acc:0.651]
Epoch [14/120    avg_loss:1.220, val_acc:0.629]
Epoch [15/120    avg_loss:1.126, val_acc:0.691]
Epoch [16/120    avg_loss:1.013, val_acc:0.696]
Epoch [17/120    avg_loss:0.977, val_acc:0.682]
Epoch [18/120    avg_loss:0.863, val_acc:0.696]
Epoch [19/120    avg_loss:0.815, val_acc:0.700]
Epoch [20/120    avg_loss:0.724, val_acc:0.737]
Epoch [21/120    avg_loss:0.762, val_acc:0.729]
Epoch [22/120    avg_loss:0.657, val_acc:0.724]
Epoch [23/120    avg_loss:0.639, val_acc:0.781]
Epoch [24/120    avg_loss:0.559, val_acc:0.804]
Epoch [25/120    avg_loss:0.546, val_acc:0.796]
Epoch [26/120    avg_loss:0.477, val_acc:0.819]
Epoch [27/120    avg_loss:0.431, val_acc:0.833]
Epoch [28/120    avg_loss:0.404, val_acc:0.849]
Epoch [29/120    avg_loss:0.410, val_acc:0.815]
Epoch [30/120    avg_loss:0.467, val_acc:0.812]
Epoch [31/120    avg_loss:0.438, val_acc:0.808]
Epoch [32/120    avg_loss:0.416, val_acc:0.850]
Epoch [33/120    avg_loss:0.327, val_acc:0.848]
Epoch [34/120    avg_loss:0.303, val_acc:0.873]
Epoch [35/120    avg_loss:0.290, val_acc:0.882]
Epoch [36/120    avg_loss:0.246, val_acc:0.890]
Epoch [37/120    avg_loss:0.268, val_acc:0.883]
Epoch [38/120    avg_loss:0.321, val_acc:0.863]
Epoch [39/120    avg_loss:0.352, val_acc:0.875]
Epoch [40/120    avg_loss:0.262, val_acc:0.881]
Epoch [41/120    avg_loss:0.235, val_acc:0.897]
Epoch [42/120    avg_loss:0.248, val_acc:0.884]
Epoch [43/120    avg_loss:0.194, val_acc:0.893]
Epoch [44/120    avg_loss:0.233, val_acc:0.894]
Epoch [45/120    avg_loss:0.247, val_acc:0.859]
Epoch [46/120    avg_loss:0.221, val_acc:0.879]
Epoch [47/120    avg_loss:0.182, val_acc:0.910]
Epoch [48/120    avg_loss:0.182, val_acc:0.906]
Epoch [49/120    avg_loss:0.193, val_acc:0.893]
Epoch [50/120    avg_loss:0.150, val_acc:0.913]
Epoch [51/120    avg_loss:0.144, val_acc:0.911]
Epoch [52/120    avg_loss:0.142, val_acc:0.910]
Epoch [53/120    avg_loss:0.142, val_acc:0.922]
Epoch [54/120    avg_loss:0.140, val_acc:0.902]
Epoch [55/120    avg_loss:0.154, val_acc:0.930]
Epoch [56/120    avg_loss:0.155, val_acc:0.921]
Epoch [57/120    avg_loss:0.124, val_acc:0.933]
Epoch [58/120    avg_loss:0.105, val_acc:0.932]
Epoch [59/120    avg_loss:0.125, val_acc:0.922]
Epoch [60/120    avg_loss:0.112, val_acc:0.931]
Epoch [61/120    avg_loss:0.091, val_acc:0.936]
Epoch [62/120    avg_loss:0.076, val_acc:0.929]
Epoch [63/120    avg_loss:0.076, val_acc:0.939]
Epoch [64/120    avg_loss:0.083, val_acc:0.924]
Epoch [65/120    avg_loss:0.128, val_acc:0.907]
Epoch [66/120    avg_loss:0.140, val_acc:0.931]
Epoch [67/120    avg_loss:0.148, val_acc:0.921]
Epoch [68/120    avg_loss:0.119, val_acc:0.927]
Epoch [69/120    avg_loss:0.085, val_acc:0.932]
Epoch [70/120    avg_loss:0.075, val_acc:0.941]
Epoch [71/120    avg_loss:0.066, val_acc:0.931]
Epoch [72/120    avg_loss:0.073, val_acc:0.940]
Epoch [73/120    avg_loss:0.054, val_acc:0.945]
Epoch [74/120    avg_loss:0.066, val_acc:0.943]
Epoch [75/120    avg_loss:0.085, val_acc:0.926]
Epoch [76/120    avg_loss:0.076, val_acc:0.933]
Epoch [77/120    avg_loss:0.062, val_acc:0.950]
Epoch [78/120    avg_loss:0.067, val_acc:0.938]
Epoch [79/120    avg_loss:0.071, val_acc:0.924]
Epoch [80/120    avg_loss:0.073, val_acc:0.950]
Epoch [81/120    avg_loss:0.079, val_acc:0.944]
Epoch [82/120    avg_loss:0.090, val_acc:0.929]
Epoch [83/120    avg_loss:0.073, val_acc:0.933]
Epoch [84/120    avg_loss:0.065, val_acc:0.944]
Epoch [85/120    avg_loss:0.059, val_acc:0.941]
Epoch [86/120    avg_loss:0.059, val_acc:0.954]
Epoch [87/120    avg_loss:0.053, val_acc:0.943]
Epoch [88/120    avg_loss:0.052, val_acc:0.954]
Epoch [89/120    avg_loss:0.044, val_acc:0.944]
Epoch [90/120    avg_loss:0.038, val_acc:0.951]
Epoch [91/120    avg_loss:0.046, val_acc:0.949]
Epoch [92/120    avg_loss:0.040, val_acc:0.943]
Epoch [93/120    avg_loss:0.047, val_acc:0.949]
Epoch [94/120    avg_loss:0.048, val_acc:0.939]
Epoch [95/120    avg_loss:0.070, val_acc:0.883]
Epoch [96/120    avg_loss:0.082, val_acc:0.935]
Epoch [97/120    avg_loss:0.052, val_acc:0.946]
Epoch [98/120    avg_loss:0.041, val_acc:0.953]
Epoch [99/120    avg_loss:0.046, val_acc:0.939]
Epoch [100/120    avg_loss:0.049, val_acc:0.943]
Epoch [101/120    avg_loss:0.054, val_acc:0.944]
Epoch [102/120    avg_loss:0.044, val_acc:0.948]
Epoch [103/120    avg_loss:0.038, val_acc:0.950]
Epoch [104/120    avg_loss:0.039, val_acc:0.948]
Epoch [105/120    avg_loss:0.029, val_acc:0.949]
Epoch [106/120    avg_loss:0.027, val_acc:0.951]
Epoch [107/120    avg_loss:0.030, val_acc:0.951]
Epoch [108/120    avg_loss:0.026, val_acc:0.956]
Epoch [109/120    avg_loss:0.026, val_acc:0.956]
Epoch [110/120    avg_loss:0.025, val_acc:0.955]
Epoch [111/120    avg_loss:0.024, val_acc:0.956]
Epoch [112/120    avg_loss:0.021, val_acc:0.955]
Epoch [113/120    avg_loss:0.031, val_acc:0.956]
Epoch [114/120    avg_loss:0.024, val_acc:0.955]
Epoch [115/120    avg_loss:0.025, val_acc:0.951]
Epoch [116/120    avg_loss:0.025, val_acc:0.955]
Epoch [117/120    avg_loss:0.025, val_acc:0.955]
Epoch [118/120    avg_loss:0.025, val_acc:0.955]
Epoch [119/120    avg_loss:0.025, val_acc:0.956]
Epoch [120/120    avg_loss:0.022, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1247    2    0    0    6    0    0    0    5   24    0    0
     0    1    0]
 [   0    0    2  695    1   21    0    0    0    6    1    3   18    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  413    0    4    0   10    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0   31   90    0   13    0    0    0    0  738    0    0    0
     0    3    0]
 [   0    0   19    0    0    0    5    0    0    0    5 2169    6    2
     4    0    0]
 [   0    0    3   36    0   10    0    0    0    0    7    1  470    0
     0    0    7]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0   10    0    0    1    0    2    0    0    0
  1126    0    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    2    0
    72  258    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
94.9159891598916

F1 scores:
[       nan 0.98765432 0.96405102 0.88478676 0.99530516 0.91472868
 0.97754491 0.92592593 0.99883856 0.6122449  0.90330477 0.98345046
 0.90733591 0.99191375 0.95870583 0.84729064 0.94186047]

Kappa:
0.9420203286689305
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fded778d860>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.705, val_acc:0.458]
Epoch [2/120    avg_loss:2.440, val_acc:0.496]
Epoch [3/120    avg_loss:2.250, val_acc:0.510]
Epoch [4/120    avg_loss:2.103, val_acc:0.531]
Epoch [5/120    avg_loss:2.014, val_acc:0.544]
Epoch [6/120    avg_loss:1.946, val_acc:0.552]
Epoch [7/120    avg_loss:1.842, val_acc:0.569]
Epoch [8/120    avg_loss:1.737, val_acc:0.603]
Epoch [9/120    avg_loss:1.645, val_acc:0.637]
Epoch [10/120    avg_loss:1.505, val_acc:0.628]
Epoch [11/120    avg_loss:1.431, val_acc:0.666]
Epoch [12/120    avg_loss:1.327, val_acc:0.692]
Epoch [13/120    avg_loss:1.202, val_acc:0.719]
Epoch [14/120    avg_loss:1.048, val_acc:0.720]
Epoch [15/120    avg_loss:0.951, val_acc:0.741]
Epoch [16/120    avg_loss:0.994, val_acc:0.751]
Epoch [17/120    avg_loss:0.949, val_acc:0.723]
Epoch [18/120    avg_loss:0.735, val_acc:0.789]
Epoch [19/120    avg_loss:0.726, val_acc:0.788]
Epoch [20/120    avg_loss:0.653, val_acc:0.828]
Epoch [21/120    avg_loss:0.590, val_acc:0.833]
Epoch [22/120    avg_loss:0.544, val_acc:0.781]
Epoch [23/120    avg_loss:0.728, val_acc:0.817]
Epoch [24/120    avg_loss:0.566, val_acc:0.863]
Epoch [25/120    avg_loss:0.533, val_acc:0.862]
Epoch [26/120    avg_loss:0.463, val_acc:0.852]
Epoch [27/120    avg_loss:0.438, val_acc:0.864]
Epoch [28/120    avg_loss:0.427, val_acc:0.885]
Epoch [29/120    avg_loss:0.372, val_acc:0.872]
Epoch [30/120    avg_loss:0.350, val_acc:0.897]
Epoch [31/120    avg_loss:0.420, val_acc:0.873]
Epoch [32/120    avg_loss:0.392, val_acc:0.892]
Epoch [33/120    avg_loss:0.358, val_acc:0.888]
Epoch [34/120    avg_loss:0.320, val_acc:0.901]
Epoch [35/120    avg_loss:0.274, val_acc:0.914]
Epoch [36/120    avg_loss:0.268, val_acc:0.886]
Epoch [37/120    avg_loss:0.273, val_acc:0.908]
Epoch [38/120    avg_loss:0.231, val_acc:0.898]
Epoch [39/120    avg_loss:0.317, val_acc:0.896]
Epoch [40/120    avg_loss:0.240, val_acc:0.912]
Epoch [41/120    avg_loss:0.204, val_acc:0.929]
Epoch [42/120    avg_loss:0.223, val_acc:0.905]
Epoch [43/120    avg_loss:0.263, val_acc:0.856]
Epoch [44/120    avg_loss:0.239, val_acc:0.910]
Epoch [45/120    avg_loss:0.215, val_acc:0.916]
Epoch [46/120    avg_loss:0.227, val_acc:0.922]
Epoch [47/120    avg_loss:0.228, val_acc:0.926]
Epoch [48/120    avg_loss:0.166, val_acc:0.934]
Epoch [49/120    avg_loss:0.183, val_acc:0.923]
Epoch [50/120    avg_loss:0.157, val_acc:0.939]
Epoch [51/120    avg_loss:0.129, val_acc:0.931]
Epoch [52/120    avg_loss:0.202, val_acc:0.922]
Epoch [53/120    avg_loss:0.187, val_acc:0.955]
Epoch [54/120    avg_loss:0.146, val_acc:0.916]
Epoch [55/120    avg_loss:0.196, val_acc:0.922]
Epoch [56/120    avg_loss:0.161, val_acc:0.914]
Epoch [57/120    avg_loss:0.174, val_acc:0.914]
Epoch [58/120    avg_loss:0.203, val_acc:0.925]
Epoch [59/120    avg_loss:0.164, val_acc:0.925]
Epoch [60/120    avg_loss:0.164, val_acc:0.895]
Epoch [61/120    avg_loss:0.196, val_acc:0.896]
Epoch [62/120    avg_loss:0.220, val_acc:0.915]
Epoch [63/120    avg_loss:0.175, val_acc:0.930]
Epoch [64/120    avg_loss:0.198, val_acc:0.940]
Epoch [65/120    avg_loss:0.159, val_acc:0.931]
Epoch [66/120    avg_loss:0.121, val_acc:0.945]
Epoch [67/120    avg_loss:0.092, val_acc:0.949]
Epoch [68/120    avg_loss:0.097, val_acc:0.949]
Epoch [69/120    avg_loss:0.077, val_acc:0.951]
Epoch [70/120    avg_loss:0.084, val_acc:0.953]
Epoch [71/120    avg_loss:0.082, val_acc:0.955]
Epoch [72/120    avg_loss:0.090, val_acc:0.954]
Epoch [73/120    avg_loss:0.061, val_acc:0.956]
Epoch [74/120    avg_loss:0.072, val_acc:0.953]
Epoch [75/120    avg_loss:0.067, val_acc:0.954]
Epoch [76/120    avg_loss:0.070, val_acc:0.953]
Epoch [77/120    avg_loss:0.063, val_acc:0.956]
Epoch [78/120    avg_loss:0.072, val_acc:0.951]
Epoch [79/120    avg_loss:0.062, val_acc:0.955]
Epoch [80/120    avg_loss:0.069, val_acc:0.954]
Epoch [81/120    avg_loss:0.062, val_acc:0.954]
Epoch [82/120    avg_loss:0.067, val_acc:0.958]
Epoch [83/120    avg_loss:0.071, val_acc:0.955]
Epoch [84/120    avg_loss:0.061, val_acc:0.954]
Epoch [85/120    avg_loss:0.058, val_acc:0.953]
Epoch [86/120    avg_loss:0.066, val_acc:0.959]
Epoch [87/120    avg_loss:0.067, val_acc:0.958]
Epoch [88/120    avg_loss:0.055, val_acc:0.956]
Epoch [89/120    avg_loss:0.061, val_acc:0.959]
Epoch [90/120    avg_loss:0.074, val_acc:0.959]
Epoch [91/120    avg_loss:0.058, val_acc:0.955]
Epoch [92/120    avg_loss:0.057, val_acc:0.958]
Epoch [93/120    avg_loss:0.060, val_acc:0.953]
Epoch [94/120    avg_loss:0.064, val_acc:0.959]
Epoch [95/120    avg_loss:0.055, val_acc:0.958]
Epoch [96/120    avg_loss:0.059, val_acc:0.959]
Epoch [97/120    avg_loss:0.066, val_acc:0.956]
Epoch [98/120    avg_loss:0.060, val_acc:0.956]
Epoch [99/120    avg_loss:0.057, val_acc:0.959]
Epoch [100/120    avg_loss:0.067, val_acc:0.956]
Epoch [101/120    avg_loss:0.059, val_acc:0.954]
Epoch [102/120    avg_loss:0.065, val_acc:0.956]
Epoch [103/120    avg_loss:0.058, val_acc:0.955]
Epoch [104/120    avg_loss:0.053, val_acc:0.958]
Epoch [105/120    avg_loss:0.057, val_acc:0.959]
Epoch [106/120    avg_loss:0.053, val_acc:0.958]
Epoch [107/120    avg_loss:0.052, val_acc:0.958]
Epoch [108/120    avg_loss:0.066, val_acc:0.961]
Epoch [109/120    avg_loss:0.050, val_acc:0.960]
Epoch [110/120    avg_loss:0.061, val_acc:0.956]
Epoch [111/120    avg_loss:0.054, val_acc:0.958]
Epoch [112/120    avg_loss:0.055, val_acc:0.960]
Epoch [113/120    avg_loss:0.061, val_acc:0.962]
Epoch [114/120    avg_loss:0.048, val_acc:0.962]
Epoch [115/120    avg_loss:0.050, val_acc:0.964]
Epoch [116/120    avg_loss:0.053, val_acc:0.951]
Epoch [117/120    avg_loss:0.060, val_acc:0.959]
Epoch [118/120    avg_loss:0.052, val_acc:0.962]
Epoch [119/120    avg_loss:0.047, val_acc:0.963]
Epoch [120/120    avg_loss:0.056, val_acc:0.958]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1218    3    0    0    2    0    0    0   14   40    1    0
     3    4    0]
 [   0    0    3  701    2   14    0    0    0    8    0    0   15    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  418    0    6    0    4    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  649    0    0    0    0    3    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   13    0    0    2    0
     0    0    0]
 [   0    0   33   88    0    3    2    0    0    0  740    4    0    0
     3    2    0]
 [   0    0   23    0    0    1   13    0    0    0    9 2139    6    5
    14    0    0]
 [   0    0    0   24   17    5    0    0    0    2   19    1  453    0
     0    1   12]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    1    0    0
  1135    0    0]
 [   0    0    0    0    0    0   27    0    0    1    0    0    0    0
    69  250    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.23306233062331

F1 scores:
[       nan 0.98765432 0.95081967 0.89527458 0.95730337 0.9543379
 0.96148148 0.89285714 1.         0.56521739 0.8910295  0.97271487
 0.89614243 0.9762533  0.95578947 0.82781457 0.93333333]

Kappa:
0.9342524715541832
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3e31116940>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.790, val_acc:0.298]
Epoch [2/120    avg_loss:2.527, val_acc:0.473]
Epoch [3/120    avg_loss:2.317, val_acc:0.519]
Epoch [4/120    avg_loss:2.147, val_acc:0.540]
Epoch [5/120    avg_loss:2.056, val_acc:0.564]
Epoch [6/120    avg_loss:1.930, val_acc:0.569]
Epoch [7/120    avg_loss:1.844, val_acc:0.593]
Epoch [8/120    avg_loss:1.757, val_acc:0.604]
Epoch [9/120    avg_loss:1.643, val_acc:0.616]
Epoch [10/120    avg_loss:1.564, val_acc:0.640]
Epoch [11/120    avg_loss:1.427, val_acc:0.652]
Epoch [12/120    avg_loss:1.323, val_acc:0.693]
Epoch [13/120    avg_loss:1.182, val_acc:0.695]
Epoch [14/120    avg_loss:1.106, val_acc:0.710]
Epoch [15/120    avg_loss:1.046, val_acc:0.714]
Epoch [16/120    avg_loss:0.923, val_acc:0.756]
Epoch [17/120    avg_loss:0.931, val_acc:0.761]
Epoch [18/120    avg_loss:0.797, val_acc:0.796]
Epoch [19/120    avg_loss:0.734, val_acc:0.775]
Epoch [20/120    avg_loss:0.734, val_acc:0.776]
Epoch [21/120    avg_loss:0.642, val_acc:0.762]
Epoch [22/120    avg_loss:0.571, val_acc:0.845]
Epoch [23/120    avg_loss:0.572, val_acc:0.775]
Epoch [24/120    avg_loss:0.555, val_acc:0.848]
Epoch [25/120    avg_loss:0.477, val_acc:0.844]
Epoch [26/120    avg_loss:0.457, val_acc:0.833]
Epoch [27/120    avg_loss:0.447, val_acc:0.856]
Epoch [28/120    avg_loss:0.425, val_acc:0.820]
Epoch [29/120    avg_loss:0.416, val_acc:0.853]
Epoch [30/120    avg_loss:0.420, val_acc:0.846]
Epoch [31/120    avg_loss:0.396, val_acc:0.852]
Epoch [32/120    avg_loss:0.344, val_acc:0.882]
Epoch [33/120    avg_loss:0.339, val_acc:0.884]
Epoch [34/120    avg_loss:0.302, val_acc:0.871]
Epoch [35/120    avg_loss:0.279, val_acc:0.892]
Epoch [36/120    avg_loss:0.255, val_acc:0.901]
Epoch [37/120    avg_loss:0.278, val_acc:0.911]
Epoch [38/120    avg_loss:0.231, val_acc:0.912]
Epoch [39/120    avg_loss:0.213, val_acc:0.923]
Epoch [40/120    avg_loss:0.232, val_acc:0.910]
Epoch [41/120    avg_loss:0.256, val_acc:0.897]
Epoch [42/120    avg_loss:0.239, val_acc:0.890]
Epoch [43/120    avg_loss:0.219, val_acc:0.924]
Epoch [44/120    avg_loss:0.187, val_acc:0.920]
Epoch [45/120    avg_loss:0.193, val_acc:0.910]
Epoch [46/120    avg_loss:0.197, val_acc:0.893]
Epoch [47/120    avg_loss:0.174, val_acc:0.888]
Epoch [48/120    avg_loss:0.160, val_acc:0.933]
Epoch [49/120    avg_loss:0.158, val_acc:0.897]
Epoch [50/120    avg_loss:0.159, val_acc:0.920]
Epoch [51/120    avg_loss:0.159, val_acc:0.930]
Epoch [52/120    avg_loss:0.141, val_acc:0.926]
Epoch [53/120    avg_loss:0.160, val_acc:0.869]
Epoch [54/120    avg_loss:0.162, val_acc:0.894]
Epoch [55/120    avg_loss:0.148, val_acc:0.931]
Epoch [56/120    avg_loss:0.112, val_acc:0.934]
Epoch [57/120    avg_loss:0.126, val_acc:0.927]
Epoch [58/120    avg_loss:0.100, val_acc:0.939]
Epoch [59/120    avg_loss:0.138, val_acc:0.926]
Epoch [60/120    avg_loss:0.177, val_acc:0.916]
Epoch [61/120    avg_loss:0.128, val_acc:0.944]
Epoch [62/120    avg_loss:0.119, val_acc:0.942]
Epoch [63/120    avg_loss:0.128, val_acc:0.914]
Epoch [64/120    avg_loss:0.185, val_acc:0.898]
Epoch [65/120    avg_loss:0.197, val_acc:0.915]
Epoch [66/120    avg_loss:0.139, val_acc:0.913]
Epoch [67/120    avg_loss:0.132, val_acc:0.913]
Epoch [68/120    avg_loss:0.099, val_acc:0.941]
Epoch [69/120    avg_loss:0.106, val_acc:0.925]
Epoch [70/120    avg_loss:0.090, val_acc:0.943]
Epoch [71/120    avg_loss:0.111, val_acc:0.925]
Epoch [72/120    avg_loss:0.101, val_acc:0.925]
Epoch [73/120    avg_loss:0.119, val_acc:0.940]
Epoch [74/120    avg_loss:0.084, val_acc:0.940]
Epoch [75/120    avg_loss:0.091, val_acc:0.955]
Epoch [76/120    avg_loss:0.060, val_acc:0.954]
Epoch [77/120    avg_loss:0.064, val_acc:0.955]
Epoch [78/120    avg_loss:0.063, val_acc:0.953]
Epoch [79/120    avg_loss:0.058, val_acc:0.953]
Epoch [80/120    avg_loss:0.057, val_acc:0.954]
Epoch [81/120    avg_loss:0.056, val_acc:0.954]
Epoch [82/120    avg_loss:0.053, val_acc:0.959]
Epoch [83/120    avg_loss:0.059, val_acc:0.958]
Epoch [84/120    avg_loss:0.053, val_acc:0.956]
Epoch [85/120    avg_loss:0.048, val_acc:0.954]
Epoch [86/120    avg_loss:0.046, val_acc:0.960]
Epoch [87/120    avg_loss:0.043, val_acc:0.959]
Epoch [88/120    avg_loss:0.056, val_acc:0.959]
Epoch [89/120    avg_loss:0.059, val_acc:0.959]
Epoch [90/120    avg_loss:0.050, val_acc:0.959]
Epoch [91/120    avg_loss:0.049, val_acc:0.961]
Epoch [92/120    avg_loss:0.051, val_acc:0.962]
Epoch [93/120    avg_loss:0.051, val_acc:0.962]
Epoch [94/120    avg_loss:0.052, val_acc:0.956]
Epoch [95/120    avg_loss:0.043, val_acc:0.959]
Epoch [96/120    avg_loss:0.049, val_acc:0.959]
Epoch [97/120    avg_loss:0.050, val_acc:0.958]
Epoch [98/120    avg_loss:0.048, val_acc:0.961]
Epoch [99/120    avg_loss:0.056, val_acc:0.961]
Epoch [100/120    avg_loss:0.047, val_acc:0.958]
Epoch [101/120    avg_loss:0.051, val_acc:0.959]
Epoch [102/120    avg_loss:0.042, val_acc:0.964]
Epoch [103/120    avg_loss:0.051, val_acc:0.959]
Epoch [104/120    avg_loss:0.045, val_acc:0.956]
Epoch [105/120    avg_loss:0.045, val_acc:0.959]
Epoch [106/120    avg_loss:0.049, val_acc:0.960]
Epoch [107/120    avg_loss:0.048, val_acc:0.961]
Epoch [108/120    avg_loss:0.041, val_acc:0.959]
Epoch [109/120    avg_loss:0.045, val_acc:0.960]
Epoch [110/120    avg_loss:0.041, val_acc:0.960]
Epoch [111/120    avg_loss:0.050, val_acc:0.961]
Epoch [112/120    avg_loss:0.044, val_acc:0.960]
Epoch [113/120    avg_loss:0.040, val_acc:0.959]
Epoch [114/120    avg_loss:0.039, val_acc:0.961]
Epoch [115/120    avg_loss:0.047, val_acc:0.962]
Epoch [116/120    avg_loss:0.038, val_acc:0.962]
Epoch [117/120    avg_loss:0.039, val_acc:0.962]
Epoch [118/120    avg_loss:0.041, val_acc:0.962]
Epoch [119/120    avg_loss:0.043, val_acc:0.962]
Epoch [120/120    avg_loss:0.040, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1198    4    0    0    0    0    0    0    4   64    6    0
     0    9    0]
 [   0    0    3  714    0    7    0    0    0    7    0    0   16    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  421    0    4    0    7    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   54   90    0    6    0    0    0    0  724    0    0    0
     1    0    0]
 [   0    0   33    0    0    0   14    0    0    0   10 2141    5    4
     3    0    0]
 [   0    0    2    8    5    4    0    0    0    0    1    0  494    0
     0    0   20]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    9    0    0    2    0    3    0    0    0
  1124    1    0]
 [   0    0    0    0    0    4   11    0    0    0    0    0    0    0
   113  219    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.07046070460704

F1 scores:
[       nan 0.94871795 0.93048544 0.91304348 0.98839907 0.94926719
 0.97904192 0.92592593 0.99767981 0.66666667 0.89327576 0.9696558
 0.93560606 0.98659517 0.94255765 0.76041667 0.89361702]

Kappa:
0.9323637255886742
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fced6239898>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.755, val_acc:0.247]
Epoch [2/120    avg_loss:2.480, val_acc:0.420]
Epoch [3/120    avg_loss:2.296, val_acc:0.451]
Epoch [4/120    avg_loss:2.103, val_acc:0.502]
Epoch [5/120    avg_loss:2.001, val_acc:0.575]
Epoch [6/120    avg_loss:1.906, val_acc:0.610]
Epoch [7/120    avg_loss:1.800, val_acc:0.621]
Epoch [8/120    avg_loss:1.740, val_acc:0.608]
Epoch [9/120    avg_loss:1.714, val_acc:0.631]
Epoch [10/120    avg_loss:1.547, val_acc:0.640]
Epoch [11/120    avg_loss:1.405, val_acc:0.683]
Epoch [12/120    avg_loss:1.324, val_acc:0.664]
Epoch [13/120    avg_loss:1.196, val_acc:0.703]
Epoch [14/120    avg_loss:1.027, val_acc:0.704]
Epoch [15/120    avg_loss:0.917, val_acc:0.728]
Epoch [16/120    avg_loss:0.859, val_acc:0.737]
Epoch [17/120    avg_loss:0.814, val_acc:0.747]
Epoch [18/120    avg_loss:0.719, val_acc:0.772]
Epoch [19/120    avg_loss:0.687, val_acc:0.801]
Epoch [20/120    avg_loss:0.650, val_acc:0.776]
Epoch [21/120    avg_loss:0.596, val_acc:0.809]
Epoch [22/120    avg_loss:0.619, val_acc:0.802]
Epoch [23/120    avg_loss:0.563, val_acc:0.826]
Epoch [24/120    avg_loss:0.532, val_acc:0.817]
Epoch [25/120    avg_loss:0.654, val_acc:0.789]
Epoch [26/120    avg_loss:0.599, val_acc:0.785]
Epoch [27/120    avg_loss:0.503, val_acc:0.824]
Epoch [28/120    avg_loss:0.410, val_acc:0.863]
Epoch [29/120    avg_loss:0.395, val_acc:0.833]
Epoch [30/120    avg_loss:0.371, val_acc:0.844]
Epoch [31/120    avg_loss:0.365, val_acc:0.883]
Epoch [32/120    avg_loss:0.338, val_acc:0.816]
Epoch [33/120    avg_loss:0.335, val_acc:0.879]
Epoch [34/120    avg_loss:0.300, val_acc:0.885]
Epoch [35/120    avg_loss:0.258, val_acc:0.874]
Epoch [36/120    avg_loss:0.227, val_acc:0.891]
Epoch [37/120    avg_loss:0.235, val_acc:0.894]
Epoch [38/120    avg_loss:0.283, val_acc:0.890]
Epoch [39/120    avg_loss:0.235, val_acc:0.898]
Epoch [40/120    avg_loss:0.231, val_acc:0.903]
Epoch [41/120    avg_loss:0.231, val_acc:0.908]
Epoch [42/120    avg_loss:0.192, val_acc:0.926]
Epoch [43/120    avg_loss:0.166, val_acc:0.917]
Epoch [44/120    avg_loss:0.164, val_acc:0.929]
Epoch [45/120    avg_loss:0.258, val_acc:0.901]
Epoch [46/120    avg_loss:0.262, val_acc:0.920]
Epoch [47/120    avg_loss:0.248, val_acc:0.879]
Epoch [48/120    avg_loss:0.192, val_acc:0.938]
Epoch [49/120    avg_loss:0.180, val_acc:0.925]
Epoch [50/120    avg_loss:0.137, val_acc:0.925]
Epoch [51/120    avg_loss:0.150, val_acc:0.932]
Epoch [52/120    avg_loss:0.147, val_acc:0.935]
Epoch [53/120    avg_loss:0.138, val_acc:0.941]
Epoch [54/120    avg_loss:0.139, val_acc:0.943]
Epoch [55/120    avg_loss:0.126, val_acc:0.936]
Epoch [56/120    avg_loss:0.093, val_acc:0.944]
Epoch [57/120    avg_loss:0.095, val_acc:0.929]
Epoch [58/120    avg_loss:0.113, val_acc:0.936]
Epoch [59/120    avg_loss:0.206, val_acc:0.903]
Epoch [60/120    avg_loss:0.175, val_acc:0.896]
Epoch [61/120    avg_loss:0.160, val_acc:0.929]
Epoch [62/120    avg_loss:0.176, val_acc:0.927]
Epoch [63/120    avg_loss:0.128, val_acc:0.925]
Epoch [64/120    avg_loss:0.120, val_acc:0.929]
Epoch [65/120    avg_loss:0.092, val_acc:0.930]
Epoch [66/120    avg_loss:0.129, val_acc:0.911]
Epoch [67/120    avg_loss:0.145, val_acc:0.932]
Epoch [68/120    avg_loss:0.107, val_acc:0.932]
Epoch [69/120    avg_loss:0.084, val_acc:0.948]
Epoch [70/120    avg_loss:0.084, val_acc:0.946]
Epoch [71/120    avg_loss:0.081, val_acc:0.941]
Epoch [72/120    avg_loss:0.070, val_acc:0.950]
Epoch [73/120    avg_loss:0.079, val_acc:0.942]
Epoch [74/120    avg_loss:0.070, val_acc:0.945]
Epoch [75/120    avg_loss:0.087, val_acc:0.950]
Epoch [76/120    avg_loss:0.076, val_acc:0.945]
Epoch [77/120    avg_loss:0.063, val_acc:0.951]
Epoch [78/120    avg_loss:0.075, val_acc:0.950]
Epoch [79/120    avg_loss:0.074, val_acc:0.949]
Epoch [80/120    avg_loss:0.075, val_acc:0.940]
Epoch [81/120    avg_loss:0.051, val_acc:0.953]
Epoch [82/120    avg_loss:0.058, val_acc:0.930]
Epoch [83/120    avg_loss:0.070, val_acc:0.938]
Epoch [84/120    avg_loss:0.064, val_acc:0.951]
Epoch [85/120    avg_loss:0.064, val_acc:0.954]
Epoch [86/120    avg_loss:0.059, val_acc:0.960]
Epoch [87/120    avg_loss:0.054, val_acc:0.958]
Epoch [88/120    avg_loss:0.059, val_acc:0.964]
Epoch [89/120    avg_loss:0.038, val_acc:0.958]
Epoch [90/120    avg_loss:0.048, val_acc:0.968]
Epoch [91/120    avg_loss:0.056, val_acc:0.956]
Epoch [92/120    avg_loss:0.083, val_acc:0.953]
Epoch [93/120    avg_loss:0.069, val_acc:0.953]
Epoch [94/120    avg_loss:0.042, val_acc:0.960]
Epoch [95/120    avg_loss:0.040, val_acc:0.965]
Epoch [96/120    avg_loss:0.042, val_acc:0.971]
Epoch [97/120    avg_loss:0.034, val_acc:0.969]
Epoch [98/120    avg_loss:0.033, val_acc:0.952]
Epoch [99/120    avg_loss:0.050, val_acc:0.959]
Epoch [100/120    avg_loss:0.082, val_acc:0.960]
Epoch [101/120    avg_loss:0.056, val_acc:0.955]
Epoch [102/120    avg_loss:0.040, val_acc:0.968]
Epoch [103/120    avg_loss:0.038, val_acc:0.968]
Epoch [104/120    avg_loss:0.038, val_acc:0.961]
Epoch [105/120    avg_loss:0.042, val_acc:0.952]
Epoch [106/120    avg_loss:0.040, val_acc:0.962]
Epoch [107/120    avg_loss:0.047, val_acc:0.967]
Epoch [108/120    avg_loss:0.038, val_acc:0.965]
Epoch [109/120    avg_loss:0.031, val_acc:0.963]
Epoch [110/120    avg_loss:0.029, val_acc:0.969]
Epoch [111/120    avg_loss:0.026, val_acc:0.974]
Epoch [112/120    avg_loss:0.026, val_acc:0.975]
Epoch [113/120    avg_loss:0.018, val_acc:0.973]
Epoch [114/120    avg_loss:0.028, val_acc:0.973]
Epoch [115/120    avg_loss:0.022, val_acc:0.975]
Epoch [116/120    avg_loss:0.022, val_acc:0.974]
Epoch [117/120    avg_loss:0.020, val_acc:0.975]
Epoch [118/120    avg_loss:0.017, val_acc:0.975]
Epoch [119/120    avg_loss:0.018, val_acc:0.975]
Epoch [120/120    avg_loss:0.024, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    1    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0   13 1212    2    0    0    2    0    0    0   13   41    0    0
     0    2    0]
 [   0    0    6  714    1    8    0    0    0    6    0    0   10    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    1    5    0    6    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    4    0    0   11    0    1    2    0
     0    0    0]
 [   0    0   28   90    0    9    0    0    0    0  743    0    0    0
     0    5    0]
 [   0    0   15    0    0    1   16    0    0    0    5 2170    1    2
     0    0    0]
 [   0    0    0   33    7   17    0    0    0    0    6    0  467    0
     0    2    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    0    0    0
  1133    3    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    95  236    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.65582655826559

F1 scores:
[       nan 0.81318681 0.95170789 0.90037831 0.98156682 0.94512878
 0.96812454 0.90909091 1.         0.53658537 0.90169903 0.98101266
 0.91929134 0.98659517 0.95611814 0.79327731 0.97619048]

Kappa:
0.9390411342010251
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff8e2c44860>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.777, val_acc:0.299]
Epoch [2/120    avg_loss:2.498, val_acc:0.450]
Epoch [3/120    avg_loss:2.353, val_acc:0.460]
Epoch [4/120    avg_loss:2.171, val_acc:0.478]
Epoch [5/120    avg_loss:2.058, val_acc:0.489]
Epoch [6/120    avg_loss:1.959, val_acc:0.524]
Epoch [7/120    avg_loss:1.871, val_acc:0.594]
Epoch [8/120    avg_loss:1.716, val_acc:0.569]
Epoch [9/120    avg_loss:1.600, val_acc:0.572]
Epoch [10/120    avg_loss:1.531, val_acc:0.572]
Epoch [11/120    avg_loss:1.410, val_acc:0.582]
Epoch [12/120    avg_loss:1.304, val_acc:0.600]
Epoch [13/120    avg_loss:1.156, val_acc:0.646]
Epoch [14/120    avg_loss:1.126, val_acc:0.662]
Epoch [15/120    avg_loss:0.992, val_acc:0.695]
Epoch [16/120    avg_loss:0.909, val_acc:0.697]
Epoch [17/120    avg_loss:0.851, val_acc:0.683]
Epoch [18/120    avg_loss:0.800, val_acc:0.716]
Epoch [19/120    avg_loss:0.764, val_acc:0.744]
Epoch [20/120    avg_loss:0.836, val_acc:0.726]
Epoch [21/120    avg_loss:0.623, val_acc:0.793]
Epoch [22/120    avg_loss:0.536, val_acc:0.805]
Epoch [23/120    avg_loss:0.554, val_acc:0.817]
Epoch [24/120    avg_loss:0.485, val_acc:0.848]
Epoch [25/120    avg_loss:0.436, val_acc:0.855]
Epoch [26/120    avg_loss:0.372, val_acc:0.858]
Epoch [27/120    avg_loss:0.419, val_acc:0.829]
Epoch [28/120    avg_loss:0.373, val_acc:0.815]
Epoch [29/120    avg_loss:0.335, val_acc:0.824]
Epoch [30/120    avg_loss:0.339, val_acc:0.875]
Epoch [31/120    avg_loss:0.304, val_acc:0.887]
Epoch [32/120    avg_loss:0.297, val_acc:0.890]
Epoch [33/120    avg_loss:0.219, val_acc:0.902]
Epoch [34/120    avg_loss:0.193, val_acc:0.907]
Epoch [35/120    avg_loss:0.193, val_acc:0.912]
Epoch [36/120    avg_loss:0.154, val_acc:0.924]
Epoch [37/120    avg_loss:0.167, val_acc:0.909]
Epoch [38/120    avg_loss:0.167, val_acc:0.889]
Epoch [39/120    avg_loss:0.158, val_acc:0.912]
Epoch [40/120    avg_loss:0.146, val_acc:0.904]
Epoch [41/120    avg_loss:0.120, val_acc:0.927]
Epoch [42/120    avg_loss:0.113, val_acc:0.931]
Epoch [43/120    avg_loss:0.117, val_acc:0.931]
Epoch [44/120    avg_loss:0.110, val_acc:0.918]
Epoch [45/120    avg_loss:0.110, val_acc:0.929]
Epoch [46/120    avg_loss:0.113, val_acc:0.932]
Epoch [47/120    avg_loss:0.090, val_acc:0.942]
Epoch [48/120    avg_loss:0.077, val_acc:0.941]
Epoch [49/120    avg_loss:0.080, val_acc:0.941]
Epoch [50/120    avg_loss:0.071, val_acc:0.940]
Epoch [51/120    avg_loss:0.082, val_acc:0.940]
Epoch [52/120    avg_loss:0.078, val_acc:0.924]
Epoch [53/120    avg_loss:0.062, val_acc:0.926]
Epoch [54/120    avg_loss:0.064, val_acc:0.946]
Epoch [55/120    avg_loss:0.059, val_acc:0.953]
Epoch [56/120    avg_loss:0.074, val_acc:0.944]
Epoch [57/120    avg_loss:0.061, val_acc:0.945]
Epoch [58/120    avg_loss:0.049, val_acc:0.944]
Epoch [59/120    avg_loss:0.049, val_acc:0.947]
Epoch [60/120    avg_loss:0.054, val_acc:0.929]
Epoch [61/120    avg_loss:0.062, val_acc:0.952]
Epoch [62/120    avg_loss:0.061, val_acc:0.951]
Epoch [63/120    avg_loss:0.046, val_acc:0.944]
Epoch [64/120    avg_loss:0.042, val_acc:0.956]
Epoch [65/120    avg_loss:0.051, val_acc:0.957]
Epoch [66/120    avg_loss:0.056, val_acc:0.957]
Epoch [67/120    avg_loss:0.048, val_acc:0.968]
Epoch [68/120    avg_loss:0.048, val_acc:0.952]
Epoch [69/120    avg_loss:0.056, val_acc:0.940]
Epoch [70/120    avg_loss:0.044, val_acc:0.949]
Epoch [71/120    avg_loss:0.040, val_acc:0.954]
Epoch [72/120    avg_loss:0.052, val_acc:0.950]
Epoch [73/120    avg_loss:0.050, val_acc:0.950]
Epoch [74/120    avg_loss:0.043, val_acc:0.964]
Epoch [75/120    avg_loss:0.036, val_acc:0.947]
Epoch [76/120    avg_loss:0.062, val_acc:0.932]
Epoch [77/120    avg_loss:0.061, val_acc:0.941]
Epoch [78/120    avg_loss:0.037, val_acc:0.957]
Epoch [79/120    avg_loss:0.027, val_acc:0.957]
Epoch [80/120    avg_loss:0.036, val_acc:0.961]
Epoch [81/120    avg_loss:0.027, val_acc:0.967]
Epoch [82/120    avg_loss:0.022, val_acc:0.969]
Epoch [83/120    avg_loss:0.021, val_acc:0.967]
Epoch [84/120    avg_loss:0.017, val_acc:0.969]
Epoch [85/120    avg_loss:0.017, val_acc:0.968]
Epoch [86/120    avg_loss:0.018, val_acc:0.966]
Epoch [87/120    avg_loss:0.019, val_acc:0.966]
Epoch [88/120    avg_loss:0.024, val_acc:0.966]
Epoch [89/120    avg_loss:0.015, val_acc:0.967]
Epoch [90/120    avg_loss:0.018, val_acc:0.967]
Epoch [91/120    avg_loss:0.024, val_acc:0.967]
Epoch [92/120    avg_loss:0.017, val_acc:0.967]
Epoch [93/120    avg_loss:0.018, val_acc:0.968]
Epoch [94/120    avg_loss:0.018, val_acc:0.968]
Epoch [95/120    avg_loss:0.020, val_acc:0.967]
Epoch [96/120    avg_loss:0.017, val_acc:0.971]
Epoch [97/120    avg_loss:0.016, val_acc:0.967]
Epoch [98/120    avg_loss:0.035, val_acc:0.968]
Epoch [99/120    avg_loss:0.025, val_acc:0.966]
Epoch [100/120    avg_loss:0.034, val_acc:0.971]
Epoch [101/120    avg_loss:0.022, val_acc:0.971]
Epoch [102/120    avg_loss:0.020, val_acc:0.971]
Epoch [103/120    avg_loss:0.020, val_acc:0.970]
Epoch [104/120    avg_loss:0.018, val_acc:0.971]
Epoch [105/120    avg_loss:0.021, val_acc:0.969]
Epoch [106/120    avg_loss:0.018, val_acc:0.969]
Epoch [107/120    avg_loss:0.019, val_acc:0.969]
Epoch [108/120    avg_loss:0.018, val_acc:0.970]
Epoch [109/120    avg_loss:0.017, val_acc:0.968]
Epoch [110/120    avg_loss:0.016, val_acc:0.969]
Epoch [111/120    avg_loss:0.017, val_acc:0.969]
Epoch [112/120    avg_loss:0.016, val_acc:0.968]
Epoch [113/120    avg_loss:0.018, val_acc:0.970]
Epoch [114/120    avg_loss:0.024, val_acc:0.968]
Epoch [115/120    avg_loss:0.017, val_acc:0.971]
Epoch [116/120    avg_loss:0.015, val_acc:0.969]
Epoch [117/120    avg_loss:0.015, val_acc:0.970]
Epoch [118/120    avg_loss:0.015, val_acc:0.969]
Epoch [119/120    avg_loss:0.013, val_acc:0.971]
Epoch [120/120    avg_loss:0.016, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    1    0    0    0    0    1    0    0    1
     0    0    0]
 [   0    0 1243    2    7    0    2    0    0    0    9   22    0    0
     0    0    0]
 [   0    0    0  727    5    0    0    0    0    1    0    7    7    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    2    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    1    0    0    0    0  851   17    0    0
     0    0    0]
 [   0    0    1    0    0    0    1    0    0    0   38 2144   26    0
     0    0    0]
 [   0    0    1    3    2    0    0    0    0    0    0    4  519    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    32  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.58265582655827

F1 scores:
[       nan 0.96202532 0.98028391 0.98243243 0.96583144 0.99425947
 0.99168556 0.97959184 1.         0.97297297 0.95941375 0.97343927
 0.9558011  0.99730458 0.98129622 0.92399404 0.98823529]

Kappa:
0.9724516731366025
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f41edcfb7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.786, val_acc:0.267]
Epoch [2/120    avg_loss:2.508, val_acc:0.405]
Epoch [3/120    avg_loss:2.331, val_acc:0.523]
Epoch [4/120    avg_loss:2.171, val_acc:0.537]
Epoch [5/120    avg_loss:2.017, val_acc:0.510]
Epoch [6/120    avg_loss:1.955, val_acc:0.546]
Epoch [7/120    avg_loss:1.825, val_acc:0.545]
Epoch [8/120    avg_loss:1.749, val_acc:0.593]
Epoch [9/120    avg_loss:1.616, val_acc:0.597]
Epoch [10/120    avg_loss:1.531, val_acc:0.614]
Epoch [11/120    avg_loss:1.430, val_acc:0.637]
Epoch [12/120    avg_loss:1.308, val_acc:0.681]
Epoch [13/120    avg_loss:1.212, val_acc:0.673]
Epoch [14/120    avg_loss:1.158, val_acc:0.698]
Epoch [15/120    avg_loss:1.129, val_acc:0.670]
Epoch [16/120    avg_loss:1.072, val_acc:0.728]
Epoch [17/120    avg_loss:0.975, val_acc:0.684]
Epoch [18/120    avg_loss:0.935, val_acc:0.705]
Epoch [19/120    avg_loss:0.821, val_acc:0.744]
Epoch [20/120    avg_loss:0.729, val_acc:0.765]
Epoch [21/120    avg_loss:0.697, val_acc:0.775]
Epoch [22/120    avg_loss:0.645, val_acc:0.789]
Epoch [23/120    avg_loss:0.567, val_acc:0.814]
Epoch [24/120    avg_loss:0.526, val_acc:0.824]
Epoch [25/120    avg_loss:0.476, val_acc:0.830]
Epoch [26/120    avg_loss:0.418, val_acc:0.843]
Epoch [27/120    avg_loss:0.519, val_acc:0.831]
Epoch [28/120    avg_loss:0.491, val_acc:0.849]
Epoch [29/120    avg_loss:0.376, val_acc:0.855]
Epoch [30/120    avg_loss:0.329, val_acc:0.840]
Epoch [31/120    avg_loss:0.431, val_acc:0.794]
Epoch [32/120    avg_loss:0.403, val_acc:0.855]
Epoch [33/120    avg_loss:0.300, val_acc:0.889]
Epoch [34/120    avg_loss:0.244, val_acc:0.886]
Epoch [35/120    avg_loss:0.207, val_acc:0.903]
Epoch [36/120    avg_loss:0.204, val_acc:0.906]
Epoch [37/120    avg_loss:0.170, val_acc:0.912]
Epoch [38/120    avg_loss:0.170, val_acc:0.919]
Epoch [39/120    avg_loss:0.162, val_acc:0.909]
Epoch [40/120    avg_loss:0.153, val_acc:0.921]
Epoch [41/120    avg_loss:0.150, val_acc:0.895]
Epoch [42/120    avg_loss:0.152, val_acc:0.919]
Epoch [43/120    avg_loss:0.158, val_acc:0.912]
Epoch [44/120    avg_loss:0.135, val_acc:0.930]
Epoch [45/120    avg_loss:0.105, val_acc:0.939]
Epoch [46/120    avg_loss:0.133, val_acc:0.929]
Epoch [47/120    avg_loss:0.125, val_acc:0.939]
Epoch [48/120    avg_loss:0.113, val_acc:0.953]
Epoch [49/120    avg_loss:0.088, val_acc:0.956]
Epoch [50/120    avg_loss:0.080, val_acc:0.958]
Epoch [51/120    avg_loss:0.079, val_acc:0.948]
Epoch [52/120    avg_loss:0.124, val_acc:0.931]
Epoch [53/120    avg_loss:0.082, val_acc:0.951]
Epoch [54/120    avg_loss:0.074, val_acc:0.948]
Epoch [55/120    avg_loss:0.086, val_acc:0.949]
Epoch [56/120    avg_loss:0.092, val_acc:0.948]
Epoch [57/120    avg_loss:0.073, val_acc:0.957]
Epoch [58/120    avg_loss:0.050, val_acc:0.961]
Epoch [59/120    avg_loss:0.047, val_acc:0.961]
Epoch [60/120    avg_loss:0.043, val_acc:0.961]
Epoch [61/120    avg_loss:0.050, val_acc:0.958]
Epoch [62/120    avg_loss:0.054, val_acc:0.960]
Epoch [63/120    avg_loss:0.047, val_acc:0.966]
Epoch [64/120    avg_loss:0.051, val_acc:0.955]
Epoch [65/120    avg_loss:0.044, val_acc:0.967]
Epoch [66/120    avg_loss:0.049, val_acc:0.961]
Epoch [67/120    avg_loss:0.035, val_acc:0.970]
Epoch [68/120    avg_loss:0.029, val_acc:0.970]
Epoch [69/120    avg_loss:0.024, val_acc:0.972]
Epoch [70/120    avg_loss:0.035, val_acc:0.954]
Epoch [71/120    avg_loss:0.026, val_acc:0.968]
Epoch [72/120    avg_loss:0.027, val_acc:0.971]
Epoch [73/120    avg_loss:0.026, val_acc:0.966]
Epoch [74/120    avg_loss:0.024, val_acc:0.965]
Epoch [75/120    avg_loss:0.024, val_acc:0.970]
Epoch [76/120    avg_loss:0.024, val_acc:0.969]
Epoch [77/120    avg_loss:0.022, val_acc:0.973]
Epoch [78/120    avg_loss:0.023, val_acc:0.970]
Epoch [79/120    avg_loss:0.021, val_acc:0.968]
Epoch [80/120    avg_loss:0.019, val_acc:0.970]
Epoch [81/120    avg_loss:0.021, val_acc:0.975]
Epoch [82/120    avg_loss:0.016, val_acc:0.972]
Epoch [83/120    avg_loss:0.024, val_acc:0.970]
Epoch [84/120    avg_loss:0.020, val_acc:0.969]
Epoch [85/120    avg_loss:0.022, val_acc:0.967]
Epoch [86/120    avg_loss:0.028, val_acc:0.972]
Epoch [87/120    avg_loss:0.021, val_acc:0.974]
Epoch [88/120    avg_loss:0.025, val_acc:0.947]
Epoch [89/120    avg_loss:0.047, val_acc:0.958]
Epoch [90/120    avg_loss:0.029, val_acc:0.968]
Epoch [91/120    avg_loss:0.035, val_acc:0.967]
Epoch [92/120    avg_loss:0.032, val_acc:0.958]
Epoch [93/120    avg_loss:0.026, val_acc:0.973]
Epoch [94/120    avg_loss:0.020, val_acc:0.973]
Epoch [95/120    avg_loss:0.021, val_acc:0.974]
Epoch [96/120    avg_loss:0.014, val_acc:0.974]
Epoch [97/120    avg_loss:0.016, val_acc:0.974]
Epoch [98/120    avg_loss:0.014, val_acc:0.974]
Epoch [99/120    avg_loss:0.013, val_acc:0.974]
Epoch [100/120    avg_loss:0.014, val_acc:0.974]
Epoch [101/120    avg_loss:0.014, val_acc:0.974]
Epoch [102/120    avg_loss:0.016, val_acc:0.974]
Epoch [103/120    avg_loss:0.013, val_acc:0.974]
Epoch [104/120    avg_loss:0.011, val_acc:0.973]
Epoch [105/120    avg_loss:0.012, val_acc:0.973]
Epoch [106/120    avg_loss:0.014, val_acc:0.974]
Epoch [107/120    avg_loss:0.014, val_acc:0.975]
Epoch [108/120    avg_loss:0.012, val_acc:0.974]
Epoch [109/120    avg_loss:0.011, val_acc:0.974]
Epoch [110/120    avg_loss:0.010, val_acc:0.973]
Epoch [111/120    avg_loss:0.013, val_acc:0.974]
Epoch [112/120    avg_loss:0.010, val_acc:0.974]
Epoch [113/120    avg_loss:0.011, val_acc:0.973]
Epoch [114/120    avg_loss:0.012, val_acc:0.973]
Epoch [115/120    avg_loss:0.011, val_acc:0.974]
Epoch [116/120    avg_loss:0.010, val_acc:0.976]
Epoch [117/120    avg_loss:0.010, val_acc:0.976]
Epoch [118/120    avg_loss:0.010, val_acc:0.976]
Epoch [119/120    avg_loss:0.014, val_acc:0.973]
Epoch [120/120    avg_loss:0.012, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1272    2    0    0    0    0    0    0    0   11    0    0
     0    0    0]
 [   0    0    4  728    4    0    0    0    0    2    0    4    5    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    3    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   15    0    0    1    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0  848   26    0    0
     0    0    0]
 [   0    0   13    0    0    0    0    0    0    2   18 2161   16    0
     0    0    0]
 [   0    0    0    3    0    1    0    0    0    0    0    7  519    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1123   16    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    33  309    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.91869918699187

F1 scores:
[       nan 1.         0.98796117 0.98311951 0.98834499 0.99074074
 0.99393939 0.94339623 1.         0.81081081 0.97415279 0.97804933
 0.96289425 1.         0.97652174 0.91691395 0.97005988]

Kappa:
0.9762674443562884
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdb0ef84828>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.735, val_acc:0.290]
Epoch [2/120    avg_loss:2.471, val_acc:0.443]
Epoch [3/120    avg_loss:2.315, val_acc:0.489]
Epoch [4/120    avg_loss:2.181, val_acc:0.524]
Epoch [5/120    avg_loss:2.020, val_acc:0.519]
Epoch [6/120    avg_loss:1.908, val_acc:0.571]
Epoch [7/120    avg_loss:1.810, val_acc:0.593]
Epoch [8/120    avg_loss:1.678, val_acc:0.626]
Epoch [9/120    avg_loss:1.574, val_acc:0.659]
Epoch [10/120    avg_loss:1.460, val_acc:0.671]
Epoch [11/120    avg_loss:1.318, val_acc:0.685]
Epoch [12/120    avg_loss:1.201, val_acc:0.716]
Epoch [13/120    avg_loss:1.116, val_acc:0.714]
Epoch [14/120    avg_loss:0.966, val_acc:0.711]
Epoch [15/120    avg_loss:0.881, val_acc:0.739]
Epoch [16/120    avg_loss:0.803, val_acc:0.796]
Epoch [17/120    avg_loss:0.732, val_acc:0.728]
Epoch [18/120    avg_loss:0.659, val_acc:0.780]
Epoch [19/120    avg_loss:0.636, val_acc:0.771]
Epoch [20/120    avg_loss:0.578, val_acc:0.808]
Epoch [21/120    avg_loss:0.493, val_acc:0.776]
Epoch [22/120    avg_loss:0.460, val_acc:0.834]
Epoch [23/120    avg_loss:0.397, val_acc:0.816]
Epoch [24/120    avg_loss:0.384, val_acc:0.861]
Epoch [25/120    avg_loss:0.326, val_acc:0.880]
Epoch [26/120    avg_loss:0.358, val_acc:0.857]
Epoch [27/120    avg_loss:0.427, val_acc:0.844]
Epoch [28/120    avg_loss:0.381, val_acc:0.840]
Epoch [29/120    avg_loss:0.310, val_acc:0.899]
Epoch [30/120    avg_loss:0.299, val_acc:0.855]
Epoch [31/120    avg_loss:0.244, val_acc:0.901]
Epoch [32/120    avg_loss:0.208, val_acc:0.903]
Epoch [33/120    avg_loss:0.198, val_acc:0.918]
Epoch [34/120    avg_loss:0.174, val_acc:0.930]
Epoch [35/120    avg_loss:0.156, val_acc:0.930]
Epoch [36/120    avg_loss:0.149, val_acc:0.921]
Epoch [37/120    avg_loss:0.246, val_acc:0.844]
Epoch [38/120    avg_loss:0.262, val_acc:0.859]
Epoch [39/120    avg_loss:0.295, val_acc:0.899]
Epoch [40/120    avg_loss:0.192, val_acc:0.921]
Epoch [41/120    avg_loss:0.158, val_acc:0.931]
Epoch [42/120    avg_loss:0.148, val_acc:0.924]
Epoch [43/120    avg_loss:0.124, val_acc:0.938]
Epoch [44/120    avg_loss:0.107, val_acc:0.928]
Epoch [45/120    avg_loss:0.145, val_acc:0.929]
Epoch [46/120    avg_loss:0.121, val_acc:0.938]
Epoch [47/120    avg_loss:0.120, val_acc:0.932]
Epoch [48/120    avg_loss:0.143, val_acc:0.941]
Epoch [49/120    avg_loss:0.098, val_acc:0.933]
Epoch [50/120    avg_loss:0.083, val_acc:0.951]
Epoch [51/120    avg_loss:0.065, val_acc:0.956]
Epoch [52/120    avg_loss:0.094, val_acc:0.959]
Epoch [53/120    avg_loss:0.074, val_acc:0.959]
Epoch [54/120    avg_loss:0.068, val_acc:0.956]
Epoch [55/120    avg_loss:0.063, val_acc:0.950]
Epoch [56/120    avg_loss:0.060, val_acc:0.968]
Epoch [57/120    avg_loss:0.044, val_acc:0.966]
Epoch [58/120    avg_loss:0.059, val_acc:0.961]
Epoch [59/120    avg_loss:0.059, val_acc:0.957]
Epoch [60/120    avg_loss:0.054, val_acc:0.958]
Epoch [61/120    avg_loss:0.048, val_acc:0.952]
Epoch [62/120    avg_loss:0.056, val_acc:0.959]
Epoch [63/120    avg_loss:0.055, val_acc:0.957]
Epoch [64/120    avg_loss:0.054, val_acc:0.959]
Epoch [65/120    avg_loss:0.046, val_acc:0.951]
Epoch [66/120    avg_loss:0.036, val_acc:0.964]
Epoch [67/120    avg_loss:0.036, val_acc:0.969]
Epoch [68/120    avg_loss:0.035, val_acc:0.970]
Epoch [69/120    avg_loss:0.043, val_acc:0.961]
Epoch [70/120    avg_loss:0.042, val_acc:0.958]
Epoch [71/120    avg_loss:0.043, val_acc:0.968]
Epoch [72/120    avg_loss:0.053, val_acc:0.956]
Epoch [73/120    avg_loss:0.033, val_acc:0.967]
Epoch [74/120    avg_loss:0.043, val_acc:0.943]
Epoch [75/120    avg_loss:0.065, val_acc:0.956]
Epoch [76/120    avg_loss:0.045, val_acc:0.946]
Epoch [77/120    avg_loss:0.039, val_acc:0.972]
Epoch [78/120    avg_loss:0.042, val_acc:0.964]
Epoch [79/120    avg_loss:0.035, val_acc:0.964]
Epoch [80/120    avg_loss:0.041, val_acc:0.973]
Epoch [81/120    avg_loss:0.046, val_acc:0.960]
Epoch [82/120    avg_loss:0.031, val_acc:0.961]
Epoch [83/120    avg_loss:0.037, val_acc:0.961]
Epoch [84/120    avg_loss:0.025, val_acc:0.970]
Epoch [85/120    avg_loss:0.021, val_acc:0.971]
Epoch [86/120    avg_loss:0.017, val_acc:0.973]
Epoch [87/120    avg_loss:0.026, val_acc:0.973]
Epoch [88/120    avg_loss:0.031, val_acc:0.966]
Epoch [89/120    avg_loss:0.036, val_acc:0.975]
Epoch [90/120    avg_loss:0.038, val_acc:0.965]
Epoch [91/120    avg_loss:0.025, val_acc:0.968]
Epoch [92/120    avg_loss:0.019, val_acc:0.977]
Epoch [93/120    avg_loss:0.019, val_acc:0.976]
Epoch [94/120    avg_loss:0.023, val_acc:0.966]
Epoch [95/120    avg_loss:0.029, val_acc:0.967]
Epoch [96/120    avg_loss:0.023, val_acc:0.969]
Epoch [97/120    avg_loss:0.020, val_acc:0.972]
Epoch [98/120    avg_loss:0.017, val_acc:0.970]
Epoch [99/120    avg_loss:0.014, val_acc:0.968]
Epoch [100/120    avg_loss:0.015, val_acc:0.973]
Epoch [101/120    avg_loss:0.019, val_acc:0.974]
Epoch [102/120    avg_loss:0.019, val_acc:0.973]
Epoch [103/120    avg_loss:0.015, val_acc:0.974]
Epoch [104/120    avg_loss:0.013, val_acc:0.978]
Epoch [105/120    avg_loss:0.016, val_acc:0.974]
Epoch [106/120    avg_loss:0.020, val_acc:0.974]
Epoch [107/120    avg_loss:0.019, val_acc:0.973]
Epoch [108/120    avg_loss:0.023, val_acc:0.972]
Epoch [109/120    avg_loss:0.025, val_acc:0.969]
Epoch [110/120    avg_loss:0.017, val_acc:0.969]
Epoch [111/120    avg_loss:0.016, val_acc:0.976]
Epoch [112/120    avg_loss:0.013, val_acc:0.978]
Epoch [113/120    avg_loss:0.014, val_acc:0.976]
Epoch [114/120    avg_loss:0.016, val_acc:0.974]
Epoch [115/120    avg_loss:0.013, val_acc:0.974]
Epoch [116/120    avg_loss:0.017, val_acc:0.968]
Epoch [117/120    avg_loss:0.014, val_acc:0.975]
Epoch [118/120    avg_loss:0.012, val_acc:0.974]
Epoch [119/120    avg_loss:0.013, val_acc:0.972]
Epoch [120/120    avg_loss:0.014, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    2    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    0    0    8    1    0    0    0    2    7    0    0
     0    0    0]
 [   0    0    3  720   12    0    0    0    0    1    2    9    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    2    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    0    0    0    0    0  853   13    0    0
     0    0    0]
 [   0    0   35    0    0    0    0    0    0    0   25 2135   10    3
     1    1    0]
 [   0    0    0    9    0    0    0    0    0    0    1    5  516    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1122   17    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    47  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
97.38753387533875

F1 scores:
[       nan 0.975      0.97499038 0.97560976 0.97260274 0.98052692
 0.99544765 1.         1.         0.97297297 0.97042093 0.97510847
 0.96901408 0.9919571  0.96933045 0.8962406  0.96341463]

Kappa:
0.9702245568438117
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff117721898>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.833, val_acc:0.257]
Epoch [2/120    avg_loss:2.536, val_acc:0.403]
Epoch [3/120    avg_loss:2.358, val_acc:0.499]
Epoch [4/120    avg_loss:2.161, val_acc:0.552]
Epoch [5/120    avg_loss:2.036, val_acc:0.543]
Epoch [6/120    avg_loss:1.942, val_acc:0.560]
Epoch [7/120    avg_loss:1.836, val_acc:0.539]
Epoch [8/120    avg_loss:1.714, val_acc:0.579]
Epoch [9/120    avg_loss:1.648, val_acc:0.613]
Epoch [10/120    avg_loss:1.526, val_acc:0.610]
Epoch [11/120    avg_loss:1.415, val_acc:0.658]
Epoch [12/120    avg_loss:1.384, val_acc:0.635]
Epoch [13/120    avg_loss:1.206, val_acc:0.666]
Epoch [14/120    avg_loss:1.121, val_acc:0.690]
Epoch [15/120    avg_loss:1.059, val_acc:0.697]
Epoch [16/120    avg_loss:0.969, val_acc:0.739]
Epoch [17/120    avg_loss:0.943, val_acc:0.702]
Epoch [18/120    avg_loss:0.853, val_acc:0.781]
Epoch [19/120    avg_loss:0.803, val_acc:0.759]
Epoch [20/120    avg_loss:0.733, val_acc:0.760]
Epoch [21/120    avg_loss:0.677, val_acc:0.780]
Epoch [22/120    avg_loss:0.637, val_acc:0.801]
Epoch [23/120    avg_loss:0.585, val_acc:0.805]
Epoch [24/120    avg_loss:0.498, val_acc:0.838]
Epoch [25/120    avg_loss:0.459, val_acc:0.845]
Epoch [26/120    avg_loss:0.385, val_acc:0.878]
Epoch [27/120    avg_loss:0.348, val_acc:0.889]
Epoch [28/120    avg_loss:0.336, val_acc:0.870]
Epoch [29/120    avg_loss:0.342, val_acc:0.890]
Epoch [30/120    avg_loss:0.316, val_acc:0.883]
Epoch [31/120    avg_loss:0.268, val_acc:0.870]
Epoch [32/120    avg_loss:0.252, val_acc:0.905]
Epoch [33/120    avg_loss:0.256, val_acc:0.895]
Epoch [34/120    avg_loss:0.272, val_acc:0.858]
Epoch [35/120    avg_loss:0.355, val_acc:0.866]
Epoch [36/120    avg_loss:0.375, val_acc:0.870]
Epoch [37/120    avg_loss:0.359, val_acc:0.884]
Epoch [38/120    avg_loss:0.274, val_acc:0.915]
Epoch [39/120    avg_loss:0.189, val_acc:0.924]
Epoch [40/120    avg_loss:0.166, val_acc:0.916]
Epoch [41/120    avg_loss:0.171, val_acc:0.914]
Epoch [42/120    avg_loss:0.162, val_acc:0.932]
Epoch [43/120    avg_loss:0.138, val_acc:0.941]
Epoch [44/120    avg_loss:0.131, val_acc:0.896]
Epoch [45/120    avg_loss:0.124, val_acc:0.942]
Epoch [46/120    avg_loss:0.097, val_acc:0.941]
Epoch [47/120    avg_loss:0.085, val_acc:0.942]
Epoch [48/120    avg_loss:0.087, val_acc:0.939]
Epoch [49/120    avg_loss:0.107, val_acc:0.933]
Epoch [50/120    avg_loss:0.092, val_acc:0.945]
Epoch [51/120    avg_loss:0.080, val_acc:0.935]
Epoch [52/120    avg_loss:0.071, val_acc:0.950]
Epoch [53/120    avg_loss:0.075, val_acc:0.935]
Epoch [54/120    avg_loss:0.081, val_acc:0.942]
Epoch [55/120    avg_loss:0.073, val_acc:0.944]
Epoch [56/120    avg_loss:0.079, val_acc:0.957]
Epoch [57/120    avg_loss:0.085, val_acc:0.950]
Epoch [58/120    avg_loss:0.080, val_acc:0.965]
Epoch [59/120    avg_loss:0.057, val_acc:0.959]
Epoch [60/120    avg_loss:0.072, val_acc:0.947]
Epoch [61/120    avg_loss:0.068, val_acc:0.958]
Epoch [62/120    avg_loss:0.083, val_acc:0.957]
Epoch [63/120    avg_loss:0.061, val_acc:0.963]
Epoch [64/120    avg_loss:0.058, val_acc:0.964]
Epoch [65/120    avg_loss:0.045, val_acc:0.945]
Epoch [66/120    avg_loss:0.054, val_acc:0.954]
Epoch [67/120    avg_loss:0.038, val_acc:0.970]
Epoch [68/120    avg_loss:0.041, val_acc:0.958]
Epoch [69/120    avg_loss:0.040, val_acc:0.968]
Epoch [70/120    avg_loss:0.042, val_acc:0.967]
Epoch [71/120    avg_loss:0.051, val_acc:0.957]
Epoch [72/120    avg_loss:0.056, val_acc:0.956]
Epoch [73/120    avg_loss:0.041, val_acc:0.960]
Epoch [74/120    avg_loss:0.040, val_acc:0.964]
Epoch [75/120    avg_loss:0.050, val_acc:0.968]
Epoch [76/120    avg_loss:0.031, val_acc:0.970]
Epoch [77/120    avg_loss:0.035, val_acc:0.975]
Epoch [78/120    avg_loss:0.030, val_acc:0.974]
Epoch [79/120    avg_loss:0.030, val_acc:0.974]
Epoch [80/120    avg_loss:0.030, val_acc:0.974]
Epoch [81/120    avg_loss:0.029, val_acc:0.977]
Epoch [82/120    avg_loss:0.030, val_acc:0.972]
Epoch [83/120    avg_loss:0.031, val_acc:0.961]
Epoch [84/120    avg_loss:0.035, val_acc:0.973]
Epoch [85/120    avg_loss:0.020, val_acc:0.978]
Epoch [86/120    avg_loss:0.020, val_acc:0.966]
Epoch [87/120    avg_loss:0.023, val_acc:0.967]
Epoch [88/120    avg_loss:0.020, val_acc:0.972]
Epoch [89/120    avg_loss:0.023, val_acc:0.971]
Epoch [90/120    avg_loss:0.027, val_acc:0.970]
Epoch [91/120    avg_loss:0.022, val_acc:0.968]
Epoch [92/120    avg_loss:0.019, val_acc:0.974]
Epoch [93/120    avg_loss:0.019, val_acc:0.973]
Epoch [94/120    avg_loss:0.021, val_acc:0.974]
Epoch [95/120    avg_loss:0.017, val_acc:0.976]
Epoch [96/120    avg_loss:0.016, val_acc:0.977]
Epoch [97/120    avg_loss:0.017, val_acc:0.976]
Epoch [98/120    avg_loss:0.017, val_acc:0.977]
Epoch [99/120    avg_loss:0.011, val_acc:0.978]
Epoch [100/120    avg_loss:0.013, val_acc:0.977]
Epoch [101/120    avg_loss:0.014, val_acc:0.978]
Epoch [102/120    avg_loss:0.013, val_acc:0.979]
Epoch [103/120    avg_loss:0.012, val_acc:0.981]
Epoch [104/120    avg_loss:0.011, val_acc:0.982]
Epoch [105/120    avg_loss:0.009, val_acc:0.982]
Epoch [106/120    avg_loss:0.013, val_acc:0.980]
Epoch [107/120    avg_loss:0.011, val_acc:0.981]
Epoch [108/120    avg_loss:0.011, val_acc:0.981]
Epoch [109/120    avg_loss:0.011, val_acc:0.981]
Epoch [110/120    avg_loss:0.011, val_acc:0.982]
Epoch [111/120    avg_loss:0.013, val_acc:0.983]
Epoch [112/120    avg_loss:0.011, val_acc:0.981]
Epoch [113/120    avg_loss:0.012, val_acc:0.982]
Epoch [114/120    avg_loss:0.013, val_acc:0.980]
Epoch [115/120    avg_loss:0.014, val_acc:0.983]
Epoch [116/120    avg_loss:0.010, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.981]
Epoch [118/120    avg_loss:0.010, val_acc:0.981]
Epoch [119/120    avg_loss:0.010, val_acc:0.981]
Epoch [120/120    avg_loss:0.012, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1265    6    0    0    0    0    0    0    0   11    3    0
     0    0    0]
 [   0    0    0  715    1    0    0    0    0    2    1    4   17    7
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  420    0    0    0    0    0    1    0    0
    14    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0  854   18    1    0
     0    1    0]
 [   0    0   26    0    0    1    0    0    0    0    6 2172    5    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    0    2  525    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
  1119   18    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    37  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.8319783197832

F1 scores:
[       nan 0.98765432 0.98176174 0.9721278  0.99765808 0.97902098
 0.99543379 1.         1.         0.85714286 0.98330455 0.98280543
 0.96685083 0.98143236 0.96883117 0.91445428 0.98224852]

Kappa:
0.9752768675927921
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8d5e976898>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.743, val_acc:0.346]
Epoch [2/120    avg_loss:2.520, val_acc:0.389]
Epoch [3/120    avg_loss:2.331, val_acc:0.422]
Epoch [4/120    avg_loss:2.202, val_acc:0.459]
Epoch [5/120    avg_loss:2.058, val_acc:0.455]
Epoch [6/120    avg_loss:1.930, val_acc:0.524]
Epoch [7/120    avg_loss:1.841, val_acc:0.521]
Epoch [8/120    avg_loss:1.729, val_acc:0.550]
Epoch [9/120    avg_loss:1.618, val_acc:0.608]
Epoch [10/120    avg_loss:1.513, val_acc:0.625]
Epoch [11/120    avg_loss:1.430, val_acc:0.564]
Epoch [12/120    avg_loss:1.332, val_acc:0.642]
Epoch [13/120    avg_loss:1.269, val_acc:0.668]
Epoch [14/120    avg_loss:1.142, val_acc:0.673]
Epoch [15/120    avg_loss:1.021, val_acc:0.710]
Epoch [16/120    avg_loss:0.951, val_acc:0.717]
Epoch [17/120    avg_loss:0.886, val_acc:0.717]
Epoch [18/120    avg_loss:0.786, val_acc:0.778]
Epoch [19/120    avg_loss:0.706, val_acc:0.783]
Epoch [20/120    avg_loss:0.612, val_acc:0.791]
Epoch [21/120    avg_loss:0.520, val_acc:0.828]
Epoch [22/120    avg_loss:0.487, val_acc:0.781]
Epoch [23/120    avg_loss:0.481, val_acc:0.810]
Epoch [24/120    avg_loss:0.444, val_acc:0.849]
Epoch [25/120    avg_loss:0.382, val_acc:0.856]
Epoch [26/120    avg_loss:0.354, val_acc:0.881]
Epoch [27/120    avg_loss:0.298, val_acc:0.875]
Epoch [28/120    avg_loss:0.273, val_acc:0.899]
Epoch [29/120    avg_loss:0.245, val_acc:0.904]
Epoch [30/120    avg_loss:0.240, val_acc:0.900]
Epoch [31/120    avg_loss:0.193, val_acc:0.916]
Epoch [32/120    avg_loss:0.197, val_acc:0.894]
Epoch [33/120    avg_loss:0.181, val_acc:0.921]
Epoch [34/120    avg_loss:0.162, val_acc:0.918]
Epoch [35/120    avg_loss:0.362, val_acc:0.906]
Epoch [36/120    avg_loss:0.407, val_acc:0.872]
Epoch [37/120    avg_loss:0.246, val_acc:0.881]
Epoch [38/120    avg_loss:0.230, val_acc:0.905]
Epoch [39/120    avg_loss:0.170, val_acc:0.917]
Epoch [40/120    avg_loss:0.142, val_acc:0.902]
Epoch [41/120    avg_loss:0.187, val_acc:0.915]
Epoch [42/120    avg_loss:0.161, val_acc:0.920]
Epoch [43/120    avg_loss:0.111, val_acc:0.927]
Epoch [44/120    avg_loss:0.146, val_acc:0.924]
Epoch [45/120    avg_loss:0.101, val_acc:0.941]
Epoch [46/120    avg_loss:0.099, val_acc:0.934]
Epoch [47/120    avg_loss:0.097, val_acc:0.945]
Epoch [48/120    avg_loss:0.073, val_acc:0.945]
Epoch [49/120    avg_loss:0.072, val_acc:0.959]
Epoch [50/120    avg_loss:0.067, val_acc:0.953]
Epoch [51/120    avg_loss:0.058, val_acc:0.951]
Epoch [52/120    avg_loss:0.066, val_acc:0.961]
Epoch [53/120    avg_loss:0.070, val_acc:0.948]
Epoch [54/120    avg_loss:0.186, val_acc:0.854]
Epoch [55/120    avg_loss:0.272, val_acc:0.899]
Epoch [56/120    avg_loss:0.133, val_acc:0.931]
Epoch [57/120    avg_loss:0.107, val_acc:0.949]
Epoch [58/120    avg_loss:0.080, val_acc:0.943]
Epoch [59/120    avg_loss:0.065, val_acc:0.952]
Epoch [60/120    avg_loss:0.057, val_acc:0.956]
Epoch [61/120    avg_loss:0.059, val_acc:0.958]
Epoch [62/120    avg_loss:0.063, val_acc:0.952]
Epoch [63/120    avg_loss:0.041, val_acc:0.964]
Epoch [64/120    avg_loss:0.057, val_acc:0.951]
Epoch [65/120    avg_loss:0.042, val_acc:0.961]
Epoch [66/120    avg_loss:0.038, val_acc:0.963]
Epoch [67/120    avg_loss:0.041, val_acc:0.970]
Epoch [68/120    avg_loss:0.042, val_acc:0.969]
Epoch [69/120    avg_loss:0.038, val_acc:0.969]
Epoch [70/120    avg_loss:0.033, val_acc:0.974]
Epoch [71/120    avg_loss:0.030, val_acc:0.956]
Epoch [72/120    avg_loss:0.036, val_acc:0.949]
Epoch [73/120    avg_loss:0.030, val_acc:0.976]
Epoch [74/120    avg_loss:0.023, val_acc:0.967]
Epoch [75/120    avg_loss:0.019, val_acc:0.974]
Epoch [76/120    avg_loss:0.019, val_acc:0.975]
Epoch [77/120    avg_loss:0.024, val_acc:0.973]
Epoch [78/120    avg_loss:0.020, val_acc:0.976]
Epoch [79/120    avg_loss:0.018, val_acc:0.975]
Epoch [80/120    avg_loss:0.016, val_acc:0.982]
Epoch [81/120    avg_loss:0.015, val_acc:0.975]
Epoch [82/120    avg_loss:0.016, val_acc:0.976]
Epoch [83/120    avg_loss:0.020, val_acc:0.979]
Epoch [84/120    avg_loss:0.017, val_acc:0.975]
Epoch [85/120    avg_loss:0.014, val_acc:0.973]
Epoch [86/120    avg_loss:0.020, val_acc:0.978]
Epoch [87/120    avg_loss:0.018, val_acc:0.978]
Epoch [88/120    avg_loss:0.017, val_acc:0.977]
Epoch [89/120    avg_loss:0.015, val_acc:0.974]
Epoch [90/120    avg_loss:0.021, val_acc:0.979]
Epoch [91/120    avg_loss:0.016, val_acc:0.973]
Epoch [92/120    avg_loss:0.019, val_acc:0.960]
Epoch [93/120    avg_loss:0.023, val_acc:0.968]
Epoch [94/120    avg_loss:0.020, val_acc:0.972]
Epoch [95/120    avg_loss:0.017, val_acc:0.976]
Epoch [96/120    avg_loss:0.022, val_acc:0.980]
Epoch [97/120    avg_loss:0.011, val_acc:0.980]
Epoch [98/120    avg_loss:0.012, val_acc:0.982]
Epoch [99/120    avg_loss:0.019, val_acc:0.981]
Epoch [100/120    avg_loss:0.014, val_acc:0.980]
Epoch [101/120    avg_loss:0.013, val_acc:0.978]
Epoch [102/120    avg_loss:0.014, val_acc:0.980]
Epoch [103/120    avg_loss:0.011, val_acc:0.979]
Epoch [104/120    avg_loss:0.011, val_acc:0.981]
Epoch [105/120    avg_loss:0.011, val_acc:0.983]
Epoch [106/120    avg_loss:0.012, val_acc:0.981]
Epoch [107/120    avg_loss:0.012, val_acc:0.979]
Epoch [108/120    avg_loss:0.010, val_acc:0.981]
Epoch [109/120    avg_loss:0.012, val_acc:0.982]
Epoch [110/120    avg_loss:0.010, val_acc:0.982]
Epoch [111/120    avg_loss:0.012, val_acc:0.982]
Epoch [112/120    avg_loss:0.012, val_acc:0.981]
Epoch [113/120    avg_loss:0.008, val_acc:0.981]
Epoch [114/120    avg_loss:0.011, val_acc:0.982]
Epoch [115/120    avg_loss:0.012, val_acc:0.982]
Epoch [116/120    avg_loss:0.011, val_acc:0.982]
Epoch [117/120    avg_loss:0.010, val_acc:0.983]
Epoch [118/120    avg_loss:0.012, val_acc:0.983]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.011, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1245    2    7    0    1    0    0    0    4   26    0    0
     0    0    0]
 [   0    0    0  719    3    6    0    0    0    3    0   12    2    2
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    1    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    1    1    0    1    0    0    0    0  852   19    0    0
     0    1    0]
 [   0    0    4    1    0    0    0    0    0    0   28 2154   21    2
     0    0    0]
 [   0    0    0    1    0    2    0    0    0    0    0    6  521    0
     1    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    50  295    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.48509485094851

F1 scores:
[       nan 0.975      0.9818612  0.97756628 0.97471264 0.98630137
 0.99619772 1.         1.         0.89473684 0.96763203 0.97268006
 0.96481481 0.98930481 0.97243755 0.90214067 0.98224852]

Kappa:
0.9713211702506226
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1bd32f4898>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.758, val_acc:0.294]
Epoch [2/120    avg_loss:2.454, val_acc:0.391]
Epoch [3/120    avg_loss:2.314, val_acc:0.404]
Epoch [4/120    avg_loss:2.167, val_acc:0.443]
Epoch [5/120    avg_loss:2.035, val_acc:0.526]
Epoch [6/120    avg_loss:1.929, val_acc:0.528]
Epoch [7/120    avg_loss:1.831, val_acc:0.536]
Epoch [8/120    avg_loss:1.707, val_acc:0.544]
Epoch [9/120    avg_loss:1.606, val_acc:0.613]
Epoch [10/120    avg_loss:1.464, val_acc:0.611]
Epoch [11/120    avg_loss:1.374, val_acc:0.642]
Epoch [12/120    avg_loss:1.292, val_acc:0.648]
Epoch [13/120    avg_loss:1.129, val_acc:0.665]
Epoch [14/120    avg_loss:1.068, val_acc:0.703]
Epoch [15/120    avg_loss:0.964, val_acc:0.692]
Epoch [16/120    avg_loss:0.931, val_acc:0.736]
Epoch [17/120    avg_loss:0.812, val_acc:0.756]
Epoch [18/120    avg_loss:0.718, val_acc:0.767]
Epoch [19/120    avg_loss:0.629, val_acc:0.776]
Epoch [20/120    avg_loss:0.646, val_acc:0.787]
Epoch [21/120    avg_loss:0.672, val_acc:0.762]
Epoch [22/120    avg_loss:0.571, val_acc:0.804]
Epoch [23/120    avg_loss:0.490, val_acc:0.836]
Epoch [24/120    avg_loss:0.427, val_acc:0.845]
Epoch [25/120    avg_loss:0.405, val_acc:0.864]
Epoch [26/120    avg_loss:0.365, val_acc:0.861]
Epoch [27/120    avg_loss:0.336, val_acc:0.846]
Epoch [28/120    avg_loss:0.353, val_acc:0.849]
Epoch [29/120    avg_loss:0.332, val_acc:0.871]
Epoch [30/120    avg_loss:0.273, val_acc:0.884]
Epoch [31/120    avg_loss:0.279, val_acc:0.877]
Epoch [32/120    avg_loss:0.252, val_acc:0.900]
Epoch [33/120    avg_loss:0.208, val_acc:0.909]
Epoch [34/120    avg_loss:0.197, val_acc:0.909]
Epoch [35/120    avg_loss:0.195, val_acc:0.923]
Epoch [36/120    avg_loss:0.166, val_acc:0.918]
Epoch [37/120    avg_loss:0.166, val_acc:0.914]
Epoch [38/120    avg_loss:0.140, val_acc:0.933]
Epoch [39/120    avg_loss:0.138, val_acc:0.933]
Epoch [40/120    avg_loss:0.145, val_acc:0.931]
Epoch [41/120    avg_loss:0.116, val_acc:0.935]
Epoch [42/120    avg_loss:0.132, val_acc:0.938]
Epoch [43/120    avg_loss:0.123, val_acc:0.926]
Epoch [44/120    avg_loss:0.107, val_acc:0.927]
Epoch [45/120    avg_loss:0.111, val_acc:0.931]
Epoch [46/120    avg_loss:0.111, val_acc:0.923]
Epoch [47/120    avg_loss:0.108, val_acc:0.942]
Epoch [48/120    avg_loss:0.119, val_acc:0.911]
Epoch [49/120    avg_loss:0.104, val_acc:0.943]
Epoch [50/120    avg_loss:0.083, val_acc:0.947]
Epoch [51/120    avg_loss:0.094, val_acc:0.941]
Epoch [52/120    avg_loss:0.077, val_acc:0.953]
Epoch [53/120    avg_loss:0.075, val_acc:0.922]
Epoch [54/120    avg_loss:0.096, val_acc:0.943]
Epoch [55/120    avg_loss:0.099, val_acc:0.936]
Epoch [56/120    avg_loss:0.070, val_acc:0.944]
Epoch [57/120    avg_loss:0.070, val_acc:0.942]
Epoch [58/120    avg_loss:0.050, val_acc:0.961]
Epoch [59/120    avg_loss:0.059, val_acc:0.951]
Epoch [60/120    avg_loss:0.053, val_acc:0.953]
Epoch [61/120    avg_loss:0.047, val_acc:0.959]
Epoch [62/120    avg_loss:0.042, val_acc:0.965]
Epoch [63/120    avg_loss:0.040, val_acc:0.969]
Epoch [64/120    avg_loss:0.078, val_acc:0.952]
Epoch [65/120    avg_loss:0.080, val_acc:0.953]
Epoch [66/120    avg_loss:0.066, val_acc:0.952]
Epoch [67/120    avg_loss:0.053, val_acc:0.956]
Epoch [68/120    avg_loss:0.052, val_acc:0.965]
Epoch [69/120    avg_loss:0.043, val_acc:0.965]
Epoch [70/120    avg_loss:0.035, val_acc:0.966]
Epoch [71/120    avg_loss:0.039, val_acc:0.961]
Epoch [72/120    avg_loss:0.046, val_acc:0.959]
Epoch [73/120    avg_loss:0.036, val_acc:0.948]
Epoch [74/120    avg_loss:0.043, val_acc:0.958]
Epoch [75/120    avg_loss:0.035, val_acc:0.968]
Epoch [76/120    avg_loss:0.036, val_acc:0.961]
Epoch [77/120    avg_loss:0.029, val_acc:0.969]
Epoch [78/120    avg_loss:0.027, val_acc:0.975]
Epoch [79/120    avg_loss:0.021, val_acc:0.977]
Epoch [80/120    avg_loss:0.024, val_acc:0.976]
Epoch [81/120    avg_loss:0.021, val_acc:0.976]
Epoch [82/120    avg_loss:0.019, val_acc:0.976]
Epoch [83/120    avg_loss:0.026, val_acc:0.980]
Epoch [84/120    avg_loss:0.023, val_acc:0.977]
Epoch [85/120    avg_loss:0.022, val_acc:0.978]
Epoch [86/120    avg_loss:0.020, val_acc:0.978]
Epoch [87/120    avg_loss:0.024, val_acc:0.975]
Epoch [88/120    avg_loss:0.019, val_acc:0.978]
Epoch [89/120    avg_loss:0.021, val_acc:0.979]
Epoch [90/120    avg_loss:0.020, val_acc:0.978]
Epoch [91/120    avg_loss:0.023, val_acc:0.976]
Epoch [92/120    avg_loss:0.024, val_acc:0.976]
Epoch [93/120    avg_loss:0.018, val_acc:0.975]
Epoch [94/120    avg_loss:0.020, val_acc:0.977]
Epoch [95/120    avg_loss:0.018, val_acc:0.976]
Epoch [96/120    avg_loss:0.017, val_acc:0.978]
Epoch [97/120    avg_loss:0.018, val_acc:0.978]
Epoch [98/120    avg_loss:0.019, val_acc:0.978]
Epoch [99/120    avg_loss:0.016, val_acc:0.977]
Epoch [100/120    avg_loss:0.018, val_acc:0.977]
Epoch [101/120    avg_loss:0.017, val_acc:0.977]
Epoch [102/120    avg_loss:0.018, val_acc:0.977]
Epoch [103/120    avg_loss:0.020, val_acc:0.977]
Epoch [104/120    avg_loss:0.017, val_acc:0.977]
Epoch [105/120    avg_loss:0.017, val_acc:0.977]
Epoch [106/120    avg_loss:0.017, val_acc:0.977]
Epoch [107/120    avg_loss:0.017, val_acc:0.977]
Epoch [108/120    avg_loss:0.016, val_acc:0.977]
Epoch [109/120    avg_loss:0.017, val_acc:0.977]
Epoch [110/120    avg_loss:0.019, val_acc:0.977]
Epoch [111/120    avg_loss:0.020, val_acc:0.977]
Epoch [112/120    avg_loss:0.018, val_acc:0.977]
Epoch [113/120    avg_loss:0.019, val_acc:0.977]
Epoch [114/120    avg_loss:0.019, val_acc:0.977]
Epoch [115/120    avg_loss:0.017, val_acc:0.977]
Epoch [116/120    avg_loss:0.017, val_acc:0.977]
Epoch [117/120    avg_loss:0.017, val_acc:0.977]
Epoch [118/120    avg_loss:0.019, val_acc:0.977]
Epoch [119/120    avg_loss:0.019, val_acc:0.977]
Epoch [120/120    avg_loss:0.019, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    0    0    0    0    0    1    1    0    0
     0    0    0]
 [   0    0 1253    3    3    0    0    0    0    0    2   24    0    0
     0    0    0]
 [   0    0    0  722    5    0    0    0    0    5    1    8    4    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    4    0    1    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    4    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    4    0    0    1    0    0    0    0  842   28    0    0
     0    0    0]
 [   0    0    6    0    0    0    1    0    0    0   13 2174   16    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    0    3  524    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1119   20    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    17  328    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.8970189701897

F1 scores:
[       nan 0.96202532 0.98313064 0.97898305 0.98156682 0.98839907
 0.99390244 0.92592593 1.         0.82926829 0.97116494 0.97620117
 0.97126969 0.99462366 0.98200965 0.94117647 0.98224852]

Kappa:
0.9760159303593694
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f04d8e057b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.730, val_acc:0.127]
Epoch [2/120    avg_loss:2.521, val_acc:0.376]
Epoch [3/120    avg_loss:2.304, val_acc:0.475]
Epoch [4/120    avg_loss:2.146, val_acc:0.519]
Epoch [5/120    avg_loss:2.041, val_acc:0.531]
Epoch [6/120    avg_loss:1.907, val_acc:0.546]
Epoch [7/120    avg_loss:1.814, val_acc:0.555]
Epoch [8/120    avg_loss:1.754, val_acc:0.548]
Epoch [9/120    avg_loss:1.622, val_acc:0.526]
Epoch [10/120    avg_loss:1.496, val_acc:0.611]
Epoch [11/120    avg_loss:1.394, val_acc:0.613]
Epoch [12/120    avg_loss:1.305, val_acc:0.658]
Epoch [13/120    avg_loss:1.244, val_acc:0.664]
Epoch [14/120    avg_loss:1.189, val_acc:0.657]
Epoch [15/120    avg_loss:1.115, val_acc:0.697]
Epoch [16/120    avg_loss:1.054, val_acc:0.697]
Epoch [17/120    avg_loss:0.958, val_acc:0.749]
Epoch [18/120    avg_loss:0.780, val_acc:0.752]
Epoch [19/120    avg_loss:0.712, val_acc:0.718]
Epoch [20/120    avg_loss:0.688, val_acc:0.759]
Epoch [21/120    avg_loss:0.646, val_acc:0.801]
Epoch [22/120    avg_loss:0.610, val_acc:0.781]
Epoch [23/120    avg_loss:0.602, val_acc:0.827]
Epoch [24/120    avg_loss:0.500, val_acc:0.826]
Epoch [25/120    avg_loss:0.471, val_acc:0.848]
Epoch [26/120    avg_loss:0.385, val_acc:0.863]
Epoch [27/120    avg_loss:0.328, val_acc:0.876]
Epoch [28/120    avg_loss:0.298, val_acc:0.904]
Epoch [29/120    avg_loss:0.320, val_acc:0.855]
Epoch [30/120    avg_loss:0.305, val_acc:0.872]
Epoch [31/120    avg_loss:0.252, val_acc:0.910]
Epoch [32/120    avg_loss:0.238, val_acc:0.890]
Epoch [33/120    avg_loss:0.216, val_acc:0.895]
Epoch [34/120    avg_loss:0.199, val_acc:0.909]
Epoch [35/120    avg_loss:0.180, val_acc:0.926]
Epoch [36/120    avg_loss:0.209, val_acc:0.903]
Epoch [37/120    avg_loss:0.186, val_acc:0.926]
Epoch [38/120    avg_loss:0.144, val_acc:0.924]
Epoch [39/120    avg_loss:0.163, val_acc:0.903]
Epoch [40/120    avg_loss:0.212, val_acc:0.900]
Epoch [41/120    avg_loss:0.170, val_acc:0.932]
Epoch [42/120    avg_loss:0.121, val_acc:0.939]
Epoch [43/120    avg_loss:0.116, val_acc:0.942]
Epoch [44/120    avg_loss:0.114, val_acc:0.933]
Epoch [45/120    avg_loss:0.112, val_acc:0.928]
Epoch [46/120    avg_loss:0.093, val_acc:0.939]
Epoch [47/120    avg_loss:0.089, val_acc:0.938]
Epoch [48/120    avg_loss:0.152, val_acc:0.920]
Epoch [49/120    avg_loss:0.136, val_acc:0.930]
Epoch [50/120    avg_loss:0.085, val_acc:0.950]
Epoch [51/120    avg_loss:0.066, val_acc:0.950]
Epoch [52/120    avg_loss:0.071, val_acc:0.942]
Epoch [53/120    avg_loss:0.068, val_acc:0.945]
Epoch [54/120    avg_loss:0.078, val_acc:0.933]
Epoch [55/120    avg_loss:0.065, val_acc:0.959]
Epoch [56/120    avg_loss:0.064, val_acc:0.954]
Epoch [57/120    avg_loss:0.072, val_acc:0.949]
Epoch [58/120    avg_loss:0.066, val_acc:0.958]
Epoch [59/120    avg_loss:0.053, val_acc:0.951]
Epoch [60/120    avg_loss:0.042, val_acc:0.958]
Epoch [61/120    avg_loss:0.064, val_acc:0.945]
Epoch [62/120    avg_loss:0.086, val_acc:0.952]
Epoch [63/120    avg_loss:0.059, val_acc:0.950]
Epoch [64/120    avg_loss:0.046, val_acc:0.967]
Epoch [65/120    avg_loss:0.049, val_acc:0.953]
Epoch [66/120    avg_loss:0.045, val_acc:0.958]
Epoch [67/120    avg_loss:0.047, val_acc:0.960]
Epoch [68/120    avg_loss:0.048, val_acc:0.954]
Epoch [69/120    avg_loss:0.053, val_acc:0.961]
Epoch [70/120    avg_loss:0.047, val_acc:0.952]
Epoch [71/120    avg_loss:0.038, val_acc:0.957]
Epoch [72/120    avg_loss:0.047, val_acc:0.929]
Epoch [73/120    avg_loss:0.054, val_acc:0.953]
Epoch [74/120    avg_loss:0.051, val_acc:0.956]
Epoch [75/120    avg_loss:0.045, val_acc:0.949]
Epoch [76/120    avg_loss:0.040, val_acc:0.967]
Epoch [77/120    avg_loss:0.048, val_acc:0.959]
Epoch [78/120    avg_loss:0.160, val_acc:0.887]
Epoch [79/120    avg_loss:0.171, val_acc:0.952]
Epoch [80/120    avg_loss:0.075, val_acc:0.939]
Epoch [81/120    avg_loss:0.052, val_acc:0.955]
Epoch [82/120    avg_loss:0.043, val_acc:0.942]
Epoch [83/120    avg_loss:0.059, val_acc:0.912]
Epoch [84/120    avg_loss:0.057, val_acc:0.961]
Epoch [85/120    avg_loss:0.036, val_acc:0.964]
Epoch [86/120    avg_loss:0.033, val_acc:0.943]
Epoch [87/120    avg_loss:0.043, val_acc:0.966]
Epoch [88/120    avg_loss:0.034, val_acc:0.961]
Epoch [89/120    avg_loss:0.027, val_acc:0.957]
Epoch [90/120    avg_loss:0.023, val_acc:0.967]
Epoch [91/120    avg_loss:0.017, val_acc:0.964]
Epoch [92/120    avg_loss:0.016, val_acc:0.964]
Epoch [93/120    avg_loss:0.016, val_acc:0.964]
Epoch [94/120    avg_loss:0.018, val_acc:0.964]
Epoch [95/120    avg_loss:0.017, val_acc:0.963]
Epoch [96/120    avg_loss:0.017, val_acc:0.964]
Epoch [97/120    avg_loss:0.017, val_acc:0.965]
Epoch [98/120    avg_loss:0.017, val_acc:0.964]
Epoch [99/120    avg_loss:0.015, val_acc:0.963]
Epoch [100/120    avg_loss:0.015, val_acc:0.963]
Epoch [101/120    avg_loss:0.016, val_acc:0.964]
Epoch [102/120    avg_loss:0.013, val_acc:0.964]
Epoch [103/120    avg_loss:0.016, val_acc:0.964]
Epoch [104/120    avg_loss:0.014, val_acc:0.964]
Epoch [105/120    avg_loss:0.015, val_acc:0.964]
Epoch [106/120    avg_loss:0.016, val_acc:0.964]
Epoch [107/120    avg_loss:0.014, val_acc:0.964]
Epoch [108/120    avg_loss:0.014, val_acc:0.964]
Epoch [109/120    avg_loss:0.013, val_acc:0.964]
Epoch [110/120    avg_loss:0.014, val_acc:0.964]
Epoch [111/120    avg_loss:0.016, val_acc:0.964]
Epoch [112/120    avg_loss:0.017, val_acc:0.964]
Epoch [113/120    avg_loss:0.016, val_acc:0.964]
Epoch [114/120    avg_loss:0.016, val_acc:0.964]
Epoch [115/120    avg_loss:0.015, val_acc:0.964]
Epoch [116/120    avg_loss:0.016, val_acc:0.964]
Epoch [117/120    avg_loss:0.017, val_acc:0.964]
Epoch [118/120    avg_loss:0.015, val_acc:0.964]
Epoch [119/120    avg_loss:0.013, val_acc:0.964]
Epoch [120/120    avg_loss:0.014, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0 1244    4    6    3    0    0    0    0    4   23    1    0
     0    0    0]
 [   0    0    0  714    4    5    4    0    0    2    3   10    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    1    0    0  425    0    3    0    1    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    4    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  850   21    0    0
     0    0    0]
 [   0    0   29    0    0    0    1    0    0    0   23 2148    8    0
     0    1    0]
 [   0    0    1    1    0    0    0    0    0    0    2    8  516    0
     0    4    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1122   17    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
    62  268    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.79132791327913

F1 scores:
[       nan 0.96202532 0.97035881 0.97407913 0.97706422 0.97813579
 0.9797145  0.94339623 0.997669   0.92307692 0.96755834 0.97106691
 0.96538821 0.99728997 0.96350365 0.84144427 0.98823529]

Kappa:
0.9633999963810148
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc4a9221828>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.702, val_acc:0.221]
Epoch [2/120    avg_loss:2.484, val_acc:0.342]
Epoch [3/120    avg_loss:2.305, val_acc:0.414]
Epoch [4/120    avg_loss:2.184, val_acc:0.468]
Epoch [5/120    avg_loss:2.065, val_acc:0.508]
Epoch [6/120    avg_loss:1.994, val_acc:0.555]
Epoch [7/120    avg_loss:1.869, val_acc:0.576]
Epoch [8/120    avg_loss:1.775, val_acc:0.602]
Epoch [9/120    avg_loss:1.635, val_acc:0.623]
Epoch [10/120    avg_loss:1.501, val_acc:0.635]
Epoch [11/120    avg_loss:1.429, val_acc:0.641]
Epoch [12/120    avg_loss:1.278, val_acc:0.672]
Epoch [13/120    avg_loss:1.166, val_acc:0.696]
Epoch [14/120    avg_loss:1.077, val_acc:0.721]
Epoch [15/120    avg_loss:1.016, val_acc:0.690]
Epoch [16/120    avg_loss:0.951, val_acc:0.741]
Epoch [17/120    avg_loss:0.836, val_acc:0.766]
Epoch [18/120    avg_loss:0.819, val_acc:0.739]
Epoch [19/120    avg_loss:0.656, val_acc:0.786]
Epoch [20/120    avg_loss:0.568, val_acc:0.779]
Epoch [21/120    avg_loss:0.497, val_acc:0.838]
Epoch [22/120    avg_loss:0.512, val_acc:0.802]
Epoch [23/120    avg_loss:0.450, val_acc:0.841]
Epoch [24/120    avg_loss:0.382, val_acc:0.854]
Epoch [25/120    avg_loss:0.335, val_acc:0.877]
Epoch [26/120    avg_loss:0.417, val_acc:0.820]
Epoch [27/120    avg_loss:0.390, val_acc:0.859]
Epoch [28/120    avg_loss:0.345, val_acc:0.856]
Epoch [29/120    avg_loss:0.304, val_acc:0.880]
Epoch [30/120    avg_loss:0.281, val_acc:0.894]
Epoch [31/120    avg_loss:0.239, val_acc:0.916]
Epoch [32/120    avg_loss:0.200, val_acc:0.920]
Epoch [33/120    avg_loss:0.265, val_acc:0.892]
Epoch [34/120    avg_loss:0.235, val_acc:0.905]
Epoch [35/120    avg_loss:0.186, val_acc:0.929]
Epoch [36/120    avg_loss:0.164, val_acc:0.928]
Epoch [37/120    avg_loss:0.126, val_acc:0.944]
Epoch [38/120    avg_loss:0.120, val_acc:0.953]
Epoch [39/120    avg_loss:0.116, val_acc:0.938]
Epoch [40/120    avg_loss:0.119, val_acc:0.948]
Epoch [41/120    avg_loss:0.108, val_acc:0.951]
Epoch [42/120    avg_loss:0.082, val_acc:0.956]
Epoch [43/120    avg_loss:0.100, val_acc:0.960]
Epoch [44/120    avg_loss:0.085, val_acc:0.949]
Epoch [45/120    avg_loss:0.082, val_acc:0.954]
Epoch [46/120    avg_loss:0.089, val_acc:0.950]
Epoch [47/120    avg_loss:0.086, val_acc:0.952]
Epoch [48/120    avg_loss:0.074, val_acc:0.954]
Epoch [49/120    avg_loss:0.093, val_acc:0.949]
Epoch [50/120    avg_loss:0.089, val_acc:0.967]
Epoch [51/120    avg_loss:0.065, val_acc:0.966]
Epoch [52/120    avg_loss:0.066, val_acc:0.965]
Epoch [53/120    avg_loss:0.055, val_acc:0.968]
Epoch [54/120    avg_loss:0.060, val_acc:0.966]
Epoch [55/120    avg_loss:0.052, val_acc:0.968]
Epoch [56/120    avg_loss:0.042, val_acc:0.966]
Epoch [57/120    avg_loss:0.054, val_acc:0.953]
Epoch [58/120    avg_loss:0.078, val_acc:0.967]
Epoch [59/120    avg_loss:0.059, val_acc:0.964]
Epoch [60/120    avg_loss:0.048, val_acc:0.970]
Epoch [61/120    avg_loss:0.064, val_acc:0.952]
Epoch [62/120    avg_loss:0.075, val_acc:0.963]
Epoch [63/120    avg_loss:0.061, val_acc:0.957]
Epoch [64/120    avg_loss:0.050, val_acc:0.958]
Epoch [65/120    avg_loss:0.048, val_acc:0.967]
Epoch [66/120    avg_loss:0.034, val_acc:0.969]
Epoch [67/120    avg_loss:0.030, val_acc:0.979]
Epoch [68/120    avg_loss:0.034, val_acc:0.971]
Epoch [69/120    avg_loss:0.036, val_acc:0.974]
Epoch [70/120    avg_loss:0.029, val_acc:0.974]
Epoch [71/120    avg_loss:0.024, val_acc:0.979]
Epoch [72/120    avg_loss:0.022, val_acc:0.973]
Epoch [73/120    avg_loss:0.028, val_acc:0.966]
Epoch [74/120    avg_loss:0.026, val_acc:0.982]
Epoch [75/120    avg_loss:0.026, val_acc:0.982]
Epoch [76/120    avg_loss:0.022, val_acc:0.980]
Epoch [77/120    avg_loss:0.016, val_acc:0.982]
Epoch [78/120    avg_loss:0.020, val_acc:0.982]
Epoch [79/120    avg_loss:0.018, val_acc:0.985]
Epoch [80/120    avg_loss:0.021, val_acc:0.980]
Epoch [81/120    avg_loss:0.017, val_acc:0.978]
Epoch [82/120    avg_loss:0.014, val_acc:0.984]
Epoch [83/120    avg_loss:0.020, val_acc:0.971]
Epoch [84/120    avg_loss:0.020, val_acc:0.975]
Epoch [85/120    avg_loss:0.024, val_acc:0.976]
Epoch [86/120    avg_loss:0.032, val_acc:0.972]
Epoch [87/120    avg_loss:0.019, val_acc:0.978]
Epoch [88/120    avg_loss:0.018, val_acc:0.978]
Epoch [89/120    avg_loss:0.017, val_acc:0.979]
Epoch [90/120    avg_loss:0.018, val_acc:0.975]
Epoch [91/120    avg_loss:0.020, val_acc:0.979]
Epoch [92/120    avg_loss:0.039, val_acc:0.967]
Epoch [93/120    avg_loss:0.043, val_acc:0.982]
Epoch [94/120    avg_loss:0.023, val_acc:0.982]
Epoch [95/120    avg_loss:0.017, val_acc:0.982]
Epoch [96/120    avg_loss:0.018, val_acc:0.984]
Epoch [97/120    avg_loss:0.021, val_acc:0.984]
Epoch [98/120    avg_loss:0.017, val_acc:0.985]
Epoch [99/120    avg_loss:0.018, val_acc:0.982]
Epoch [100/120    avg_loss:0.013, val_acc:0.982]
Epoch [101/120    avg_loss:0.018, val_acc:0.980]
Epoch [102/120    avg_loss:0.014, val_acc:0.980]
Epoch [103/120    avg_loss:0.014, val_acc:0.982]
Epoch [104/120    avg_loss:0.013, val_acc:0.983]
Epoch [105/120    avg_loss:0.011, val_acc:0.982]
Epoch [106/120    avg_loss:0.016, val_acc:0.983]
Epoch [107/120    avg_loss:0.012, val_acc:0.982]
Epoch [108/120    avg_loss:0.013, val_acc:0.982]
Epoch [109/120    avg_loss:0.012, val_acc:0.982]
Epoch [110/120    avg_loss:0.014, val_acc:0.983]
Epoch [111/120    avg_loss:0.015, val_acc:0.984]
Epoch [112/120    avg_loss:0.012, val_acc:0.984]
Epoch [113/120    avg_loss:0.012, val_acc:0.984]
Epoch [114/120    avg_loss:0.011, val_acc:0.984]
Epoch [115/120    avg_loss:0.014, val_acc:0.984]
Epoch [116/120    avg_loss:0.013, val_acc:0.984]
Epoch [117/120    avg_loss:0.014, val_acc:0.984]
Epoch [118/120    avg_loss:0.012, val_acc:0.984]
Epoch [119/120    avg_loss:0.012, val_acc:0.984]
Epoch [120/120    avg_loss:0.016, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1253    2    5    0    0    0    0    0    3   22    0    0
     0    0    0]
 [   0    0    0  724    0    1    0    0    0    0    3    8   11    0
     0    0    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    6    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  852   16    0    0
     2    0    0]
 [   0    7    9    0    0    0    0    0    0    1   36 2149    8    0
     0    0    0]
 [   0    0    4    4    0    0    0    0    0    0    0    4  518    0
     2    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    2    0    0    0    0
  1125   12    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    71  276    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.24661246612466

F1 scores:
[       nan 0.91111111 0.98043818 0.9797023  0.98368298 0.99539171
 0.99465241 0.98039216 0.99883586 0.89473684 0.96325608 0.97349943
 0.96551724 1.         0.96112772 0.86792453 0.98809524]

Kappa:
0.9685977854828373
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbd8535a8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.769, val_acc:0.253]
Epoch [2/120    avg_loss:2.494, val_acc:0.411]
Epoch [3/120    avg_loss:2.313, val_acc:0.476]
Epoch [4/120    avg_loss:2.184, val_acc:0.505]
Epoch [5/120    avg_loss:2.050, val_acc:0.534]
Epoch [6/120    avg_loss:1.894, val_acc:0.542]
Epoch [7/120    avg_loss:1.804, val_acc:0.557]
Epoch [8/120    avg_loss:1.706, val_acc:0.544]
Epoch [9/120    avg_loss:1.581, val_acc:0.618]
Epoch [10/120    avg_loss:1.479, val_acc:0.648]
Epoch [11/120    avg_loss:1.374, val_acc:0.643]
Epoch [12/120    avg_loss:1.350, val_acc:0.607]
Epoch [13/120    avg_loss:1.147, val_acc:0.624]
Epoch [14/120    avg_loss:1.065, val_acc:0.690]
Epoch [15/120    avg_loss:0.979, val_acc:0.686]
Epoch [16/120    avg_loss:0.933, val_acc:0.727]
Epoch [17/120    avg_loss:0.871, val_acc:0.697]
Epoch [18/120    avg_loss:0.873, val_acc:0.730]
Epoch [19/120    avg_loss:0.732, val_acc:0.760]
Epoch [20/120    avg_loss:0.657, val_acc:0.804]
Epoch [21/120    avg_loss:0.651, val_acc:0.793]
Epoch [22/120    avg_loss:0.568, val_acc:0.795]
Epoch [23/120    avg_loss:0.517, val_acc:0.823]
Epoch [24/120    avg_loss:0.466, val_acc:0.795]
Epoch [25/120    avg_loss:0.428, val_acc:0.845]
Epoch [26/120    avg_loss:0.353, val_acc:0.848]
Epoch [27/120    avg_loss:0.319, val_acc:0.860]
Epoch [28/120    avg_loss:0.283, val_acc:0.876]
Epoch [29/120    avg_loss:0.256, val_acc:0.877]
Epoch [30/120    avg_loss:0.254, val_acc:0.872]
Epoch [31/120    avg_loss:0.250, val_acc:0.871]
Epoch [32/120    avg_loss:0.249, val_acc:0.887]
Epoch [33/120    avg_loss:0.276, val_acc:0.854]
Epoch [34/120    avg_loss:0.240, val_acc:0.866]
Epoch [35/120    avg_loss:0.198, val_acc:0.892]
Epoch [36/120    avg_loss:0.180, val_acc:0.903]
Epoch [37/120    avg_loss:0.186, val_acc:0.874]
Epoch [38/120    avg_loss:0.194, val_acc:0.901]
Epoch [39/120    avg_loss:0.167, val_acc:0.907]
Epoch [40/120    avg_loss:0.156, val_acc:0.905]
Epoch [41/120    avg_loss:0.181, val_acc:0.896]
Epoch [42/120    avg_loss:0.182, val_acc:0.873]
Epoch [43/120    avg_loss:0.161, val_acc:0.891]
Epoch [44/120    avg_loss:0.129, val_acc:0.924]
Epoch [45/120    avg_loss:0.160, val_acc:0.918]
Epoch [46/120    avg_loss:0.111, val_acc:0.921]
Epoch [47/120    avg_loss:0.101, val_acc:0.924]
Epoch [48/120    avg_loss:0.122, val_acc:0.927]
Epoch [49/120    avg_loss:0.115, val_acc:0.929]
Epoch [50/120    avg_loss:0.123, val_acc:0.918]
Epoch [51/120    avg_loss:0.117, val_acc:0.927]
Epoch [52/120    avg_loss:0.107, val_acc:0.939]
Epoch [53/120    avg_loss:0.079, val_acc:0.946]
Epoch [54/120    avg_loss:0.063, val_acc:0.923]
Epoch [55/120    avg_loss:0.081, val_acc:0.942]
Epoch [56/120    avg_loss:0.085, val_acc:0.942]
Epoch [57/120    avg_loss:0.079, val_acc:0.934]
Epoch [58/120    avg_loss:0.068, val_acc:0.943]
Epoch [59/120    avg_loss:0.092, val_acc:0.943]
Epoch [60/120    avg_loss:0.061, val_acc:0.957]
Epoch [61/120    avg_loss:0.044, val_acc:0.957]
Epoch [62/120    avg_loss:0.052, val_acc:0.956]
Epoch [63/120    avg_loss:0.043, val_acc:0.954]
Epoch [64/120    avg_loss:0.056, val_acc:0.955]
Epoch [65/120    avg_loss:0.084, val_acc:0.949]
Epoch [66/120    avg_loss:0.059, val_acc:0.951]
Epoch [67/120    avg_loss:0.060, val_acc:0.944]
Epoch [68/120    avg_loss:0.064, val_acc:0.929]
Epoch [69/120    avg_loss:0.071, val_acc:0.958]
Epoch [70/120    avg_loss:0.043, val_acc:0.966]
Epoch [71/120    avg_loss:0.044, val_acc:0.961]
Epoch [72/120    avg_loss:0.044, val_acc:0.951]
Epoch [73/120    avg_loss:0.042, val_acc:0.963]
Epoch [74/120    avg_loss:0.035, val_acc:0.960]
Epoch [75/120    avg_loss:0.032, val_acc:0.958]
Epoch [76/120    avg_loss:0.029, val_acc:0.965]
Epoch [77/120    avg_loss:0.022, val_acc:0.970]
Epoch [78/120    avg_loss:0.023, val_acc:0.967]
Epoch [79/120    avg_loss:0.035, val_acc:0.951]
Epoch [80/120    avg_loss:0.051, val_acc:0.947]
Epoch [81/120    avg_loss:0.072, val_acc:0.923]
Epoch [82/120    avg_loss:0.068, val_acc:0.946]
Epoch [83/120    avg_loss:0.039, val_acc:0.958]
Epoch [84/120    avg_loss:0.045, val_acc:0.958]
Epoch [85/120    avg_loss:0.043, val_acc:0.956]
Epoch [86/120    avg_loss:0.030, val_acc:0.963]
Epoch [87/120    avg_loss:0.038, val_acc:0.966]
Epoch [88/120    avg_loss:0.042, val_acc:0.964]
Epoch [89/120    avg_loss:0.032, val_acc:0.965]
Epoch [90/120    avg_loss:0.024, val_acc:0.972]
Epoch [91/120    avg_loss:0.020, val_acc:0.977]
Epoch [92/120    avg_loss:0.017, val_acc:0.968]
Epoch [93/120    avg_loss:0.018, val_acc:0.978]
Epoch [94/120    avg_loss:0.016, val_acc:0.979]
Epoch [95/120    avg_loss:0.021, val_acc:0.966]
Epoch [96/120    avg_loss:0.020, val_acc:0.977]
Epoch [97/120    avg_loss:0.015, val_acc:0.974]
Epoch [98/120    avg_loss:0.013, val_acc:0.971]
Epoch [99/120    avg_loss:0.014, val_acc:0.975]
Epoch [100/120    avg_loss:0.013, val_acc:0.967]
Epoch [101/120    avg_loss:0.016, val_acc:0.969]
Epoch [102/120    avg_loss:0.020, val_acc:0.970]
Epoch [103/120    avg_loss:0.022, val_acc:0.963]
Epoch [104/120    avg_loss:0.022, val_acc:0.963]
Epoch [105/120    avg_loss:0.021, val_acc:0.972]
Epoch [106/120    avg_loss:0.022, val_acc:0.966]
Epoch [107/120    avg_loss:0.017, val_acc:0.974]
Epoch [108/120    avg_loss:0.015, val_acc:0.975]
Epoch [109/120    avg_loss:0.015, val_acc:0.976]
Epoch [110/120    avg_loss:0.011, val_acc:0.975]
Epoch [111/120    avg_loss:0.011, val_acc:0.976]
Epoch [112/120    avg_loss:0.011, val_acc:0.976]
Epoch [113/120    avg_loss:0.011, val_acc:0.978]
Epoch [114/120    avg_loss:0.012, val_acc:0.977]
Epoch [115/120    avg_loss:0.008, val_acc:0.977]
Epoch [116/120    avg_loss:0.009, val_acc:0.979]
Epoch [117/120    avg_loss:0.011, val_acc:0.979]
Epoch [118/120    avg_loss:0.009, val_acc:0.977]
Epoch [119/120    avg_loss:0.010, val_acc:0.979]
Epoch [120/120    avg_loss:0.010, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1267    2    0    1    0    0    0    1    1   13    0    0
     0    0    0]
 [   0    0    1  716    4    2    4    0    0   12    1    1    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    4    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    6    0    0    1    0    0    0    0  853   15    0    0
     0    0    0]
 [   0    0    3    0    0    1    1    0    0    4   28 2159   14    0
     0    0    0]
 [   0    0    0    3    3    2    0    0    0    0    2    2  520    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1123   13    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    39  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.63685636856368

F1 scores:
[       nan 0.94117647 0.98907104 0.97547684 0.98383372 0.98052692
 0.98269375 1.         0.9953271  0.65384615 0.96876775 0.98069498
 0.96834264 1.         0.97398092 0.90440061 0.99408284]

Kappa:
0.9730664134392122
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f37fd9860>
supervision:full
center_pixel:True
Network :
Number of parameter: 48814==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.736, val_acc:0.249]
Epoch [2/120    avg_loss:2.480, val_acc:0.312]
Epoch [3/120    avg_loss:2.330, val_acc:0.348]
Epoch [4/120    avg_loss:2.185, val_acc:0.476]
Epoch [5/120    avg_loss:2.100, val_acc:0.514]
Epoch [6/120    avg_loss:1.998, val_acc:0.530]
Epoch [7/120    avg_loss:1.869, val_acc:0.546]
Epoch [8/120    avg_loss:1.763, val_acc:0.558]
Epoch [9/120    avg_loss:1.671, val_acc:0.568]
Epoch [10/120    avg_loss:1.577, val_acc:0.589]
Epoch [11/120    avg_loss:1.493, val_acc:0.585]
Epoch [12/120    avg_loss:1.437, val_acc:0.600]
Epoch [13/120    avg_loss:1.336, val_acc:0.609]
Epoch [14/120    avg_loss:1.232, val_acc:0.643]
Epoch [15/120    avg_loss:1.142, val_acc:0.671]
Epoch [16/120    avg_loss:1.063, val_acc:0.633]
Epoch [17/120    avg_loss:1.027, val_acc:0.703]
Epoch [18/120    avg_loss:0.867, val_acc:0.704]
Epoch [19/120    avg_loss:0.817, val_acc:0.730]
Epoch [20/120    avg_loss:0.743, val_acc:0.756]
Epoch [21/120    avg_loss:0.663, val_acc:0.792]
Epoch [22/120    avg_loss:0.590, val_acc:0.805]
Epoch [23/120    avg_loss:0.628, val_acc:0.785]
Epoch [24/120    avg_loss:0.538, val_acc:0.794]
Epoch [25/120    avg_loss:0.568, val_acc:0.814]
Epoch [26/120    avg_loss:0.436, val_acc:0.798]
Epoch [27/120    avg_loss:0.381, val_acc:0.841]
Epoch [28/120    avg_loss:0.357, val_acc:0.836]
Epoch [29/120    avg_loss:0.356, val_acc:0.826]
Epoch [30/120    avg_loss:0.289, val_acc:0.860]
Epoch [31/120    avg_loss:0.229, val_acc:0.896]
Epoch [32/120    avg_loss:0.248, val_acc:0.874]
Epoch [33/120    avg_loss:0.294, val_acc:0.858]
Epoch [34/120    avg_loss:0.266, val_acc:0.873]
Epoch [35/120    avg_loss:0.238, val_acc:0.897]
Epoch [36/120    avg_loss:0.191, val_acc:0.892]
Epoch [37/120    avg_loss:0.173, val_acc:0.889]
Epoch [38/120    avg_loss:0.145, val_acc:0.916]
Epoch [39/120    avg_loss:0.124, val_acc:0.902]
Epoch [40/120    avg_loss:0.157, val_acc:0.908]
Epoch [41/120    avg_loss:0.126, val_acc:0.926]
Epoch [42/120    avg_loss:0.112, val_acc:0.931]
Epoch [43/120    avg_loss:0.114, val_acc:0.933]
Epoch [44/120    avg_loss:0.096, val_acc:0.944]
Epoch [45/120    avg_loss:0.092, val_acc:0.942]
Epoch [46/120    avg_loss:0.092, val_acc:0.925]
Epoch [47/120    avg_loss:0.099, val_acc:0.932]
Epoch [48/120    avg_loss:0.100, val_acc:0.910]
Epoch [49/120    avg_loss:0.082, val_acc:0.942]
Epoch [50/120    avg_loss:0.079, val_acc:0.947]
Epoch [51/120    avg_loss:0.085, val_acc:0.925]
Epoch [52/120    avg_loss:0.076, val_acc:0.932]
Epoch [53/120    avg_loss:0.071, val_acc:0.933]
Epoch [54/120    avg_loss:0.070, val_acc:0.928]
Epoch [55/120    avg_loss:0.072, val_acc:0.939]
Epoch [56/120    avg_loss:0.072, val_acc:0.941]
Epoch [57/120    avg_loss:0.071, val_acc:0.936]
Epoch [58/120    avg_loss:0.065, val_acc:0.931]
Epoch [59/120    avg_loss:0.071, val_acc:0.905]
Epoch [60/120    avg_loss:0.084, val_acc:0.944]
Epoch [61/120    avg_loss:0.055, val_acc:0.949]
Epoch [62/120    avg_loss:0.068, val_acc:0.945]
Epoch [63/120    avg_loss:0.070, val_acc:0.931]
Epoch [64/120    avg_loss:0.061, val_acc:0.943]
Epoch [65/120    avg_loss:0.058, val_acc:0.952]
Epoch [66/120    avg_loss:0.056, val_acc:0.954]
Epoch [67/120    avg_loss:0.045, val_acc:0.957]
Epoch [68/120    avg_loss:0.049, val_acc:0.955]
Epoch [69/120    avg_loss:0.050, val_acc:0.947]
Epoch [70/120    avg_loss:0.048, val_acc:0.950]
Epoch [71/120    avg_loss:0.053, val_acc:0.958]
Epoch [72/120    avg_loss:0.052, val_acc:0.951]
Epoch [73/120    avg_loss:0.057, val_acc:0.949]
Epoch [74/120    avg_loss:0.038, val_acc:0.942]
Epoch [75/120    avg_loss:0.031, val_acc:0.957]
Epoch [76/120    avg_loss:0.039, val_acc:0.954]
Epoch [77/120    avg_loss:0.035, val_acc:0.955]
Epoch [78/120    avg_loss:0.034, val_acc:0.961]
Epoch [79/120    avg_loss:0.026, val_acc:0.951]
Epoch [80/120    avg_loss:0.039, val_acc:0.932]
Epoch [81/120    avg_loss:0.043, val_acc:0.964]
Epoch [82/120    avg_loss:0.032, val_acc:0.960]
Epoch [83/120    avg_loss:0.023, val_acc:0.961]
Epoch [84/120    avg_loss:0.135, val_acc:0.823]
Epoch [85/120    avg_loss:0.294, val_acc:0.859]
Epoch [86/120    avg_loss:0.468, val_acc:0.820]
Epoch [87/120    avg_loss:0.317, val_acc:0.883]
Epoch [88/120    avg_loss:0.174, val_acc:0.899]
Epoch [89/120    avg_loss:0.126, val_acc:0.935]
Epoch [90/120    avg_loss:0.074, val_acc:0.932]
Epoch [91/120    avg_loss:0.074, val_acc:0.951]
Epoch [92/120    avg_loss:0.080, val_acc:0.939]
Epoch [93/120    avg_loss:0.060, val_acc:0.928]
Epoch [94/120    avg_loss:0.049, val_acc:0.945]
Epoch [95/120    avg_loss:0.041, val_acc:0.954]
Epoch [96/120    avg_loss:0.033, val_acc:0.956]
Epoch [97/120    avg_loss:0.032, val_acc:0.957]
Epoch [98/120    avg_loss:0.033, val_acc:0.957]
Epoch [99/120    avg_loss:0.029, val_acc:0.960]
Epoch [100/120    avg_loss:0.031, val_acc:0.961]
Epoch [101/120    avg_loss:0.027, val_acc:0.959]
Epoch [102/120    avg_loss:0.028, val_acc:0.961]
Epoch [103/120    avg_loss:0.029, val_acc:0.959]
Epoch [104/120    avg_loss:0.029, val_acc:0.960]
Epoch [105/120    avg_loss:0.031, val_acc:0.961]
Epoch [106/120    avg_loss:0.028, val_acc:0.958]
Epoch [107/120    avg_loss:0.026, val_acc:0.959]
Epoch [108/120    avg_loss:0.027, val_acc:0.959]
Epoch [109/120    avg_loss:0.028, val_acc:0.960]
Epoch [110/120    avg_loss:0.024, val_acc:0.961]
Epoch [111/120    avg_loss:0.031, val_acc:0.961]
Epoch [112/120    avg_loss:0.025, val_acc:0.961]
Epoch [113/120    avg_loss:0.026, val_acc:0.961]
Epoch [114/120    avg_loss:0.026, val_acc:0.963]
Epoch [115/120    avg_loss:0.025, val_acc:0.961]
Epoch [116/120    avg_loss:0.026, val_acc:0.961]
Epoch [117/120    avg_loss:0.034, val_acc:0.963]
Epoch [118/120    avg_loss:0.026, val_acc:0.961]
Epoch [119/120    avg_loss:0.024, val_acc:0.963]
Epoch [120/120    avg_loss:0.025, val_acc:0.963]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    2 1244    8    4    0    0    0    0    0    3   22    2    0
     0    0    0]
 [   0    0    0  731    0    3    0    0    0    4    1    7    1    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    1    1    0    0    0  814   57    0    0
     0    0    0]
 [   0    0   24    0    0    0    2    0    0    0   32 2132   20    0
     0    0    0]
 [   0    0    0    7    4    0    0    0    0    0    2    0  514    0
     2    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1116   22    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    59  287    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.66124661246613

F1 scores:
[       nan 0.97619048 0.97377691 0.97858099 0.97921478 0.98966705
 0.99544073 0.98039216 1.         0.9        0.94267516 0.96252822
 0.95895522 0.99728997 0.96206897 0.87101669 0.98224852]

Kappa:
0.9619230151793733
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5dcd252898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.756, val_acc:0.200]
Epoch [2/120    avg_loss:2.493, val_acc:0.326]
Epoch [3/120    avg_loss:2.319, val_acc:0.474]
Epoch [4/120    avg_loss:2.203, val_acc:0.478]
Epoch [5/120    avg_loss:2.072, val_acc:0.517]
Epoch [6/120    avg_loss:2.000, val_acc:0.542]
Epoch [7/120    avg_loss:1.839, val_acc:0.571]
Epoch [8/120    avg_loss:1.731, val_acc:0.574]
Epoch [9/120    avg_loss:1.603, val_acc:0.639]
Epoch [10/120    avg_loss:1.463, val_acc:0.636]
Epoch [11/120    avg_loss:1.302, val_acc:0.691]
Epoch [12/120    avg_loss:1.214, val_acc:0.671]
Epoch [13/120    avg_loss:1.149, val_acc:0.665]
Epoch [14/120    avg_loss:1.023, val_acc:0.753]
Epoch [15/120    avg_loss:0.923, val_acc:0.765]
Epoch [16/120    avg_loss:0.809, val_acc:0.780]
Epoch [17/120    avg_loss:0.751, val_acc:0.768]
Epoch [18/120    avg_loss:0.666, val_acc:0.825]
Epoch [19/120    avg_loss:0.604, val_acc:0.817]
Epoch [20/120    avg_loss:0.550, val_acc:0.838]
Epoch [21/120    avg_loss:0.488, val_acc:0.833]
Epoch [22/120    avg_loss:0.498, val_acc:0.873]
Epoch [23/120    avg_loss:0.454, val_acc:0.847]
Epoch [24/120    avg_loss:0.431, val_acc:0.851]
Epoch [25/120    avg_loss:0.347, val_acc:0.882]
Epoch [26/120    avg_loss:0.314, val_acc:0.889]
Epoch [27/120    avg_loss:0.289, val_acc:0.906]
Epoch [28/120    avg_loss:0.294, val_acc:0.900]
Epoch [29/120    avg_loss:0.376, val_acc:0.895]
Epoch [30/120    avg_loss:0.282, val_acc:0.902]
Epoch [31/120    avg_loss:0.239, val_acc:0.908]
Epoch [32/120    avg_loss:0.244, val_acc:0.908]
Epoch [33/120    avg_loss:0.201, val_acc:0.909]
Epoch [34/120    avg_loss:0.159, val_acc:0.931]
Epoch [35/120    avg_loss:0.148, val_acc:0.925]
Epoch [36/120    avg_loss:0.152, val_acc:0.943]
Epoch [37/120    avg_loss:0.127, val_acc:0.928]
Epoch [38/120    avg_loss:0.131, val_acc:0.921]
Epoch [39/120    avg_loss:0.142, val_acc:0.905]
Epoch [40/120    avg_loss:0.158, val_acc:0.942]
Epoch [41/120    avg_loss:0.130, val_acc:0.929]
Epoch [42/120    avg_loss:0.138, val_acc:0.929]
Epoch [43/120    avg_loss:0.115, val_acc:0.956]
Epoch [44/120    avg_loss:0.110, val_acc:0.951]
Epoch [45/120    avg_loss:0.096, val_acc:0.948]
Epoch [46/120    avg_loss:0.087, val_acc:0.955]
Epoch [47/120    avg_loss:0.078, val_acc:0.948]
Epoch [48/120    avg_loss:0.085, val_acc:0.940]
Epoch [49/120    avg_loss:0.102, val_acc:0.949]
Epoch [50/120    avg_loss:0.116, val_acc:0.952]
Epoch [51/120    avg_loss:0.116, val_acc:0.942]
Epoch [52/120    avg_loss:0.106, val_acc:0.960]
Epoch [53/120    avg_loss:0.094, val_acc:0.960]
Epoch [54/120    avg_loss:0.058, val_acc:0.967]
Epoch [55/120    avg_loss:0.053, val_acc:0.968]
Epoch [56/120    avg_loss:0.052, val_acc:0.967]
Epoch [57/120    avg_loss:0.053, val_acc:0.948]
Epoch [58/120    avg_loss:0.078, val_acc:0.954]
Epoch [59/120    avg_loss:0.056, val_acc:0.969]
Epoch [60/120    avg_loss:0.054, val_acc:0.964]
Epoch [61/120    avg_loss:0.074, val_acc:0.960]
Epoch [62/120    avg_loss:0.051, val_acc:0.964]
Epoch [63/120    avg_loss:0.070, val_acc:0.959]
Epoch [64/120    avg_loss:0.071, val_acc:0.955]
Epoch [65/120    avg_loss:0.055, val_acc:0.971]
Epoch [66/120    avg_loss:0.040, val_acc:0.977]
Epoch [67/120    avg_loss:0.043, val_acc:0.958]
Epoch [68/120    avg_loss:0.047, val_acc:0.969]
Epoch [69/120    avg_loss:0.034, val_acc:0.978]
Epoch [70/120    avg_loss:0.034, val_acc:0.975]
Epoch [71/120    avg_loss:0.041, val_acc:0.961]
Epoch [72/120    avg_loss:0.032, val_acc:0.971]
Epoch [73/120    avg_loss:0.032, val_acc:0.978]
Epoch [74/120    avg_loss:0.029, val_acc:0.976]
Epoch [75/120    avg_loss:0.036, val_acc:0.972]
Epoch [76/120    avg_loss:0.035, val_acc:0.970]
Epoch [77/120    avg_loss:0.031, val_acc:0.973]
Epoch [78/120    avg_loss:0.024, val_acc:0.978]
Epoch [79/120    avg_loss:0.024, val_acc:0.976]
Epoch [80/120    avg_loss:0.028, val_acc:0.975]
Epoch [81/120    avg_loss:0.031, val_acc:0.981]
Epoch [82/120    avg_loss:0.024, val_acc:0.976]
Epoch [83/120    avg_loss:0.021, val_acc:0.985]
Epoch [84/120    avg_loss:0.021, val_acc:0.968]
Epoch [85/120    avg_loss:0.026, val_acc:0.979]
Epoch [86/120    avg_loss:0.017, val_acc:0.976]
Epoch [87/120    avg_loss:0.024, val_acc:0.980]
Epoch [88/120    avg_loss:0.022, val_acc:0.980]
Epoch [89/120    avg_loss:0.021, val_acc:0.979]
Epoch [90/120    avg_loss:0.024, val_acc:0.977]
Epoch [91/120    avg_loss:0.031, val_acc:0.980]
Epoch [92/120    avg_loss:0.029, val_acc:0.969]
Epoch [93/120    avg_loss:0.027, val_acc:0.965]
Epoch [94/120    avg_loss:0.032, val_acc:0.974]
Epoch [95/120    avg_loss:0.044, val_acc:0.965]
Epoch [96/120    avg_loss:0.035, val_acc:0.968]
Epoch [97/120    avg_loss:0.031, val_acc:0.976]
Epoch [98/120    avg_loss:0.022, val_acc:0.979]
Epoch [99/120    avg_loss:0.019, val_acc:0.978]
Epoch [100/120    avg_loss:0.016, val_acc:0.977]
Epoch [101/120    avg_loss:0.015, val_acc:0.979]
Epoch [102/120    avg_loss:0.014, val_acc:0.979]
Epoch [103/120    avg_loss:0.013, val_acc:0.980]
Epoch [104/120    avg_loss:0.015, val_acc:0.981]
Epoch [105/120    avg_loss:0.017, val_acc:0.978]
Epoch [106/120    avg_loss:0.017, val_acc:0.978]
Epoch [107/120    avg_loss:0.015, val_acc:0.978]
Epoch [108/120    avg_loss:0.013, val_acc:0.978]
Epoch [109/120    avg_loss:0.012, val_acc:0.980]
Epoch [110/120    avg_loss:0.014, val_acc:0.981]
Epoch [111/120    avg_loss:0.011, val_acc:0.981]
Epoch [112/120    avg_loss:0.013, val_acc:0.981]
Epoch [113/120    avg_loss:0.010, val_acc:0.981]
Epoch [114/120    avg_loss:0.014, val_acc:0.981]
Epoch [115/120    avg_loss:0.011, val_acc:0.981]
Epoch [116/120    avg_loss:0.013, val_acc:0.981]
Epoch [117/120    avg_loss:0.013, val_acc:0.981]
Epoch [118/120    avg_loss:0.011, val_acc:0.981]
Epoch [119/120    avg_loss:0.012, val_acc:0.981]
Epoch [120/120    avg_loss:0.012, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    2    0    1    0
     0    0    0]
 [   0    0 1247    6    0    0    0    0    0    2    5   24    1    0
     0    0    0]
 [   0    0    0  714    1    6    1    0    0   10    4    0   11    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    5    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0  849   22    0    0
     1    2    0]
 [   0    0   17    0    0    1    2    0    0    0    6 2178    4    0
     0    2    0]
 [   0    0    0    3    0    0    0    0    0    0    9    0  516    0
     2    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    1    0    0    0
  1134    1    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    13  332    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.01626016260163

F1 scores:
[       nan 0.96202532 0.97842291 0.97076818 0.99765808 0.98285714
 0.99468489 0.90909091 0.9953271  0.69387755 0.96973158 0.98240866
 0.96178938 1.         0.99082569 0.96793003 0.98224852]

Kappa:
0.9773826462896681
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f149475d8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.724, val_acc:0.338]
Epoch [2/120    avg_loss:2.502, val_acc:0.384]
Epoch [3/120    avg_loss:2.324, val_acc:0.385]
Epoch [4/120    avg_loss:2.185, val_acc:0.437]
Epoch [5/120    avg_loss:2.083, val_acc:0.492]
Epoch [6/120    avg_loss:1.964, val_acc:0.564]
Epoch [7/120    avg_loss:1.877, val_acc:0.602]
Epoch [8/120    avg_loss:1.736, val_acc:0.593]
Epoch [9/120    avg_loss:1.657, val_acc:0.623]
Epoch [10/120    avg_loss:1.593, val_acc:0.638]
Epoch [11/120    avg_loss:1.499, val_acc:0.672]
Epoch [12/120    avg_loss:1.382, val_acc:0.684]
Epoch [13/120    avg_loss:1.242, val_acc:0.711]
Epoch [14/120    avg_loss:1.126, val_acc:0.718]
Epoch [15/120    avg_loss:1.017, val_acc:0.764]
Epoch [16/120    avg_loss:0.922, val_acc:0.752]
Epoch [17/120    avg_loss:0.850, val_acc:0.789]
Epoch [18/120    avg_loss:0.779, val_acc:0.779]
Epoch [19/120    avg_loss:0.644, val_acc:0.800]
Epoch [20/120    avg_loss:0.614, val_acc:0.802]
Epoch [21/120    avg_loss:0.543, val_acc:0.854]
Epoch [22/120    avg_loss:0.495, val_acc:0.840]
Epoch [23/120    avg_loss:0.515, val_acc:0.838]
Epoch [24/120    avg_loss:0.416, val_acc:0.868]
Epoch [25/120    avg_loss:0.448, val_acc:0.876]
Epoch [26/120    avg_loss:0.367, val_acc:0.874]
Epoch [27/120    avg_loss:0.330, val_acc:0.879]
Epoch [28/120    avg_loss:0.273, val_acc:0.904]
Epoch [29/120    avg_loss:0.229, val_acc:0.902]
Epoch [30/120    avg_loss:0.203, val_acc:0.918]
Epoch [31/120    avg_loss:0.198, val_acc:0.913]
Epoch [32/120    avg_loss:0.180, val_acc:0.914]
Epoch [33/120    avg_loss:0.172, val_acc:0.923]
Epoch [34/120    avg_loss:0.155, val_acc:0.922]
Epoch [35/120    avg_loss:0.198, val_acc:0.910]
Epoch [36/120    avg_loss:0.185, val_acc:0.924]
Epoch [37/120    avg_loss:0.153, val_acc:0.937]
Epoch [38/120    avg_loss:0.129, val_acc:0.933]
Epoch [39/120    avg_loss:0.116, val_acc:0.934]
Epoch [40/120    avg_loss:0.129, val_acc:0.939]
Epoch [41/120    avg_loss:0.137, val_acc:0.940]
Epoch [42/120    avg_loss:0.119, val_acc:0.946]
Epoch [43/120    avg_loss:0.099, val_acc:0.940]
Epoch [44/120    avg_loss:0.096, val_acc:0.939]
Epoch [45/120    avg_loss:0.105, val_acc:0.912]
Epoch [46/120    avg_loss:0.101, val_acc:0.930]
Epoch [47/120    avg_loss:0.080, val_acc:0.953]
Epoch [48/120    avg_loss:0.072, val_acc:0.958]
Epoch [49/120    avg_loss:0.061, val_acc:0.959]
Epoch [50/120    avg_loss:0.061, val_acc:0.964]
Epoch [51/120    avg_loss:0.070, val_acc:0.947]
Epoch [52/120    avg_loss:0.074, val_acc:0.954]
Epoch [53/120    avg_loss:0.061, val_acc:0.957]
Epoch [54/120    avg_loss:0.055, val_acc:0.965]
Epoch [55/120    avg_loss:0.050, val_acc:0.960]
Epoch [56/120    avg_loss:0.057, val_acc:0.964]
Epoch [57/120    avg_loss:0.045, val_acc:0.965]
Epoch [58/120    avg_loss:0.048, val_acc:0.964]
Epoch [59/120    avg_loss:0.049, val_acc:0.965]
Epoch [60/120    avg_loss:0.037, val_acc:0.966]
Epoch [61/120    avg_loss:0.037, val_acc:0.970]
Epoch [62/120    avg_loss:0.047, val_acc:0.958]
Epoch [63/120    avg_loss:0.057, val_acc:0.959]
Epoch [64/120    avg_loss:0.045, val_acc:0.966]
Epoch [65/120    avg_loss:0.050, val_acc:0.958]
Epoch [66/120    avg_loss:0.050, val_acc:0.953]
Epoch [67/120    avg_loss:0.054, val_acc:0.946]
Epoch [68/120    avg_loss:0.058, val_acc:0.966]
Epoch [69/120    avg_loss:0.067, val_acc:0.958]
Epoch [70/120    avg_loss:0.082, val_acc:0.952]
Epoch [71/120    avg_loss:0.062, val_acc:0.959]
Epoch [72/120    avg_loss:0.053, val_acc:0.965]
Epoch [73/120    avg_loss:0.053, val_acc:0.965]
Epoch [74/120    avg_loss:0.047, val_acc:0.966]
Epoch [75/120    avg_loss:0.032, val_acc:0.967]
Epoch [76/120    avg_loss:0.028, val_acc:0.973]
Epoch [77/120    avg_loss:0.029, val_acc:0.974]
Epoch [78/120    avg_loss:0.025, val_acc:0.974]
Epoch [79/120    avg_loss:0.028, val_acc:0.977]
Epoch [80/120    avg_loss:0.027, val_acc:0.974]
Epoch [81/120    avg_loss:0.025, val_acc:0.973]
Epoch [82/120    avg_loss:0.026, val_acc:0.973]
Epoch [83/120    avg_loss:0.024, val_acc:0.973]
Epoch [84/120    avg_loss:0.021, val_acc:0.974]
Epoch [85/120    avg_loss:0.023, val_acc:0.976]
Epoch [86/120    avg_loss:0.021, val_acc:0.976]
Epoch [87/120    avg_loss:0.026, val_acc:0.975]
Epoch [88/120    avg_loss:0.022, val_acc:0.975]
Epoch [89/120    avg_loss:0.023, val_acc:0.973]
Epoch [90/120    avg_loss:0.024, val_acc:0.978]
Epoch [91/120    avg_loss:0.021, val_acc:0.975]
Epoch [92/120    avg_loss:0.021, val_acc:0.975]
Epoch [93/120    avg_loss:0.021, val_acc:0.976]
Epoch [94/120    avg_loss:0.018, val_acc:0.975]
Epoch [95/120    avg_loss:0.021, val_acc:0.975]
Epoch [96/120    avg_loss:0.020, val_acc:0.975]
Epoch [97/120    avg_loss:0.020, val_acc:0.976]
Epoch [98/120    avg_loss:0.018, val_acc:0.975]
Epoch [99/120    avg_loss:0.026, val_acc:0.974]
Epoch [100/120    avg_loss:0.017, val_acc:0.977]
Epoch [101/120    avg_loss:0.021, val_acc:0.975]
Epoch [102/120    avg_loss:0.019, val_acc:0.976]
Epoch [103/120    avg_loss:0.019, val_acc:0.976]
Epoch [104/120    avg_loss:0.022, val_acc:0.977]
Epoch [105/120    avg_loss:0.017, val_acc:0.977]
Epoch [106/120    avg_loss:0.018, val_acc:0.977]
Epoch [107/120    avg_loss:0.017, val_acc:0.977]
Epoch [108/120    avg_loss:0.020, val_acc:0.977]
Epoch [109/120    avg_loss:0.024, val_acc:0.978]
Epoch [110/120    avg_loss:0.021, val_acc:0.978]
Epoch [111/120    avg_loss:0.021, val_acc:0.978]
Epoch [112/120    avg_loss:0.017, val_acc:0.978]
Epoch [113/120    avg_loss:0.020, val_acc:0.978]
Epoch [114/120    avg_loss:0.016, val_acc:0.978]
Epoch [115/120    avg_loss:0.018, val_acc:0.978]
Epoch [116/120    avg_loss:0.017, val_acc:0.978]
Epoch [117/120    avg_loss:0.016, val_acc:0.978]
Epoch [118/120    avg_loss:0.021, val_acc:0.978]
Epoch [119/120    avg_loss:0.019, val_acc:0.978]
Epoch [120/120    avg_loss:0.020, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1267    2    1    0    1    0    0    1    2   10    1    0
     0    0    0]
 [   0    0    0  701    4    9    0    0    0    7    2    6   17    0
     1    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  426    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    0    0    0    0    1  854    9    1    0
     0    0    0]
 [   0    0   33    0    0    0    4    0    0    0   22 2145    4    1
     1    0    0]
 [   0    0    0    6    0    0    0    0    0    0    6    1  517    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1134    5    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    50  297    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.46341463414635

F1 scores:
[       nan 0.9382716  0.9761171  0.96291209 0.98604651 0.98861048
 0.99468489 1.         0.9953271  0.8        0.96880318 0.97878166
 0.95918367 0.99730458 0.97464547 0.91525424 0.97647059]

Kappa:
0.9710863480468347
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0a43900898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.785, val_acc:0.294]
Epoch [2/120    avg_loss:2.533, val_acc:0.471]
Epoch [3/120    avg_loss:2.340, val_acc:0.490]
Epoch [4/120    avg_loss:2.244, val_acc:0.545]
Epoch [5/120    avg_loss:2.109, val_acc:0.557]
Epoch [6/120    avg_loss:2.009, val_acc:0.570]
Epoch [7/120    avg_loss:1.871, val_acc:0.579]
Epoch [8/120    avg_loss:1.764, val_acc:0.592]
Epoch [9/120    avg_loss:1.641, val_acc:0.614]
Epoch [10/120    avg_loss:1.577, val_acc:0.628]
Epoch [11/120    avg_loss:1.438, val_acc:0.643]
Epoch [12/120    avg_loss:1.334, val_acc:0.671]
Epoch [13/120    avg_loss:1.231, val_acc:0.674]
Epoch [14/120    avg_loss:1.103, val_acc:0.705]
Epoch [15/120    avg_loss:1.022, val_acc:0.722]
Epoch [16/120    avg_loss:0.918, val_acc:0.782]
Epoch [17/120    avg_loss:0.822, val_acc:0.801]
Epoch [18/120    avg_loss:0.744, val_acc:0.807]
Epoch [19/120    avg_loss:0.677, val_acc:0.816]
Epoch [20/120    avg_loss:0.576, val_acc:0.842]
Epoch [21/120    avg_loss:0.558, val_acc:0.842]
Epoch [22/120    avg_loss:0.482, val_acc:0.850]
Epoch [23/120    avg_loss:0.432, val_acc:0.869]
Epoch [24/120    avg_loss:0.412, val_acc:0.874]
Epoch [25/120    avg_loss:0.352, val_acc:0.887]
Epoch [26/120    avg_loss:0.320, val_acc:0.897]
Epoch [27/120    avg_loss:0.305, val_acc:0.898]
Epoch [28/120    avg_loss:0.287, val_acc:0.890]
Epoch [29/120    avg_loss:0.256, val_acc:0.904]
Epoch [30/120    avg_loss:0.349, val_acc:0.808]
Epoch [31/120    avg_loss:0.416, val_acc:0.791]
Epoch [32/120    avg_loss:0.422, val_acc:0.833]
Epoch [33/120    avg_loss:0.317, val_acc:0.873]
Epoch [34/120    avg_loss:0.242, val_acc:0.890]
Epoch [35/120    avg_loss:0.235, val_acc:0.892]
Epoch [36/120    avg_loss:0.214, val_acc:0.897]
Epoch [37/120    avg_loss:0.183, val_acc:0.912]
Epoch [38/120    avg_loss:0.176, val_acc:0.911]
Epoch [39/120    avg_loss:0.147, val_acc:0.915]
Epoch [40/120    avg_loss:0.147, val_acc:0.928]
Epoch [41/120    avg_loss:0.130, val_acc:0.944]
Epoch [42/120    avg_loss:0.103, val_acc:0.936]
Epoch [43/120    avg_loss:0.106, val_acc:0.935]
Epoch [44/120    avg_loss:0.089, val_acc:0.926]
Epoch [45/120    avg_loss:0.116, val_acc:0.933]
Epoch [46/120    avg_loss:0.109, val_acc:0.950]
Epoch [47/120    avg_loss:0.109, val_acc:0.947]
Epoch [48/120    avg_loss:0.093, val_acc:0.944]
Epoch [49/120    avg_loss:0.083, val_acc:0.938]
Epoch [50/120    avg_loss:0.079, val_acc:0.947]
Epoch [51/120    avg_loss:0.087, val_acc:0.941]
Epoch [52/120    avg_loss:0.074, val_acc:0.957]
Epoch [53/120    avg_loss:0.066, val_acc:0.955]
Epoch [54/120    avg_loss:0.068, val_acc:0.948]
Epoch [55/120    avg_loss:0.060, val_acc:0.959]
Epoch [56/120    avg_loss:0.063, val_acc:0.946]
Epoch [57/120    avg_loss:0.068, val_acc:0.942]
Epoch [58/120    avg_loss:0.058, val_acc:0.956]
Epoch [59/120    avg_loss:0.054, val_acc:0.949]
Epoch [60/120    avg_loss:0.055, val_acc:0.932]
Epoch [61/120    avg_loss:0.059, val_acc:0.958]
Epoch [62/120    avg_loss:0.041, val_acc:0.960]
Epoch [63/120    avg_loss:0.043, val_acc:0.968]
Epoch [64/120    avg_loss:0.044, val_acc:0.957]
Epoch [65/120    avg_loss:0.036, val_acc:0.964]
Epoch [66/120    avg_loss:0.038, val_acc:0.955]
Epoch [67/120    avg_loss:0.045, val_acc:0.959]
Epoch [68/120    avg_loss:0.045, val_acc:0.961]
Epoch [69/120    avg_loss:0.040, val_acc:0.946]
Epoch [70/120    avg_loss:0.052, val_acc:0.960]
Epoch [71/120    avg_loss:0.060, val_acc:0.949]
Epoch [72/120    avg_loss:0.111, val_acc:0.951]
Epoch [73/120    avg_loss:0.070, val_acc:0.949]
Epoch [74/120    avg_loss:0.071, val_acc:0.953]
Epoch [75/120    avg_loss:0.102, val_acc:0.928]
Epoch [76/120    avg_loss:0.073, val_acc:0.960]
Epoch [77/120    avg_loss:0.047, val_acc:0.964]
Epoch [78/120    avg_loss:0.033, val_acc:0.963]
Epoch [79/120    avg_loss:0.038, val_acc:0.959]
Epoch [80/120    avg_loss:0.032, val_acc:0.965]
Epoch [81/120    avg_loss:0.030, val_acc:0.963]
Epoch [82/120    avg_loss:0.035, val_acc:0.966]
Epoch [83/120    avg_loss:0.027, val_acc:0.968]
Epoch [84/120    avg_loss:0.030, val_acc:0.964]
Epoch [85/120    avg_loss:0.028, val_acc:0.965]
Epoch [86/120    avg_loss:0.026, val_acc:0.968]
Epoch [87/120    avg_loss:0.038, val_acc:0.968]
Epoch [88/120    avg_loss:0.021, val_acc:0.965]
Epoch [89/120    avg_loss:0.031, val_acc:0.969]
Epoch [90/120    avg_loss:0.026, val_acc:0.967]
Epoch [91/120    avg_loss:0.023, val_acc:0.964]
Epoch [92/120    avg_loss:0.026, val_acc:0.968]
Epoch [93/120    avg_loss:0.024, val_acc:0.970]
Epoch [94/120    avg_loss:0.022, val_acc:0.967]
Epoch [95/120    avg_loss:0.023, val_acc:0.969]
Epoch [96/120    avg_loss:0.024, val_acc:0.970]
Epoch [97/120    avg_loss:0.025, val_acc:0.968]
Epoch [98/120    avg_loss:0.023, val_acc:0.968]
Epoch [99/120    avg_loss:0.026, val_acc:0.966]
Epoch [100/120    avg_loss:0.021, val_acc:0.968]
Epoch [101/120    avg_loss:0.022, val_acc:0.970]
Epoch [102/120    avg_loss:0.024, val_acc:0.969]
Epoch [103/120    avg_loss:0.023, val_acc:0.970]
Epoch [104/120    avg_loss:0.021, val_acc:0.972]
Epoch [105/120    avg_loss:0.021, val_acc:0.970]
Epoch [106/120    avg_loss:0.022, val_acc:0.971]
Epoch [107/120    avg_loss:0.022, val_acc:0.975]
Epoch [108/120    avg_loss:0.022, val_acc:0.972]
Epoch [109/120    avg_loss:0.022, val_acc:0.973]
Epoch [110/120    avg_loss:0.025, val_acc:0.975]
Epoch [111/120    avg_loss:0.021, val_acc:0.972]
Epoch [112/120    avg_loss:0.021, val_acc:0.974]
Epoch [113/120    avg_loss:0.023, val_acc:0.972]
Epoch [114/120    avg_loss:0.018, val_acc:0.973]
Epoch [115/120    avg_loss:0.021, val_acc:0.975]
Epoch [116/120    avg_loss:0.020, val_acc:0.977]
Epoch [117/120    avg_loss:0.022, val_acc:0.978]
Epoch [118/120    avg_loss:0.018, val_acc:0.974]
Epoch [119/120    avg_loss:0.020, val_acc:0.974]
Epoch [120/120    avg_loss:0.019, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1245    2    4    0    0    0    0    2    3   28    1    0
     0    0    0]
 [   0    0    0  719   13    0    0    0    0    2    3    1    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    5    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    3    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    3    0    0    1  825   38    0    0
     0    3    0]
 [   0    0    1    1    0    0    5    0    0    1    3 2186   13    0
     0    0    0]
 [   0    0    0    5    4    0    0    0    0    0    0    2  520    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    0    0    0
  1135    2    0]
 [   0    0    0    0    0    0   38    0    0    0    0    0    0    0
    17  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.50677506775068

F1 scores:
[       nan 0.98765432 0.9818612  0.97557666 0.95302013 0.98957126
 0.96236162 0.90909091 1.         0.85714286 0.96434833 0.97829492
 0.96474954 1.         0.98910675 0.90542636 0.98224852]

Kappa:
0.9715552034169952
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdf0460a898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.761, val_acc:0.291]
Epoch [2/120    avg_loss:2.506, val_acc:0.413]
Epoch [3/120    avg_loss:2.317, val_acc:0.462]
Epoch [4/120    avg_loss:2.173, val_acc:0.487]
Epoch [5/120    avg_loss:2.038, val_acc:0.499]
Epoch [6/120    avg_loss:1.925, val_acc:0.514]
Epoch [7/120    avg_loss:1.770, val_acc:0.547]
Epoch [8/120    avg_loss:1.663, val_acc:0.552]
Epoch [9/120    avg_loss:1.557, val_acc:0.565]
Epoch [10/120    avg_loss:1.441, val_acc:0.592]
Epoch [11/120    avg_loss:1.375, val_acc:0.629]
Epoch [12/120    avg_loss:1.264, val_acc:0.642]
Epoch [13/120    avg_loss:1.225, val_acc:0.640]
Epoch [14/120    avg_loss:1.120, val_acc:0.673]
Epoch [15/120    avg_loss:1.038, val_acc:0.721]
Epoch [16/120    avg_loss:0.957, val_acc:0.709]
Epoch [17/120    avg_loss:0.836, val_acc:0.774]
Epoch [18/120    avg_loss:0.797, val_acc:0.758]
Epoch [19/120    avg_loss:0.754, val_acc:0.779]
Epoch [20/120    avg_loss:0.738, val_acc:0.791]
Epoch [21/120    avg_loss:0.625, val_acc:0.802]
Epoch [22/120    avg_loss:0.629, val_acc:0.829]
Epoch [23/120    avg_loss:0.508, val_acc:0.851]
Epoch [24/120    avg_loss:0.460, val_acc:0.879]
Epoch [25/120    avg_loss:0.391, val_acc:0.878]
Epoch [26/120    avg_loss:0.353, val_acc:0.890]
Epoch [27/120    avg_loss:0.339, val_acc:0.892]
Epoch [28/120    avg_loss:0.297, val_acc:0.909]
Epoch [29/120    avg_loss:0.285, val_acc:0.914]
Epoch [30/120    avg_loss:0.247, val_acc:0.927]
Epoch [31/120    avg_loss:0.224, val_acc:0.915]
Epoch [32/120    avg_loss:0.212, val_acc:0.918]
Epoch [33/120    avg_loss:0.227, val_acc:0.882]
Epoch [34/120    avg_loss:0.256, val_acc:0.913]
Epoch [35/120    avg_loss:0.207, val_acc:0.943]
Epoch [36/120    avg_loss:0.171, val_acc:0.927]
Epoch [37/120    avg_loss:0.166, val_acc:0.940]
Epoch [38/120    avg_loss:0.178, val_acc:0.926]
Epoch [39/120    avg_loss:0.183, val_acc:0.867]
Epoch [40/120    avg_loss:0.156, val_acc:0.928]
Epoch [41/120    avg_loss:0.131, val_acc:0.952]
Epoch [42/120    avg_loss:0.128, val_acc:0.924]
Epoch [43/120    avg_loss:0.126, val_acc:0.936]
Epoch [44/120    avg_loss:0.103, val_acc:0.948]
Epoch [45/120    avg_loss:0.091, val_acc:0.950]
Epoch [46/120    avg_loss:0.153, val_acc:0.936]
Epoch [47/120    avg_loss:0.139, val_acc:0.934]
Epoch [48/120    avg_loss:0.100, val_acc:0.971]
Epoch [49/120    avg_loss:0.084, val_acc:0.948]
Epoch [50/120    avg_loss:0.080, val_acc:0.962]
Epoch [51/120    avg_loss:0.081, val_acc:0.960]
Epoch [52/120    avg_loss:0.069, val_acc:0.963]
Epoch [53/120    avg_loss:0.087, val_acc:0.942]
Epoch [54/120    avg_loss:0.082, val_acc:0.948]
Epoch [55/120    avg_loss:0.093, val_acc:0.963]
Epoch [56/120    avg_loss:0.062, val_acc:0.964]
Epoch [57/120    avg_loss:0.060, val_acc:0.913]
Epoch [58/120    avg_loss:0.061, val_acc:0.965]
Epoch [59/120    avg_loss:0.047, val_acc:0.968]
Epoch [60/120    avg_loss:0.045, val_acc:0.970]
Epoch [61/120    avg_loss:0.052, val_acc:0.967]
Epoch [62/120    avg_loss:0.047, val_acc:0.972]
Epoch [63/120    avg_loss:0.044, val_acc:0.973]
Epoch [64/120    avg_loss:0.037, val_acc:0.973]
Epoch [65/120    avg_loss:0.036, val_acc:0.976]
Epoch [66/120    avg_loss:0.032, val_acc:0.978]
Epoch [67/120    avg_loss:0.030, val_acc:0.978]
Epoch [68/120    avg_loss:0.028, val_acc:0.978]
Epoch [69/120    avg_loss:0.032, val_acc:0.978]
Epoch [70/120    avg_loss:0.031, val_acc:0.973]
Epoch [71/120    avg_loss:0.033, val_acc:0.975]
Epoch [72/120    avg_loss:0.026, val_acc:0.980]
Epoch [73/120    avg_loss:0.030, val_acc:0.980]
Epoch [74/120    avg_loss:0.026, val_acc:0.976]
Epoch [75/120    avg_loss:0.030, val_acc:0.976]
Epoch [76/120    avg_loss:0.031, val_acc:0.980]
Epoch [77/120    avg_loss:0.027, val_acc:0.974]
Epoch [78/120    avg_loss:0.028, val_acc:0.973]
Epoch [79/120    avg_loss:0.027, val_acc:0.967]
Epoch [80/120    avg_loss:0.024, val_acc:0.976]
Epoch [81/120    avg_loss:0.025, val_acc:0.973]
Epoch [82/120    avg_loss:0.027, val_acc:0.973]
Epoch [83/120    avg_loss:0.025, val_acc:0.978]
Epoch [84/120    avg_loss:0.026, val_acc:0.978]
Epoch [85/120    avg_loss:0.026, val_acc:0.974]
Epoch [86/120    avg_loss:0.024, val_acc:0.972]
Epoch [87/120    avg_loss:0.023, val_acc:0.976]
Epoch [88/120    avg_loss:0.030, val_acc:0.978]
Epoch [89/120    avg_loss:0.023, val_acc:0.978]
Epoch [90/120    avg_loss:0.022, val_acc:0.978]
Epoch [91/120    avg_loss:0.023, val_acc:0.978]
Epoch [92/120    avg_loss:0.023, val_acc:0.978]
Epoch [93/120    avg_loss:0.030, val_acc:0.978]
Epoch [94/120    avg_loss:0.027, val_acc:0.978]
Epoch [95/120    avg_loss:0.026, val_acc:0.978]
Epoch [96/120    avg_loss:0.026, val_acc:0.976]
Epoch [97/120    avg_loss:0.026, val_acc:0.976]
Epoch [98/120    avg_loss:0.024, val_acc:0.976]
Epoch [99/120    avg_loss:0.021, val_acc:0.977]
Epoch [100/120    avg_loss:0.023, val_acc:0.977]
Epoch [101/120    avg_loss:0.025, val_acc:0.978]
Epoch [102/120    avg_loss:0.027, val_acc:0.976]
Epoch [103/120    avg_loss:0.024, val_acc:0.976]
Epoch [104/120    avg_loss:0.026, val_acc:0.976]
Epoch [105/120    avg_loss:0.022, val_acc:0.976]
Epoch [106/120    avg_loss:0.024, val_acc:0.976]
Epoch [107/120    avg_loss:0.021, val_acc:0.976]
Epoch [108/120    avg_loss:0.025, val_acc:0.976]
Epoch [109/120    avg_loss:0.024, val_acc:0.976]
Epoch [110/120    avg_loss:0.022, val_acc:0.976]
Epoch [111/120    avg_loss:0.024, val_acc:0.976]
Epoch [112/120    avg_loss:0.025, val_acc:0.976]
Epoch [113/120    avg_loss:0.023, val_acc:0.976]
Epoch [114/120    avg_loss:0.024, val_acc:0.976]
Epoch [115/120    avg_loss:0.027, val_acc:0.976]
Epoch [116/120    avg_loss:0.023, val_acc:0.976]
Epoch [117/120    avg_loss:0.026, val_acc:0.976]
Epoch [118/120    avg_loss:0.023, val_acc:0.976]
Epoch [119/120    avg_loss:0.024, val_acc:0.976]
Epoch [120/120    avg_loss:0.023, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1248    2    0    0    0    0    0    1    5   27    2    0
     0    0    0]
 [   0    0    0  702    4    0    0    0    0   12    2    8   19    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    4    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    7    0    0    1    0    0    0    2  834   29    1    0
     0    1    0]
 [   0    0    9    0    0    2    1    0    0    2   11 2180    5    0
     0    0    0]
 [   0    0    0   13    0    0    0    0    0    1    3    0  511    0
     0    2    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    0    0    0    0
  1132    1    0]
 [   0    0    0    0    0    0    8    0    0    3    0    0    0    0
    23  313    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.48509485094851

F1 scores:
[       nan 0.92857143 0.97920753 0.95901639 0.99069767 0.98514286
 0.99167298 0.92592593 0.9953271  0.60714286 0.9630485  0.97867565
 0.95069767 1.         0.98649237 0.94277108 0.96470588]

Kappa:
0.9713156883402495
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1e423aa898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.752, val_acc:0.406]
Epoch [2/120    avg_loss:2.478, val_acc:0.384]
Epoch [3/120    avg_loss:2.308, val_acc:0.445]
Epoch [4/120    avg_loss:2.157, val_acc:0.494]
Epoch [5/120    avg_loss:2.036, val_acc:0.575]
Epoch [6/120    avg_loss:1.876, val_acc:0.577]
Epoch [7/120    avg_loss:1.746, val_acc:0.604]
Epoch [8/120    avg_loss:1.675, val_acc:0.609]
Epoch [9/120    avg_loss:1.574, val_acc:0.620]
Epoch [10/120    avg_loss:1.433, val_acc:0.646]
Epoch [11/120    avg_loss:1.308, val_acc:0.670]
Epoch [12/120    avg_loss:1.168, val_acc:0.684]
Epoch [13/120    avg_loss:1.054, val_acc:0.706]
Epoch [14/120    avg_loss:0.932, val_acc:0.742]
Epoch [15/120    avg_loss:0.882, val_acc:0.739]
Epoch [16/120    avg_loss:0.860, val_acc:0.742]
Epoch [17/120    avg_loss:0.775, val_acc:0.774]
Epoch [18/120    avg_loss:0.695, val_acc:0.808]
Epoch [19/120    avg_loss:0.614, val_acc:0.818]
Epoch [20/120    avg_loss:0.510, val_acc:0.846]
Epoch [21/120    avg_loss:0.458, val_acc:0.828]
Epoch [22/120    avg_loss:0.486, val_acc:0.832]
Epoch [23/120    avg_loss:0.492, val_acc:0.859]
Epoch [24/120    avg_loss:0.388, val_acc:0.884]
Epoch [25/120    avg_loss:0.355, val_acc:0.876]
Epoch [26/120    avg_loss:0.324, val_acc:0.884]
Epoch [27/120    avg_loss:0.284, val_acc:0.890]
Epoch [28/120    avg_loss:0.279, val_acc:0.896]
Epoch [29/120    avg_loss:0.260, val_acc:0.918]
Epoch [30/120    avg_loss:0.241, val_acc:0.911]
Epoch [31/120    avg_loss:0.239, val_acc:0.908]
Epoch [32/120    avg_loss:0.284, val_acc:0.876]
Epoch [33/120    avg_loss:0.228, val_acc:0.929]
Epoch [34/120    avg_loss:0.171, val_acc:0.928]
Epoch [35/120    avg_loss:0.181, val_acc:0.897]
Epoch [36/120    avg_loss:0.188, val_acc:0.926]
Epoch [37/120    avg_loss:0.147, val_acc:0.915]
Epoch [38/120    avg_loss:0.127, val_acc:0.901]
Epoch [39/120    avg_loss:0.158, val_acc:0.920]
Epoch [40/120    avg_loss:0.117, val_acc:0.947]
Epoch [41/120    avg_loss:0.122, val_acc:0.925]
Epoch [42/120    avg_loss:0.123, val_acc:0.927]
Epoch [43/120    avg_loss:0.123, val_acc:0.956]
Epoch [44/120    avg_loss:0.095, val_acc:0.947]
Epoch [45/120    avg_loss:0.081, val_acc:0.960]
Epoch [46/120    avg_loss:0.091, val_acc:0.955]
Epoch [47/120    avg_loss:0.075, val_acc:0.944]
Epoch [48/120    avg_loss:0.077, val_acc:0.947]
Epoch [49/120    avg_loss:0.093, val_acc:0.954]
Epoch [50/120    avg_loss:0.072, val_acc:0.964]
Epoch [51/120    avg_loss:0.079, val_acc:0.960]
Epoch [52/120    avg_loss:0.065, val_acc:0.963]
Epoch [53/120    avg_loss:0.057, val_acc:0.961]
Epoch [54/120    avg_loss:0.052, val_acc:0.959]
Epoch [55/120    avg_loss:0.059, val_acc:0.960]
Epoch [56/120    avg_loss:0.081, val_acc:0.953]
Epoch [57/120    avg_loss:0.059, val_acc:0.958]
Epoch [58/120    avg_loss:0.062, val_acc:0.968]
Epoch [59/120    avg_loss:0.050, val_acc:0.972]
Epoch [60/120    avg_loss:0.055, val_acc:0.961]
Epoch [61/120    avg_loss:0.044, val_acc:0.965]
Epoch [62/120    avg_loss:0.046, val_acc:0.967]
Epoch [63/120    avg_loss:0.054, val_acc:0.961]
Epoch [64/120    avg_loss:0.059, val_acc:0.963]
Epoch [65/120    avg_loss:0.103, val_acc:0.940]
Epoch [66/120    avg_loss:0.068, val_acc:0.953]
Epoch [67/120    avg_loss:0.070, val_acc:0.959]
Epoch [68/120    avg_loss:0.052, val_acc:0.973]
Epoch [69/120    avg_loss:0.047, val_acc:0.965]
Epoch [70/120    avg_loss:0.050, val_acc:0.965]
Epoch [71/120    avg_loss:0.034, val_acc:0.950]
Epoch [72/120    avg_loss:0.038, val_acc:0.966]
Epoch [73/120    avg_loss:0.039, val_acc:0.959]
Epoch [74/120    avg_loss:0.038, val_acc:0.967]
Epoch [75/120    avg_loss:0.050, val_acc:0.964]
Epoch [76/120    avg_loss:0.051, val_acc:0.969]
Epoch [77/120    avg_loss:0.053, val_acc:0.959]
Epoch [78/120    avg_loss:0.069, val_acc:0.964]
Epoch [79/120    avg_loss:0.049, val_acc:0.966]
Epoch [80/120    avg_loss:0.038, val_acc:0.972]
Epoch [81/120    avg_loss:0.032, val_acc:0.973]
Epoch [82/120    avg_loss:0.030, val_acc:0.960]
Epoch [83/120    avg_loss:0.042, val_acc:0.966]
Epoch [84/120    avg_loss:0.033, val_acc:0.966]
Epoch [85/120    avg_loss:0.026, val_acc:0.975]
Epoch [86/120    avg_loss:0.022, val_acc:0.975]
Epoch [87/120    avg_loss:0.022, val_acc:0.971]
Epoch [88/120    avg_loss:0.030, val_acc:0.971]
Epoch [89/120    avg_loss:0.033, val_acc:0.967]
Epoch [90/120    avg_loss:0.021, val_acc:0.976]
Epoch [91/120    avg_loss:0.016, val_acc:0.978]
Epoch [92/120    avg_loss:0.019, val_acc:0.972]
Epoch [93/120    avg_loss:0.017, val_acc:0.977]
Epoch [94/120    avg_loss:0.020, val_acc:0.975]
Epoch [95/120    avg_loss:0.023, val_acc:0.975]
Epoch [96/120    avg_loss:0.018, val_acc:0.977]
Epoch [97/120    avg_loss:0.020, val_acc:0.978]
Epoch [98/120    avg_loss:0.033, val_acc:0.967]
Epoch [99/120    avg_loss:0.030, val_acc:0.972]
Epoch [100/120    avg_loss:0.033, val_acc:0.981]
Epoch [101/120    avg_loss:0.026, val_acc:0.976]
Epoch [102/120    avg_loss:0.019, val_acc:0.977]
Epoch [103/120    avg_loss:0.019, val_acc:0.967]
Epoch [104/120    avg_loss:0.019, val_acc:0.981]
Epoch [105/120    avg_loss:0.017, val_acc:0.980]
Epoch [106/120    avg_loss:0.014, val_acc:0.975]
Epoch [107/120    avg_loss:0.012, val_acc:0.978]
Epoch [108/120    avg_loss:0.014, val_acc:0.981]
Epoch [109/120    avg_loss:0.015, val_acc:0.972]
Epoch [110/120    avg_loss:0.018, val_acc:0.978]
Epoch [111/120    avg_loss:0.021, val_acc:0.973]
Epoch [112/120    avg_loss:0.018, val_acc:0.977]
Epoch [113/120    avg_loss:0.012, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.986]
Epoch [115/120    avg_loss:0.009, val_acc:0.982]
Epoch [116/120    avg_loss:0.012, val_acc:0.981]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.012, val_acc:0.982]
Epoch [119/120    avg_loss:0.017, val_acc:0.979]
Epoch [120/120    avg_loss:0.012, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1266    3    4    0    1    0    0    0    0   11    0    0
     0    0    0]
 [   0    0    0  717    0    1    3    0    0    3    1    4   18    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    7    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    3    3    0    0    0  845   12    0    0
     0    2    0]
 [   0    0    6    0    0    0    5    0    0    0   16 2166   16    1
     0    0    0]
 [   0    0    0    7    0    1    0    0    0    0    1   12  512    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0    0
  1130    8    0]
 [   0    0    0    0    0    1   16    0    0    0    0    0    0    0
    42  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.47425474254743

F1 scores:
[       nan 0.97619048 0.98636541 0.97154472 0.98598131 0.97916667
 0.97837435 0.87719298 0.997669   0.9        0.97238205 0.98097826
 0.9455217  0.99730458 0.97582038 0.89302326 0.97590361]

Kappa:
0.9711993148373378
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3589fee898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.768, val_acc:0.282]
Epoch [2/120    avg_loss:2.524, val_acc:0.481]
Epoch [3/120    avg_loss:2.334, val_acc:0.483]
Epoch [4/120    avg_loss:2.170, val_acc:0.507]
Epoch [5/120    avg_loss:2.033, val_acc:0.523]
Epoch [6/120    avg_loss:1.950, val_acc:0.545]
Epoch [7/120    avg_loss:1.842, val_acc:0.606]
Epoch [8/120    avg_loss:1.671, val_acc:0.609]
Epoch [9/120    avg_loss:1.608, val_acc:0.626]
Epoch [10/120    avg_loss:1.468, val_acc:0.651]
Epoch [11/120    avg_loss:1.361, val_acc:0.649]
Epoch [12/120    avg_loss:1.243, val_acc:0.661]
Epoch [13/120    avg_loss:1.264, val_acc:0.695]
Epoch [14/120    avg_loss:1.083, val_acc:0.708]
Epoch [15/120    avg_loss:1.097, val_acc:0.710]
Epoch [16/120    avg_loss:0.913, val_acc:0.734]
Epoch [17/120    avg_loss:0.769, val_acc:0.781]
Epoch [18/120    avg_loss:0.687, val_acc:0.807]
Epoch [19/120    avg_loss:0.667, val_acc:0.795]
Epoch [20/120    avg_loss:0.601, val_acc:0.797]
Epoch [21/120    avg_loss:0.567, val_acc:0.793]
Epoch [22/120    avg_loss:0.497, val_acc:0.825]
Epoch [23/120    avg_loss:0.467, val_acc:0.827]
Epoch [24/120    avg_loss:0.431, val_acc:0.842]
Epoch [25/120    avg_loss:0.401, val_acc:0.831]
Epoch [26/120    avg_loss:0.365, val_acc:0.841]
Epoch [27/120    avg_loss:0.306, val_acc:0.849]
Epoch [28/120    avg_loss:0.304, val_acc:0.855]
Epoch [29/120    avg_loss:0.273, val_acc:0.865]
Epoch [30/120    avg_loss:0.279, val_acc:0.880]
Epoch [31/120    avg_loss:0.266, val_acc:0.895]
Epoch [32/120    avg_loss:0.259, val_acc:0.863]
Epoch [33/120    avg_loss:0.264, val_acc:0.860]
Epoch [34/120    avg_loss:0.209, val_acc:0.901]
Epoch [35/120    avg_loss:0.229, val_acc:0.904]
Epoch [36/120    avg_loss:0.263, val_acc:0.851]
Epoch [37/120    avg_loss:0.208, val_acc:0.908]
Epoch [38/120    avg_loss:0.170, val_acc:0.924]
Epoch [39/120    avg_loss:0.143, val_acc:0.908]
Epoch [40/120    avg_loss:0.124, val_acc:0.922]
Epoch [41/120    avg_loss:0.116, val_acc:0.917]
Epoch [42/120    avg_loss:0.143, val_acc:0.910]
Epoch [43/120    avg_loss:0.171, val_acc:0.905]
Epoch [44/120    avg_loss:0.169, val_acc:0.905]
Epoch [45/120    avg_loss:0.125, val_acc:0.916]
Epoch [46/120    avg_loss:0.120, val_acc:0.915]
Epoch [47/120    avg_loss:0.103, val_acc:0.929]
Epoch [48/120    avg_loss:0.106, val_acc:0.931]
Epoch [49/120    avg_loss:0.100, val_acc:0.929]
Epoch [50/120    avg_loss:0.095, val_acc:0.945]
Epoch [51/120    avg_loss:0.097, val_acc:0.945]
Epoch [52/120    avg_loss:0.083, val_acc:0.928]
Epoch [53/120    avg_loss:0.074, val_acc:0.944]
Epoch [54/120    avg_loss:0.073, val_acc:0.941]
Epoch [55/120    avg_loss:0.094, val_acc:0.935]
Epoch [56/120    avg_loss:0.066, val_acc:0.949]
Epoch [57/120    avg_loss:0.077, val_acc:0.933]
Epoch [58/120    avg_loss:0.113, val_acc:0.923]
Epoch [59/120    avg_loss:0.183, val_acc:0.931]
Epoch [60/120    avg_loss:0.131, val_acc:0.927]
Epoch [61/120    avg_loss:0.147, val_acc:0.920]
Epoch [62/120    avg_loss:0.080, val_acc:0.941]
Epoch [63/120    avg_loss:0.088, val_acc:0.927]
Epoch [64/120    avg_loss:0.080, val_acc:0.928]
Epoch [65/120    avg_loss:0.063, val_acc:0.939]
Epoch [66/120    avg_loss:0.051, val_acc:0.947]
Epoch [67/120    avg_loss:0.058, val_acc:0.925]
Epoch [68/120    avg_loss:0.054, val_acc:0.945]
Epoch [69/120    avg_loss:0.062, val_acc:0.931]
Epoch [70/120    avg_loss:0.046, val_acc:0.949]
Epoch [71/120    avg_loss:0.043, val_acc:0.948]
Epoch [72/120    avg_loss:0.038, val_acc:0.950]
Epoch [73/120    avg_loss:0.034, val_acc:0.951]
Epoch [74/120    avg_loss:0.034, val_acc:0.950]
Epoch [75/120    avg_loss:0.031, val_acc:0.956]
Epoch [76/120    avg_loss:0.040, val_acc:0.957]
Epoch [77/120    avg_loss:0.030, val_acc:0.958]
Epoch [78/120    avg_loss:0.038, val_acc:0.956]
Epoch [79/120    avg_loss:0.038, val_acc:0.955]
Epoch [80/120    avg_loss:0.032, val_acc:0.956]
Epoch [81/120    avg_loss:0.030, val_acc:0.959]
Epoch [82/120    avg_loss:0.035, val_acc:0.957]
Epoch [83/120    avg_loss:0.030, val_acc:0.955]
Epoch [84/120    avg_loss:0.029, val_acc:0.954]
Epoch [85/120    avg_loss:0.033, val_acc:0.951]
Epoch [86/120    avg_loss:0.031, val_acc:0.956]
Epoch [87/120    avg_loss:0.029, val_acc:0.957]
Epoch [88/120    avg_loss:0.029, val_acc:0.957]
Epoch [89/120    avg_loss:0.029, val_acc:0.955]
Epoch [90/120    avg_loss:0.038, val_acc:0.958]
Epoch [91/120    avg_loss:0.028, val_acc:0.959]
Epoch [92/120    avg_loss:0.024, val_acc:0.960]
Epoch [93/120    avg_loss:0.030, val_acc:0.956]
Epoch [94/120    avg_loss:0.033, val_acc:0.958]
Epoch [95/120    avg_loss:0.026, val_acc:0.957]
Epoch [96/120    avg_loss:0.024, val_acc:0.959]
Epoch [97/120    avg_loss:0.031, val_acc:0.960]
Epoch [98/120    avg_loss:0.027, val_acc:0.961]
Epoch [99/120    avg_loss:0.024, val_acc:0.959]
Epoch [100/120    avg_loss:0.029, val_acc:0.959]
Epoch [101/120    avg_loss:0.026, val_acc:0.961]
Epoch [102/120    avg_loss:0.031, val_acc:0.960]
Epoch [103/120    avg_loss:0.026, val_acc:0.961]
Epoch [104/120    avg_loss:0.025, val_acc:0.961]
Epoch [105/120    avg_loss:0.025, val_acc:0.963]
Epoch [106/120    avg_loss:0.026, val_acc:0.959]
Epoch [107/120    avg_loss:0.027, val_acc:0.959]
Epoch [108/120    avg_loss:0.026, val_acc:0.959]
Epoch [109/120    avg_loss:0.025, val_acc:0.961]
Epoch [110/120    avg_loss:0.026, val_acc:0.961]
Epoch [111/120    avg_loss:0.031, val_acc:0.960]
Epoch [112/120    avg_loss:0.027, val_acc:0.961]
Epoch [113/120    avg_loss:0.028, val_acc:0.958]
Epoch [114/120    avg_loss:0.024, val_acc:0.960]
Epoch [115/120    avg_loss:0.027, val_acc:0.963]
Epoch [116/120    avg_loss:0.021, val_acc:0.963]
Epoch [117/120    avg_loss:0.024, val_acc:0.963]
Epoch [118/120    avg_loss:0.025, val_acc:0.960]
Epoch [119/120    avg_loss:0.020, val_acc:0.959]
Epoch [120/120    avg_loss:0.026, val_acc:0.959]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    2    0    0    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0 1246    1    6    0    0    0    0    0    5   27    0    0
     0    0    0]
 [   0    0    0  718    8    0    0    0    0    7    5    3    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    5    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   17    0    0    1    0    0    0    3  844    8    0    0
     0    2    0]
 [   0    0   13    0    0    1    2    0    0    1   25 2163    5    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0    0    0    1  520    0
     0    4    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1131    6    0]
 [   0    0    0    0    0    1   11    0    0    3    0    0    0    0
    15  317    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.6910569105691

F1 scores:
[       nan 0.96202532 0.97229809 0.97620666 0.96818182 0.98611111
 0.98944193 0.90909091 0.99767442 0.72       0.96182336 0.98050771
 0.97378277 1.         0.98777293 0.93786982 0.97076023]

Kappa:
0.9736841657661911
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1968762828>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.699, val_acc:0.187]
Epoch [2/120    avg_loss:2.479, val_acc:0.341]
Epoch [3/120    avg_loss:2.299, val_acc:0.375]
Epoch [4/120    avg_loss:2.161, val_acc:0.393]
Epoch [5/120    avg_loss:2.034, val_acc:0.489]
Epoch [6/120    avg_loss:1.966, val_acc:0.535]
Epoch [7/120    avg_loss:1.838, val_acc:0.539]
Epoch [8/120    avg_loss:1.735, val_acc:0.582]
Epoch [9/120    avg_loss:1.674, val_acc:0.602]
Epoch [10/120    avg_loss:1.552, val_acc:0.603]
Epoch [11/120    avg_loss:1.461, val_acc:0.609]
Epoch [12/120    avg_loss:1.333, val_acc:0.632]
Epoch [13/120    avg_loss:1.244, val_acc:0.620]
Epoch [14/120    avg_loss:1.171, val_acc:0.659]
Epoch [15/120    avg_loss:1.092, val_acc:0.677]
Epoch [16/120    avg_loss:1.009, val_acc:0.685]
Epoch [17/120    avg_loss:0.918, val_acc:0.733]
Epoch [18/120    avg_loss:0.897, val_acc:0.758]
Epoch [19/120    avg_loss:0.773, val_acc:0.801]
Epoch [20/120    avg_loss:0.716, val_acc:0.762]
Epoch [21/120    avg_loss:0.755, val_acc:0.800]
Epoch [22/120    avg_loss:0.678, val_acc:0.793]
Epoch [23/120    avg_loss:0.569, val_acc:0.824]
Epoch [24/120    avg_loss:0.521, val_acc:0.834]
Epoch [25/120    avg_loss:0.496, val_acc:0.843]
Epoch [26/120    avg_loss:0.488, val_acc:0.811]
Epoch [27/120    avg_loss:0.452, val_acc:0.848]
Epoch [28/120    avg_loss:0.408, val_acc:0.837]
Epoch [29/120    avg_loss:0.374, val_acc:0.824]
Epoch [30/120    avg_loss:0.401, val_acc:0.883]
Epoch [31/120    avg_loss:0.379, val_acc:0.823]
Epoch [32/120    avg_loss:0.456, val_acc:0.854]
Epoch [33/120    avg_loss:0.356, val_acc:0.890]
Epoch [34/120    avg_loss:0.288, val_acc:0.882]
Epoch [35/120    avg_loss:0.323, val_acc:0.873]
Epoch [36/120    avg_loss:0.237, val_acc:0.892]
Epoch [37/120    avg_loss:0.209, val_acc:0.911]
Epoch [38/120    avg_loss:0.235, val_acc:0.891]
Epoch [39/120    avg_loss:0.199, val_acc:0.917]
Epoch [40/120    avg_loss:0.188, val_acc:0.924]
Epoch [41/120    avg_loss:0.192, val_acc:0.882]
Epoch [42/120    avg_loss:0.237, val_acc:0.900]
Epoch [43/120    avg_loss:0.222, val_acc:0.921]
Epoch [44/120    avg_loss:0.197, val_acc:0.918]
Epoch [45/120    avg_loss:0.150, val_acc:0.930]
Epoch [46/120    avg_loss:0.128, val_acc:0.950]
Epoch [47/120    avg_loss:0.119, val_acc:0.954]
Epoch [48/120    avg_loss:0.105, val_acc:0.935]
Epoch [49/120    avg_loss:0.094, val_acc:0.961]
Epoch [50/120    avg_loss:0.094, val_acc:0.962]
Epoch [51/120    avg_loss:0.102, val_acc:0.960]
Epoch [52/120    avg_loss:0.099, val_acc:0.939]
Epoch [53/120    avg_loss:0.105, val_acc:0.947]
Epoch [54/120    avg_loss:0.107, val_acc:0.938]
Epoch [55/120    avg_loss:0.083, val_acc:0.967]
Epoch [56/120    avg_loss:0.090, val_acc:0.952]
Epoch [57/120    avg_loss:0.162, val_acc:0.934]
Epoch [58/120    avg_loss:0.096, val_acc:0.954]
Epoch [59/120    avg_loss:0.092, val_acc:0.960]
Epoch [60/120    avg_loss:0.097, val_acc:0.960]
Epoch [61/120    avg_loss:0.078, val_acc:0.970]
Epoch [62/120    avg_loss:0.063, val_acc:0.966]
Epoch [63/120    avg_loss:0.072, val_acc:0.939]
Epoch [64/120    avg_loss:0.069, val_acc:0.970]
Epoch [65/120    avg_loss:0.061, val_acc:0.966]
Epoch [66/120    avg_loss:0.067, val_acc:0.960]
Epoch [67/120    avg_loss:0.058, val_acc:0.974]
Epoch [68/120    avg_loss:0.064, val_acc:0.965]
Epoch [69/120    avg_loss:0.075, val_acc:0.963]
Epoch [70/120    avg_loss:0.179, val_acc:0.839]
Epoch [71/120    avg_loss:0.329, val_acc:0.882]
Epoch [72/120    avg_loss:0.374, val_acc:0.868]
Epoch [73/120    avg_loss:0.194, val_acc:0.926]
Epoch [74/120    avg_loss:0.141, val_acc:0.945]
Epoch [75/120    avg_loss:0.086, val_acc:0.962]
Epoch [76/120    avg_loss:0.084, val_acc:0.938]
Epoch [77/120    avg_loss:0.124, val_acc:0.923]
Epoch [78/120    avg_loss:0.078, val_acc:0.966]
Epoch [79/120    avg_loss:0.063, val_acc:0.961]
Epoch [80/120    avg_loss:0.053, val_acc:0.970]
Epoch [81/120    avg_loss:0.048, val_acc:0.971]
Epoch [82/120    avg_loss:0.043, val_acc:0.971]
Epoch [83/120    avg_loss:0.035, val_acc:0.973]
Epoch [84/120    avg_loss:0.040, val_acc:0.974]
Epoch [85/120    avg_loss:0.046, val_acc:0.972]
Epoch [86/120    avg_loss:0.034, val_acc:0.975]
Epoch [87/120    avg_loss:0.038, val_acc:0.976]
Epoch [88/120    avg_loss:0.033, val_acc:0.976]
Epoch [89/120    avg_loss:0.037, val_acc:0.974]
Epoch [90/120    avg_loss:0.031, val_acc:0.975]
Epoch [91/120    avg_loss:0.033, val_acc:0.975]
Epoch [92/120    avg_loss:0.038, val_acc:0.975]
Epoch [93/120    avg_loss:0.029, val_acc:0.977]
Epoch [94/120    avg_loss:0.039, val_acc:0.979]
Epoch [95/120    avg_loss:0.032, val_acc:0.980]
Epoch [96/120    avg_loss:0.036, val_acc:0.977]
Epoch [97/120    avg_loss:0.033, val_acc:0.977]
Epoch [98/120    avg_loss:0.031, val_acc:0.975]
Epoch [99/120    avg_loss:0.029, val_acc:0.977]
Epoch [100/120    avg_loss:0.029, val_acc:0.977]
Epoch [101/120    avg_loss:0.030, val_acc:0.977]
Epoch [102/120    avg_loss:0.030, val_acc:0.978]
Epoch [103/120    avg_loss:0.032, val_acc:0.979]
Epoch [104/120    avg_loss:0.037, val_acc:0.979]
Epoch [105/120    avg_loss:0.028, val_acc:0.980]
Epoch [106/120    avg_loss:0.030, val_acc:0.979]
Epoch [107/120    avg_loss:0.032, val_acc:0.978]
Epoch [108/120    avg_loss:0.027, val_acc:0.977]
Epoch [109/120    avg_loss:0.029, val_acc:0.978]
Epoch [110/120    avg_loss:0.028, val_acc:0.977]
Epoch [111/120    avg_loss:0.030, val_acc:0.980]
Epoch [112/120    avg_loss:0.028, val_acc:0.982]
Epoch [113/120    avg_loss:0.033, val_acc:0.980]
Epoch [114/120    avg_loss:0.036, val_acc:0.978]
Epoch [115/120    avg_loss:0.031, val_acc:0.980]
Epoch [116/120    avg_loss:0.030, val_acc:0.979]
Epoch [117/120    avg_loss:0.029, val_acc:0.978]
Epoch [118/120    avg_loss:0.029, val_acc:0.980]
Epoch [119/120    avg_loss:0.025, val_acc:0.979]
Epoch [120/120    avg_loss:0.029, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1243    1    0    0    1    0    0    1    3   36    0    0
     0    0    0]
 [   0    0    0  707    4    1    3    0    0   11    3    2   13    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   24    0    0    4    6    0    0    0  789   50    1    0
     0    1    0]
 [   0    0    6    0    0    6    1    0    0    0   16 2176    4    1
     0    0    0]
 [   0    0    0    3    4    0    0    0    0    0    9    0  515    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    0    0    0    0
  1119   14    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    32  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.65040650406505

F1 scores:
[       nan 0.94871795 0.97185301 0.96915696 0.98156682 0.97853107
 0.97249071 0.98039216 1.         0.68085106 0.92878164 0.97251397
 0.96261682 0.98930481 0.97643979 0.89296636 0.9704142 ]

Kappa:
0.9617767855458329
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa0275aa860>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.747, val_acc:0.216]
Epoch [2/120    avg_loss:2.492, val_acc:0.267]
Epoch [3/120    avg_loss:2.344, val_acc:0.360]
Epoch [4/120    avg_loss:2.169, val_acc:0.509]
Epoch [5/120    avg_loss:2.059, val_acc:0.552]
Epoch [6/120    avg_loss:1.964, val_acc:0.567]
Epoch [7/120    avg_loss:1.831, val_acc:0.580]
Epoch [8/120    avg_loss:1.732, val_acc:0.630]
Epoch [9/120    avg_loss:1.627, val_acc:0.637]
Epoch [10/120    avg_loss:1.503, val_acc:0.647]
Epoch [11/120    avg_loss:1.390, val_acc:0.664]
Epoch [12/120    avg_loss:1.280, val_acc:0.689]
Epoch [13/120    avg_loss:1.129, val_acc:0.721]
Epoch [14/120    avg_loss:1.052, val_acc:0.734]
Epoch [15/120    avg_loss:0.984, val_acc:0.732]
Epoch [16/120    avg_loss:0.914, val_acc:0.758]
Epoch [17/120    avg_loss:0.856, val_acc:0.800]
Epoch [18/120    avg_loss:0.730, val_acc:0.811]
Epoch [19/120    avg_loss:0.632, val_acc:0.832]
Epoch [20/120    avg_loss:0.626, val_acc:0.817]
Epoch [21/120    avg_loss:0.563, val_acc:0.809]
Epoch [22/120    avg_loss:0.522, val_acc:0.841]
Epoch [23/120    avg_loss:0.523, val_acc:0.800]
Epoch [24/120    avg_loss:0.478, val_acc:0.860]
Epoch [25/120    avg_loss:0.408, val_acc:0.823]
Epoch [26/120    avg_loss:0.424, val_acc:0.859]
Epoch [27/120    avg_loss:0.350, val_acc:0.895]
Epoch [28/120    avg_loss:0.299, val_acc:0.900]
Epoch [29/120    avg_loss:0.251, val_acc:0.900]
Epoch [30/120    avg_loss:0.225, val_acc:0.924]
Epoch [31/120    avg_loss:0.225, val_acc:0.901]
Epoch [32/120    avg_loss:0.202, val_acc:0.938]
Epoch [33/120    avg_loss:0.166, val_acc:0.921]
Epoch [34/120    avg_loss:0.248, val_acc:0.909]
Epoch [35/120    avg_loss:0.217, val_acc:0.922]
Epoch [36/120    avg_loss:0.181, val_acc:0.927]
Epoch [37/120    avg_loss:0.183, val_acc:0.918]
Epoch [38/120    avg_loss:0.170, val_acc:0.902]
Epoch [39/120    avg_loss:0.153, val_acc:0.890]
Epoch [40/120    avg_loss:0.193, val_acc:0.911]
Epoch [41/120    avg_loss:0.153, val_acc:0.928]
Epoch [42/120    avg_loss:0.124, val_acc:0.939]
Epoch [43/120    avg_loss:0.107, val_acc:0.947]
Epoch [44/120    avg_loss:0.114, val_acc:0.945]
Epoch [45/120    avg_loss:0.085, val_acc:0.945]
Epoch [46/120    avg_loss:0.105, val_acc:0.941]
Epoch [47/120    avg_loss:0.080, val_acc:0.952]
Epoch [48/120    avg_loss:0.088, val_acc:0.938]
Epoch [49/120    avg_loss:0.078, val_acc:0.953]
Epoch [50/120    avg_loss:0.060, val_acc:0.958]
Epoch [51/120    avg_loss:0.081, val_acc:0.951]
Epoch [52/120    avg_loss:0.077, val_acc:0.958]
Epoch [53/120    avg_loss:0.054, val_acc:0.950]
Epoch [54/120    avg_loss:0.076, val_acc:0.962]
Epoch [55/120    avg_loss:0.064, val_acc:0.957]
Epoch [56/120    avg_loss:0.057, val_acc:0.964]
Epoch [57/120    avg_loss:0.065, val_acc:0.966]
Epoch [58/120    avg_loss:0.067, val_acc:0.948]
Epoch [59/120    avg_loss:0.073, val_acc:0.963]
Epoch [60/120    avg_loss:0.054, val_acc:0.961]
Epoch [61/120    avg_loss:0.054, val_acc:0.966]
Epoch [62/120    avg_loss:0.051, val_acc:0.965]
Epoch [63/120    avg_loss:0.039, val_acc:0.967]
Epoch [64/120    avg_loss:0.043, val_acc:0.948]
Epoch [65/120    avg_loss:0.054, val_acc:0.963]
Epoch [66/120    avg_loss:0.047, val_acc:0.961]
Epoch [67/120    avg_loss:0.053, val_acc:0.964]
Epoch [68/120    avg_loss:0.052, val_acc:0.967]
Epoch [69/120    avg_loss:0.046, val_acc:0.973]
Epoch [70/120    avg_loss:0.051, val_acc:0.971]
Epoch [71/120    avg_loss:0.046, val_acc:0.957]
Epoch [72/120    avg_loss:0.056, val_acc:0.955]
Epoch [73/120    avg_loss:0.050, val_acc:0.963]
Epoch [74/120    avg_loss:0.041, val_acc:0.961]
Epoch [75/120    avg_loss:0.055, val_acc:0.943]
Epoch [76/120    avg_loss:0.045, val_acc:0.968]
Epoch [77/120    avg_loss:0.033, val_acc:0.964]
Epoch [78/120    avg_loss:0.041, val_acc:0.966]
Epoch [79/120    avg_loss:0.033, val_acc:0.968]
Epoch [80/120    avg_loss:0.024, val_acc:0.970]
Epoch [81/120    avg_loss:0.022, val_acc:0.972]
Epoch [82/120    avg_loss:0.035, val_acc:0.962]
Epoch [83/120    avg_loss:0.027, val_acc:0.963]
Epoch [84/120    avg_loss:0.029, val_acc:0.964]
Epoch [85/120    avg_loss:0.021, val_acc:0.971]
Epoch [86/120    avg_loss:0.019, val_acc:0.970]
Epoch [87/120    avg_loss:0.018, val_acc:0.970]
Epoch [88/120    avg_loss:0.017, val_acc:0.971]
Epoch [89/120    avg_loss:0.017, val_acc:0.971]
Epoch [90/120    avg_loss:0.019, val_acc:0.970]
Epoch [91/120    avg_loss:0.017, val_acc:0.971]
Epoch [92/120    avg_loss:0.015, val_acc:0.971]
Epoch [93/120    avg_loss:0.017, val_acc:0.971]
Epoch [94/120    avg_loss:0.016, val_acc:0.970]
Epoch [95/120    avg_loss:0.017, val_acc:0.971]
Epoch [96/120    avg_loss:0.014, val_acc:0.971]
Epoch [97/120    avg_loss:0.017, val_acc:0.971]
Epoch [98/120    avg_loss:0.022, val_acc:0.971]
Epoch [99/120    avg_loss:0.013, val_acc:0.971]
Epoch [100/120    avg_loss:0.016, val_acc:0.971]
Epoch [101/120    avg_loss:0.016, val_acc:0.971]
Epoch [102/120    avg_loss:0.016, val_acc:0.971]
Epoch [103/120    avg_loss:0.018, val_acc:0.971]
Epoch [104/120    avg_loss:0.017, val_acc:0.971]
Epoch [105/120    avg_loss:0.015, val_acc:0.970]
Epoch [106/120    avg_loss:0.021, val_acc:0.971]
Epoch [107/120    avg_loss:0.017, val_acc:0.971]
Epoch [108/120    avg_loss:0.014, val_acc:0.970]
Epoch [109/120    avg_loss:0.017, val_acc:0.970]
Epoch [110/120    avg_loss:0.016, val_acc:0.970]
Epoch [111/120    avg_loss:0.018, val_acc:0.970]
Epoch [112/120    avg_loss:0.016, val_acc:0.970]
Epoch [113/120    avg_loss:0.016, val_acc:0.970]
Epoch [114/120    avg_loss:0.013, val_acc:0.970]
Epoch [115/120    avg_loss:0.021, val_acc:0.970]
Epoch [116/120    avg_loss:0.012, val_acc:0.970]
Epoch [117/120    avg_loss:0.017, val_acc:0.971]
Epoch [118/120    avg_loss:0.018, val_acc:0.970]
Epoch [119/120    avg_loss:0.014, val_acc:0.970]
Epoch [120/120    avg_loss:0.015, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1252    0    2    0    1    0    0    1    6   23    0    0
     0    0    0]
 [   0    0    0  729    0    2    0    0    0    5    2    0    6    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    6    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    5    1    0    2    2    0    0    0  835   25    1    0
     2    2    0]
 [   0    0    2    0    0    1    1    0    0    0    6 2186   13    1
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    1    8    7  510    0
     2    0    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1117   22    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    28  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.63685636856368

F1 scores:
[       nan 0.975      0.98427673 0.98513514 0.9953271  0.98269896
 0.98642534 0.89285714 1.         0.77272727 0.96309112 0.98181002
 0.95595127 0.98659517 0.97554585 0.90721649 0.9704142 ]

Kappa:
0.9730468784082169
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0e7f42f898>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.755, val_acc:0.242]
Epoch [2/120    avg_loss:2.496, val_acc:0.385]
Epoch [3/120    avg_loss:2.315, val_acc:0.467]
Epoch [4/120    avg_loss:2.220, val_acc:0.503]
Epoch [5/120    avg_loss:2.069, val_acc:0.526]
Epoch [6/120    avg_loss:1.944, val_acc:0.547]
Epoch [7/120    avg_loss:1.861, val_acc:0.551]
Epoch [8/120    avg_loss:1.726, val_acc:0.560]
Epoch [9/120    avg_loss:1.642, val_acc:0.600]
Epoch [10/120    avg_loss:1.523, val_acc:0.625]
Epoch [11/120    avg_loss:1.426, val_acc:0.654]
Epoch [12/120    avg_loss:1.311, val_acc:0.653]
Epoch [13/120    avg_loss:1.279, val_acc:0.641]
Epoch [14/120    avg_loss:1.167, val_acc:0.630]
Epoch [15/120    avg_loss:1.156, val_acc:0.660]
Epoch [16/120    avg_loss:0.990, val_acc:0.702]
Epoch [17/120    avg_loss:0.844, val_acc:0.737]
Epoch [18/120    avg_loss:0.814, val_acc:0.765]
Epoch [19/120    avg_loss:0.727, val_acc:0.733]
Epoch [20/120    avg_loss:0.679, val_acc:0.767]
Epoch [21/120    avg_loss:0.593, val_acc:0.788]
Epoch [22/120    avg_loss:0.509, val_acc:0.812]
Epoch [23/120    avg_loss:0.525, val_acc:0.753]
Epoch [24/120    avg_loss:0.484, val_acc:0.793]
Epoch [25/120    avg_loss:0.404, val_acc:0.809]
Epoch [26/120    avg_loss:0.369, val_acc:0.873]
Epoch [27/120    avg_loss:0.318, val_acc:0.880]
Epoch [28/120    avg_loss:0.271, val_acc:0.877]
Epoch [29/120    avg_loss:0.267, val_acc:0.893]
Epoch [30/120    avg_loss:0.221, val_acc:0.876]
Epoch [31/120    avg_loss:0.217, val_acc:0.908]
Epoch [32/120    avg_loss:0.217, val_acc:0.900]
Epoch [33/120    avg_loss:0.236, val_acc:0.883]
Epoch [34/120    avg_loss:0.198, val_acc:0.890]
Epoch [35/120    avg_loss:0.179, val_acc:0.914]
Epoch [36/120    avg_loss:0.164, val_acc:0.901]
Epoch [37/120    avg_loss:0.168, val_acc:0.916]
Epoch [38/120    avg_loss:0.174, val_acc:0.912]
Epoch [39/120    avg_loss:0.148, val_acc:0.920]
Epoch [40/120    avg_loss:0.138, val_acc:0.938]
Epoch [41/120    avg_loss:0.116, val_acc:0.938]
Epoch [42/120    avg_loss:0.126, val_acc:0.918]
Epoch [43/120    avg_loss:0.119, val_acc:0.947]
Epoch [44/120    avg_loss:0.126, val_acc:0.926]
Epoch [45/120    avg_loss:0.102, val_acc:0.939]
Epoch [46/120    avg_loss:0.097, val_acc:0.941]
Epoch [47/120    avg_loss:0.093, val_acc:0.950]
Epoch [48/120    avg_loss:0.075, val_acc:0.943]
Epoch [49/120    avg_loss:0.090, val_acc:0.932]
Epoch [50/120    avg_loss:0.071, val_acc:0.960]
Epoch [51/120    avg_loss:0.082, val_acc:0.939]
Epoch [52/120    avg_loss:0.070, val_acc:0.947]
Epoch [53/120    avg_loss:0.063, val_acc:0.940]
Epoch [54/120    avg_loss:0.061, val_acc:0.951]
Epoch [55/120    avg_loss:0.048, val_acc:0.957]
Epoch [56/120    avg_loss:0.060, val_acc:0.962]
Epoch [57/120    avg_loss:0.063, val_acc:0.950]
Epoch [58/120    avg_loss:0.090, val_acc:0.951]
Epoch [59/120    avg_loss:0.116, val_acc:0.937]
Epoch [60/120    avg_loss:0.077, val_acc:0.955]
Epoch [61/120    avg_loss:0.054, val_acc:0.960]
Epoch [62/120    avg_loss:0.068, val_acc:0.940]
Epoch [63/120    avg_loss:0.058, val_acc:0.967]
Epoch [64/120    avg_loss:0.076, val_acc:0.961]
Epoch [65/120    avg_loss:0.139, val_acc:0.953]
Epoch [66/120    avg_loss:0.084, val_acc:0.955]
Epoch [67/120    avg_loss:0.073, val_acc:0.911]
Epoch [68/120    avg_loss:0.080, val_acc:0.963]
Epoch [69/120    avg_loss:0.064, val_acc:0.952]
Epoch [70/120    avg_loss:0.063, val_acc:0.963]
Epoch [71/120    avg_loss:0.075, val_acc:0.947]
Epoch [72/120    avg_loss:0.087, val_acc:0.942]
Epoch [73/120    avg_loss:0.060, val_acc:0.963]
Epoch [74/120    avg_loss:0.046, val_acc:0.970]
Epoch [75/120    avg_loss:0.042, val_acc:0.958]
Epoch [76/120    avg_loss:0.055, val_acc:0.954]
Epoch [77/120    avg_loss:0.050, val_acc:0.962]
Epoch [78/120    avg_loss:0.033, val_acc:0.973]
Epoch [79/120    avg_loss:0.052, val_acc:0.962]
Epoch [80/120    avg_loss:0.043, val_acc:0.948]
Epoch [81/120    avg_loss:0.040, val_acc:0.978]
Epoch [82/120    avg_loss:0.034, val_acc:0.972]
Epoch [83/120    avg_loss:0.046, val_acc:0.966]
Epoch [84/120    avg_loss:0.051, val_acc:0.952]
Epoch [85/120    avg_loss:0.036, val_acc:0.958]
Epoch [86/120    avg_loss:0.029, val_acc:0.974]
Epoch [87/120    avg_loss:0.024, val_acc:0.975]
Epoch [88/120    avg_loss:0.023, val_acc:0.966]
Epoch [89/120    avg_loss:0.022, val_acc:0.978]
Epoch [90/120    avg_loss:0.029, val_acc:0.974]
Epoch [91/120    avg_loss:0.029, val_acc:0.973]
Epoch [92/120    avg_loss:0.024, val_acc:0.978]
Epoch [93/120    avg_loss:0.018, val_acc:0.975]
Epoch [94/120    avg_loss:0.018, val_acc:0.976]
Epoch [95/120    avg_loss:0.014, val_acc:0.975]
Epoch [96/120    avg_loss:0.020, val_acc:0.973]
Epoch [97/120    avg_loss:0.016, val_acc:0.976]
Epoch [98/120    avg_loss:0.019, val_acc:0.972]
Epoch [99/120    avg_loss:0.023, val_acc:0.980]
Epoch [100/120    avg_loss:0.018, val_acc:0.976]
Epoch [101/120    avg_loss:0.015, val_acc:0.978]
Epoch [102/120    avg_loss:0.015, val_acc:0.980]
Epoch [103/120    avg_loss:0.015, val_acc:0.976]
Epoch [104/120    avg_loss:0.015, val_acc:0.980]
Epoch [105/120    avg_loss:0.012, val_acc:0.977]
Epoch [106/120    avg_loss:0.013, val_acc:0.979]
Epoch [107/120    avg_loss:0.009, val_acc:0.983]
Epoch [108/120    avg_loss:0.012, val_acc:0.978]
Epoch [109/120    avg_loss:0.013, val_acc:0.977]
Epoch [110/120    avg_loss:0.012, val_acc:0.974]
Epoch [111/120    avg_loss:0.019, val_acc:0.968]
Epoch [112/120    avg_loss:0.028, val_acc:0.971]
Epoch [113/120    avg_loss:0.023, val_acc:0.974]
Epoch [114/120    avg_loss:0.017, val_acc:0.972]
Epoch [115/120    avg_loss:0.021, val_acc:0.949]
Epoch [116/120    avg_loss:0.026, val_acc:0.960]
Epoch [117/120    avg_loss:0.021, val_acc:0.968]
Epoch [118/120    avg_loss:0.014, val_acc:0.974]
Epoch [119/120    avg_loss:0.010, val_acc:0.976]
Epoch [120/120    avg_loss:0.013, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1236    8    6    0    0    0    0    0    4   29    2    0
     0    0    0]
 [   0    0    0  731    4    0    3    0    0    3    0    0    3    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    1    2    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0    7    1    0    2    3    0    0    0  822   36    1    0
     0    3    0]
 [   0    0    6    2    0    0    0    0    0    1   14 2175   11    1
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0    0  524    0
     0    6    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   13    2    0    0    0    0    0    0    0
  1123    1    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
     9  313    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.53929539295393

F1 scores:
[       nan 0.975      0.97476341 0.98120805 0.97706422 0.97610922
 0.97477745 0.96153846 0.997669   0.81081081 0.95860058 0.97752809
 0.96947271 0.98930481 0.9876869  0.93432836 0.97647059]

Kappa:
0.9719414209304197
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd6115df710>
supervision:full
center_pixel:True
Network :
Number of parameter: 52014==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.746, val_acc:0.319]
Epoch [2/120    avg_loss:2.509, val_acc:0.400]
Epoch [3/120    avg_loss:2.360, val_acc:0.493]
Epoch [4/120    avg_loss:2.198, val_acc:0.523]
Epoch [5/120    avg_loss:2.056, val_acc:0.560]
Epoch [6/120    avg_loss:1.924, val_acc:0.582]
Epoch [7/120    avg_loss:1.787, val_acc:0.596]
Epoch [8/120    avg_loss:1.690, val_acc:0.608]
Epoch [9/120    avg_loss:1.497, val_acc:0.624]
Epoch [10/120    avg_loss:1.412, val_acc:0.654]
Epoch [11/120    avg_loss:1.249, val_acc:0.631]
Epoch [12/120    avg_loss:1.179, val_acc:0.666]
Epoch [13/120    avg_loss:1.111, val_acc:0.699]
Epoch [14/120    avg_loss:0.963, val_acc:0.730]
Epoch [15/120    avg_loss:0.911, val_acc:0.740]
Epoch [16/120    avg_loss:0.824, val_acc:0.777]
Epoch [17/120    avg_loss:0.746, val_acc:0.803]
Epoch [18/120    avg_loss:0.703, val_acc:0.801]
Epoch [19/120    avg_loss:0.629, val_acc:0.823]
Epoch [20/120    avg_loss:0.560, val_acc:0.824]
Epoch [21/120    avg_loss:0.544, val_acc:0.812]
Epoch [22/120    avg_loss:0.641, val_acc:0.810]
Epoch [23/120    avg_loss:0.506, val_acc:0.844]
Epoch [24/120    avg_loss:0.513, val_acc:0.858]
Epoch [25/120    avg_loss:0.439, val_acc:0.864]
Epoch [26/120    avg_loss:0.383, val_acc:0.849]
Epoch [27/120    avg_loss:0.376, val_acc:0.865]
Epoch [28/120    avg_loss:0.311, val_acc:0.876]
Epoch [29/120    avg_loss:0.310, val_acc:0.863]
Epoch [30/120    avg_loss:0.305, val_acc:0.902]
Epoch [31/120    avg_loss:0.259, val_acc:0.920]
Epoch [32/120    avg_loss:0.212, val_acc:0.922]
Epoch [33/120    avg_loss:0.187, val_acc:0.919]
Epoch [34/120    avg_loss:0.168, val_acc:0.927]
Epoch [35/120    avg_loss:0.213, val_acc:0.902]
Epoch [36/120    avg_loss:0.227, val_acc:0.919]
Epoch [37/120    avg_loss:0.188, val_acc:0.919]
Epoch [38/120    avg_loss:0.181, val_acc:0.931]
Epoch [39/120    avg_loss:0.154, val_acc:0.936]
Epoch [40/120    avg_loss:0.126, val_acc:0.944]
Epoch [41/120    avg_loss:0.137, val_acc:0.919]
Epoch [42/120    avg_loss:0.160, val_acc:0.927]
Epoch [43/120    avg_loss:0.137, val_acc:0.961]
Epoch [44/120    avg_loss:0.101, val_acc:0.931]
Epoch [45/120    avg_loss:0.104, val_acc:0.956]
Epoch [46/120    avg_loss:0.109, val_acc:0.947]
Epoch [47/120    avg_loss:0.109, val_acc:0.947]
Epoch [48/120    avg_loss:0.106, val_acc:0.947]
Epoch [49/120    avg_loss:0.115, val_acc:0.939]
Epoch [50/120    avg_loss:0.155, val_acc:0.914]
Epoch [51/120    avg_loss:0.145, val_acc:0.934]
Epoch [52/120    avg_loss:0.144, val_acc:0.940]
Epoch [53/120    avg_loss:0.246, val_acc:0.915]
Epoch [54/120    avg_loss:0.158, val_acc:0.928]
Epoch [55/120    avg_loss:0.111, val_acc:0.952]
Epoch [56/120    avg_loss:0.085, val_acc:0.943]
Epoch [57/120    avg_loss:0.084, val_acc:0.957]
Epoch [58/120    avg_loss:0.063, val_acc:0.967]
Epoch [59/120    avg_loss:0.064, val_acc:0.970]
Epoch [60/120    avg_loss:0.059, val_acc:0.968]
Epoch [61/120    avg_loss:0.056, val_acc:0.968]
Epoch [62/120    avg_loss:0.054, val_acc:0.974]
Epoch [63/120    avg_loss:0.055, val_acc:0.974]
Epoch [64/120    avg_loss:0.056, val_acc:0.970]
Epoch [65/120    avg_loss:0.057, val_acc:0.974]
Epoch [66/120    avg_loss:0.051, val_acc:0.972]
Epoch [67/120    avg_loss:0.056, val_acc:0.973]
Epoch [68/120    avg_loss:0.047, val_acc:0.972]
Epoch [69/120    avg_loss:0.055, val_acc:0.974]
Epoch [70/120    avg_loss:0.046, val_acc:0.975]
Epoch [71/120    avg_loss:0.046, val_acc:0.975]
Epoch [72/120    avg_loss:0.049, val_acc:0.975]
Epoch [73/120    avg_loss:0.044, val_acc:0.975]
Epoch [74/120    avg_loss:0.044, val_acc:0.974]
Epoch [75/120    avg_loss:0.048, val_acc:0.973]
Epoch [76/120    avg_loss:0.044, val_acc:0.976]
Epoch [77/120    avg_loss:0.042, val_acc:0.973]
Epoch [78/120    avg_loss:0.043, val_acc:0.975]
Epoch [79/120    avg_loss:0.044, val_acc:0.975]
Epoch [80/120    avg_loss:0.044, val_acc:0.976]
Epoch [81/120    avg_loss:0.052, val_acc:0.974]
Epoch [82/120    avg_loss:0.044, val_acc:0.973]
Epoch [83/120    avg_loss:0.043, val_acc:0.975]
Epoch [84/120    avg_loss:0.039, val_acc:0.974]
Epoch [85/120    avg_loss:0.039, val_acc:0.972]
Epoch [86/120    avg_loss:0.044, val_acc:0.973]
Epoch [87/120    avg_loss:0.041, val_acc:0.973]
Epoch [88/120    avg_loss:0.048, val_acc:0.974]
Epoch [89/120    avg_loss:0.044, val_acc:0.975]
Epoch [90/120    avg_loss:0.045, val_acc:0.971]
Epoch [91/120    avg_loss:0.045, val_acc:0.975]
Epoch [92/120    avg_loss:0.038, val_acc:0.974]
Epoch [93/120    avg_loss:0.049, val_acc:0.975]
Epoch [94/120    avg_loss:0.044, val_acc:0.975]
Epoch [95/120    avg_loss:0.039, val_acc:0.975]
Epoch [96/120    avg_loss:0.045, val_acc:0.975]
Epoch [97/120    avg_loss:0.040, val_acc:0.975]
Epoch [98/120    avg_loss:0.038, val_acc:0.975]
Epoch [99/120    avg_loss:0.044, val_acc:0.975]
Epoch [100/120    avg_loss:0.043, val_acc:0.974]
Epoch [101/120    avg_loss:0.043, val_acc:0.975]
Epoch [102/120    avg_loss:0.043, val_acc:0.975]
Epoch [103/120    avg_loss:0.036, val_acc:0.975]
Epoch [104/120    avg_loss:0.041, val_acc:0.975]
Epoch [105/120    avg_loss:0.041, val_acc:0.975]
Epoch [106/120    avg_loss:0.036, val_acc:0.974]
Epoch [107/120    avg_loss:0.036, val_acc:0.974]
Epoch [108/120    avg_loss:0.036, val_acc:0.974]
Epoch [109/120    avg_loss:0.041, val_acc:0.974]
Epoch [110/120    avg_loss:0.037, val_acc:0.975]
Epoch [111/120    avg_loss:0.041, val_acc:0.975]
Epoch [112/120    avg_loss:0.044, val_acc:0.975]
Epoch [113/120    avg_loss:0.040, val_acc:0.975]
Epoch [114/120    avg_loss:0.047, val_acc:0.975]
Epoch [115/120    avg_loss:0.039, val_acc:0.975]
Epoch [116/120    avg_loss:0.045, val_acc:0.975]
Epoch [117/120    avg_loss:0.040, val_acc:0.975]
Epoch [118/120    avg_loss:0.041, val_acc:0.975]
Epoch [119/120    avg_loss:0.045, val_acc:0.975]
Epoch [120/120    avg_loss:0.044, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1238    2    0    0    0    0    0    1   12   32    0    0
     0    0    0]
 [   0    0    2  693    0    0    0    0    0    9    5    0   34    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    4    0    2    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    1    0    0
     0    0    0]
 [   0    0   10    0    0    2    1    0    0    4  827   25    0    0
     2    4    0]
 [   0    0   14    3    0    0    2    0    0    3   15 2138   26    1
     8    0    0]
 [   0    0    0    3    0    1    0    0    0    0    0    2  523    0
     2    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    1    0    1    1
  1120   12    0]
 [   0    0    0    0    0    0   18    0    0    1    0    1    0    0
     7  320    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.72628726287263

F1 scores:
[       nan 0.975      0.97136132 0.95652174 1.         0.97803468
 0.98121713 0.92592593 0.997669   0.59259259 0.95221647 0.96895536
 0.93226381 0.98404255 0.98073555 0.93567251 0.97619048]

Kappa:
0.962699518813818
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7dbe86f978>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.778, val_acc:0.300]
Epoch [2/120    avg_loss:2.525, val_acc:0.433]
Epoch [3/120    avg_loss:2.350, val_acc:0.441]
Epoch [4/120    avg_loss:2.163, val_acc:0.430]
Epoch [5/120    avg_loss:2.077, val_acc:0.472]
Epoch [6/120    avg_loss:1.961, val_acc:0.458]
Epoch [7/120    avg_loss:1.878, val_acc:0.536]
Epoch [8/120    avg_loss:1.775, val_acc:0.571]
Epoch [9/120    avg_loss:1.695, val_acc:0.567]
Epoch [10/120    avg_loss:1.617, val_acc:0.537]
Epoch [11/120    avg_loss:1.494, val_acc:0.630]
Epoch [12/120    avg_loss:1.344, val_acc:0.650]
Epoch [13/120    avg_loss:1.226, val_acc:0.626]
Epoch [14/120    avg_loss:1.134, val_acc:0.677]
Epoch [15/120    avg_loss:1.069, val_acc:0.697]
Epoch [16/120    avg_loss:0.943, val_acc:0.664]
Epoch [17/120    avg_loss:0.882, val_acc:0.733]
Epoch [18/120    avg_loss:0.862, val_acc:0.729]
Epoch [19/120    avg_loss:0.776, val_acc:0.704]
Epoch [20/120    avg_loss:0.758, val_acc:0.770]
Epoch [21/120    avg_loss:0.718, val_acc:0.766]
Epoch [22/120    avg_loss:0.660, val_acc:0.768]
Epoch [23/120    avg_loss:0.613, val_acc:0.770]
Epoch [24/120    avg_loss:0.614, val_acc:0.763]
Epoch [25/120    avg_loss:0.573, val_acc:0.813]
Epoch [26/120    avg_loss:0.517, val_acc:0.824]
Epoch [27/120    avg_loss:0.467, val_acc:0.798]
Epoch [28/120    avg_loss:0.463, val_acc:0.836]
Epoch [29/120    avg_loss:0.424, val_acc:0.854]
Epoch [30/120    avg_loss:0.375, val_acc:0.842]
Epoch [31/120    avg_loss:0.356, val_acc:0.842]
Epoch [32/120    avg_loss:0.289, val_acc:0.880]
Epoch [33/120    avg_loss:0.271, val_acc:0.896]
Epoch [34/120    avg_loss:0.340, val_acc:0.827]
Epoch [35/120    avg_loss:0.407, val_acc:0.851]
Epoch [36/120    avg_loss:0.324, val_acc:0.870]
Epoch [37/120    avg_loss:0.263, val_acc:0.870]
Epoch [38/120    avg_loss:0.265, val_acc:0.895]
Epoch [39/120    avg_loss:0.248, val_acc:0.888]
Epoch [40/120    avg_loss:0.195, val_acc:0.921]
Epoch [41/120    avg_loss:0.181, val_acc:0.914]
Epoch [42/120    avg_loss:0.193, val_acc:0.917]
Epoch [43/120    avg_loss:0.166, val_acc:0.924]
Epoch [44/120    avg_loss:0.161, val_acc:0.921]
Epoch [45/120    avg_loss:0.121, val_acc:0.933]
Epoch [46/120    avg_loss:0.126, val_acc:0.935]
Epoch [47/120    avg_loss:0.148, val_acc:0.925]
Epoch [48/120    avg_loss:0.152, val_acc:0.928]
Epoch [49/120    avg_loss:0.158, val_acc:0.949]
Epoch [50/120    avg_loss:0.197, val_acc:0.935]
Epoch [51/120    avg_loss:0.157, val_acc:0.914]
Epoch [52/120    avg_loss:0.133, val_acc:0.938]
Epoch [53/120    avg_loss:0.136, val_acc:0.945]
Epoch [54/120    avg_loss:0.127, val_acc:0.951]
Epoch [55/120    avg_loss:0.100, val_acc:0.957]
Epoch [56/120    avg_loss:0.094, val_acc:0.938]
Epoch [57/120    avg_loss:0.100, val_acc:0.940]
Epoch [58/120    avg_loss:0.096, val_acc:0.945]
Epoch [59/120    avg_loss:0.111, val_acc:0.938]
Epoch [60/120    avg_loss:0.079, val_acc:0.945]
Epoch [61/120    avg_loss:0.082, val_acc:0.954]
Epoch [62/120    avg_loss:0.080, val_acc:0.957]
Epoch [63/120    avg_loss:0.072, val_acc:0.952]
Epoch [64/120    avg_loss:0.061, val_acc:0.941]
Epoch [65/120    avg_loss:0.057, val_acc:0.957]
Epoch [66/120    avg_loss:0.062, val_acc:0.960]
Epoch [67/120    avg_loss:0.048, val_acc:0.964]
Epoch [68/120    avg_loss:0.048, val_acc:0.963]
Epoch [69/120    avg_loss:0.048, val_acc:0.964]
Epoch [70/120    avg_loss:0.056, val_acc:0.959]
Epoch [71/120    avg_loss:0.062, val_acc:0.941]
Epoch [72/120    avg_loss:0.130, val_acc:0.946]
Epoch [73/120    avg_loss:0.164, val_acc:0.907]
Epoch [74/120    avg_loss:0.107, val_acc:0.951]
Epoch [75/120    avg_loss:0.091, val_acc:0.955]
Epoch [76/120    avg_loss:0.072, val_acc:0.959]
Epoch [77/120    avg_loss:0.071, val_acc:0.953]
Epoch [78/120    avg_loss:0.063, val_acc:0.963]
Epoch [79/120    avg_loss:0.082, val_acc:0.952]
Epoch [80/120    avg_loss:0.081, val_acc:0.947]
Epoch [81/120    avg_loss:0.067, val_acc:0.955]
Epoch [82/120    avg_loss:0.053, val_acc:0.966]
Epoch [83/120    avg_loss:0.049, val_acc:0.968]
Epoch [84/120    avg_loss:0.037, val_acc:0.959]
Epoch [85/120    avg_loss:0.048, val_acc:0.964]
Epoch [86/120    avg_loss:0.054, val_acc:0.958]
Epoch [87/120    avg_loss:0.073, val_acc:0.951]
Epoch [88/120    avg_loss:0.060, val_acc:0.955]
Epoch [89/120    avg_loss:0.076, val_acc:0.952]
Epoch [90/120    avg_loss:0.069, val_acc:0.935]
Epoch [91/120    avg_loss:0.072, val_acc:0.966]
Epoch [92/120    avg_loss:0.060, val_acc:0.970]
Epoch [93/120    avg_loss:0.049, val_acc:0.961]
Epoch [94/120    avg_loss:0.051, val_acc:0.964]
Epoch [95/120    avg_loss:0.035, val_acc:0.967]
Epoch [96/120    avg_loss:0.030, val_acc:0.965]
Epoch [97/120    avg_loss:0.027, val_acc:0.967]
Epoch [98/120    avg_loss:0.027, val_acc:0.976]
Epoch [99/120    avg_loss:0.031, val_acc:0.964]
Epoch [100/120    avg_loss:0.030, val_acc:0.968]
Epoch [101/120    avg_loss:0.028, val_acc:0.967]
Epoch [102/120    avg_loss:0.024, val_acc:0.972]
Epoch [103/120    avg_loss:0.029, val_acc:0.971]
Epoch [104/120    avg_loss:0.032, val_acc:0.970]
Epoch [105/120    avg_loss:0.029, val_acc:0.965]
Epoch [106/120    avg_loss:0.033, val_acc:0.976]
Epoch [107/120    avg_loss:0.041, val_acc:0.967]
Epoch [108/120    avg_loss:0.030, val_acc:0.977]
Epoch [109/120    avg_loss:0.026, val_acc:0.974]
Epoch [110/120    avg_loss:0.023, val_acc:0.970]
Epoch [111/120    avg_loss:0.024, val_acc:0.975]
Epoch [112/120    avg_loss:0.034, val_acc:0.970]
Epoch [113/120    avg_loss:0.045, val_acc:0.966]
Epoch [114/120    avg_loss:0.031, val_acc:0.961]
Epoch [115/120    avg_loss:0.055, val_acc:0.966]
Epoch [116/120    avg_loss:0.038, val_acc:0.958]
Epoch [117/120    avg_loss:0.033, val_acc:0.978]
Epoch [118/120    avg_loss:0.020, val_acc:0.975]
Epoch [119/120    avg_loss:0.018, val_acc:0.974]
Epoch [120/120    avg_loss:0.017, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1215    0    4    0    0    0    0    0   20   46    0    0
     0    0    0]
 [   0    0    5  699    6    7    0    0    0   16    2    0   12    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    3    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    8    0    0    3    0    0    0    1  838   21    0    0
     0    4    0]
 [   0    0    1    0    0    1    2    0    0    0   15 2184    7    0
     0    0    0]
 [   0    0    0    0    3    6    0    0    0    1    9    4  506    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   18    0    0    1    3    1    0    0    2
  1111    3    0]
 [   0    0    0    0    0    0   30    0    0    0    0    0    0    0
    11  306    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.83468834688347

F1 scores:
[       nan 0.96202532 0.96658711 0.96680498 0.97038724 0.95671476
 0.97470238 1.         0.99416569 0.57627119 0.9506523  0.97783747
 0.95112782 0.99462366 0.98231653 0.92727273 0.97109827]

Kappa:
0.9639072417318414
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f137bd1c860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.784, val_acc:0.309]
Epoch [2/120    avg_loss:2.519, val_acc:0.422]
Epoch [3/120    avg_loss:2.330, val_acc:0.428]
Epoch [4/120    avg_loss:2.203, val_acc:0.500]
Epoch [5/120    avg_loss:2.110, val_acc:0.511]
Epoch [6/120    avg_loss:2.028, val_acc:0.551]
Epoch [7/120    avg_loss:1.919, val_acc:0.564]
Epoch [8/120    avg_loss:1.820, val_acc:0.600]
Epoch [9/120    avg_loss:1.735, val_acc:0.624]
Epoch [10/120    avg_loss:1.693, val_acc:0.622]
Epoch [11/120    avg_loss:1.588, val_acc:0.649]
Epoch [12/120    avg_loss:1.514, val_acc:0.665]
Epoch [13/120    avg_loss:1.483, val_acc:0.672]
Epoch [14/120    avg_loss:1.349, val_acc:0.680]
Epoch [15/120    avg_loss:1.286, val_acc:0.714]
Epoch [16/120    avg_loss:1.150, val_acc:0.747]
Epoch [17/120    avg_loss:1.013, val_acc:0.753]
Epoch [18/120    avg_loss:0.937, val_acc:0.776]
Epoch [19/120    avg_loss:0.920, val_acc:0.772]
Epoch [20/120    avg_loss:0.864, val_acc:0.798]
Epoch [21/120    avg_loss:0.742, val_acc:0.797]
Epoch [22/120    avg_loss:0.655, val_acc:0.837]
Epoch [23/120    avg_loss:0.612, val_acc:0.823]
Epoch [24/120    avg_loss:0.597, val_acc:0.832]
Epoch [25/120    avg_loss:0.560, val_acc:0.809]
Epoch [26/120    avg_loss:0.535, val_acc:0.812]
Epoch [27/120    avg_loss:0.458, val_acc:0.857]
Epoch [28/120    avg_loss:0.464, val_acc:0.847]
Epoch [29/120    avg_loss:0.376, val_acc:0.876]
Epoch [30/120    avg_loss:0.361, val_acc:0.883]
Epoch [31/120    avg_loss:0.314, val_acc:0.860]
Epoch [32/120    avg_loss:0.404, val_acc:0.854]
Epoch [33/120    avg_loss:0.323, val_acc:0.893]
Epoch [34/120    avg_loss:0.323, val_acc:0.915]
Epoch [35/120    avg_loss:0.446, val_acc:0.882]
Epoch [36/120    avg_loss:0.329, val_acc:0.880]
Epoch [37/120    avg_loss:0.263, val_acc:0.897]
Epoch [38/120    avg_loss:0.247, val_acc:0.913]
Epoch [39/120    avg_loss:0.201, val_acc:0.918]
Epoch [40/120    avg_loss:0.231, val_acc:0.916]
Epoch [41/120    avg_loss:0.214, val_acc:0.923]
Epoch [42/120    avg_loss:0.190, val_acc:0.907]
Epoch [43/120    avg_loss:0.190, val_acc:0.918]
Epoch [44/120    avg_loss:0.167, val_acc:0.929]
Epoch [45/120    avg_loss:0.150, val_acc:0.946]
Epoch [46/120    avg_loss:0.126, val_acc:0.947]
Epoch [47/120    avg_loss:0.113, val_acc:0.946]
Epoch [48/120    avg_loss:0.127, val_acc:0.943]
Epoch [49/120    avg_loss:0.117, val_acc:0.958]
Epoch [50/120    avg_loss:0.127, val_acc:0.929]
Epoch [51/120    avg_loss:0.118, val_acc:0.960]
Epoch [52/120    avg_loss:0.120, val_acc:0.949]
Epoch [53/120    avg_loss:0.119, val_acc:0.951]
Epoch [54/120    avg_loss:0.173, val_acc:0.902]
Epoch [55/120    avg_loss:0.229, val_acc:0.904]
Epoch [56/120    avg_loss:0.212, val_acc:0.911]
Epoch [57/120    avg_loss:0.146, val_acc:0.939]
Epoch [58/120    avg_loss:0.124, val_acc:0.936]
Epoch [59/120    avg_loss:0.134, val_acc:0.945]
Epoch [60/120    avg_loss:0.127, val_acc:0.954]
Epoch [61/120    avg_loss:0.120, val_acc:0.952]
Epoch [62/120    avg_loss:0.110, val_acc:0.946]
Epoch [63/120    avg_loss:0.099, val_acc:0.965]
Epoch [64/120    avg_loss:0.101, val_acc:0.929]
Epoch [65/120    avg_loss:0.098, val_acc:0.937]
Epoch [66/120    avg_loss:0.076, val_acc:0.961]
Epoch [67/120    avg_loss:0.063, val_acc:0.970]
Epoch [68/120    avg_loss:0.060, val_acc:0.964]
Epoch [69/120    avg_loss:0.060, val_acc:0.984]
Epoch [70/120    avg_loss:0.059, val_acc:0.964]
Epoch [71/120    avg_loss:0.051, val_acc:0.968]
Epoch [72/120    avg_loss:0.059, val_acc:0.971]
Epoch [73/120    avg_loss:0.056, val_acc:0.968]
Epoch [74/120    avg_loss:0.053, val_acc:0.980]
Epoch [75/120    avg_loss:0.049, val_acc:0.964]
Epoch [76/120    avg_loss:0.047, val_acc:0.965]
Epoch [77/120    avg_loss:0.058, val_acc:0.950]
Epoch [78/120    avg_loss:0.050, val_acc:0.974]
Epoch [79/120    avg_loss:0.046, val_acc:0.967]
Epoch [80/120    avg_loss:0.051, val_acc:0.975]
Epoch [81/120    avg_loss:0.038, val_acc:0.979]
Epoch [82/120    avg_loss:0.035, val_acc:0.973]
Epoch [83/120    avg_loss:0.033, val_acc:0.978]
Epoch [84/120    avg_loss:0.027, val_acc:0.982]
Epoch [85/120    avg_loss:0.025, val_acc:0.979]
Epoch [86/120    avg_loss:0.026, val_acc:0.980]
Epoch [87/120    avg_loss:0.028, val_acc:0.980]
Epoch [88/120    avg_loss:0.024, val_acc:0.984]
Epoch [89/120    avg_loss:0.026, val_acc:0.983]
Epoch [90/120    avg_loss:0.025, val_acc:0.983]
Epoch [91/120    avg_loss:0.025, val_acc:0.983]
Epoch [92/120    avg_loss:0.029, val_acc:0.983]
Epoch [93/120    avg_loss:0.023, val_acc:0.979]
Epoch [94/120    avg_loss:0.029, val_acc:0.983]
Epoch [95/120    avg_loss:0.024, val_acc:0.985]
Epoch [96/120    avg_loss:0.022, val_acc:0.985]
Epoch [97/120    avg_loss:0.023, val_acc:0.984]
Epoch [98/120    avg_loss:0.022, val_acc:0.985]
Epoch [99/120    avg_loss:0.020, val_acc:0.985]
Epoch [100/120    avg_loss:0.025, val_acc:0.984]
Epoch [101/120    avg_loss:0.024, val_acc:0.984]
Epoch [102/120    avg_loss:0.023, val_acc:0.983]
Epoch [103/120    avg_loss:0.023, val_acc:0.984]
Epoch [104/120    avg_loss:0.020, val_acc:0.983]
Epoch [105/120    avg_loss:0.025, val_acc:0.982]
Epoch [106/120    avg_loss:0.025, val_acc:0.982]
Epoch [107/120    avg_loss:0.022, val_acc:0.980]
Epoch [108/120    avg_loss:0.019, val_acc:0.984]
Epoch [109/120    avg_loss:0.020, val_acc:0.983]
Epoch [110/120    avg_loss:0.025, val_acc:0.983]
Epoch [111/120    avg_loss:0.021, val_acc:0.982]
Epoch [112/120    avg_loss:0.023, val_acc:0.984]
Epoch [113/120    avg_loss:0.026, val_acc:0.984]
Epoch [114/120    avg_loss:0.022, val_acc:0.983]
Epoch [115/120    avg_loss:0.022, val_acc:0.983]
Epoch [116/120    avg_loss:0.021, val_acc:0.983]
Epoch [117/120    avg_loss:0.019, val_acc:0.983]
Epoch [118/120    avg_loss:0.021, val_acc:0.983]
Epoch [119/120    avg_loss:0.020, val_acc:0.983]
Epoch [120/120    avg_loss:0.020, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1240    0   10    0    3    0    0    0    7   22    3    0
     0    0    0]
 [   0    0    1  711    1    2    3    0    0    9    4    0   16    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    7    0    0   10    0    0    1    0
     0    0    0]
 [   0    0   17    0    0    7    2    0    0    0  832   17    0    0
     0    0    0]
 [   0    0    5    2    0    0    3    0    2    0   19 2173    5    1
     0    0    0]
 [   0    0    1    0    3    0    0    0    0    0    4    0  521    0
     1    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1134    3    0]
 [   0    0    0    0    0    0   21    0    0    1    0    0    0    0
    17  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.47425474254743

F1 scores:
[       nan 0.975      0.97254902 0.9739726  0.96818182 0.98514286
 0.9704142  1.         0.99652375 0.5        0.95467585 0.982591
 0.96481481 0.99730458 0.98909725 0.93617021 0.97674419]

Kappa:
0.9712071686163021
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe224953898>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.774, val_acc:0.246]
Epoch [2/120    avg_loss:2.558, val_acc:0.412]
Epoch [3/120    avg_loss:2.345, val_acc:0.458]
Epoch [4/120    avg_loss:2.232, val_acc:0.518]
Epoch [5/120    avg_loss:2.125, val_acc:0.521]
Epoch [6/120    avg_loss:2.004, val_acc:0.532]
Epoch [7/120    avg_loss:1.881, val_acc:0.524]
Epoch [8/120    avg_loss:1.782, val_acc:0.534]
Epoch [9/120    avg_loss:1.683, val_acc:0.559]
Epoch [10/120    avg_loss:1.563, val_acc:0.580]
Epoch [11/120    avg_loss:1.479, val_acc:0.584]
Epoch [12/120    avg_loss:1.359, val_acc:0.589]
Epoch [13/120    avg_loss:1.321, val_acc:0.651]
Epoch [14/120    avg_loss:1.200, val_acc:0.659]
Epoch [15/120    avg_loss:1.096, val_acc:0.686]
Epoch [16/120    avg_loss:0.966, val_acc:0.690]
Epoch [17/120    avg_loss:0.867, val_acc:0.736]
Epoch [18/120    avg_loss:0.785, val_acc:0.715]
Epoch [19/120    avg_loss:0.823, val_acc:0.782]
Epoch [20/120    avg_loss:0.740, val_acc:0.755]
Epoch [21/120    avg_loss:0.640, val_acc:0.764]
Epoch [22/120    avg_loss:0.644, val_acc:0.790]
Epoch [23/120    avg_loss:0.515, val_acc:0.827]
Epoch [24/120    avg_loss:0.492, val_acc:0.814]
Epoch [25/120    avg_loss:0.506, val_acc:0.815]
Epoch [26/120    avg_loss:0.439, val_acc:0.837]
Epoch [27/120    avg_loss:0.430, val_acc:0.834]
Epoch [28/120    avg_loss:0.356, val_acc:0.864]
Epoch [29/120    avg_loss:0.335, val_acc:0.840]
Epoch [30/120    avg_loss:0.347, val_acc:0.848]
Epoch [31/120    avg_loss:0.313, val_acc:0.872]
Epoch [32/120    avg_loss:0.302, val_acc:0.884]
Epoch [33/120    avg_loss:0.277, val_acc:0.890]
Epoch [34/120    avg_loss:0.244, val_acc:0.875]
Epoch [35/120    avg_loss:0.233, val_acc:0.902]
Epoch [36/120    avg_loss:0.234, val_acc:0.889]
Epoch [37/120    avg_loss:0.224, val_acc:0.928]
Epoch [38/120    avg_loss:0.181, val_acc:0.907]
Epoch [39/120    avg_loss:0.199, val_acc:0.913]
Epoch [40/120    avg_loss:0.164, val_acc:0.916]
Epoch [41/120    avg_loss:0.159, val_acc:0.914]
Epoch [42/120    avg_loss:0.121, val_acc:0.934]
Epoch [43/120    avg_loss:0.128, val_acc:0.924]
Epoch [44/120    avg_loss:0.108, val_acc:0.926]
Epoch [45/120    avg_loss:0.094, val_acc:0.942]
Epoch [46/120    avg_loss:0.108, val_acc:0.940]
Epoch [47/120    avg_loss:0.099, val_acc:0.926]
Epoch [48/120    avg_loss:0.111, val_acc:0.925]
Epoch [49/120    avg_loss:0.103, val_acc:0.940]
Epoch [50/120    avg_loss:0.077, val_acc:0.941]
Epoch [51/120    avg_loss:0.078, val_acc:0.954]
Epoch [52/120    avg_loss:0.072, val_acc:0.947]
Epoch [53/120    avg_loss:0.114, val_acc:0.927]
Epoch [54/120    avg_loss:0.080, val_acc:0.953]
Epoch [55/120    avg_loss:0.083, val_acc:0.948]
Epoch [56/120    avg_loss:0.063, val_acc:0.954]
Epoch [57/120    avg_loss:0.053, val_acc:0.955]
Epoch [58/120    avg_loss:0.055, val_acc:0.950]
Epoch [59/120    avg_loss:0.078, val_acc:0.952]
Epoch [60/120    avg_loss:0.057, val_acc:0.957]
Epoch [61/120    avg_loss:0.053, val_acc:0.957]
Epoch [62/120    avg_loss:0.051, val_acc:0.942]
Epoch [63/120    avg_loss:0.065, val_acc:0.952]
Epoch [64/120    avg_loss:0.076, val_acc:0.950]
Epoch [65/120    avg_loss:0.067, val_acc:0.961]
Epoch [66/120    avg_loss:0.054, val_acc:0.960]
Epoch [67/120    avg_loss:0.077, val_acc:0.942]
Epoch [68/120    avg_loss:0.060, val_acc:0.960]
Epoch [69/120    avg_loss:0.053, val_acc:0.962]
Epoch [70/120    avg_loss:0.047, val_acc:0.965]
Epoch [71/120    avg_loss:0.048, val_acc:0.962]
Epoch [72/120    avg_loss:0.056, val_acc:0.963]
Epoch [73/120    avg_loss:0.054, val_acc:0.962]
Epoch [74/120    avg_loss:0.047, val_acc:0.955]
Epoch [75/120    avg_loss:0.050, val_acc:0.951]
Epoch [76/120    avg_loss:0.048, val_acc:0.963]
Epoch [77/120    avg_loss:0.053, val_acc:0.966]
Epoch [78/120    avg_loss:0.039, val_acc:0.971]
Epoch [79/120    avg_loss:0.032, val_acc:0.960]
Epoch [80/120    avg_loss:0.038, val_acc:0.967]
Epoch [81/120    avg_loss:0.031, val_acc:0.964]
Epoch [82/120    avg_loss:0.026, val_acc:0.970]
Epoch [83/120    avg_loss:0.024, val_acc:0.973]
Epoch [84/120    avg_loss:0.035, val_acc:0.962]
Epoch [85/120    avg_loss:0.042, val_acc:0.968]
Epoch [86/120    avg_loss:0.028, val_acc:0.968]
Epoch [87/120    avg_loss:0.030, val_acc:0.965]
Epoch [88/120    avg_loss:0.025, val_acc:0.970]
Epoch [89/120    avg_loss:0.027, val_acc:0.967]
Epoch [90/120    avg_loss:0.021, val_acc:0.972]
Epoch [91/120    avg_loss:0.017, val_acc:0.973]
Epoch [92/120    avg_loss:0.016, val_acc:0.968]
Epoch [93/120    avg_loss:0.021, val_acc:0.957]
Epoch [94/120    avg_loss:0.022, val_acc:0.973]
Epoch [95/120    avg_loss:0.022, val_acc:0.973]
Epoch [96/120    avg_loss:0.035, val_acc:0.959]
Epoch [97/120    avg_loss:0.255, val_acc:0.890]
Epoch [98/120    avg_loss:0.197, val_acc:0.930]
Epoch [99/120    avg_loss:0.133, val_acc:0.948]
Epoch [100/120    avg_loss:0.091, val_acc:0.927]
Epoch [101/120    avg_loss:0.125, val_acc:0.951]
Epoch [102/120    avg_loss:0.066, val_acc:0.947]
Epoch [103/120    avg_loss:0.062, val_acc:0.952]
Epoch [104/120    avg_loss:0.091, val_acc:0.952]
Epoch [105/120    avg_loss:0.055, val_acc:0.967]
Epoch [106/120    avg_loss:0.033, val_acc:0.966]
Epoch [107/120    avg_loss:0.031, val_acc:0.963]
Epoch [108/120    avg_loss:0.034, val_acc:0.963]
Epoch [109/120    avg_loss:0.028, val_acc:0.970]
Epoch [110/120    avg_loss:0.023, val_acc:0.970]
Epoch [111/120    avg_loss:0.019, val_acc:0.971]
Epoch [112/120    avg_loss:0.020, val_acc:0.972]
Epoch [113/120    avg_loss:0.023, val_acc:0.967]
Epoch [114/120    avg_loss:0.018, val_acc:0.972]
Epoch [115/120    avg_loss:0.024, val_acc:0.971]
Epoch [116/120    avg_loss:0.026, val_acc:0.971]
Epoch [117/120    avg_loss:0.020, val_acc:0.973]
Epoch [118/120    avg_loss:0.020, val_acc:0.974]
Epoch [119/120    avg_loss:0.018, val_acc:0.972]
Epoch [120/120    avg_loss:0.017, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1245    0    7    0    0    0    0    0    4   27    2    0
     0    0    0]
 [   0    0    0  697    2   11    1    0    0   13    1    6   16    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   16    1    0    3    0    0    0    0  848    3    0    0
     0    4    0]
 [   0    0   10    0    0    0    3    0    0    0   15 2180    0    1
     1    0    0]
 [   0    0    0    0    0    5    1    0    0    1   11    0  514    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    1    0    3    0    0    0
  1126    1    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
     9  313    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.5609756097561

F1 scores:
[       nan 0.975      0.9741784  0.96470588 0.97931034 0.96759777
 0.97691735 1.         0.99650757 0.68       0.9641842  0.98464318
 0.95895522 0.99730458 0.98989011 0.94135338 0.97005988]

Kappa:
0.972196146563068
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd0f1a33860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.776, val_acc:0.251]
Epoch [2/120    avg_loss:2.551, val_acc:0.263]
Epoch [3/120    avg_loss:2.393, val_acc:0.460]
Epoch [4/120    avg_loss:2.243, val_acc:0.429]
Epoch [5/120    avg_loss:2.123, val_acc:0.465]
Epoch [6/120    avg_loss:2.042, val_acc:0.523]
Epoch [7/120    avg_loss:1.916, val_acc:0.543]
Epoch [8/120    avg_loss:1.811, val_acc:0.542]
Epoch [9/120    avg_loss:1.730, val_acc:0.560]
Epoch [10/120    avg_loss:1.642, val_acc:0.577]
Epoch [11/120    avg_loss:1.555, val_acc:0.600]
Epoch [12/120    avg_loss:1.425, val_acc:0.616]
Epoch [13/120    avg_loss:1.361, val_acc:0.657]
Epoch [14/120    avg_loss:1.237, val_acc:0.682]
Epoch [15/120    avg_loss:1.171, val_acc:0.685]
Epoch [16/120    avg_loss:1.038, val_acc:0.699]
Epoch [17/120    avg_loss:0.992, val_acc:0.727]
Epoch [18/120    avg_loss:0.960, val_acc:0.746]
Epoch [19/120    avg_loss:0.846, val_acc:0.780]
Epoch [20/120    avg_loss:0.774, val_acc:0.746]
Epoch [21/120    avg_loss:0.772, val_acc:0.774]
Epoch [22/120    avg_loss:0.701, val_acc:0.782]
Epoch [23/120    avg_loss:0.648, val_acc:0.784]
Epoch [24/120    avg_loss:0.609, val_acc:0.798]
Epoch [25/120    avg_loss:0.572, val_acc:0.797]
Epoch [26/120    avg_loss:0.549, val_acc:0.797]
Epoch [27/120    avg_loss:0.489, val_acc:0.826]
Epoch [28/120    avg_loss:0.437, val_acc:0.826]
Epoch [29/120    avg_loss:0.442, val_acc:0.828]
Epoch [30/120    avg_loss:0.390, val_acc:0.859]
Epoch [31/120    avg_loss:0.339, val_acc:0.871]
Epoch [32/120    avg_loss:0.313, val_acc:0.885]
Epoch [33/120    avg_loss:0.293, val_acc:0.874]
Epoch [34/120    avg_loss:0.288, val_acc:0.879]
Epoch [35/120    avg_loss:0.241, val_acc:0.915]
Epoch [36/120    avg_loss:0.199, val_acc:0.899]
Epoch [37/120    avg_loss:0.203, val_acc:0.933]
Epoch [38/120    avg_loss:0.195, val_acc:0.909]
Epoch [39/120    avg_loss:0.177, val_acc:0.933]
Epoch [40/120    avg_loss:0.193, val_acc:0.932]
Epoch [41/120    avg_loss:0.155, val_acc:0.937]
Epoch [42/120    avg_loss:0.138, val_acc:0.922]
Epoch [43/120    avg_loss:0.163, val_acc:0.905]
Epoch [44/120    avg_loss:0.217, val_acc:0.926]
Epoch [45/120    avg_loss:0.157, val_acc:0.923]
Epoch [46/120    avg_loss:0.128, val_acc:0.939]
Epoch [47/120    avg_loss:0.140, val_acc:0.943]
Epoch [48/120    avg_loss:0.143, val_acc:0.932]
Epoch [49/120    avg_loss:0.130, val_acc:0.942]
Epoch [50/120    avg_loss:0.110, val_acc:0.939]
Epoch [51/120    avg_loss:0.108, val_acc:0.938]
Epoch [52/120    avg_loss:0.098, val_acc:0.940]
Epoch [53/120    avg_loss:0.107, val_acc:0.950]
Epoch [54/120    avg_loss:0.100, val_acc:0.943]
Epoch [55/120    avg_loss:0.092, val_acc:0.949]
Epoch [56/120    avg_loss:0.107, val_acc:0.940]
Epoch [57/120    avg_loss:0.079, val_acc:0.947]
Epoch [58/120    avg_loss:0.067, val_acc:0.963]
Epoch [59/120    avg_loss:0.066, val_acc:0.949]
Epoch [60/120    avg_loss:0.071, val_acc:0.947]
Epoch [61/120    avg_loss:0.071, val_acc:0.959]
Epoch [62/120    avg_loss:0.055, val_acc:0.959]
Epoch [63/120    avg_loss:0.060, val_acc:0.962]
Epoch [64/120    avg_loss:0.059, val_acc:0.951]
Epoch [65/120    avg_loss:0.076, val_acc:0.935]
Epoch [66/120    avg_loss:0.074, val_acc:0.948]
Epoch [67/120    avg_loss:0.077, val_acc:0.953]
Epoch [68/120    avg_loss:0.053, val_acc:0.960]
Epoch [69/120    avg_loss:0.056, val_acc:0.955]
Epoch [70/120    avg_loss:0.057, val_acc:0.958]
Epoch [71/120    avg_loss:0.097, val_acc:0.886]
Epoch [72/120    avg_loss:0.097, val_acc:0.937]
Epoch [73/120    avg_loss:0.055, val_acc:0.954]
Epoch [74/120    avg_loss:0.041, val_acc:0.957]
Epoch [75/120    avg_loss:0.042, val_acc:0.959]
Epoch [76/120    avg_loss:0.045, val_acc:0.960]
Epoch [77/120    avg_loss:0.037, val_acc:0.961]
Epoch [78/120    avg_loss:0.037, val_acc:0.964]
Epoch [79/120    avg_loss:0.037, val_acc:0.964]
Epoch [80/120    avg_loss:0.042, val_acc:0.964]
Epoch [81/120    avg_loss:0.049, val_acc:0.962]
Epoch [82/120    avg_loss:0.035, val_acc:0.960]
Epoch [83/120    avg_loss:0.044, val_acc:0.963]
Epoch [84/120    avg_loss:0.034, val_acc:0.962]
Epoch [85/120    avg_loss:0.039, val_acc:0.963]
Epoch [86/120    avg_loss:0.040, val_acc:0.963]
Epoch [87/120    avg_loss:0.032, val_acc:0.964]
Epoch [88/120    avg_loss:0.039, val_acc:0.965]
Epoch [89/120    avg_loss:0.030, val_acc:0.963]
Epoch [90/120    avg_loss:0.033, val_acc:0.963]
Epoch [91/120    avg_loss:0.029, val_acc:0.961]
Epoch [92/120    avg_loss:0.035, val_acc:0.962]
Epoch [93/120    avg_loss:0.034, val_acc:0.965]
Epoch [94/120    avg_loss:0.029, val_acc:0.965]
Epoch [95/120    avg_loss:0.032, val_acc:0.965]
Epoch [96/120    avg_loss:0.032, val_acc:0.964]
Epoch [97/120    avg_loss:0.028, val_acc:0.964]
Epoch [98/120    avg_loss:0.034, val_acc:0.965]
Epoch [99/120    avg_loss:0.034, val_acc:0.965]
Epoch [100/120    avg_loss:0.028, val_acc:0.965]
Epoch [101/120    avg_loss:0.026, val_acc:0.960]
Epoch [102/120    avg_loss:0.030, val_acc:0.963]
Epoch [103/120    avg_loss:0.027, val_acc:0.961]
Epoch [104/120    avg_loss:0.032, val_acc:0.963]
Epoch [105/120    avg_loss:0.028, val_acc:0.965]
Epoch [106/120    avg_loss:0.028, val_acc:0.963]
Epoch [107/120    avg_loss:0.031, val_acc:0.962]
Epoch [108/120    avg_loss:0.029, val_acc:0.962]
Epoch [109/120    avg_loss:0.033, val_acc:0.965]
Epoch [110/120    avg_loss:0.032, val_acc:0.965]
Epoch [111/120    avg_loss:0.027, val_acc:0.963]
Epoch [112/120    avg_loss:0.027, val_acc:0.964]
Epoch [113/120    avg_loss:0.028, val_acc:0.964]
Epoch [114/120    avg_loss:0.028, val_acc:0.965]
Epoch [115/120    avg_loss:0.028, val_acc:0.965]
Epoch [116/120    avg_loss:0.033, val_acc:0.963]
Epoch [117/120    avg_loss:0.028, val_acc:0.963]
Epoch [118/120    avg_loss:0.027, val_acc:0.966]
Epoch [119/120    avg_loss:0.028, val_acc:0.966]
Epoch [120/120    avg_loss:0.030, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    0    0    0    0    0    0    0    0    6    0    0    0
     0    0    0]
 [   0    0 1235    4    0    0    2    0    0    1   15   28    0    0
     0    0    0]
 [   0    0    1  710    1    3    0    0    0    6    4    0   19    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    3    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   30    0    0    2    0    0    0    0  811   30    0    0
     0    2    0]
 [   0    0   10    0    0    0    7    0    0    0    7 2163   17    3
     2    1    0]
 [   0    0    2    1    4    0    0    0    0    1    6    1  511    0
     1    2    5]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0  183
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    1    0    0
  1129    6    0]
 [   0    0    0    0    0    0   38    0    0    1    0    0    0    0
    13  295    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.75880758807588

F1 scores:
[       nan 0.92105263 0.9637144  0.97127223 0.98839907 0.98507463
 0.96546657 0.94339623 1.         0.69565217 0.93920093 0.97586285
 0.94367498 0.97860963 0.98861646 0.90352221 0.97109827]

Kappa:
0.9630425763214294
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6754166860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.760, val_acc:0.422]
Epoch [2/120    avg_loss:2.541, val_acc:0.473]
Epoch [3/120    avg_loss:2.376, val_acc:0.516]
Epoch [4/120    avg_loss:2.259, val_acc:0.505]
Epoch [5/120    avg_loss:2.125, val_acc:0.517]
Epoch [6/120    avg_loss:1.971, val_acc:0.530]
Epoch [7/120    avg_loss:1.918, val_acc:0.563]
Epoch [8/120    avg_loss:1.825, val_acc:0.590]
Epoch [9/120    avg_loss:1.739, val_acc:0.596]
Epoch [10/120    avg_loss:1.663, val_acc:0.597]
Epoch [11/120    avg_loss:1.570, val_acc:0.620]
Epoch [12/120    avg_loss:1.445, val_acc:0.636]
Epoch [13/120    avg_loss:1.350, val_acc:0.630]
Epoch [14/120    avg_loss:1.203, val_acc:0.628]
Epoch [15/120    avg_loss:1.155, val_acc:0.657]
Epoch [16/120    avg_loss:1.052, val_acc:0.661]
Epoch [17/120    avg_loss:0.965, val_acc:0.700]
Epoch [18/120    avg_loss:0.898, val_acc:0.718]
Epoch [19/120    avg_loss:0.821, val_acc:0.724]
Epoch [20/120    avg_loss:0.743, val_acc:0.737]
Epoch [21/120    avg_loss:0.702, val_acc:0.752]
Epoch [22/120    avg_loss:0.658, val_acc:0.774]
Epoch [23/120    avg_loss:0.615, val_acc:0.765]
Epoch [24/120    avg_loss:0.582, val_acc:0.787]
Epoch [25/120    avg_loss:0.527, val_acc:0.795]
Epoch [26/120    avg_loss:0.493, val_acc:0.824]
Epoch [27/120    avg_loss:0.500, val_acc:0.772]
Epoch [28/120    avg_loss:0.440, val_acc:0.858]
Epoch [29/120    avg_loss:0.383, val_acc:0.867]
Epoch [30/120    avg_loss:0.386, val_acc:0.852]
Epoch [31/120    avg_loss:0.412, val_acc:0.859]
Epoch [32/120    avg_loss:0.321, val_acc:0.868]
Epoch [33/120    avg_loss:0.273, val_acc:0.878]
Epoch [34/120    avg_loss:0.254, val_acc:0.898]
Epoch [35/120    avg_loss:0.221, val_acc:0.918]
Epoch [36/120    avg_loss:0.218, val_acc:0.917]
Epoch [37/120    avg_loss:0.267, val_acc:0.872]
Epoch [38/120    avg_loss:0.325, val_acc:0.873]
Epoch [39/120    avg_loss:0.312, val_acc:0.899]
Epoch [40/120    avg_loss:0.195, val_acc:0.895]
Epoch [41/120    avg_loss:0.170, val_acc:0.922]
Epoch [42/120    avg_loss:0.173, val_acc:0.897]
Epoch [43/120    avg_loss:0.162, val_acc:0.923]
Epoch [44/120    avg_loss:0.168, val_acc:0.914]
Epoch [45/120    avg_loss:0.195, val_acc:0.905]
Epoch [46/120    avg_loss:0.167, val_acc:0.920]
Epoch [47/120    avg_loss:0.175, val_acc:0.918]
Epoch [48/120    avg_loss:0.136, val_acc:0.922]
Epoch [49/120    avg_loss:0.123, val_acc:0.947]
Epoch [50/120    avg_loss:0.117, val_acc:0.957]
Epoch [51/120    avg_loss:0.089, val_acc:0.952]
Epoch [52/120    avg_loss:0.104, val_acc:0.937]
Epoch [53/120    avg_loss:0.109, val_acc:0.954]
Epoch [54/120    avg_loss:0.110, val_acc:0.951]
Epoch [55/120    avg_loss:0.101, val_acc:0.936]
Epoch [56/120    avg_loss:0.097, val_acc:0.947]
Epoch [57/120    avg_loss:0.089, val_acc:0.945]
Epoch [58/120    avg_loss:0.074, val_acc:0.963]
Epoch [59/120    avg_loss:0.070, val_acc:0.950]
Epoch [60/120    avg_loss:0.071, val_acc:0.950]
Epoch [61/120    avg_loss:0.067, val_acc:0.955]
Epoch [62/120    avg_loss:0.084, val_acc:0.947]
Epoch [63/120    avg_loss:0.090, val_acc:0.955]
Epoch [64/120    avg_loss:0.078, val_acc:0.965]
Epoch [65/120    avg_loss:0.064, val_acc:0.962]
Epoch [66/120    avg_loss:0.058, val_acc:0.950]
Epoch [67/120    avg_loss:0.103, val_acc:0.929]
Epoch [68/120    avg_loss:0.074, val_acc:0.959]
Epoch [69/120    avg_loss:0.054, val_acc:0.967]
Epoch [70/120    avg_loss:0.049, val_acc:0.958]
Epoch [71/120    avg_loss:0.054, val_acc:0.961]
Epoch [72/120    avg_loss:0.047, val_acc:0.963]
Epoch [73/120    avg_loss:0.071, val_acc:0.941]
Epoch [74/120    avg_loss:0.079, val_acc:0.938]
Epoch [75/120    avg_loss:0.074, val_acc:0.963]
Epoch [76/120    avg_loss:0.056, val_acc:0.970]
Epoch [77/120    avg_loss:0.038, val_acc:0.967]
Epoch [78/120    avg_loss:0.037, val_acc:0.973]
Epoch [79/120    avg_loss:0.039, val_acc:0.958]
Epoch [80/120    avg_loss:0.041, val_acc:0.962]
Epoch [81/120    avg_loss:0.050, val_acc:0.958]
Epoch [82/120    avg_loss:0.062, val_acc:0.928]
Epoch [83/120    avg_loss:0.070, val_acc:0.962]
Epoch [84/120    avg_loss:0.034, val_acc:0.968]
Epoch [85/120    avg_loss:0.038, val_acc:0.968]
Epoch [86/120    avg_loss:0.027, val_acc:0.973]
Epoch [87/120    avg_loss:0.026, val_acc:0.966]
Epoch [88/120    avg_loss:0.023, val_acc:0.966]
Epoch [89/120    avg_loss:0.027, val_acc:0.968]
Epoch [90/120    avg_loss:0.026, val_acc:0.976]
Epoch [91/120    avg_loss:0.017, val_acc:0.971]
Epoch [92/120    avg_loss:0.027, val_acc:0.967]
Epoch [93/120    avg_loss:0.032, val_acc:0.971]
Epoch [94/120    avg_loss:0.026, val_acc:0.961]
Epoch [95/120    avg_loss:0.034, val_acc:0.971]
Epoch [96/120    avg_loss:0.027, val_acc:0.966]
Epoch [97/120    avg_loss:0.038, val_acc:0.970]
Epoch [98/120    avg_loss:0.022, val_acc:0.972]
Epoch [99/120    avg_loss:0.029, val_acc:0.965]
Epoch [100/120    avg_loss:0.025, val_acc:0.972]
Epoch [101/120    avg_loss:0.020, val_acc:0.967]
Epoch [102/120    avg_loss:0.027, val_acc:0.960]
Epoch [103/120    avg_loss:0.030, val_acc:0.965]
Epoch [104/120    avg_loss:0.022, val_acc:0.971]
Epoch [105/120    avg_loss:0.017, val_acc:0.972]
Epoch [106/120    avg_loss:0.020, val_acc:0.973]
Epoch [107/120    avg_loss:0.015, val_acc:0.972]
Epoch [108/120    avg_loss:0.016, val_acc:0.972]
Epoch [109/120    avg_loss:0.019, val_acc:0.974]
Epoch [110/120    avg_loss:0.016, val_acc:0.972]
Epoch [111/120    avg_loss:0.015, val_acc:0.973]
Epoch [112/120    avg_loss:0.016, val_acc:0.974]
Epoch [113/120    avg_loss:0.015, val_acc:0.972]
Epoch [114/120    avg_loss:0.016, val_acc:0.973]
Epoch [115/120    avg_loss:0.015, val_acc:0.976]
Epoch [116/120    avg_loss:0.014, val_acc:0.977]
Epoch [117/120    avg_loss:0.015, val_acc:0.974]
Epoch [118/120    avg_loss:0.012, val_acc:0.974]
Epoch [119/120    avg_loss:0.014, val_acc:0.974]
Epoch [120/120    avg_loss:0.013, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    3    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1252    1    2    0    0    0    0    0    6   21    0    0
     0    3    0]
 [   0    0    1  704    0   14    1    0    0    6    0    7   13    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    1    0    0    2    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   14    0    0    5    3    0    0    0  844    4    0    0
     0    5    0]
 [   0    0   11    0    0    1    5    1    2    0   12 2174    4    0
     0    0    0]
 [   0    0    0    0    1    3    0    0    0    0    3   15  508    0
     1    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    1    0    0    2
  1132    1    0]
 [   0    0    0    0    0    0   32    0    0    1    0    0    0    0
     8  306    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.51761517615176

F1 scores:
[       nan 0.94871795 0.97583788 0.96969697 0.99300699 0.96520763
 0.96826568 0.98039216 0.99534884 0.8        0.96900115 0.98082563
 0.95758718 0.99462366 0.99254713 0.92168675 0.98823529]

Kappa:
0.9716941006876021
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd172ef0898>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.754, val_acc:0.258]
Epoch [2/120    avg_loss:2.477, val_acc:0.332]
Epoch [3/120    avg_loss:2.270, val_acc:0.434]
Epoch [4/120    avg_loss:2.156, val_acc:0.386]
Epoch [5/120    avg_loss:2.043, val_acc:0.479]
Epoch [6/120    avg_loss:1.948, val_acc:0.543]
Epoch [7/120    avg_loss:1.868, val_acc:0.582]
Epoch [8/120    avg_loss:1.804, val_acc:0.597]
Epoch [9/120    avg_loss:1.680, val_acc:0.607]
Epoch [10/120    avg_loss:1.550, val_acc:0.621]
Epoch [11/120    avg_loss:1.470, val_acc:0.652]
Epoch [12/120    avg_loss:1.335, val_acc:0.697]
Epoch [13/120    avg_loss:1.236, val_acc:0.673]
Epoch [14/120    avg_loss:1.211, val_acc:0.659]
Epoch [15/120    avg_loss:1.095, val_acc:0.679]
Epoch [16/120    avg_loss:0.969, val_acc:0.722]
Epoch [17/120    avg_loss:0.847, val_acc:0.759]
Epoch [18/120    avg_loss:0.761, val_acc:0.782]
Epoch [19/120    avg_loss:0.698, val_acc:0.790]
Epoch [20/120    avg_loss:0.654, val_acc:0.812]
Epoch [21/120    avg_loss:0.590, val_acc:0.807]
Epoch [22/120    avg_loss:0.574, val_acc:0.821]
Epoch [23/120    avg_loss:0.489, val_acc:0.865]
Epoch [24/120    avg_loss:0.462, val_acc:0.841]
Epoch [25/120    avg_loss:0.450, val_acc:0.846]
Epoch [26/120    avg_loss:0.398, val_acc:0.851]
Epoch [27/120    avg_loss:0.388, val_acc:0.847]
Epoch [28/120    avg_loss:0.426, val_acc:0.851]
Epoch [29/120    avg_loss:0.395, val_acc:0.839]
Epoch [30/120    avg_loss:0.387, val_acc:0.872]
Epoch [31/120    avg_loss:0.335, val_acc:0.870]
Epoch [32/120    avg_loss:0.270, val_acc:0.902]
Epoch [33/120    avg_loss:0.263, val_acc:0.900]
Epoch [34/120    avg_loss:0.257, val_acc:0.902]
Epoch [35/120    avg_loss:0.227, val_acc:0.884]
Epoch [36/120    avg_loss:0.214, val_acc:0.915]
Epoch [37/120    avg_loss:0.217, val_acc:0.900]
Epoch [38/120    avg_loss:0.205, val_acc:0.902]
Epoch [39/120    avg_loss:0.217, val_acc:0.901]
Epoch [40/120    avg_loss:0.211, val_acc:0.909]
Epoch [41/120    avg_loss:0.166, val_acc:0.922]
Epoch [42/120    avg_loss:0.134, val_acc:0.922]
Epoch [43/120    avg_loss:0.138, val_acc:0.923]
Epoch [44/120    avg_loss:0.110, val_acc:0.935]
Epoch [45/120    avg_loss:0.131, val_acc:0.923]
Epoch [46/120    avg_loss:0.108, val_acc:0.923]
Epoch [47/120    avg_loss:0.136, val_acc:0.904]
Epoch [48/120    avg_loss:0.228, val_acc:0.883]
Epoch [49/120    avg_loss:0.313, val_acc:0.893]
Epoch [50/120    avg_loss:0.206, val_acc:0.910]
Epoch [51/120    avg_loss:0.137, val_acc:0.937]
Epoch [52/120    avg_loss:0.145, val_acc:0.926]
Epoch [53/120    avg_loss:0.116, val_acc:0.935]
Epoch [54/120    avg_loss:0.134, val_acc:0.923]
Epoch [55/120    avg_loss:0.118, val_acc:0.938]
Epoch [56/120    avg_loss:0.117, val_acc:0.927]
Epoch [57/120    avg_loss:0.108, val_acc:0.930]
Epoch [58/120    avg_loss:0.132, val_acc:0.937]
Epoch [59/120    avg_loss:0.105, val_acc:0.945]
Epoch [60/120    avg_loss:0.093, val_acc:0.943]
Epoch [61/120    avg_loss:0.070, val_acc:0.946]
Epoch [62/120    avg_loss:0.099, val_acc:0.955]
Epoch [63/120    avg_loss:0.073, val_acc:0.935]
Epoch [64/120    avg_loss:0.067, val_acc:0.938]
Epoch [65/120    avg_loss:0.068, val_acc:0.935]
Epoch [66/120    avg_loss:0.063, val_acc:0.947]
Epoch [67/120    avg_loss:0.054, val_acc:0.958]
Epoch [68/120    avg_loss:0.063, val_acc:0.952]
Epoch [69/120    avg_loss:0.068, val_acc:0.952]
Epoch [70/120    avg_loss:0.057, val_acc:0.945]
Epoch [71/120    avg_loss:0.072, val_acc:0.945]
Epoch [72/120    avg_loss:0.062, val_acc:0.958]
Epoch [73/120    avg_loss:0.062, val_acc:0.951]
Epoch [74/120    avg_loss:0.055, val_acc:0.960]
Epoch [75/120    avg_loss:0.078, val_acc:0.938]
Epoch [76/120    avg_loss:0.093, val_acc:0.935]
Epoch [77/120    avg_loss:0.066, val_acc:0.964]
Epoch [78/120    avg_loss:0.056, val_acc:0.955]
Epoch [79/120    avg_loss:0.041, val_acc:0.963]
Epoch [80/120    avg_loss:0.048, val_acc:0.961]
Epoch [81/120    avg_loss:0.059, val_acc:0.957]
Epoch [82/120    avg_loss:0.047, val_acc:0.950]
Epoch [83/120    avg_loss:0.046, val_acc:0.962]
Epoch [84/120    avg_loss:0.052, val_acc:0.974]
Epoch [85/120    avg_loss:0.048, val_acc:0.955]
Epoch [86/120    avg_loss:0.039, val_acc:0.958]
Epoch [87/120    avg_loss:0.041, val_acc:0.967]
Epoch [88/120    avg_loss:0.067, val_acc:0.936]
Epoch [89/120    avg_loss:0.056, val_acc:0.951]
Epoch [90/120    avg_loss:0.051, val_acc:0.954]
Epoch [91/120    avg_loss:0.051, val_acc:0.963]
Epoch [92/120    avg_loss:0.040, val_acc:0.958]
Epoch [93/120    avg_loss:0.035, val_acc:0.951]
Epoch [94/120    avg_loss:0.032, val_acc:0.964]
Epoch [95/120    avg_loss:0.030, val_acc:0.960]
Epoch [96/120    avg_loss:0.031, val_acc:0.968]
Epoch [97/120    avg_loss:0.033, val_acc:0.962]
Epoch [98/120    avg_loss:0.033, val_acc:0.966]
Epoch [99/120    avg_loss:0.023, val_acc:0.970]
Epoch [100/120    avg_loss:0.024, val_acc:0.970]
Epoch [101/120    avg_loss:0.025, val_acc:0.973]
Epoch [102/120    avg_loss:0.019, val_acc:0.974]
Epoch [103/120    avg_loss:0.023, val_acc:0.975]
Epoch [104/120    avg_loss:0.019, val_acc:0.973]
Epoch [105/120    avg_loss:0.022, val_acc:0.976]
Epoch [106/120    avg_loss:0.016, val_acc:0.975]
Epoch [107/120    avg_loss:0.022, val_acc:0.977]
Epoch [108/120    avg_loss:0.016, val_acc:0.975]
Epoch [109/120    avg_loss:0.016, val_acc:0.975]
Epoch [110/120    avg_loss:0.018, val_acc:0.973]
Epoch [111/120    avg_loss:0.016, val_acc:0.976]
Epoch [112/120    avg_loss:0.021, val_acc:0.976]
Epoch [113/120    avg_loss:0.015, val_acc:0.976]
Epoch [114/120    avg_loss:0.017, val_acc:0.977]
Epoch [115/120    avg_loss:0.018, val_acc:0.976]
Epoch [116/120    avg_loss:0.018, val_acc:0.975]
Epoch [117/120    avg_loss:0.015, val_acc:0.975]
Epoch [118/120    avg_loss:0.014, val_acc:0.975]
Epoch [119/120    avg_loss:0.015, val_acc:0.977]
Epoch [120/120    avg_loss:0.020, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1240    0    4    0    1    0    0    1   11   25    3    0
     0    0    0]
 [   0    0    0  689    1    8    1    0    0   12    3    0   32    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    5    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   16    0    0    6    1    0    0    0  837    5    1    0
     0    9    0]
 [   0    0   11    0    0    1    2    0    1    0   19 2172    1    1
     0    2    0]
 [   0    0    0    6    4    1    0    0    0    0    0    0  521    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    1    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0    3    0    0    6    0    0    0    0
    42  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
97.12737127371274

F1 scores:
[       nan 0.975      0.97178683 0.9556172  0.97931034 0.97278912
 0.99090909 0.90909091 0.99883856 0.59259259 0.9576659  0.98391846
 0.94813467 0.99462366 0.98057833 0.90519878 0.95121951]

Kappa:
0.9672563416079897
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f14dd5f5908>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.794, val_acc:0.141]
Epoch [2/120    avg_loss:2.543, val_acc:0.386]
Epoch [3/120    avg_loss:2.369, val_acc:0.432]
Epoch [4/120    avg_loss:2.247, val_acc:0.446]
Epoch [5/120    avg_loss:2.121, val_acc:0.495]
Epoch [6/120    avg_loss:2.014, val_acc:0.508]
Epoch [7/120    avg_loss:1.927, val_acc:0.522]
Epoch [8/120    avg_loss:1.807, val_acc:0.575]
Epoch [9/120    avg_loss:1.692, val_acc:0.610]
Epoch [10/120    avg_loss:1.610, val_acc:0.622]
Epoch [11/120    avg_loss:1.525, val_acc:0.601]
Epoch [12/120    avg_loss:1.366, val_acc:0.666]
Epoch [13/120    avg_loss:1.262, val_acc:0.673]
Epoch [14/120    avg_loss:1.148, val_acc:0.670]
Epoch [15/120    avg_loss:1.037, val_acc:0.715]
Epoch [16/120    avg_loss:0.944, val_acc:0.708]
Epoch [17/120    avg_loss:0.946, val_acc:0.749]
Epoch [18/120    avg_loss:0.838, val_acc:0.713]
Epoch [19/120    avg_loss:0.836, val_acc:0.720]
Epoch [20/120    avg_loss:0.819, val_acc:0.762]
Epoch [21/120    avg_loss:0.685, val_acc:0.762]
Epoch [22/120    avg_loss:0.630, val_acc:0.790]
Epoch [23/120    avg_loss:0.570, val_acc:0.821]
Epoch [24/120    avg_loss:0.575, val_acc:0.785]
Epoch [25/120    avg_loss:0.613, val_acc:0.796]
Epoch [26/120    avg_loss:0.472, val_acc:0.827]
Epoch [27/120    avg_loss:0.441, val_acc:0.842]
Epoch [28/120    avg_loss:0.366, val_acc:0.857]
Epoch [29/120    avg_loss:0.390, val_acc:0.845]
Epoch [30/120    avg_loss:0.342, val_acc:0.870]
Epoch [31/120    avg_loss:0.307, val_acc:0.830]
Epoch [32/120    avg_loss:0.303, val_acc:0.880]
Epoch [33/120    avg_loss:0.271, val_acc:0.898]
Epoch [34/120    avg_loss:0.261, val_acc:0.903]
Epoch [35/120    avg_loss:0.238, val_acc:0.898]
Epoch [36/120    avg_loss:0.255, val_acc:0.903]
Epoch [37/120    avg_loss:0.214, val_acc:0.899]
Epoch [38/120    avg_loss:0.171, val_acc:0.912]
Epoch [39/120    avg_loss:0.199, val_acc:0.897]
Epoch [40/120    avg_loss:0.184, val_acc:0.911]
Epoch [41/120    avg_loss:0.182, val_acc:0.915]
Epoch [42/120    avg_loss:0.201, val_acc:0.920]
Epoch [43/120    avg_loss:0.158, val_acc:0.940]
Epoch [44/120    avg_loss:0.131, val_acc:0.932]
Epoch [45/120    avg_loss:0.139, val_acc:0.938]
Epoch [46/120    avg_loss:0.160, val_acc:0.929]
Epoch [47/120    avg_loss:0.174, val_acc:0.910]
Epoch [48/120    avg_loss:0.147, val_acc:0.916]
Epoch [49/120    avg_loss:0.102, val_acc:0.945]
Epoch [50/120    avg_loss:0.113, val_acc:0.933]
Epoch [51/120    avg_loss:0.104, val_acc:0.934]
Epoch [52/120    avg_loss:0.097, val_acc:0.941]
Epoch [53/120    avg_loss:0.088, val_acc:0.940]
Epoch [54/120    avg_loss:0.082, val_acc:0.939]
Epoch [55/120    avg_loss:0.096, val_acc:0.915]
Epoch [56/120    avg_loss:0.109, val_acc:0.942]
Epoch [57/120    avg_loss:0.086, val_acc:0.938]
Epoch [58/120    avg_loss:0.099, val_acc:0.945]
Epoch [59/120    avg_loss:0.097, val_acc:0.928]
Epoch [60/120    avg_loss:0.122, val_acc:0.926]
Epoch [61/120    avg_loss:0.077, val_acc:0.946]
Epoch [62/120    avg_loss:0.073, val_acc:0.952]
Epoch [63/120    avg_loss:0.066, val_acc:0.949]
Epoch [64/120    avg_loss:0.074, val_acc:0.941]
Epoch [65/120    avg_loss:0.073, val_acc:0.948]
Epoch [66/120    avg_loss:0.065, val_acc:0.954]
Epoch [67/120    avg_loss:0.050, val_acc:0.946]
Epoch [68/120    avg_loss:0.046, val_acc:0.963]
Epoch [69/120    avg_loss:0.061, val_acc:0.959]
Epoch [70/120    avg_loss:0.062, val_acc:0.946]
Epoch [71/120    avg_loss:0.070, val_acc:0.950]
Epoch [72/120    avg_loss:0.052, val_acc:0.965]
Epoch [73/120    avg_loss:0.041, val_acc:0.958]
Epoch [74/120    avg_loss:0.052, val_acc:0.955]
Epoch [75/120    avg_loss:0.052, val_acc:0.948]
Epoch [76/120    avg_loss:0.038, val_acc:0.950]
Epoch [77/120    avg_loss:0.041, val_acc:0.960]
Epoch [78/120    avg_loss:0.036, val_acc:0.964]
Epoch [79/120    avg_loss:0.038, val_acc:0.959]
Epoch [80/120    avg_loss:0.038, val_acc:0.940]
Epoch [81/120    avg_loss:0.052, val_acc:0.949]
Epoch [82/120    avg_loss:0.034, val_acc:0.971]
Epoch [83/120    avg_loss:0.031, val_acc:0.968]
Epoch [84/120    avg_loss:0.034, val_acc:0.965]
Epoch [85/120    avg_loss:0.026, val_acc:0.968]
Epoch [86/120    avg_loss:0.022, val_acc:0.971]
Epoch [87/120    avg_loss:0.031, val_acc:0.965]
Epoch [88/120    avg_loss:0.026, val_acc:0.951]
Epoch [89/120    avg_loss:0.031, val_acc:0.949]
Epoch [90/120    avg_loss:0.039, val_acc:0.965]
Epoch [91/120    avg_loss:0.029, val_acc:0.966]
Epoch [92/120    avg_loss:0.034, val_acc:0.957]
Epoch [93/120    avg_loss:0.025, val_acc:0.968]
Epoch [94/120    avg_loss:0.024, val_acc:0.972]
Epoch [95/120    avg_loss:0.025, val_acc:0.965]
Epoch [96/120    avg_loss:0.028, val_acc:0.955]
Epoch [97/120    avg_loss:0.035, val_acc:0.978]
Epoch [98/120    avg_loss:0.030, val_acc:0.963]
Epoch [99/120    avg_loss:0.035, val_acc:0.968]
Epoch [100/120    avg_loss:0.040, val_acc:0.964]
Epoch [101/120    avg_loss:0.035, val_acc:0.957]
Epoch [102/120    avg_loss:0.025, val_acc:0.973]
Epoch [103/120    avg_loss:0.024, val_acc:0.961]
Epoch [104/120    avg_loss:0.020, val_acc:0.965]
Epoch [105/120    avg_loss:0.018, val_acc:0.967]
Epoch [106/120    avg_loss:0.018, val_acc:0.972]
Epoch [107/120    avg_loss:0.027, val_acc:0.967]
Epoch [108/120    avg_loss:0.023, val_acc:0.963]
Epoch [109/120    avg_loss:0.029, val_acc:0.958]
Epoch [110/120    avg_loss:0.030, val_acc:0.973]
Epoch [111/120    avg_loss:0.021, val_acc:0.972]
Epoch [112/120    avg_loss:0.014, val_acc:0.968]
Epoch [113/120    avg_loss:0.014, val_acc:0.971]
Epoch [114/120    avg_loss:0.016, val_acc:0.972]
Epoch [115/120    avg_loss:0.013, val_acc:0.973]
Epoch [116/120    avg_loss:0.012, val_acc:0.974]
Epoch [117/120    avg_loss:0.017, val_acc:0.973]
Epoch [118/120    avg_loss:0.012, val_acc:0.974]
Epoch [119/120    avg_loss:0.011, val_acc:0.975]
Epoch [120/120    avg_loss:0.015, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1259    1    4    0    1    0    0    0    3   17    0    0
     0    0    0]
 [   0    0    0  701    1   13    0    0    0   14    0    7   11    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    3    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    6    0    0    0    0  844   10    0    0
     1    5    0]
 [   0    0   10    0    0    0    1    0    0    0    4 2185    5    5
     0    0    0]
 [   0    0    4    0    0    7    0    0    0    0    9    5  506    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   12    0    0    0    0    3    1    0    0
  1120    3    0]
 [   0    0    0    0    0    0   44    0    0    0    0    0    0    0
     0  303    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.51761517615176

F1 scores:
[       nan 0.98765432 0.98091157 0.96756384 0.98839907 0.95469613
 0.96236162 1.         1.         0.72       0.9706728  0.98467778
 0.95742668 0.98666667 0.98896247 0.91957511 0.98224852]

Kappa:
0.9716969263694707
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff43b5ec908>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.815, val_acc:0.449]
Epoch [2/120    avg_loss:2.536, val_acc:0.436]
Epoch [3/120    avg_loss:2.312, val_acc:0.509]
Epoch [4/120    avg_loss:2.200, val_acc:0.538]
Epoch [5/120    avg_loss:2.064, val_acc:0.563]
Epoch [6/120    avg_loss:1.969, val_acc:0.584]
Epoch [7/120    avg_loss:1.881, val_acc:0.586]
Epoch [8/120    avg_loss:1.747, val_acc:0.609]
Epoch [9/120    avg_loss:1.667, val_acc:0.628]
Epoch [10/120    avg_loss:1.537, val_acc:0.663]
Epoch [11/120    avg_loss:1.444, val_acc:0.642]
Epoch [12/120    avg_loss:1.338, val_acc:0.659]
Epoch [13/120    avg_loss:1.228, val_acc:0.687]
Epoch [14/120    avg_loss:1.133, val_acc:0.711]
Epoch [15/120    avg_loss:0.976, val_acc:0.692]
Epoch [16/120    avg_loss:0.890, val_acc:0.729]
Epoch [17/120    avg_loss:0.795, val_acc:0.738]
Epoch [18/120    avg_loss:0.758, val_acc:0.736]
Epoch [19/120    avg_loss:0.700, val_acc:0.792]
Epoch [20/120    avg_loss:0.634, val_acc:0.796]
Epoch [21/120    avg_loss:0.576, val_acc:0.796]
Epoch [22/120    avg_loss:0.528, val_acc:0.801]
Epoch [23/120    avg_loss:0.555, val_acc:0.801]
Epoch [24/120    avg_loss:0.533, val_acc:0.824]
Epoch [25/120    avg_loss:0.475, val_acc:0.780]
Epoch [26/120    avg_loss:0.481, val_acc:0.834]
Epoch [27/120    avg_loss:0.400, val_acc:0.857]
Epoch [28/120    avg_loss:0.327, val_acc:0.901]
Epoch [29/120    avg_loss:0.306, val_acc:0.882]
Epoch [30/120    avg_loss:0.295, val_acc:0.849]
Epoch [31/120    avg_loss:0.278, val_acc:0.884]
Epoch [32/120    avg_loss:0.271, val_acc:0.901]
Epoch [33/120    avg_loss:0.225, val_acc:0.897]
Epoch [34/120    avg_loss:0.213, val_acc:0.901]
Epoch [35/120    avg_loss:0.194, val_acc:0.888]
Epoch [36/120    avg_loss:0.205, val_acc:0.890]
Epoch [37/120    avg_loss:0.218, val_acc:0.900]
Epoch [38/120    avg_loss:0.177, val_acc:0.904]
Epoch [39/120    avg_loss:0.190, val_acc:0.904]
Epoch [40/120    avg_loss:0.177, val_acc:0.908]
Epoch [41/120    avg_loss:0.175, val_acc:0.920]
Epoch [42/120    avg_loss:0.221, val_acc:0.854]
Epoch [43/120    avg_loss:0.213, val_acc:0.929]
Epoch [44/120    avg_loss:0.170, val_acc:0.908]
Epoch [45/120    avg_loss:0.130, val_acc:0.922]
Epoch [46/120    avg_loss:0.105, val_acc:0.938]
Epoch [47/120    avg_loss:0.103, val_acc:0.949]
Epoch [48/120    avg_loss:0.131, val_acc:0.947]
Epoch [49/120    avg_loss:0.099, val_acc:0.945]
Epoch [50/120    avg_loss:0.104, val_acc:0.946]
Epoch [51/120    avg_loss:0.136, val_acc:0.929]
Epoch [52/120    avg_loss:0.109, val_acc:0.942]
Epoch [53/120    avg_loss:0.123, val_acc:0.951]
Epoch [54/120    avg_loss:0.107, val_acc:0.942]
Epoch [55/120    avg_loss:0.092, val_acc:0.954]
Epoch [56/120    avg_loss:0.088, val_acc:0.952]
Epoch [57/120    avg_loss:0.072, val_acc:0.954]
Epoch [58/120    avg_loss:0.070, val_acc:0.966]
Epoch [59/120    avg_loss:0.078, val_acc:0.955]
Epoch [60/120    avg_loss:0.060, val_acc:0.960]
Epoch [61/120    avg_loss:0.058, val_acc:0.963]
Epoch [62/120    avg_loss:0.053, val_acc:0.955]
Epoch [63/120    avg_loss:0.058, val_acc:0.965]
Epoch [64/120    avg_loss:0.081, val_acc:0.962]
Epoch [65/120    avg_loss:0.063, val_acc:0.965]
Epoch [66/120    avg_loss:0.050, val_acc:0.971]
Epoch [67/120    avg_loss:0.051, val_acc:0.975]
Epoch [68/120    avg_loss:0.038, val_acc:0.966]
Epoch [69/120    avg_loss:0.040, val_acc:0.967]
Epoch [70/120    avg_loss:0.046, val_acc:0.968]
Epoch [71/120    avg_loss:0.046, val_acc:0.974]
Epoch [72/120    avg_loss:0.048, val_acc:0.966]
Epoch [73/120    avg_loss:0.036, val_acc:0.975]
Epoch [74/120    avg_loss:0.040, val_acc:0.968]
Epoch [75/120    avg_loss:0.033, val_acc:0.971]
Epoch [76/120    avg_loss:0.038, val_acc:0.967]
Epoch [77/120    avg_loss:0.036, val_acc:0.978]
Epoch [78/120    avg_loss:0.033, val_acc:0.964]
Epoch [79/120    avg_loss:0.038, val_acc:0.958]
Epoch [80/120    avg_loss:0.052, val_acc:0.946]
Epoch [81/120    avg_loss:0.069, val_acc:0.972]
Epoch [82/120    avg_loss:0.042, val_acc:0.965]
Epoch [83/120    avg_loss:0.037, val_acc:0.968]
Epoch [84/120    avg_loss:0.037, val_acc:0.966]
Epoch [85/120    avg_loss:0.048, val_acc:0.975]
Epoch [86/120    avg_loss:0.049, val_acc:0.949]
Epoch [87/120    avg_loss:0.055, val_acc:0.967]
Epoch [88/120    avg_loss:0.041, val_acc:0.971]
Epoch [89/120    avg_loss:0.040, val_acc:0.971]
Epoch [90/120    avg_loss:0.039, val_acc:0.974]
Epoch [91/120    avg_loss:0.040, val_acc:0.983]
Epoch [92/120    avg_loss:0.023, val_acc:0.983]
Epoch [93/120    avg_loss:0.022, val_acc:0.985]
Epoch [94/120    avg_loss:0.023, val_acc:0.985]
Epoch [95/120    avg_loss:0.021, val_acc:0.985]
Epoch [96/120    avg_loss:0.019, val_acc:0.985]
Epoch [97/120    avg_loss:0.020, val_acc:0.984]
Epoch [98/120    avg_loss:0.017, val_acc:0.985]
Epoch [99/120    avg_loss:0.019, val_acc:0.984]
Epoch [100/120    avg_loss:0.018, val_acc:0.984]
Epoch [101/120    avg_loss:0.019, val_acc:0.983]
Epoch [102/120    avg_loss:0.017, val_acc:0.984]
Epoch [103/120    avg_loss:0.020, val_acc:0.985]
Epoch [104/120    avg_loss:0.018, val_acc:0.983]
Epoch [105/120    avg_loss:0.017, val_acc:0.980]
Epoch [106/120    avg_loss:0.016, val_acc:0.980]
Epoch [107/120    avg_loss:0.016, val_acc:0.982]
Epoch [108/120    avg_loss:0.016, val_acc:0.980]
Epoch [109/120    avg_loss:0.020, val_acc:0.983]
Epoch [110/120    avg_loss:0.018, val_acc:0.983]
Epoch [111/120    avg_loss:0.016, val_acc:0.985]
Epoch [112/120    avg_loss:0.015, val_acc:0.983]
Epoch [113/120    avg_loss:0.017, val_acc:0.982]
Epoch [114/120    avg_loss:0.016, val_acc:0.982]
Epoch [115/120    avg_loss:0.017, val_acc:0.983]
Epoch [116/120    avg_loss:0.014, val_acc:0.983]
Epoch [117/120    avg_loss:0.016, val_acc:0.983]
Epoch [118/120    avg_loss:0.015, val_acc:0.982]
Epoch [119/120    avg_loss:0.015, val_acc:0.982]
Epoch [120/120    avg_loss:0.015, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1256    3    4    0    2    0    0    0    6   14    0    0
     0    0    0]
 [   0    0    0  718    0    4    0    0    0    5    1    1   16    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    2    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   12    0    0    3    1    0    0    0  847    9    0    0
     0    3    0]
 [   0    0    6    0    0    0    4    0    0    0   16 2178    4    2
     0    0    0]
 [   0    0    0    1    2    2    0    0    0    0   10    0  515    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    1    0    0    0
  1127    3    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    23  316    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.85365853658537

F1 scores:
[       nan 0.975      0.98163345 0.97620666 0.98611111 0.97266515
 0.98489426 1.         0.99883856 0.7804878  0.96359499 0.98730734
 0.96261682 0.98930481 0.98       0.94469357 0.97076023]

Kappa:
0.9755336220112772
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb609257940>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.778, val_acc:0.345]
Epoch [2/120    avg_loss:2.550, val_acc:0.424]
Epoch [3/120    avg_loss:2.337, val_acc:0.480]
Epoch [4/120    avg_loss:2.227, val_acc:0.492]
Epoch [5/120    avg_loss:2.076, val_acc:0.512]
Epoch [6/120    avg_loss:1.993, val_acc:0.551]
Epoch [7/120    avg_loss:1.866, val_acc:0.603]
Epoch [8/120    avg_loss:1.763, val_acc:0.613]
Epoch [9/120    avg_loss:1.676, val_acc:0.630]
Epoch [10/120    avg_loss:1.584, val_acc:0.624]
Epoch [11/120    avg_loss:1.452, val_acc:0.653]
Epoch [12/120    avg_loss:1.363, val_acc:0.653]
Epoch [13/120    avg_loss:1.233, val_acc:0.654]
Epoch [14/120    avg_loss:1.157, val_acc:0.708]
Epoch [15/120    avg_loss:1.064, val_acc:0.722]
Epoch [16/120    avg_loss:0.936, val_acc:0.735]
Epoch [17/120    avg_loss:0.878, val_acc:0.739]
Epoch [18/120    avg_loss:0.824, val_acc:0.753]
Epoch [19/120    avg_loss:0.739, val_acc:0.782]
Epoch [20/120    avg_loss:0.706, val_acc:0.795]
Epoch [21/120    avg_loss:0.625, val_acc:0.815]
Epoch [22/120    avg_loss:0.610, val_acc:0.820]
Epoch [23/120    avg_loss:0.540, val_acc:0.803]
Epoch [24/120    avg_loss:0.469, val_acc:0.874]
Epoch [25/120    avg_loss:0.490, val_acc:0.850]
Epoch [26/120    avg_loss:0.518, val_acc:0.833]
Epoch [27/120    avg_loss:0.446, val_acc:0.868]
Epoch [28/120    avg_loss:0.410, val_acc:0.871]
Epoch [29/120    avg_loss:0.407, val_acc:0.882]
Epoch [30/120    avg_loss:0.365, val_acc:0.847]
Epoch [31/120    avg_loss:0.309, val_acc:0.875]
Epoch [32/120    avg_loss:0.318, val_acc:0.884]
Epoch [33/120    avg_loss:0.277, val_acc:0.892]
Epoch [34/120    avg_loss:0.264, val_acc:0.884]
Epoch [35/120    avg_loss:0.245, val_acc:0.898]
Epoch [36/120    avg_loss:0.236, val_acc:0.908]
Epoch [37/120    avg_loss:0.192, val_acc:0.942]
Epoch [38/120    avg_loss:0.169, val_acc:0.933]
Epoch [39/120    avg_loss:0.185, val_acc:0.916]
Epoch [40/120    avg_loss:0.211, val_acc:0.920]
Epoch [41/120    avg_loss:0.184, val_acc:0.920]
Epoch [42/120    avg_loss:0.169, val_acc:0.918]
Epoch [43/120    avg_loss:0.142, val_acc:0.941]
Epoch [44/120    avg_loss:0.137, val_acc:0.914]
Epoch [45/120    avg_loss:0.145, val_acc:0.915]
Epoch [46/120    avg_loss:0.152, val_acc:0.920]
Epoch [47/120    avg_loss:0.203, val_acc:0.922]
Epoch [48/120    avg_loss:0.282, val_acc:0.896]
Epoch [49/120    avg_loss:0.197, val_acc:0.922]
Epoch [50/120    avg_loss:0.167, val_acc:0.947]
Epoch [51/120    avg_loss:0.134, val_acc:0.896]
Epoch [52/120    avg_loss:0.130, val_acc:0.925]
Epoch [53/120    avg_loss:0.117, val_acc:0.936]
Epoch [54/120    avg_loss:0.103, val_acc:0.954]
Epoch [55/120    avg_loss:0.083, val_acc:0.955]
Epoch [56/120    avg_loss:0.101, val_acc:0.936]
Epoch [57/120    avg_loss:0.100, val_acc:0.958]
Epoch [58/120    avg_loss:0.079, val_acc:0.951]
Epoch [59/120    avg_loss:0.078, val_acc:0.949]
Epoch [60/120    avg_loss:0.088, val_acc:0.954]
Epoch [61/120    avg_loss:0.077, val_acc:0.953]
Epoch [62/120    avg_loss:0.099, val_acc:0.946]
Epoch [63/120    avg_loss:0.110, val_acc:0.938]
Epoch [64/120    avg_loss:0.095, val_acc:0.942]
Epoch [65/120    avg_loss:0.079, val_acc:0.945]
Epoch [66/120    avg_loss:0.156, val_acc:0.937]
Epoch [67/120    avg_loss:0.093, val_acc:0.954]
Epoch [68/120    avg_loss:0.073, val_acc:0.953]
Epoch [69/120    avg_loss:0.052, val_acc:0.952]
Epoch [70/120    avg_loss:0.206, val_acc:0.904]
Epoch [71/120    avg_loss:0.193, val_acc:0.934]
Epoch [72/120    avg_loss:0.118, val_acc:0.941]
Epoch [73/120    avg_loss:0.084, val_acc:0.953]
Epoch [74/120    avg_loss:0.071, val_acc:0.960]
Epoch [75/120    avg_loss:0.080, val_acc:0.963]
Epoch [76/120    avg_loss:0.066, val_acc:0.961]
Epoch [77/120    avg_loss:0.059, val_acc:0.962]
Epoch [78/120    avg_loss:0.053, val_acc:0.963]
Epoch [79/120    avg_loss:0.061, val_acc:0.964]
Epoch [80/120    avg_loss:0.059, val_acc:0.963]
Epoch [81/120    avg_loss:0.056, val_acc:0.963]
Epoch [82/120    avg_loss:0.059, val_acc:0.964]
Epoch [83/120    avg_loss:0.055, val_acc:0.962]
Epoch [84/120    avg_loss:0.054, val_acc:0.965]
Epoch [85/120    avg_loss:0.049, val_acc:0.963]
Epoch [86/120    avg_loss:0.043, val_acc:0.965]
Epoch [87/120    avg_loss:0.043, val_acc:0.964]
Epoch [88/120    avg_loss:0.055, val_acc:0.965]
Epoch [89/120    avg_loss:0.037, val_acc:0.963]
Epoch [90/120    avg_loss:0.048, val_acc:0.962]
Epoch [91/120    avg_loss:0.050, val_acc:0.964]
Epoch [92/120    avg_loss:0.048, val_acc:0.964]
Epoch [93/120    avg_loss:0.044, val_acc:0.963]
Epoch [94/120    avg_loss:0.039, val_acc:0.963]
Epoch [95/120    avg_loss:0.038, val_acc:0.964]
Epoch [96/120    avg_loss:0.042, val_acc:0.960]
Epoch [97/120    avg_loss:0.048, val_acc:0.963]
Epoch [98/120    avg_loss:0.046, val_acc:0.964]
Epoch [99/120    avg_loss:0.043, val_acc:0.961]
Epoch [100/120    avg_loss:0.041, val_acc:0.959]
Epoch [101/120    avg_loss:0.034, val_acc:0.960]
Epoch [102/120    avg_loss:0.045, val_acc:0.963]
Epoch [103/120    avg_loss:0.035, val_acc:0.963]
Epoch [104/120    avg_loss:0.055, val_acc:0.963]
Epoch [105/120    avg_loss:0.035, val_acc:0.963]
Epoch [106/120    avg_loss:0.034, val_acc:0.963]
Epoch [107/120    avg_loss:0.038, val_acc:0.963]
Epoch [108/120    avg_loss:0.037, val_acc:0.962]
Epoch [109/120    avg_loss:0.044, val_acc:0.962]
Epoch [110/120    avg_loss:0.040, val_acc:0.962]
Epoch [111/120    avg_loss:0.036, val_acc:0.962]
Epoch [112/120    avg_loss:0.036, val_acc:0.962]
Epoch [113/120    avg_loss:0.038, val_acc:0.962]
Epoch [114/120    avg_loss:0.043, val_acc:0.962]
Epoch [115/120    avg_loss:0.041, val_acc:0.962]
Epoch [116/120    avg_loss:0.036, val_acc:0.962]
Epoch [117/120    avg_loss:0.037, val_acc:0.962]
Epoch [118/120    avg_loss:0.044, val_acc:0.963]
Epoch [119/120    avg_loss:0.036, val_acc:0.962]
Epoch [120/120    avg_loss:0.039, val_acc:0.962]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1212    0    7    0    3    0    0    1    8   54    0    0
     0    0    0]
 [   0    0    1  680    1   16    0    0    0   11    7    0   31    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    1    2    0    2    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  422    0    0    0    0    0
     0    0    8]
 [   0    0    0    1    0    0    0    0    0   15    0    0    2    0
     0    0    0]
 [   0    0   21    1    0    9    5    0    0    0  832    1    2    0
     0    4    0]
 [   0    0   15    0    0    0    2    0    0    0   12 2171    5    1
     4    0    0]
 [   0    0    0    5    5   13    0    0    0    0    7    0  496    0
     2    0    6]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    1    0    0    0
  1126   10    0]
 [   0    0    0    0    0    0   29    0    0    1    0    0    0    0
    10  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.23848238482385

F1 scores:
[       nan 0.93506494 0.95659037 0.94839609 0.97038724 0.9432703
 0.96587537 0.96153846 0.989449   0.625      0.95248998 0.97858914
 0.9271028  0.99459459 0.98254799 0.91916168 0.92307692]

Kappa:
0.9571222884936654
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0e244fc860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55854==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.759, val_acc:0.255]
Epoch [2/120    avg_loss:2.540, val_acc:0.273]
Epoch [3/120    avg_loss:2.352, val_acc:0.385]
Epoch [4/120    avg_loss:2.231, val_acc:0.437]
Epoch [5/120    avg_loss:2.124, val_acc:0.521]
Epoch [6/120    avg_loss:1.977, val_acc:0.547]
Epoch [7/120    avg_loss:1.883, val_acc:0.552]
Epoch [8/120    avg_loss:1.812, val_acc:0.573]
Epoch [9/120    avg_loss:1.708, val_acc:0.566]
Epoch [10/120    avg_loss:1.582, val_acc:0.629]
Epoch [11/120    avg_loss:1.499, val_acc:0.633]
Epoch [12/120    avg_loss:1.403, val_acc:0.637]
Epoch [13/120    avg_loss:1.311, val_acc:0.667]
Epoch [14/120    avg_loss:1.257, val_acc:0.642]
Epoch [15/120    avg_loss:1.107, val_acc:0.692]
Epoch [16/120    avg_loss:0.996, val_acc:0.717]
Epoch [17/120    avg_loss:0.886, val_acc:0.745]
Epoch [18/120    avg_loss:0.846, val_acc:0.775]
Epoch [19/120    avg_loss:0.774, val_acc:0.789]
Epoch [20/120    avg_loss:0.692, val_acc:0.804]
Epoch [21/120    avg_loss:0.625, val_acc:0.818]
Epoch [22/120    avg_loss:0.578, val_acc:0.839]
Epoch [23/120    avg_loss:0.530, val_acc:0.864]
Epoch [24/120    avg_loss:0.448, val_acc:0.863]
Epoch [25/120    avg_loss:0.429, val_acc:0.853]
Epoch [26/120    avg_loss:0.452, val_acc:0.862]
Epoch [27/120    avg_loss:0.406, val_acc:0.875]
Epoch [28/120    avg_loss:0.377, val_acc:0.868]
Epoch [29/120    avg_loss:0.299, val_acc:0.900]
Epoch [30/120    avg_loss:0.316, val_acc:0.895]
Epoch [31/120    avg_loss:0.274, val_acc:0.859]
Epoch [32/120    avg_loss:0.367, val_acc:0.854]
Epoch [33/120    avg_loss:0.311, val_acc:0.909]
Epoch [34/120    avg_loss:0.272, val_acc:0.899]
Epoch [35/120    avg_loss:0.260, val_acc:0.910]
Epoch [36/120    avg_loss:0.201, val_acc:0.892]
Epoch [37/120    avg_loss:0.209, val_acc:0.891]
Epoch [38/120    avg_loss:0.201, val_acc:0.917]
Epoch [39/120    avg_loss:0.170, val_acc:0.924]
Epoch [40/120    avg_loss:0.171, val_acc:0.922]
Epoch [41/120    avg_loss:0.138, val_acc:0.938]
Epoch [42/120    avg_loss:0.132, val_acc:0.934]
Epoch [43/120    avg_loss:0.151, val_acc:0.925]
Epoch [44/120    avg_loss:0.171, val_acc:0.910]
Epoch [45/120    avg_loss:0.158, val_acc:0.937]
Epoch [46/120    avg_loss:0.158, val_acc:0.910]
Epoch [47/120    avg_loss:0.121, val_acc:0.934]
Epoch [48/120    avg_loss:0.122, val_acc:0.928]
Epoch [49/120    avg_loss:0.107, val_acc:0.948]
Epoch [50/120    avg_loss:0.104, val_acc:0.938]
Epoch [51/120    avg_loss:0.101, val_acc:0.932]
Epoch [52/120    avg_loss:0.082, val_acc:0.948]
Epoch [53/120    avg_loss:0.069, val_acc:0.957]
Epoch [54/120    avg_loss:0.066, val_acc:0.958]
Epoch [55/120    avg_loss:0.066, val_acc:0.953]
Epoch [56/120    avg_loss:0.075, val_acc:0.952]
Epoch [57/120    avg_loss:0.063, val_acc:0.959]
Epoch [58/120    avg_loss:0.060, val_acc:0.958]
Epoch [59/120    avg_loss:0.055, val_acc:0.957]
Epoch [60/120    avg_loss:0.065, val_acc:0.967]
Epoch [61/120    avg_loss:0.071, val_acc:0.954]
Epoch [62/120    avg_loss:0.056, val_acc:0.959]
Epoch [63/120    avg_loss:0.051, val_acc:0.961]
Epoch [64/120    avg_loss:0.059, val_acc:0.960]
Epoch [65/120    avg_loss:0.078, val_acc:0.961]
Epoch [66/120    avg_loss:0.049, val_acc:0.949]
Epoch [67/120    avg_loss:0.074, val_acc:0.961]
Epoch [68/120    avg_loss:0.068, val_acc:0.951]
Epoch [69/120    avg_loss:0.062, val_acc:0.951]
Epoch [70/120    avg_loss:0.071, val_acc:0.945]
Epoch [71/120    avg_loss:0.057, val_acc:0.961]
Epoch [72/120    avg_loss:0.043, val_acc:0.964]
Epoch [73/120    avg_loss:0.041, val_acc:0.964]
Epoch [74/120    avg_loss:0.036, val_acc:0.967]
Epoch [75/120    avg_loss:0.029, val_acc:0.968]
Epoch [76/120    avg_loss:0.027, val_acc:0.970]
Epoch [77/120    avg_loss:0.026, val_acc:0.967]
Epoch [78/120    avg_loss:0.026, val_acc:0.967]
Epoch [79/120    avg_loss:0.026, val_acc:0.970]
Epoch [80/120    avg_loss:0.026, val_acc:0.970]
Epoch [81/120    avg_loss:0.025, val_acc:0.970]
Epoch [82/120    avg_loss:0.024, val_acc:0.968]
Epoch [83/120    avg_loss:0.026, val_acc:0.968]
Epoch [84/120    avg_loss:0.027, val_acc:0.970]
Epoch [85/120    avg_loss:0.026, val_acc:0.971]
Epoch [86/120    avg_loss:0.024, val_acc:0.970]
Epoch [87/120    avg_loss:0.026, val_acc:0.971]
Epoch [88/120    avg_loss:0.025, val_acc:0.968]
Epoch [89/120    avg_loss:0.022, val_acc:0.968]
Epoch [90/120    avg_loss:0.023, val_acc:0.968]
Epoch [91/120    avg_loss:0.022, val_acc:0.968]
Epoch [92/120    avg_loss:0.024, val_acc:0.970]
Epoch [93/120    avg_loss:0.019, val_acc:0.970]
Epoch [94/120    avg_loss:0.022, val_acc:0.968]
Epoch [95/120    avg_loss:0.021, val_acc:0.968]
Epoch [96/120    avg_loss:0.023, val_acc:0.968]
Epoch [97/120    avg_loss:0.021, val_acc:0.971]
Epoch [98/120    avg_loss:0.022, val_acc:0.968]
Epoch [99/120    avg_loss:0.020, val_acc:0.968]
Epoch [100/120    avg_loss:0.020, val_acc:0.968]
Epoch [101/120    avg_loss:0.024, val_acc:0.971]
Epoch [102/120    avg_loss:0.022, val_acc:0.970]
Epoch [103/120    avg_loss:0.024, val_acc:0.971]
Epoch [104/120    avg_loss:0.018, val_acc:0.972]
Epoch [105/120    avg_loss:0.021, val_acc:0.968]
Epoch [106/120    avg_loss:0.021, val_acc:0.968]
Epoch [107/120    avg_loss:0.020, val_acc:0.971]
Epoch [108/120    avg_loss:0.028, val_acc:0.972]
Epoch [109/120    avg_loss:0.021, val_acc:0.971]
Epoch [110/120    avg_loss:0.020, val_acc:0.970]
Epoch [111/120    avg_loss:0.017, val_acc:0.970]
Epoch [112/120    avg_loss:0.021, val_acc:0.972]
Epoch [113/120    avg_loss:0.024, val_acc:0.970]
Epoch [114/120    avg_loss:0.020, val_acc:0.970]
Epoch [115/120    avg_loss:0.020, val_acc:0.973]
Epoch [116/120    avg_loss:0.020, val_acc:0.971]
Epoch [117/120    avg_loss:0.021, val_acc:0.975]
Epoch [118/120    avg_loss:0.017, val_acc:0.973]
Epoch [119/120    avg_loss:0.023, val_acc:0.974]
Epoch [120/120    avg_loss:0.018, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1231    1    0    1    0    0    0    3    7   39    3    0
     0    0    0]
 [   0    0    1  699    0    9    0    0    0    9    1    2   24    0
     2    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    3    0    1    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   10    0    0    7    3    0    0    0  843   10    0    0
     1    1    0]
 [   0    0    5    0    0    0    2    1    0    0   22 2167   11    2
     0    0    0]
 [   0    0    0    0    0   15    0    0    0    0    3    4  509    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1136    2    0]
 [   0    0    0    0    0    0   38    0    0    4    0    0    0    0
    10  295    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.00813008130082

F1 scores:
[       nan 0.96202532 0.97235387 0.96613683 1.         0.95173962
 0.96678967 0.92592593 0.99883856 0.65384615 0.96123147 0.977447
 0.93998153 0.99191375 0.98954704 0.91472868 0.97647059]

Kappa:
0.965890363985626
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f75d58e0898>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.748, val_acc:0.252]
Epoch [2/120    avg_loss:2.524, val_acc:0.514]
Epoch [3/120    avg_loss:2.349, val_acc:0.509]
Epoch [4/120    avg_loss:2.233, val_acc:0.548]
Epoch [5/120    avg_loss:2.143, val_acc:0.568]
Epoch [6/120    avg_loss:2.054, val_acc:0.563]
Epoch [7/120    avg_loss:1.915, val_acc:0.593]
Epoch [8/120    avg_loss:1.829, val_acc:0.630]
Epoch [9/120    avg_loss:1.772, val_acc:0.633]
Epoch [10/120    avg_loss:1.650, val_acc:0.640]
Epoch [11/120    avg_loss:1.599, val_acc:0.655]
Epoch [12/120    avg_loss:1.517, val_acc:0.647]
Epoch [13/120    avg_loss:1.452, val_acc:0.674]
Epoch [14/120    avg_loss:1.306, val_acc:0.689]
Epoch [15/120    avg_loss:1.179, val_acc:0.707]
Epoch [16/120    avg_loss:1.066, val_acc:0.743]
Epoch [17/120    avg_loss:1.011, val_acc:0.732]
Epoch [18/120    avg_loss:0.961, val_acc:0.739]
Epoch [19/120    avg_loss:0.856, val_acc:0.773]
Epoch [20/120    avg_loss:0.775, val_acc:0.783]
Epoch [21/120    avg_loss:0.751, val_acc:0.791]
Epoch [22/120    avg_loss:0.752, val_acc:0.779]
Epoch [23/120    avg_loss:0.718, val_acc:0.810]
Epoch [24/120    avg_loss:0.601, val_acc:0.817]
Epoch [25/120    avg_loss:0.597, val_acc:0.853]
Epoch [26/120    avg_loss:0.499, val_acc:0.813]
Epoch [27/120    avg_loss:0.499, val_acc:0.825]
Epoch [28/120    avg_loss:0.499, val_acc:0.816]
Epoch [29/120    avg_loss:0.487, val_acc:0.828]
Epoch [30/120    avg_loss:0.446, val_acc:0.836]
Epoch [31/120    avg_loss:0.402, val_acc:0.870]
Epoch [32/120    avg_loss:0.345, val_acc:0.879]
Epoch [33/120    avg_loss:0.347, val_acc:0.886]
Epoch [34/120    avg_loss:0.319, val_acc:0.893]
Epoch [35/120    avg_loss:0.291, val_acc:0.896]
Epoch [36/120    avg_loss:0.287, val_acc:0.901]
Epoch [37/120    avg_loss:0.244, val_acc:0.918]
Epoch [38/120    avg_loss:0.205, val_acc:0.902]
Epoch [39/120    avg_loss:0.255, val_acc:0.898]
Epoch [40/120    avg_loss:0.212, val_acc:0.910]
Epoch [41/120    avg_loss:0.229, val_acc:0.920]
Epoch [42/120    avg_loss:0.182, val_acc:0.911]
Epoch [43/120    avg_loss:0.203, val_acc:0.911]
Epoch [44/120    avg_loss:0.218, val_acc:0.916]
Epoch [45/120    avg_loss:0.199, val_acc:0.904]
Epoch [46/120    avg_loss:0.181, val_acc:0.926]
Epoch [47/120    avg_loss:0.174, val_acc:0.936]
Epoch [48/120    avg_loss:0.159, val_acc:0.933]
Epoch [49/120    avg_loss:0.138, val_acc:0.926]
Epoch [50/120    avg_loss:0.156, val_acc:0.922]
Epoch [51/120    avg_loss:0.156, val_acc:0.937]
Epoch [52/120    avg_loss:0.133, val_acc:0.923]
Epoch [53/120    avg_loss:0.116, val_acc:0.930]
Epoch [54/120    avg_loss:0.107, val_acc:0.936]
Epoch [55/120    avg_loss:0.107, val_acc:0.937]
Epoch [56/120    avg_loss:0.101, val_acc:0.937]
Epoch [57/120    avg_loss:0.088, val_acc:0.946]
Epoch [58/120    avg_loss:0.068, val_acc:0.942]
Epoch [59/120    avg_loss:0.083, val_acc:0.925]
Epoch [60/120    avg_loss:0.118, val_acc:0.921]
Epoch [61/120    avg_loss:0.078, val_acc:0.940]
Epoch [62/120    avg_loss:0.077, val_acc:0.939]
Epoch [63/120    avg_loss:0.083, val_acc:0.937]
Epoch [64/120    avg_loss:0.084, val_acc:0.947]
Epoch [65/120    avg_loss:0.096, val_acc:0.945]
Epoch [66/120    avg_loss:0.086, val_acc:0.954]
Epoch [67/120    avg_loss:0.082, val_acc:0.949]
Epoch [68/120    avg_loss:0.090, val_acc:0.939]
Epoch [69/120    avg_loss:0.117, val_acc:0.930]
Epoch [70/120    avg_loss:0.114, val_acc:0.932]
Epoch [71/120    avg_loss:0.096, val_acc:0.948]
Epoch [72/120    avg_loss:0.106, val_acc:0.948]
Epoch [73/120    avg_loss:0.085, val_acc:0.952]
Epoch [74/120    avg_loss:0.066, val_acc:0.951]
Epoch [75/120    avg_loss:0.115, val_acc:0.863]
Epoch [76/120    avg_loss:0.228, val_acc:0.907]
Epoch [77/120    avg_loss:0.145, val_acc:0.926]
Epoch [78/120    avg_loss:0.111, val_acc:0.929]
Epoch [79/120    avg_loss:0.116, val_acc:0.933]
Epoch [80/120    avg_loss:0.077, val_acc:0.937]
Epoch [81/120    avg_loss:0.067, val_acc:0.942]
Epoch [82/120    avg_loss:0.059, val_acc:0.948]
Epoch [83/120    avg_loss:0.054, val_acc:0.946]
Epoch [84/120    avg_loss:0.055, val_acc:0.948]
Epoch [85/120    avg_loss:0.052, val_acc:0.947]
Epoch [86/120    avg_loss:0.059, val_acc:0.948]
Epoch [87/120    avg_loss:0.057, val_acc:0.949]
Epoch [88/120    avg_loss:0.049, val_acc:0.949]
Epoch [89/120    avg_loss:0.049, val_acc:0.955]
Epoch [90/120    avg_loss:0.050, val_acc:0.953]
Epoch [91/120    avg_loss:0.049, val_acc:0.945]
Epoch [92/120    avg_loss:0.056, val_acc:0.947]
Epoch [93/120    avg_loss:0.061, val_acc:0.954]
Epoch [94/120    avg_loss:0.048, val_acc:0.953]
Epoch [95/120    avg_loss:0.042, val_acc:0.951]
Epoch [96/120    avg_loss:0.044, val_acc:0.948]
Epoch [97/120    avg_loss:0.043, val_acc:0.952]
Epoch [98/120    avg_loss:0.047, val_acc:0.951]
Epoch [99/120    avg_loss:0.044, val_acc:0.959]
Epoch [100/120    avg_loss:0.041, val_acc:0.955]
Epoch [101/120    avg_loss:0.044, val_acc:0.951]
Epoch [102/120    avg_loss:0.041, val_acc:0.953]
Epoch [103/120    avg_loss:0.044, val_acc:0.953]
Epoch [104/120    avg_loss:0.040, val_acc:0.955]
Epoch [105/120    avg_loss:0.045, val_acc:0.958]
Epoch [106/120    avg_loss:0.038, val_acc:0.955]
Epoch [107/120    avg_loss:0.038, val_acc:0.958]
Epoch [108/120    avg_loss:0.048, val_acc:0.960]
Epoch [109/120    avg_loss:0.041, val_acc:0.959]
Epoch [110/120    avg_loss:0.037, val_acc:0.958]
Epoch [111/120    avg_loss:0.042, val_acc:0.961]
Epoch [112/120    avg_loss:0.044, val_acc:0.957]
Epoch [113/120    avg_loss:0.038, val_acc:0.953]
Epoch [114/120    avg_loss:0.037, val_acc:0.959]
Epoch [115/120    avg_loss:0.036, val_acc:0.953]
Epoch [116/120    avg_loss:0.036, val_acc:0.961]
Epoch [117/120    avg_loss:0.036, val_acc:0.958]
Epoch [118/120    avg_loss:0.034, val_acc:0.959]
Epoch [119/120    avg_loss:0.036, val_acc:0.963]
Epoch [120/120    avg_loss:0.031, val_acc:0.960]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1209    2    4    0    2    0    0    0   10   57    1    0
     0    0    0]
 [   0    0    0  705    0   30    0    0    0    6    0    0    5    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    6    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    8   23    0    9    1    0    0    0  826    2    0    0
     1    5    0]
 [   0    0   10    0    0    3    8    0    0    0   27 2153    4    4
     1    0    0]
 [   0    0    0    2    2   10    0    0    0    0    5   11  499    0
     0    0    5]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    8    0    0    0    0    3    1    0    0
  1127    0    0]
 [   0    0    0    0    0    0   38    0    0    2    0    0    0    0
    54  253    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.85907859078591

F1 scores:
[       nan 0.96202532 0.96257962 0.95334686 0.98611111 0.92640693
 0.96252755 1.         0.997669   0.69387755 0.94453974 0.97069432
 0.95319962 0.98659517 0.96987952 0.83636364 0.96511628]

Kappa:
0.952780350880202
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f90f7036828>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.738, val_acc:0.109]
Epoch [2/120    avg_loss:2.560, val_acc:0.351]
Epoch [3/120    avg_loss:2.339, val_acc:0.401]
Epoch [4/120    avg_loss:2.227, val_acc:0.508]
Epoch [5/120    avg_loss:2.108, val_acc:0.493]
Epoch [6/120    avg_loss:1.972, val_acc:0.559]
Epoch [7/120    avg_loss:1.852, val_acc:0.569]
Epoch [8/120    avg_loss:1.752, val_acc:0.584]
Epoch [9/120    avg_loss:1.654, val_acc:0.575]
Epoch [10/120    avg_loss:1.538, val_acc:0.615]
Epoch [11/120    avg_loss:1.421, val_acc:0.630]
Epoch [12/120    avg_loss:1.321, val_acc:0.674]
Epoch [13/120    avg_loss:1.255, val_acc:0.677]
Epoch [14/120    avg_loss:1.153, val_acc:0.692]
Epoch [15/120    avg_loss:1.047, val_acc:0.740]
Epoch [16/120    avg_loss:1.094, val_acc:0.722]
Epoch [17/120    avg_loss:0.975, val_acc:0.744]
Epoch [18/120    avg_loss:0.865, val_acc:0.725]
Epoch [19/120    avg_loss:0.873, val_acc:0.741]
Epoch [20/120    avg_loss:0.783, val_acc:0.777]
Epoch [21/120    avg_loss:0.680, val_acc:0.791]
Epoch [22/120    avg_loss:0.677, val_acc:0.757]
Epoch [23/120    avg_loss:0.712, val_acc:0.800]
Epoch [24/120    avg_loss:0.597, val_acc:0.824]
Epoch [25/120    avg_loss:0.538, val_acc:0.823]
Epoch [26/120    avg_loss:0.532, val_acc:0.812]
Epoch [27/120    avg_loss:0.523, val_acc:0.839]
Epoch [28/120    avg_loss:0.452, val_acc:0.881]
Epoch [29/120    avg_loss:0.416, val_acc:0.898]
Epoch [30/120    avg_loss:0.385, val_acc:0.895]
Epoch [31/120    avg_loss:0.346, val_acc:0.893]
Epoch [32/120    avg_loss:0.291, val_acc:0.893]
Epoch [33/120    avg_loss:0.279, val_acc:0.894]
Epoch [34/120    avg_loss:0.291, val_acc:0.919]
Epoch [35/120    avg_loss:0.275, val_acc:0.870]
Epoch [36/120    avg_loss:0.278, val_acc:0.900]
Epoch [37/120    avg_loss:0.289, val_acc:0.900]
Epoch [38/120    avg_loss:0.258, val_acc:0.893]
Epoch [39/120    avg_loss:0.237, val_acc:0.923]
Epoch [40/120    avg_loss:0.211, val_acc:0.907]
Epoch [41/120    avg_loss:0.172, val_acc:0.925]
Epoch [42/120    avg_loss:0.165, val_acc:0.932]
Epoch [43/120    avg_loss:0.230, val_acc:0.926]
Epoch [44/120    avg_loss:0.202, val_acc:0.923]
Epoch [45/120    avg_loss:0.186, val_acc:0.922]
Epoch [46/120    avg_loss:0.149, val_acc:0.919]
Epoch [47/120    avg_loss:0.147, val_acc:0.936]
Epoch [48/120    avg_loss:0.140, val_acc:0.944]
Epoch [49/120    avg_loss:0.135, val_acc:0.931]
Epoch [50/120    avg_loss:0.136, val_acc:0.949]
Epoch [51/120    avg_loss:0.128, val_acc:0.941]
Epoch [52/120    avg_loss:0.133, val_acc:0.932]
Epoch [53/120    avg_loss:0.124, val_acc:0.943]
Epoch [54/120    avg_loss:0.125, val_acc:0.948]
Epoch [55/120    avg_loss:0.156, val_acc:0.924]
Epoch [56/120    avg_loss:0.197, val_acc:0.924]
Epoch [57/120    avg_loss:0.157, val_acc:0.920]
Epoch [58/120    avg_loss:0.133, val_acc:0.940]
Epoch [59/120    avg_loss:0.106, val_acc:0.938]
Epoch [60/120    avg_loss:0.089, val_acc:0.950]
Epoch [61/120    avg_loss:0.090, val_acc:0.938]
Epoch [62/120    avg_loss:0.111, val_acc:0.945]
Epoch [63/120    avg_loss:0.103, val_acc:0.949]
Epoch [64/120    avg_loss:0.092, val_acc:0.942]
Epoch [65/120    avg_loss:0.090, val_acc:0.953]
Epoch [66/120    avg_loss:0.100, val_acc:0.958]
Epoch [67/120    avg_loss:0.077, val_acc:0.951]
Epoch [68/120    avg_loss:0.074, val_acc:0.945]
Epoch [69/120    avg_loss:0.077, val_acc:0.943]
Epoch [70/120    avg_loss:0.092, val_acc:0.945]
Epoch [71/120    avg_loss:0.089, val_acc:0.936]
Epoch [72/120    avg_loss:0.115, val_acc:0.938]
Epoch [73/120    avg_loss:0.103, val_acc:0.952]
Epoch [74/120    avg_loss:0.083, val_acc:0.958]
Epoch [75/120    avg_loss:0.101, val_acc:0.948]
Epoch [76/120    avg_loss:0.080, val_acc:0.955]
Epoch [77/120    avg_loss:0.068, val_acc:0.955]
Epoch [78/120    avg_loss:0.061, val_acc:0.960]
Epoch [79/120    avg_loss:0.073, val_acc:0.960]
Epoch [80/120    avg_loss:0.066, val_acc:0.951]
Epoch [81/120    avg_loss:0.057, val_acc:0.920]
Epoch [82/120    avg_loss:0.063, val_acc:0.953]
Epoch [83/120    avg_loss:0.081, val_acc:0.939]
Epoch [84/120    avg_loss:0.114, val_acc:0.943]
Epoch [85/120    avg_loss:0.106, val_acc:0.956]
Epoch [86/120    avg_loss:0.102, val_acc:0.930]
Epoch [87/120    avg_loss:0.149, val_acc:0.915]
Epoch [88/120    avg_loss:0.114, val_acc:0.933]
Epoch [89/120    avg_loss:0.110, val_acc:0.947]
Epoch [90/120    avg_loss:0.085, val_acc:0.948]
Epoch [91/120    avg_loss:0.074, val_acc:0.955]
Epoch [92/120    avg_loss:0.066, val_acc:0.955]
Epoch [93/120    avg_loss:0.049, val_acc:0.965]
Epoch [94/120    avg_loss:0.048, val_acc:0.972]
Epoch [95/120    avg_loss:0.048, val_acc:0.972]
Epoch [96/120    avg_loss:0.047, val_acc:0.973]
Epoch [97/120    avg_loss:0.039, val_acc:0.972]
Epoch [98/120    avg_loss:0.038, val_acc:0.969]
Epoch [99/120    avg_loss:0.043, val_acc:0.968]
Epoch [100/120    avg_loss:0.039, val_acc:0.969]
Epoch [101/120    avg_loss:0.038, val_acc:0.969]
Epoch [102/120    avg_loss:0.038, val_acc:0.970]
Epoch [103/120    avg_loss:0.037, val_acc:0.969]
Epoch [104/120    avg_loss:0.028, val_acc:0.973]
Epoch [105/120    avg_loss:0.037, val_acc:0.973]
Epoch [106/120    avg_loss:0.036, val_acc:0.973]
Epoch [107/120    avg_loss:0.038, val_acc:0.972]
Epoch [108/120    avg_loss:0.035, val_acc:0.975]
Epoch [109/120    avg_loss:0.040, val_acc:0.975]
Epoch [110/120    avg_loss:0.040, val_acc:0.975]
Epoch [111/120    avg_loss:0.035, val_acc:0.975]
Epoch [112/120    avg_loss:0.032, val_acc:0.975]
Epoch [113/120    avg_loss:0.030, val_acc:0.973]
Epoch [114/120    avg_loss:0.032, val_acc:0.974]
Epoch [115/120    avg_loss:0.034, val_acc:0.973]
Epoch [116/120    avg_loss:0.032, val_acc:0.972]
Epoch [117/120    avg_loss:0.030, val_acc:0.972]
Epoch [118/120    avg_loss:0.038, val_acc:0.973]
Epoch [119/120    avg_loss:0.038, val_acc:0.975]
Epoch [120/120    avg_loss:0.034, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1224    3    1    0    4    0    0    0    7   41    5    0
     0    0    0]
 [   0    0    1  697    3   11    0    0    0   17    0    1   15    0
     2    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    0    0    7    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   14    0    0    2    0
     0    0    0]
 [   0    0   30   65    0    8    0    0    0    0  759    6    3    0
     0    4    0]
 [   0    0   17    0    0    0    5    0    3    0   25 2154    0    6
     0    0    0]
 [   0    0    2   22    7    0    0    0    0    0   13    5  477    0
     0    1    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    1    0    1    0    0    0
  1132    1    0]
 [   0    0    0    0    0    0   21    0    0    0    0    0    0    0
    54  272    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.16531165311653

F1 scores:
[       nan 0.96202532 0.95662368 0.90696161 0.97247706 0.96027242
 0.97615499 1.         0.99537037 0.5        0.90196078 0.97488119
 0.91907514 0.98404255 0.97084048 0.8704     0.94797688]

Kappa:
0.9448746301963735
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc6a0d8d860>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.783, val_acc:0.172]
Epoch [2/120    avg_loss:2.602, val_acc:0.289]
Epoch [3/120    avg_loss:2.487, val_acc:0.357]
Epoch [4/120    avg_loss:2.331, val_acc:0.398]
Epoch [5/120    avg_loss:2.228, val_acc:0.447]
Epoch [6/120    avg_loss:2.137, val_acc:0.476]
Epoch [7/120    avg_loss:2.019, val_acc:0.525]
Epoch [8/120    avg_loss:1.921, val_acc:0.564]
Epoch [9/120    avg_loss:1.802, val_acc:0.581]
Epoch [10/120    avg_loss:1.691, val_acc:0.602]
Epoch [11/120    avg_loss:1.609, val_acc:0.613]
Epoch [12/120    avg_loss:1.500, val_acc:0.633]
Epoch [13/120    avg_loss:1.403, val_acc:0.636]
Epoch [14/120    avg_loss:1.353, val_acc:0.676]
Epoch [15/120    avg_loss:1.197, val_acc:0.675]
Epoch [16/120    avg_loss:1.190, val_acc:0.651]
Epoch [17/120    avg_loss:1.102, val_acc:0.686]
Epoch [18/120    avg_loss:0.993, val_acc:0.731]
Epoch [19/120    avg_loss:0.902, val_acc:0.726]
Epoch [20/120    avg_loss:0.848, val_acc:0.748]
Epoch [21/120    avg_loss:0.793, val_acc:0.787]
Epoch [22/120    avg_loss:0.700, val_acc:0.748]
Epoch [23/120    avg_loss:0.697, val_acc:0.775]
Epoch [24/120    avg_loss:0.633, val_acc:0.803]
Epoch [25/120    avg_loss:0.532, val_acc:0.812]
Epoch [26/120    avg_loss:0.548, val_acc:0.820]
Epoch [27/120    avg_loss:0.569, val_acc:0.824]
Epoch [28/120    avg_loss:0.493, val_acc:0.826]
Epoch [29/120    avg_loss:0.506, val_acc:0.840]
Epoch [30/120    avg_loss:0.528, val_acc:0.815]
Epoch [31/120    avg_loss:0.472, val_acc:0.798]
Epoch [32/120    avg_loss:0.470, val_acc:0.815]
Epoch [33/120    avg_loss:0.445, val_acc:0.860]
Epoch [34/120    avg_loss:0.369, val_acc:0.860]
Epoch [35/120    avg_loss:0.329, val_acc:0.874]
Epoch [36/120    avg_loss:0.269, val_acc:0.859]
Epoch [37/120    avg_loss:0.268, val_acc:0.866]
Epoch [38/120    avg_loss:0.251, val_acc:0.885]
Epoch [39/120    avg_loss:0.217, val_acc:0.900]
Epoch [40/120    avg_loss:0.219, val_acc:0.903]
Epoch [41/120    avg_loss:0.195, val_acc:0.905]
Epoch [42/120    avg_loss:0.204, val_acc:0.906]
Epoch [43/120    avg_loss:0.174, val_acc:0.911]
Epoch [44/120    avg_loss:0.201, val_acc:0.906]
Epoch [45/120    avg_loss:0.181, val_acc:0.919]
Epoch [46/120    avg_loss:0.171, val_acc:0.893]
Epoch [47/120    avg_loss:0.195, val_acc:0.909]
Epoch [48/120    avg_loss:0.197, val_acc:0.873]
Epoch [49/120    avg_loss:0.171, val_acc:0.909]
Epoch [50/120    avg_loss:0.145, val_acc:0.915]
Epoch [51/120    avg_loss:0.139, val_acc:0.910]
Epoch [52/120    avg_loss:0.128, val_acc:0.915]
Epoch [53/120    avg_loss:0.116, val_acc:0.922]
Epoch [54/120    avg_loss:0.117, val_acc:0.931]
Epoch [55/120    avg_loss:0.117, val_acc:0.933]
Epoch [56/120    avg_loss:0.135, val_acc:0.920]
Epoch [57/120    avg_loss:0.104, val_acc:0.923]
Epoch [58/120    avg_loss:0.109, val_acc:0.942]
Epoch [59/120    avg_loss:0.132, val_acc:0.931]
Epoch [60/120    avg_loss:0.120, val_acc:0.930]
Epoch [61/120    avg_loss:0.085, val_acc:0.930]
Epoch [62/120    avg_loss:0.081, val_acc:0.932]
Epoch [63/120    avg_loss:0.101, val_acc:0.938]
Epoch [64/120    avg_loss:0.115, val_acc:0.934]
Epoch [65/120    avg_loss:0.071, val_acc:0.932]
Epoch [66/120    avg_loss:0.080, val_acc:0.920]
Epoch [67/120    avg_loss:0.089, val_acc:0.930]
Epoch [68/120    avg_loss:0.109, val_acc:0.940]
Epoch [69/120    avg_loss:0.087, val_acc:0.935]
Epoch [70/120    avg_loss:0.104, val_acc:0.938]
Epoch [71/120    avg_loss:0.076, val_acc:0.936]
Epoch [72/120    avg_loss:0.062, val_acc:0.939]
Epoch [73/120    avg_loss:0.060, val_acc:0.945]
Epoch [74/120    avg_loss:0.048, val_acc:0.947]
Epoch [75/120    avg_loss:0.047, val_acc:0.948]
Epoch [76/120    avg_loss:0.053, val_acc:0.950]
Epoch [77/120    avg_loss:0.050, val_acc:0.948]
Epoch [78/120    avg_loss:0.044, val_acc:0.947]
Epoch [79/120    avg_loss:0.042, val_acc:0.948]
Epoch [80/120    avg_loss:0.041, val_acc:0.945]
Epoch [81/120    avg_loss:0.046, val_acc:0.948]
Epoch [82/120    avg_loss:0.042, val_acc:0.950]
Epoch [83/120    avg_loss:0.047, val_acc:0.949]
Epoch [84/120    avg_loss:0.048, val_acc:0.950]
Epoch [85/120    avg_loss:0.043, val_acc:0.951]
Epoch [86/120    avg_loss:0.046, val_acc:0.951]
Epoch [87/120    avg_loss:0.039, val_acc:0.950]
Epoch [88/120    avg_loss:0.039, val_acc:0.955]
Epoch [89/120    avg_loss:0.046, val_acc:0.951]
Epoch [90/120    avg_loss:0.041, val_acc:0.950]
Epoch [91/120    avg_loss:0.040, val_acc:0.953]
Epoch [92/120    avg_loss:0.039, val_acc:0.950]
Epoch [93/120    avg_loss:0.038, val_acc:0.953]
Epoch [94/120    avg_loss:0.040, val_acc:0.950]
Epoch [95/120    avg_loss:0.036, val_acc:0.955]
Epoch [96/120    avg_loss:0.048, val_acc:0.951]
Epoch [97/120    avg_loss:0.036, val_acc:0.952]
Epoch [98/120    avg_loss:0.041, val_acc:0.953]
Epoch [99/120    avg_loss:0.039, val_acc:0.957]
Epoch [100/120    avg_loss:0.039, val_acc:0.955]
Epoch [101/120    avg_loss:0.040, val_acc:0.952]
Epoch [102/120    avg_loss:0.038, val_acc:0.957]
Epoch [103/120    avg_loss:0.038, val_acc:0.955]
Epoch [104/120    avg_loss:0.030, val_acc:0.956]
Epoch [105/120    avg_loss:0.035, val_acc:0.955]
Epoch [106/120    avg_loss:0.032, val_acc:0.958]
Epoch [107/120    avg_loss:0.041, val_acc:0.960]
Epoch [108/120    avg_loss:0.033, val_acc:0.958]
Epoch [109/120    avg_loss:0.039, val_acc:0.958]
Epoch [110/120    avg_loss:0.031, val_acc:0.958]
Epoch [111/120    avg_loss:0.033, val_acc:0.959]
Epoch [112/120    avg_loss:0.033, val_acc:0.959]
Epoch [113/120    avg_loss:0.030, val_acc:0.959]
Epoch [114/120    avg_loss:0.037, val_acc:0.963]
Epoch [115/120    avg_loss:0.042, val_acc:0.960]
Epoch [116/120    avg_loss:0.032, val_acc:0.956]
Epoch [117/120    avg_loss:0.041, val_acc:0.959]
Epoch [118/120    avg_loss:0.037, val_acc:0.957]
Epoch [119/120    avg_loss:0.039, val_acc:0.956]
Epoch [120/120    avg_loss:0.035, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1229    9    0    0    1    0    0    3   12   31    0    0
     0    0    0]
 [   0    0    8  688    2   12    0    0    0   14    0    0   20    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    2    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   14    0    0    2    0
     0    0    0]
 [   0    0   38   46    0    4    1    0    0    0  746   38    0    0
     0    2    0]
 [   0    0   11    0    0    0    6    0    0    0    5 2179    5    4
     0    0    0]
 [   0    0    0    3    6    8    0    0    0    0   11    0  502    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    3    1    0    0
  1133    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    43  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
95.85907859078591

F1 scores:
[       nan 0.94871795 0.95604823 0.92163429 0.98156682 0.96512936
 0.98561696 1.         0.99883856 0.50909091 0.90096618 0.97691101
 0.94183865 0.98143236 0.97714532 0.92615385 0.95857988]

Kappa:
0.9527544075172504
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f112e335898>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.767, val_acc:0.237]
Epoch [2/120    avg_loss:2.499, val_acc:0.323]
Epoch [3/120    avg_loss:2.339, val_acc:0.385]
Epoch [4/120    avg_loss:2.215, val_acc:0.401]
Epoch [5/120    avg_loss:2.132, val_acc:0.425]
Epoch [6/120    avg_loss:2.018, val_acc:0.544]
Epoch [7/120    avg_loss:1.926, val_acc:0.548]
Epoch [8/120    avg_loss:1.830, val_acc:0.599]
Epoch [9/120    avg_loss:1.669, val_acc:0.597]
Epoch [10/120    avg_loss:1.606, val_acc:0.625]
Epoch [11/120    avg_loss:1.541, val_acc:0.633]
Epoch [12/120    avg_loss:1.453, val_acc:0.678]
Epoch [13/120    avg_loss:1.328, val_acc:0.657]
Epoch [14/120    avg_loss:1.237, val_acc:0.670]
Epoch [15/120    avg_loss:1.136, val_acc:0.683]
Epoch [16/120    avg_loss:1.119, val_acc:0.710]
Epoch [17/120    avg_loss:0.998, val_acc:0.743]
Epoch [18/120    avg_loss:0.915, val_acc:0.711]
Epoch [19/120    avg_loss:0.890, val_acc:0.741]
Epoch [20/120    avg_loss:0.863, val_acc:0.777]
Epoch [21/120    avg_loss:0.783, val_acc:0.801]
Epoch [22/120    avg_loss:0.739, val_acc:0.783]
Epoch [23/120    avg_loss:0.719, val_acc:0.801]
Epoch [24/120    avg_loss:0.666, val_acc:0.823]
Epoch [25/120    avg_loss:0.578, val_acc:0.849]
Epoch [26/120    avg_loss:0.506, val_acc:0.825]
Epoch [27/120    avg_loss:0.511, val_acc:0.805]
Epoch [28/120    avg_loss:0.609, val_acc:0.800]
Epoch [29/120    avg_loss:0.523, val_acc:0.843]
Epoch [30/120    avg_loss:0.416, val_acc:0.872]
Epoch [31/120    avg_loss:0.375, val_acc:0.875]
Epoch [32/120    avg_loss:0.380, val_acc:0.869]
Epoch [33/120    avg_loss:0.315, val_acc:0.898]
Epoch [34/120    avg_loss:0.346, val_acc:0.857]
Epoch [35/120    avg_loss:0.346, val_acc:0.878]
Epoch [36/120    avg_loss:0.298, val_acc:0.878]
Epoch [37/120    avg_loss:0.264, val_acc:0.907]
Epoch [38/120    avg_loss:0.271, val_acc:0.880]
Epoch [39/120    avg_loss:0.289, val_acc:0.874]
Epoch [40/120    avg_loss:0.262, val_acc:0.892]
Epoch [41/120    avg_loss:0.213, val_acc:0.906]
Epoch [42/120    avg_loss:0.193, val_acc:0.906]
Epoch [43/120    avg_loss:0.174, val_acc:0.912]
Epoch [44/120    avg_loss:0.172, val_acc:0.918]
Epoch [45/120    avg_loss:0.191, val_acc:0.910]
Epoch [46/120    avg_loss:0.162, val_acc:0.914]
Epoch [47/120    avg_loss:0.155, val_acc:0.931]
Epoch [48/120    avg_loss:0.143, val_acc:0.918]
Epoch [49/120    avg_loss:0.165, val_acc:0.912]
Epoch [50/120    avg_loss:0.149, val_acc:0.915]
Epoch [51/120    avg_loss:0.141, val_acc:0.927]
Epoch [52/120    avg_loss:0.121, val_acc:0.933]
Epoch [53/120    avg_loss:0.111, val_acc:0.931]
Epoch [54/120    avg_loss:0.101, val_acc:0.938]
Epoch [55/120    avg_loss:0.112, val_acc:0.940]
Epoch [56/120    avg_loss:0.110, val_acc:0.941]
Epoch [57/120    avg_loss:0.105, val_acc:0.941]
Epoch [58/120    avg_loss:0.082, val_acc:0.941]
Epoch [59/120    avg_loss:0.073, val_acc:0.942]
Epoch [60/120    avg_loss:0.080, val_acc:0.943]
Epoch [61/120    avg_loss:0.084, val_acc:0.947]
Epoch [62/120    avg_loss:0.084, val_acc:0.948]
Epoch [63/120    avg_loss:0.075, val_acc:0.945]
Epoch [64/120    avg_loss:0.089, val_acc:0.939]
Epoch [65/120    avg_loss:0.120, val_acc:0.933]
Epoch [66/120    avg_loss:0.118, val_acc:0.925]
Epoch [67/120    avg_loss:0.109, val_acc:0.917]
Epoch [68/120    avg_loss:0.091, val_acc:0.942]
Epoch [69/120    avg_loss:0.091, val_acc:0.941]
Epoch [70/120    avg_loss:0.080, val_acc:0.947]
Epoch [71/120    avg_loss:0.088, val_acc:0.942]
Epoch [72/120    avg_loss:0.075, val_acc:0.926]
Epoch [73/120    avg_loss:0.076, val_acc:0.938]
Epoch [74/120    avg_loss:0.071, val_acc:0.944]
Epoch [75/120    avg_loss:0.074, val_acc:0.928]
Epoch [76/120    avg_loss:0.101, val_acc:0.942]
Epoch [77/120    avg_loss:0.066, val_acc:0.951]
Epoch [78/120    avg_loss:0.054, val_acc:0.956]
Epoch [79/120    avg_loss:0.048, val_acc:0.955]
Epoch [80/120    avg_loss:0.042, val_acc:0.953]
Epoch [81/120    avg_loss:0.046, val_acc:0.951]
Epoch [82/120    avg_loss:0.037, val_acc:0.955]
Epoch [83/120    avg_loss:0.035, val_acc:0.955]
Epoch [84/120    avg_loss:0.042, val_acc:0.957]
Epoch [85/120    avg_loss:0.038, val_acc:0.957]
Epoch [86/120    avg_loss:0.037, val_acc:0.958]
Epoch [87/120    avg_loss:0.037, val_acc:0.956]
Epoch [88/120    avg_loss:0.040, val_acc:0.953]
Epoch [89/120    avg_loss:0.034, val_acc:0.958]
Epoch [90/120    avg_loss:0.036, val_acc:0.952]
Epoch [91/120    avg_loss:0.033, val_acc:0.953]
Epoch [92/120    avg_loss:0.040, val_acc:0.955]
Epoch [93/120    avg_loss:0.037, val_acc:0.957]
Epoch [94/120    avg_loss:0.037, val_acc:0.952]
Epoch [95/120    avg_loss:0.035, val_acc:0.955]
Epoch [96/120    avg_loss:0.037, val_acc:0.956]
Epoch [97/120    avg_loss:0.033, val_acc:0.955]
Epoch [98/120    avg_loss:0.034, val_acc:0.961]
Epoch [99/120    avg_loss:0.033, val_acc:0.960]
Epoch [100/120    avg_loss:0.038, val_acc:0.956]
Epoch [101/120    avg_loss:0.034, val_acc:0.958]
Epoch [102/120    avg_loss:0.033, val_acc:0.956]
Epoch [103/120    avg_loss:0.038, val_acc:0.955]
Epoch [104/120    avg_loss:0.029, val_acc:0.964]
Epoch [105/120    avg_loss:0.032, val_acc:0.960]
Epoch [106/120    avg_loss:0.029, val_acc:0.958]
Epoch [107/120    avg_loss:0.031, val_acc:0.958]
Epoch [108/120    avg_loss:0.035, val_acc:0.953]
Epoch [109/120    avg_loss:0.034, val_acc:0.961]
Epoch [110/120    avg_loss:0.033, val_acc:0.953]
Epoch [111/120    avg_loss:0.031, val_acc:0.957]
Epoch [112/120    avg_loss:0.035, val_acc:0.963]
Epoch [113/120    avg_loss:0.035, val_acc:0.953]
Epoch [114/120    avg_loss:0.030, val_acc:0.958]
Epoch [115/120    avg_loss:0.031, val_acc:0.963]
Epoch [116/120    avg_loss:0.028, val_acc:0.961]
Epoch [117/120    avg_loss:0.029, val_acc:0.964]
Epoch [118/120    avg_loss:0.027, val_acc:0.963]
Epoch [119/120    avg_loss:0.029, val_acc:0.960]
Epoch [120/120    avg_loss:0.030, val_acc:0.958]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1221   13    0    0    4    0    0    0   10   36    1    0
     0    0    0]
 [   0    0    0  654    2   23    0    0    0   16    0    0   47    5
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    3    0    6    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0  655    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   13    0    2    1    0
     0    0    0]
 [   0    0   16   64    0    9    0    0    0    0  772    6    0    0
     0    8    0]
 [   0    0   15    0    0    2    9    0    3    0   20 2149    6    3
     3    0    0]
 [   0    0    0   17   11    7    0    0    0    0    7    3  478    0
     0    6    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    4    1    0    1    3    0    0
  1123    2    0]
 [   0    0    0    0    0    0   41    0    0    0    0    0    0    0
    75  231    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.25474254742548

F1 scores:
[       nan 0.94871795 0.96217494 0.87374749 0.97038724 0.93936053
 0.95900439 0.87719298 0.99537037 0.49056604 0.91415038 0.97460317
 0.89513109 0.97883598 0.95982906 0.77777778 0.96511628]

Kappa:
0.9345177579924999
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe957659860>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.786, val_acc:0.309]
Epoch [2/120    avg_loss:2.516, val_acc:0.440]
Epoch [3/120    avg_loss:2.333, val_acc:0.455]
Epoch [4/120    avg_loss:2.236, val_acc:0.482]
Epoch [5/120    avg_loss:2.073, val_acc:0.553]
Epoch [6/120    avg_loss:1.960, val_acc:0.576]
Epoch [7/120    avg_loss:1.836, val_acc:0.595]
Epoch [8/120    avg_loss:1.774, val_acc:0.577]
Epoch [9/120    avg_loss:1.699, val_acc:0.614]
Epoch [10/120    avg_loss:1.590, val_acc:0.610]
Epoch [11/120    avg_loss:1.494, val_acc:0.634]
Epoch [12/120    avg_loss:1.372, val_acc:0.661]
Epoch [13/120    avg_loss:1.313, val_acc:0.660]
Epoch [14/120    avg_loss:1.250, val_acc:0.626]
Epoch [15/120    avg_loss:1.173, val_acc:0.669]
Epoch [16/120    avg_loss:1.063, val_acc:0.657]
Epoch [17/120    avg_loss:0.989, val_acc:0.697]
Epoch [18/120    avg_loss:0.896, val_acc:0.718]
Epoch [19/120    avg_loss:0.828, val_acc:0.732]
Epoch [20/120    avg_loss:0.749, val_acc:0.733]
Epoch [21/120    avg_loss:0.804, val_acc:0.718]
Epoch [22/120    avg_loss:0.675, val_acc:0.759]
Epoch [23/120    avg_loss:0.619, val_acc:0.775]
Epoch [24/120    avg_loss:0.605, val_acc:0.756]
Epoch [25/120    avg_loss:0.542, val_acc:0.770]
Epoch [26/120    avg_loss:0.508, val_acc:0.768]
Epoch [27/120    avg_loss:0.473, val_acc:0.803]
Epoch [28/120    avg_loss:0.463, val_acc:0.751]
Epoch [29/120    avg_loss:0.521, val_acc:0.789]
Epoch [30/120    avg_loss:0.445, val_acc:0.801]
Epoch [31/120    avg_loss:0.425, val_acc:0.822]
Epoch [32/120    avg_loss:0.386, val_acc:0.809]
Epoch [33/120    avg_loss:0.360, val_acc:0.831]
Epoch [34/120    avg_loss:0.325, val_acc:0.866]
Epoch [35/120    avg_loss:0.288, val_acc:0.861]
Epoch [36/120    avg_loss:0.285, val_acc:0.851]
Epoch [37/120    avg_loss:0.251, val_acc:0.857]
Epoch [38/120    avg_loss:0.227, val_acc:0.894]
Epoch [39/120    avg_loss:0.212, val_acc:0.884]
Epoch [40/120    avg_loss:0.244, val_acc:0.897]
Epoch [41/120    avg_loss:0.228, val_acc:0.897]
Epoch [42/120    avg_loss:0.208, val_acc:0.887]
Epoch [43/120    avg_loss:0.214, val_acc:0.883]
Epoch [44/120    avg_loss:0.209, val_acc:0.874]
Epoch [45/120    avg_loss:0.197, val_acc:0.909]
Epoch [46/120    avg_loss:0.180, val_acc:0.908]
Epoch [47/120    avg_loss:0.185, val_acc:0.884]
Epoch [48/120    avg_loss:0.178, val_acc:0.902]
Epoch [49/120    avg_loss:0.214, val_acc:0.880]
Epoch [50/120    avg_loss:0.192, val_acc:0.912]
Epoch [51/120    avg_loss:0.159, val_acc:0.908]
Epoch [52/120    avg_loss:0.129, val_acc:0.918]
Epoch [53/120    avg_loss:0.109, val_acc:0.927]
Epoch [54/120    avg_loss:0.105, val_acc:0.915]
Epoch [55/120    avg_loss:0.127, val_acc:0.909]
Epoch [56/120    avg_loss:0.124, val_acc:0.923]
Epoch [57/120    avg_loss:0.096, val_acc:0.934]
Epoch [58/120    avg_loss:0.087, val_acc:0.932]
Epoch [59/120    avg_loss:0.083, val_acc:0.939]
Epoch [60/120    avg_loss:0.089, val_acc:0.926]
Epoch [61/120    avg_loss:0.076, val_acc:0.939]
Epoch [62/120    avg_loss:0.084, val_acc:0.927]
Epoch [63/120    avg_loss:0.085, val_acc:0.944]
Epoch [64/120    avg_loss:0.075, val_acc:0.926]
Epoch [65/120    avg_loss:0.101, val_acc:0.945]
Epoch [66/120    avg_loss:0.090, val_acc:0.936]
Epoch [67/120    avg_loss:0.066, val_acc:0.951]
Epoch [68/120    avg_loss:0.064, val_acc:0.942]
Epoch [69/120    avg_loss:0.071, val_acc:0.934]
Epoch [70/120    avg_loss:0.092, val_acc:0.936]
Epoch [71/120    avg_loss:0.076, val_acc:0.942]
Epoch [72/120    avg_loss:0.076, val_acc:0.930]
Epoch [73/120    avg_loss:0.116, val_acc:0.898]
Epoch [74/120    avg_loss:0.143, val_acc:0.916]
Epoch [75/120    avg_loss:0.146, val_acc:0.899]
Epoch [76/120    avg_loss:0.126, val_acc:0.931]
Epoch [77/120    avg_loss:0.097, val_acc:0.935]
Epoch [78/120    avg_loss:0.143, val_acc:0.881]
Epoch [79/120    avg_loss:0.190, val_acc:0.915]
Epoch [80/120    avg_loss:0.140, val_acc:0.917]
Epoch [81/120    avg_loss:0.100, val_acc:0.925]
Epoch [82/120    avg_loss:0.077, val_acc:0.932]
Epoch [83/120    avg_loss:0.068, val_acc:0.931]
Epoch [84/120    avg_loss:0.063, val_acc:0.932]
Epoch [85/120    avg_loss:0.068, val_acc:0.935]
Epoch [86/120    avg_loss:0.068, val_acc:0.941]
Epoch [87/120    avg_loss:0.054, val_acc:0.943]
Epoch [88/120    avg_loss:0.061, val_acc:0.947]
Epoch [89/120    avg_loss:0.047, val_acc:0.948]
Epoch [90/120    avg_loss:0.057, val_acc:0.943]
Epoch [91/120    avg_loss:0.049, val_acc:0.947]
Epoch [92/120    avg_loss:0.051, val_acc:0.949]
Epoch [93/120    avg_loss:0.057, val_acc:0.945]
Epoch [94/120    avg_loss:0.048, val_acc:0.947]
Epoch [95/120    avg_loss:0.051, val_acc:0.947]
Epoch [96/120    avg_loss:0.047, val_acc:0.948]
Epoch [97/120    avg_loss:0.050, val_acc:0.948]
Epoch [98/120    avg_loss:0.044, val_acc:0.948]
Epoch [99/120    avg_loss:0.047, val_acc:0.948]
Epoch [100/120    avg_loss:0.052, val_acc:0.947]
Epoch [101/120    avg_loss:0.052, val_acc:0.947]
Epoch [102/120    avg_loss:0.045, val_acc:0.947]
Epoch [103/120    avg_loss:0.044, val_acc:0.947]
Epoch [104/120    avg_loss:0.043, val_acc:0.947]
Epoch [105/120    avg_loss:0.051, val_acc:0.949]
Epoch [106/120    avg_loss:0.050, val_acc:0.948]
Epoch [107/120    avg_loss:0.047, val_acc:0.948]
Epoch [108/120    avg_loss:0.042, val_acc:0.948]
Epoch [109/120    avg_loss:0.047, val_acc:0.948]
Epoch [110/120    avg_loss:0.049, val_acc:0.948]
Epoch [111/120    avg_loss:0.050, val_acc:0.948]
Epoch [112/120    avg_loss:0.048, val_acc:0.948]
Epoch [113/120    avg_loss:0.045, val_acc:0.948]
Epoch [114/120    avg_loss:0.044, val_acc:0.948]
Epoch [115/120    avg_loss:0.049, val_acc:0.948]
Epoch [116/120    avg_loss:0.045, val_acc:0.948]
Epoch [117/120    avg_loss:0.052, val_acc:0.948]
Epoch [118/120    avg_loss:0.043, val_acc:0.948]
Epoch [119/120    avg_loss:0.044, val_acc:0.948]
Epoch [120/120    avg_loss:0.049, val_acc:0.948]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1208    0    0    0    9    0    0    0   14   51    2    0
     0    1    0]
 [   0    0    0  691    1   13    0    0    0   19    1    0   16    6
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    5    0    4    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0   13    0    0    0    0
     0    0    0]
 [   0    0   30   59    0    6    0    0    0    0  737   32    0    0
     3    8    0]
 [   0    0    9    3    0    0   14    0    0    0   18 2148    7    3
     8    0    0]
 [   0    0    0    4    5    1    0    0    0    0   15   25  476    0
     0    1    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   12    0    0    0    0    5    1    0    0
  1121    0    0]
 [   0    0    0    0    0    0   33    0    0    0    0    0    0    0
    62  252    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.22222222222223

F1 scores:
[       nan 0.90243902 0.95418641 0.9158383  0.98611111 0.94938133
 0.9568398  0.90909091 0.9953271  0.48148148 0.88316357 0.96150403
 0.91980676 0.9762533  0.95852929 0.82758621 0.96      ]

Kappa:
0.9340704212657283
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f864c8c2898>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.783, val_acc:0.361]
Epoch [2/120    avg_loss:2.541, val_acc:0.433]
Epoch [3/120    avg_loss:2.346, val_acc:0.512]
Epoch [4/120    avg_loss:2.230, val_acc:0.515]
Epoch [5/120    avg_loss:2.120, val_acc:0.509]
Epoch [6/120    avg_loss:2.036, val_acc:0.567]
Epoch [7/120    avg_loss:1.882, val_acc:0.616]
Epoch [8/120    avg_loss:1.774, val_acc:0.624]
Epoch [9/120    avg_loss:1.646, val_acc:0.638]
Epoch [10/120    avg_loss:1.507, val_acc:0.655]
Epoch [11/120    avg_loss:1.335, val_acc:0.671]
Epoch [12/120    avg_loss:1.264, val_acc:0.686]
Epoch [13/120    avg_loss:1.211, val_acc:0.666]
Epoch [14/120    avg_loss:1.047, val_acc:0.730]
Epoch [15/120    avg_loss:1.016, val_acc:0.732]
Epoch [16/120    avg_loss:0.899, val_acc:0.720]
Epoch [17/120    avg_loss:0.847, val_acc:0.759]
Epoch [18/120    avg_loss:0.835, val_acc:0.797]
Epoch [19/120    avg_loss:0.762, val_acc:0.804]
Epoch [20/120    avg_loss:0.672, val_acc:0.805]
Epoch [21/120    avg_loss:0.588, val_acc:0.825]
Epoch [22/120    avg_loss:0.568, val_acc:0.803]
Epoch [23/120    avg_loss:0.628, val_acc:0.800]
Epoch [24/120    avg_loss:0.603, val_acc:0.829]
Epoch [25/120    avg_loss:0.520, val_acc:0.847]
Epoch [26/120    avg_loss:0.468, val_acc:0.829]
Epoch [27/120    avg_loss:0.437, val_acc:0.853]
Epoch [28/120    avg_loss:0.389, val_acc:0.873]
Epoch [29/120    avg_loss:0.365, val_acc:0.843]
Epoch [30/120    avg_loss:0.361, val_acc:0.895]
Epoch [31/120    avg_loss:0.304, val_acc:0.873]
Epoch [32/120    avg_loss:0.262, val_acc:0.910]
Epoch [33/120    avg_loss:0.247, val_acc:0.888]
Epoch [34/120    avg_loss:0.293, val_acc:0.885]
Epoch [35/120    avg_loss:0.228, val_acc:0.914]
Epoch [36/120    avg_loss:0.252, val_acc:0.901]
Epoch [37/120    avg_loss:0.245, val_acc:0.888]
Epoch [38/120    avg_loss:0.293, val_acc:0.889]
Epoch [39/120    avg_loss:0.260, val_acc:0.890]
Epoch [40/120    avg_loss:0.248, val_acc:0.905]
Epoch [41/120    avg_loss:0.192, val_acc:0.904]
Epoch [42/120    avg_loss:0.179, val_acc:0.934]
Epoch [43/120    avg_loss:0.152, val_acc:0.928]
Epoch [44/120    avg_loss:0.150, val_acc:0.914]
Epoch [45/120    avg_loss:0.180, val_acc:0.913]
Epoch [46/120    avg_loss:0.158, val_acc:0.934]
Epoch [47/120    avg_loss:0.146, val_acc:0.920]
Epoch [48/120    avg_loss:0.166, val_acc:0.928]
Epoch [49/120    avg_loss:0.133, val_acc:0.928]
Epoch [50/120    avg_loss:0.116, val_acc:0.916]
Epoch [51/120    avg_loss:0.136, val_acc:0.937]
Epoch [52/120    avg_loss:0.119, val_acc:0.926]
Epoch [53/120    avg_loss:0.147, val_acc:0.937]
Epoch [54/120    avg_loss:0.144, val_acc:0.934]
Epoch [55/120    avg_loss:0.127, val_acc:0.935]
Epoch [56/120    avg_loss:0.099, val_acc:0.930]
Epoch [57/120    avg_loss:0.114, val_acc:0.940]
Epoch [58/120    avg_loss:0.088, val_acc:0.957]
Epoch [59/120    avg_loss:0.087, val_acc:0.945]
Epoch [60/120    avg_loss:0.088, val_acc:0.945]
Epoch [61/120    avg_loss:0.082, val_acc:0.929]
Epoch [62/120    avg_loss:0.105, val_acc:0.942]
Epoch [63/120    avg_loss:0.096, val_acc:0.936]
Epoch [64/120    avg_loss:0.106, val_acc:0.926]
Epoch [65/120    avg_loss:0.129, val_acc:0.928]
Epoch [66/120    avg_loss:0.127, val_acc:0.939]
Epoch [67/120    avg_loss:0.078, val_acc:0.950]
Epoch [68/120    avg_loss:0.078, val_acc:0.946]
Epoch [69/120    avg_loss:0.084, val_acc:0.946]
Epoch [70/120    avg_loss:0.077, val_acc:0.941]
Epoch [71/120    avg_loss:0.063, val_acc:0.949]
Epoch [72/120    avg_loss:0.053, val_acc:0.954]
Epoch [73/120    avg_loss:0.051, val_acc:0.950]
Epoch [74/120    avg_loss:0.043, val_acc:0.953]
Epoch [75/120    avg_loss:0.045, val_acc:0.958]
Epoch [76/120    avg_loss:0.042, val_acc:0.961]
Epoch [77/120    avg_loss:0.042, val_acc:0.963]
Epoch [78/120    avg_loss:0.037, val_acc:0.962]
Epoch [79/120    avg_loss:0.037, val_acc:0.962]
Epoch [80/120    avg_loss:0.039, val_acc:0.961]
Epoch [81/120    avg_loss:0.037, val_acc:0.957]
Epoch [82/120    avg_loss:0.039, val_acc:0.955]
Epoch [83/120    avg_loss:0.044, val_acc:0.958]
Epoch [84/120    avg_loss:0.036, val_acc:0.959]
Epoch [85/120    avg_loss:0.039, val_acc:0.959]
Epoch [86/120    avg_loss:0.034, val_acc:0.960]
Epoch [87/120    avg_loss:0.041, val_acc:0.960]
Epoch [88/120    avg_loss:0.044, val_acc:0.959]
Epoch [89/120    avg_loss:0.035, val_acc:0.959]
Epoch [90/120    avg_loss:0.037, val_acc:0.958]
Epoch [91/120    avg_loss:0.040, val_acc:0.959]
Epoch [92/120    avg_loss:0.041, val_acc:0.959]
Epoch [93/120    avg_loss:0.035, val_acc:0.959]
Epoch [94/120    avg_loss:0.039, val_acc:0.959]
Epoch [95/120    avg_loss:0.038, val_acc:0.959]
Epoch [96/120    avg_loss:0.034, val_acc:0.959]
Epoch [97/120    avg_loss:0.036, val_acc:0.959]
Epoch [98/120    avg_loss:0.038, val_acc:0.959]
Epoch [99/120    avg_loss:0.042, val_acc:0.959]
Epoch [100/120    avg_loss:0.035, val_acc:0.959]
Epoch [101/120    avg_loss:0.036, val_acc:0.960]
Epoch [102/120    avg_loss:0.033, val_acc:0.960]
Epoch [103/120    avg_loss:0.041, val_acc:0.960]
Epoch [104/120    avg_loss:0.037, val_acc:0.960]
Epoch [105/120    avg_loss:0.037, val_acc:0.960]
Epoch [106/120    avg_loss:0.043, val_acc:0.960]
Epoch [107/120    avg_loss:0.043, val_acc:0.960]
Epoch [108/120    avg_loss:0.038, val_acc:0.960]
Epoch [109/120    avg_loss:0.035, val_acc:0.960]
Epoch [110/120    avg_loss:0.037, val_acc:0.960]
Epoch [111/120    avg_loss:0.033, val_acc:0.960]
Epoch [112/120    avg_loss:0.035, val_acc:0.960]
Epoch [113/120    avg_loss:0.040, val_acc:0.960]
Epoch [114/120    avg_loss:0.034, val_acc:0.960]
Epoch [115/120    avg_loss:0.034, val_acc:0.960]
Epoch [116/120    avg_loss:0.037, val_acc:0.960]
Epoch [117/120    avg_loss:0.039, val_acc:0.960]
Epoch [118/120    avg_loss:0.037, val_acc:0.960]
Epoch [119/120    avg_loss:0.035, val_acc:0.960]
Epoch [120/120    avg_loss:0.035, val_acc:0.960]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    1    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1223    2    2    0    2    0    0    0    8   41    1    0
     1    5    0]
 [   0    0    2  693    3   16    0    0    0    5    0    0   25    2
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    3    0    3    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   31   66    0    7    0    0    0    0  751    9    0    0
     1   10    0]
 [   0    0   16    0    0    1    7    0    5    0   17 2157    2    2
     3    0    0]
 [   0    0    2   37    1   12    0    0    0    0    5    0  471    0
     1    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    3    0    0    0
  1131    3    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    51  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.28455284552845

F1 scores:
[       nan 0.94871795 0.95546875 0.89708738 0.98611111 0.94983278
 0.99014405 0.94339623 0.99192618 0.79069767 0.90373045 0.97557666
 0.90926641 0.98930481 0.97081545 0.89561271 0.95906433]

Kappa:
0.9462369538658905
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbef37a7940>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.743, val_acc:0.351]
Epoch [2/120    avg_loss:2.521, val_acc:0.423]
Epoch [3/120    avg_loss:2.330, val_acc:0.491]
Epoch [4/120    avg_loss:2.214, val_acc:0.492]
Epoch [5/120    avg_loss:2.103, val_acc:0.495]
Epoch [6/120    avg_loss:2.027, val_acc:0.512]
Epoch [7/120    avg_loss:1.959, val_acc:0.523]
Epoch [8/120    avg_loss:1.870, val_acc:0.556]
Epoch [9/120    avg_loss:1.829, val_acc:0.581]
Epoch [10/120    avg_loss:1.736, val_acc:0.601]
Epoch [11/120    avg_loss:1.686, val_acc:0.633]
Epoch [12/120    avg_loss:1.574, val_acc:0.628]
Epoch [13/120    avg_loss:1.504, val_acc:0.668]
Epoch [14/120    avg_loss:1.407, val_acc:0.682]
Epoch [15/120    avg_loss:1.284, val_acc:0.683]
Epoch [16/120    avg_loss:1.191, val_acc:0.703]
Epoch [17/120    avg_loss:1.135, val_acc:0.706]
Epoch [18/120    avg_loss:1.042, val_acc:0.733]
Epoch [19/120    avg_loss:0.972, val_acc:0.722]
Epoch [20/120    avg_loss:0.907, val_acc:0.765]
Epoch [21/120    avg_loss:0.782, val_acc:0.765]
Epoch [22/120    avg_loss:0.782, val_acc:0.789]
Epoch [23/120    avg_loss:0.785, val_acc:0.739]
Epoch [24/120    avg_loss:0.720, val_acc:0.786]
Epoch [25/120    avg_loss:0.637, val_acc:0.776]
Epoch [26/120    avg_loss:0.604, val_acc:0.815]
Epoch [27/120    avg_loss:0.592, val_acc:0.792]
Epoch [28/120    avg_loss:0.534, val_acc:0.810]
Epoch [29/120    avg_loss:0.465, val_acc:0.822]
Epoch [30/120    avg_loss:0.456, val_acc:0.810]
Epoch [31/120    avg_loss:0.472, val_acc:0.801]
Epoch [32/120    avg_loss:0.403, val_acc:0.855]
Epoch [33/120    avg_loss:0.397, val_acc:0.857]
Epoch [34/120    avg_loss:0.351, val_acc:0.877]
Epoch [35/120    avg_loss:0.332, val_acc:0.887]
Epoch [36/120    avg_loss:0.271, val_acc:0.843]
Epoch [37/120    avg_loss:0.241, val_acc:0.900]
Epoch [38/120    avg_loss:0.263, val_acc:0.887]
Epoch [39/120    avg_loss:0.294, val_acc:0.886]
Epoch [40/120    avg_loss:0.296, val_acc:0.832]
Epoch [41/120    avg_loss:0.233, val_acc:0.890]
Epoch [42/120    avg_loss:0.246, val_acc:0.873]
Epoch [43/120    avg_loss:0.286, val_acc:0.880]
Epoch [44/120    avg_loss:0.278, val_acc:0.856]
Epoch [45/120    avg_loss:0.285, val_acc:0.860]
Epoch [46/120    avg_loss:0.215, val_acc:0.882]
Epoch [47/120    avg_loss:0.185, val_acc:0.886]
Epoch [48/120    avg_loss:0.193, val_acc:0.898]
Epoch [49/120    avg_loss:0.164, val_acc:0.909]
Epoch [50/120    avg_loss:0.140, val_acc:0.906]
Epoch [51/120    avg_loss:0.177, val_acc:0.894]
Epoch [52/120    avg_loss:0.186, val_acc:0.899]
Epoch [53/120    avg_loss:0.153, val_acc:0.922]
Epoch [54/120    avg_loss:0.111, val_acc:0.927]
Epoch [55/120    avg_loss:0.117, val_acc:0.935]
Epoch [56/120    avg_loss:0.101, val_acc:0.924]
Epoch [57/120    avg_loss:0.104, val_acc:0.924]
Epoch [58/120    avg_loss:0.092, val_acc:0.936]
Epoch [59/120    avg_loss:0.088, val_acc:0.932]
Epoch [60/120    avg_loss:0.109, val_acc:0.919]
Epoch [61/120    avg_loss:0.097, val_acc:0.934]
Epoch [62/120    avg_loss:0.102, val_acc:0.936]
Epoch [63/120    avg_loss:0.091, val_acc:0.941]
Epoch [64/120    avg_loss:0.077, val_acc:0.936]
Epoch [65/120    avg_loss:0.090, val_acc:0.938]
Epoch [66/120    avg_loss:0.081, val_acc:0.939]
Epoch [67/120    avg_loss:0.071, val_acc:0.947]
Epoch [68/120    avg_loss:0.081, val_acc:0.935]
Epoch [69/120    avg_loss:0.086, val_acc:0.942]
Epoch [70/120    avg_loss:0.078, val_acc:0.938]
Epoch [71/120    avg_loss:0.072, val_acc:0.948]
Epoch [72/120    avg_loss:0.071, val_acc:0.935]
Epoch [73/120    avg_loss:0.136, val_acc:0.939]
Epoch [74/120    avg_loss:0.116, val_acc:0.933]
Epoch [75/120    avg_loss:0.077, val_acc:0.933]
Epoch [76/120    avg_loss:0.068, val_acc:0.950]
Epoch [77/120    avg_loss:0.051, val_acc:0.956]
Epoch [78/120    avg_loss:0.059, val_acc:0.936]
Epoch [79/120    avg_loss:0.077, val_acc:0.958]
Epoch [80/120    avg_loss:0.083, val_acc:0.955]
Epoch [81/120    avg_loss:0.072, val_acc:0.940]
Epoch [82/120    avg_loss:0.057, val_acc:0.942]
Epoch [83/120    avg_loss:0.050, val_acc:0.957]
Epoch [84/120    avg_loss:0.046, val_acc:0.942]
Epoch [85/120    avg_loss:0.076, val_acc:0.940]
Epoch [86/120    avg_loss:0.055, val_acc:0.959]
Epoch [87/120    avg_loss:0.041, val_acc:0.960]
Epoch [88/120    avg_loss:0.046, val_acc:0.952]
Epoch [89/120    avg_loss:0.049, val_acc:0.959]
Epoch [90/120    avg_loss:0.042, val_acc:0.960]
Epoch [91/120    avg_loss:0.045, val_acc:0.963]
Epoch [92/120    avg_loss:0.042, val_acc:0.956]
Epoch [93/120    avg_loss:0.038, val_acc:0.965]
Epoch [94/120    avg_loss:0.034, val_acc:0.964]
Epoch [95/120    avg_loss:0.047, val_acc:0.959]
Epoch [96/120    avg_loss:0.120, val_acc:0.944]
Epoch [97/120    avg_loss:0.090, val_acc:0.948]
Epoch [98/120    avg_loss:0.064, val_acc:0.943]
Epoch [99/120    avg_loss:0.078, val_acc:0.936]
Epoch [100/120    avg_loss:0.074, val_acc:0.953]
Epoch [101/120    avg_loss:0.060, val_acc:0.953]
Epoch [102/120    avg_loss:0.044, val_acc:0.964]
Epoch [103/120    avg_loss:0.046, val_acc:0.957]
Epoch [104/120    avg_loss:0.047, val_acc:0.956]
Epoch [105/120    avg_loss:0.033, val_acc:0.968]
Epoch [106/120    avg_loss:0.037, val_acc:0.967]
Epoch [107/120    avg_loss:0.027, val_acc:0.958]
Epoch [108/120    avg_loss:0.031, val_acc:0.963]
Epoch [109/120    avg_loss:0.030, val_acc:0.964]
Epoch [110/120    avg_loss:0.037, val_acc:0.956]
Epoch [111/120    avg_loss:0.036, val_acc:0.964]
Epoch [112/120    avg_loss:0.035, val_acc:0.968]
Epoch [113/120    avg_loss:0.028, val_acc:0.961]
Epoch [114/120    avg_loss:0.025, val_acc:0.963]
Epoch [115/120    avg_loss:0.021, val_acc:0.969]
Epoch [116/120    avg_loss:0.023, val_acc:0.966]
Epoch [117/120    avg_loss:0.021, val_acc:0.961]
Epoch [118/120    avg_loss:0.020, val_acc:0.964]
Epoch [119/120    avg_loss:0.024, val_acc:0.968]
Epoch [120/120    avg_loss:0.024, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    1 1250    3    0    0    0    0    0    0    9   17    5    0
     0    0    0]
 [   0    0    2  708    0   20    1    0    0    3    0    0   13    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    5    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   44   62    0    0    0    0    0    0  764    2    0    0
     0    3    0]
 [   0    0   38    0    0    0    3    0    0    0   13 2152    3    1
     0    0    0]
 [   0    0    0   11   28    0    0    0    0    0    9    0  445    0
     0    0   41]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0   12    0    2    0    1    1    0    0
  1117    6    0]
 [   0    0    1    0    0    0   23    0    0    2    0    0    0    0
    25  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.47967479674797

F1 scores:
[       nan 0.95       0.95419847 0.9248857  0.93832599 0.96825397
 0.9704142  1.         0.99767981 0.7826087  0.91278375 0.98197582
 0.89       0.99730458 0.97810858 0.90797546 0.80382775]

Kappa:
0.9485053710123212
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd082bd4898>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.796, val_acc:0.159]
Epoch [2/120    avg_loss:2.580, val_acc:0.388]
Epoch [3/120    avg_loss:2.401, val_acc:0.480]
Epoch [4/120    avg_loss:2.224, val_acc:0.515]
Epoch [5/120    avg_loss:2.110, val_acc:0.536]
Epoch [6/120    avg_loss:1.991, val_acc:0.566]
Epoch [7/120    avg_loss:1.929, val_acc:0.591]
Epoch [8/120    avg_loss:1.848, val_acc:0.627]
Epoch [9/120    avg_loss:1.771, val_acc:0.642]
Epoch [10/120    avg_loss:1.651, val_acc:0.645]
Epoch [11/120    avg_loss:1.575, val_acc:0.685]
Epoch [12/120    avg_loss:1.499, val_acc:0.667]
Epoch [13/120    avg_loss:1.373, val_acc:0.694]
Epoch [14/120    avg_loss:1.269, val_acc:0.728]
Epoch [15/120    avg_loss:1.206, val_acc:0.748]
Epoch [16/120    avg_loss:1.161, val_acc:0.764]
Epoch [17/120    avg_loss:1.067, val_acc:0.750]
Epoch [18/120    avg_loss:0.934, val_acc:0.785]
Epoch [19/120    avg_loss:0.841, val_acc:0.802]
Epoch [20/120    avg_loss:0.761, val_acc:0.797]
Epoch [21/120    avg_loss:0.741, val_acc:0.777]
Epoch [22/120    avg_loss:0.684, val_acc:0.827]
Epoch [23/120    avg_loss:0.679, val_acc:0.812]
Epoch [24/120    avg_loss:0.657, val_acc:0.808]
Epoch [25/120    avg_loss:0.559, val_acc:0.835]
Epoch [26/120    avg_loss:0.527, val_acc:0.883]
Epoch [27/120    avg_loss:0.407, val_acc:0.893]
Epoch [28/120    avg_loss:0.403, val_acc:0.899]
Epoch [29/120    avg_loss:0.390, val_acc:0.878]
Epoch [30/120    avg_loss:0.370, val_acc:0.909]
Epoch [31/120    avg_loss:0.453, val_acc:0.849]
Epoch [32/120    avg_loss:0.486, val_acc:0.894]
Epoch [33/120    avg_loss:0.400, val_acc:0.901]
Epoch [34/120    avg_loss:0.295, val_acc:0.900]
Epoch [35/120    avg_loss:0.300, val_acc:0.914]
Epoch [36/120    avg_loss:0.259, val_acc:0.918]
Epoch [37/120    avg_loss:0.267, val_acc:0.895]
Epoch [38/120    avg_loss:0.404, val_acc:0.887]
Epoch [39/120    avg_loss:0.324, val_acc:0.910]
Epoch [40/120    avg_loss:0.245, val_acc:0.909]
Epoch [41/120    avg_loss:0.256, val_acc:0.923]
Epoch [42/120    avg_loss:0.241, val_acc:0.922]
Epoch [43/120    avg_loss:0.199, val_acc:0.919]
Epoch [44/120    avg_loss:0.191, val_acc:0.938]
Epoch [45/120    avg_loss:0.212, val_acc:0.922]
Epoch [46/120    avg_loss:0.188, val_acc:0.926]
Epoch [47/120    avg_loss:0.160, val_acc:0.947]
Epoch [48/120    avg_loss:0.170, val_acc:0.938]
Epoch [49/120    avg_loss:0.148, val_acc:0.943]
Epoch [50/120    avg_loss:0.146, val_acc:0.933]
Epoch [51/120    avg_loss:0.132, val_acc:0.952]
Epoch [52/120    avg_loss:0.125, val_acc:0.945]
Epoch [53/120    avg_loss:0.139, val_acc:0.958]
Epoch [54/120    avg_loss:0.118, val_acc:0.949]
Epoch [55/120    avg_loss:0.107, val_acc:0.952]
Epoch [56/120    avg_loss:0.122, val_acc:0.963]
Epoch [57/120    avg_loss:0.096, val_acc:0.963]
Epoch [58/120    avg_loss:0.111, val_acc:0.949]
Epoch [59/120    avg_loss:0.104, val_acc:0.947]
Epoch [60/120    avg_loss:0.103, val_acc:0.948]
Epoch [61/120    avg_loss:0.092, val_acc:0.953]
Epoch [62/120    avg_loss:0.086, val_acc:0.958]
Epoch [63/120    avg_loss:0.077, val_acc:0.955]
Epoch [64/120    avg_loss:0.087, val_acc:0.972]
Epoch [65/120    avg_loss:0.083, val_acc:0.958]
Epoch [66/120    avg_loss:0.088, val_acc:0.949]
Epoch [67/120    avg_loss:0.100, val_acc:0.949]
Epoch [68/120    avg_loss:0.122, val_acc:0.952]
Epoch [69/120    avg_loss:0.099, val_acc:0.964]
Epoch [70/120    avg_loss:0.086, val_acc:0.967]
Epoch [71/120    avg_loss:0.072, val_acc:0.968]
Epoch [72/120    avg_loss:0.079, val_acc:0.963]
Epoch [73/120    avg_loss:0.065, val_acc:0.967]
Epoch [74/120    avg_loss:0.054, val_acc:0.966]
Epoch [75/120    avg_loss:0.050, val_acc:0.976]
Epoch [76/120    avg_loss:0.054, val_acc:0.973]
Epoch [77/120    avg_loss:0.059, val_acc:0.953]
Epoch [78/120    avg_loss:0.065, val_acc:0.956]
Epoch [79/120    avg_loss:0.088, val_acc:0.956]
Epoch [80/120    avg_loss:0.120, val_acc:0.927]
Epoch [81/120    avg_loss:0.128, val_acc:0.959]
Epoch [82/120    avg_loss:0.093, val_acc:0.959]
Epoch [83/120    avg_loss:0.094, val_acc:0.943]
Epoch [84/120    avg_loss:0.066, val_acc:0.965]
Epoch [85/120    avg_loss:0.065, val_acc:0.952]
Epoch [86/120    avg_loss:0.070, val_acc:0.949]
Epoch [87/120    avg_loss:0.082, val_acc:0.960]
Epoch [88/120    avg_loss:0.091, val_acc:0.970]
Epoch [89/120    avg_loss:0.056, val_acc:0.973]
Epoch [90/120    avg_loss:0.050, val_acc:0.975]
Epoch [91/120    avg_loss:0.041, val_acc:0.975]
Epoch [92/120    avg_loss:0.046, val_acc:0.978]
Epoch [93/120    avg_loss:0.045, val_acc:0.976]
Epoch [94/120    avg_loss:0.039, val_acc:0.975]
Epoch [95/120    avg_loss:0.039, val_acc:0.980]
Epoch [96/120    avg_loss:0.033, val_acc:0.977]
Epoch [97/120    avg_loss:0.037, val_acc:0.977]
Epoch [98/120    avg_loss:0.029, val_acc:0.980]
Epoch [99/120    avg_loss:0.034, val_acc:0.980]
Epoch [100/120    avg_loss:0.030, val_acc:0.975]
Epoch [101/120    avg_loss:0.037, val_acc:0.976]
Epoch [102/120    avg_loss:0.033, val_acc:0.978]
Epoch [103/120    avg_loss:0.035, val_acc:0.976]
Epoch [104/120    avg_loss:0.031, val_acc:0.978]
Epoch [105/120    avg_loss:0.033, val_acc:0.980]
Epoch [106/120    avg_loss:0.030, val_acc:0.976]
Epoch [107/120    avg_loss:0.027, val_acc:0.978]
Epoch [108/120    avg_loss:0.028, val_acc:0.980]
Epoch [109/120    avg_loss:0.029, val_acc:0.981]
Epoch [110/120    avg_loss:0.027, val_acc:0.980]
Epoch [111/120    avg_loss:0.023, val_acc:0.978]
Epoch [112/120    avg_loss:0.029, val_acc:0.980]
Epoch [113/120    avg_loss:0.027, val_acc:0.981]
Epoch [114/120    avg_loss:0.031, val_acc:0.980]
Epoch [115/120    avg_loss:0.029, val_acc:0.981]
Epoch [116/120    avg_loss:0.027, val_acc:0.980]
Epoch [117/120    avg_loss:0.029, val_acc:0.980]
Epoch [118/120    avg_loss:0.026, val_acc:0.981]
Epoch [119/120    avg_loss:0.031, val_acc:0.981]
Epoch [120/120    avg_loss:0.028, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1253    1    1    0    5    0    0    0    4   19    2    0
     0    0    0]
 [   0    0    5  694   11   15    0    0    0   14    1    0    7    0
     0    0    0]
 [   0    0    0    0  212    0    1    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    2    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   25   42    0    3    1    0    0    0  799    3    0    0
     1    1    0]
 [   0    0   24    1    0    0    6    0    0    0   13 2165    0    1
     0    0    0]
 [   0    0    0    2   12    1    0    1    0    0    1    2  508    0
     1    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    1    1    0    0
  1130    0    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    79  245    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.17344173441734

F1 scores:
[       nan 0.96202532 0.96682099 0.933423   0.94432071 0.9674523
 0.97257228 0.94339623 0.99883856 0.66666667 0.94166176 0.9838673
 0.96577947 0.99730458 0.96170213 0.82630691 0.96551724]

Kappa:
0.9563670607174273
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faf48fa47f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.786, val_acc:0.181]
Epoch [2/120    avg_loss:2.547, val_acc:0.425]
Epoch [3/120    avg_loss:2.347, val_acc:0.497]
Epoch [4/120    avg_loss:2.230, val_acc:0.526]
Epoch [5/120    avg_loss:2.091, val_acc:0.540]
Epoch [6/120    avg_loss:1.993, val_acc:0.540]
Epoch [7/120    avg_loss:1.906, val_acc:0.555]
Epoch [8/120    avg_loss:1.790, val_acc:0.547]
Epoch [9/120    avg_loss:1.726, val_acc:0.566]
Epoch [10/120    avg_loss:1.720, val_acc:0.594]
Epoch [11/120    avg_loss:1.586, val_acc:0.618]
Epoch [12/120    avg_loss:1.453, val_acc:0.627]
Epoch [13/120    avg_loss:1.379, val_acc:0.651]
Epoch [14/120    avg_loss:1.339, val_acc:0.650]
Epoch [15/120    avg_loss:1.215, val_acc:0.664]
Epoch [16/120    avg_loss:1.164, val_acc:0.705]
Epoch [17/120    avg_loss:1.091, val_acc:0.699]
Epoch [18/120    avg_loss:1.014, val_acc:0.709]
Epoch [19/120    avg_loss:0.928, val_acc:0.732]
Epoch [20/120    avg_loss:0.863, val_acc:0.748]
Epoch [21/120    avg_loss:0.757, val_acc:0.768]
Epoch [22/120    avg_loss:0.750, val_acc:0.790]
Epoch [23/120    avg_loss:0.680, val_acc:0.823]
Epoch [24/120    avg_loss:0.627, val_acc:0.762]
Epoch [25/120    avg_loss:0.670, val_acc:0.791]
Epoch [26/120    avg_loss:0.598, val_acc:0.794]
Epoch [27/120    avg_loss:0.577, val_acc:0.819]
Epoch [28/120    avg_loss:0.526, val_acc:0.802]
Epoch [29/120    avg_loss:0.547, val_acc:0.809]
Epoch [30/120    avg_loss:0.437, val_acc:0.856]
Epoch [31/120    avg_loss:0.397, val_acc:0.863]
Epoch [32/120    avg_loss:0.365, val_acc:0.869]
Epoch [33/120    avg_loss:0.355, val_acc:0.886]
Epoch [34/120    avg_loss:0.339, val_acc:0.876]
Epoch [35/120    avg_loss:0.317, val_acc:0.874]
Epoch [36/120    avg_loss:0.336, val_acc:0.822]
Epoch [37/120    avg_loss:0.283, val_acc:0.903]
Epoch [38/120    avg_loss:0.252, val_acc:0.891]
Epoch [39/120    avg_loss:0.244, val_acc:0.912]
Epoch [40/120    avg_loss:0.225, val_acc:0.909]
Epoch [41/120    avg_loss:0.215, val_acc:0.911]
Epoch [42/120    avg_loss:0.189, val_acc:0.914]
Epoch [43/120    avg_loss:0.166, val_acc:0.920]
Epoch [44/120    avg_loss:0.195, val_acc:0.918]
Epoch [45/120    avg_loss:0.174, val_acc:0.916]
Epoch [46/120    avg_loss:0.175, val_acc:0.924]
Epoch [47/120    avg_loss:0.165, val_acc:0.905]
Epoch [48/120    avg_loss:0.196, val_acc:0.933]
Epoch [49/120    avg_loss:0.132, val_acc:0.923]
Epoch [50/120    avg_loss:0.263, val_acc:0.848]
Epoch [51/120    avg_loss:0.332, val_acc:0.900]
Epoch [52/120    avg_loss:0.311, val_acc:0.891]
Epoch [53/120    avg_loss:0.266, val_acc:0.885]
Epoch [54/120    avg_loss:0.266, val_acc:0.883]
Epoch [55/120    avg_loss:0.261, val_acc:0.886]
Epoch [56/120    avg_loss:0.208, val_acc:0.911]
Epoch [57/120    avg_loss:0.177, val_acc:0.932]
Epoch [58/120    avg_loss:0.141, val_acc:0.941]
Epoch [59/120    avg_loss:0.113, val_acc:0.935]
Epoch [60/120    avg_loss:0.110, val_acc:0.918]
Epoch [61/120    avg_loss:0.111, val_acc:0.934]
Epoch [62/120    avg_loss:0.119, val_acc:0.936]
Epoch [63/120    avg_loss:0.121, val_acc:0.940]
Epoch [64/120    avg_loss:0.140, val_acc:0.942]
Epoch [65/120    avg_loss:0.096, val_acc:0.924]
Epoch [66/120    avg_loss:0.113, val_acc:0.934]
Epoch [67/120    avg_loss:0.114, val_acc:0.934]
Epoch [68/120    avg_loss:0.119, val_acc:0.925]
Epoch [69/120    avg_loss:0.171, val_acc:0.915]
Epoch [70/120    avg_loss:0.140, val_acc:0.924]
Epoch [71/120    avg_loss:0.100, val_acc:0.941]
Epoch [72/120    avg_loss:0.086, val_acc:0.942]
Epoch [73/120    avg_loss:0.070, val_acc:0.952]
Epoch [74/120    avg_loss:0.080, val_acc:0.943]
Epoch [75/120    avg_loss:0.070, val_acc:0.938]
Epoch [76/120    avg_loss:0.074, val_acc:0.947]
Epoch [77/120    avg_loss:0.069, val_acc:0.942]
Epoch [78/120    avg_loss:0.095, val_acc:0.942]
Epoch [79/120    avg_loss:0.081, val_acc:0.951]
Epoch [80/120    avg_loss:0.071, val_acc:0.947]
Epoch [81/120    avg_loss:0.069, val_acc:0.949]
Epoch [82/120    avg_loss:0.077, val_acc:0.942]
Epoch [83/120    avg_loss:0.064, val_acc:0.960]
Epoch [84/120    avg_loss:0.067, val_acc:0.949]
Epoch [85/120    avg_loss:0.056, val_acc:0.947]
Epoch [86/120    avg_loss:0.054, val_acc:0.942]
Epoch [87/120    avg_loss:0.051, val_acc:0.951]
Epoch [88/120    avg_loss:0.041, val_acc:0.952]
Epoch [89/120    avg_loss:0.033, val_acc:0.961]
Epoch [90/120    avg_loss:0.032, val_acc:0.957]
Epoch [91/120    avg_loss:0.033, val_acc:0.958]
Epoch [92/120    avg_loss:0.029, val_acc:0.951]
Epoch [93/120    avg_loss:0.029, val_acc:0.957]
Epoch [94/120    avg_loss:0.025, val_acc:0.956]
Epoch [95/120    avg_loss:0.034, val_acc:0.958]
Epoch [96/120    avg_loss:0.056, val_acc:0.957]
Epoch [97/120    avg_loss:0.041, val_acc:0.957]
Epoch [98/120    avg_loss:0.036, val_acc:0.963]
Epoch [99/120    avg_loss:0.025, val_acc:0.953]
Epoch [100/120    avg_loss:0.040, val_acc:0.949]
Epoch [101/120    avg_loss:0.036, val_acc:0.960]
Epoch [102/120    avg_loss:0.034, val_acc:0.959]
Epoch [103/120    avg_loss:0.029, val_acc:0.955]
Epoch [104/120    avg_loss:0.025, val_acc:0.964]
Epoch [105/120    avg_loss:0.027, val_acc:0.958]
Epoch [106/120    avg_loss:0.045, val_acc:0.955]
Epoch [107/120    avg_loss:0.037, val_acc:0.955]
Epoch [108/120    avg_loss:0.038, val_acc:0.959]
Epoch [109/120    avg_loss:0.034, val_acc:0.956]
Epoch [110/120    avg_loss:0.030, val_acc:0.961]
Epoch [111/120    avg_loss:0.026, val_acc:0.959]
Epoch [112/120    avg_loss:0.029, val_acc:0.960]
Epoch [113/120    avg_loss:0.026, val_acc:0.961]
Epoch [114/120    avg_loss:0.020, val_acc:0.966]
Epoch [115/120    avg_loss:0.016, val_acc:0.959]
Epoch [116/120    avg_loss:0.015, val_acc:0.960]
Epoch [117/120    avg_loss:0.016, val_acc:0.961]
Epoch [118/120    avg_loss:0.020, val_acc:0.967]
Epoch [119/120    avg_loss:0.016, val_acc:0.965]
Epoch [120/120    avg_loss:0.021, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1212    0    0    0    4    0    0    0   29   35    2    0
     2    1    0]
 [   0    0    1  680   30   14    1    0    0    9    0    0   12    0
     0    0    0]
 [   0    0    0    0  211    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  425    0    0    0   10    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    9    0    0    8    0    0    1    0
     0    0    0]
 [   0    0   14   74    0    8    0    0    0    0  758   11    0    0
     0   10    0]
 [   0    0   13    0    0    0    9    0    0    0    7 2163   14    4
     0    0    0]
 [   0    0    0    1   11   11    0    0    0    0    5    2  500    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    3    0    0    0
  1133    0    0]
 [   0    0    0    0    0    0    3    0    0    2    0    0    0    0
    48  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.37127371273712

F1 scores:
[       nan 0.91566265 0.96       0.90545939 0.90752688 0.94866071
 0.9798357  1.         0.9953271  0.34042553 0.90238095 0.97851165
 0.93896714 0.98930481 0.97546276 0.90045942 0.98245614]

Kappa:
0.9472565181480987
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc85d6b4860>
supervision:full
center_pixel:True
Network :
Number of parameter: 60334==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.816, val_acc:0.252]
Epoch [2/120    avg_loss:2.539, val_acc:0.354]
Epoch [3/120    avg_loss:2.320, val_acc:0.393]
Epoch [4/120    avg_loss:2.212, val_acc:0.454]
Epoch [5/120    avg_loss:2.093, val_acc:0.502]
Epoch [6/120    avg_loss:2.006, val_acc:0.543]
Epoch [7/120    avg_loss:1.925, val_acc:0.597]
Epoch [8/120    avg_loss:1.814, val_acc:0.623]
Epoch [9/120    avg_loss:1.784, val_acc:0.628]
Epoch [10/120    avg_loss:1.669, val_acc:0.667]
Epoch [11/120    avg_loss:1.597, val_acc:0.677]
Epoch [12/120    avg_loss:1.494, val_acc:0.682]
Epoch [13/120    avg_loss:1.377, val_acc:0.691]
Epoch [14/120    avg_loss:1.262, val_acc:0.726]
Epoch [15/120    avg_loss:1.105, val_acc:0.735]
Epoch [16/120    avg_loss:1.085, val_acc:0.707]
Epoch [17/120    avg_loss:1.011, val_acc:0.686]
Epoch [18/120    avg_loss:0.859, val_acc:0.774]
Epoch [19/120    avg_loss:0.822, val_acc:0.779]
Epoch [20/120    avg_loss:0.730, val_acc:0.782]
Epoch [21/120    avg_loss:0.682, val_acc:0.785]
Epoch [22/120    avg_loss:0.677, val_acc:0.809]
Epoch [23/120    avg_loss:0.608, val_acc:0.785]
Epoch [24/120    avg_loss:0.659, val_acc:0.761]
Epoch [25/120    avg_loss:0.600, val_acc:0.803]
Epoch [26/120    avg_loss:0.526, val_acc:0.813]
Epoch [27/120    avg_loss:0.510, val_acc:0.822]
Epoch [28/120    avg_loss:0.447, val_acc:0.838]
Epoch [29/120    avg_loss:0.401, val_acc:0.843]
Epoch [30/120    avg_loss:0.401, val_acc:0.857]
Epoch [31/120    avg_loss:0.372, val_acc:0.880]
Epoch [32/120    avg_loss:0.360, val_acc:0.872]
Epoch [33/120    avg_loss:0.331, val_acc:0.837]
Epoch [34/120    avg_loss:0.305, val_acc:0.892]
Epoch [35/120    avg_loss:0.270, val_acc:0.889]
Epoch [36/120    avg_loss:0.284, val_acc:0.892]
Epoch [37/120    avg_loss:0.265, val_acc:0.895]
Epoch [38/120    avg_loss:0.300, val_acc:0.883]
Epoch [39/120    avg_loss:0.236, val_acc:0.912]
Epoch [40/120    avg_loss:0.218, val_acc:0.917]
Epoch [41/120    avg_loss:0.211, val_acc:0.921]
Epoch [42/120    avg_loss:0.180, val_acc:0.903]
Epoch [43/120    avg_loss:0.302, val_acc:0.891]
Epoch [44/120    avg_loss:0.263, val_acc:0.908]
Epoch [45/120    avg_loss:0.244, val_acc:0.925]
Epoch [46/120    avg_loss:0.189, val_acc:0.896]
Epoch [47/120    avg_loss:0.196, val_acc:0.918]
Epoch [48/120    avg_loss:0.216, val_acc:0.900]
Epoch [49/120    avg_loss:0.227, val_acc:0.893]
Epoch [50/120    avg_loss:0.222, val_acc:0.911]
Epoch [51/120    avg_loss:0.202, val_acc:0.926]
Epoch [52/120    avg_loss:0.152, val_acc:0.921]
Epoch [53/120    avg_loss:0.159, val_acc:0.930]
Epoch [54/120    avg_loss:0.138, val_acc:0.938]
Epoch [55/120    avg_loss:0.128, val_acc:0.940]
Epoch [56/120    avg_loss:0.126, val_acc:0.945]
Epoch [57/120    avg_loss:0.095, val_acc:0.947]
Epoch [58/120    avg_loss:0.104, val_acc:0.947]
Epoch [59/120    avg_loss:0.091, val_acc:0.928]
Epoch [60/120    avg_loss:0.104, val_acc:0.930]
Epoch [61/120    avg_loss:0.111, val_acc:0.953]
Epoch [62/120    avg_loss:0.099, val_acc:0.954]
Epoch [63/120    avg_loss:0.081, val_acc:0.948]
Epoch [64/120    avg_loss:0.086, val_acc:0.954]
Epoch [65/120    avg_loss:0.070, val_acc:0.955]
Epoch [66/120    avg_loss:0.071, val_acc:0.949]
Epoch [67/120    avg_loss:0.063, val_acc:0.946]
Epoch [68/120    avg_loss:0.059, val_acc:0.952]
Epoch [69/120    avg_loss:0.060, val_acc:0.951]
Epoch [70/120    avg_loss:0.074, val_acc:0.959]
Epoch [71/120    avg_loss:0.063, val_acc:0.947]
Epoch [72/120    avg_loss:0.064, val_acc:0.959]
Epoch [73/120    avg_loss:0.052, val_acc:0.955]
Epoch [74/120    avg_loss:0.047, val_acc:0.958]
Epoch [75/120    avg_loss:0.048, val_acc:0.963]
Epoch [76/120    avg_loss:0.047, val_acc:0.962]
Epoch [77/120    avg_loss:0.050, val_acc:0.960]
Epoch [78/120    avg_loss:0.044, val_acc:0.962]
Epoch [79/120    avg_loss:0.044, val_acc:0.963]
Epoch [80/120    avg_loss:0.040, val_acc:0.965]
Epoch [81/120    avg_loss:0.037, val_acc:0.970]
Epoch [82/120    avg_loss:0.032, val_acc:0.965]
Epoch [83/120    avg_loss:0.037, val_acc:0.963]
Epoch [84/120    avg_loss:0.042, val_acc:0.962]
Epoch [85/120    avg_loss:0.038, val_acc:0.965]
Epoch [86/120    avg_loss:0.049, val_acc:0.957]
Epoch [87/120    avg_loss:0.054, val_acc:0.961]
Epoch [88/120    avg_loss:0.041, val_acc:0.963]
Epoch [89/120    avg_loss:0.028, val_acc:0.966]
Epoch [90/120    avg_loss:0.029, val_acc:0.972]
Epoch [91/120    avg_loss:0.031, val_acc:0.973]
Epoch [92/120    avg_loss:0.034, val_acc:0.972]
Epoch [93/120    avg_loss:0.030, val_acc:0.967]
Epoch [94/120    avg_loss:0.028, val_acc:0.974]
Epoch [95/120    avg_loss:0.023, val_acc:0.974]
Epoch [96/120    avg_loss:0.024, val_acc:0.966]
Epoch [97/120    avg_loss:0.028, val_acc:0.971]
Epoch [98/120    avg_loss:0.041, val_acc:0.964]
Epoch [99/120    avg_loss:0.032, val_acc:0.970]
Epoch [100/120    avg_loss:0.039, val_acc:0.957]
Epoch [101/120    avg_loss:0.039, val_acc:0.971]
Epoch [102/120    avg_loss:0.025, val_acc:0.972]
Epoch [103/120    avg_loss:0.022, val_acc:0.973]
Epoch [104/120    avg_loss:0.030, val_acc:0.968]
Epoch [105/120    avg_loss:0.032, val_acc:0.965]
Epoch [106/120    avg_loss:0.035, val_acc:0.963]
Epoch [107/120    avg_loss:0.029, val_acc:0.966]
Epoch [108/120    avg_loss:0.029, val_acc:0.971]
Epoch [109/120    avg_loss:0.024, val_acc:0.972]
Epoch [110/120    avg_loss:0.014, val_acc:0.972]
Epoch [111/120    avg_loss:0.018, val_acc:0.973]
Epoch [112/120    avg_loss:0.019, val_acc:0.972]
Epoch [113/120    avg_loss:0.019, val_acc:0.972]
Epoch [114/120    avg_loss:0.016, val_acc:0.974]
Epoch [115/120    avg_loss:0.015, val_acc:0.974]
Epoch [116/120    avg_loss:0.018, val_acc:0.973]
Epoch [117/120    avg_loss:0.014, val_acc:0.973]
Epoch [118/120    avg_loss:0.015, val_acc:0.974]
Epoch [119/120    avg_loss:0.014, val_acc:0.974]
Epoch [120/120    avg_loss:0.013, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1234   10    0    0    0    0    0    0    9   27    5    0
     0    0    0]
 [   0    0    0  697    3   29    1    0    0    8    1    0    8    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    5    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   16   57    0    4    0    0    0    0  789    2    7    0
     0    0    0]
 [   0    0   12    0    0    0    3    0    1    0   23 2163    6    2
     0    0    0]
 [   0    0    0   13    1    7    0    0    0    2   12    2  493    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    3    1    0    0
  1128    0    0]
 [   0    0    0    0    0    0    7    0    0    2    0    0    0    0
    45  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
96.11924119241192

F1 scores:
[       nan 0.94871795 0.96898312 0.91349934 0.98598131 0.94285714
 0.99093656 0.98039216 0.99767981 0.65384615 0.91958042 0.98184294
 0.93194707 0.99462366 0.97535668 0.915625   0.95808383]

Kappa:
0.9557705583535268
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffab16cf898>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.786, val_acc:0.391]
Epoch [2/120    avg_loss:2.564, val_acc:0.400]
Epoch [3/120    avg_loss:2.404, val_acc:0.394]
Epoch [4/120    avg_loss:2.242, val_acc:0.435]
Epoch [5/120    avg_loss:2.150, val_acc:0.465]
Epoch [6/120    avg_loss:2.019, val_acc:0.537]
Epoch [7/120    avg_loss:1.992, val_acc:0.541]
Epoch [8/120    avg_loss:1.889, val_acc:0.528]
Epoch [9/120    avg_loss:1.837, val_acc:0.539]
Epoch [10/120    avg_loss:1.768, val_acc:0.562]
Epoch [11/120    avg_loss:1.692, val_acc:0.573]
Epoch [12/120    avg_loss:1.625, val_acc:0.622]
Epoch [13/120    avg_loss:1.524, val_acc:0.613]
Epoch [14/120    avg_loss:1.410, val_acc:0.652]
Epoch [15/120    avg_loss:1.364, val_acc:0.648]
Epoch [16/120    avg_loss:1.258, val_acc:0.674]
Epoch [17/120    avg_loss:1.175, val_acc:0.666]
Epoch [18/120    avg_loss:1.017, val_acc:0.710]
Epoch [19/120    avg_loss:0.983, val_acc:0.700]
Epoch [20/120    avg_loss:0.946, val_acc:0.697]
Epoch [21/120    avg_loss:0.871, val_acc:0.758]
Epoch [22/120    avg_loss:0.806, val_acc:0.720]
Epoch [23/120    avg_loss:0.763, val_acc:0.749]
Epoch [24/120    avg_loss:0.686, val_acc:0.794]
Epoch [25/120    avg_loss:0.669, val_acc:0.774]
Epoch [26/120    avg_loss:0.591, val_acc:0.767]
Epoch [27/120    avg_loss:0.539, val_acc:0.801]
Epoch [28/120    avg_loss:0.494, val_acc:0.822]
Epoch [29/120    avg_loss:0.466, val_acc:0.834]
Epoch [30/120    avg_loss:0.436, val_acc:0.815]
Epoch [31/120    avg_loss:0.459, val_acc:0.819]
Epoch [32/120    avg_loss:0.418, val_acc:0.833]
Epoch [33/120    avg_loss:0.368, val_acc:0.858]
Epoch [34/120    avg_loss:0.348, val_acc:0.849]
Epoch [35/120    avg_loss:0.325, val_acc:0.859]
Epoch [36/120    avg_loss:0.310, val_acc:0.845]
Epoch [37/120    avg_loss:0.330, val_acc:0.863]
Epoch [38/120    avg_loss:0.339, val_acc:0.836]
Epoch [39/120    avg_loss:0.281, val_acc:0.885]
Epoch [40/120    avg_loss:0.285, val_acc:0.875]
Epoch [41/120    avg_loss:0.246, val_acc:0.878]
Epoch [42/120    avg_loss:0.242, val_acc:0.898]
Epoch [43/120    avg_loss:0.242, val_acc:0.897]
Epoch [44/120    avg_loss:0.216, val_acc:0.901]
Epoch [45/120    avg_loss:0.226, val_acc:0.893]
Epoch [46/120    avg_loss:0.226, val_acc:0.876]
Epoch [47/120    avg_loss:0.248, val_acc:0.852]
Epoch [48/120    avg_loss:0.313, val_acc:0.880]
Epoch [49/120    avg_loss:0.259, val_acc:0.900]
Epoch [50/120    avg_loss:0.216, val_acc:0.903]
Epoch [51/120    avg_loss:0.187, val_acc:0.887]
Epoch [52/120    avg_loss:0.181, val_acc:0.907]
Epoch [53/120    avg_loss:0.176, val_acc:0.893]
Epoch [54/120    avg_loss:0.196, val_acc:0.900]
Epoch [55/120    avg_loss:0.159, val_acc:0.903]
Epoch [56/120    avg_loss:0.163, val_acc:0.910]
Epoch [57/120    avg_loss:0.145, val_acc:0.918]
Epoch [58/120    avg_loss:0.167, val_acc:0.916]
Epoch [59/120    avg_loss:0.309, val_acc:0.894]
Epoch [60/120    avg_loss:0.177, val_acc:0.923]
Epoch [61/120    avg_loss:0.142, val_acc:0.919]
Epoch [62/120    avg_loss:0.161, val_acc:0.931]
Epoch [63/120    avg_loss:0.154, val_acc:0.874]
Epoch [64/120    avg_loss:0.198, val_acc:0.906]
Epoch [65/120    avg_loss:0.132, val_acc:0.910]
Epoch [66/120    avg_loss:0.143, val_acc:0.917]
Epoch [67/120    avg_loss:0.139, val_acc:0.928]
Epoch [68/120    avg_loss:0.096, val_acc:0.933]
Epoch [69/120    avg_loss:0.105, val_acc:0.922]
Epoch [70/120    avg_loss:0.097, val_acc:0.931]
Epoch [71/120    avg_loss:0.097, val_acc:0.931]
Epoch [72/120    avg_loss:0.090, val_acc:0.933]
Epoch [73/120    avg_loss:0.087, val_acc:0.924]
Epoch [74/120    avg_loss:0.095, val_acc:0.927]
Epoch [75/120    avg_loss:0.085, val_acc:0.940]
Epoch [76/120    avg_loss:0.082, val_acc:0.945]
Epoch [77/120    avg_loss:0.111, val_acc:0.936]
Epoch [78/120    avg_loss:0.099, val_acc:0.933]
Epoch [79/120    avg_loss:0.077, val_acc:0.936]
Epoch [80/120    avg_loss:0.085, val_acc:0.920]
Epoch [81/120    avg_loss:0.131, val_acc:0.936]
Epoch [82/120    avg_loss:0.106, val_acc:0.939]
Epoch [83/120    avg_loss:0.077, val_acc:0.945]
Epoch [84/120    avg_loss:0.062, val_acc:0.952]
Epoch [85/120    avg_loss:0.062, val_acc:0.952]
Epoch [86/120    avg_loss:0.081, val_acc:0.944]
Epoch [87/120    avg_loss:0.065, val_acc:0.948]
Epoch [88/120    avg_loss:0.065, val_acc:0.947]
Epoch [89/120    avg_loss:0.060, val_acc:0.955]
Epoch [90/120    avg_loss:0.058, val_acc:0.939]
Epoch [91/120    avg_loss:0.061, val_acc:0.942]
Epoch [92/120    avg_loss:0.076, val_acc:0.938]
Epoch [93/120    avg_loss:0.067, val_acc:0.941]
Epoch [94/120    avg_loss:0.044, val_acc:0.950]
Epoch [95/120    avg_loss:0.041, val_acc:0.964]
Epoch [96/120    avg_loss:0.041, val_acc:0.955]
Epoch [97/120    avg_loss:0.049, val_acc:0.945]
Epoch [98/120    avg_loss:0.042, val_acc:0.951]
Epoch [99/120    avg_loss:0.040, val_acc:0.963]
Epoch [100/120    avg_loss:0.032, val_acc:0.966]
Epoch [101/120    avg_loss:0.039, val_acc:0.960]
Epoch [102/120    avg_loss:0.044, val_acc:0.961]
Epoch [103/120    avg_loss:0.042, val_acc:0.967]
Epoch [104/120    avg_loss:0.044, val_acc:0.964]
Epoch [105/120    avg_loss:0.030, val_acc:0.965]
Epoch [106/120    avg_loss:0.030, val_acc:0.963]
Epoch [107/120    avg_loss:0.029, val_acc:0.973]
Epoch [108/120    avg_loss:0.033, val_acc:0.970]
Epoch [109/120    avg_loss:0.028, val_acc:0.963]
Epoch [110/120    avg_loss:0.043, val_acc:0.970]
Epoch [111/120    avg_loss:0.037, val_acc:0.966]
Epoch [112/120    avg_loss:0.029, val_acc:0.960]
Epoch [113/120    avg_loss:0.027, val_acc:0.961]
Epoch [114/120    avg_loss:0.035, val_acc:0.967]
Epoch [115/120    avg_loss:0.022, val_acc:0.966]
Epoch [116/120    avg_loss:0.027, val_acc:0.961]
Epoch [117/120    avg_loss:0.023, val_acc:0.961]
Epoch [118/120    avg_loss:0.026, val_acc:0.968]
Epoch [119/120    avg_loss:0.021, val_acc:0.972]
Epoch [120/120    avg_loss:0.028, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1218    6    0    0    5    0    0    0    7   47    0    0
     0    2    0]
 [   0    0    5  686    1    8    1    0    0   16    0    0   26    4
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  425    0    5    0    4    0    0    0    1
     0    0    0]
 [   0    0    0    0    0    0  649    0    0    0    0    4    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   29   89    0    6    1    0    0    0  736    4    0    0
     0   10    0]
 [   0    0   17    0    0    0    5    0    0    0   24 2157    1    1
     5    0    0]
 [   0    0    0    7    3   11    0    0    0    0    7    0  491    0
     0    3   12]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    2    0    0
  1135    0    0]
 [   0    0    0    0    0    1   67    0    0    0    0    0    0    0
    89  190    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.08130081300813

F1 scores:
[       nan 0.975      0.95342466 0.89381107 0.98834499 0.95936795
 0.93718412 0.90909091 0.99883856 0.61818182 0.89158086 0.97513562
 0.9316888  0.98404255 0.95699831 0.6884058  0.93333333]

Kappa:
0.9324843145011551
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fed25a3a8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.766, val_acc:0.418]
Epoch [2/120    avg_loss:2.545, val_acc:0.508]
Epoch [3/120    avg_loss:2.357, val_acc:0.487]
Epoch [4/120    avg_loss:2.247, val_acc:0.476]
Epoch [5/120    avg_loss:2.107, val_acc:0.509]
Epoch [6/120    avg_loss:2.053, val_acc:0.564]
Epoch [7/120    avg_loss:1.947, val_acc:0.581]
Epoch [8/120    avg_loss:1.868, val_acc:0.589]
Epoch [9/120    avg_loss:1.792, val_acc:0.618]
Epoch [10/120    avg_loss:1.696, val_acc:0.624]
Epoch [11/120    avg_loss:1.595, val_acc:0.617]
Epoch [12/120    avg_loss:1.472, val_acc:0.650]
Epoch [13/120    avg_loss:1.380, val_acc:0.664]
Epoch [14/120    avg_loss:1.281, val_acc:0.650]
Epoch [15/120    avg_loss:1.166, val_acc:0.680]
Epoch [16/120    avg_loss:1.058, val_acc:0.741]
Epoch [17/120    avg_loss:0.983, val_acc:0.726]
Epoch [18/120    avg_loss:0.912, val_acc:0.760]
Epoch [19/120    avg_loss:0.852, val_acc:0.758]
Epoch [20/120    avg_loss:0.801, val_acc:0.793]
Epoch [21/120    avg_loss:0.754, val_acc:0.770]
Epoch [22/120    avg_loss:0.675, val_acc:0.808]
Epoch [23/120    avg_loss:0.671, val_acc:0.792]
Epoch [24/120    avg_loss:0.619, val_acc:0.809]
Epoch [25/120    avg_loss:0.542, val_acc:0.828]
Epoch [26/120    avg_loss:0.497, val_acc:0.853]
Epoch [27/120    avg_loss:0.518, val_acc:0.836]
Epoch [28/120    avg_loss:0.506, val_acc:0.834]
Epoch [29/120    avg_loss:0.443, val_acc:0.849]
Epoch [30/120    avg_loss:0.407, val_acc:0.874]
Epoch [31/120    avg_loss:0.390, val_acc:0.836]
Epoch [32/120    avg_loss:0.405, val_acc:0.873]
Epoch [33/120    avg_loss:0.376, val_acc:0.856]
Epoch [34/120    avg_loss:0.352, val_acc:0.884]
Epoch [35/120    avg_loss:0.320, val_acc:0.867]
Epoch [36/120    avg_loss:0.327, val_acc:0.893]
Epoch [37/120    avg_loss:0.335, val_acc:0.872]
Epoch [38/120    avg_loss:0.262, val_acc:0.889]
Epoch [39/120    avg_loss:0.268, val_acc:0.910]
Epoch [40/120    avg_loss:0.244, val_acc:0.906]
Epoch [41/120    avg_loss:0.227, val_acc:0.908]
Epoch [42/120    avg_loss:0.258, val_acc:0.900]
Epoch [43/120    avg_loss:0.227, val_acc:0.893]
Epoch [44/120    avg_loss:0.228, val_acc:0.886]
Epoch [45/120    avg_loss:0.228, val_acc:0.902]
Epoch [46/120    avg_loss:0.189, val_acc:0.914]
Epoch [47/120    avg_loss:0.177, val_acc:0.911]
Epoch [48/120    avg_loss:0.212, val_acc:0.910]
Epoch [49/120    avg_loss:0.164, val_acc:0.931]
Epoch [50/120    avg_loss:0.153, val_acc:0.914]
Epoch [51/120    avg_loss:0.137, val_acc:0.911]
Epoch [52/120    avg_loss:0.133, val_acc:0.922]
Epoch [53/120    avg_loss:0.150, val_acc:0.923]
Epoch [54/120    avg_loss:0.124, val_acc:0.938]
Epoch [55/120    avg_loss:0.136, val_acc:0.920]
Epoch [56/120    avg_loss:0.166, val_acc:0.923]
Epoch [57/120    avg_loss:0.162, val_acc:0.934]
Epoch [58/120    avg_loss:0.131, val_acc:0.935]
Epoch [59/120    avg_loss:0.128, val_acc:0.926]
Epoch [60/120    avg_loss:0.145, val_acc:0.930]
Epoch [61/120    avg_loss:0.147, val_acc:0.923]
Epoch [62/120    avg_loss:0.146, val_acc:0.910]
Epoch [63/120    avg_loss:0.140, val_acc:0.939]
Epoch [64/120    avg_loss:0.093, val_acc:0.951]
Epoch [65/120    avg_loss:0.106, val_acc:0.926]
Epoch [66/120    avg_loss:0.111, val_acc:0.928]
Epoch [67/120    avg_loss:0.141, val_acc:0.927]
Epoch [68/120    avg_loss:0.096, val_acc:0.941]
Epoch [69/120    avg_loss:0.121, val_acc:0.936]
Epoch [70/120    avg_loss:0.113, val_acc:0.941]
Epoch [71/120    avg_loss:0.100, val_acc:0.952]
Epoch [72/120    avg_loss:0.099, val_acc:0.936]
Epoch [73/120    avg_loss:0.098, val_acc:0.955]
Epoch [74/120    avg_loss:0.078, val_acc:0.950]
Epoch [75/120    avg_loss:0.080, val_acc:0.945]
Epoch [76/120    avg_loss:0.086, val_acc:0.927]
Epoch [77/120    avg_loss:0.105, val_acc:0.945]
Epoch [78/120    avg_loss:0.083, val_acc:0.956]
Epoch [79/120    avg_loss:0.076, val_acc:0.957]
Epoch [80/120    avg_loss:0.068, val_acc:0.948]
Epoch [81/120    avg_loss:0.066, val_acc:0.964]
Epoch [82/120    avg_loss:0.063, val_acc:0.966]
Epoch [83/120    avg_loss:0.066, val_acc:0.964]
Epoch [84/120    avg_loss:0.086, val_acc:0.943]
Epoch [85/120    avg_loss:0.086, val_acc:0.958]
Epoch [86/120    avg_loss:0.078, val_acc:0.957]
Epoch [87/120    avg_loss:0.057, val_acc:0.957]
Epoch [88/120    avg_loss:0.055, val_acc:0.967]
Epoch [89/120    avg_loss:0.054, val_acc:0.952]
Epoch [90/120    avg_loss:0.063, val_acc:0.964]
Epoch [91/120    avg_loss:0.040, val_acc:0.974]
Epoch [92/120    avg_loss:0.041, val_acc:0.968]
Epoch [93/120    avg_loss:0.052, val_acc:0.958]
Epoch [94/120    avg_loss:0.040, val_acc:0.966]
Epoch [95/120    avg_loss:0.053, val_acc:0.960]
Epoch [96/120    avg_loss:0.049, val_acc:0.958]
Epoch [97/120    avg_loss:0.095, val_acc:0.951]
Epoch [98/120    avg_loss:0.064, val_acc:0.960]
Epoch [99/120    avg_loss:0.060, val_acc:0.957]
Epoch [100/120    avg_loss:0.052, val_acc:0.967]
Epoch [101/120    avg_loss:0.047, val_acc:0.967]
Epoch [102/120    avg_loss:0.042, val_acc:0.958]
Epoch [103/120    avg_loss:0.039, val_acc:0.963]
Epoch [104/120    avg_loss:0.046, val_acc:0.963]
Epoch [105/120    avg_loss:0.039, val_acc:0.964]
Epoch [106/120    avg_loss:0.033, val_acc:0.966]
Epoch [107/120    avg_loss:0.030, val_acc:0.967]
Epoch [108/120    avg_loss:0.026, val_acc:0.965]
Epoch [109/120    avg_loss:0.028, val_acc:0.968]
Epoch [110/120    avg_loss:0.025, val_acc:0.969]
Epoch [111/120    avg_loss:0.027, val_acc:0.972]
Epoch [112/120    avg_loss:0.027, val_acc:0.974]
Epoch [113/120    avg_loss:0.027, val_acc:0.973]
Epoch [114/120    avg_loss:0.025, val_acc:0.972]
Epoch [115/120    avg_loss:0.022, val_acc:0.974]
Epoch [116/120    avg_loss:0.031, val_acc:0.972]
Epoch [117/120    avg_loss:0.026, val_acc:0.974]
Epoch [118/120    avg_loss:0.026, val_acc:0.974]
Epoch [119/120    avg_loss:0.022, val_acc:0.973]
Epoch [120/120    avg_loss:0.030, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1192    0    0    0    8    0    0    0    9   72    0    0
     0    4    0]
 [   0    0    3  687    1   11    0    0    0   14    0    0   31    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    1    0    7    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    4    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  425    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0   23   90    0    6    0    0    0    0  747    4    0    0
     0    5    0]
 [   0    0    9    0    0    1    7    0    0    0    5 2180    4    4
     0    0    0]
 [   0    0    0   32    6   11    0    0    0    0    9    0  467    0
     0    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    1    0    2    2    0    0
  1132    0    0]
 [   0    0    0    0    0    1   60    0    0    3    0    0    3    0
    70  210    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
94.02710027100271

F1 scores:
[       nan 0.90243902 0.94904459 0.88303342 0.98383372 0.95525727
 0.94143167 0.98039216 0.99299065 0.52631579 0.90490612 0.97495528
 0.89292543 0.98930481 0.96628254 0.74204947 0.93103448]

Kappa:
0.9318349007158979
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6990456860>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.709, val_acc:0.352]
Epoch [2/120    avg_loss:2.481, val_acc:0.511]
Epoch [3/120    avg_loss:2.304, val_acc:0.448]
Epoch [4/120    avg_loss:2.216, val_acc:0.493]
Epoch [5/120    avg_loss:2.097, val_acc:0.544]
Epoch [6/120    avg_loss:2.027, val_acc:0.573]
Epoch [7/120    avg_loss:1.977, val_acc:0.582]
Epoch [8/120    avg_loss:1.873, val_acc:0.605]
Epoch [9/120    avg_loss:1.776, val_acc:0.633]
Epoch [10/120    avg_loss:1.678, val_acc:0.609]
Epoch [11/120    avg_loss:1.592, val_acc:0.655]
Epoch [12/120    avg_loss:1.515, val_acc:0.670]
Epoch [13/120    avg_loss:1.442, val_acc:0.660]
Epoch [14/120    avg_loss:1.310, val_acc:0.662]
Epoch [15/120    avg_loss:1.189, val_acc:0.682]
Epoch [16/120    avg_loss:1.076, val_acc:0.693]
Epoch [17/120    avg_loss:1.000, val_acc:0.723]
Epoch [18/120    avg_loss:1.020, val_acc:0.703]
Epoch [19/120    avg_loss:0.953, val_acc:0.724]
Epoch [20/120    avg_loss:0.830, val_acc:0.744]
Epoch [21/120    avg_loss:0.814, val_acc:0.744]
Epoch [22/120    avg_loss:0.756, val_acc:0.755]
Epoch [23/120    avg_loss:0.684, val_acc:0.757]
Epoch [24/120    avg_loss:0.633, val_acc:0.780]
Epoch [25/120    avg_loss:0.629, val_acc:0.817]
Epoch [26/120    avg_loss:0.571, val_acc:0.814]
Epoch [27/120    avg_loss:0.539, val_acc:0.831]
Epoch [28/120    avg_loss:0.524, val_acc:0.831]
Epoch [29/120    avg_loss:0.520, val_acc:0.841]
Epoch [30/120    avg_loss:0.480, val_acc:0.806]
Epoch [31/120    avg_loss:0.440, val_acc:0.859]
Epoch [32/120    avg_loss:0.384, val_acc:0.857]
Epoch [33/120    avg_loss:0.381, val_acc:0.844]
Epoch [34/120    avg_loss:0.416, val_acc:0.810]
Epoch [35/120    avg_loss:0.415, val_acc:0.834]
Epoch [36/120    avg_loss:0.312, val_acc:0.872]
Epoch [37/120    avg_loss:0.317, val_acc:0.858]
Epoch [38/120    avg_loss:0.360, val_acc:0.865]
Epoch [39/120    avg_loss:0.349, val_acc:0.864]
Epoch [40/120    avg_loss:0.341, val_acc:0.864]
Epoch [41/120    avg_loss:0.297, val_acc:0.882]
Epoch [42/120    avg_loss:0.260, val_acc:0.892]
Epoch [43/120    avg_loss:0.226, val_acc:0.892]
Epoch [44/120    avg_loss:0.241, val_acc:0.883]
Epoch [45/120    avg_loss:0.215, val_acc:0.891]
Epoch [46/120    avg_loss:0.210, val_acc:0.905]
Epoch [47/120    avg_loss:0.194, val_acc:0.914]
Epoch [48/120    avg_loss:0.225, val_acc:0.906]
Epoch [49/120    avg_loss:0.198, val_acc:0.917]
Epoch [50/120    avg_loss:0.292, val_acc:0.889]
Epoch [51/120    avg_loss:0.253, val_acc:0.909]
Epoch [52/120    avg_loss:0.239, val_acc:0.920]
Epoch [53/120    avg_loss:0.169, val_acc:0.910]
Epoch [54/120    avg_loss:0.167, val_acc:0.917]
Epoch [55/120    avg_loss:0.147, val_acc:0.922]
Epoch [56/120    avg_loss:0.187, val_acc:0.907]
Epoch [57/120    avg_loss:0.141, val_acc:0.907]
Epoch [58/120    avg_loss:0.166, val_acc:0.914]
Epoch [59/120    avg_loss:0.137, val_acc:0.923]
Epoch [60/120    avg_loss:0.133, val_acc:0.915]
Epoch [61/120    avg_loss:0.146, val_acc:0.934]
Epoch [62/120    avg_loss:0.119, val_acc:0.920]
Epoch [63/120    avg_loss:0.138, val_acc:0.934]
Epoch [64/120    avg_loss:0.110, val_acc:0.928]
Epoch [65/120    avg_loss:0.099, val_acc:0.924]
Epoch [66/120    avg_loss:0.097, val_acc:0.940]
Epoch [67/120    avg_loss:0.091, val_acc:0.935]
Epoch [68/120    avg_loss:0.094, val_acc:0.927]
Epoch [69/120    avg_loss:0.088, val_acc:0.934]
Epoch [70/120    avg_loss:0.080, val_acc:0.932]
Epoch [71/120    avg_loss:0.098, val_acc:0.933]
Epoch [72/120    avg_loss:0.089, val_acc:0.932]
Epoch [73/120    avg_loss:0.086, val_acc:0.927]
Epoch [74/120    avg_loss:0.084, val_acc:0.939]
Epoch [75/120    avg_loss:0.070, val_acc:0.933]
Epoch [76/120    avg_loss:0.067, val_acc:0.953]
Epoch [77/120    avg_loss:0.084, val_acc:0.928]
Epoch [78/120    avg_loss:0.078, val_acc:0.941]
Epoch [79/120    avg_loss:0.067, val_acc:0.943]
Epoch [80/120    avg_loss:0.067, val_acc:0.941]
Epoch [81/120    avg_loss:0.064, val_acc:0.948]
Epoch [82/120    avg_loss:0.141, val_acc:0.884]
Epoch [83/120    avg_loss:0.188, val_acc:0.917]
Epoch [84/120    avg_loss:0.136, val_acc:0.920]
Epoch [85/120    avg_loss:0.112, val_acc:0.923]
Epoch [86/120    avg_loss:0.109, val_acc:0.926]
Epoch [87/120    avg_loss:0.136, val_acc:0.923]
Epoch [88/120    avg_loss:0.156, val_acc:0.899]
Epoch [89/120    avg_loss:0.116, val_acc:0.933]
Epoch [90/120    avg_loss:0.085, val_acc:0.933]
Epoch [91/120    avg_loss:0.086, val_acc:0.939]
Epoch [92/120    avg_loss:0.070, val_acc:0.938]
Epoch [93/120    avg_loss:0.073, val_acc:0.939]
Epoch [94/120    avg_loss:0.072, val_acc:0.939]
Epoch [95/120    avg_loss:0.067, val_acc:0.941]
Epoch [96/120    avg_loss:0.074, val_acc:0.941]
Epoch [97/120    avg_loss:0.057, val_acc:0.940]
Epoch [98/120    avg_loss:0.065, val_acc:0.939]
Epoch [99/120    avg_loss:0.060, val_acc:0.942]
Epoch [100/120    avg_loss:0.066, val_acc:0.940]
Epoch [101/120    avg_loss:0.057, val_acc:0.941]
Epoch [102/120    avg_loss:0.054, val_acc:0.942]
Epoch [103/120    avg_loss:0.057, val_acc:0.943]
Epoch [104/120    avg_loss:0.061, val_acc:0.943]
Epoch [105/120    avg_loss:0.057, val_acc:0.943]
Epoch [106/120    avg_loss:0.059, val_acc:0.943]
Epoch [107/120    avg_loss:0.058, val_acc:0.943]
Epoch [108/120    avg_loss:0.054, val_acc:0.943]
Epoch [109/120    avg_loss:0.061, val_acc:0.943]
Epoch [110/120    avg_loss:0.054, val_acc:0.943]
Epoch [111/120    avg_loss:0.066, val_acc:0.943]
Epoch [112/120    avg_loss:0.059, val_acc:0.943]
Epoch [113/120    avg_loss:0.056, val_acc:0.943]
Epoch [114/120    avg_loss:0.058, val_acc:0.943]
Epoch [115/120    avg_loss:0.067, val_acc:0.943]
Epoch [116/120    avg_loss:0.054, val_acc:0.943]
Epoch [117/120    avg_loss:0.061, val_acc:0.943]
Epoch [118/120    avg_loss:0.060, val_acc:0.943]
Epoch [119/120    avg_loss:0.065, val_acc:0.943]
Epoch [120/120    avg_loss:0.054, val_acc:0.943]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    4 1178    5    0    0    1    0    0    1    5   84    6    0
     1    0    0]
 [   0    0    2  698    1   10    0    0    0   21    0    0    5   10
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    5    0    3    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  651    0    0    1    0    3    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   13    0    0    2    0
     0    0    0]
 [   0    0   52   89    0    9    0    0    0    0  713    4    4    0
     4    0    0]
 [   0    0   15    1    0    2   17    0    7    0   13 2141    4    2
     8    0    0]
 [   0    0    5   11    2    3    0    0    0    0   10    1  486    0
     0    0   16]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   14    0    0    1    0    4    1    1    0
  1118    0    0]
 [   0    0    0    0    0    0    3    0    0    9    0    0    0    0
   133  202    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
93.19241192411924

F1 scores:
[       nan 0.90243902 0.92865589 0.8977492  0.99065421 0.94537347
 0.97968397 0.90909091 0.99078341 0.39393939 0.87807882 0.96354635
 0.9328215  0.96858639 0.92857143 0.73588342 0.91304348]

Kappa:
0.9223253801930833
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f40c01c8898>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.781, val_acc:0.323]
Epoch [2/120    avg_loss:2.523, val_acc:0.397]
Epoch [3/120    avg_loss:2.337, val_acc:0.457]
Epoch [4/120    avg_loss:2.235, val_acc:0.489]
Epoch [5/120    avg_loss:2.110, val_acc:0.481]
Epoch [6/120    avg_loss:1.995, val_acc:0.497]
Epoch [7/120    avg_loss:1.926, val_acc:0.530]
Epoch [8/120    avg_loss:1.836, val_acc:0.533]
Epoch [9/120    avg_loss:1.739, val_acc:0.607]
Epoch [10/120    avg_loss:1.668, val_acc:0.606]
Epoch [11/120    avg_loss:1.542, val_acc:0.650]
Epoch [12/120    avg_loss:1.427, val_acc:0.647]
Epoch [13/120    avg_loss:1.283, val_acc:0.658]
Epoch [14/120    avg_loss:1.221, val_acc:0.664]
Epoch [15/120    avg_loss:1.123, val_acc:0.634]
Epoch [16/120    avg_loss:1.049, val_acc:0.693]
Epoch [17/120    avg_loss:0.951, val_acc:0.701]
Epoch [18/120    avg_loss:0.935, val_acc:0.691]
Epoch [19/120    avg_loss:0.873, val_acc:0.718]
Epoch [20/120    avg_loss:0.809, val_acc:0.730]
Epoch [21/120    avg_loss:0.761, val_acc:0.757]
Epoch [22/120    avg_loss:0.675, val_acc:0.764]
Epoch [23/120    avg_loss:0.633, val_acc:0.783]
Epoch [24/120    avg_loss:0.636, val_acc:0.803]
Epoch [25/120    avg_loss:0.604, val_acc:0.799]
Epoch [26/120    avg_loss:0.517, val_acc:0.826]
Epoch [27/120    avg_loss:0.535, val_acc:0.760]
Epoch [28/120    avg_loss:0.585, val_acc:0.784]
Epoch [29/120    avg_loss:0.543, val_acc:0.818]
Epoch [30/120    avg_loss:0.470, val_acc:0.824]
Epoch [31/120    avg_loss:0.428, val_acc:0.828]
Epoch [32/120    avg_loss:0.439, val_acc:0.825]
Epoch [33/120    avg_loss:0.397, val_acc:0.859]
Epoch [34/120    avg_loss:0.379, val_acc:0.850]
Epoch [35/120    avg_loss:0.333, val_acc:0.855]
Epoch [36/120    avg_loss:0.322, val_acc:0.856]
Epoch [37/120    avg_loss:0.314, val_acc:0.848]
Epoch [38/120    avg_loss:0.305, val_acc:0.847]
Epoch [39/120    avg_loss:0.287, val_acc:0.877]
Epoch [40/120    avg_loss:0.268, val_acc:0.884]
Epoch [41/120    avg_loss:0.258, val_acc:0.875]
Epoch [42/120    avg_loss:0.235, val_acc:0.906]
Epoch [43/120    avg_loss:0.216, val_acc:0.911]
Epoch [44/120    avg_loss:0.245, val_acc:0.897]
Epoch [45/120    avg_loss:0.245, val_acc:0.881]
Epoch [46/120    avg_loss:0.265, val_acc:0.892]
Epoch [47/120    avg_loss:0.231, val_acc:0.907]
Epoch [48/120    avg_loss:0.193, val_acc:0.890]
Epoch [49/120    avg_loss:0.181, val_acc:0.919]
Epoch [50/120    avg_loss:0.185, val_acc:0.925]
Epoch [51/120    avg_loss:0.165, val_acc:0.912]
Epoch [52/120    avg_loss:0.162, val_acc:0.917]
Epoch [53/120    avg_loss:0.168, val_acc:0.912]
Epoch [54/120    avg_loss:0.169, val_acc:0.912]
Epoch [55/120    avg_loss:0.230, val_acc:0.870]
Epoch [56/120    avg_loss:0.246, val_acc:0.894]
Epoch [57/120    avg_loss:0.186, val_acc:0.923]
Epoch [58/120    avg_loss:0.160, val_acc:0.924]
Epoch [59/120    avg_loss:0.158, val_acc:0.924]
Epoch [60/120    avg_loss:0.146, val_acc:0.920]
Epoch [61/120    avg_loss:0.175, val_acc:0.925]
Epoch [62/120    avg_loss:0.131, val_acc:0.924]
Epoch [63/120    avg_loss:0.129, val_acc:0.930]
Epoch [64/120    avg_loss:0.155, val_acc:0.934]
Epoch [65/120    avg_loss:0.140, val_acc:0.925]
Epoch [66/120    avg_loss:0.127, val_acc:0.938]
Epoch [67/120    avg_loss:0.113, val_acc:0.936]
Epoch [68/120    avg_loss:0.136, val_acc:0.931]
Epoch [69/120    avg_loss:0.112, val_acc:0.934]
Epoch [70/120    avg_loss:0.104, val_acc:0.938]
Epoch [71/120    avg_loss:0.095, val_acc:0.938]
Epoch [72/120    avg_loss:0.093, val_acc:0.935]
Epoch [73/120    avg_loss:0.079, val_acc:0.955]
Epoch [74/120    avg_loss:0.088, val_acc:0.942]
Epoch [75/120    avg_loss:0.078, val_acc:0.931]
Epoch [76/120    avg_loss:0.073, val_acc:0.947]
Epoch [77/120    avg_loss:0.084, val_acc:0.940]
Epoch [78/120    avg_loss:0.103, val_acc:0.941]
Epoch [79/120    avg_loss:0.093, val_acc:0.947]
Epoch [80/120    avg_loss:0.096, val_acc:0.934]
Epoch [81/120    avg_loss:0.100, val_acc:0.941]
Epoch [82/120    avg_loss:0.095, val_acc:0.950]
Epoch [83/120    avg_loss:0.080, val_acc:0.950]
Epoch [84/120    avg_loss:0.083, val_acc:0.936]
Epoch [85/120    avg_loss:0.071, val_acc:0.944]
Epoch [86/120    avg_loss:0.132, val_acc:0.910]
Epoch [87/120    avg_loss:0.116, val_acc:0.932]
Epoch [88/120    avg_loss:0.084, val_acc:0.944]
Epoch [89/120    avg_loss:0.060, val_acc:0.944]
Epoch [90/120    avg_loss:0.062, val_acc:0.950]
Epoch [91/120    avg_loss:0.061, val_acc:0.951]
Epoch [92/120    avg_loss:0.051, val_acc:0.953]
Epoch [93/120    avg_loss:0.054, val_acc:0.956]
Epoch [94/120    avg_loss:0.054, val_acc:0.956]
Epoch [95/120    avg_loss:0.046, val_acc:0.958]
Epoch [96/120    avg_loss:0.052, val_acc:0.957]
Epoch [97/120    avg_loss:0.060, val_acc:0.960]
Epoch [98/120    avg_loss:0.060, val_acc:0.959]
Epoch [99/120    avg_loss:0.051, val_acc:0.963]
Epoch [100/120    avg_loss:0.048, val_acc:0.963]
Epoch [101/120    avg_loss:0.052, val_acc:0.963]
Epoch [102/120    avg_loss:0.048, val_acc:0.963]
Epoch [103/120    avg_loss:0.047, val_acc:0.960]
Epoch [104/120    avg_loss:0.044, val_acc:0.958]
Epoch [105/120    avg_loss:0.042, val_acc:0.960]
Epoch [106/120    avg_loss:0.043, val_acc:0.960]
Epoch [107/120    avg_loss:0.044, val_acc:0.963]
Epoch [108/120    avg_loss:0.046, val_acc:0.960]
Epoch [109/120    avg_loss:0.045, val_acc:0.963]
Epoch [110/120    avg_loss:0.045, val_acc:0.961]
Epoch [111/120    avg_loss:0.042, val_acc:0.960]
Epoch [112/120    avg_loss:0.045, val_acc:0.964]
Epoch [113/120    avg_loss:0.053, val_acc:0.959]
Epoch [114/120    avg_loss:0.047, val_acc:0.960]
Epoch [115/120    avg_loss:0.042, val_acc:0.965]
Epoch [116/120    avg_loss:0.047, val_acc:0.966]
Epoch [117/120    avg_loss:0.050, val_acc:0.963]
Epoch [118/120    avg_loss:0.041, val_acc:0.959]
Epoch [119/120    avg_loss:0.037, val_acc:0.964]
Epoch [120/120    avg_loss:0.041, val_acc:0.963]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1204    4    0    0    1    0    0    0   29   44    1    0
     0    2    0]
 [   0    0    0  692    1    0    0    0    0    7    0    0   42    1
     0    4    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    3    0    4    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    3    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   36   90    0    7    0    0    0    0  738    1    0    0
     0    3    0]
 [   0    0   25    0    0    1    9    0    0    0   15 2144    0    4
    12    0    0]
 [   0    0    0   10   10   11    0    0    0    0    6    4  484    0
     1    2    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    1    0    3    2    2    0
  1127    0    0]
 [   0    0    0    0    0    0   47    0    0    0    3    0    0   23
    81  193    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
93.73441734417344

F1 scores:
[       nan 0.975      0.94431373 0.89695399 0.97482838 0.96145125
 0.95307918 0.94339623 0.99883856 0.73913043 0.88330341 0.97277677
 0.90806754 0.92964824 0.95185811 0.70054446 0.95348837]

Kappa:
0.9285536408459238
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd81795e898>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.777, val_acc:0.277]
Epoch [2/120    avg_loss:2.557, val_acc:0.439]
Epoch [3/120    avg_loss:2.363, val_acc:0.498]
Epoch [4/120    avg_loss:2.227, val_acc:0.551]
Epoch [5/120    avg_loss:2.123, val_acc:0.561]
Epoch [6/120    avg_loss:2.037, val_acc:0.582]
Epoch [7/120    avg_loss:1.946, val_acc:0.569]
Epoch [8/120    avg_loss:1.881, val_acc:0.616]
Epoch [9/120    avg_loss:1.738, val_acc:0.623]
Epoch [10/120    avg_loss:1.626, val_acc:0.658]
Epoch [11/120    avg_loss:1.520, val_acc:0.665]
Epoch [12/120    avg_loss:1.389, val_acc:0.656]
Epoch [13/120    avg_loss:1.301, val_acc:0.692]
Epoch [14/120    avg_loss:1.177, val_acc:0.682]
Epoch [15/120    avg_loss:1.083, val_acc:0.700]
Epoch [16/120    avg_loss:0.982, val_acc:0.723]
Epoch [17/120    avg_loss:0.900, val_acc:0.752]
Epoch [18/120    avg_loss:0.853, val_acc:0.794]
Epoch [19/120    avg_loss:0.821, val_acc:0.799]
Epoch [20/120    avg_loss:0.793, val_acc:0.773]
Epoch [21/120    avg_loss:0.723, val_acc:0.794]
Epoch [22/120    avg_loss:0.633, val_acc:0.811]
Epoch [23/120    avg_loss:0.622, val_acc:0.791]
Epoch [24/120    avg_loss:0.588, val_acc:0.818]
Epoch [25/120    avg_loss:0.578, val_acc:0.844]
Epoch [26/120    avg_loss:0.514, val_acc:0.853]
Epoch [27/120    avg_loss:0.469, val_acc:0.873]
Epoch [28/120    avg_loss:0.426, val_acc:0.873]
Epoch [29/120    avg_loss:0.381, val_acc:0.867]
Epoch [30/120    avg_loss:0.467, val_acc:0.866]
Epoch [31/120    avg_loss:0.449, val_acc:0.860]
Epoch [32/120    avg_loss:0.406, val_acc:0.887]
Epoch [33/120    avg_loss:0.356, val_acc:0.858]
Epoch [34/120    avg_loss:0.326, val_acc:0.893]
Epoch [35/120    avg_loss:0.278, val_acc:0.895]
Epoch [36/120    avg_loss:0.293, val_acc:0.906]
Epoch [37/120    avg_loss:0.259, val_acc:0.911]
Epoch [38/120    avg_loss:0.274, val_acc:0.863]
Epoch [39/120    avg_loss:0.309, val_acc:0.903]
Epoch [40/120    avg_loss:0.248, val_acc:0.906]
Epoch [41/120    avg_loss:0.238, val_acc:0.930]
Epoch [42/120    avg_loss:0.226, val_acc:0.897]
Epoch [43/120    avg_loss:0.257, val_acc:0.920]
Epoch [44/120    avg_loss:0.223, val_acc:0.925]
Epoch [45/120    avg_loss:0.190, val_acc:0.931]
Epoch [46/120    avg_loss:0.198, val_acc:0.923]
Epoch [47/120    avg_loss:0.192, val_acc:0.936]
Epoch [48/120    avg_loss:0.155, val_acc:0.940]
Epoch [49/120    avg_loss:0.169, val_acc:0.935]
Epoch [50/120    avg_loss:0.147, val_acc:0.941]
Epoch [51/120    avg_loss:0.141, val_acc:0.911]
Epoch [52/120    avg_loss:0.207, val_acc:0.925]
Epoch [53/120    avg_loss:0.411, val_acc:0.860]
Epoch [54/120    avg_loss:0.293, val_acc:0.908]
Epoch [55/120    avg_loss:0.197, val_acc:0.935]
Epoch [56/120    avg_loss:0.157, val_acc:0.930]
Epoch [57/120    avg_loss:0.133, val_acc:0.934]
Epoch [58/120    avg_loss:0.126, val_acc:0.951]
Epoch [59/120    avg_loss:0.152, val_acc:0.933]
Epoch [60/120    avg_loss:0.154, val_acc:0.950]
Epoch [61/120    avg_loss:0.132, val_acc:0.950]
Epoch [62/120    avg_loss:0.119, val_acc:0.956]
Epoch [63/120    avg_loss:0.116, val_acc:0.957]
Epoch [64/120    avg_loss:0.106, val_acc:0.957]
Epoch [65/120    avg_loss:0.096, val_acc:0.964]
Epoch [66/120    avg_loss:0.089, val_acc:0.967]
Epoch [67/120    avg_loss:0.091, val_acc:0.949]
Epoch [68/120    avg_loss:0.092, val_acc:0.953]
Epoch [69/120    avg_loss:0.114, val_acc:0.944]
Epoch [70/120    avg_loss:0.121, val_acc:0.938]
Epoch [71/120    avg_loss:0.147, val_acc:0.948]
Epoch [72/120    avg_loss:0.099, val_acc:0.951]
Epoch [73/120    avg_loss:0.084, val_acc:0.956]
Epoch [74/120    avg_loss:0.092, val_acc:0.939]
Epoch [75/120    avg_loss:0.128, val_acc:0.947]
Epoch [76/120    avg_loss:0.144, val_acc:0.942]
Epoch [77/120    avg_loss:0.161, val_acc:0.922]
Epoch [78/120    avg_loss:0.089, val_acc:0.961]
Epoch [79/120    avg_loss:0.103, val_acc:0.945]
Epoch [80/120    avg_loss:0.076, val_acc:0.956]
Epoch [81/120    avg_loss:0.060, val_acc:0.966]
Epoch [82/120    avg_loss:0.059, val_acc:0.966]
Epoch [83/120    avg_loss:0.057, val_acc:0.967]
Epoch [84/120    avg_loss:0.061, val_acc:0.967]
Epoch [85/120    avg_loss:0.054, val_acc:0.968]
Epoch [86/120    avg_loss:0.055, val_acc:0.966]
Epoch [87/120    avg_loss:0.054, val_acc:0.968]
Epoch [88/120    avg_loss:0.050, val_acc:0.970]
Epoch [89/120    avg_loss:0.051, val_acc:0.973]
Epoch [90/120    avg_loss:0.055, val_acc:0.969]
Epoch [91/120    avg_loss:0.049, val_acc:0.969]
Epoch [92/120    avg_loss:0.049, val_acc:0.973]
Epoch [93/120    avg_loss:0.052, val_acc:0.974]
Epoch [94/120    avg_loss:0.048, val_acc:0.974]
Epoch [95/120    avg_loss:0.048, val_acc:0.972]
Epoch [96/120    avg_loss:0.046, val_acc:0.973]
Epoch [97/120    avg_loss:0.046, val_acc:0.973]
Epoch [98/120    avg_loss:0.048, val_acc:0.973]
Epoch [99/120    avg_loss:0.044, val_acc:0.969]
Epoch [100/120    avg_loss:0.045, val_acc:0.973]
Epoch [101/120    avg_loss:0.050, val_acc:0.974]
Epoch [102/120    avg_loss:0.048, val_acc:0.974]
Epoch [103/120    avg_loss:0.045, val_acc:0.972]
Epoch [104/120    avg_loss:0.042, val_acc:0.973]
Epoch [105/120    avg_loss:0.046, val_acc:0.973]
Epoch [106/120    avg_loss:0.044, val_acc:0.973]
Epoch [107/120    avg_loss:0.041, val_acc:0.973]
Epoch [108/120    avg_loss:0.040, val_acc:0.975]
Epoch [109/120    avg_loss:0.038, val_acc:0.975]
Epoch [110/120    avg_loss:0.046, val_acc:0.975]
Epoch [111/120    avg_loss:0.043, val_acc:0.974]
Epoch [112/120    avg_loss:0.041, val_acc:0.972]
Epoch [113/120    avg_loss:0.040, val_acc:0.973]
Epoch [114/120    avg_loss:0.041, val_acc:0.973]
Epoch [115/120    avg_loss:0.042, val_acc:0.974]
Epoch [116/120    avg_loss:0.040, val_acc:0.974]
Epoch [117/120    avg_loss:0.044, val_acc:0.974]
Epoch [118/120    avg_loss:0.036, val_acc:0.974]
Epoch [119/120    avg_loss:0.037, val_acc:0.973]
Epoch [120/120    avg_loss:0.035, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1219    3    0    0    1    0    0    6   11   40    3    0
     0    2    0]
 [   0    0    4  712    2    0    0    0    0   14    0    0   12    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    3    0    4    0    0    0    0
     5    0    0]
 [   0    0    0    0    1    0  651    0    0    0    0    3    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0   58   90    0    8    0    0    0    0  711    5    0    0
     0    3    0]
 [   0    0   33    0    0    1   17    0    0    0    7 2136   10    5
     1    0    0]
 [   0    0    0    1    3   10    0    0    0    0    8    7  493    0
     0    0   12]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    4    2    0    0
  1132    0    0]
 [   0    0    0    0    0    0    1    0    0    2    0    0    0    0
    64  280    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.79674796747967

F1 scores:
[       nan 0.93975904 0.9380531  0.91693496 0.98611111 0.96465222
 0.98116051 0.94339623 0.995338   0.50847458 0.87886279 0.97024756
 0.93459716 0.97883598 0.96628254 0.88607595 0.93333333]

Kappa:
0.9406870473276776
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb4356fc898>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.763, val_acc:0.273]
Epoch [2/120    avg_loss:2.519, val_acc:0.375]
Epoch [3/120    avg_loss:2.345, val_acc:0.578]
Epoch [4/120    avg_loss:2.209, val_acc:0.543]
Epoch [5/120    avg_loss:2.125, val_acc:0.530]
Epoch [6/120    avg_loss:2.034, val_acc:0.558]
Epoch [7/120    avg_loss:1.960, val_acc:0.611]
Epoch [8/120    avg_loss:1.876, val_acc:0.627]
Epoch [9/120    avg_loss:1.799, val_acc:0.635]
Epoch [10/120    avg_loss:1.714, val_acc:0.628]
Epoch [11/120    avg_loss:1.587, val_acc:0.647]
Epoch [12/120    avg_loss:1.516, val_acc:0.649]
Epoch [13/120    avg_loss:1.417, val_acc:0.657]
Epoch [14/120    avg_loss:1.346, val_acc:0.675]
Epoch [15/120    avg_loss:1.254, val_acc:0.693]
Epoch [16/120    avg_loss:1.141, val_acc:0.725]
Epoch [17/120    avg_loss:1.088, val_acc:0.736]
Epoch [18/120    avg_loss:0.982, val_acc:0.703]
Epoch [19/120    avg_loss:0.965, val_acc:0.768]
Epoch [20/120    avg_loss:0.850, val_acc:0.802]
Epoch [21/120    avg_loss:0.798, val_acc:0.797]
Epoch [22/120    avg_loss:0.754, val_acc:0.814]
Epoch [23/120    avg_loss:0.686, val_acc:0.801]
Epoch [24/120    avg_loss:0.622, val_acc:0.850]
Epoch [25/120    avg_loss:0.573, val_acc:0.844]
Epoch [26/120    avg_loss:0.531, val_acc:0.873]
Epoch [27/120    avg_loss:0.480, val_acc:0.856]
Epoch [28/120    avg_loss:0.519, val_acc:0.855]
Epoch [29/120    avg_loss:0.440, val_acc:0.857]
Epoch [30/120    avg_loss:0.433, val_acc:0.857]
Epoch [31/120    avg_loss:0.470, val_acc:0.851]
Epoch [32/120    avg_loss:0.435, val_acc:0.866]
Epoch [33/120    avg_loss:0.376, val_acc:0.880]
Epoch [34/120    avg_loss:0.390, val_acc:0.876]
Epoch [35/120    avg_loss:0.333, val_acc:0.870]
Epoch [36/120    avg_loss:0.296, val_acc:0.858]
Epoch [37/120    avg_loss:0.297, val_acc:0.899]
Epoch [38/120    avg_loss:0.280, val_acc:0.895]
Epoch [39/120    avg_loss:0.263, val_acc:0.909]
Epoch [40/120    avg_loss:0.238, val_acc:0.898]
Epoch [41/120    avg_loss:0.204, val_acc:0.915]
Epoch [42/120    avg_loss:0.206, val_acc:0.908]
Epoch [43/120    avg_loss:0.205, val_acc:0.924]
Epoch [44/120    avg_loss:0.194, val_acc:0.916]
Epoch [45/120    avg_loss:0.208, val_acc:0.895]
Epoch [46/120    avg_loss:0.189, val_acc:0.918]
Epoch [47/120    avg_loss:0.163, val_acc:0.926]
Epoch [48/120    avg_loss:0.172, val_acc:0.930]
Epoch [49/120    avg_loss:0.154, val_acc:0.930]
Epoch [50/120    avg_loss:0.140, val_acc:0.926]
Epoch [51/120    avg_loss:0.174, val_acc:0.924]
Epoch [52/120    avg_loss:0.147, val_acc:0.915]
Epoch [53/120    avg_loss:0.144, val_acc:0.923]
Epoch [54/120    avg_loss:0.154, val_acc:0.926]
Epoch [55/120    avg_loss:0.144, val_acc:0.934]
Epoch [56/120    avg_loss:0.158, val_acc:0.932]
Epoch [57/120    avg_loss:0.151, val_acc:0.931]
Epoch [58/120    avg_loss:0.268, val_acc:0.912]
Epoch [59/120    avg_loss:0.170, val_acc:0.928]
Epoch [60/120    avg_loss:0.146, val_acc:0.886]
Epoch [61/120    avg_loss:0.166, val_acc:0.924]
Epoch [62/120    avg_loss:0.157, val_acc:0.917]
Epoch [63/120    avg_loss:0.176, val_acc:0.924]
Epoch [64/120    avg_loss:0.126, val_acc:0.924]
Epoch [65/120    avg_loss:0.105, val_acc:0.939]
Epoch [66/120    avg_loss:0.092, val_acc:0.949]
Epoch [67/120    avg_loss:0.104, val_acc:0.943]
Epoch [68/120    avg_loss:0.077, val_acc:0.951]
Epoch [69/120    avg_loss:0.084, val_acc:0.948]
Epoch [70/120    avg_loss:0.098, val_acc:0.947]
Epoch [71/120    avg_loss:0.103, val_acc:0.947]
Epoch [72/120    avg_loss:0.090, val_acc:0.943]
Epoch [73/120    avg_loss:0.073, val_acc:0.945]
Epoch [74/120    avg_loss:0.082, val_acc:0.941]
Epoch [75/120    avg_loss:0.116, val_acc:0.938]
Epoch [76/120    avg_loss:0.115, val_acc:0.935]
Epoch [77/120    avg_loss:0.093, val_acc:0.950]
Epoch [78/120    avg_loss:0.072, val_acc:0.956]
Epoch [79/120    avg_loss:0.066, val_acc:0.951]
Epoch [80/120    avg_loss:0.071, val_acc:0.949]
Epoch [81/120    avg_loss:0.070, val_acc:0.941]
Epoch [82/120    avg_loss:0.056, val_acc:0.951]
Epoch [83/120    avg_loss:0.055, val_acc:0.956]
Epoch [84/120    avg_loss:0.073, val_acc:0.957]
Epoch [85/120    avg_loss:0.049, val_acc:0.961]
Epoch [86/120    avg_loss:0.057, val_acc:0.957]
Epoch [87/120    avg_loss:0.056, val_acc:0.958]
Epoch [88/120    avg_loss:0.058, val_acc:0.958]
Epoch [89/120    avg_loss:0.062, val_acc:0.952]
Epoch [90/120    avg_loss:0.040, val_acc:0.956]
Epoch [91/120    avg_loss:0.049, val_acc:0.961]
Epoch [92/120    avg_loss:0.044, val_acc:0.964]
Epoch [93/120    avg_loss:0.054, val_acc:0.966]
Epoch [94/120    avg_loss:0.036, val_acc:0.964]
Epoch [95/120    avg_loss:0.043, val_acc:0.957]
Epoch [96/120    avg_loss:0.057, val_acc:0.959]
Epoch [97/120    avg_loss:0.035, val_acc:0.960]
Epoch [98/120    avg_loss:0.040, val_acc:0.955]
Epoch [99/120    avg_loss:0.045, val_acc:0.953]
Epoch [100/120    avg_loss:0.043, val_acc:0.968]
Epoch [101/120    avg_loss:0.041, val_acc:0.970]
Epoch [102/120    avg_loss:0.039, val_acc:0.965]
Epoch [103/120    avg_loss:0.036, val_acc:0.970]
Epoch [104/120    avg_loss:0.045, val_acc:0.960]
Epoch [105/120    avg_loss:0.052, val_acc:0.952]
Epoch [106/120    avg_loss:0.059, val_acc:0.973]
Epoch [107/120    avg_loss:0.035, val_acc:0.967]
Epoch [108/120    avg_loss:0.039, val_acc:0.965]
Epoch [109/120    avg_loss:0.044, val_acc:0.960]
Epoch [110/120    avg_loss:0.061, val_acc:0.958]
Epoch [111/120    avg_loss:0.057, val_acc:0.949]
Epoch [112/120    avg_loss:0.064, val_acc:0.965]
Epoch [113/120    avg_loss:0.038, val_acc:0.965]
Epoch [114/120    avg_loss:0.041, val_acc:0.968]
Epoch [115/120    avg_loss:0.033, val_acc:0.970]
Epoch [116/120    avg_loss:0.031, val_acc:0.974]
Epoch [117/120    avg_loss:0.028, val_acc:0.968]
Epoch [118/120    avg_loss:0.024, val_acc:0.970]
Epoch [119/120    avg_loss:0.024, val_acc:0.976]
Epoch [120/120    avg_loss:0.028, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1209    3    0    0    9    0    0    0   10   41    3    0
     0    2    8]
 [   0    0    1  707    1   19    0    0    0   13    0    0    1    5
     0    0    0]
 [   0    0    0    2  210    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  417    4    4    0    7    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    4    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   13    0    1    3    0
     0    0    0]
 [   0    0   35   90    0    2    0    0    0    0  722   20    2    0
     2    2    0]
 [   0    0   75    0    0    0   11    0    0    0    0 2116    0    3
     5    0    0]
 [   0    0    0    1    0    0    0    0    0    0   18   21  488    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    4    0    0
  1133    0    0]
 [   0    0    0    0    0    0   32    0    0    2    0    0    0    0
    91  222    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
93.78861788617886

F1 scores:
[       nan 0.98765432 0.92785879 0.91166989 0.99056604 0.95423341
 0.95454545 0.92592593 1.         0.49056604 0.88806888 0.95811637
 0.94573643 0.97883598 0.95410526 0.77486911 0.92307692]

Kappa:
0.9291274779589497
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3fe9094860>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.752, val_acc:0.377]
Epoch [2/120    avg_loss:2.490, val_acc:0.484]
Epoch [3/120    avg_loss:2.341, val_acc:0.495]
Epoch [4/120    avg_loss:2.207, val_acc:0.545]
Epoch [5/120    avg_loss:2.093, val_acc:0.541]
Epoch [6/120    avg_loss:2.012, val_acc:0.583]
Epoch [7/120    avg_loss:1.955, val_acc:0.597]
Epoch [8/120    avg_loss:1.865, val_acc:0.589]
Epoch [9/120    avg_loss:1.760, val_acc:0.602]
Epoch [10/120    avg_loss:1.659, val_acc:0.619]
Epoch [11/120    avg_loss:1.570, val_acc:0.645]
Epoch [12/120    avg_loss:1.472, val_acc:0.653]
Epoch [13/120    avg_loss:1.323, val_acc:0.677]
Epoch [14/120    avg_loss:1.179, val_acc:0.686]
Epoch [15/120    avg_loss:1.163, val_acc:0.689]
Epoch [16/120    avg_loss:1.092, val_acc:0.666]
Epoch [17/120    avg_loss:0.971, val_acc:0.719]
Epoch [18/120    avg_loss:0.924, val_acc:0.747]
Epoch [19/120    avg_loss:0.860, val_acc:0.783]
Epoch [20/120    avg_loss:0.725, val_acc:0.749]
Epoch [21/120    avg_loss:0.735, val_acc:0.787]
Epoch [22/120    avg_loss:0.693, val_acc:0.806]
Epoch [23/120    avg_loss:0.697, val_acc:0.818]
Epoch [24/120    avg_loss:0.565, val_acc:0.833]
Epoch [25/120    avg_loss:0.565, val_acc:0.827]
Epoch [26/120    avg_loss:0.554, val_acc:0.822]
Epoch [27/120    avg_loss:0.532, val_acc:0.818]
Epoch [28/120    avg_loss:0.453, val_acc:0.827]
Epoch [29/120    avg_loss:0.401, val_acc:0.866]
Epoch [30/120    avg_loss:0.414, val_acc:0.856]
Epoch [31/120    avg_loss:0.403, val_acc:0.869]
Epoch [32/120    avg_loss:0.380, val_acc:0.866]
Epoch [33/120    avg_loss:0.376, val_acc:0.894]
Epoch [34/120    avg_loss:0.338, val_acc:0.894]
Epoch [35/120    avg_loss:0.352, val_acc:0.868]
Epoch [36/120    avg_loss:0.329, val_acc:0.886]
Epoch [37/120    avg_loss:0.291, val_acc:0.895]
Epoch [38/120    avg_loss:0.298, val_acc:0.886]
Epoch [39/120    avg_loss:0.266, val_acc:0.890]
Epoch [40/120    avg_loss:0.240, val_acc:0.872]
Epoch [41/120    avg_loss:0.274, val_acc:0.884]
Epoch [42/120    avg_loss:0.281, val_acc:0.909]
Epoch [43/120    avg_loss:0.239, val_acc:0.915]
Epoch [44/120    avg_loss:0.232, val_acc:0.906]
Epoch [45/120    avg_loss:0.202, val_acc:0.891]
Epoch [46/120    avg_loss:0.178, val_acc:0.920]
Epoch [47/120    avg_loss:0.170, val_acc:0.922]
Epoch [48/120    avg_loss:0.187, val_acc:0.918]
Epoch [49/120    avg_loss:0.185, val_acc:0.928]
Epoch [50/120    avg_loss:0.183, val_acc:0.919]
Epoch [51/120    avg_loss:0.186, val_acc:0.907]
Epoch [52/120    avg_loss:0.196, val_acc:0.916]
Epoch [53/120    avg_loss:0.198, val_acc:0.917]
Epoch [54/120    avg_loss:0.155, val_acc:0.915]
Epoch [55/120    avg_loss:0.151, val_acc:0.919]
Epoch [56/120    avg_loss:0.146, val_acc:0.940]
Epoch [57/120    avg_loss:0.129, val_acc:0.930]
Epoch [58/120    avg_loss:0.129, val_acc:0.938]
Epoch [59/120    avg_loss:0.104, val_acc:0.940]
Epoch [60/120    avg_loss:0.122, val_acc:0.940]
Epoch [61/120    avg_loss:0.137, val_acc:0.939]
Epoch [62/120    avg_loss:0.109, val_acc:0.940]
Epoch [63/120    avg_loss:0.105, val_acc:0.943]
Epoch [64/120    avg_loss:0.121, val_acc:0.936]
Epoch [65/120    avg_loss:0.166, val_acc:0.873]
Epoch [66/120    avg_loss:0.200, val_acc:0.931]
Epoch [67/120    avg_loss:0.161, val_acc:0.923]
Epoch [68/120    avg_loss:0.191, val_acc:0.931]
Epoch [69/120    avg_loss:0.178, val_acc:0.910]
Epoch [70/120    avg_loss:0.210, val_acc:0.874]
Epoch [71/120    avg_loss:0.180, val_acc:0.920]
Epoch [72/120    avg_loss:0.132, val_acc:0.928]
Epoch [73/120    avg_loss:0.109, val_acc:0.932]
Epoch [74/120    avg_loss:0.097, val_acc:0.944]
Epoch [75/120    avg_loss:0.096, val_acc:0.935]
Epoch [76/120    avg_loss:0.089, val_acc:0.928]
Epoch [77/120    avg_loss:0.084, val_acc:0.945]
Epoch [78/120    avg_loss:0.101, val_acc:0.945]
Epoch [79/120    avg_loss:0.097, val_acc:0.953]
Epoch [80/120    avg_loss:0.092, val_acc:0.948]
Epoch [81/120    avg_loss:0.078, val_acc:0.938]
Epoch [82/120    avg_loss:0.077, val_acc:0.949]
Epoch [83/120    avg_loss:0.084, val_acc:0.955]
Epoch [84/120    avg_loss:0.095, val_acc:0.943]
Epoch [85/120    avg_loss:0.073, val_acc:0.948]
Epoch [86/120    avg_loss:0.126, val_acc:0.898]
Epoch [87/120    avg_loss:0.134, val_acc:0.943]
Epoch [88/120    avg_loss:0.099, val_acc:0.944]
Epoch [89/120    avg_loss:0.088, val_acc:0.952]
Epoch [90/120    avg_loss:0.083, val_acc:0.950]
Epoch [91/120    avg_loss:0.063, val_acc:0.960]
Epoch [92/120    avg_loss:0.059, val_acc:0.950]
Epoch [93/120    avg_loss:0.061, val_acc:0.945]
Epoch [94/120    avg_loss:0.072, val_acc:0.951]
Epoch [95/120    avg_loss:0.066, val_acc:0.951]
Epoch [96/120    avg_loss:0.064, val_acc:0.943]
Epoch [97/120    avg_loss:0.067, val_acc:0.948]
Epoch [98/120    avg_loss:0.057, val_acc:0.950]
Epoch [99/120    avg_loss:0.079, val_acc:0.958]
Epoch [100/120    avg_loss:0.058, val_acc:0.959]
Epoch [101/120    avg_loss:0.055, val_acc:0.960]
Epoch [102/120    avg_loss:0.054, val_acc:0.949]
Epoch [103/120    avg_loss:0.050, val_acc:0.951]
Epoch [104/120    avg_loss:0.044, val_acc:0.953]
Epoch [105/120    avg_loss:0.049, val_acc:0.963]
Epoch [106/120    avg_loss:0.047, val_acc:0.948]
Epoch [107/120    avg_loss:0.057, val_acc:0.957]
Epoch [108/120    avg_loss:0.046, val_acc:0.957]
Epoch [109/120    avg_loss:0.035, val_acc:0.964]
Epoch [110/120    avg_loss:0.033, val_acc:0.966]
Epoch [111/120    avg_loss:0.046, val_acc:0.957]
Epoch [112/120    avg_loss:0.039, val_acc:0.964]
Epoch [113/120    avg_loss:0.038, val_acc:0.966]
Epoch [114/120    avg_loss:0.038, val_acc:0.965]
Epoch [115/120    avg_loss:0.035, val_acc:0.964]
Epoch [116/120    avg_loss:0.030, val_acc:0.963]
Epoch [117/120    avg_loss:0.032, val_acc:0.958]
Epoch [118/120    avg_loss:0.031, val_acc:0.963]
Epoch [119/120    avg_loss:0.067, val_acc:0.958]
Epoch [120/120    avg_loss:0.043, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1159   15    6    0    1    0    0    3   37   59    0    0
     0    5    0]
 [   0    0    0  700   16   10    0    0    0   16    0    1    0    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  408    0    8    0    7    0    0    0    0
    12    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0   41   90    0    1    0    0    0    0  735    2    0    0
     0    6    0]
 [   0    0   23    0    0    5   13    0    1    0   23 2139    0    6
     0    0    0]
 [   0    0    3   18    3    1    0    0    0    2   31    8  460    0
     0    0    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    2    0    1    0    3    4    0    0
  1126    0    0]
 [   0    0    1    0    0    0   25    0    0    3    0    0    0    0
    82  236    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
93.27913279132791

F1 scores:
[       nan 0.94871795 0.9227707  0.88945362 0.94456763 0.94553882
 0.9689808  0.86206897 0.99651568 0.44444444 0.86065574 0.96699819
 0.92369478 0.97368421 0.9546418  0.79461279 0.94857143]

Kappa:
0.9233674072250888
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f26f65158d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.762, val_acc:0.342]
Epoch [2/120    avg_loss:2.560, val_acc:0.362]
Epoch [3/120    avg_loss:2.393, val_acc:0.362]
Epoch [4/120    avg_loss:2.251, val_acc:0.493]
Epoch [5/120    avg_loss:2.152, val_acc:0.530]
Epoch [6/120    avg_loss:2.074, val_acc:0.552]
Epoch [7/120    avg_loss:1.962, val_acc:0.559]
Epoch [8/120    avg_loss:1.838, val_acc:0.590]
Epoch [9/120    avg_loss:1.731, val_acc:0.615]
Epoch [10/120    avg_loss:1.667, val_acc:0.639]
Epoch [11/120    avg_loss:1.517, val_acc:0.660]
Epoch [12/120    avg_loss:1.388, val_acc:0.656]
Epoch [13/120    avg_loss:1.267, val_acc:0.699]
Epoch [14/120    avg_loss:1.180, val_acc:0.702]
Epoch [15/120    avg_loss:1.051, val_acc:0.676]
Epoch [16/120    avg_loss:1.060, val_acc:0.730]
Epoch [17/120    avg_loss:0.903, val_acc:0.774]
Epoch [18/120    avg_loss:0.850, val_acc:0.765]
Epoch [19/120    avg_loss:0.809, val_acc:0.757]
Epoch [20/120    avg_loss:0.704, val_acc:0.797]
Epoch [21/120    avg_loss:0.665, val_acc:0.799]
Epoch [22/120    avg_loss:0.667, val_acc:0.749]
Epoch [23/120    avg_loss:0.637, val_acc:0.810]
Epoch [24/120    avg_loss:0.622, val_acc:0.835]
Epoch [25/120    avg_loss:0.584, val_acc:0.833]
Epoch [26/120    avg_loss:0.529, val_acc:0.859]
Epoch [27/120    avg_loss:0.502, val_acc:0.849]
Epoch [28/120    avg_loss:0.455, val_acc:0.869]
Epoch [29/120    avg_loss:0.456, val_acc:0.869]
Epoch [30/120    avg_loss:0.435, val_acc:0.848]
Epoch [31/120    avg_loss:0.394, val_acc:0.899]
Epoch [32/120    avg_loss:0.335, val_acc:0.897]
Epoch [33/120    avg_loss:0.297, val_acc:0.876]
Epoch [34/120    avg_loss:0.306, val_acc:0.907]
Epoch [35/120    avg_loss:0.303, val_acc:0.899]
Epoch [36/120    avg_loss:0.325, val_acc:0.880]
Epoch [37/120    avg_loss:0.300, val_acc:0.901]
Epoch [38/120    avg_loss:0.280, val_acc:0.897]
Epoch [39/120    avg_loss:0.236, val_acc:0.906]
Epoch [40/120    avg_loss:0.257, val_acc:0.893]
Epoch [41/120    avg_loss:0.245, val_acc:0.932]
Epoch [42/120    avg_loss:0.241, val_acc:0.920]
Epoch [43/120    avg_loss:0.221, val_acc:0.915]
Epoch [44/120    avg_loss:0.224, val_acc:0.910]
Epoch [45/120    avg_loss:0.214, val_acc:0.914]
Epoch [46/120    avg_loss:0.174, val_acc:0.940]
Epoch [47/120    avg_loss:0.138, val_acc:0.936]
Epoch [48/120    avg_loss:0.171, val_acc:0.933]
Epoch [49/120    avg_loss:0.189, val_acc:0.924]
Epoch [50/120    avg_loss:0.159, val_acc:0.925]
Epoch [51/120    avg_loss:0.145, val_acc:0.933]
Epoch [52/120    avg_loss:0.136, val_acc:0.947]
Epoch [53/120    avg_loss:0.188, val_acc:0.924]
Epoch [54/120    avg_loss:0.178, val_acc:0.910]
Epoch [55/120    avg_loss:0.169, val_acc:0.936]
Epoch [56/120    avg_loss:0.167, val_acc:0.934]
Epoch [57/120    avg_loss:0.177, val_acc:0.948]
Epoch [58/120    avg_loss:0.146, val_acc:0.949]
Epoch [59/120    avg_loss:0.146, val_acc:0.940]
Epoch [60/120    avg_loss:0.162, val_acc:0.925]
Epoch [61/120    avg_loss:0.116, val_acc:0.940]
Epoch [62/120    avg_loss:0.190, val_acc:0.878]
Epoch [63/120    avg_loss:0.227, val_acc:0.893]
Epoch [64/120    avg_loss:0.179, val_acc:0.947]
Epoch [65/120    avg_loss:0.144, val_acc:0.938]
Epoch [66/120    avg_loss:0.117, val_acc:0.957]
Epoch [67/120    avg_loss:0.101, val_acc:0.951]
Epoch [68/120    avg_loss:0.095, val_acc:0.953]
Epoch [69/120    avg_loss:0.087, val_acc:0.958]
Epoch [70/120    avg_loss:0.091, val_acc:0.957]
Epoch [71/120    avg_loss:0.114, val_acc:0.944]
Epoch [72/120    avg_loss:0.087, val_acc:0.956]
Epoch [73/120    avg_loss:0.087, val_acc:0.950]
Epoch [74/120    avg_loss:0.093, val_acc:0.957]
Epoch [75/120    avg_loss:0.085, val_acc:0.960]
Epoch [76/120    avg_loss:0.072, val_acc:0.956]
Epoch [77/120    avg_loss:0.073, val_acc:0.955]
Epoch [78/120    avg_loss:0.073, val_acc:0.958]
Epoch [79/120    avg_loss:0.077, val_acc:0.963]
Epoch [80/120    avg_loss:0.070, val_acc:0.956]
Epoch [81/120    avg_loss:0.070, val_acc:0.958]
Epoch [82/120    avg_loss:0.066, val_acc:0.958]
Epoch [83/120    avg_loss:0.060, val_acc:0.956]
Epoch [84/120    avg_loss:0.052, val_acc:0.966]
Epoch [85/120    avg_loss:0.047, val_acc:0.968]
Epoch [86/120    avg_loss:0.049, val_acc:0.966]
Epoch [87/120    avg_loss:0.047, val_acc:0.966]
Epoch [88/120    avg_loss:0.050, val_acc:0.955]
Epoch [89/120    avg_loss:0.054, val_acc:0.956]
Epoch [90/120    avg_loss:0.050, val_acc:0.965]
Epoch [91/120    avg_loss:0.048, val_acc:0.966]
Epoch [92/120    avg_loss:0.061, val_acc:0.964]
Epoch [93/120    avg_loss:0.049, val_acc:0.963]
Epoch [94/120    avg_loss:0.041, val_acc:0.964]
Epoch [95/120    avg_loss:0.040, val_acc:0.966]
Epoch [96/120    avg_loss:0.046, val_acc:0.964]
Epoch [97/120    avg_loss:0.046, val_acc:0.961]
Epoch [98/120    avg_loss:0.042, val_acc:0.965]
Epoch [99/120    avg_loss:0.037, val_acc:0.963]
Epoch [100/120    avg_loss:0.030, val_acc:0.965]
Epoch [101/120    avg_loss:0.028, val_acc:0.969]
Epoch [102/120    avg_loss:0.022, val_acc:0.970]
Epoch [103/120    avg_loss:0.030, val_acc:0.969]
Epoch [104/120    avg_loss:0.023, val_acc:0.968]
Epoch [105/120    avg_loss:0.027, val_acc:0.969]
Epoch [106/120    avg_loss:0.027, val_acc:0.968]
Epoch [107/120    avg_loss:0.028, val_acc:0.968]
Epoch [108/120    avg_loss:0.021, val_acc:0.969]
Epoch [109/120    avg_loss:0.024, val_acc:0.970]
Epoch [110/120    avg_loss:0.024, val_acc:0.973]
Epoch [111/120    avg_loss:0.025, val_acc:0.972]
Epoch [112/120    avg_loss:0.024, val_acc:0.970]
Epoch [113/120    avg_loss:0.027, val_acc:0.970]
Epoch [114/120    avg_loss:0.021, val_acc:0.970]
Epoch [115/120    avg_loss:0.022, val_acc:0.968]
Epoch [116/120    avg_loss:0.026, val_acc:0.970]
Epoch [117/120    avg_loss:0.025, val_acc:0.969]
Epoch [118/120    avg_loss:0.023, val_acc:0.972]
Epoch [119/120    avg_loss:0.022, val_acc:0.972]
Epoch [120/120    avg_loss:0.021, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1253    0    0    0    1    0    0    0    1   24    1    0
     0    5    0]
 [   0    0    1  698    1   21    0    0    0    6    0    0   20    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    5    0    7    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   11    0    0    3    0
     0    0    0]
 [   0    0   38   88    0    5    2    0    0    0  726    2    0    0
     0   14    0]
 [   0    0   24    0    0    3   12    0    4    0    5 2148    3    4
     7    0    0]
 [   0    0    0   23    3    4    0    0    0    0   14    0  480    0
     0    0   10]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   12    0    0    0    0    3    1    1    0
  1122    0    0]
 [   0    0    0    0    0    1   44    0    0    0    0    0    1    0
    95  206    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.22222222222223

F1 scores:
[       nan 0.96202532 0.96347559 0.89487179 0.99069767 0.93584071
 0.95626822 0.90909091 0.99069767 0.52380952 0.89244007 0.97948016
 0.91515729 0.98930481 0.94964029 0.72027972 0.93181818]

Kappa:
0.934122812305071
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f72e86de8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.773, val_acc:0.192]
Epoch [2/120    avg_loss:2.597, val_acc:0.484]
Epoch [3/120    avg_loss:2.409, val_acc:0.477]
Epoch [4/120    avg_loss:2.284, val_acc:0.510]
Epoch [5/120    avg_loss:2.160, val_acc:0.526]
Epoch [6/120    avg_loss:2.046, val_acc:0.552]
Epoch [7/120    avg_loss:1.975, val_acc:0.593]
Epoch [8/120    avg_loss:1.893, val_acc:0.569]
Epoch [9/120    avg_loss:1.803, val_acc:0.599]
Epoch [10/120    avg_loss:1.702, val_acc:0.615]
Epoch [11/120    avg_loss:1.618, val_acc:0.632]
Epoch [12/120    avg_loss:1.507, val_acc:0.649]
Epoch [13/120    avg_loss:1.421, val_acc:0.658]
Epoch [14/120    avg_loss:1.281, val_acc:0.675]
Epoch [15/120    avg_loss:1.237, val_acc:0.675]
Epoch [16/120    avg_loss:1.073, val_acc:0.703]
Epoch [17/120    avg_loss:0.995, val_acc:0.724]
Epoch [18/120    avg_loss:0.924, val_acc:0.740]
Epoch [19/120    avg_loss:0.847, val_acc:0.736]
Epoch [20/120    avg_loss:0.735, val_acc:0.773]
Epoch [21/120    avg_loss:0.691, val_acc:0.782]
Epoch [22/120    avg_loss:0.641, val_acc:0.780]
Epoch [23/120    avg_loss:0.586, val_acc:0.812]
Epoch [24/120    avg_loss:0.525, val_acc:0.814]
Epoch [25/120    avg_loss:0.500, val_acc:0.815]
Epoch [26/120    avg_loss:0.540, val_acc:0.800]
Epoch [27/120    avg_loss:0.496, val_acc:0.838]
Epoch [28/120    avg_loss:0.431, val_acc:0.814]
Epoch [29/120    avg_loss:0.433, val_acc:0.869]
Epoch [30/120    avg_loss:0.425, val_acc:0.840]
Epoch [31/120    avg_loss:0.354, val_acc:0.865]
Epoch [32/120    avg_loss:0.358, val_acc:0.844]
Epoch [33/120    avg_loss:0.372, val_acc:0.870]
Epoch [34/120    avg_loss:0.385, val_acc:0.850]
Epoch [35/120    avg_loss:0.344, val_acc:0.873]
Epoch [36/120    avg_loss:0.289, val_acc:0.858]
Epoch [37/120    avg_loss:0.257, val_acc:0.897]
Epoch [38/120    avg_loss:0.271, val_acc:0.880]
Epoch [39/120    avg_loss:0.246, val_acc:0.872]
Epoch [40/120    avg_loss:0.320, val_acc:0.850]
Epoch [41/120    avg_loss:0.346, val_acc:0.852]
Epoch [42/120    avg_loss:0.306, val_acc:0.883]
Epoch [43/120    avg_loss:0.227, val_acc:0.903]
Epoch [44/120    avg_loss:0.218, val_acc:0.895]
Epoch [45/120    avg_loss:0.239, val_acc:0.894]
Epoch [46/120    avg_loss:0.221, val_acc:0.905]
Epoch [47/120    avg_loss:0.186, val_acc:0.918]
Epoch [48/120    avg_loss:0.178, val_acc:0.907]
Epoch [49/120    avg_loss:0.199, val_acc:0.910]
Epoch [50/120    avg_loss:0.192, val_acc:0.912]
Epoch [51/120    avg_loss:0.173, val_acc:0.919]
Epoch [52/120    avg_loss:0.193, val_acc:0.895]
Epoch [53/120    avg_loss:0.219, val_acc:0.918]
Epoch [54/120    avg_loss:0.143, val_acc:0.930]
Epoch [55/120    avg_loss:0.125, val_acc:0.933]
Epoch [56/120    avg_loss:0.116, val_acc:0.928]
Epoch [57/120    avg_loss:0.117, val_acc:0.938]
Epoch [58/120    avg_loss:0.117, val_acc:0.934]
Epoch [59/120    avg_loss:0.120, val_acc:0.922]
Epoch [60/120    avg_loss:0.195, val_acc:0.914]
Epoch [61/120    avg_loss:0.160, val_acc:0.918]
Epoch [62/120    avg_loss:0.139, val_acc:0.933]
Epoch [63/120    avg_loss:0.128, val_acc:0.927]
Epoch [64/120    avg_loss:0.119, val_acc:0.924]
Epoch [65/120    avg_loss:0.116, val_acc:0.941]
Epoch [66/120    avg_loss:0.082, val_acc:0.930]
Epoch [67/120    avg_loss:0.090, val_acc:0.939]
Epoch [68/120    avg_loss:0.112, val_acc:0.939]
Epoch [69/120    avg_loss:0.089, val_acc:0.942]
Epoch [70/120    avg_loss:0.086, val_acc:0.950]
Epoch [71/120    avg_loss:0.085, val_acc:0.938]
Epoch [72/120    avg_loss:0.083, val_acc:0.938]
Epoch [73/120    avg_loss:0.077, val_acc:0.942]
Epoch [74/120    avg_loss:0.077, val_acc:0.939]
Epoch [75/120    avg_loss:0.069, val_acc:0.950]
Epoch [76/120    avg_loss:0.071, val_acc:0.940]
Epoch [77/120    avg_loss:0.091, val_acc:0.952]
Epoch [78/120    avg_loss:0.065, val_acc:0.953]
Epoch [79/120    avg_loss:0.055, val_acc:0.951]
Epoch [80/120    avg_loss:0.056, val_acc:0.950]
Epoch [81/120    avg_loss:0.060, val_acc:0.950]
Epoch [82/120    avg_loss:0.062, val_acc:0.949]
Epoch [83/120    avg_loss:0.067, val_acc:0.948]
Epoch [84/120    avg_loss:0.080, val_acc:0.948]
Epoch [85/120    avg_loss:0.077, val_acc:0.958]
Epoch [86/120    avg_loss:0.061, val_acc:0.961]
Epoch [87/120    avg_loss:0.059, val_acc:0.949]
Epoch [88/120    avg_loss:0.061, val_acc:0.953]
Epoch [89/120    avg_loss:0.044, val_acc:0.958]
Epoch [90/120    avg_loss:0.044, val_acc:0.957]
Epoch [91/120    avg_loss:0.067, val_acc:0.961]
Epoch [92/120    avg_loss:0.077, val_acc:0.958]
Epoch [93/120    avg_loss:0.054, val_acc:0.950]
Epoch [94/120    avg_loss:0.049, val_acc:0.956]
Epoch [95/120    avg_loss:0.044, val_acc:0.948]
Epoch [96/120    avg_loss:0.048, val_acc:0.958]
Epoch [97/120    avg_loss:0.045, val_acc:0.963]
Epoch [98/120    avg_loss:0.039, val_acc:0.963]
Epoch [99/120    avg_loss:0.035, val_acc:0.967]
Epoch [100/120    avg_loss:0.030, val_acc:0.968]
Epoch [101/120    avg_loss:0.040, val_acc:0.964]
Epoch [102/120    avg_loss:0.031, val_acc:0.964]
Epoch [103/120    avg_loss:0.034, val_acc:0.965]
Epoch [104/120    avg_loss:0.039, val_acc:0.966]
Epoch [105/120    avg_loss:0.044, val_acc:0.963]
Epoch [106/120    avg_loss:0.033, val_acc:0.963]
Epoch [107/120    avg_loss:0.041, val_acc:0.958]
Epoch [108/120    avg_loss:0.035, val_acc:0.950]
Epoch [109/120    avg_loss:0.030, val_acc:0.965]
Epoch [110/120    avg_loss:0.033, val_acc:0.961]
Epoch [111/120    avg_loss:0.033, val_acc:0.953]
Epoch [112/120    avg_loss:0.031, val_acc:0.960]
Epoch [113/120    avg_loss:0.041, val_acc:0.964]
Epoch [114/120    avg_loss:0.039, val_acc:0.964]
Epoch [115/120    avg_loss:0.029, val_acc:0.963]
Epoch [116/120    avg_loss:0.031, val_acc:0.966]
Epoch [117/120    avg_loss:0.022, val_acc:0.966]
Epoch [118/120    avg_loss:0.023, val_acc:0.965]
Epoch [119/120    avg_loss:0.020, val_acc:0.966]
Epoch [120/120    avg_loss:0.023, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1214    0    0    0    7    0    0    0   13   49    2    0
     0    0    0]
 [   0    0    1  691    2   15    0    0    0   13    0    0   25    0
     0    0    0]
 [   0    0    0    0  212    0    1    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    1    0    6    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    6   90    0    5    0    0    0    0  763    9    0    0
     2    0    0]
 [   0    0   22    0    0    0   15    0    0    0   27 2134    1    4
     7    0    0]
 [   0    0    0   21   11    7    0    0    0    0   12    0  475    0
     0    0    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    1    0    0
  1135    0    0]
 [   0    0    0    0    0    0   32    0    0    0    0    0    0    0
    99  216    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
94.22222222222223

F1 scores:
[       nan 0.96202532 0.96044304 0.88989053 0.96803653 0.95711061
 0.95830285 0.98039216 1.         0.54901961 0.89976415 0.96911898
 0.91346154 0.98930481 0.9509845  0.76731794 0.93641618]

Kappa:
0.934113166443025
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1c48f1c908>
supervision:full
center_pixel:True
Network :
Number of parameter: 65454==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.800, val_acc:0.218]
Epoch [2/120    avg_loss:2.601, val_acc:0.441]
Epoch [3/120    avg_loss:2.387, val_acc:0.483]
Epoch [4/120    avg_loss:2.270, val_acc:0.499]
Epoch [5/120    avg_loss:2.158, val_acc:0.519]
Epoch [6/120    avg_loss:2.068, val_acc:0.548]
Epoch [7/120    avg_loss:1.944, val_acc:0.574]
Epoch [8/120    avg_loss:1.873, val_acc:0.597]
Epoch [9/120    avg_loss:1.796, val_acc:0.598]
Epoch [10/120    avg_loss:1.707, val_acc:0.611]
Epoch [11/120    avg_loss:1.631, val_acc:0.630]
Epoch [12/120    avg_loss:1.529, val_acc:0.641]
Epoch [13/120    avg_loss:1.450, val_acc:0.649]
Epoch [14/120    avg_loss:1.361, val_acc:0.673]
Epoch [15/120    avg_loss:1.276, val_acc:0.631]
Epoch [16/120    avg_loss:1.262, val_acc:0.673]
Epoch [17/120    avg_loss:1.166, val_acc:0.705]
Epoch [18/120    avg_loss:1.026, val_acc:0.698]
Epoch [19/120    avg_loss:0.918, val_acc:0.726]
Epoch [20/120    avg_loss:0.850, val_acc:0.758]
Epoch [21/120    avg_loss:0.830, val_acc:0.766]
Epoch [22/120    avg_loss:0.821, val_acc:0.748]
Epoch [23/120    avg_loss:0.752, val_acc:0.820]
Epoch [24/120    avg_loss:0.613, val_acc:0.823]
Epoch [25/120    avg_loss:0.612, val_acc:0.778]
Epoch [26/120    avg_loss:0.570, val_acc:0.809]
Epoch [27/120    avg_loss:0.582, val_acc:0.795]
Epoch [28/120    avg_loss:0.531, val_acc:0.844]
Epoch [29/120    avg_loss:0.405, val_acc:0.845]
Epoch [30/120    avg_loss:0.410, val_acc:0.878]
Epoch [31/120    avg_loss:0.383, val_acc:0.849]
Epoch [32/120    avg_loss:0.425, val_acc:0.810]
Epoch [33/120    avg_loss:0.399, val_acc:0.852]
Epoch [34/120    avg_loss:0.335, val_acc:0.856]
Epoch [35/120    avg_loss:0.387, val_acc:0.822]
Epoch [36/120    avg_loss:0.380, val_acc:0.887]
Epoch [37/120    avg_loss:0.340, val_acc:0.875]
Epoch [38/120    avg_loss:0.305, val_acc:0.869]
Epoch [39/120    avg_loss:0.287, val_acc:0.880]
Epoch [40/120    avg_loss:0.282, val_acc:0.892]
Epoch [41/120    avg_loss:0.269, val_acc:0.884]
Epoch [42/120    avg_loss:0.250, val_acc:0.905]
Epoch [43/120    avg_loss:0.257, val_acc:0.908]
Epoch [44/120    avg_loss:0.199, val_acc:0.911]
Epoch [45/120    avg_loss:0.231, val_acc:0.820]
Epoch [46/120    avg_loss:0.247, val_acc:0.894]
Epoch [47/120    avg_loss:0.361, val_acc:0.843]
Epoch [48/120    avg_loss:0.321, val_acc:0.895]
Epoch [49/120    avg_loss:0.259, val_acc:0.897]
Epoch [50/120    avg_loss:0.218, val_acc:0.926]
Epoch [51/120    avg_loss:0.203, val_acc:0.923]
Epoch [52/120    avg_loss:0.162, val_acc:0.932]
Epoch [53/120    avg_loss:0.159, val_acc:0.918]
Epoch [54/120    avg_loss:0.241, val_acc:0.887]
Epoch [55/120    avg_loss:0.235, val_acc:0.925]
Epoch [56/120    avg_loss:0.189, val_acc:0.887]
Epoch [57/120    avg_loss:0.219, val_acc:0.930]
Epoch [58/120    avg_loss:0.165, val_acc:0.928]
Epoch [59/120    avg_loss:0.132, val_acc:0.925]
Epoch [60/120    avg_loss:0.140, val_acc:0.915]
Epoch [61/120    avg_loss:0.135, val_acc:0.947]
Epoch [62/120    avg_loss:0.163, val_acc:0.923]
Epoch [63/120    avg_loss:0.179, val_acc:0.936]
Epoch [64/120    avg_loss:0.153, val_acc:0.933]
Epoch [65/120    avg_loss:0.126, val_acc:0.940]
Epoch [66/120    avg_loss:0.220, val_acc:0.882]
Epoch [67/120    avg_loss:0.298, val_acc:0.901]
Epoch [68/120    avg_loss:0.183, val_acc:0.925]
Epoch [69/120    avg_loss:0.147, val_acc:0.910]
Epoch [70/120    avg_loss:0.131, val_acc:0.933]
Epoch [71/120    avg_loss:0.111, val_acc:0.943]
Epoch [72/120    avg_loss:0.106, val_acc:0.925]
Epoch [73/120    avg_loss:0.119, val_acc:0.935]
Epoch [74/120    avg_loss:0.088, val_acc:0.945]
Epoch [75/120    avg_loss:0.078, val_acc:0.945]
Epoch [76/120    avg_loss:0.082, val_acc:0.947]
Epoch [77/120    avg_loss:0.080, val_acc:0.948]
Epoch [78/120    avg_loss:0.068, val_acc:0.949]
Epoch [79/120    avg_loss:0.068, val_acc:0.950]
Epoch [80/120    avg_loss:0.072, val_acc:0.949]
Epoch [81/120    avg_loss:0.079, val_acc:0.953]
Epoch [82/120    avg_loss:0.071, val_acc:0.953]
Epoch [83/120    avg_loss:0.084, val_acc:0.953]
Epoch [84/120    avg_loss:0.075, val_acc:0.956]
Epoch [85/120    avg_loss:0.063, val_acc:0.953]
Epoch [86/120    avg_loss:0.070, val_acc:0.952]
Epoch [87/120    avg_loss:0.060, val_acc:0.950]
Epoch [88/120    avg_loss:0.063, val_acc:0.952]
Epoch [89/120    avg_loss:0.063, val_acc:0.953]
Epoch [90/120    avg_loss:0.074, val_acc:0.955]
Epoch [91/120    avg_loss:0.064, val_acc:0.951]
Epoch [92/120    avg_loss:0.062, val_acc:0.953]
Epoch [93/120    avg_loss:0.065, val_acc:0.951]
Epoch [94/120    avg_loss:0.062, val_acc:0.951]
Epoch [95/120    avg_loss:0.060, val_acc:0.952]
Epoch [96/120    avg_loss:0.060, val_acc:0.953]
Epoch [97/120    avg_loss:0.057, val_acc:0.953]
Epoch [98/120    avg_loss:0.062, val_acc:0.953]
Epoch [99/120    avg_loss:0.058, val_acc:0.953]
Epoch [100/120    avg_loss:0.064, val_acc:0.953]
Epoch [101/120    avg_loss:0.064, val_acc:0.951]
Epoch [102/120    avg_loss:0.056, val_acc:0.952]
Epoch [103/120    avg_loss:0.059, val_acc:0.952]
Epoch [104/120    avg_loss:0.058, val_acc:0.951]
Epoch [105/120    avg_loss:0.061, val_acc:0.951]
Epoch [106/120    avg_loss:0.065, val_acc:0.951]
Epoch [107/120    avg_loss:0.052, val_acc:0.952]
Epoch [108/120    avg_loss:0.063, val_acc:0.951]
Epoch [109/120    avg_loss:0.070, val_acc:0.952]
Epoch [110/120    avg_loss:0.051, val_acc:0.952]
Epoch [111/120    avg_loss:0.065, val_acc:0.952]
Epoch [112/120    avg_loss:0.065, val_acc:0.952]
Epoch [113/120    avg_loss:0.052, val_acc:0.952]
Epoch [114/120    avg_loss:0.068, val_acc:0.952]
Epoch [115/120    avg_loss:0.054, val_acc:0.952]
Epoch [116/120    avg_loss:0.059, val_acc:0.952]
Epoch [117/120    avg_loss:0.068, val_acc:0.952]
Epoch [118/120    avg_loss:0.062, val_acc:0.952]
Epoch [119/120    avg_loss:0.064, val_acc:0.952]
Epoch [120/120    avg_loss:0.055, val_acc:0.952]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    2 1174    0    0    0    7    0    0    3    8   86    5    0
     0    0    0]
 [   0    0    1  687    0    8    0    0    0   28    0    0   20    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  418    0    4    0    4    0    0    0    0
     9    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    4    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   13    0    0    2    0
     0    0    0]
 [   0    0   40   90    0    4    0    0    0    0  707   19    0    0
     3   12    0]
 [   0    0   24    0    0    5   20    0    1    0    8 2137    3    7
     5    0    0]
 [   0    0    0   13    5    5    0    0    0    0   17    7  475    0
     0    0   12]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    1    0    7    5    0    0
  1118    0    0]
 [   0    0    0    0    0    0   22    0    0    0    0    0    0    0
   126  199    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
92.70460704607046

F1 scores:
[       nan 0.925      0.93026941 0.89220779 0.98839907 0.94677237
 0.95946942 0.92592593 0.99767981 0.39393939 0.8696187  0.95658013
 0.91346154 0.97368421 0.93089092 0.71326165 0.9273743 ]

Kappa:
0.9167242758502486
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6468be4780>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.331, val_acc:0.456]
Epoch [2/120    avg_loss:1.719, val_acc:0.600]
Epoch [3/120    avg_loss:1.441, val_acc:0.678]
Epoch [4/120    avg_loss:1.185, val_acc:0.733]
Epoch [5/120    avg_loss:1.019, val_acc:0.720]
Epoch [6/120    avg_loss:0.877, val_acc:0.762]
Epoch [7/120    avg_loss:0.778, val_acc:0.731]
Epoch [8/120    avg_loss:0.696, val_acc:0.756]
Epoch [9/120    avg_loss:0.752, val_acc:0.804]
Epoch [10/120    avg_loss:0.607, val_acc:0.833]
Epoch [11/120    avg_loss:0.550, val_acc:0.833]
Epoch [12/120    avg_loss:0.498, val_acc:0.827]
Epoch [13/120    avg_loss:0.480, val_acc:0.828]
Epoch [14/120    avg_loss:0.477, val_acc:0.868]
Epoch [15/120    avg_loss:0.378, val_acc:0.867]
Epoch [16/120    avg_loss:0.424, val_acc:0.853]
Epoch [17/120    avg_loss:0.380, val_acc:0.925]
Epoch [18/120    avg_loss:0.263, val_acc:0.916]
Epoch [19/120    avg_loss:0.292, val_acc:0.904]
Epoch [20/120    avg_loss:0.303, val_acc:0.845]
Epoch [21/120    avg_loss:0.288, val_acc:0.901]
Epoch [22/120    avg_loss:0.293, val_acc:0.871]
Epoch [23/120    avg_loss:0.272, val_acc:0.924]
Epoch [24/120    avg_loss:0.231, val_acc:0.921]
Epoch [25/120    avg_loss:0.217, val_acc:0.938]
Epoch [26/120    avg_loss:0.236, val_acc:0.904]
Epoch [27/120    avg_loss:0.184, val_acc:0.930]
Epoch [28/120    avg_loss:0.210, val_acc:0.842]
Epoch [29/120    avg_loss:0.176, val_acc:0.944]
Epoch [30/120    avg_loss:0.186, val_acc:0.946]
Epoch [31/120    avg_loss:0.184, val_acc:0.908]
Epoch [32/120    avg_loss:0.167, val_acc:0.931]
Epoch [33/120    avg_loss:0.109, val_acc:0.943]
Epoch [34/120    avg_loss:0.119, val_acc:0.938]
Epoch [35/120    avg_loss:0.092, val_acc:0.955]
Epoch [36/120    avg_loss:0.125, val_acc:0.938]
Epoch [37/120    avg_loss:0.230, val_acc:0.926]
Epoch [38/120    avg_loss:0.114, val_acc:0.949]
Epoch [39/120    avg_loss:0.092, val_acc:0.956]
Epoch [40/120    avg_loss:0.100, val_acc:0.919]
Epoch [41/120    avg_loss:0.215, val_acc:0.945]
Epoch [42/120    avg_loss:0.079, val_acc:0.968]
Epoch [43/120    avg_loss:0.102, val_acc:0.963]
Epoch [44/120    avg_loss:0.053, val_acc:0.958]
Epoch [45/120    avg_loss:0.042, val_acc:0.956]
Epoch [46/120    avg_loss:0.071, val_acc:0.948]
Epoch [47/120    avg_loss:0.085, val_acc:0.966]
Epoch [48/120    avg_loss:0.150, val_acc:0.920]
Epoch [49/120    avg_loss:0.110, val_acc:0.954]
Epoch [50/120    avg_loss:0.059, val_acc:0.965]
Epoch [51/120    avg_loss:0.057, val_acc:0.964]
Epoch [52/120    avg_loss:0.035, val_acc:0.958]
Epoch [53/120    avg_loss:0.063, val_acc:0.918]
Epoch [54/120    avg_loss:0.063, val_acc:0.965]
Epoch [55/120    avg_loss:0.057, val_acc:0.961]
Epoch [56/120    avg_loss:0.042, val_acc:0.971]
Epoch [57/120    avg_loss:0.034, val_acc:0.970]
Epoch [58/120    avg_loss:0.028, val_acc:0.971]
Epoch [59/120    avg_loss:0.032, val_acc:0.967]
Epoch [60/120    avg_loss:0.020, val_acc:0.969]
Epoch [61/120    avg_loss:0.022, val_acc:0.971]
Epoch [62/120    avg_loss:0.029, val_acc:0.972]
Epoch [63/120    avg_loss:0.020, val_acc:0.971]
Epoch [64/120    avg_loss:0.018, val_acc:0.970]
Epoch [65/120    avg_loss:0.022, val_acc:0.971]
Epoch [66/120    avg_loss:0.016, val_acc:0.971]
Epoch [67/120    avg_loss:0.018, val_acc:0.970]
Epoch [68/120    avg_loss:0.030, val_acc:0.974]
Epoch [69/120    avg_loss:0.017, val_acc:0.972]
Epoch [70/120    avg_loss:0.018, val_acc:0.972]
Epoch [71/120    avg_loss:0.018, val_acc:0.972]
Epoch [72/120    avg_loss:0.017, val_acc:0.972]
Epoch [73/120    avg_loss:0.015, val_acc:0.971]
Epoch [74/120    avg_loss:0.019, val_acc:0.972]
Epoch [75/120    avg_loss:0.026, val_acc:0.969]
Epoch [76/120    avg_loss:0.015, val_acc:0.972]
Epoch [77/120    avg_loss:0.019, val_acc:0.972]
Epoch [78/120    avg_loss:0.037, val_acc:0.970]
Epoch [79/120    avg_loss:0.019, val_acc:0.969]
Epoch [80/120    avg_loss:0.014, val_acc:0.968]
Epoch [81/120    avg_loss:0.014, val_acc:0.970]
Epoch [82/120    avg_loss:0.018, val_acc:0.970]
Epoch [83/120    avg_loss:0.018, val_acc:0.970]
Epoch [84/120    avg_loss:0.017, val_acc:0.970]
Epoch [85/120    avg_loss:0.021, val_acc:0.970]
Epoch [86/120    avg_loss:0.018, val_acc:0.970]
Epoch [87/120    avg_loss:0.014, val_acc:0.970]
Epoch [88/120    avg_loss:0.013, val_acc:0.970]
Epoch [89/120    avg_loss:0.020, val_acc:0.970]
Epoch [90/120    avg_loss:0.017, val_acc:0.970]
Epoch [91/120    avg_loss:0.014, val_acc:0.969]
Epoch [92/120    avg_loss:0.015, val_acc:0.970]
Epoch [93/120    avg_loss:0.016, val_acc:0.970]
Epoch [94/120    avg_loss:0.015, val_acc:0.970]
Epoch [95/120    avg_loss:0.017, val_acc:0.970]
Epoch [96/120    avg_loss:0.016, val_acc:0.970]
Epoch [97/120    avg_loss:0.016, val_acc:0.970]
Epoch [98/120    avg_loss:0.019, val_acc:0.970]
Epoch [99/120    avg_loss:0.013, val_acc:0.970]
Epoch [100/120    avg_loss:0.016, val_acc:0.970]
Epoch [101/120    avg_loss:0.018, val_acc:0.970]
Epoch [102/120    avg_loss:0.017, val_acc:0.970]
Epoch [103/120    avg_loss:0.014, val_acc:0.970]
Epoch [104/120    avg_loss:0.016, val_acc:0.970]
Epoch [105/120    avg_loss:0.016, val_acc:0.970]
Epoch [106/120    avg_loss:0.018, val_acc:0.970]
Epoch [107/120    avg_loss:0.012, val_acc:0.970]
Epoch [108/120    avg_loss:0.014, val_acc:0.970]
Epoch [109/120    avg_loss:0.015, val_acc:0.970]
Epoch [110/120    avg_loss:0.013, val_acc:0.970]
Epoch [111/120    avg_loss:0.017, val_acc:0.970]
Epoch [112/120    avg_loss:0.017, val_acc:0.970]
Epoch [113/120    avg_loss:0.011, val_acc:0.970]
Epoch [114/120    avg_loss:0.020, val_acc:0.970]
Epoch [115/120    avg_loss:0.019, val_acc:0.970]
Epoch [116/120    avg_loss:0.016, val_acc:0.970]
Epoch [117/120    avg_loss:0.016, val_acc:0.970]
Epoch [118/120    avg_loss:0.015, val_acc:0.970]
Epoch [119/120    avg_loss:0.017, val_acc:0.970]
Epoch [120/120    avg_loss:0.014, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1260    5    0    3    0    0    0    0    7    9    0    0
     0    1    0]
 [   0    0    0  722    4    0    0    0    0    0    1    8   10    0
     1    1    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  832   35    0    0
     1    1    0]
 [   0    0   22    0    0    0    0    0    0    0   14 2167    5    0
     0    2    0]
 [   0    0    1    2    0    0    0    0    0    0    0    0  531    0
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1122   17    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   100  247    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.0189701897019

F1 scores:
[       nan 0.96202532 0.97902098 0.97699594 0.98598131 0.98847926
 0.99694656 1.         1.         1.         0.96073903 0.97810878
 0.98060942 0.99728997 0.9464361  0.80194805 0.98181818]

Kappa:
0.9659748134957057
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff59a7ba748>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.277, val_acc:0.525]
Epoch [2/120    avg_loss:1.725, val_acc:0.592]
Epoch [3/120    avg_loss:1.401, val_acc:0.625]
Epoch [4/120    avg_loss:1.172, val_acc:0.698]
Epoch [5/120    avg_loss:0.939, val_acc:0.723]
Epoch [6/120    avg_loss:0.820, val_acc:0.731]
Epoch [7/120    avg_loss:0.737, val_acc:0.738]
Epoch [8/120    avg_loss:0.692, val_acc:0.775]
Epoch [9/120    avg_loss:0.587, val_acc:0.789]
Epoch [10/120    avg_loss:0.490, val_acc:0.856]
Epoch [11/120    avg_loss:0.524, val_acc:0.841]
Epoch [12/120    avg_loss:0.434, val_acc:0.858]
Epoch [13/120    avg_loss:0.397, val_acc:0.823]
Epoch [14/120    avg_loss:0.327, val_acc:0.868]
Epoch [15/120    avg_loss:0.346, val_acc:0.888]
Epoch [16/120    avg_loss:0.364, val_acc:0.829]
Epoch [17/120    avg_loss:0.358, val_acc:0.887]
Epoch [18/120    avg_loss:0.261, val_acc:0.879]
Epoch [19/120    avg_loss:0.234, val_acc:0.861]
Epoch [20/120    avg_loss:0.306, val_acc:0.903]
Epoch [21/120    avg_loss:0.194, val_acc:0.932]
Epoch [22/120    avg_loss:0.235, val_acc:0.913]
Epoch [23/120    avg_loss:0.171, val_acc:0.935]
Epoch [24/120    avg_loss:0.159, val_acc:0.942]
Epoch [25/120    avg_loss:0.161, val_acc:0.905]
Epoch [26/120    avg_loss:0.162, val_acc:0.918]
Epoch [27/120    avg_loss:0.184, val_acc:0.941]
Epoch [28/120    avg_loss:0.143, val_acc:0.935]
Epoch [29/120    avg_loss:0.104, val_acc:0.944]
Epoch [30/120    avg_loss:0.206, val_acc:0.942]
Epoch [31/120    avg_loss:0.224, val_acc:0.842]
Epoch [32/120    avg_loss:0.236, val_acc:0.893]
Epoch [33/120    avg_loss:0.178, val_acc:0.949]
Epoch [34/120    avg_loss:0.100, val_acc:0.942]
Epoch [35/120    avg_loss:0.064, val_acc:0.944]
Epoch [36/120    avg_loss:0.098, val_acc:0.948]
Epoch [37/120    avg_loss:0.077, val_acc:0.965]
Epoch [38/120    avg_loss:0.057, val_acc:0.952]
Epoch [39/120    avg_loss:0.062, val_acc:0.966]
Epoch [40/120    avg_loss:0.053, val_acc:0.947]
Epoch [41/120    avg_loss:0.057, val_acc:0.947]
Epoch [42/120    avg_loss:0.157, val_acc:0.936]
Epoch [43/120    avg_loss:0.092, val_acc:0.966]
Epoch [44/120    avg_loss:0.055, val_acc:0.972]
Epoch [45/120    avg_loss:0.044, val_acc:0.968]
Epoch [46/120    avg_loss:0.036, val_acc:0.976]
Epoch [47/120    avg_loss:0.054, val_acc:0.964]
Epoch [48/120    avg_loss:0.053, val_acc:0.968]
Epoch [49/120    avg_loss:0.029, val_acc:0.970]
Epoch [50/120    avg_loss:0.038, val_acc:0.973]
Epoch [51/120    avg_loss:0.029, val_acc:0.965]
Epoch [52/120    avg_loss:0.020, val_acc:0.971]
Epoch [53/120    avg_loss:0.019, val_acc:0.978]
Epoch [54/120    avg_loss:0.025, val_acc:0.958]
Epoch [55/120    avg_loss:0.227, val_acc:0.867]
Epoch [56/120    avg_loss:0.230, val_acc:0.934]
Epoch [57/120    avg_loss:0.066, val_acc:0.956]
Epoch [58/120    avg_loss:0.077, val_acc:0.963]
Epoch [59/120    avg_loss:0.052, val_acc:0.977]
Epoch [60/120    avg_loss:0.052, val_acc:0.940]
Epoch [61/120    avg_loss:0.053, val_acc:0.958]
Epoch [62/120    avg_loss:0.024, val_acc:0.977]
Epoch [63/120    avg_loss:0.028, val_acc:0.971]
Epoch [64/120    avg_loss:0.029, val_acc:0.974]
Epoch [65/120    avg_loss:0.028, val_acc:0.973]
Epoch [66/120    avg_loss:0.024, val_acc:0.974]
Epoch [67/120    avg_loss:0.015, val_acc:0.976]
Epoch [68/120    avg_loss:0.015, val_acc:0.977]
Epoch [69/120    avg_loss:0.017, val_acc:0.977]
Epoch [70/120    avg_loss:0.011, val_acc:0.980]
Epoch [71/120    avg_loss:0.010, val_acc:0.978]
Epoch [72/120    avg_loss:0.014, val_acc:0.980]
Epoch [73/120    avg_loss:0.010, val_acc:0.980]
Epoch [74/120    avg_loss:0.009, val_acc:0.977]
Epoch [75/120    avg_loss:0.010, val_acc:0.979]
Epoch [76/120    avg_loss:0.009, val_acc:0.979]
Epoch [77/120    avg_loss:0.014, val_acc:0.978]
Epoch [78/120    avg_loss:0.011, val_acc:0.978]
Epoch [79/120    avg_loss:0.011, val_acc:0.979]
Epoch [80/120    avg_loss:0.010, val_acc:0.979]
Epoch [81/120    avg_loss:0.009, val_acc:0.979]
Epoch [82/120    avg_loss:0.012, val_acc:0.982]
Epoch [83/120    avg_loss:0.010, val_acc:0.982]
Epoch [84/120    avg_loss:0.009, val_acc:0.980]
Epoch [85/120    avg_loss:0.010, val_acc:0.981]
Epoch [86/120    avg_loss:0.009, val_acc:0.981]
Epoch [87/120    avg_loss:0.012, val_acc:0.981]
Epoch [88/120    avg_loss:0.007, val_acc:0.982]
Epoch [89/120    avg_loss:0.007, val_acc:0.982]
Epoch [90/120    avg_loss:0.007, val_acc:0.981]
Epoch [91/120    avg_loss:0.008, val_acc:0.978]
Epoch [92/120    avg_loss:0.010, val_acc:0.979]
Epoch [93/120    avg_loss:0.010, val_acc:0.982]
Epoch [94/120    avg_loss:0.012, val_acc:0.981]
Epoch [95/120    avg_loss:0.009, val_acc:0.982]
Epoch [96/120    avg_loss:0.009, val_acc:0.981]
Epoch [97/120    avg_loss:0.008, val_acc:0.982]
Epoch [98/120    avg_loss:0.008, val_acc:0.980]
Epoch [99/120    avg_loss:0.011, val_acc:0.979]
Epoch [100/120    avg_loss:0.008, val_acc:0.981]
Epoch [101/120    avg_loss:0.006, val_acc:0.982]
Epoch [102/120    avg_loss:0.007, val_acc:0.981]
Epoch [103/120    avg_loss:0.006, val_acc:0.979]
Epoch [104/120    avg_loss:0.008, val_acc:0.981]
Epoch [105/120    avg_loss:0.007, val_acc:0.981]
Epoch [106/120    avg_loss:0.007, val_acc:0.981]
Epoch [107/120    avg_loss:0.008, val_acc:0.980]
Epoch [108/120    avg_loss:0.011, val_acc:0.981]
Epoch [109/120    avg_loss:0.006, val_acc:0.982]
Epoch [110/120    avg_loss:0.007, val_acc:0.981]
Epoch [111/120    avg_loss:0.008, val_acc:0.981]
Epoch [112/120    avg_loss:0.006, val_acc:0.982]
Epoch [113/120    avg_loss:0.008, val_acc:0.982]
Epoch [114/120    avg_loss:0.007, val_acc:0.983]
Epoch [115/120    avg_loss:0.007, val_acc:0.983]
Epoch [116/120    avg_loss:0.006, val_acc:0.982]
Epoch [117/120    avg_loss:0.007, val_acc:0.982]
Epoch [118/120    avg_loss:0.007, val_acc:0.981]
Epoch [119/120    avg_loss:0.007, val_acc:0.981]
Epoch [120/120    avg_loss:0.007, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1268    3    0    0    0    0    0    0    5    9    0    0
     0    0    0]
 [   0    0    0  721    2    0    0    0    0    1    0   11   12    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    1    0    0    0  852   19    0    0
     0    0    0]
 [   0    0    8    0    0    1    0    0    0    0    6 2177   17    0
     1    0    0]
 [   0    0    2    1    0    0    0    0    0    0    1    0  526    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    0    0    0    0
  1119   14    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
   113  233    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.289972899729

F1 scores:
[       nan 0.98765432 0.98792365 0.97961957 0.9953271  0.99088838
 0.99541985 1.         0.99883586 0.97297297 0.97987349 0.98328817
 0.96425298 0.99728997 0.94191919 0.78451178 0.97076023]

Kappa:
0.9690709679163645
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdf7f0db780>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.370, val_acc:0.329]
Epoch [2/120    avg_loss:1.814, val_acc:0.543]
Epoch [3/120    avg_loss:1.524, val_acc:0.561]
Epoch [4/120    avg_loss:1.245, val_acc:0.692]
Epoch [5/120    avg_loss:1.116, val_acc:0.664]
Epoch [6/120    avg_loss:0.969, val_acc:0.723]
Epoch [7/120    avg_loss:0.890, val_acc:0.731]
Epoch [8/120    avg_loss:0.724, val_acc:0.752]
Epoch [9/120    avg_loss:0.610, val_acc:0.784]
Epoch [10/120    avg_loss:0.582, val_acc:0.809]
Epoch [11/120    avg_loss:0.543, val_acc:0.815]
Epoch [12/120    avg_loss:0.530, val_acc:0.820]
Epoch [13/120    avg_loss:0.455, val_acc:0.844]
Epoch [14/120    avg_loss:0.402, val_acc:0.832]
Epoch [15/120    avg_loss:0.368, val_acc:0.869]
Epoch [16/120    avg_loss:0.341, val_acc:0.855]
Epoch [17/120    avg_loss:0.401, val_acc:0.836]
Epoch [18/120    avg_loss:0.310, val_acc:0.877]
Epoch [19/120    avg_loss:0.265, val_acc:0.900]
Epoch [20/120    avg_loss:0.280, val_acc:0.907]
Epoch [21/120    avg_loss:0.244, val_acc:0.816]
Epoch [22/120    avg_loss:0.317, val_acc:0.894]
Epoch [23/120    avg_loss:0.221, val_acc:0.884]
Epoch [24/120    avg_loss:0.172, val_acc:0.905]
Epoch [25/120    avg_loss:0.147, val_acc:0.934]
Epoch [26/120    avg_loss:0.137, val_acc:0.931]
Epoch [27/120    avg_loss:0.178, val_acc:0.886]
Epoch [28/120    avg_loss:0.172, val_acc:0.907]
Epoch [29/120    avg_loss:0.168, val_acc:0.929]
Epoch [30/120    avg_loss:0.098, val_acc:0.914]
Epoch [31/120    avg_loss:0.111, val_acc:0.939]
Epoch [32/120    avg_loss:0.135, val_acc:0.909]
Epoch [33/120    avg_loss:0.121, val_acc:0.932]
Epoch [34/120    avg_loss:0.087, val_acc:0.931]
Epoch [35/120    avg_loss:0.091, val_acc:0.960]
Epoch [36/120    avg_loss:0.118, val_acc:0.954]
Epoch [37/120    avg_loss:0.098, val_acc:0.945]
Epoch [38/120    avg_loss:0.095, val_acc:0.936]
Epoch [39/120    avg_loss:0.071, val_acc:0.959]
Epoch [40/120    avg_loss:0.106, val_acc:0.893]
Epoch [41/120    avg_loss:0.156, val_acc:0.942]
Epoch [42/120    avg_loss:0.085, val_acc:0.955]
Epoch [43/120    avg_loss:0.067, val_acc:0.963]
Epoch [44/120    avg_loss:0.044, val_acc:0.953]
Epoch [45/120    avg_loss:0.074, val_acc:0.971]
Epoch [46/120    avg_loss:0.100, val_acc:0.963]
Epoch [47/120    avg_loss:0.052, val_acc:0.953]
Epoch [48/120    avg_loss:0.049, val_acc:0.971]
Epoch [49/120    avg_loss:0.034, val_acc:0.966]
Epoch [50/120    avg_loss:0.090, val_acc:0.906]
Epoch [51/120    avg_loss:0.099, val_acc:0.960]
Epoch [52/120    avg_loss:0.051, val_acc:0.936]
Epoch [53/120    avg_loss:0.154, val_acc:0.945]
Epoch [54/120    avg_loss:0.063, val_acc:0.972]
Epoch [55/120    avg_loss:0.039, val_acc:0.976]
Epoch [56/120    avg_loss:0.034, val_acc:0.947]
Epoch [57/120    avg_loss:0.033, val_acc:0.976]
Epoch [58/120    avg_loss:0.020, val_acc:0.973]
Epoch [59/120    avg_loss:0.015, val_acc:0.979]
Epoch [60/120    avg_loss:0.015, val_acc:0.981]
Epoch [61/120    avg_loss:0.017, val_acc:0.974]
Epoch [62/120    avg_loss:0.031, val_acc:0.965]
Epoch [63/120    avg_loss:0.051, val_acc:0.969]
Epoch [64/120    avg_loss:0.079, val_acc:0.958]
Epoch [65/120    avg_loss:0.030, val_acc:0.972]
Epoch [66/120    avg_loss:0.015, val_acc:0.979]
Epoch [67/120    avg_loss:0.024, val_acc:0.974]
Epoch [68/120    avg_loss:0.013, val_acc:0.978]
Epoch [69/120    avg_loss:0.008, val_acc:0.977]
Epoch [70/120    avg_loss:0.011, val_acc:0.983]
Epoch [71/120    avg_loss:0.011, val_acc:0.984]
Epoch [72/120    avg_loss:0.016, val_acc:0.981]
Epoch [73/120    avg_loss:0.016, val_acc:0.973]
Epoch [74/120    avg_loss:0.035, val_acc:0.969]
Epoch [75/120    avg_loss:0.104, val_acc:0.880]
Epoch [76/120    avg_loss:0.182, val_acc:0.927]
Epoch [77/120    avg_loss:0.048, val_acc:0.954]
Epoch [78/120    avg_loss:0.046, val_acc:0.964]
Epoch [79/120    avg_loss:0.035, val_acc:0.967]
Epoch [80/120    avg_loss:0.043, val_acc:0.941]
Epoch [81/120    avg_loss:0.043, val_acc:0.972]
Epoch [82/120    avg_loss:0.020, val_acc:0.976]
Epoch [83/120    avg_loss:0.013, val_acc:0.981]
Epoch [84/120    avg_loss:0.011, val_acc:0.984]
Epoch [85/120    avg_loss:0.011, val_acc:0.981]
Epoch [86/120    avg_loss:0.016, val_acc:0.973]
Epoch [87/120    avg_loss:0.033, val_acc:0.973]
Epoch [88/120    avg_loss:0.022, val_acc:0.976]
Epoch [89/120    avg_loss:0.015, val_acc:0.981]
Epoch [90/120    avg_loss:0.009, val_acc:0.980]
Epoch [91/120    avg_loss:0.007, val_acc:0.986]
Epoch [92/120    avg_loss:0.004, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.011, val_acc:0.982]
Epoch [95/120    avg_loss:0.005, val_acc:0.984]
Epoch [96/120    avg_loss:0.005, val_acc:0.982]
Epoch [97/120    avg_loss:0.005, val_acc:0.984]
Epoch [98/120    avg_loss:0.030, val_acc:0.963]
Epoch [99/120    avg_loss:0.032, val_acc:0.925]
Epoch [100/120    avg_loss:0.052, val_acc:0.951]
Epoch [101/120    avg_loss:0.047, val_acc:0.965]
Epoch [102/120    avg_loss:0.028, val_acc:0.968]
Epoch [103/120    avg_loss:0.017, val_acc:0.989]
Epoch [104/120    avg_loss:0.009, val_acc:0.982]
Epoch [105/120    avg_loss:0.008, val_acc:0.979]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.016, val_acc:0.974]
Epoch [108/120    avg_loss:0.018, val_acc:0.973]
Epoch [109/120    avg_loss:0.012, val_acc:0.984]
Epoch [110/120    avg_loss:0.036, val_acc:0.969]
Epoch [111/120    avg_loss:0.052, val_acc:0.978]
Epoch [112/120    avg_loss:0.013, val_acc:0.983]
Epoch [113/120    avg_loss:0.008, val_acc:0.981]
Epoch [114/120    avg_loss:0.011, val_acc:0.982]
Epoch [115/120    avg_loss:0.006, val_acc:0.979]
Epoch [116/120    avg_loss:0.011, val_acc:0.977]
Epoch [117/120    avg_loss:0.009, val_acc:0.979]
Epoch [118/120    avg_loss:0.008, val_acc:0.981]
Epoch [119/120    avg_loss:0.007, val_acc:0.982]
Epoch [120/120    avg_loss:0.004, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1271    5    0    0    0    0    0    0    2    6    1    0
     0    0    0]
 [   0    0    1  730    2    2    0    0    0    1    0    6    4    0
     0    1    0]
 [   0    0    0    0  212    0    1    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    3    0    0  432    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   12    0    0    0    0    0    0    0  829   32    0    0
     1    1    0]
 [   0    0    7    1    0    0    0    0    0    1    6 2166   24    3
     2    0    0]
 [   0    0    2    2    0    0    0    0    0    0    0    0  527    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1135    4    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
    99  247    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
97.3550135501355

F1 scores:
[       nan 0.98765432 0.98450813 0.98316498 0.99297424 0.99310345
 0.99771516 1.         1.         0.94736842 0.96845794 0.97986881
 0.96255708 0.9919571  0.95498528 0.82196339 0.95757576]

Kappa:
0.9698154680324966
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f84f50737b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.277, val_acc:0.532]
Epoch [2/120    avg_loss:1.741, val_acc:0.548]
Epoch [3/120    avg_loss:1.457, val_acc:0.636]
Epoch [4/120    avg_loss:1.255, val_acc:0.681]
Epoch [5/120    avg_loss:1.019, val_acc:0.737]
Epoch [6/120    avg_loss:0.848, val_acc:0.765]
Epoch [7/120    avg_loss:0.809, val_acc:0.758]
Epoch [8/120    avg_loss:0.605, val_acc:0.837]
Epoch [9/120    avg_loss:0.599, val_acc:0.837]
Epoch [10/120    avg_loss:0.493, val_acc:0.832]
Epoch [11/120    avg_loss:0.465, val_acc:0.857]
Epoch [12/120    avg_loss:0.438, val_acc:0.889]
Epoch [13/120    avg_loss:0.333, val_acc:0.851]
Epoch [14/120    avg_loss:0.383, val_acc:0.814]
Epoch [15/120    avg_loss:0.363, val_acc:0.874]
Epoch [16/120    avg_loss:0.274, val_acc:0.898]
Epoch [17/120    avg_loss:0.241, val_acc:0.889]
Epoch [18/120    avg_loss:0.263, val_acc:0.831]
Epoch [19/120    avg_loss:0.337, val_acc:0.871]
Epoch [20/120    avg_loss:0.223, val_acc:0.910]
Epoch [21/120    avg_loss:0.178, val_acc:0.915]
Epoch [22/120    avg_loss:0.215, val_acc:0.923]
Epoch [23/120    avg_loss:0.170, val_acc:0.903]
Epoch [24/120    avg_loss:0.181, val_acc:0.932]
Epoch [25/120    avg_loss:0.131, val_acc:0.949]
Epoch [26/120    avg_loss:0.140, val_acc:0.944]
Epoch [27/120    avg_loss:0.133, val_acc:0.948]
Epoch [28/120    avg_loss:0.096, val_acc:0.949]
Epoch [29/120    avg_loss:0.129, val_acc:0.899]
Epoch [30/120    avg_loss:0.169, val_acc:0.943]
Epoch [31/120    avg_loss:0.171, val_acc:0.916]
Epoch [32/120    avg_loss:0.115, val_acc:0.932]
Epoch [33/120    avg_loss:0.114, val_acc:0.958]
Epoch [34/120    avg_loss:0.064, val_acc:0.956]
Epoch [35/120    avg_loss:0.071, val_acc:0.950]
Epoch [36/120    avg_loss:0.115, val_acc:0.945]
Epoch [37/120    avg_loss:0.114, val_acc:0.947]
Epoch [38/120    avg_loss:0.110, val_acc:0.950]
Epoch [39/120    avg_loss:0.056, val_acc:0.934]
Epoch [40/120    avg_loss:0.101, val_acc:0.943]
Epoch [41/120    avg_loss:0.077, val_acc:0.950]
Epoch [42/120    avg_loss:0.062, val_acc:0.948]
Epoch [43/120    avg_loss:0.041, val_acc:0.971]
Epoch [44/120    avg_loss:0.103, val_acc:0.924]
Epoch [45/120    avg_loss:0.049, val_acc:0.961]
Epoch [46/120    avg_loss:0.027, val_acc:0.969]
Epoch [47/120    avg_loss:0.031, val_acc:0.963]
Epoch [48/120    avg_loss:0.044, val_acc:0.966]
Epoch [49/120    avg_loss:0.041, val_acc:0.962]
Epoch [50/120    avg_loss:0.067, val_acc:0.944]
Epoch [51/120    avg_loss:0.052, val_acc:0.969]
Epoch [52/120    avg_loss:0.032, val_acc:0.969]
Epoch [53/120    avg_loss:0.017, val_acc:0.976]
Epoch [54/120    avg_loss:0.024, val_acc:0.974]
Epoch [55/120    avg_loss:0.021, val_acc:0.972]
Epoch [56/120    avg_loss:0.026, val_acc:0.974]
Epoch [57/120    avg_loss:0.019, val_acc:0.981]
Epoch [58/120    avg_loss:0.080, val_acc:0.971]
Epoch [59/120    avg_loss:0.041, val_acc:0.966]
Epoch [60/120    avg_loss:0.070, val_acc:0.960]
Epoch [61/120    avg_loss:0.062, val_acc:0.964]
Epoch [62/120    avg_loss:0.027, val_acc:0.974]
Epoch [63/120    avg_loss:0.015, val_acc:0.978]
Epoch [64/120    avg_loss:0.018, val_acc:0.969]
Epoch [65/120    avg_loss:0.034, val_acc:0.951]
Epoch [66/120    avg_loss:0.015, val_acc:0.978]
Epoch [67/120    avg_loss:0.013, val_acc:0.980]
Epoch [68/120    avg_loss:0.024, val_acc:0.957]
Epoch [69/120    avg_loss:0.030, val_acc:0.958]
Epoch [70/120    avg_loss:0.055, val_acc:0.972]
Epoch [71/120    avg_loss:0.018, val_acc:0.974]
Epoch [72/120    avg_loss:0.020, val_acc:0.976]
Epoch [73/120    avg_loss:0.012, val_acc:0.977]
Epoch [74/120    avg_loss:0.011, val_acc:0.978]
Epoch [75/120    avg_loss:0.014, val_acc:0.978]
Epoch [76/120    avg_loss:0.012, val_acc:0.978]
Epoch [77/120    avg_loss:0.013, val_acc:0.978]
Epoch [78/120    avg_loss:0.013, val_acc:0.981]
Epoch [79/120    avg_loss:0.011, val_acc:0.980]
Epoch [80/120    avg_loss:0.008, val_acc:0.980]
Epoch [81/120    avg_loss:0.011, val_acc:0.980]
Epoch [82/120    avg_loss:0.013, val_acc:0.981]
Epoch [83/120    avg_loss:0.013, val_acc:0.982]
Epoch [84/120    avg_loss:0.012, val_acc:0.983]
Epoch [85/120    avg_loss:0.007, val_acc:0.983]
Epoch [86/120    avg_loss:0.010, val_acc:0.983]
Epoch [87/120    avg_loss:0.009, val_acc:0.983]
Epoch [88/120    avg_loss:0.009, val_acc:0.983]
Epoch [89/120    avg_loss:0.008, val_acc:0.982]
Epoch [90/120    avg_loss:0.008, val_acc:0.983]
Epoch [91/120    avg_loss:0.009, val_acc:0.983]
Epoch [92/120    avg_loss:0.007, val_acc:0.983]
Epoch [93/120    avg_loss:0.009, val_acc:0.982]
Epoch [94/120    avg_loss:0.006, val_acc:0.983]
Epoch [95/120    avg_loss:0.008, val_acc:0.984]
Epoch [96/120    avg_loss:0.006, val_acc:0.984]
Epoch [97/120    avg_loss:0.007, val_acc:0.983]
Epoch [98/120    avg_loss:0.015, val_acc:0.981]
Epoch [99/120    avg_loss:0.009, val_acc:0.982]
Epoch [100/120    avg_loss:0.008, val_acc:0.983]
Epoch [101/120    avg_loss:0.010, val_acc:0.984]
Epoch [102/120    avg_loss:0.007, val_acc:0.984]
Epoch [103/120    avg_loss:0.010, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.009, val_acc:0.983]
Epoch [106/120    avg_loss:0.007, val_acc:0.983]
Epoch [107/120    avg_loss:0.008, val_acc:0.983]
Epoch [108/120    avg_loss:0.007, val_acc:0.983]
Epoch [109/120    avg_loss:0.009, val_acc:0.981]
Epoch [110/120    avg_loss:0.009, val_acc:0.982]
Epoch [111/120    avg_loss:0.007, val_acc:0.982]
Epoch [112/120    avg_loss:0.008, val_acc:0.983]
Epoch [113/120    avg_loss:0.008, val_acc:0.982]
Epoch [114/120    avg_loss:0.009, val_acc:0.982]
Epoch [115/120    avg_loss:0.008, val_acc:0.983]
Epoch [116/120    avg_loss:0.008, val_acc:0.983]
Epoch [117/120    avg_loss:0.007, val_acc:0.983]
Epoch [118/120    avg_loss:0.010, val_acc:0.983]
Epoch [119/120    avg_loss:0.008, val_acc:0.983]
Epoch [120/120    avg_loss:0.007, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1252   12    0    0    0    0    0    1    6   14    0    0
     0    0    0]
 [   0    0    0  728    0    0    0    0    0    4    0    5    5    4
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    0    0    0    0    0  839   27    0    1
     0    0    0]
 [   0    0   24    0    0    0    0    0    0    2   16 2166    1    1
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    5    2  520    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1133    6    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    82  265    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.36585365853658

F1 scores:
[       nan 0.96202532 0.97431907 0.97718121 1.         0.99884925
 0.99847561 1.         1.         0.8372093  0.96270797 0.97920434
 0.97836312 0.98404255 0.9613916  0.85346216 0.97005988]

Kappa:
0.9699437828369376
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3c1980f780>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.368, val_acc:0.488]
Epoch [2/120    avg_loss:1.827, val_acc:0.560]
Epoch [3/120    avg_loss:1.501, val_acc:0.617]
Epoch [4/120    avg_loss:1.223, val_acc:0.661]
Epoch [5/120    avg_loss:1.043, val_acc:0.705]
Epoch [6/120    avg_loss:0.911, val_acc:0.711]
Epoch [7/120    avg_loss:0.769, val_acc:0.795]
Epoch [8/120    avg_loss:0.700, val_acc:0.813]
Epoch [9/120    avg_loss:0.637, val_acc:0.810]
Epoch [10/120    avg_loss:0.609, val_acc:0.777]
Epoch [11/120    avg_loss:0.518, val_acc:0.843]
Epoch [12/120    avg_loss:0.452, val_acc:0.876]
Epoch [13/120    avg_loss:0.375, val_acc:0.883]
Epoch [14/120    avg_loss:0.357, val_acc:0.870]
Epoch [15/120    avg_loss:0.433, val_acc:0.872]
Epoch [16/120    avg_loss:0.299, val_acc:0.814]
Epoch [17/120    avg_loss:0.242, val_acc:0.886]
Epoch [18/120    avg_loss:0.184, val_acc:0.917]
Epoch [19/120    avg_loss:0.238, val_acc:0.909]
Epoch [20/120    avg_loss:0.211, val_acc:0.897]
Epoch [21/120    avg_loss:0.230, val_acc:0.905]
Epoch [22/120    avg_loss:0.183, val_acc:0.912]
Epoch [23/120    avg_loss:0.214, val_acc:0.909]
Epoch [24/120    avg_loss:0.239, val_acc:0.884]
Epoch [25/120    avg_loss:0.220, val_acc:0.914]
Epoch [26/120    avg_loss:0.157, val_acc:0.943]
Epoch [27/120    avg_loss:0.141, val_acc:0.934]
Epoch [28/120    avg_loss:0.120, val_acc:0.936]
Epoch [29/120    avg_loss:0.137, val_acc:0.927]
Epoch [30/120    avg_loss:0.115, val_acc:0.939]
Epoch [31/120    avg_loss:0.087, val_acc:0.937]
Epoch [32/120    avg_loss:0.147, val_acc:0.939]
Epoch [33/120    avg_loss:0.124, val_acc:0.906]
Epoch [34/120    avg_loss:0.081, val_acc:0.946]
Epoch [35/120    avg_loss:0.085, val_acc:0.941]
Epoch [36/120    avg_loss:0.062, val_acc:0.957]
Epoch [37/120    avg_loss:0.070, val_acc:0.958]
Epoch [38/120    avg_loss:0.068, val_acc:0.955]
Epoch [39/120    avg_loss:0.083, val_acc:0.943]
Epoch [40/120    avg_loss:0.098, val_acc:0.913]
Epoch [41/120    avg_loss:0.082, val_acc:0.952]
Epoch [42/120    avg_loss:0.291, val_acc:0.920]
Epoch [43/120    avg_loss:0.110, val_acc:0.957]
Epoch [44/120    avg_loss:0.132, val_acc:0.932]
Epoch [45/120    avg_loss:0.091, val_acc:0.929]
Epoch [46/120    avg_loss:0.062, val_acc:0.959]
Epoch [47/120    avg_loss:0.044, val_acc:0.966]
Epoch [48/120    avg_loss:0.045, val_acc:0.962]
Epoch [49/120    avg_loss:0.066, val_acc:0.955]
Epoch [50/120    avg_loss:0.047, val_acc:0.942]
Epoch [51/120    avg_loss:0.035, val_acc:0.963]
Epoch [52/120    avg_loss:0.027, val_acc:0.966]
Epoch [53/120    avg_loss:0.028, val_acc:0.959]
Epoch [54/120    avg_loss:0.028, val_acc:0.964]
Epoch [55/120    avg_loss:0.050, val_acc:0.967]
Epoch [56/120    avg_loss:0.032, val_acc:0.968]
Epoch [57/120    avg_loss:0.034, val_acc:0.967]
Epoch [58/120    avg_loss:0.021, val_acc:0.976]
Epoch [59/120    avg_loss:0.017, val_acc:0.971]
Epoch [60/120    avg_loss:0.021, val_acc:0.976]
Epoch [61/120    avg_loss:0.013, val_acc:0.973]
Epoch [62/120    avg_loss:0.012, val_acc:0.978]
Epoch [63/120    avg_loss:0.018, val_acc:0.964]
Epoch [64/120    avg_loss:0.024, val_acc:0.962]
Epoch [65/120    avg_loss:0.040, val_acc:0.970]
Epoch [66/120    avg_loss:0.016, val_acc:0.978]
Epoch [67/120    avg_loss:0.011, val_acc:0.976]
Epoch [68/120    avg_loss:0.029, val_acc:0.965]
Epoch [69/120    avg_loss:0.018, val_acc:0.976]
Epoch [70/120    avg_loss:0.012, val_acc:0.968]
Epoch [71/120    avg_loss:0.013, val_acc:0.966]
Epoch [72/120    avg_loss:0.016, val_acc:0.974]
Epoch [73/120    avg_loss:0.011, val_acc:0.978]
Epoch [74/120    avg_loss:0.020, val_acc:0.968]
Epoch [75/120    avg_loss:0.022, val_acc:0.961]
Epoch [76/120    avg_loss:0.025, val_acc:0.974]
Epoch [77/120    avg_loss:0.023, val_acc:0.972]
Epoch [78/120    avg_loss:0.008, val_acc:0.974]
Epoch [79/120    avg_loss:0.025, val_acc:0.945]
Epoch [80/120    avg_loss:0.109, val_acc:0.948]
Epoch [81/120    avg_loss:0.100, val_acc:0.946]
Epoch [82/120    avg_loss:0.031, val_acc:0.967]
Epoch [83/120    avg_loss:0.028, val_acc:0.967]
Epoch [84/120    avg_loss:0.022, val_acc:0.966]
Epoch [85/120    avg_loss:0.018, val_acc:0.970]
Epoch [86/120    avg_loss:0.014, val_acc:0.973]
Epoch [87/120    avg_loss:0.014, val_acc:0.977]
Epoch [88/120    avg_loss:0.011, val_acc:0.977]
Epoch [89/120    avg_loss:0.008, val_acc:0.978]
Epoch [90/120    avg_loss:0.008, val_acc:0.979]
Epoch [91/120    avg_loss:0.007, val_acc:0.982]
Epoch [92/120    avg_loss:0.009, val_acc:0.981]
Epoch [93/120    avg_loss:0.011, val_acc:0.979]
Epoch [94/120    avg_loss:0.007, val_acc:0.978]
Epoch [95/120    avg_loss:0.010, val_acc:0.977]
Epoch [96/120    avg_loss:0.008, val_acc:0.980]
Epoch [97/120    avg_loss:0.008, val_acc:0.979]
Epoch [98/120    avg_loss:0.007, val_acc:0.981]
Epoch [99/120    avg_loss:0.008, val_acc:0.981]
Epoch [100/120    avg_loss:0.008, val_acc:0.981]
Epoch [101/120    avg_loss:0.008, val_acc:0.980]
Epoch [102/120    avg_loss:0.009, val_acc:0.981]
Epoch [103/120    avg_loss:0.010, val_acc:0.980]
Epoch [104/120    avg_loss:0.006, val_acc:0.981]
Epoch [105/120    avg_loss:0.008, val_acc:0.980]
Epoch [106/120    avg_loss:0.008, val_acc:0.980]
Epoch [107/120    avg_loss:0.006, val_acc:0.980]
Epoch [108/120    avg_loss:0.008, val_acc:0.980]
Epoch [109/120    avg_loss:0.007, val_acc:0.980]
Epoch [110/120    avg_loss:0.008, val_acc:0.980]
Epoch [111/120    avg_loss:0.007, val_acc:0.981]
Epoch [112/120    avg_loss:0.008, val_acc:0.981]
Epoch [113/120    avg_loss:0.006, val_acc:0.981]
Epoch [114/120    avg_loss:0.007, val_acc:0.981]
Epoch [115/120    avg_loss:0.006, val_acc:0.981]
Epoch [116/120    avg_loss:0.006, val_acc:0.981]
Epoch [117/120    avg_loss:0.008, val_acc:0.981]
Epoch [118/120    avg_loss:0.007, val_acc:0.982]
Epoch [119/120    avg_loss:0.007, val_acc:0.981]
Epoch [120/120    avg_loss:0.006, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1271    2    0    2    0    0    0    0    4    5    1    0
     0    0    0]
 [   0    0    0  718    3    0    0    0    0    1    0   10   12    1
     0    2    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   14    0    0    0    0    0    0    0  835   24    1    0
     0    1    0]
 [   0    0    4    0    0    0    0    0    0    1    7 2173   23    1
     1    0    0]
 [   0    0    3    6    0    0    0    0    0    0    0    0  522    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1125   14    0]
 [   0    0    0    0    0    1    2    0    0    0    0    0    0    0
    98  246    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.22493224932249

F1 scores:
[       nan 0.96385542 0.98641832 0.97488119 0.99300699 0.99310345
 0.99695586 1.         0.997669   0.94736842 0.96980256 0.982591
 0.95429616 0.99462366 0.95057034 0.80523732 0.98224852]

Kappa:
0.9683353212251334
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f080de547f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.347, val_acc:0.504]
Epoch [2/120    avg_loss:1.760, val_acc:0.621]
Epoch [3/120    avg_loss:1.469, val_acc:0.624]
Epoch [4/120    avg_loss:1.199, val_acc:0.716]
Epoch [5/120    avg_loss:1.082, val_acc:0.639]
Epoch [6/120    avg_loss:0.926, val_acc:0.768]
Epoch [7/120    avg_loss:0.790, val_acc:0.769]
Epoch [8/120    avg_loss:0.724, val_acc:0.798]
Epoch [9/120    avg_loss:0.678, val_acc:0.744]
Epoch [10/120    avg_loss:0.585, val_acc:0.772]
Epoch [11/120    avg_loss:0.585, val_acc:0.811]
Epoch [12/120    avg_loss:0.488, val_acc:0.836]
Epoch [13/120    avg_loss:0.401, val_acc:0.857]
Epoch [14/120    avg_loss:0.415, val_acc:0.847]
Epoch [15/120    avg_loss:0.369, val_acc:0.856]
Epoch [16/120    avg_loss:0.301, val_acc:0.866]
Epoch [17/120    avg_loss:0.291, val_acc:0.870]
Epoch [18/120    avg_loss:0.287, val_acc:0.819]
Epoch [19/120    avg_loss:0.219, val_acc:0.903]
Epoch [20/120    avg_loss:0.270, val_acc:0.883]
Epoch [21/120    avg_loss:0.233, val_acc:0.905]
Epoch [22/120    avg_loss:0.247, val_acc:0.929]
Epoch [23/120    avg_loss:0.162, val_acc:0.935]
Epoch [24/120    avg_loss:0.137, val_acc:0.924]
Epoch [25/120    avg_loss:0.170, val_acc:0.934]
Epoch [26/120    avg_loss:0.201, val_acc:0.852]
Epoch [27/120    avg_loss:0.196, val_acc:0.939]
Epoch [28/120    avg_loss:0.140, val_acc:0.912]
Epoch [29/120    avg_loss:0.184, val_acc:0.914]
Epoch [30/120    avg_loss:0.181, val_acc:0.933]
Epoch [31/120    avg_loss:0.108, val_acc:0.936]
Epoch [32/120    avg_loss:0.095, val_acc:0.944]
Epoch [33/120    avg_loss:0.109, val_acc:0.933]
Epoch [34/120    avg_loss:0.149, val_acc:0.948]
Epoch [35/120    avg_loss:0.153, val_acc:0.949]
Epoch [36/120    avg_loss:0.103, val_acc:0.954]
Epoch [37/120    avg_loss:0.123, val_acc:0.949]
Epoch [38/120    avg_loss:0.100, val_acc:0.950]
Epoch [39/120    avg_loss:0.048, val_acc:0.959]
Epoch [40/120    avg_loss:0.060, val_acc:0.971]
Epoch [41/120    avg_loss:0.053, val_acc:0.965]
Epoch [42/120    avg_loss:0.058, val_acc:0.965]
Epoch [43/120    avg_loss:0.068, val_acc:0.958]
Epoch [44/120    avg_loss:0.144, val_acc:0.926]
Epoch [45/120    avg_loss:0.168, val_acc:0.940]
Epoch [46/120    avg_loss:0.082, val_acc:0.957]
Epoch [47/120    avg_loss:0.115, val_acc:0.952]
Epoch [48/120    avg_loss:0.105, val_acc:0.952]
Epoch [49/120    avg_loss:0.139, val_acc:0.917]
Epoch [50/120    avg_loss:0.110, val_acc:0.965]
Epoch [51/120    avg_loss:0.116, val_acc:0.928]
Epoch [52/120    avg_loss:0.128, val_acc:0.967]
Epoch [53/120    avg_loss:0.057, val_acc:0.968]
Epoch [54/120    avg_loss:0.031, val_acc:0.978]
Epoch [55/120    avg_loss:0.036, val_acc:0.977]
Epoch [56/120    avg_loss:0.029, val_acc:0.980]
Epoch [57/120    avg_loss:0.030, val_acc:0.981]
Epoch [58/120    avg_loss:0.029, val_acc:0.979]
Epoch [59/120    avg_loss:0.030, val_acc:0.981]
Epoch [60/120    avg_loss:0.024, val_acc:0.983]
Epoch [61/120    avg_loss:0.020, val_acc:0.983]
Epoch [62/120    avg_loss:0.026, val_acc:0.981]
Epoch [63/120    avg_loss:0.029, val_acc:0.982]
Epoch [64/120    avg_loss:0.023, val_acc:0.981]
Epoch [65/120    avg_loss:0.027, val_acc:0.980]
Epoch [66/120    avg_loss:0.025, val_acc:0.982]
Epoch [67/120    avg_loss:0.028, val_acc:0.981]
Epoch [68/120    avg_loss:0.023, val_acc:0.980]
Epoch [69/120    avg_loss:0.024, val_acc:0.981]
Epoch [70/120    avg_loss:0.032, val_acc:0.982]
Epoch [71/120    avg_loss:0.023, val_acc:0.982]
Epoch [72/120    avg_loss:0.025, val_acc:0.982]
Epoch [73/120    avg_loss:0.017, val_acc:0.983]
Epoch [74/120    avg_loss:0.019, val_acc:0.984]
Epoch [75/120    avg_loss:0.019, val_acc:0.982]
Epoch [76/120    avg_loss:0.017, val_acc:0.982]
Epoch [77/120    avg_loss:0.019, val_acc:0.982]
Epoch [78/120    avg_loss:0.016, val_acc:0.983]
Epoch [79/120    avg_loss:0.018, val_acc:0.982]
Epoch [80/120    avg_loss:0.025, val_acc:0.984]
Epoch [81/120    avg_loss:0.021, val_acc:0.984]
Epoch [82/120    avg_loss:0.019, val_acc:0.983]
Epoch [83/120    avg_loss:0.019, val_acc:0.982]
Epoch [84/120    avg_loss:0.021, val_acc:0.983]
Epoch [85/120    avg_loss:0.014, val_acc:0.984]
Epoch [86/120    avg_loss:0.021, val_acc:0.983]
Epoch [87/120    avg_loss:0.020, val_acc:0.981]
Epoch [88/120    avg_loss:0.016, val_acc:0.981]
Epoch [89/120    avg_loss:0.015, val_acc:0.982]
Epoch [90/120    avg_loss:0.022, val_acc:0.981]
Epoch [91/120    avg_loss:0.019, val_acc:0.981]
Epoch [92/120    avg_loss:0.024, val_acc:0.984]
Epoch [93/120    avg_loss:0.019, val_acc:0.982]
Epoch [94/120    avg_loss:0.015, val_acc:0.982]
Epoch [95/120    avg_loss:0.017, val_acc:0.984]
Epoch [96/120    avg_loss:0.016, val_acc:0.981]
Epoch [97/120    avg_loss:0.013, val_acc:0.982]
Epoch [98/120    avg_loss:0.015, val_acc:0.985]
Epoch [99/120    avg_loss:0.022, val_acc:0.980]
Epoch [100/120    avg_loss:0.021, val_acc:0.982]
Epoch [101/120    avg_loss:0.014, val_acc:0.985]
Epoch [102/120    avg_loss:0.011, val_acc:0.984]
Epoch [103/120    avg_loss:0.017, val_acc:0.985]
Epoch [104/120    avg_loss:0.016, val_acc:0.981]
Epoch [105/120    avg_loss:0.016, val_acc:0.983]
Epoch [106/120    avg_loss:0.014, val_acc:0.982]
Epoch [107/120    avg_loss:0.017, val_acc:0.984]
Epoch [108/120    avg_loss:0.014, val_acc:0.984]
Epoch [109/120    avg_loss:0.013, val_acc:0.984]
Epoch [110/120    avg_loss:0.014, val_acc:0.982]
Epoch [111/120    avg_loss:0.019, val_acc:0.981]
Epoch [112/120    avg_loss:0.015, val_acc:0.983]
Epoch [113/120    avg_loss:0.020, val_acc:0.981]
Epoch [114/120    avg_loss:0.015, val_acc:0.981]
Epoch [115/120    avg_loss:0.015, val_acc:0.982]
Epoch [116/120    avg_loss:0.015, val_acc:0.982]
Epoch [117/120    avg_loss:0.014, val_acc:0.982]
Epoch [118/120    avg_loss:0.011, val_acc:0.982]
Epoch [119/120    avg_loss:0.013, val_acc:0.982]
Epoch [120/120    avg_loss:0.013, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1265    1    6    1    1    0    0    0    4    7    0    0
     0    0    0]
 [   0    0    0  725    3    0    0    0    0    2    0    7    5    0
     1    4    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    1    0    0    0    0    0    0    0
     2    1    0]
 [   0    0    0    0    0    0  653    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    1    0    0    0  856   14    0    0
     0    1    0]
 [   0    0   19    1    0    0    0    0    0    0   19 2153   15    1
     1    1    0]
 [   0    0    0    4    1    0    0    0    0    0    2    0  523    0
     0    3    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1118   21    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    82  265    0]
 [   0    5    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   78]]

Accuracy:
97.31165311653116

F1 scores:
[       nan 0.91764706 0.9836703  0.98105548 0.97706422 0.99423299
 0.9946687  1.         1.         0.94736842 0.9738339  0.98041894
 0.9703154  0.99730458 0.95311168 0.82426128 0.95705521]

Kappa:
0.9693500247934157
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f61e34177f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.282, val_acc:0.501]
Epoch [2/120    avg_loss:1.737, val_acc:0.595]
Epoch [3/120    avg_loss:1.426, val_acc:0.613]
Epoch [4/120    avg_loss:1.244, val_acc:0.705]
Epoch [5/120    avg_loss:1.009, val_acc:0.688]
Epoch [6/120    avg_loss:0.952, val_acc:0.707]
Epoch [7/120    avg_loss:0.823, val_acc:0.785]
Epoch [8/120    avg_loss:0.669, val_acc:0.773]
Epoch [9/120    avg_loss:0.627, val_acc:0.746]
Epoch [10/120    avg_loss:0.545, val_acc:0.773]
Epoch [11/120    avg_loss:0.510, val_acc:0.782]
Epoch [12/120    avg_loss:0.479, val_acc:0.750]
Epoch [13/120    avg_loss:0.423, val_acc:0.782]
Epoch [14/120    avg_loss:0.443, val_acc:0.849]
Epoch [15/120    avg_loss:0.384, val_acc:0.822]
Epoch [16/120    avg_loss:0.344, val_acc:0.883]
Epoch [17/120    avg_loss:0.282, val_acc:0.895]
Epoch [18/120    avg_loss:0.305, val_acc:0.845]
Epoch [19/120    avg_loss:0.277, val_acc:0.915]
Epoch [20/120    avg_loss:0.207, val_acc:0.915]
Epoch [21/120    avg_loss:0.225, val_acc:0.892]
Epoch [22/120    avg_loss:0.191, val_acc:0.894]
Epoch [23/120    avg_loss:0.191, val_acc:0.917]
Epoch [24/120    avg_loss:0.197, val_acc:0.903]
Epoch [25/120    avg_loss:0.166, val_acc:0.910]
Epoch [26/120    avg_loss:0.130, val_acc:0.923]
Epoch [27/120    avg_loss:0.135, val_acc:0.922]
Epoch [28/120    avg_loss:0.180, val_acc:0.932]
Epoch [29/120    avg_loss:0.132, val_acc:0.923]
Epoch [30/120    avg_loss:0.124, val_acc:0.939]
Epoch [31/120    avg_loss:0.148, val_acc:0.928]
Epoch [32/120    avg_loss:0.109, val_acc:0.933]
Epoch [33/120    avg_loss:0.108, val_acc:0.941]
Epoch [34/120    avg_loss:0.102, val_acc:0.913]
Epoch [35/120    avg_loss:0.146, val_acc:0.942]
Epoch [36/120    avg_loss:0.100, val_acc:0.938]
Epoch [37/120    avg_loss:0.090, val_acc:0.950]
Epoch [38/120    avg_loss:0.068, val_acc:0.957]
Epoch [39/120    avg_loss:0.063, val_acc:0.948]
Epoch [40/120    avg_loss:0.051, val_acc:0.947]
Epoch [41/120    avg_loss:0.058, val_acc:0.930]
Epoch [42/120    avg_loss:0.061, val_acc:0.948]
Epoch [43/120    avg_loss:0.072, val_acc:0.938]
Epoch [44/120    avg_loss:0.211, val_acc:0.874]
Epoch [45/120    avg_loss:0.219, val_acc:0.942]
Epoch [46/120    avg_loss:0.143, val_acc:0.943]
Epoch [47/120    avg_loss:0.092, val_acc:0.929]
Epoch [48/120    avg_loss:0.059, val_acc:0.955]
Epoch [49/120    avg_loss:0.042, val_acc:0.959]
Epoch [50/120    avg_loss:0.054, val_acc:0.963]
Epoch [51/120    avg_loss:0.044, val_acc:0.943]
Epoch [52/120    avg_loss:0.066, val_acc:0.956]
Epoch [53/120    avg_loss:0.071, val_acc:0.914]
Epoch [54/120    avg_loss:0.100, val_acc:0.935]
Epoch [55/120    avg_loss:0.155, val_acc:0.949]
Epoch [56/120    avg_loss:0.113, val_acc:0.952]
Epoch [57/120    avg_loss:0.044, val_acc:0.968]
Epoch [58/120    avg_loss:0.040, val_acc:0.953]
Epoch [59/120    avg_loss:0.075, val_acc:0.965]
Epoch [60/120    avg_loss:0.037, val_acc:0.932]
Epoch [61/120    avg_loss:0.047, val_acc:0.971]
Epoch [62/120    avg_loss:0.037, val_acc:0.952]
Epoch [63/120    avg_loss:0.024, val_acc:0.959]
Epoch [64/120    avg_loss:0.067, val_acc:0.952]
Epoch [65/120    avg_loss:0.045, val_acc:0.964]
Epoch [66/120    avg_loss:0.020, val_acc:0.967]
Epoch [67/120    avg_loss:0.034, val_acc:0.965]
Epoch [68/120    avg_loss:0.028, val_acc:0.974]
Epoch [69/120    avg_loss:0.032, val_acc:0.965]
Epoch [70/120    avg_loss:0.029, val_acc:0.967]
Epoch [71/120    avg_loss:0.027, val_acc:0.961]
Epoch [72/120    avg_loss:0.020, val_acc:0.972]
Epoch [73/120    avg_loss:0.038, val_acc:0.965]
Epoch [74/120    avg_loss:0.035, val_acc:0.959]
Epoch [75/120    avg_loss:0.021, val_acc:0.969]
Epoch [76/120    avg_loss:0.013, val_acc:0.974]
Epoch [77/120    avg_loss:0.013, val_acc:0.976]
Epoch [78/120    avg_loss:0.017, val_acc:0.963]
Epoch [79/120    avg_loss:0.034, val_acc:0.967]
Epoch [80/120    avg_loss:0.017, val_acc:0.976]
Epoch [81/120    avg_loss:0.011, val_acc:0.978]
Epoch [82/120    avg_loss:0.008, val_acc:0.981]
Epoch [83/120    avg_loss:0.012, val_acc:0.976]
Epoch [84/120    avg_loss:0.020, val_acc:0.972]
Epoch [85/120    avg_loss:0.020, val_acc:0.954]
Epoch [86/120    avg_loss:0.036, val_acc:0.976]
Epoch [87/120    avg_loss:0.011, val_acc:0.974]
Epoch [88/120    avg_loss:0.009, val_acc:0.983]
Epoch [89/120    avg_loss:0.039, val_acc:0.955]
Epoch [90/120    avg_loss:0.018, val_acc:0.980]
Epoch [91/120    avg_loss:0.015, val_acc:0.974]
Epoch [92/120    avg_loss:0.010, val_acc:0.976]
Epoch [93/120    avg_loss:0.007, val_acc:0.980]
Epoch [94/120    avg_loss:0.013, val_acc:0.978]
Epoch [95/120    avg_loss:0.017, val_acc:0.975]
Epoch [96/120    avg_loss:0.018, val_acc:0.972]
Epoch [97/120    avg_loss:0.016, val_acc:0.966]
Epoch [98/120    avg_loss:0.016, val_acc:0.978]
Epoch [99/120    avg_loss:0.014, val_acc:0.977]
Epoch [100/120    avg_loss:0.009, val_acc:0.975]
Epoch [101/120    avg_loss:0.010, val_acc:0.971]
Epoch [102/120    avg_loss:0.009, val_acc:0.974]
Epoch [103/120    avg_loss:0.007, val_acc:0.978]
Epoch [104/120    avg_loss:0.005, val_acc:0.978]
Epoch [105/120    avg_loss:0.004, val_acc:0.978]
Epoch [106/120    avg_loss:0.005, val_acc:0.980]
Epoch [107/120    avg_loss:0.031, val_acc:0.980]
Epoch [108/120    avg_loss:0.005, val_acc:0.980]
Epoch [109/120    avg_loss:0.005, val_acc:0.980]
Epoch [110/120    avg_loss:0.006, val_acc:0.980]
Epoch [111/120    avg_loss:0.005, val_acc:0.981]
Epoch [112/120    avg_loss:0.005, val_acc:0.981]
Epoch [113/120    avg_loss:0.004, val_acc:0.981]
Epoch [114/120    avg_loss:0.005, val_acc:0.978]
Epoch [115/120    avg_loss:0.004, val_acc:0.978]
Epoch [116/120    avg_loss:0.004, val_acc:0.978]
Epoch [117/120    avg_loss:0.005, val_acc:0.978]
Epoch [118/120    avg_loss:0.004, val_acc:0.978]
Epoch [119/120    avg_loss:0.005, val_acc:0.978]
Epoch [120/120    avg_loss:0.003, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1248    3    5    6    0    0    0    0    3   15    2    1
     0    2    0]
 [   0    0    0  720    1    0    0    0    0    2    1   13    9    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    1    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0  842   29    0    0
     0    2    0]
 [   0    0   15    0    0    0    0    0    0    0    7 2161   17    1
     1    0    8]
 [   0    0    0    2    0    0    0    0    0    0    0    0  528    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1122   17    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    81  264    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.13821138211382

F1 scores:
[       nan 0.975      0.97843983 0.97826087 0.98611111 0.98737084
 0.9946687  1.         1.         0.94736842 0.9739734  0.97540059
 0.96791934 0.99462366 0.95570698 0.83149606 0.93785311]

Kappa:
0.9673560600155767
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7079613780>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.353, val_acc:0.399]
Epoch [2/120    avg_loss:1.779, val_acc:0.580]
Epoch [3/120    avg_loss:1.468, val_acc:0.636]
Epoch [4/120    avg_loss:1.161, val_acc:0.716]
Epoch [5/120    avg_loss:0.979, val_acc:0.767]
Epoch [6/120    avg_loss:0.850, val_acc:0.754]
Epoch [7/120    avg_loss:0.707, val_acc:0.815]
Epoch [8/120    avg_loss:0.610, val_acc:0.728]
Epoch [9/120    avg_loss:0.537, val_acc:0.838]
Epoch [10/120    avg_loss:0.538, val_acc:0.811]
Epoch [11/120    avg_loss:0.499, val_acc:0.731]
Epoch [12/120    avg_loss:0.431, val_acc:0.849]
Epoch [13/120    avg_loss:0.473, val_acc:0.891]
Epoch [14/120    avg_loss:0.346, val_acc:0.856]
Epoch [15/120    avg_loss:0.319, val_acc:0.863]
Epoch [16/120    avg_loss:0.397, val_acc:0.879]
Epoch [17/120    avg_loss:0.322, val_acc:0.853]
Epoch [18/120    avg_loss:0.282, val_acc:0.901]
Epoch [19/120    avg_loss:0.238, val_acc:0.906]
Epoch [20/120    avg_loss:0.242, val_acc:0.887]
Epoch [21/120    avg_loss:0.200, val_acc:0.883]
Epoch [22/120    avg_loss:0.196, val_acc:0.859]
Epoch [23/120    avg_loss:0.290, val_acc:0.906]
Epoch [24/120    avg_loss:0.168, val_acc:0.929]
Epoch [25/120    avg_loss:0.148, val_acc:0.921]
Epoch [26/120    avg_loss:0.142, val_acc:0.922]
Epoch [27/120    avg_loss:0.107, val_acc:0.923]
Epoch [28/120    avg_loss:0.126, val_acc:0.922]
Epoch [29/120    avg_loss:0.115, val_acc:0.911]
Epoch [30/120    avg_loss:0.110, val_acc:0.943]
Epoch [31/120    avg_loss:0.117, val_acc:0.940]
Epoch [32/120    avg_loss:0.103, val_acc:0.938]
Epoch [33/120    avg_loss:0.110, val_acc:0.951]
Epoch [34/120    avg_loss:0.118, val_acc:0.930]
Epoch [35/120    avg_loss:0.126, val_acc:0.944]
Epoch [36/120    avg_loss:0.094, val_acc:0.940]
Epoch [37/120    avg_loss:0.081, val_acc:0.951]
Epoch [38/120    avg_loss:0.090, val_acc:0.954]
Epoch [39/120    avg_loss:0.232, val_acc:0.921]
Epoch [40/120    avg_loss:0.077, val_acc:0.950]
Epoch [41/120    avg_loss:0.067, val_acc:0.954]
Epoch [42/120    avg_loss:0.093, val_acc:0.926]
Epoch [43/120    avg_loss:0.174, val_acc:0.944]
Epoch [44/120    avg_loss:0.071, val_acc:0.956]
Epoch [45/120    avg_loss:0.117, val_acc:0.944]
Epoch [46/120    avg_loss:0.080, val_acc:0.934]
Epoch [47/120    avg_loss:0.049, val_acc:0.957]
Epoch [48/120    avg_loss:0.041, val_acc:0.966]
Epoch [49/120    avg_loss:0.046, val_acc:0.948]
Epoch [50/120    avg_loss:0.059, val_acc:0.955]
Epoch [51/120    avg_loss:0.043, val_acc:0.959]
Epoch [52/120    avg_loss:0.044, val_acc:0.970]
Epoch [53/120    avg_loss:0.035, val_acc:0.960]
Epoch [54/120    avg_loss:0.035, val_acc:0.964]
Epoch [55/120    avg_loss:0.034, val_acc:0.967]
Epoch [56/120    avg_loss:0.053, val_acc:0.956]
Epoch [57/120    avg_loss:0.033, val_acc:0.969]
Epoch [58/120    avg_loss:0.020, val_acc:0.973]
Epoch [59/120    avg_loss:0.018, val_acc:0.973]
Epoch [60/120    avg_loss:0.028, val_acc:0.968]
Epoch [61/120    avg_loss:0.013, val_acc:0.973]
Epoch [62/120    avg_loss:0.021, val_acc:0.952]
Epoch [63/120    avg_loss:0.015, val_acc:0.972]
Epoch [64/120    avg_loss:0.025, val_acc:0.969]
Epoch [65/120    avg_loss:0.040, val_acc:0.957]
Epoch [66/120    avg_loss:0.015, val_acc:0.970]
Epoch [67/120    avg_loss:0.016, val_acc:0.972]
Epoch [68/120    avg_loss:0.015, val_acc:0.976]
Epoch [69/120    avg_loss:0.017, val_acc:0.974]
Epoch [70/120    avg_loss:0.013, val_acc:0.966]
Epoch [71/120    avg_loss:0.012, val_acc:0.973]
Epoch [72/120    avg_loss:0.014, val_acc:0.977]
Epoch [73/120    avg_loss:0.017, val_acc:0.965]
Epoch [74/120    avg_loss:0.022, val_acc:0.971]
Epoch [75/120    avg_loss:0.012, val_acc:0.956]
Epoch [76/120    avg_loss:0.016, val_acc:0.977]
Epoch [77/120    avg_loss:0.019, val_acc:0.971]
Epoch [78/120    avg_loss:0.137, val_acc:0.911]
Epoch [79/120    avg_loss:0.118, val_acc:0.897]
Epoch [80/120    avg_loss:0.184, val_acc:0.942]
Epoch [81/120    avg_loss:0.091, val_acc:0.916]
Epoch [82/120    avg_loss:0.253, val_acc:0.952]
Epoch [83/120    avg_loss:0.068, val_acc:0.933]
Epoch [84/120    avg_loss:0.050, val_acc:0.956]
Epoch [85/120    avg_loss:0.034, val_acc:0.963]
Epoch [86/120    avg_loss:0.035, val_acc:0.975]
Epoch [87/120    avg_loss:0.024, val_acc:0.963]
Epoch [88/120    avg_loss:0.023, val_acc:0.977]
Epoch [89/120    avg_loss:0.019, val_acc:0.983]
Epoch [90/120    avg_loss:0.019, val_acc:0.976]
Epoch [91/120    avg_loss:0.012, val_acc:0.980]
Epoch [92/120    avg_loss:0.019, val_acc:0.974]
Epoch [93/120    avg_loss:0.010, val_acc:0.972]
Epoch [94/120    avg_loss:0.009, val_acc:0.977]
Epoch [95/120    avg_loss:0.013, val_acc:0.970]
Epoch [96/120    avg_loss:0.008, val_acc:0.976]
Epoch [97/120    avg_loss:0.008, val_acc:0.978]
Epoch [98/120    avg_loss:0.007, val_acc:0.982]
Epoch [99/120    avg_loss:0.010, val_acc:0.975]
Epoch [100/120    avg_loss:0.022, val_acc:0.976]
Epoch [101/120    avg_loss:0.009, val_acc:0.979]
Epoch [102/120    avg_loss:0.008, val_acc:0.980]
Epoch [103/120    avg_loss:0.006, val_acc:0.981]
Epoch [104/120    avg_loss:0.006, val_acc:0.981]
Epoch [105/120    avg_loss:0.007, val_acc:0.981]
Epoch [106/120    avg_loss:0.005, val_acc:0.982]
Epoch [107/120    avg_loss:0.008, val_acc:0.982]
Epoch [108/120    avg_loss:0.005, val_acc:0.981]
Epoch [109/120    avg_loss:0.005, val_acc:0.981]
Epoch [110/120    avg_loss:0.005, val_acc:0.981]
Epoch [111/120    avg_loss:0.004, val_acc:0.981]
Epoch [112/120    avg_loss:0.007, val_acc:0.983]
Epoch [113/120    avg_loss:0.004, val_acc:0.983]
Epoch [114/120    avg_loss:0.004, val_acc:0.982]
Epoch [115/120    avg_loss:0.005, val_acc:0.982]
Epoch [116/120    avg_loss:0.005, val_acc:0.982]
Epoch [117/120    avg_loss:0.004, val_acc:0.982]
Epoch [118/120    avg_loss:0.004, val_acc:0.982]
Epoch [119/120    avg_loss:0.005, val_acc:0.982]
Epoch [120/120    avg_loss:0.004, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1266    6    0    0    0    0    0    0    6    6    0    0
     0    1    0]
 [   0    0    0  729    0    1    0    0    0    0    0    6    8    0
     2    1    0]
 [   0    0    0    0  208    0    0    0    0    0    0    0    5    0
     0    0    0]
 [   0    0    0    0    0  429    4    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0  860   10    0    0
     0    3    0]
 [   0    0   18    0    0    0    0    0    0    0    7 2182    1    0
     2    0    0]
 [   0    0    0    1    2    0    0    0    0    0    0    0  529    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    1    0    0    0    0    0    0
  1126   12    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    72  265    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.8970189701897

F1 scores:
[       nan 0.98765432 0.98483081 0.98314228 0.98345154 0.99190751
 0.98945783 0.98039216 0.99883586 1.         0.9834191  0.98867241
 0.98053753 1.         0.9611609  0.84260731 0.98224852]

Kappa:
0.9760105092909964
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd882d9b7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.291, val_acc:0.458]
Epoch [2/120    avg_loss:1.765, val_acc:0.522]
Epoch [3/120    avg_loss:1.445, val_acc:0.514]
Epoch [4/120    avg_loss:1.156, val_acc:0.680]
Epoch [5/120    avg_loss:0.955, val_acc:0.702]
Epoch [6/120    avg_loss:0.800, val_acc:0.750]
Epoch [7/120    avg_loss:0.719, val_acc:0.684]
Epoch [8/120    avg_loss:0.666, val_acc:0.787]
Epoch [9/120    avg_loss:0.602, val_acc:0.779]
Epoch [10/120    avg_loss:0.537, val_acc:0.802]
Epoch [11/120    avg_loss:0.578, val_acc:0.803]
Epoch [12/120    avg_loss:0.451, val_acc:0.843]
Epoch [13/120    avg_loss:0.443, val_acc:0.786]
Epoch [14/120    avg_loss:0.455, val_acc:0.837]
Epoch [15/120    avg_loss:0.416, val_acc:0.867]
Epoch [16/120    avg_loss:0.331, val_acc:0.849]
Epoch [17/120    avg_loss:0.401, val_acc:0.857]
Epoch [18/120    avg_loss:0.350, val_acc:0.857]
Epoch [19/120    avg_loss:0.282, val_acc:0.906]
Epoch [20/120    avg_loss:0.258, val_acc:0.864]
Epoch [21/120    avg_loss:0.250, val_acc:0.898]
Epoch [22/120    avg_loss:0.277, val_acc:0.901]
Epoch [23/120    avg_loss:0.201, val_acc:0.907]
Epoch [24/120    avg_loss:0.183, val_acc:0.881]
Epoch [25/120    avg_loss:0.252, val_acc:0.874]
Epoch [26/120    avg_loss:0.275, val_acc:0.923]
Epoch [27/120    avg_loss:0.154, val_acc:0.928]
Epoch [28/120    avg_loss:0.156, val_acc:0.932]
Epoch [29/120    avg_loss:0.154, val_acc:0.866]
Epoch [30/120    avg_loss:0.175, val_acc:0.882]
Epoch [31/120    avg_loss:0.114, val_acc:0.944]
Epoch [32/120    avg_loss:0.135, val_acc:0.918]
Epoch [33/120    avg_loss:0.102, val_acc:0.919]
Epoch [34/120    avg_loss:0.145, val_acc:0.938]
Epoch [35/120    avg_loss:0.085, val_acc:0.938]
Epoch [36/120    avg_loss:0.087, val_acc:0.921]
Epoch [37/120    avg_loss:0.105, val_acc:0.928]
Epoch [38/120    avg_loss:0.106, val_acc:0.954]
Epoch [39/120    avg_loss:0.111, val_acc:0.909]
Epoch [40/120    avg_loss:0.078, val_acc:0.906]
Epoch [41/120    avg_loss:0.134, val_acc:0.936]
Epoch [42/120    avg_loss:0.131, val_acc:0.933]
Epoch [43/120    avg_loss:0.083, val_acc:0.946]
Epoch [44/120    avg_loss:0.076, val_acc:0.939]
Epoch [45/120    avg_loss:0.093, val_acc:0.940]
Epoch [46/120    avg_loss:0.059, val_acc:0.941]
Epoch [47/120    avg_loss:0.035, val_acc:0.943]
Epoch [48/120    avg_loss:0.054, val_acc:0.934]
Epoch [49/120    avg_loss:0.095, val_acc:0.919]
Epoch [50/120    avg_loss:0.047, val_acc:0.955]
Epoch [51/120    avg_loss:0.037, val_acc:0.947]
Epoch [52/120    avg_loss:0.036, val_acc:0.959]
Epoch [53/120    avg_loss:0.030, val_acc:0.920]
Epoch [54/120    avg_loss:0.048, val_acc:0.958]
Epoch [55/120    avg_loss:0.096, val_acc:0.926]
Epoch [56/120    avg_loss:0.072, val_acc:0.960]
Epoch [57/120    avg_loss:0.034, val_acc:0.964]
Epoch [58/120    avg_loss:0.047, val_acc:0.950]
Epoch [59/120    avg_loss:0.034, val_acc:0.969]
Epoch [60/120    avg_loss:0.028, val_acc:0.968]
Epoch [61/120    avg_loss:0.058, val_acc:0.897]
Epoch [62/120    avg_loss:0.064, val_acc:0.966]
Epoch [63/120    avg_loss:0.023, val_acc:0.966]
Epoch [64/120    avg_loss:0.028, val_acc:0.965]
Epoch [65/120    avg_loss:0.021, val_acc:0.967]
Epoch [66/120    avg_loss:0.020, val_acc:0.958]
Epoch [67/120    avg_loss:0.019, val_acc:0.969]
Epoch [68/120    avg_loss:0.021, val_acc:0.967]
Epoch [69/120    avg_loss:0.023, val_acc:0.961]
Epoch [70/120    avg_loss:0.023, val_acc:0.957]
Epoch [71/120    avg_loss:0.022, val_acc:0.966]
Epoch [72/120    avg_loss:0.017, val_acc:0.966]
Epoch [73/120    avg_loss:0.025, val_acc:0.944]
Epoch [74/120    avg_loss:0.022, val_acc:0.961]
Epoch [75/120    avg_loss:0.013, val_acc:0.967]
Epoch [76/120    avg_loss:0.136, val_acc:0.835]
Epoch [77/120    avg_loss:0.196, val_acc:0.945]
Epoch [78/120    avg_loss:0.095, val_acc:0.955]
Epoch [79/120    avg_loss:0.045, val_acc:0.950]
Epoch [80/120    avg_loss:0.027, val_acc:0.958]
Epoch [81/120    avg_loss:0.020, val_acc:0.965]
Epoch [82/120    avg_loss:0.015, val_acc:0.966]
Epoch [83/120    avg_loss:0.019, val_acc:0.966]
Epoch [84/120    avg_loss:0.015, val_acc:0.965]
Epoch [85/120    avg_loss:0.021, val_acc:0.967]
Epoch [86/120    avg_loss:0.015, val_acc:0.967]
Epoch [87/120    avg_loss:0.015, val_acc:0.966]
Epoch [88/120    avg_loss:0.018, val_acc:0.968]
Epoch [89/120    avg_loss:0.015, val_acc:0.968]
Epoch [90/120    avg_loss:0.016, val_acc:0.965]
Epoch [91/120    avg_loss:0.015, val_acc:0.966]
Epoch [92/120    avg_loss:0.016, val_acc:0.967]
Epoch [93/120    avg_loss:0.015, val_acc:0.968]
Epoch [94/120    avg_loss:0.011, val_acc:0.968]
Epoch [95/120    avg_loss:0.014, val_acc:0.968]
Epoch [96/120    avg_loss:0.012, val_acc:0.969]
Epoch [97/120    avg_loss:0.015, val_acc:0.969]
Epoch [98/120    avg_loss:0.015, val_acc:0.968]
Epoch [99/120    avg_loss:0.014, val_acc:0.968]
Epoch [100/120    avg_loss:0.014, val_acc:0.968]
Epoch [101/120    avg_loss:0.012, val_acc:0.968]
Epoch [102/120    avg_loss:0.013, val_acc:0.968]
Epoch [103/120    avg_loss:0.012, val_acc:0.967]
Epoch [104/120    avg_loss:0.023, val_acc:0.967]
Epoch [105/120    avg_loss:0.011, val_acc:0.967]
Epoch [106/120    avg_loss:0.012, val_acc:0.968]
Epoch [107/120    avg_loss:0.011, val_acc:0.968]
Epoch [108/120    avg_loss:0.012, val_acc:0.967]
Epoch [109/120    avg_loss:0.010, val_acc:0.967]
Epoch [110/120    avg_loss:0.010, val_acc:0.967]
Epoch [111/120    avg_loss:0.014, val_acc:0.967]
Epoch [112/120    avg_loss:0.017, val_acc:0.967]
Epoch [113/120    avg_loss:0.019, val_acc:0.967]
Epoch [114/120    avg_loss:0.016, val_acc:0.967]
Epoch [115/120    avg_loss:0.017, val_acc:0.967]
Epoch [116/120    avg_loss:0.014, val_acc:0.967]
Epoch [117/120    avg_loss:0.014, val_acc:0.967]
Epoch [118/120    avg_loss:0.010, val_acc:0.967]
Epoch [119/120    avg_loss:0.018, val_acc:0.967]
Epoch [120/120    avg_loss:0.010, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   31    1    0    0    0    0    0    0    0    9    0    0    0
     0    0    0]
 [   0    0 1253    5    0    5    0    0    0    0    7   15    0    0
     0    0    0]
 [   0    0    0  728    5    0    0    0    0    0    0    5    7    0
     1    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    3    2    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0  851   23    0    0
     0    0    0]
 [   0    0   17    0    0    0    0    0    0    0   16 2157   18    0
     0    2    0]
 [   0    0    0    6    2    0    0    0    0    0    0    0  522    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  184
     0    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1119   20    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    95  251    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.92140921409214

F1 scores:
[       nan 0.84931507 0.98005475 0.97981157 0.98383372 0.98148148
 0.99391172 0.96153846 0.99883586 1.         0.96814562 0.97734481
 0.96487985 0.99728997 0.94830508 0.80448718 0.98224852]

Kappa:
0.9648797799706619
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe4b079d780>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.314, val_acc:0.511]
Epoch [2/120    avg_loss:1.759, val_acc:0.596]
Epoch [3/120    avg_loss:1.447, val_acc:0.610]
Epoch [4/120    avg_loss:1.262, val_acc:0.626]
Epoch [5/120    avg_loss:1.081, val_acc:0.700]
Epoch [6/120    avg_loss:0.973, val_acc:0.708]
Epoch [7/120    avg_loss:0.764, val_acc:0.785]
Epoch [8/120    avg_loss:0.711, val_acc:0.802]
Epoch [9/120    avg_loss:0.595, val_acc:0.761]
Epoch [10/120    avg_loss:0.565, val_acc:0.764]
Epoch [11/120    avg_loss:0.455, val_acc:0.812]
Epoch [12/120    avg_loss:0.533, val_acc:0.845]
Epoch [13/120    avg_loss:0.436, val_acc:0.831]
Epoch [14/120    avg_loss:0.360, val_acc:0.858]
Epoch [15/120    avg_loss:0.380, val_acc:0.812]
Epoch [16/120    avg_loss:0.387, val_acc:0.852]
Epoch [17/120    avg_loss:0.326, val_acc:0.867]
Epoch [18/120    avg_loss:0.352, val_acc:0.863]
Epoch [19/120    avg_loss:0.263, val_acc:0.844]
Epoch [20/120    avg_loss:0.279, val_acc:0.860]
Epoch [21/120    avg_loss:0.232, val_acc:0.892]
Epoch [22/120    avg_loss:0.216, val_acc:0.882]
Epoch [23/120    avg_loss:0.168, val_acc:0.922]
Epoch [24/120    avg_loss:0.151, val_acc:0.913]
Epoch [25/120    avg_loss:0.155, val_acc:0.910]
Epoch [26/120    avg_loss:0.128, val_acc:0.929]
Epoch [27/120    avg_loss:0.122, val_acc:0.913]
Epoch [28/120    avg_loss:0.168, val_acc:0.925]
Epoch [29/120    avg_loss:0.123, val_acc:0.931]
Epoch [30/120    avg_loss:0.130, val_acc:0.902]
Epoch [31/120    avg_loss:0.181, val_acc:0.858]
Epoch [32/120    avg_loss:0.181, val_acc:0.884]
Epoch [33/120    avg_loss:0.174, val_acc:0.901]
Epoch [34/120    avg_loss:0.109, val_acc:0.943]
Epoch [35/120    avg_loss:0.138, val_acc:0.929]
Epoch [36/120    avg_loss:0.095, val_acc:0.936]
Epoch [37/120    avg_loss:0.077, val_acc:0.940]
Epoch [38/120    avg_loss:0.115, val_acc:0.917]
Epoch [39/120    avg_loss:0.104, val_acc:0.947]
Epoch [40/120    avg_loss:0.102, val_acc:0.935]
Epoch [41/120    avg_loss:0.115, val_acc:0.916]
Epoch [42/120    avg_loss:0.121, val_acc:0.924]
Epoch [43/120    avg_loss:0.067, val_acc:0.948]
Epoch [44/120    avg_loss:0.074, val_acc:0.940]
Epoch [45/120    avg_loss:0.063, val_acc:0.929]
Epoch [46/120    avg_loss:0.060, val_acc:0.940]
Epoch [47/120    avg_loss:0.073, val_acc:0.934]
Epoch [48/120    avg_loss:0.052, val_acc:0.943]
Epoch [49/120    avg_loss:0.039, val_acc:0.953]
Epoch [50/120    avg_loss:0.058, val_acc:0.954]
Epoch [51/120    avg_loss:0.069, val_acc:0.946]
Epoch [52/120    avg_loss:0.099, val_acc:0.949]
Epoch [53/120    avg_loss:0.050, val_acc:0.950]
Epoch [54/120    avg_loss:0.051, val_acc:0.959]
Epoch [55/120    avg_loss:0.028, val_acc:0.958]
Epoch [56/120    avg_loss:0.027, val_acc:0.955]
Epoch [57/120    avg_loss:0.046, val_acc:0.936]
Epoch [58/120    avg_loss:0.043, val_acc:0.954]
Epoch [59/120    avg_loss:0.024, val_acc:0.945]
Epoch [60/120    avg_loss:0.053, val_acc:0.949]
Epoch [61/120    avg_loss:0.033, val_acc:0.957]
Epoch [62/120    avg_loss:0.031, val_acc:0.959]
Epoch [63/120    avg_loss:0.035, val_acc:0.958]
Epoch [64/120    avg_loss:0.036, val_acc:0.958]
Epoch [65/120    avg_loss:0.022, val_acc:0.966]
Epoch [66/120    avg_loss:0.046, val_acc:0.940]
Epoch [67/120    avg_loss:0.043, val_acc:0.956]
Epoch [68/120    avg_loss:0.035, val_acc:0.954]
Epoch [69/120    avg_loss:0.032, val_acc:0.958]
Epoch [70/120    avg_loss:0.036, val_acc:0.964]
Epoch [71/120    avg_loss:0.017, val_acc:0.956]
Epoch [72/120    avg_loss:0.023, val_acc:0.954]
Epoch [73/120    avg_loss:0.020, val_acc:0.960]
Epoch [74/120    avg_loss:0.018, val_acc:0.963]
Epoch [75/120    avg_loss:0.023, val_acc:0.970]
Epoch [76/120    avg_loss:0.014, val_acc:0.959]
Epoch [77/120    avg_loss:0.032, val_acc:0.956]
Epoch [78/120    avg_loss:0.059, val_acc:0.915]
Epoch [79/120    avg_loss:0.133, val_acc:0.962]
Epoch [80/120    avg_loss:0.045, val_acc:0.958]
Epoch [81/120    avg_loss:0.029, val_acc:0.963]
Epoch [82/120    avg_loss:0.018, val_acc:0.962]
Epoch [83/120    avg_loss:0.017, val_acc:0.967]
Epoch [84/120    avg_loss:0.017, val_acc:0.953]
Epoch [85/120    avg_loss:0.010, val_acc:0.963]
Epoch [86/120    avg_loss:0.017, val_acc:0.958]
Epoch [87/120    avg_loss:0.033, val_acc:0.952]
Epoch [88/120    avg_loss:0.019, val_acc:0.948]
Epoch [89/120    avg_loss:0.024, val_acc:0.962]
Epoch [90/120    avg_loss:0.014, val_acc:0.964]
Epoch [91/120    avg_loss:0.012, val_acc:0.965]
Epoch [92/120    avg_loss:0.010, val_acc:0.965]
Epoch [93/120    avg_loss:0.007, val_acc:0.966]
Epoch [94/120    avg_loss:0.009, val_acc:0.966]
Epoch [95/120    avg_loss:0.010, val_acc:0.968]
Epoch [96/120    avg_loss:0.009, val_acc:0.967]
Epoch [97/120    avg_loss:0.008, val_acc:0.967]
Epoch [98/120    avg_loss:0.009, val_acc:0.968]
Epoch [99/120    avg_loss:0.007, val_acc:0.969]
Epoch [100/120    avg_loss:0.007, val_acc:0.968]
Epoch [101/120    avg_loss:0.009, val_acc:0.967]
Epoch [102/120    avg_loss:0.006, val_acc:0.968]
Epoch [103/120    avg_loss:0.011, val_acc:0.968]
Epoch [104/120    avg_loss:0.007, val_acc:0.968]
Epoch [105/120    avg_loss:0.007, val_acc:0.969]
Epoch [106/120    avg_loss:0.008, val_acc:0.969]
Epoch [107/120    avg_loss:0.007, val_acc:0.969]
Epoch [108/120    avg_loss:0.006, val_acc:0.969]
Epoch [109/120    avg_loss:0.006, val_acc:0.968]
Epoch [110/120    avg_loss:0.006, val_acc:0.969]
Epoch [111/120    avg_loss:0.007, val_acc:0.968]
Epoch [112/120    avg_loss:0.006, val_acc:0.969]
Epoch [113/120    avg_loss:0.007, val_acc:0.968]
Epoch [114/120    avg_loss:0.007, val_acc:0.968]
Epoch [115/120    avg_loss:0.007, val_acc:0.968]
Epoch [116/120    avg_loss:0.005, val_acc:0.968]
Epoch [117/120    avg_loss:0.007, val_acc:0.968]
Epoch [118/120    avg_loss:0.010, val_acc:0.968]
Epoch [119/120    avg_loss:0.007, val_acc:0.968]
Epoch [120/120    avg_loss:0.007, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1261    4    2    2    0    0    0    2    4    8    1    0
     0    1    0]
 [   0    0    0  737    1    0    0    0    0    1    0    5    2    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    1    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0  839   34    0    0
     0    1    0]
 [   0    0   19    0    0    0    0    0    0    0    7 2184    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    1    1  525    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    2    0    0    0    0
  1120   14    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    92  255    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.59349593495935

F1 scores:
[       nan 0.975      0.98285269 0.98859826 0.99300699 0.99198167
 0.99847793 0.98039216 1.         0.87804878 0.97106481 0.98334084
 0.98777046 1.         0.95238095 0.82125604 0.98224852]

Kappa:
0.972534108594927
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe3607e6828>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.324, val_acc:0.385]
Epoch [2/120    avg_loss:1.766, val_acc:0.515]
Epoch [3/120    avg_loss:1.557, val_acc:0.579]
Epoch [4/120    avg_loss:1.303, val_acc:0.646]
Epoch [5/120    avg_loss:1.040, val_acc:0.738]
Epoch [6/120    avg_loss:0.831, val_acc:0.729]
Epoch [7/120    avg_loss:0.767, val_acc:0.757]
Epoch [8/120    avg_loss:0.694, val_acc:0.797]
Epoch [9/120    avg_loss:0.671, val_acc:0.827]
Epoch [10/120    avg_loss:0.565, val_acc:0.789]
Epoch [11/120    avg_loss:0.581, val_acc:0.795]
Epoch [12/120    avg_loss:0.479, val_acc:0.817]
Epoch [13/120    avg_loss:0.476, val_acc:0.825]
Epoch [14/120    avg_loss:0.495, val_acc:0.830]
Epoch [15/120    avg_loss:0.412, val_acc:0.866]
Epoch [16/120    avg_loss:0.318, val_acc:0.802]
Epoch [17/120    avg_loss:0.354, val_acc:0.863]
Epoch [18/120    avg_loss:0.293, val_acc:0.903]
Epoch [19/120    avg_loss:0.292, val_acc:0.886]
Epoch [20/120    avg_loss:0.249, val_acc:0.842]
Epoch [21/120    avg_loss:0.275, val_acc:0.859]
Epoch [22/120    avg_loss:0.194, val_acc:0.904]
Epoch [23/120    avg_loss:0.203, val_acc:0.907]
Epoch [24/120    avg_loss:0.203, val_acc:0.903]
Epoch [25/120    avg_loss:0.209, val_acc:0.917]
Epoch [26/120    avg_loss:0.167, val_acc:0.834]
Epoch [27/120    avg_loss:0.195, val_acc:0.934]
Epoch [28/120    avg_loss:0.166, val_acc:0.871]
Epoch [29/120    avg_loss:0.152, val_acc:0.947]
Epoch [30/120    avg_loss:0.096, val_acc:0.949]
Epoch [31/120    avg_loss:0.106, val_acc:0.947]
Epoch [32/120    avg_loss:0.099, val_acc:0.922]
Epoch [33/120    avg_loss:0.147, val_acc:0.918]
Epoch [34/120    avg_loss:0.176, val_acc:0.948]
Epoch [35/120    avg_loss:0.117, val_acc:0.945]
Epoch [36/120    avg_loss:0.087, val_acc:0.942]
Epoch [37/120    avg_loss:0.102, val_acc:0.941]
Epoch [38/120    avg_loss:0.095, val_acc:0.965]
Epoch [39/120    avg_loss:0.054, val_acc:0.956]
Epoch [40/120    avg_loss:0.073, val_acc:0.960]
Epoch [41/120    avg_loss:0.056, val_acc:0.960]
Epoch [42/120    avg_loss:0.076, val_acc:0.919]
Epoch [43/120    avg_loss:0.101, val_acc:0.879]
Epoch [44/120    avg_loss:0.078, val_acc:0.949]
Epoch [45/120    avg_loss:0.067, val_acc:0.954]
Epoch [46/120    avg_loss:0.075, val_acc:0.926]
Epoch [47/120    avg_loss:0.057, val_acc:0.966]
Epoch [48/120    avg_loss:0.040, val_acc:0.957]
Epoch [49/120    avg_loss:0.055, val_acc:0.934]
Epoch [50/120    avg_loss:0.059, val_acc:0.943]
Epoch [51/120    avg_loss:0.046, val_acc:0.948]
Epoch [52/120    avg_loss:0.110, val_acc:0.945]
Epoch [53/120    avg_loss:0.103, val_acc:0.867]
Epoch [54/120    avg_loss:0.065, val_acc:0.969]
Epoch [55/120    avg_loss:0.028, val_acc:0.967]
Epoch [56/120    avg_loss:0.056, val_acc:0.955]
Epoch [57/120    avg_loss:0.080, val_acc:0.933]
Epoch [58/120    avg_loss:0.074, val_acc:0.963]
Epoch [59/120    avg_loss:0.159, val_acc:0.891]
Epoch [60/120    avg_loss:0.161, val_acc:0.941]
Epoch [61/120    avg_loss:0.057, val_acc:0.958]
Epoch [62/120    avg_loss:0.052, val_acc:0.960]
Epoch [63/120    avg_loss:0.051, val_acc:0.959]
Epoch [64/120    avg_loss:0.032, val_acc:0.969]
Epoch [65/120    avg_loss:0.021, val_acc:0.910]
Epoch [66/120    avg_loss:0.191, val_acc:0.943]
Epoch [67/120    avg_loss:0.073, val_acc:0.953]
Epoch [68/120    avg_loss:0.064, val_acc:0.952]
Epoch [69/120    avg_loss:0.046, val_acc:0.958]
Epoch [70/120    avg_loss:0.040, val_acc:0.964]
Epoch [71/120    avg_loss:0.030, val_acc:0.971]
Epoch [72/120    avg_loss:0.027, val_acc:0.969]
Epoch [73/120    avg_loss:0.016, val_acc:0.975]
Epoch [74/120    avg_loss:0.013, val_acc:0.972]
Epoch [75/120    avg_loss:0.016, val_acc:0.976]
Epoch [76/120    avg_loss:0.016, val_acc:0.973]
Epoch [77/120    avg_loss:0.050, val_acc:0.971]
Epoch [78/120    avg_loss:0.035, val_acc:0.954]
Epoch [79/120    avg_loss:0.033, val_acc:0.972]
Epoch [80/120    avg_loss:0.019, val_acc:0.974]
Epoch [81/120    avg_loss:0.011, val_acc:0.975]
Epoch [82/120    avg_loss:0.022, val_acc:0.967]
Epoch [83/120    avg_loss:0.027, val_acc:0.974]
Epoch [84/120    avg_loss:0.017, val_acc:0.969]
Epoch [85/120    avg_loss:0.017, val_acc:0.971]
Epoch [86/120    avg_loss:0.009, val_acc:0.975]
Epoch [87/120    avg_loss:0.008, val_acc:0.981]
Epoch [88/120    avg_loss:0.019, val_acc:0.970]
Epoch [89/120    avg_loss:0.017, val_acc:0.975]
Epoch [90/120    avg_loss:0.012, val_acc:0.974]
Epoch [91/120    avg_loss:0.009, val_acc:0.977]
Epoch [92/120    avg_loss:0.013, val_acc:0.972]
Epoch [93/120    avg_loss:0.009, val_acc:0.977]
Epoch [94/120    avg_loss:0.006, val_acc:0.975]
Epoch [95/120    avg_loss:0.005, val_acc:0.981]
Epoch [96/120    avg_loss:0.007, val_acc:0.979]
Epoch [97/120    avg_loss:0.008, val_acc:0.981]
Epoch [98/120    avg_loss:0.006, val_acc:0.979]
Epoch [99/120    avg_loss:0.005, val_acc:0.980]
Epoch [100/120    avg_loss:0.005, val_acc:0.982]
Epoch [101/120    avg_loss:0.004, val_acc:0.982]
Epoch [102/120    avg_loss:0.008, val_acc:0.972]
Epoch [103/120    avg_loss:0.008, val_acc:0.981]
Epoch [104/120    avg_loss:0.010, val_acc:0.971]
Epoch [105/120    avg_loss:0.009, val_acc:0.983]
Epoch [106/120    avg_loss:0.006, val_acc:0.980]
Epoch [107/120    avg_loss:0.006, val_acc:0.978]
Epoch [108/120    avg_loss:0.006, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.981]
Epoch [110/120    avg_loss:0.004, val_acc:0.982]
Epoch [111/120    avg_loss:0.007, val_acc:0.980]
Epoch [112/120    avg_loss:0.004, val_acc:0.983]
Epoch [113/120    avg_loss:0.008, val_acc:0.982]
Epoch [114/120    avg_loss:0.005, val_acc:0.980]
Epoch [115/120    avg_loss:0.006, val_acc:0.984]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.004, val_acc:0.986]
Epoch [118/120    avg_loss:0.003, val_acc:0.983]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.004, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1246    0    1    0    1    0    0    0    4   33    0    0
     0    0    0]
 [   0    0    0  725    8    0    0    0    0    6    1    6    1    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    2    0    0    0    0  857    8    0    0
     1    2    0]
 [   0    0    2    0    0    0    3    0    0    0    3 2193    7    1
     1    0    0]
 [   0    0    3    0    4    3    0    0    0    0    4    1  513    0
     0    2    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    84  263    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.6260162601626

F1 scores:
[       nan 0.975      0.98071625 0.98505435 0.97038724 0.99428571
 0.9969651  1.         1.         0.85714286 0.98167239 0.98539654
 0.97159091 0.99730458 0.95700298 0.83624801 0.97076023]

Kappa:
0.972908569454193
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2c1d313748>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.362, val_acc:0.377]
Epoch [2/120    avg_loss:1.769, val_acc:0.547]
Epoch [3/120    avg_loss:1.410, val_acc:0.653]
Epoch [4/120    avg_loss:1.087, val_acc:0.657]
Epoch [5/120    avg_loss:1.017, val_acc:0.710]
Epoch [6/120    avg_loss:0.853, val_acc:0.738]
Epoch [7/120    avg_loss:0.784, val_acc:0.773]
Epoch [8/120    avg_loss:0.621, val_acc:0.814]
Epoch [9/120    avg_loss:0.582, val_acc:0.804]
Epoch [10/120    avg_loss:0.636, val_acc:0.821]
Epoch [11/120    avg_loss:0.517, val_acc:0.853]
Epoch [12/120    avg_loss:0.429, val_acc:0.863]
Epoch [13/120    avg_loss:0.450, val_acc:0.861]
Epoch [14/120    avg_loss:0.356, val_acc:0.846]
Epoch [15/120    avg_loss:0.432, val_acc:0.824]
Epoch [16/120    avg_loss:0.333, val_acc:0.874]
Epoch [17/120    avg_loss:0.252, val_acc:0.881]
Epoch [18/120    avg_loss:0.395, val_acc:0.872]
Epoch [19/120    avg_loss:0.293, val_acc:0.901]
Epoch [20/120    avg_loss:0.208, val_acc:0.891]
Epoch [21/120    avg_loss:0.180, val_acc:0.898]
Epoch [22/120    avg_loss:0.233, val_acc:0.898]
Epoch [23/120    avg_loss:0.554, val_acc:0.659]
Epoch [24/120    avg_loss:0.556, val_acc:0.866]
Epoch [25/120    avg_loss:0.263, val_acc:0.903]
Epoch [26/120    avg_loss:0.249, val_acc:0.877]
Epoch [27/120    avg_loss:0.307, val_acc:0.831]
Epoch [28/120    avg_loss:0.217, val_acc:0.911]
Epoch [29/120    avg_loss:0.146, val_acc:0.914]
Epoch [30/120    avg_loss:0.123, val_acc:0.913]
Epoch [31/120    avg_loss:0.122, val_acc:0.932]
Epoch [32/120    avg_loss:0.113, val_acc:0.930]
Epoch [33/120    avg_loss:0.101, val_acc:0.927]
Epoch [34/120    avg_loss:0.093, val_acc:0.902]
Epoch [35/120    avg_loss:0.100, val_acc:0.923]
Epoch [36/120    avg_loss:0.106, val_acc:0.926]
Epoch [37/120    avg_loss:0.103, val_acc:0.941]
Epoch [38/120    avg_loss:0.066, val_acc:0.929]
Epoch [39/120    avg_loss:0.078, val_acc:0.941]
Epoch [40/120    avg_loss:0.088, val_acc:0.930]
Epoch [41/120    avg_loss:0.071, val_acc:0.954]
Epoch [42/120    avg_loss:0.055, val_acc:0.956]
Epoch [43/120    avg_loss:0.055, val_acc:0.943]
Epoch [44/120    avg_loss:0.059, val_acc:0.932]
Epoch [45/120    avg_loss:0.085, val_acc:0.956]
Epoch [46/120    avg_loss:0.072, val_acc:0.951]
Epoch [47/120    avg_loss:0.081, val_acc:0.942]
Epoch [48/120    avg_loss:0.087, val_acc:0.902]
Epoch [49/120    avg_loss:0.101, val_acc:0.952]
Epoch [50/120    avg_loss:0.056, val_acc:0.934]
Epoch [51/120    avg_loss:0.075, val_acc:0.925]
Epoch [52/120    avg_loss:0.052, val_acc:0.955]
Epoch [53/120    avg_loss:0.059, val_acc:0.951]
Epoch [54/120    avg_loss:0.044, val_acc:0.963]
Epoch [55/120    avg_loss:0.026, val_acc:0.960]
Epoch [56/120    avg_loss:0.023, val_acc:0.962]
Epoch [57/120    avg_loss:0.042, val_acc:0.959]
Epoch [58/120    avg_loss:0.030, val_acc:0.953]
Epoch [59/120    avg_loss:0.035, val_acc:0.945]
Epoch [60/120    avg_loss:0.038, val_acc:0.956]
Epoch [61/120    avg_loss:0.028, val_acc:0.961]
Epoch [62/120    avg_loss:0.021, val_acc:0.962]
Epoch [63/120    avg_loss:0.021, val_acc:0.949]
Epoch [64/120    avg_loss:0.049, val_acc:0.950]
Epoch [65/120    avg_loss:0.032, val_acc:0.962]
Epoch [66/120    avg_loss:0.028, val_acc:0.952]
Epoch [67/120    avg_loss:0.024, val_acc:0.970]
Epoch [68/120    avg_loss:0.013, val_acc:0.971]
Epoch [69/120    avg_loss:0.021, val_acc:0.960]
Epoch [70/120    avg_loss:0.020, val_acc:0.969]
Epoch [71/120    avg_loss:0.020, val_acc:0.965]
Epoch [72/120    avg_loss:0.014, val_acc:0.975]
Epoch [73/120    avg_loss:0.019, val_acc:0.962]
Epoch [74/120    avg_loss:0.014, val_acc:0.971]
Epoch [75/120    avg_loss:0.008, val_acc:0.972]
Epoch [76/120    avg_loss:0.013, val_acc:0.957]
Epoch [77/120    avg_loss:0.009, val_acc:0.952]
Epoch [78/120    avg_loss:0.018, val_acc:0.967]
Epoch [79/120    avg_loss:0.018, val_acc:0.955]
Epoch [80/120    avg_loss:0.027, val_acc:0.954]
Epoch [81/120    avg_loss:0.011, val_acc:0.972]
Epoch [82/120    avg_loss:0.028, val_acc:0.955]
Epoch [83/120    avg_loss:0.016, val_acc:0.961]
Epoch [84/120    avg_loss:0.025, val_acc:0.957]
Epoch [85/120    avg_loss:0.017, val_acc:0.956]
Epoch [86/120    avg_loss:0.020, val_acc:0.967]
Epoch [87/120    avg_loss:0.011, val_acc:0.975]
Epoch [88/120    avg_loss:0.009, val_acc:0.974]
Epoch [89/120    avg_loss:0.007, val_acc:0.976]
Epoch [90/120    avg_loss:0.021, val_acc:0.974]
Epoch [91/120    avg_loss:0.008, val_acc:0.976]
Epoch [92/120    avg_loss:0.006, val_acc:0.975]
Epoch [93/120    avg_loss:0.005, val_acc:0.976]
Epoch [94/120    avg_loss:0.007, val_acc:0.978]
Epoch [95/120    avg_loss:0.007, val_acc:0.977]
Epoch [96/120    avg_loss:0.008, val_acc:0.978]
Epoch [97/120    avg_loss:0.006, val_acc:0.978]
Epoch [98/120    avg_loss:0.005, val_acc:0.980]
Epoch [99/120    avg_loss:0.008, val_acc:0.975]
Epoch [100/120    avg_loss:0.008, val_acc:0.977]
Epoch [101/120    avg_loss:0.005, val_acc:0.977]
Epoch [102/120    avg_loss:0.006, val_acc:0.976]
Epoch [103/120    avg_loss:0.006, val_acc:0.976]
Epoch [104/120    avg_loss:0.007, val_acc:0.977]
Epoch [105/120    avg_loss:0.006, val_acc:0.977]
Epoch [106/120    avg_loss:0.007, val_acc:0.980]
Epoch [107/120    avg_loss:0.005, val_acc:0.980]
Epoch [108/120    avg_loss:0.006, val_acc:0.977]
Epoch [109/120    avg_loss:0.005, val_acc:0.976]
Epoch [110/120    avg_loss:0.005, val_acc:0.977]
Epoch [111/120    avg_loss:0.006, val_acc:0.976]
Epoch [112/120    avg_loss:0.008, val_acc:0.977]
Epoch [113/120    avg_loss:0.005, val_acc:0.975]
Epoch [114/120    avg_loss:0.007, val_acc:0.978]
Epoch [115/120    avg_loss:0.007, val_acc:0.975]
Epoch [116/120    avg_loss:0.004, val_acc:0.975]
Epoch [117/120    avg_loss:0.005, val_acc:0.976]
Epoch [118/120    avg_loss:0.005, val_acc:0.981]
Epoch [119/120    avg_loss:0.005, val_acc:0.980]
Epoch [120/120    avg_loss:0.004, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1260    2    0    0    0    0    0    0    6   15    0    0
     0    2    0]
 [   0    0    0  728    2    0    0    0    0    2    1    0    8    6
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    2    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    3    0    0    0  857    8    0    0
     4    2    0]
 [   0    0    7    0    0    0    1    0    0    0    4 2191    4    2
     1    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0  530    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1127   11    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    37  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.40650406504065

F1 scores:
[       nan 0.96202532 0.98668755 0.98511502 0.9953271  0.99884925
 0.99237805 1.         1.         0.91891892 0.9805492  0.9898351
 0.98330241 0.97883598 0.97491349 0.92261905 0.97619048]

Kappa:
0.9818283924883372
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f819a0d0780>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.350, val_acc:0.526]
Epoch [2/120    avg_loss:1.754, val_acc:0.592]
Epoch [3/120    avg_loss:1.464, val_acc:0.654]
Epoch [4/120    avg_loss:1.220, val_acc:0.715]
Epoch [5/120    avg_loss:1.024, val_acc:0.700]
Epoch [6/120    avg_loss:0.912, val_acc:0.757]
Epoch [7/120    avg_loss:0.831, val_acc:0.740]
Epoch [8/120    avg_loss:0.738, val_acc:0.728]
Epoch [9/120    avg_loss:0.644, val_acc:0.783]
Epoch [10/120    avg_loss:0.576, val_acc:0.843]
Epoch [11/120    avg_loss:0.486, val_acc:0.833]
Epoch [12/120    avg_loss:0.423, val_acc:0.853]
Epoch [13/120    avg_loss:0.529, val_acc:0.826]
Epoch [14/120    avg_loss:0.416, val_acc:0.867]
Epoch [15/120    avg_loss:0.392, val_acc:0.849]
Epoch [16/120    avg_loss:0.355, val_acc:0.882]
Epoch [17/120    avg_loss:0.290, val_acc:0.875]
Epoch [18/120    avg_loss:0.299, val_acc:0.858]
Epoch [19/120    avg_loss:0.281, val_acc:0.883]
Epoch [20/120    avg_loss:0.306, val_acc:0.916]
Epoch [21/120    avg_loss:0.241, val_acc:0.896]
Epoch [22/120    avg_loss:0.282, val_acc:0.881]
Epoch [23/120    avg_loss:0.340, val_acc:0.884]
Epoch [24/120    avg_loss:0.231, val_acc:0.902]
Epoch [25/120    avg_loss:0.154, val_acc:0.911]
Epoch [26/120    avg_loss:0.265, val_acc:0.859]
Epoch [27/120    avg_loss:0.261, val_acc:0.896]
Epoch [28/120    avg_loss:0.164, val_acc:0.926]
Epoch [29/120    avg_loss:0.175, val_acc:0.920]
Epoch [30/120    avg_loss:0.133, val_acc:0.911]
Epoch [31/120    avg_loss:0.239, val_acc:0.928]
Epoch [32/120    avg_loss:0.129, val_acc:0.934]
Epoch [33/120    avg_loss:0.115, val_acc:0.958]
Epoch [34/120    avg_loss:0.074, val_acc:0.954]
Epoch [35/120    avg_loss:0.105, val_acc:0.875]
Epoch [36/120    avg_loss:0.125, val_acc:0.933]
Epoch [37/120    avg_loss:0.125, val_acc:0.946]
Epoch [38/120    avg_loss:0.098, val_acc:0.966]
Epoch [39/120    avg_loss:0.087, val_acc:0.957]
Epoch [40/120    avg_loss:0.053, val_acc:0.961]
Epoch [41/120    avg_loss:0.079, val_acc:0.957]
Epoch [42/120    avg_loss:0.049, val_acc:0.971]
Epoch [43/120    avg_loss:0.053, val_acc:0.957]
Epoch [44/120    avg_loss:0.060, val_acc:0.968]
Epoch [45/120    avg_loss:0.045, val_acc:0.959]
Epoch [46/120    avg_loss:0.049, val_acc:0.949]
Epoch [47/120    avg_loss:0.037, val_acc:0.977]
Epoch [48/120    avg_loss:0.041, val_acc:0.981]
Epoch [49/120    avg_loss:0.047, val_acc:0.955]
Epoch [50/120    avg_loss:0.063, val_acc:0.971]
Epoch [51/120    avg_loss:0.034, val_acc:0.980]
Epoch [52/120    avg_loss:0.054, val_acc:0.955]
Epoch [53/120    avg_loss:0.049, val_acc:0.967]
Epoch [54/120    avg_loss:0.033, val_acc:0.991]
Epoch [55/120    avg_loss:0.037, val_acc:0.986]
Epoch [56/120    avg_loss:0.037, val_acc:0.973]
Epoch [57/120    avg_loss:0.045, val_acc:0.981]
Epoch [58/120    avg_loss:0.020, val_acc:0.982]
Epoch [59/120    avg_loss:0.021, val_acc:0.985]
Epoch [60/120    avg_loss:0.026, val_acc:0.982]
Epoch [61/120    avg_loss:0.023, val_acc:0.978]
Epoch [62/120    avg_loss:0.025, val_acc:0.984]
Epoch [63/120    avg_loss:0.014, val_acc:0.986]
Epoch [64/120    avg_loss:0.021, val_acc:0.985]
Epoch [65/120    avg_loss:0.029, val_acc:0.947]
Epoch [66/120    avg_loss:0.086, val_acc:0.969]
Epoch [67/120    avg_loss:0.050, val_acc:0.951]
Epoch [68/120    avg_loss:0.032, val_acc:0.978]
Epoch [69/120    avg_loss:0.026, val_acc:0.984]
Epoch [70/120    avg_loss:0.019, val_acc:0.988]
Epoch [71/120    avg_loss:0.022, val_acc:0.988]
Epoch [72/120    avg_loss:0.016, val_acc:0.991]
Epoch [73/120    avg_loss:0.017, val_acc:0.990]
Epoch [74/120    avg_loss:0.015, val_acc:0.990]
Epoch [75/120    avg_loss:0.012, val_acc:0.990]
Epoch [76/120    avg_loss:0.019, val_acc:0.990]
Epoch [77/120    avg_loss:0.013, val_acc:0.988]
Epoch [78/120    avg_loss:0.010, val_acc:0.989]
Epoch [79/120    avg_loss:0.010, val_acc:0.992]
Epoch [80/120    avg_loss:0.012, val_acc:0.992]
Epoch [81/120    avg_loss:0.009, val_acc:0.991]
Epoch [82/120    avg_loss:0.010, val_acc:0.993]
Epoch [83/120    avg_loss:0.009, val_acc:0.993]
Epoch [84/120    avg_loss:0.010, val_acc:0.991]
Epoch [85/120    avg_loss:0.011, val_acc:0.991]
Epoch [86/120    avg_loss:0.009, val_acc:0.991]
Epoch [87/120    avg_loss:0.011, val_acc:0.992]
Epoch [88/120    avg_loss:0.014, val_acc:0.992]
Epoch [89/120    avg_loss:0.009, val_acc:0.993]
Epoch [90/120    avg_loss:0.015, val_acc:0.993]
Epoch [91/120    avg_loss:0.009, val_acc:0.994]
Epoch [92/120    avg_loss:0.007, val_acc:0.993]
Epoch [93/120    avg_loss:0.008, val_acc:0.993]
Epoch [94/120    avg_loss:0.010, val_acc:0.993]
Epoch [95/120    avg_loss:0.008, val_acc:0.992]
Epoch [96/120    avg_loss:0.010, val_acc:0.994]
Epoch [97/120    avg_loss:0.009, val_acc:0.994]
Epoch [98/120    avg_loss:0.008, val_acc:0.991]
Epoch [99/120    avg_loss:0.010, val_acc:0.989]
Epoch [100/120    avg_loss:0.009, val_acc:0.992]
Epoch [101/120    avg_loss:0.010, val_acc:0.991]
Epoch [102/120    avg_loss:0.009, val_acc:0.993]
Epoch [103/120    avg_loss:0.009, val_acc:0.990]
Epoch [104/120    avg_loss:0.008, val_acc:0.991]
Epoch [105/120    avg_loss:0.006, val_acc:0.991]
Epoch [106/120    avg_loss:0.008, val_acc:0.991]
Epoch [107/120    avg_loss:0.008, val_acc:0.991]
Epoch [108/120    avg_loss:0.009, val_acc:0.993]
Epoch [109/120    avg_loss:0.009, val_acc:0.992]
Epoch [110/120    avg_loss:0.007, val_acc:0.992]
Epoch [111/120    avg_loss:0.009, val_acc:0.992]
Epoch [112/120    avg_loss:0.007, val_acc:0.992]
Epoch [113/120    avg_loss:0.007, val_acc:0.992]
Epoch [114/120    avg_loss:0.008, val_acc:0.992]
Epoch [115/120    avg_loss:0.008, val_acc:0.992]
Epoch [116/120    avg_loss:0.008, val_acc:0.992]
Epoch [117/120    avg_loss:0.008, val_acc:0.992]
Epoch [118/120    avg_loss:0.007, val_acc:0.992]
Epoch [119/120    avg_loss:0.008, val_acc:0.992]
Epoch [120/120    avg_loss:0.007, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    3    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1266    0    5    0    1    0    0    0    2   10    1    0
     0    0    0]
 [   0    0    0  721    2    2    0    0    0    2    0    7   11    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    1    0    0    0    2  837   29    0    0
     0    5    0]
 [   0    0    3    0    0    3    1    0    0    0    3 2198    1    1
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    4    0  524    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1124   12    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    79  268    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.73441734417344

F1 scores:
[       nan 0.96202532 0.98983581 0.98228883 0.98383372 0.98524404
 0.99695586 0.98039216 0.997669   0.9        0.9726903  0.98675645
 0.97488372 0.9919571  0.95945369 0.84810127 0.9704142 ]

Kappa:
0.974142788563509
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd802ab77f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.289, val_acc:0.429]
Epoch [2/120    avg_loss:1.758, val_acc:0.571]
Epoch [3/120    avg_loss:1.444, val_acc:0.636]
Epoch [4/120    avg_loss:1.117, val_acc:0.681]
Epoch [5/120    avg_loss:1.046, val_acc:0.695]
Epoch [6/120    avg_loss:0.863, val_acc:0.714]
Epoch [7/120    avg_loss:0.725, val_acc:0.753]
Epoch [8/120    avg_loss:0.676, val_acc:0.821]
Epoch [9/120    avg_loss:0.541, val_acc:0.816]
Epoch [10/120    avg_loss:0.582, val_acc:0.786]
Epoch [11/120    avg_loss:0.567, val_acc:0.799]
Epoch [12/120    avg_loss:0.459, val_acc:0.843]
Epoch [13/120    avg_loss:0.470, val_acc:0.851]
Epoch [14/120    avg_loss:0.370, val_acc:0.882]
Epoch [15/120    avg_loss:0.315, val_acc:0.895]
Epoch [16/120    avg_loss:0.448, val_acc:0.790]
Epoch [17/120    avg_loss:0.369, val_acc:0.864]
Epoch [18/120    avg_loss:0.293, val_acc:0.864]
Epoch [19/120    avg_loss:0.308, val_acc:0.845]
Epoch [20/120    avg_loss:0.267, val_acc:0.792]
Epoch [21/120    avg_loss:0.265, val_acc:0.891]
Epoch [22/120    avg_loss:0.197, val_acc:0.920]
Epoch [23/120    avg_loss:0.262, val_acc:0.905]
Epoch [24/120    avg_loss:0.184, val_acc:0.911]
Epoch [25/120    avg_loss:0.167, val_acc:0.935]
Epoch [26/120    avg_loss:0.204, val_acc:0.903]
Epoch [27/120    avg_loss:0.183, val_acc:0.910]
Epoch [28/120    avg_loss:0.157, val_acc:0.911]
Epoch [29/120    avg_loss:0.141, val_acc:0.936]
Epoch [30/120    avg_loss:0.107, val_acc:0.939]
Epoch [31/120    avg_loss:0.104, val_acc:0.934]
Epoch [32/120    avg_loss:0.140, val_acc:0.851]
Epoch [33/120    avg_loss:0.166, val_acc:0.932]
Epoch [34/120    avg_loss:0.141, val_acc:0.918]
Epoch [35/120    avg_loss:0.126, val_acc:0.941]
Epoch [36/120    avg_loss:0.165, val_acc:0.911]
Epoch [37/120    avg_loss:0.115, val_acc:0.939]
Epoch [38/120    avg_loss:0.085, val_acc:0.943]
Epoch [39/120    avg_loss:0.098, val_acc:0.940]
Epoch [40/120    avg_loss:0.065, val_acc:0.920]
Epoch [41/120    avg_loss:0.084, val_acc:0.949]
Epoch [42/120    avg_loss:0.075, val_acc:0.951]
Epoch [43/120    avg_loss:0.052, val_acc:0.957]
Epoch [44/120    avg_loss:0.046, val_acc:0.938]
Epoch [45/120    avg_loss:0.054, val_acc:0.956]
Epoch [46/120    avg_loss:0.037, val_acc:0.949]
Epoch [47/120    avg_loss:0.214, val_acc:0.929]
Epoch [48/120    avg_loss:0.104, val_acc:0.947]
Epoch [49/120    avg_loss:0.064, val_acc:0.931]
Epoch [50/120    avg_loss:0.124, val_acc:0.930]
Epoch [51/120    avg_loss:0.093, val_acc:0.943]
Epoch [52/120    avg_loss:0.055, val_acc:0.957]
Epoch [53/120    avg_loss:0.065, val_acc:0.949]
Epoch [54/120    avg_loss:0.042, val_acc:0.962]
Epoch [55/120    avg_loss:0.037, val_acc:0.957]
Epoch [56/120    avg_loss:0.030, val_acc:0.953]
Epoch [57/120    avg_loss:0.022, val_acc:0.964]
Epoch [58/120    avg_loss:0.023, val_acc:0.960]
Epoch [59/120    avg_loss:0.023, val_acc:0.961]
Epoch [60/120    avg_loss:0.034, val_acc:0.964]
Epoch [61/120    avg_loss:0.030, val_acc:0.950]
Epoch [62/120    avg_loss:0.088, val_acc:0.951]
Epoch [63/120    avg_loss:0.038, val_acc:0.939]
Epoch [64/120    avg_loss:0.036, val_acc:0.960]
Epoch [65/120    avg_loss:0.048, val_acc:0.961]
Epoch [66/120    avg_loss:0.033, val_acc:0.959]
Epoch [67/120    avg_loss:0.032, val_acc:0.962]
Epoch [68/120    avg_loss:0.214, val_acc:0.919]
Epoch [69/120    avg_loss:0.297, val_acc:0.860]
Epoch [70/120    avg_loss:0.244, val_acc:0.928]
Epoch [71/120    avg_loss:0.126, val_acc:0.957]
Epoch [72/120    avg_loss:0.071, val_acc:0.964]
Epoch [73/120    avg_loss:0.048, val_acc:0.963]
Epoch [74/120    avg_loss:0.048, val_acc:0.958]
Epoch [75/120    avg_loss:0.049, val_acc:0.955]
Epoch [76/120    avg_loss:0.049, val_acc:0.958]
Epoch [77/120    avg_loss:0.042, val_acc:0.964]
Epoch [78/120    avg_loss:0.041, val_acc:0.957]
Epoch [79/120    avg_loss:0.034, val_acc:0.971]
Epoch [80/120    avg_loss:0.044, val_acc:0.974]
Epoch [81/120    avg_loss:0.031, val_acc:0.968]
Epoch [82/120    avg_loss:0.024, val_acc:0.962]
Epoch [83/120    avg_loss:0.036, val_acc:0.963]
Epoch [84/120    avg_loss:0.019, val_acc:0.970]
Epoch [85/120    avg_loss:0.024, val_acc:0.965]
Epoch [86/120    avg_loss:0.014, val_acc:0.972]
Epoch [87/120    avg_loss:0.029, val_acc:0.955]
Epoch [88/120    avg_loss:0.016, val_acc:0.964]
Epoch [89/120    avg_loss:0.015, val_acc:0.959]
Epoch [90/120    avg_loss:0.020, val_acc:0.970]
Epoch [91/120    avg_loss:0.024, val_acc:0.970]
Epoch [92/120    avg_loss:0.012, val_acc:0.975]
Epoch [93/120    avg_loss:0.021, val_acc:0.978]
Epoch [94/120    avg_loss:0.018, val_acc:0.975]
Epoch [95/120    avg_loss:0.010, val_acc:0.978]
Epoch [96/120    avg_loss:0.006, val_acc:0.977]
Epoch [97/120    avg_loss:0.016, val_acc:0.970]
Epoch [98/120    avg_loss:0.023, val_acc:0.971]
Epoch [99/120    avg_loss:0.044, val_acc:0.964]
Epoch [100/120    avg_loss:0.076, val_acc:0.943]
Epoch [101/120    avg_loss:0.044, val_acc:0.968]
Epoch [102/120    avg_loss:0.021, val_acc:0.968]
Epoch [103/120    avg_loss:0.009, val_acc:0.976]
Epoch [104/120    avg_loss:0.013, val_acc:0.975]
Epoch [105/120    avg_loss:0.013, val_acc:0.970]
Epoch [106/120    avg_loss:0.011, val_acc:0.974]
Epoch [107/120    avg_loss:0.012, val_acc:0.968]
Epoch [108/120    avg_loss:0.010, val_acc:0.975]
Epoch [109/120    avg_loss:0.034, val_acc:0.977]
Epoch [110/120    avg_loss:0.006, val_acc:0.976]
Epoch [111/120    avg_loss:0.009, val_acc:0.975]
Epoch [112/120    avg_loss:0.006, val_acc:0.978]
Epoch [113/120    avg_loss:0.005, val_acc:0.976]
Epoch [114/120    avg_loss:0.006, val_acc:0.977]
Epoch [115/120    avg_loss:0.004, val_acc:0.977]
Epoch [116/120    avg_loss:0.007, val_acc:0.977]
Epoch [117/120    avg_loss:0.005, val_acc:0.975]
Epoch [118/120    avg_loss:0.006, val_acc:0.975]
Epoch [119/120    avg_loss:0.006, val_acc:0.976]
Epoch [120/120    avg_loss:0.005, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1258    0    0    2    4    0    0    1   13    7    0    0
     0    0    0]
 [   0    0    0  722    0    1    0    0    0    9    1   10    2    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    3    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    0    2    0    0    1  847   15    0    0
     0    1    0]
 [   0    0    3    0    0    0    1    0    0    3    6 2188    8    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    6    1  524    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    5    0    0    0    1    0    0    0
  1117   15    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    47  297    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.8970189701897

F1 scores:
[       nan 0.96202532 0.98473581 0.98298162 1.         0.9908046
 0.98718915 0.94339623 1.         0.70588235 0.96689498 0.98714189
 0.98035547 0.9919571  0.97003908 0.9        0.97647059]

Kappa:
0.9760179294470037
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f111fdf0828>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.336, val_acc:0.417]
Epoch [2/120    avg_loss:1.699, val_acc:0.574]
Epoch [3/120    avg_loss:1.358, val_acc:0.658]
Epoch [4/120    avg_loss:1.090, val_acc:0.675]
Epoch [5/120    avg_loss:0.925, val_acc:0.706]
Epoch [6/120    avg_loss:0.826, val_acc:0.707]
Epoch [7/120    avg_loss:0.747, val_acc:0.763]
Epoch [8/120    avg_loss:0.729, val_acc:0.772]
Epoch [9/120    avg_loss:0.600, val_acc:0.817]
Epoch [10/120    avg_loss:0.548, val_acc:0.805]
Epoch [11/120    avg_loss:0.469, val_acc:0.819]
Epoch [12/120    avg_loss:0.475, val_acc:0.788]
Epoch [13/120    avg_loss:0.454, val_acc:0.836]
Epoch [14/120    avg_loss:0.429, val_acc:0.840]
Epoch [15/120    avg_loss:0.383, val_acc:0.862]
Epoch [16/120    avg_loss:0.313, val_acc:0.852]
Epoch [17/120    avg_loss:0.332, val_acc:0.890]
Epoch [18/120    avg_loss:0.301, val_acc:0.877]
Epoch [19/120    avg_loss:0.261, val_acc:0.871]
Epoch [20/120    avg_loss:0.239, val_acc:0.856]
Epoch [21/120    avg_loss:0.279, val_acc:0.889]
Epoch [22/120    avg_loss:0.262, val_acc:0.876]
Epoch [23/120    avg_loss:0.233, val_acc:0.900]
Epoch [24/120    avg_loss:0.193, val_acc:0.863]
Epoch [25/120    avg_loss:0.171, val_acc:0.901]
Epoch [26/120    avg_loss:0.179, val_acc:0.872]
Epoch [27/120    avg_loss:0.247, val_acc:0.909]
Epoch [28/120    avg_loss:0.170, val_acc:0.905]
Epoch [29/120    avg_loss:0.137, val_acc:0.933]
Epoch [30/120    avg_loss:0.136, val_acc:0.921]
Epoch [31/120    avg_loss:0.160, val_acc:0.918]
Epoch [32/120    avg_loss:0.141, val_acc:0.927]
Epoch [33/120    avg_loss:0.077, val_acc:0.954]
Epoch [34/120    avg_loss:0.169, val_acc:0.927]
Epoch [35/120    avg_loss:0.160, val_acc:0.939]
Epoch [36/120    avg_loss:0.090, val_acc:0.952]
Epoch [37/120    avg_loss:0.115, val_acc:0.949]
Epoch [38/120    avg_loss:0.114, val_acc:0.942]
Epoch [39/120    avg_loss:0.121, val_acc:0.945]
Epoch [40/120    avg_loss:0.070, val_acc:0.929]
Epoch [41/120    avg_loss:0.106, val_acc:0.899]
Epoch [42/120    avg_loss:0.112, val_acc:0.925]
Epoch [43/120    avg_loss:0.098, val_acc:0.936]
Epoch [44/120    avg_loss:0.078, val_acc:0.963]
Epoch [45/120    avg_loss:0.062, val_acc:0.954]
Epoch [46/120    avg_loss:0.064, val_acc:0.950]
Epoch [47/120    avg_loss:0.294, val_acc:0.832]
Epoch [48/120    avg_loss:0.231, val_acc:0.932]
Epoch [49/120    avg_loss:0.114, val_acc:0.958]
Epoch [50/120    avg_loss:0.070, val_acc:0.954]
Epoch [51/120    avg_loss:0.064, val_acc:0.958]
Epoch [52/120    avg_loss:0.056, val_acc:0.962]
Epoch [53/120    avg_loss:0.050, val_acc:0.887]
Epoch [54/120    avg_loss:0.057, val_acc:0.904]
Epoch [55/120    avg_loss:0.074, val_acc:0.949]
Epoch [56/120    avg_loss:0.048, val_acc:0.940]
Epoch [57/120    avg_loss:0.058, val_acc:0.960]
Epoch [58/120    avg_loss:0.041, val_acc:0.962]
Epoch [59/120    avg_loss:0.037, val_acc:0.964]
Epoch [60/120    avg_loss:0.025, val_acc:0.966]
Epoch [61/120    avg_loss:0.024, val_acc:0.968]
Epoch [62/120    avg_loss:0.026, val_acc:0.971]
Epoch [63/120    avg_loss:0.034, val_acc:0.969]
Epoch [64/120    avg_loss:0.029, val_acc:0.971]
Epoch [65/120    avg_loss:0.031, val_acc:0.971]
Epoch [66/120    avg_loss:0.024, val_acc:0.974]
Epoch [67/120    avg_loss:0.022, val_acc:0.970]
Epoch [68/120    avg_loss:0.029, val_acc:0.971]
Epoch [69/120    avg_loss:0.024, val_acc:0.972]
Epoch [70/120    avg_loss:0.018, val_acc:0.974]
Epoch [71/120    avg_loss:0.019, val_acc:0.974]
Epoch [72/120    avg_loss:0.021, val_acc:0.977]
Epoch [73/120    avg_loss:0.022, val_acc:0.976]
Epoch [74/120    avg_loss:0.022, val_acc:0.975]
Epoch [75/120    avg_loss:0.020, val_acc:0.976]
Epoch [76/120    avg_loss:0.022, val_acc:0.978]
Epoch [77/120    avg_loss:0.017, val_acc:0.978]
Epoch [78/120    avg_loss:0.022, val_acc:0.982]
Epoch [79/120    avg_loss:0.021, val_acc:0.980]
Epoch [80/120    avg_loss:0.022, val_acc:0.976]
Epoch [81/120    avg_loss:0.018, val_acc:0.982]
Epoch [82/120    avg_loss:0.014, val_acc:0.981]
Epoch [83/120    avg_loss:0.021, val_acc:0.979]
Epoch [84/120    avg_loss:0.018, val_acc:0.978]
Epoch [85/120    avg_loss:0.019, val_acc:0.981]
Epoch [86/120    avg_loss:0.016, val_acc:0.983]
Epoch [87/120    avg_loss:0.015, val_acc:0.983]
Epoch [88/120    avg_loss:0.013, val_acc:0.983]
Epoch [89/120    avg_loss:0.019, val_acc:0.983]
Epoch [90/120    avg_loss:0.019, val_acc:0.981]
Epoch [91/120    avg_loss:0.013, val_acc:0.983]
Epoch [92/120    avg_loss:0.019, val_acc:0.976]
Epoch [93/120    avg_loss:0.016, val_acc:0.981]
Epoch [94/120    avg_loss:0.016, val_acc:0.980]
Epoch [95/120    avg_loss:0.018, val_acc:0.982]
Epoch [96/120    avg_loss:0.013, val_acc:0.985]
Epoch [97/120    avg_loss:0.013, val_acc:0.982]
Epoch [98/120    avg_loss:0.019, val_acc:0.982]
Epoch [99/120    avg_loss:0.015, val_acc:0.978]
Epoch [100/120    avg_loss:0.019, val_acc:0.980]
Epoch [101/120    avg_loss:0.016, val_acc:0.982]
Epoch [102/120    avg_loss:0.013, val_acc:0.981]
Epoch [103/120    avg_loss:0.015, val_acc:0.980]
Epoch [104/120    avg_loss:0.013, val_acc:0.981]
Epoch [105/120    avg_loss:0.019, val_acc:0.981]
Epoch [106/120    avg_loss:0.012, val_acc:0.979]
Epoch [107/120    avg_loss:0.019, val_acc:0.980]
Epoch [108/120    avg_loss:0.014, val_acc:0.981]
Epoch [109/120    avg_loss:0.012, val_acc:0.981]
Epoch [110/120    avg_loss:0.014, val_acc:0.982]
Epoch [111/120    avg_loss:0.014, val_acc:0.982]
Epoch [112/120    avg_loss:0.021, val_acc:0.981]
Epoch [113/120    avg_loss:0.014, val_acc:0.982]
Epoch [114/120    avg_loss:0.013, val_acc:0.982]
Epoch [115/120    avg_loss:0.017, val_acc:0.982]
Epoch [116/120    avg_loss:0.023, val_acc:0.983]
Epoch [117/120    avg_loss:0.013, val_acc:0.983]
Epoch [118/120    avg_loss:0.014, val_acc:0.983]
Epoch [119/120    avg_loss:0.019, val_acc:0.982]
Epoch [120/120    avg_loss:0.012, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1250    1    0    0    1    0    0    2    7   17    4    0
     0    3    0]
 [   0    0    2  715    0    2    0    0    0    6    0    1   13    5
     0    3    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    3    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    1    0    0    0    0  849   18    0    0
     0    3    0]
 [   0    0    7    0    0    1    0    0    0    0    7 2182   12    0
     0    1    0]
 [   0    0    0    0    0    1    0    0    0    0    4    0  526    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    1    0    0
  1128    9    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
    25  321    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.07046070460704

F1 scores:
[       nan 0.93506494 0.9811617  0.97744361 1.         0.99315068
 0.99465241 1.         1.         0.81818182 0.97139588 0.98465704
 0.96513761 0.98666667 0.98300654 0.9317852  0.98809524]

Kappa:
0.9780006430261791
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f90af5bd7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.299, val_acc:0.555]
Epoch [2/120    avg_loss:1.722, val_acc:0.605]
Epoch [3/120    avg_loss:1.392, val_acc:0.619]
Epoch [4/120    avg_loss:1.188, val_acc:0.680]
Epoch [5/120    avg_loss:1.024, val_acc:0.683]
Epoch [6/120    avg_loss:0.943, val_acc:0.697]
Epoch [7/120    avg_loss:0.816, val_acc:0.734]
Epoch [8/120    avg_loss:0.745, val_acc:0.771]
Epoch [9/120    avg_loss:0.657, val_acc:0.787]
Epoch [10/120    avg_loss:0.599, val_acc:0.809]
Epoch [11/120    avg_loss:0.579, val_acc:0.829]
Epoch [12/120    avg_loss:0.558, val_acc:0.812]
Epoch [13/120    avg_loss:0.479, val_acc:0.825]
Epoch [14/120    avg_loss:0.467, val_acc:0.821]
Epoch [15/120    avg_loss:0.439, val_acc:0.825]
Epoch [16/120    avg_loss:0.428, val_acc:0.852]
Epoch [17/120    avg_loss:0.390, val_acc:0.818]
Epoch [18/120    avg_loss:0.408, val_acc:0.818]
Epoch [19/120    avg_loss:0.324, val_acc:0.871]
Epoch [20/120    avg_loss:0.237, val_acc:0.905]
Epoch [21/120    avg_loss:0.200, val_acc:0.889]
Epoch [22/120    avg_loss:0.267, val_acc:0.877]
Epoch [23/120    avg_loss:0.247, val_acc:0.900]
Epoch [24/120    avg_loss:0.216, val_acc:0.884]
Epoch [25/120    avg_loss:0.204, val_acc:0.879]
Epoch [26/120    avg_loss:0.193, val_acc:0.915]
Epoch [27/120    avg_loss:0.154, val_acc:0.933]
Epoch [28/120    avg_loss:0.154, val_acc:0.896]
Epoch [29/120    avg_loss:0.149, val_acc:0.917]
Epoch [30/120    avg_loss:0.144, val_acc:0.936]
Epoch [31/120    avg_loss:0.159, val_acc:0.934]
Epoch [32/120    avg_loss:0.216, val_acc:0.913]
Epoch [33/120    avg_loss:0.270, val_acc:0.920]
Epoch [34/120    avg_loss:0.124, val_acc:0.936]
Epoch [35/120    avg_loss:0.140, val_acc:0.892]
Epoch [36/120    avg_loss:0.101, val_acc:0.946]
Epoch [37/120    avg_loss:0.086, val_acc:0.949]
Epoch [38/120    avg_loss:0.100, val_acc:0.954]
Epoch [39/120    avg_loss:0.077, val_acc:0.953]
Epoch [40/120    avg_loss:0.117, val_acc:0.942]
Epoch [41/120    avg_loss:0.111, val_acc:0.943]
Epoch [42/120    avg_loss:0.135, val_acc:0.949]
Epoch [43/120    avg_loss:0.079, val_acc:0.934]
Epoch [44/120    avg_loss:0.106, val_acc:0.931]
Epoch [45/120    avg_loss:0.102, val_acc:0.952]
Epoch [46/120    avg_loss:0.107, val_acc:0.957]
Epoch [47/120    avg_loss:0.065, val_acc:0.945]
Epoch [48/120    avg_loss:0.058, val_acc:0.963]
Epoch [49/120    avg_loss:0.062, val_acc:0.951]
Epoch [50/120    avg_loss:0.052, val_acc:0.958]
Epoch [51/120    avg_loss:0.038, val_acc:0.953]
Epoch [52/120    avg_loss:0.046, val_acc:0.961]
Epoch [53/120    avg_loss:0.040, val_acc:0.955]
Epoch [54/120    avg_loss:0.031, val_acc:0.951]
Epoch [55/120    avg_loss:0.043, val_acc:0.957]
Epoch [56/120    avg_loss:0.087, val_acc:0.954]
Epoch [57/120    avg_loss:0.060, val_acc:0.961]
Epoch [58/120    avg_loss:0.052, val_acc:0.955]
Epoch [59/120    avg_loss:0.059, val_acc:0.933]
Epoch [60/120    avg_loss:0.037, val_acc:0.962]
Epoch [61/120    avg_loss:0.025, val_acc:0.965]
Epoch [62/120    avg_loss:0.027, val_acc:0.974]
Epoch [63/120    avg_loss:0.032, val_acc:0.962]
Epoch [64/120    avg_loss:0.030, val_acc:0.965]
Epoch [65/120    avg_loss:0.057, val_acc:0.959]
Epoch [66/120    avg_loss:0.041, val_acc:0.966]
Epoch [67/120    avg_loss:0.062, val_acc:0.952]
Epoch [68/120    avg_loss:0.055, val_acc:0.961]
Epoch [69/120    avg_loss:0.026, val_acc:0.972]
Epoch [70/120    avg_loss:0.025, val_acc:0.979]
Epoch [71/120    avg_loss:0.018, val_acc:0.966]
Epoch [72/120    avg_loss:0.019, val_acc:0.975]
Epoch [73/120    avg_loss:0.022, val_acc:0.975]
Epoch [74/120    avg_loss:0.015, val_acc:0.978]
Epoch [75/120    avg_loss:0.016, val_acc:0.978]
Epoch [76/120    avg_loss:0.016, val_acc:0.982]
Epoch [77/120    avg_loss:0.023, val_acc:0.943]
Epoch [78/120    avg_loss:0.028, val_acc:0.966]
Epoch [79/120    avg_loss:0.140, val_acc:0.915]
Epoch [80/120    avg_loss:0.093, val_acc:0.962]
Epoch [81/120    avg_loss:0.079, val_acc:0.953]
Epoch [82/120    avg_loss:0.070, val_acc:0.959]
Epoch [83/120    avg_loss:0.043, val_acc:0.964]
Epoch [84/120    avg_loss:0.040, val_acc:0.953]
Epoch [85/120    avg_loss:0.030, val_acc:0.973]
Epoch [86/120    avg_loss:0.053, val_acc:0.934]
Epoch [87/120    avg_loss:0.055, val_acc:0.965]
Epoch [88/120    avg_loss:0.024, val_acc:0.970]
Epoch [89/120    avg_loss:0.032, val_acc:0.963]
Epoch [90/120    avg_loss:0.030, val_acc:0.975]
Epoch [91/120    avg_loss:0.020, val_acc:0.974]
Epoch [92/120    avg_loss:0.014, val_acc:0.976]
Epoch [93/120    avg_loss:0.013, val_acc:0.975]
Epoch [94/120    avg_loss:0.013, val_acc:0.979]
Epoch [95/120    avg_loss:0.012, val_acc:0.977]
Epoch [96/120    avg_loss:0.013, val_acc:0.975]
Epoch [97/120    avg_loss:0.010, val_acc:0.980]
Epoch [98/120    avg_loss:0.011, val_acc:0.980]
Epoch [99/120    avg_loss:0.009, val_acc:0.979]
Epoch [100/120    avg_loss:0.015, val_acc:0.976]
Epoch [101/120    avg_loss:0.013, val_acc:0.975]
Epoch [102/120    avg_loss:0.011, val_acc:0.976]
Epoch [103/120    avg_loss:0.011, val_acc:0.976]
Epoch [104/120    avg_loss:0.012, val_acc:0.976]
Epoch [105/120    avg_loss:0.012, val_acc:0.976]
Epoch [106/120    avg_loss:0.008, val_acc:0.976]
Epoch [107/120    avg_loss:0.008, val_acc:0.976]
Epoch [108/120    avg_loss:0.008, val_acc:0.976]
Epoch [109/120    avg_loss:0.008, val_acc:0.977]
Epoch [110/120    avg_loss:0.014, val_acc:0.976]
Epoch [111/120    avg_loss:0.013, val_acc:0.975]
Epoch [112/120    avg_loss:0.011, val_acc:0.975]
Epoch [113/120    avg_loss:0.017, val_acc:0.976]
Epoch [114/120    avg_loss:0.010, val_acc:0.977]
Epoch [115/120    avg_loss:0.011, val_acc:0.977]
Epoch [116/120    avg_loss:0.009, val_acc:0.977]
Epoch [117/120    avg_loss:0.010, val_acc:0.977]
Epoch [118/120    avg_loss:0.011, val_acc:0.977]
Epoch [119/120    avg_loss:0.011, val_acc:0.977]
Epoch [120/120    avg_loss:0.008, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1251    1    0    0    1    0    0    0    8   19    5    0
     0    0    0]
 [   0    0    0  724    0    0    0    0    0    3    0    0   14    5
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0  851   14    0    0
     1    6    0]
 [   0    0    4    2    0    1    0    0    0    0    1 2182   18    0
     1    1    0]
 [   0    0    0    1    0    0    0    0    0    0    2    1  526    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    3    0    0    1    0    0    0    0
    76  267    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.59349593495935

F1 scores:
[       nan 0.975      0.98503937 0.98169492 1.         0.99196326
 0.99467681 1.         0.997669   0.9        0.9787234  0.98554652
 0.955495   0.98666667 0.9595228  0.83962264 0.9704142 ]

Kappa:
0.9725516697945304
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f16edd1b828>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.340, val_acc:0.528]
Epoch [2/120    avg_loss:1.742, val_acc:0.576]
Epoch [3/120    avg_loss:1.474, val_acc:0.660]
Epoch [4/120    avg_loss:1.254, val_acc:0.676]
Epoch [5/120    avg_loss:1.006, val_acc:0.712]
Epoch [6/120    avg_loss:0.946, val_acc:0.776]
Epoch [7/120    avg_loss:0.774, val_acc:0.734]
Epoch [8/120    avg_loss:0.736, val_acc:0.780]
Epoch [9/120    avg_loss:0.624, val_acc:0.800]
Epoch [10/120    avg_loss:0.628, val_acc:0.832]
Epoch [11/120    avg_loss:0.511, val_acc:0.786]
Epoch [12/120    avg_loss:0.627, val_acc:0.834]
Epoch [13/120    avg_loss:0.385, val_acc:0.851]
Epoch [14/120    avg_loss:0.412, val_acc:0.868]
Epoch [15/120    avg_loss:0.347, val_acc:0.895]
Epoch [16/120    avg_loss:0.284, val_acc:0.892]
Epoch [17/120    avg_loss:0.304, val_acc:0.864]
Epoch [18/120    avg_loss:0.269, val_acc:0.903]
Epoch [19/120    avg_loss:0.210, val_acc:0.896]
Epoch [20/120    avg_loss:0.212, val_acc:0.880]
Epoch [21/120    avg_loss:0.377, val_acc:0.749]
Epoch [22/120    avg_loss:0.308, val_acc:0.864]
Epoch [23/120    avg_loss:0.292, val_acc:0.922]
Epoch [24/120    avg_loss:0.197, val_acc:0.934]
Epoch [25/120    avg_loss:0.172, val_acc:0.922]
Epoch [26/120    avg_loss:0.192, val_acc:0.923]
Epoch [27/120    avg_loss:0.179, val_acc:0.926]
Epoch [28/120    avg_loss:0.129, val_acc:0.940]
Epoch [29/120    avg_loss:0.134, val_acc:0.914]
Epoch [30/120    avg_loss:0.115, val_acc:0.942]
Epoch [31/120    avg_loss:0.101, val_acc:0.917]
Epoch [32/120    avg_loss:0.100, val_acc:0.951]
Epoch [33/120    avg_loss:0.119, val_acc:0.943]
Epoch [34/120    avg_loss:0.138, val_acc:0.920]
Epoch [35/120    avg_loss:0.143, val_acc:0.892]
Epoch [36/120    avg_loss:0.139, val_acc:0.954]
Epoch [37/120    avg_loss:0.115, val_acc:0.945]
Epoch [38/120    avg_loss:0.086, val_acc:0.956]
Epoch [39/120    avg_loss:0.085, val_acc:0.938]
Epoch [40/120    avg_loss:0.116, val_acc:0.939]
Epoch [41/120    avg_loss:0.073, val_acc:0.950]
Epoch [42/120    avg_loss:0.042, val_acc:0.967]
Epoch [43/120    avg_loss:0.042, val_acc:0.964]
Epoch [44/120    avg_loss:0.057, val_acc:0.959]
Epoch [45/120    avg_loss:0.068, val_acc:0.932]
Epoch [46/120    avg_loss:0.067, val_acc:0.935]
Epoch [47/120    avg_loss:0.080, val_acc:0.957]
Epoch [48/120    avg_loss:0.048, val_acc:0.952]
Epoch [49/120    avg_loss:0.038, val_acc:0.965]
Epoch [50/120    avg_loss:0.031, val_acc:0.959]
Epoch [51/120    avg_loss:0.140, val_acc:0.958]
Epoch [52/120    avg_loss:0.065, val_acc:0.968]
Epoch [53/120    avg_loss:0.054, val_acc:0.968]
Epoch [54/120    avg_loss:0.085, val_acc:0.939]
Epoch [55/120    avg_loss:0.064, val_acc:0.912]
Epoch [56/120    avg_loss:0.108, val_acc:0.956]
Epoch [57/120    avg_loss:0.047, val_acc:0.963]
Epoch [58/120    avg_loss:0.036, val_acc:0.975]
Epoch [59/120    avg_loss:0.019, val_acc:0.973]
Epoch [60/120    avg_loss:0.040, val_acc:0.967]
Epoch [61/120    avg_loss:0.116, val_acc:0.950]
Epoch [62/120    avg_loss:0.102, val_acc:0.949]
Epoch [63/120    avg_loss:0.097, val_acc:0.944]
Epoch [64/120    avg_loss:0.052, val_acc:0.966]
Epoch [65/120    avg_loss:0.040, val_acc:0.973]
Epoch [66/120    avg_loss:0.036, val_acc:0.967]
Epoch [67/120    avg_loss:0.032, val_acc:0.974]
Epoch [68/120    avg_loss:0.023, val_acc:0.973]
Epoch [69/120    avg_loss:0.024, val_acc:0.974]
Epoch [70/120    avg_loss:0.036, val_acc:0.975]
Epoch [71/120    avg_loss:0.022, val_acc:0.965]
Epoch [72/120    avg_loss:0.026, val_acc:0.977]
Epoch [73/120    avg_loss:0.015, val_acc:0.978]
Epoch [74/120    avg_loss:0.025, val_acc:0.957]
Epoch [75/120    avg_loss:0.049, val_acc:0.958]
Epoch [76/120    avg_loss:0.036, val_acc:0.967]
Epoch [77/120    avg_loss:0.026, val_acc:0.972]
Epoch [78/120    avg_loss:0.012, val_acc:0.980]
Epoch [79/120    avg_loss:0.015, val_acc:0.977]
Epoch [80/120    avg_loss:0.018, val_acc:0.973]
Epoch [81/120    avg_loss:0.013, val_acc:0.980]
Epoch [82/120    avg_loss:0.009, val_acc:0.978]
Epoch [83/120    avg_loss:0.012, val_acc:0.979]
Epoch [84/120    avg_loss:0.011, val_acc:0.985]
Epoch [85/120    avg_loss:0.020, val_acc:0.953]
Epoch [86/120    avg_loss:0.023, val_acc:0.978]
Epoch [87/120    avg_loss:0.043, val_acc:0.972]
Epoch [88/120    avg_loss:0.014, val_acc:0.977]
Epoch [89/120    avg_loss:0.014, val_acc:0.981]
Epoch [90/120    avg_loss:0.013, val_acc:0.977]
Epoch [91/120    avg_loss:0.021, val_acc:0.963]
Epoch [92/120    avg_loss:0.024, val_acc:0.973]
Epoch [93/120    avg_loss:0.018, val_acc:0.975]
Epoch [94/120    avg_loss:0.032, val_acc:0.978]
Epoch [95/120    avg_loss:0.012, val_acc:0.981]
Epoch [96/120    avg_loss:0.015, val_acc:0.963]
Epoch [97/120    avg_loss:0.031, val_acc:0.977]
Epoch [98/120    avg_loss:0.021, val_acc:0.983]
Epoch [99/120    avg_loss:0.010, val_acc:0.984]
Epoch [100/120    avg_loss:0.008, val_acc:0.983]
Epoch [101/120    avg_loss:0.010, val_acc:0.984]
Epoch [102/120    avg_loss:0.015, val_acc:0.984]
Epoch [103/120    avg_loss:0.009, val_acc:0.985]
Epoch [104/120    avg_loss:0.009, val_acc:0.985]
Epoch [105/120    avg_loss:0.007, val_acc:0.983]
Epoch [106/120    avg_loss:0.005, val_acc:0.983]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.006, val_acc:0.983]
Epoch [109/120    avg_loss:0.007, val_acc:0.982]
Epoch [110/120    avg_loss:0.008, val_acc:0.983]
Epoch [111/120    avg_loss:0.005, val_acc:0.983]
Epoch [112/120    avg_loss:0.005, val_acc:0.984]
Epoch [113/120    avg_loss:0.005, val_acc:0.984]
Epoch [114/120    avg_loss:0.006, val_acc:0.984]
Epoch [115/120    avg_loss:0.004, val_acc:0.984]
Epoch [116/120    avg_loss:0.006, val_acc:0.984]
Epoch [117/120    avg_loss:0.006, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.984]
Epoch [120/120    avg_loss:0.005, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1272    6    0    0    0    0    0    0    3    4    0    0
     0    0    0]
 [   0    0    0  735    1    0    0    0    0    2    0    5    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0  863    6    0    0
     1    3    0]
 [   0    0    8    0    0    2    2    0    0    1    4 2186    6    0
     0    1    0]
 [   0    0    0    0    0    0    0    0    0    0    6    0  525    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1126   11    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    38  309    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.61246612466125

F1 scores:
[       nan 0.975      0.99104012 0.98790323 0.99765808 0.9954023
 0.99695586 1.         0.99883856 0.92307692 0.98403649 0.99093382
 0.98130841 1.         0.97615951 0.92101341 0.97647059]

Kappa:
0.9841799248444607
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe0c1ac8ac8>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.321, val_acc:0.533]
Epoch [2/120    avg_loss:1.771, val_acc:0.591]
Epoch [3/120    avg_loss:1.480, val_acc:0.672]
Epoch [4/120    avg_loss:1.138, val_acc:0.678]
Epoch [5/120    avg_loss:1.062, val_acc:0.724]
Epoch [6/120    avg_loss:0.876, val_acc:0.754]
Epoch [7/120    avg_loss:0.827, val_acc:0.748]
Epoch [8/120    avg_loss:0.697, val_acc:0.753]
Epoch [9/120    avg_loss:0.662, val_acc:0.796]
Epoch [10/120    avg_loss:0.592, val_acc:0.797]
Epoch [11/120    avg_loss:0.478, val_acc:0.838]
Epoch [12/120    avg_loss:0.481, val_acc:0.840]
Epoch [13/120    avg_loss:0.433, val_acc:0.841]
Epoch [14/120    avg_loss:0.363, val_acc:0.827]
Epoch [15/120    avg_loss:0.366, val_acc:0.881]
Epoch [16/120    avg_loss:0.319, val_acc:0.881]
Epoch [17/120    avg_loss:0.248, val_acc:0.886]
Epoch [18/120    avg_loss:0.281, val_acc:0.880]
Epoch [19/120    avg_loss:0.292, val_acc:0.908]
Epoch [20/120    avg_loss:0.251, val_acc:0.865]
Epoch [21/120    avg_loss:0.253, val_acc:0.853]
Epoch [22/120    avg_loss:0.341, val_acc:0.867]
Epoch [23/120    avg_loss:0.238, val_acc:0.904]
Epoch [24/120    avg_loss:0.263, val_acc:0.898]
Epoch [25/120    avg_loss:0.174, val_acc:0.901]
Epoch [26/120    avg_loss:0.138, val_acc:0.922]
Epoch [27/120    avg_loss:0.202, val_acc:0.915]
Epoch [28/120    avg_loss:0.168, val_acc:0.924]
Epoch [29/120    avg_loss:0.138, val_acc:0.939]
Epoch [30/120    avg_loss:0.105, val_acc:0.947]
Epoch [31/120    avg_loss:0.088, val_acc:0.952]
Epoch [32/120    avg_loss:0.077, val_acc:0.944]
Epoch [33/120    avg_loss:0.269, val_acc:0.931]
Epoch [34/120    avg_loss:0.141, val_acc:0.932]
Epoch [35/120    avg_loss:0.126, val_acc:0.943]
Epoch [36/120    avg_loss:0.086, val_acc:0.943]
Epoch [37/120    avg_loss:0.089, val_acc:0.949]
Epoch [38/120    avg_loss:0.085, val_acc:0.957]
Epoch [39/120    avg_loss:0.089, val_acc:0.944]
Epoch [40/120    avg_loss:0.091, val_acc:0.955]
Epoch [41/120    avg_loss:0.087, val_acc:0.960]
Epoch [42/120    avg_loss:0.045, val_acc:0.954]
Epoch [43/120    avg_loss:0.047, val_acc:0.956]
Epoch [44/120    avg_loss:0.065, val_acc:0.955]
Epoch [45/120    avg_loss:0.052, val_acc:0.966]
Epoch [46/120    avg_loss:0.099, val_acc:0.927]
Epoch [47/120    avg_loss:0.069, val_acc:0.957]
Epoch [48/120    avg_loss:0.074, val_acc:0.958]
Epoch [49/120    avg_loss:0.054, val_acc:0.963]
Epoch [50/120    avg_loss:0.060, val_acc:0.966]
Epoch [51/120    avg_loss:0.121, val_acc:0.933]
Epoch [52/120    avg_loss:0.189, val_acc:0.934]
Epoch [53/120    avg_loss:0.085, val_acc:0.931]
Epoch [54/120    avg_loss:0.091, val_acc:0.932]
Epoch [55/120    avg_loss:0.169, val_acc:0.966]
Epoch [56/120    avg_loss:0.049, val_acc:0.952]
Epoch [57/120    avg_loss:0.043, val_acc:0.967]
Epoch [58/120    avg_loss:0.033, val_acc:0.960]
Epoch [59/120    avg_loss:0.046, val_acc:0.966]
Epoch [60/120    avg_loss:0.172, val_acc:0.750]
Epoch [61/120    avg_loss:0.216, val_acc:0.945]
Epoch [62/120    avg_loss:0.088, val_acc:0.946]
Epoch [63/120    avg_loss:0.072, val_acc:0.954]
Epoch [64/120    avg_loss:0.062, val_acc:0.960]
Epoch [65/120    avg_loss:0.042, val_acc:0.973]
Epoch [66/120    avg_loss:0.033, val_acc:0.963]
Epoch [67/120    avg_loss:0.041, val_acc:0.932]
Epoch [68/120    avg_loss:0.041, val_acc:0.971]
Epoch [69/120    avg_loss:0.023, val_acc:0.976]
Epoch [70/120    avg_loss:0.018, val_acc:0.975]
Epoch [71/120    avg_loss:0.020, val_acc:0.973]
Epoch [72/120    avg_loss:0.012, val_acc:0.971]
Epoch [73/120    avg_loss:0.018, val_acc:0.981]
Epoch [74/120    avg_loss:0.019, val_acc:0.983]
Epoch [75/120    avg_loss:0.016, val_acc:0.980]
Epoch [76/120    avg_loss:0.014, val_acc:0.973]
Epoch [77/120    avg_loss:0.022, val_acc:0.970]
Epoch [78/120    avg_loss:0.018, val_acc:0.982]
Epoch [79/120    avg_loss:0.022, val_acc:0.973]
Epoch [80/120    avg_loss:0.016, val_acc:0.970]
Epoch [81/120    avg_loss:0.043, val_acc:0.973]
Epoch [82/120    avg_loss:0.024, val_acc:0.977]
Epoch [83/120    avg_loss:0.014, val_acc:0.978]
Epoch [84/120    avg_loss:0.030, val_acc:0.971]
Epoch [85/120    avg_loss:0.018, val_acc:0.978]
Epoch [86/120    avg_loss:0.015, val_acc:0.955]
Epoch [87/120    avg_loss:0.015, val_acc:0.984]
Epoch [88/120    avg_loss:0.019, val_acc:0.969]
Epoch [89/120    avg_loss:0.026, val_acc:0.976]
Epoch [90/120    avg_loss:0.018, val_acc:0.970]
Epoch [91/120    avg_loss:0.012, val_acc:0.984]
Epoch [92/120    avg_loss:0.009, val_acc:0.985]
Epoch [93/120    avg_loss:0.021, val_acc:0.946]
Epoch [94/120    avg_loss:0.039, val_acc:0.976]
Epoch [95/120    avg_loss:0.014, val_acc:0.979]
Epoch [96/120    avg_loss:0.009, val_acc:0.975]
Epoch [97/120    avg_loss:0.016, val_acc:0.974]
Epoch [98/120    avg_loss:0.016, val_acc:0.974]
Epoch [99/120    avg_loss:0.006, val_acc:0.982]
Epoch [100/120    avg_loss:0.008, val_acc:0.984]
Epoch [101/120    avg_loss:0.012, val_acc:0.972]
Epoch [102/120    avg_loss:0.009, val_acc:0.989]
Epoch [103/120    avg_loss:0.006, val_acc:0.988]
Epoch [104/120    avg_loss:0.007, val_acc:0.985]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.004, val_acc:0.990]
Epoch [107/120    avg_loss:0.006, val_acc:0.982]
Epoch [108/120    avg_loss:0.029, val_acc:0.983]
Epoch [109/120    avg_loss:0.010, val_acc:0.983]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.007, val_acc:0.981]
Epoch [112/120    avg_loss:0.011, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.980]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.009, val_acc:0.983]
Epoch [119/120    avg_loss:0.008, val_acc:0.976]
Epoch [120/120    avg_loss:0.013, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    3    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    3 1267    0    0    0    1    0    0    4    4    2    0    0
     0    4    0]
 [   0    0    0  708    3    5    0    0    0   11    1    0   11    4
     1    3    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   12    0    0    0    0    0    0    6  849    5    0    0
     0    3    0]
 [   0    0   20    0    0    2    1    0    0    2    7 2163   14    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  530    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    3    1    0    0    0
  1122   13    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    66  275    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.46341463414635

F1 scores:
[       nan 0.88607595 0.97951295 0.97319588 0.99300699 0.98855835
 0.99318698 0.98039216 1.         0.57142857 0.97586207 0.98767123
 0.97247706 0.98666667 0.96309013 0.85139319 0.97647059]

Kappa:
0.9710967547398999
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff4620db7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.339, val_acc:0.517]
Epoch [2/120    avg_loss:1.809, val_acc:0.599]
Epoch [3/120    avg_loss:1.499, val_acc:0.632]
Epoch [4/120    avg_loss:1.237, val_acc:0.688]
Epoch [5/120    avg_loss:1.024, val_acc:0.738]
Epoch [6/120    avg_loss:0.901, val_acc:0.694]
Epoch [7/120    avg_loss:0.821, val_acc:0.739]
Epoch [8/120    avg_loss:0.708, val_acc:0.780]
Epoch [9/120    avg_loss:0.669, val_acc:0.800]
Epoch [10/120    avg_loss:0.585, val_acc:0.774]
Epoch [11/120    avg_loss:0.548, val_acc:0.773]
Epoch [12/120    avg_loss:0.457, val_acc:0.671]
Epoch [13/120    avg_loss:0.457, val_acc:0.806]
Epoch [14/120    avg_loss:0.490, val_acc:0.863]
Epoch [15/120    avg_loss:0.331, val_acc:0.834]
Epoch [16/120    avg_loss:0.304, val_acc:0.882]
Epoch [17/120    avg_loss:0.321, val_acc:0.879]
Epoch [18/120    avg_loss:0.326, val_acc:0.872]
Epoch [19/120    avg_loss:0.361, val_acc:0.836]
Epoch [20/120    avg_loss:0.290, val_acc:0.884]
Epoch [21/120    avg_loss:0.247, val_acc:0.908]
Epoch [22/120    avg_loss:0.214, val_acc:0.863]
Epoch [23/120    avg_loss:0.240, val_acc:0.900]
Epoch [24/120    avg_loss:0.221, val_acc:0.892]
Epoch [25/120    avg_loss:0.169, val_acc:0.932]
Epoch [26/120    avg_loss:0.150, val_acc:0.942]
Epoch [27/120    avg_loss:0.117, val_acc:0.950]
Epoch [28/120    avg_loss:0.111, val_acc:0.943]
Epoch [29/120    avg_loss:0.143, val_acc:0.946]
Epoch [30/120    avg_loss:0.114, val_acc:0.936]
Epoch [31/120    avg_loss:0.122, val_acc:0.944]
Epoch [32/120    avg_loss:0.113, val_acc:0.943]
Epoch [33/120    avg_loss:0.082, val_acc:0.953]
Epoch [34/120    avg_loss:0.110, val_acc:0.932]
Epoch [35/120    avg_loss:0.093, val_acc:0.944]
Epoch [36/120    avg_loss:0.127, val_acc:0.953]
Epoch [37/120    avg_loss:0.142, val_acc:0.939]
Epoch [38/120    avg_loss:0.099, val_acc:0.952]
Epoch [39/120    avg_loss:0.078, val_acc:0.960]
Epoch [40/120    avg_loss:0.106, val_acc:0.907]
Epoch [41/120    avg_loss:0.137, val_acc:0.949]
Epoch [42/120    avg_loss:0.057, val_acc:0.954]
Epoch [43/120    avg_loss:0.061, val_acc:0.957]
Epoch [44/120    avg_loss:0.067, val_acc:0.954]
Epoch [45/120    avg_loss:0.057, val_acc:0.963]
Epoch [46/120    avg_loss:0.040, val_acc:0.970]
Epoch [47/120    avg_loss:0.041, val_acc:0.961]
Epoch [48/120    avg_loss:0.074, val_acc:0.944]
Epoch [49/120    avg_loss:0.202, val_acc:0.946]
Epoch [50/120    avg_loss:0.076, val_acc:0.959]
Epoch [51/120    avg_loss:0.074, val_acc:0.956]
Epoch [52/120    avg_loss:0.058, val_acc:0.965]
Epoch [53/120    avg_loss:0.060, val_acc:0.961]
Epoch [54/120    avg_loss:0.126, val_acc:0.966]
Epoch [55/120    avg_loss:0.084, val_acc:0.963]
Epoch [56/120    avg_loss:0.042, val_acc:0.950]
Epoch [57/120    avg_loss:0.031, val_acc:0.974]
Epoch [58/120    avg_loss:0.029, val_acc:0.972]
Epoch [59/120    avg_loss:0.022, val_acc:0.976]
Epoch [60/120    avg_loss:0.030, val_acc:0.954]
Epoch [61/120    avg_loss:0.038, val_acc:0.972]
Epoch [62/120    avg_loss:0.028, val_acc:0.976]
Epoch [63/120    avg_loss:0.016, val_acc:0.976]
Epoch [64/120    avg_loss:0.028, val_acc:0.972]
Epoch [65/120    avg_loss:0.021, val_acc:0.976]
Epoch [66/120    avg_loss:0.019, val_acc:0.975]
Epoch [67/120    avg_loss:0.018, val_acc:0.980]
Epoch [68/120    avg_loss:0.021, val_acc:0.979]
Epoch [69/120    avg_loss:0.011, val_acc:0.977]
Epoch [70/120    avg_loss:0.023, val_acc:0.981]
Epoch [71/120    avg_loss:0.011, val_acc:0.982]
Epoch [72/120    avg_loss:0.049, val_acc:0.966]
Epoch [73/120    avg_loss:0.031, val_acc:0.972]
Epoch [74/120    avg_loss:0.016, val_acc:0.971]
Epoch [75/120    avg_loss:0.017, val_acc:0.974]
Epoch [76/120    avg_loss:0.013, val_acc:0.976]
Epoch [77/120    avg_loss:0.017, val_acc:0.981]
Epoch [78/120    avg_loss:0.025, val_acc:0.977]
Epoch [79/120    avg_loss:0.032, val_acc:0.972]
Epoch [80/120    avg_loss:0.015, val_acc:0.972]
Epoch [81/120    avg_loss:0.012, val_acc:0.979]
Epoch [82/120    avg_loss:0.017, val_acc:0.979]
Epoch [83/120    avg_loss:0.033, val_acc:0.978]
Epoch [84/120    avg_loss:0.019, val_acc:0.972]
Epoch [85/120    avg_loss:0.011, val_acc:0.975]
Epoch [86/120    avg_loss:0.011, val_acc:0.976]
Epoch [87/120    avg_loss:0.009, val_acc:0.976]
Epoch [88/120    avg_loss:0.008, val_acc:0.978]
Epoch [89/120    avg_loss:0.010, val_acc:0.976]
Epoch [90/120    avg_loss:0.011, val_acc:0.977]
Epoch [91/120    avg_loss:0.008, val_acc:0.979]
Epoch [92/120    avg_loss:0.008, val_acc:0.977]
Epoch [93/120    avg_loss:0.009, val_acc:0.977]
Epoch [94/120    avg_loss:0.007, val_acc:0.977]
Epoch [95/120    avg_loss:0.011, val_acc:0.976]
Epoch [96/120    avg_loss:0.010, val_acc:0.977]
Epoch [97/120    avg_loss:0.006, val_acc:0.978]
Epoch [98/120    avg_loss:0.007, val_acc:0.978]
Epoch [99/120    avg_loss:0.007, val_acc:0.978]
Epoch [100/120    avg_loss:0.006, val_acc:0.978]
Epoch [101/120    avg_loss:0.006, val_acc:0.978]
Epoch [102/120    avg_loss:0.011, val_acc:0.978]
Epoch [103/120    avg_loss:0.006, val_acc:0.979]
Epoch [104/120    avg_loss:0.007, val_acc:0.979]
Epoch [105/120    avg_loss:0.008, val_acc:0.979]
Epoch [106/120    avg_loss:0.006, val_acc:0.979]
Epoch [107/120    avg_loss:0.006, val_acc:0.979]
Epoch [108/120    avg_loss:0.006, val_acc:0.979]
Epoch [109/120    avg_loss:0.006, val_acc:0.979]
Epoch [110/120    avg_loss:0.009, val_acc:0.979]
Epoch [111/120    avg_loss:0.008, val_acc:0.979]
Epoch [112/120    avg_loss:0.006, val_acc:0.979]
Epoch [113/120    avg_loss:0.007, val_acc:0.979]
Epoch [114/120    avg_loss:0.008, val_acc:0.979]
Epoch [115/120    avg_loss:0.006, val_acc:0.979]
Epoch [116/120    avg_loss:0.006, val_acc:0.979]
Epoch [117/120    avg_loss:0.008, val_acc:0.979]
Epoch [118/120    avg_loss:0.006, val_acc:0.979]
Epoch [119/120    avg_loss:0.007, val_acc:0.979]
Epoch [120/120    avg_loss:0.009, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    1    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1261   11    0    0    2    0    0    1    4    6    0    0
     0    0    0]
 [   0    0    0  717    2   11    0    0    0    3    0    5    4    0
     0    5    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    1    6    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    2    0    0    0  843   22    0    0
     0    4    0]
 [   0    0    2    0    0    0    0    0    0    0    6 2190   10    0
     1    0    1]
 [   0    0    3    2    0    1    0    0    0    0    5    0  516    0
     0    5    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    1    0    0    0
  1135    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
    45  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.8970189701897

F1 scores:
[       nan 0.92105263 0.98669797 0.97088693 0.9953271  0.97149373
 0.9939302  0.89285714 0.99883586 0.87179487 0.96952271 0.98804421
 0.96810507 1.         0.9767642  0.90936556 0.97647059]

Kappa:
0.9760130416997785
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5236273780>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.292, val_acc:0.526]
Epoch [2/120    avg_loss:1.721, val_acc:0.594]
Epoch [3/120    avg_loss:1.461, val_acc:0.655]
Epoch [4/120    avg_loss:1.193, val_acc:0.707]
Epoch [5/120    avg_loss:1.001, val_acc:0.691]
Epoch [6/120    avg_loss:0.901, val_acc:0.780]
Epoch [7/120    avg_loss:0.704, val_acc:0.796]
Epoch [8/120    avg_loss:0.742, val_acc:0.779]
Epoch [9/120    avg_loss:0.636, val_acc:0.721]
Epoch [10/120    avg_loss:0.616, val_acc:0.798]
Epoch [11/120    avg_loss:0.519, val_acc:0.832]
Epoch [12/120    avg_loss:0.457, val_acc:0.850]
Epoch [13/120    avg_loss:0.420, val_acc:0.776]
Epoch [14/120    avg_loss:0.431, val_acc:0.869]
Epoch [15/120    avg_loss:0.397, val_acc:0.779]
Epoch [16/120    avg_loss:0.379, val_acc:0.876]
Epoch [17/120    avg_loss:0.264, val_acc:0.869]
Epoch [18/120    avg_loss:0.251, val_acc:0.816]
Epoch [19/120    avg_loss:0.304, val_acc:0.858]
Epoch [20/120    avg_loss:0.265, val_acc:0.908]
Epoch [21/120    avg_loss:0.262, val_acc:0.912]
Epoch [22/120    avg_loss:0.198, val_acc:0.925]
Epoch [23/120    avg_loss:0.177, val_acc:0.928]
Epoch [24/120    avg_loss:0.181, val_acc:0.907]
Epoch [25/120    avg_loss:0.158, val_acc:0.917]
Epoch [26/120    avg_loss:0.176, val_acc:0.919]
Epoch [27/120    avg_loss:0.211, val_acc:0.869]
Epoch [28/120    avg_loss:0.124, val_acc:0.948]
Epoch [29/120    avg_loss:0.090, val_acc:0.917]
Epoch [30/120    avg_loss:0.160, val_acc:0.944]
Epoch [31/120    avg_loss:0.111, val_acc:0.945]
Epoch [32/120    avg_loss:0.115, val_acc:0.954]
Epoch [33/120    avg_loss:0.072, val_acc:0.943]
Epoch [34/120    avg_loss:0.118, val_acc:0.944]
Epoch [35/120    avg_loss:0.081, val_acc:0.946]
Epoch [36/120    avg_loss:0.133, val_acc:0.914]
Epoch [37/120    avg_loss:0.103, val_acc:0.959]
Epoch [38/120    avg_loss:0.048, val_acc:0.957]
Epoch [39/120    avg_loss:0.096, val_acc:0.956]
Epoch [40/120    avg_loss:0.064, val_acc:0.947]
Epoch [41/120    avg_loss:0.065, val_acc:0.969]
Epoch [42/120    avg_loss:0.057, val_acc:0.959]
Epoch [43/120    avg_loss:0.046, val_acc:0.961]
Epoch [44/120    avg_loss:0.089, val_acc:0.936]
Epoch [45/120    avg_loss:0.075, val_acc:0.958]
Epoch [46/120    avg_loss:0.045, val_acc:0.956]
Epoch [47/120    avg_loss:0.065, val_acc:0.965]
Epoch [48/120    avg_loss:0.060, val_acc:0.965]
Epoch [49/120    avg_loss:0.075, val_acc:0.938]
Epoch [50/120    avg_loss:0.060, val_acc:0.967]
Epoch [51/120    avg_loss:0.038, val_acc:0.969]
Epoch [52/120    avg_loss:0.041, val_acc:0.946]
Epoch [53/120    avg_loss:0.088, val_acc:0.939]
Epoch [54/120    avg_loss:0.068, val_acc:0.931]
Epoch [55/120    avg_loss:0.092, val_acc:0.974]
Epoch [56/120    avg_loss:0.038, val_acc:0.967]
Epoch [57/120    avg_loss:0.030, val_acc:0.969]
Epoch [58/120    avg_loss:0.031, val_acc:0.964]
Epoch [59/120    avg_loss:0.034, val_acc:0.970]
Epoch [60/120    avg_loss:0.033, val_acc:0.966]
Epoch [61/120    avg_loss:0.047, val_acc:0.947]
Epoch [62/120    avg_loss:0.043, val_acc:0.961]
Epoch [63/120    avg_loss:0.211, val_acc:0.926]
Epoch [64/120    avg_loss:0.063, val_acc:0.945]
Epoch [65/120    avg_loss:0.054, val_acc:0.963]
Epoch [66/120    avg_loss:0.046, val_acc:0.967]
Epoch [67/120    avg_loss:0.044, val_acc:0.963]
Epoch [68/120    avg_loss:0.021, val_acc:0.967]
Epoch [69/120    avg_loss:0.032, val_acc:0.976]
Epoch [70/120    avg_loss:0.017, val_acc:0.974]
Epoch [71/120    avg_loss:0.013, val_acc:0.976]
Epoch [72/120    avg_loss:0.014, val_acc:0.977]
Epoch [73/120    avg_loss:0.018, val_acc:0.977]
Epoch [74/120    avg_loss:0.012, val_acc:0.980]
Epoch [75/120    avg_loss:0.010, val_acc:0.978]
Epoch [76/120    avg_loss:0.012, val_acc:0.981]
Epoch [77/120    avg_loss:0.012, val_acc:0.981]
Epoch [78/120    avg_loss:0.014, val_acc:0.983]
Epoch [79/120    avg_loss:0.011, val_acc:0.981]
Epoch [80/120    avg_loss:0.012, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.984]
Epoch [83/120    avg_loss:0.009, val_acc:0.982]
Epoch [84/120    avg_loss:0.009, val_acc:0.984]
Epoch [85/120    avg_loss:0.008, val_acc:0.983]
Epoch [86/120    avg_loss:0.012, val_acc:0.981]
Epoch [87/120    avg_loss:0.011, val_acc:0.984]
Epoch [88/120    avg_loss:0.014, val_acc:0.986]
Epoch [89/120    avg_loss:0.011, val_acc:0.982]
Epoch [90/120    avg_loss:0.011, val_acc:0.984]
Epoch [91/120    avg_loss:0.013, val_acc:0.985]
Epoch [92/120    avg_loss:0.011, val_acc:0.984]
Epoch [93/120    avg_loss:0.011, val_acc:0.984]
Epoch [94/120    avg_loss:0.010, val_acc:0.985]
Epoch [95/120    avg_loss:0.011, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.986]
Epoch [97/120    avg_loss:0.011, val_acc:0.984]
Epoch [98/120    avg_loss:0.010, val_acc:0.986]
Epoch [99/120    avg_loss:0.010, val_acc:0.985]
Epoch [100/120    avg_loss:0.016, val_acc:0.985]
Epoch [101/120    avg_loss:0.012, val_acc:0.987]
Epoch [102/120    avg_loss:0.010, val_acc:0.987]
Epoch [103/120    avg_loss:0.008, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.987]
Epoch [105/120    avg_loss:0.014, val_acc:0.986]
Epoch [106/120    avg_loss:0.012, val_acc:0.987]
Epoch [107/120    avg_loss:0.009, val_acc:0.988]
Epoch [108/120    avg_loss:0.009, val_acc:0.988]
Epoch [109/120    avg_loss:0.013, val_acc:0.987]
Epoch [110/120    avg_loss:0.007, val_acc:0.985]
Epoch [111/120    avg_loss:0.008, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.987]
Epoch [113/120    avg_loss:0.011, val_acc:0.987]
Epoch [114/120    avg_loss:0.009, val_acc:0.984]
Epoch [115/120    avg_loss:0.009, val_acc:0.985]
Epoch [116/120    avg_loss:0.008, val_acc:0.985]
Epoch [117/120    avg_loss:0.012, val_acc:0.987]
Epoch [118/120    avg_loss:0.010, val_acc:0.987]
Epoch [119/120    avg_loss:0.010, val_acc:0.987]
Epoch [120/120    avg_loss:0.008, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    3    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    2    0    0    2    0    0    4    1    9    0    0
     0    0    0]
 [   0    0    0  723    0    1    0    0    0    5    0    6   12    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    5    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    2  843   21    0    0
     0    2    0]
 [   0    0    1    0    0    0    0    0    0    6    6 2184   11    1
     1    0    0]
 [   0    0    0    0    0    2    0    0    0    0    5    0  521    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    72  275    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.76693766937669

F1 scores:
[       nan 0.96202532 0.98868513 0.98100407 0.99528302 0.99078341
 0.99695586 0.90909091 1.         0.67924528 0.97456647 0.98600451
 0.96660482 0.99730458 0.96416382 0.86887836 0.96551724]

Kappa:
0.9745275096662696
creating ./logs/logs-2022-01-18IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f47b135b780>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.329, val_acc:0.472]
Epoch [2/120    avg_loss:1.802, val_acc:0.585]
Epoch [3/120    avg_loss:1.487, val_acc:0.609]
Epoch [4/120    avg_loss:1.243, val_acc:0.600]
Epoch [5/120    avg_loss:1.035, val_acc:0.731]
Epoch [6/120    avg_loss:0.897, val_acc:0.682]
Epoch [7/120    avg_loss:0.802, val_acc:0.765]
Epoch [8/120    avg_loss:0.712, val_acc:0.719]
Epoch [9/120    avg_loss:0.582, val_acc:0.787]
Epoch [10/120    avg_loss:0.574, val_acc:0.804]
Epoch [11/120    avg_loss:0.482, val_acc:0.831]
Epoch [12/120    avg_loss:0.515, val_acc:0.768]
Epoch [13/120    avg_loss:0.488, val_acc:0.853]
Epoch [14/120    avg_loss:0.407, val_acc:0.830]
Epoch [15/120    avg_loss:0.346, val_acc:0.856]
Epoch [16/120    avg_loss:0.320, val_acc:0.897]
Epoch [17/120    avg_loss:0.324, val_acc:0.863]
Epoch [18/120    avg_loss:0.329, val_acc:0.864]
Epoch [19/120    avg_loss:0.327, val_acc:0.892]
Epoch [20/120    avg_loss:0.296, val_acc:0.881]
Epoch [21/120    avg_loss:0.309, val_acc:0.892]
Epoch [22/120    avg_loss:0.224, val_acc:0.888]
Epoch [23/120    avg_loss:0.216, val_acc:0.903]
Epoch [24/120    avg_loss:0.286, val_acc:0.878]
Epoch [25/120    avg_loss:0.277, val_acc:0.912]
Epoch [26/120    avg_loss:0.228, val_acc:0.901]
Epoch [27/120    avg_loss:0.232, val_acc:0.889]
Epoch [28/120    avg_loss:0.265, val_acc:0.926]
Epoch [29/120    avg_loss:0.155, val_acc:0.932]
Epoch [30/120    avg_loss:0.138, val_acc:0.900]
Epoch [31/120    avg_loss:0.133, val_acc:0.929]
Epoch [32/120    avg_loss:0.195, val_acc:0.884]
Epoch [33/120    avg_loss:0.163, val_acc:0.909]
Epoch [34/120    avg_loss:0.123, val_acc:0.917]
Epoch [35/120    avg_loss:0.125, val_acc:0.927]
Epoch [36/120    avg_loss:0.108, val_acc:0.914]
Epoch [37/120    avg_loss:0.094, val_acc:0.930]
Epoch [38/120    avg_loss:0.103, val_acc:0.938]
Epoch [39/120    avg_loss:0.104, val_acc:0.892]
Epoch [40/120    avg_loss:0.126, val_acc:0.936]
Epoch [41/120    avg_loss:0.129, val_acc:0.930]
Epoch [42/120    avg_loss:0.121, val_acc:0.929]
Epoch [43/120    avg_loss:0.085, val_acc:0.950]
Epoch [44/120    avg_loss:0.062, val_acc:0.945]
Epoch [45/120    avg_loss:0.073, val_acc:0.955]
Epoch [46/120    avg_loss:0.140, val_acc:0.919]
Epoch [47/120    avg_loss:0.110, val_acc:0.947]
Epoch [48/120    avg_loss:0.072, val_acc:0.941]
Epoch [49/120    avg_loss:0.062, val_acc:0.948]
Epoch [50/120    avg_loss:0.049, val_acc:0.947]
Epoch [51/120    avg_loss:0.095, val_acc:0.946]
Epoch [52/120    avg_loss:0.131, val_acc:0.923]
Epoch [53/120    avg_loss:0.095, val_acc:0.964]
Epoch [54/120    avg_loss:0.078, val_acc:0.949]
Epoch [55/120    avg_loss:0.084, val_acc:0.955]
Epoch [56/120    avg_loss:0.055, val_acc:0.959]
Epoch [57/120    avg_loss:0.046, val_acc:0.952]
Epoch [58/120    avg_loss:0.064, val_acc:0.950]
Epoch [59/120    avg_loss:0.038, val_acc:0.964]
Epoch [60/120    avg_loss:0.046, val_acc:0.957]
Epoch [61/120    avg_loss:0.037, val_acc:0.956]
Epoch [62/120    avg_loss:0.035, val_acc:0.962]
Epoch [63/120    avg_loss:0.023, val_acc:0.971]
Epoch [64/120    avg_loss:0.032, val_acc:0.972]
Epoch [65/120    avg_loss:0.030, val_acc:0.956]
Epoch [66/120    avg_loss:0.054, val_acc:0.948]
Epoch [67/120    avg_loss:0.062, val_acc:0.961]
Epoch [68/120    avg_loss:0.023, val_acc:0.968]
Epoch [69/120    avg_loss:0.038, val_acc:0.955]
Epoch [70/120    avg_loss:0.038, val_acc:0.960]
Epoch [71/120    avg_loss:0.052, val_acc:0.962]
Epoch [72/120    avg_loss:0.055, val_acc:0.966]
Epoch [73/120    avg_loss:0.029, val_acc:0.971]
Epoch [74/120    avg_loss:0.031, val_acc:0.964]
Epoch [75/120    avg_loss:0.073, val_acc:0.956]
Epoch [76/120    avg_loss:0.032, val_acc:0.968]
Epoch [77/120    avg_loss:0.041, val_acc:0.957]
Epoch [78/120    avg_loss:0.031, val_acc:0.963]
Epoch [79/120    avg_loss:0.023, val_acc:0.966]
Epoch [80/120    avg_loss:0.023, val_acc:0.970]
Epoch [81/120    avg_loss:0.021, val_acc:0.969]
Epoch [82/120    avg_loss:0.019, val_acc:0.968]
Epoch [83/120    avg_loss:0.014, val_acc:0.971]
Epoch [84/120    avg_loss:0.018, val_acc:0.974]
Epoch [85/120    avg_loss:0.020, val_acc:0.970]
Epoch [86/120    avg_loss:0.014, val_acc:0.973]
Epoch [87/120    avg_loss:0.013, val_acc:0.972]
Epoch [88/120    avg_loss:0.013, val_acc:0.972]
Epoch [89/120    avg_loss:0.015, val_acc:0.974]
Epoch [90/120    avg_loss:0.014, val_acc:0.973]
Epoch [91/120    avg_loss:0.016, val_acc:0.972]
Epoch [92/120    avg_loss:0.013, val_acc:0.976]
Epoch [93/120    avg_loss:0.012, val_acc:0.976]
Epoch [94/120    avg_loss:0.012, val_acc:0.974]
Epoch [95/120    avg_loss:0.017, val_acc:0.974]
Epoch [96/120    avg_loss:0.014, val_acc:0.980]
Epoch [97/120    avg_loss:0.011, val_acc:0.977]
Epoch [98/120    avg_loss:0.011, val_acc:0.976]
Epoch [99/120    avg_loss:0.013, val_acc:0.975]
Epoch [100/120    avg_loss:0.011, val_acc:0.975]
Epoch [101/120    avg_loss:0.010, val_acc:0.975]
Epoch [102/120    avg_loss:0.013, val_acc:0.974]
Epoch [103/120    avg_loss:0.015, val_acc:0.974]
Epoch [104/120    avg_loss:0.038, val_acc:0.975]
Epoch [105/120    avg_loss:0.015, val_acc:0.976]
Epoch [106/120    avg_loss:0.009, val_acc:0.977]
Epoch [107/120    avg_loss:0.013, val_acc:0.975]
Epoch [108/120    avg_loss:0.013, val_acc:0.974]
Epoch [109/120    avg_loss:0.014, val_acc:0.974]
Epoch [110/120    avg_loss:0.012, val_acc:0.975]
Epoch [111/120    avg_loss:0.014, val_acc:0.975]
Epoch [112/120    avg_loss:0.018, val_acc:0.975]
Epoch [113/120    avg_loss:0.011, val_acc:0.975]
Epoch [114/120    avg_loss:0.012, val_acc:0.975]
Epoch [115/120    avg_loss:0.012, val_acc:0.975]
Epoch [116/120    avg_loss:0.015, val_acc:0.975]
Epoch [117/120    avg_loss:0.010, val_acc:0.975]
Epoch [118/120    avg_loss:0.010, val_acc:0.975]
Epoch [119/120    avg_loss:0.011, val_acc:0.975]
Epoch [120/120    avg_loss:0.011, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1267    4    0    0    0    0    0    0    8    4    2    0
     0    0    0]
 [   0    0    0  715    0   10    0    0    0    1    6    0   15    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0  860    6    3    0
     0    3    0]
 [   0    0    7    1    0    0    2    0    0    0   23 2154   13    9
     0    1    0]
 [   0    0    0    0    0    4    0    0    0    0    0    0  525    0
     0    2    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    52  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.84281842818429

F1 scores:
[       nan 0.975      0.99023056 0.97345133 0.99528302 0.98083427
 0.99314547 1.         0.99883856 0.97297297 0.96792347 0.98491084
 0.95978062 0.9762533  0.97383097 0.90712074 0.9704142 ]

Kappa:
0.9754209309917107
